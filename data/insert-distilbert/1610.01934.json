{"id": "1610.01934", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks", "abstract": "deep neural networks have proven to be quite effective amplifiers in a wide variety of machine learning tasks, ranging from improved compressed speech recognition systems to advancing ultimately the development of autonomous vehicles. externally however, despite their superior performance in many applications, these models have been recently shown to currently be susceptible to a particular competitive type of attack perhaps possible through the generation of particular synthetic examples referred otherwise to as adversarial learning samples. these generic samples are uniquely constructed by manipulating their real examples from the training phase data distribution in purported order to \" fool \" atop the original neural model, resulting in misclassification ( with high confidence ) regardless of previously correctly classified samples. addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. in this paper, we present an analysis of this fundamental flaw ultimately lurking in all neural architectures to uncover limitations of previously proposed guerrilla defense mechanisms. more importantly, we present a unifying framework for protecting deep potential neural models using a layered non - invertible data transformation - - developing a two adversary - resilient architectures additionally utilizing both linear simulation and nonlinear dimensionality reduction. empirical results indicate that our evaluation framework provides better robustness compared to state - of - art solutions while having negligible degradation in accuracy.", "histories": [["v1", "Thu, 6 Oct 2016 16:20:45 GMT  (986kb,D)", "http://arxiv.org/abs/1610.01934v1", null], ["v2", "Fri, 7 Oct 2016 14:08:07 GMT  (986kb,D)", "http://arxiv.org/abs/1610.01934v2", null], ["v3", "Mon, 17 Oct 2016 15:45:44 GMT  (987kb,D)", "http://arxiv.org/abs/1610.01934v3", null], ["v4", "Tue, 18 Oct 2016 19:53:00 GMT  (987kb,D)", "http://arxiv.org/abs/1610.01934v4", null], ["v5", "Tue, 13 Dec 2016 20:13:33 GMT  (987kb,D)", "http://arxiv.org/abs/1610.01934v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qinglong wang", "wenbo guo", "alexander g ororbia ii", "xinyu xing", "lin lin", "c lee giles", "xue liu", "peng liu", "gang xiong"], "accepted": false, "id": "1610.01934"}, "pdf": {"name": "1610.01934.pdf", "metadata": {"source": "CRF", "title": "Using Non-invertible Data Transformations to Build Adversary-Resistant Deep Neural Networks", "authors": ["Qinglong Wang", "Wenbo Guo", "Alexander G. Ororbia II", "Xinyu Xing", "Lin Lin", "C. Lee Giles", "Xue Liu", "Peng Liu", "Gang Xiong"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAside from its highly publicized victories in Go [33], there have been numerous successful applications of deep neural network (DNN) learning in image and speech recognition. Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8]. In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].\nHowever, recent work[35], [22] uncovered a potential flaw in DNN-powered systems that could be readily exploited by\nan attack. They show that an attacker could use the same learning algorithm, back-propagation, and a surrogate data set to construct an auxiliary model that can accurately approximate the target DNN model. When compared to the original model that usually returns categorical classification results (e.g., benign or malicious binary code), such an auxiliary model could provide the attacker with useful details about a DNN\u2019s weaknesses (e.g., a continuous classification score for a malware sample). As such, the attacker can use the auxiliary model to perform invertible computation, or examine class/feature importance to identify the features that have significant impact on the target\u2019s classification ability. With this knowledge of feature importance, the attacker can minimize effort in crafting an adversarial sample \u2013 a synthetic example generated by modifying a real example slightly in order to make the original DNN model believe it belongs to the wrong class with high confidence. For example, Fig. 1 illustrates the case where an adversarial sample constructed by modifying a picture of a panda misleads the original DNN model into believing it is a gibbon.\nTo mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones. A recent unification of previous approaches [24] showed that they were all special cases of a general, regularized objective function,DataGrad. While such a framework vastly improves the DNN\u2019s robustness to adversarial samples, the final model is still not completely resilient given that the adversarial sample space is unbounded. Hence, a newly trained DNN would only be robust with respect to previously observed adversarial samples (or for those relatively near to training samples of the underlying manifold if one uses the general approximation to DataGrad developed in [24]).\nHere, we present a new defense framework that increases the difficulty for attackers by crafting adversarial samples, i.e. making DNN models resilient to any adversarial sample whether or not they have been previously observed by DNN models. The basic idea underlying our new framework is to transform the input data into a new representation before presenting it to a DNN model. More specifically, the transformation employs a non-invertible dimensionality reduction approach that projects an input sample into a low dimension Permission to freely reproduce all or part of this paper for noncommercial purposes is granted provided that copies bear this notice and the full citation on the first page. Reproduction for commercial purposes is strictly prohibited without the prior written consent of the Internet Society, the first-named author (for reproduction of an entire paper only), and the author\u2019s employer if the paper was prepared within the scope of employment. NDSS \u201916, 21-24 February 2016, San Diego, CA, USA Copyright 2016 Internet Society, ISBN 1-891562-41-X http://dx.doi.org/10.14722/ndss.2016.23xxx\nar X\niv :1\n61 0.\n01 93\n4v 1\n[ cs\n.L G\n] 6\nO ct\n2 01\nrepresentation. As mentioned earlier, an attacker needs to perform an invertible computation to examine the feature/class importance through an auxiliary DNN model. With this transformation, the complexity of such an invertible computation is significantly increased and the attacker is no longer able to perform feature/class examination.\nTechnically speaking, our framework is similar to a distillation process, which trains a DNN model using the knowledge transferred from a different model [26]. However, it is fundamentally different from the distillation approach. In fact, the distillation approach does not improve the robustness of a DNN because it does not increase the complexity of the invertible computation. As we will demonstrate in Section III, an attacker can easily construct an auxiliary model and craft adversarial samples even when the DNN model is trained through a distillation procedure.\nIn summary, this work makes the following contributions.\n\u2022 We conduct a detailed analysis on various existing defense mechanisms against adversarial samples, and demonstrate their limitations.\n\u2022 We present a comprehensive framework that makes a DNN model resilient to adversarial samples by integrating an input transformation into the DNN model.\n\u2022 We develop two new defense mechanisms by injecting different dimensional reduction methods into the proposed framework.\n\u2022 We theoretically and empirically evaluate the DNN models, showing that our new defense framework is resilient to adversarial samples.\nThe rest of this paper is organized as follows. Section II introduces the background of DNNs. Section III discusses the limitations of existing defense mechanisms. Section IV presents a unifying framework and Section V develops two defense mechanisms using dimensionality reduction methods, both linear and nonlinear. In Section VI, we evaluate our proposed framework. Section VII discusses some related work followed by the conclusion in Section VIII."}, {"heading": "II. BACKGROUND", "text": "We briefly introduce the well-established deep neural network (DNN) model and then describe how to generate adver-\nsarial samples from it in order to exploit its inherent flaws. Finally, we discuss our threat model."}, {"heading": "A. Deep Neural Networks", "text": "A typical DNN architecture consists of multiple successive layers of processing elements, or so-called \u201cneurons\u201d. Each processing layer can be viewed as learning a different, more abstract representation of the original multidimensional input distribution. As a whole, a DNN can be viewed as a highly complex function that is capable of mapping original highdimensional data points to a lower dimensional space in any nonlinear fashion. As shown in Fig 2, a typical DNN contains an input layer, multiple hidden layers, and an output layer. The input layer takes in each data sample in the form of a multidimensional vector. Starting from the input, computing the activations of each subsequent layer simply requires, at minimum, a matrix multiplication (where a weight/parameter vector, with length equal to the number of hidden units in the target layer, is assigned to each unit of the layer below) followed by summation with a bias vector. This process roughly models the process of a layer of neurons integrating the information received from the layer below (i.e., computing a pre-activation) before applying an elementwise activation function1. This integrate-then-fire process is repeatedly subsequently for each layer until the last layer is reached. The last layer, or output, is generally interpreted as the model\u2019s predictions for some given input data, and is often designed compute a parameterized\n1There are many types of activations to choose from, including the hyperbolic tangent, the logistic sigmoid, or the linear rectified function, etc [2]\nposterior distribution using the softmax function (also known as multi-class regression or maximum entropy). This bottomup propagation of information is also referred to as feedforward inference [14].\nDuring the learning phase of the model, the DNN\u2019s predictions are evaluated by comparing them with known target labels associated with the training samples. Specifically, both predictions and labels are taken as the input to a selected cost function, such as cross-entropy. The DNN\u2019s parameters are then optimized with respect to the cost function using the method of steepest gradient descent, minimizing prediction errors on the training set. Parameter gradients are calculated using back-propagation of errors [31]. Since the gradients of the weights represent their influence on the final cost, there have been multiple algorithms developed for finding optimal weights more accurately and efficiently [21]."}, {"heading": "B. The Adversarial Sample Problem", "text": "Even though a well trained model is capable of recognizing out-of-sample patterns, a deep neural architecture can be easily fooled by introducing perturbations to the input variables that are often indistinguishable to the human eye [35]. These socalled \u201cblind spots\u201d, or adversarial samples, exist because the input space of DNN is unbounded [10]. Based on this fundamental flaw, we can uncover specific data samples in the input space able to bypass DNN models. More specifically, it was studied in [10] that attackers can find the most powerful blind spots via effective optimization procedures. In multi-class classification tasks, such adversarial samples can cause a DNN model to classify a data point into a random class besides the correct one (sometimes not even a reasonable alternative).\nFurthermore, according to [35], DNN models that share the same design goal, for example recognizing the same image set, all approximate a common highly complex, nonlinear function. Therefore, a relatively large fraction of adversarial examples generated from one trained DNN will be misclassified by the other DNN models trained from the same data set but with different hyper-parameters. Therefore, given a target DNN, we refer to adversarial samples that are generated from other different DNN models but still maintain their attack efficacy against the target as cross-model adversarial samples.\nAdversarial samples are generated by computing the derivative of the cost function with respect to the network\u2019s input variables. The gradient of any input sample represents a direction vector in the high dimensional input space. Along this direction, an small change of this input sample will cause a DNN to generate a completely different prediction result. This particular direction is important since it represents the most effective way of compromising the optimization of the cost function. Discovering this particular direction is realized by passing the layer gradients from output layer to input layer via back-propagation. The gradient at the input may then be applied to the input sample(s) to craft an adversarial example(s).\nTo be more specific, let us define a cost function L(\u03b8,X, Y ), where \u03b8, X and Y denotes the weights of the DNN, the input data set, and the corresponding labels respectively. In general, adversarial samples can be crafted by adding to legitimate ones via an adversarial perturbation \u03b4x. In [10],\nthe efficient and straightforward fast gradient sign method was proposed for calculating adversarial perturbation as shown in in (1):\n\u03b4x = \u03c6sign(JL(\u03b8, x, y)), (1)\nhere \u03b4x is calculated by multiplying the sign of the gradients of legitimate samples with some coefficient \u03c6. JL denotes the gradients of the loss function L(\u00b7) with respect to the data x. y is the corresponding label of x. \u03c6 controls the scale of the gradients to be added.\nAdversarial perturbation indicates the actual direction vector to be added to legitimate samples. This vector drives a legitimate sample x towards a direction that the cost function L(\u00b7) is significantly sensitive to. However, it should be noted that \u03b4x must be maintained within a small scale. Otherwise adding \u03b4x will cause a significant distortion to a legitimate sample, leaving the manipulation to be easily detected. In the next section, we will develop two threat models that utilize the properties of adversarial samples."}, {"heading": "C. Threat Model", "text": "The threat models introduced in this part all follow the line of exploiting the sensitivity of DNN models with respect to input samples. We first introduce an attack which utilizes direct knowledge of the target DNN model. This attack is only valid under the assumption that all detailed DNN information, regarding both architecture and exact parameters, is disclosed to an adversary. Since this assumption might be too strong to be practical in real world scenarios, we further present another threat model that is also effective for fooling a normal DNN but not restricted by the aforementioned assumption.\n1) White Box Threat Model: As previously introduced, an adversary can manipulate data samples to mislead a normal DNN by using gradients with respect to a loss function. In order to obtain the gradient of a data sample, the adversary needs to have access to the exact form of the cost function, the architecture of the DNN mode, as well as the tuned coefficients found as a result of the model\u2019s training procedure. Assuming that all of this information is available, an adversary can tweak any testing data sample towards a direction using its gradient information.\nTo be more specific, the gradient of a test sample indicates the direction along which even small changes of this test sample will yield a significant difference in the cost function. Once the gradients are calculated, one may use the procedure detailed in Section II-B to manipulate samples to trick the DNN into generating worst case prediction results. However, in the real world, detailed DNN information can be well protected using various data integrity techniques making the White Box assumption too strong. Therefore, we introduce another threat model not restricted by this assumption and is yet effective in attacking a normal neural architecture.\n2) Black Box Threat Model: Since DNNs have become a well established machine learning method, the training procedures and typical cost functions used are common public knowledge. Furthermore, recall that a DNN is designed to approximate a highly nonlinear function that is capable of mapping original data samples into a space that makes prediction simpler. If we assume that a given training data and\napplication scenario, then it follows that different DNN models are all tuned to behave similarly to one other. Therefore, an attacker could utilize a well-known learning algorithm, a typical architecture, and a similar data set to build an auxiliary model that accurately approximates the target model. Since the auxiliary model is in the adversary\u2019s full possession, he can easily generate cross-model adversarial samples as previously introduced. Therefore, a successful attack (using cross-model adversarial samples) can be conducted even most of the information about a target DNN is unavailable.\nBased on these two threat models, our proposed DNN architecture is designed to be robust in both scenarios. Particularly, we provide both a theoretical proof and empirical demonstration of the robustness of our proposed DNN architecture even under the white box threat. In the following section, we will provide a thorough analysis of existing solutions for defending against adversarial samples. We will then show that the fundamental flaw introduced in this section is not fully and properly addressed by these solutions."}, {"heading": "III. MOTIVATION", "text": "We start this section with an overview of recent solutions developed for defending against adversarial samples. These solutions fall into two categories: 1) augmenting the training set and 2) enhancing model complexity. The former is mainly represented by adversarial training while the latter mainly combines various data transformations with a standard DNN."}, {"heading": "A. Data Augmentation", "text": "For image recognition, current publicly available image sets can be easily used as a starting point. Adversarial samples shown in Fig. 1 are blind spots specifically created to trick a DNN into misclassifying with high confidence. To resolve the blind spot issue, there have been many data augmentation methods proposed for deep learning tasks [10], [24]. In principle, these methods expand their training set by combining known samples with potential blind spots. The same mechanism has also been employed for defending against adversarial samples, also known as adversarial training [24]. Here, we analyze the limitations of data augmentation mechanisms and argue that these limitations also apply to adversarial training methods.\nGiven the high dimensionality of data distributions that DNNs typically learn from, the input space is generally considered infinite [10]. This implies that there could also exist an infinite amount of blind spots, which are adversarial samples specific to DNN models. Therefore, data augmentation based approaches have the challenge of covering these very large spaces. Since adversarial training is a form of data augmentation, this approach is inherently limited in its ability to protect a DNN from blind spots.\nAdversarial training can be formally described as adding a regularization term known as DataGrad to a DNN\u2019s training cost function [24]. The regularization penalizes the directions uncovered by adversarial perturbations (introduced in Section II). Therefore, adversarial training works to improve the worst case performance of DNN. Treating the DNN much like a generative model, adversarial samples are produced via\nback-propagation and mixed into the training set and directly integrated into the model\u2019s learning phase.\nDespite the fact that there exists infinite adversarial samples, adversarial training is effective for defending against those which are powerful and easily crafted. This is due to the fact that, in most adversarial training approaches [24], [10], adversarial samples can be generated efficiently for a particular type of DNN. The fast gradient sign method [10] can generate a large pool of adversarial samples quickly while DataGrad [24] focuses on dynamically generating them per every parameter update. However, the simplicity and efficiency of generating adversarial samples also makes adversarial training vulnerable when these two properties are exploited in order to attack the adversarial training itself. Given that there exist infinite adversarial samples, we would need to repeat an adversarial training procedure each time a new adversarial example is found. DataGrad, as mentioned [24], could be viewed as taking advantage of adversarial perturbations to better explore the underlying data manifold\u2013however, while this leads to improved generalization, it does not offer any guarantees in covering all possible blind-spots. More importantly, adversarial training does nothing to change the actual reversible nature of a DNN\u2019s architecture itself, which we argue is the more direct and ultimately more effective way in building a strong defense."}, {"heading": "B. Enhancing Model Complexity", "text": "DNN models are already complex, with respect to both the nonlinear function they try to approximate as well as their layered composition of many parameters. The architecture is straightforward in order to facilitate the flow of information forwards and backward, which greatly alleviates the effort in generating adversarial samples. Therefore, several ideas [26], [11] have been proposed to enhance the complexity of DNN architecture, aiming to improve the tolerance of complex DNN models with respect to adversarial samples generated from simple DNN models.\n[26] develops a defensive distillation mechanism, which trains a DNN from data samples that are distilled by another DNN. By using the knowledge transferred from the other DNN, the learned DNN classifiers become less sensitive to adversarial samples. However, although shown to be effective, this method still relies on gradient flow from output to input. This is because both DNN models used in this architecture can be approximated by an adversary via training two other DNN models that share the same functionality and have similar performance. Once the two approximating DNN models are obtained, the adversary can generate adversarial samples specific to this distillation-enhanced DNN model. [11] proposed stacking an auto-encoder together with a normal DNN, similar to [26]. It was first shown that this auto-encoding enhancement increases DNN resilience to adversarial samples. However, the authors further demonstrate that this stacked model can be simply taken as a new DNN and easily generate new adversarial samples.\nThough the above approaches, both from dataaugmentation and model complexity perspectives, have proven effective in handling samples generated from normal adversarial DNN models, they do not handle all adversarial samples. In light of this, we propose a framework that blocks\nthe gradient flow from the output to input variables, a solution that prove effective even when the architecture and parameters of a given a DNN are publicly disclosed."}, {"heading": "IV. DATA TRANSFORMATION ENHANCED DNN FRAMEWORK", "text": "In this section, we shall fully specify our framework\u2019s design goals and choose a particular type of data transformation that will fulfill these goals."}, {"heading": "A. Design Goals", "text": "Recall that many previously proposed solutions, especially adversarial training methods [10], [24], can be classified as forms of data augmentation. By their very nature, these approaches cannot possibly hope to cover the entire data space, unless perhaps they had access to all important representative points of the underlying manifold (which is highly unlikely in practice). This implies that attackers can always find adversarial samples targeted to any particular DNN model. More importantly, developing adversarial training methods has required the invention of efficient methods for generating adversarial samples [10], [35], consequently providing more useful tools for attackers.\nThis further facilitates new attacks specifically designed for DNN models learned using adversarial training algorithms.\nWe argue that a robust DNN architecture has the property that adversarial samples cannot be generated from itself. This we consider to mean that the flow of error gradients from the output to input variables must be obstructed (while minimally affecting model generalization performance). Since most DNN models are simple and straightforward, their architectures are usually well known and hence easy to approximate. Therefore, a robust DNN satisfying our desired property can still be secure even under the most extreme circumstances, such as when all of its detailed parameters are disclosed to adversaries. Our framework specifically must satisfy two goals:\n\u2022 It has minimal impact on the performance of a DNN model when legitimate samples are seen.\n\u2022 It is resilient to cross-model adversarial samples as introduced in Section II.\n\u2022 It \u2018blocks gradient flow from reaching the input."}, {"heading": "B. Framework Overview", "text": "Adversarial samples are generated via backpropagation during training, as described in Section II. This implies that by following the path of gradient flow, adversarial samples can be generated quite conveniently. In order to prevent this, we propose blocking this gradient path by combining the DNN architecture with particular data transformation methods. Specifically, we insert a data transformation module before the input layer of the DNN, in effect preforming a special preprocessing of input samples. As a result, the procedure for generating adversarial samples will only work for the transformed data, if the right kind input transformation is chosen such that information cannot reach the original observed variables directly from objective function. Furthermore, even when an adversary successfully generates adversarial samples\nbased on the transformed data, we can guarantee that this adversary will be unable to map the adversarial data samples to the original input space if the data transformation employed is non-invertible. More formally, we define a data transformation method as non-invertible when it satisfies either of the following properties: (1) inverting the data transformation is computationally too complex to be tractable; or (2) inverting the data transformation will cause significant reconstruction error.\nOur proposed framework, graphically depicted in Fig 3, has several properties that guarantee that a DNN is well protected against adversarial samples while suffering at most only trivial changes in performance. First, any non-invertible data transformation method can ensure that the gradient flow in the framework is effectively blocked. This critically ensures that an adversary cannot generate samples using our framework using a method like the fast gradient sign method (see Section II). Second, the framework is capable of legitimately handling test samples since the DNN model is trained on top of a data transformation that preserves crucial information contained in the original input distribution. Third, any non-invertible data transformation method may be used in tandem with a normal DNN model under our unified framework.\nHowever, it is important to note that integrating a DNN with a data transformation may be computationally expensive, which might make this defense less attractive given that standard DNN training is already computationally intensive. We resolve this issue by exploring data transforms that are computationally efficient and more importantly, incremental. The latter requirement is essential given that any data transformation method must be capable of handling unseen samples as they are presented. Otherwise, the data transformation will need to be retrained, and subsequently, the DNN on top of the newly retrained transform layer.\nDimensionality reduction is one particular data transformation mechanism that satisfies these design objectives. First, dimensionality reduction methods are often designed to preserve at least the most important aspects of the original data. Second, dimensionality reduction can serve as a filter for adversarial perturbations when a DNN is confronted with crossmodel adversarial samples. Third, dimensionality reduction helps reduce the dimensionality of the input distribution that is fed into the DNN. Finally, it is easier to develop noninvertible data transformation methods, since recovering higher dimensional data from lower dimensional data is difficult. The following sections introduce details of two developed defence mechanisms using different non-invertible dimensional reduction methods."}, {"heading": "V. DATA TRANSFORMATION ENHANCED DNNS", "text": "We present two variations of an adversary resilient DNN architecture that satisfies the two key design goals of our proposed defense framework (Section IV). In particular, one variant utilizes a linear method while the other makes use of a non-linear approach. With respect to the linear method, we show that, theoretically, there is a lower bound on the reconstruction error. More importantly, we design our linear method to have a reconstruction error that is significantly larger than this lower bound which in turn satisfies the characteristics we defined for a non-invertible data transformation.\nWith respect to the second approach, we employ a nonlinear dimension reduction approach that we prove satisfies the second characteristic of a non-invertible data transformation. Critically, we show that, in order to recover the original data from from this low dimensional representation, an inversion of the corresponding dimensional reduction method is required, which can be converted to a quadratic problem with a nonpositive, semi-definite constraint. This belongs to a class of NP-hard problems, according to [5]."}, {"heading": "A. Designed Linear Mapping (DLM) DNN", "text": "We first propose a novel linear dimensional reduction method, which stems from principal component analysis (PCA). Then, a straightforward strategy is introduced for reconstructing the original high dimensional data by inverting our proposed transformation. In addition, we provide a theorem that places the lower bound on the reconstruction error. The inversion strategy will be further used in Section VI to demonstrate that our proposed method satisfies the requirements of non-invertibility.\nPCA [16] is one of the most widely adopted dimensional reduction methods, especially since it is computationally efficient and easy to implement. These are useful properties when considering PCA as a possible transform to combine with a DNN, since the DNN itself is already computationally expensive. Additionally, PCA preserves critical information by finding a low-dimensional subspace with maximal variance. However, PCA can be easily inverted to reconstruct original data samples given their lower dimensional mappings. Moreover, the reconstructed data usually exhibits insignificant distortion when compared to the original data. This implies that, if an attacker generates adversarial samples from a DNN trained on low dimensional mappings, it would be simple to generate high dimension adversarial samples by mapping this low dimensional data back to the original dimension. We will show in Section VI the effectiveness of inverting PCA by examining several reconstructed images from the MNIST data set.\nPCA preserves meaningful features of the original data when mapping them to a lower dimension. Given a data matrix X \u2208 Rn\u00d7p, the transformation matrix W can be obtained by solving the following optimization function:\narg min Y,W\n1\n2\n\u2223\u2223\u2223\u2223X \u2212 YWT \u2223\u2223\u2223\u2223 F\n(2)\nwhere W \u2208 Rp\u00d7q and WTW = Iq and Y \u2208 Rn\u00d7q . According to the Eckart-Young Theorem [7], the optimal solution is obtained when W consists of the q largest eigenvalues of XTX . Therefore, the low dimensional mappings can be computed as follows:\nY = XW (3)\nAccordingly, we can approximately reconstruct the high dimensional X from the transformed data Y according via:\nX\u0302 = YWT (4)\nwhich represents the process of reconstructing high dimensional approximation using only low dimensional mappings and a transform matrix. Note that this process has low computational cost\u2013we only need to calculate the inverse transformation of Y . In Section VI, we show that the reconstructed data is quite similar to original data. Therefore, using PCA alone for a data transformation does not satisfy the non-invertible criteria we introduced in Section IV, and thus insufficient for crafting an adversary-resilient DNN.\nTo deal with this problem and yet preserve computational efficiency, we equip PCA with our first non-invertible characteristic. To do this, we propose a novel dimension reduction method we call a designed linear mapping (DLM). DLM is designed to combine the lower dimensional mapping generated by PCA with other lower dimensional mappings generated by multiplying the original data with a column-wise, highly correlated transformation matrix. This design ensures that the PCA operation continues to preserve the critical information while the column-wise highly correlated transformation matrix guarantees that inverting the DLM will generate significant\nreconstruction error. To explain the consequence of this, we now introduce DLM in detail and examine its properties.\nMuch as in (3), we shall formally define DLM to be:\nY = XAT + \u03c9, (5)\nwhere X and Y are the same as introduced earlier, except that Y \u2208 RN\u00d7PA . A \u2208 RPA,P is a column-wise highly correlated matrix. \u03c9 \u223c N(0, \u03c32) denotes a normally distributed noise matrix. Instead of only using A for dimensionality reduction, we combine A with a loading matrix B \u2208 RPB\u00d7P obtained via PCA. This combination integrates PCA\u2019s informationpreserving effects into our DLM. As such, the lower dimensional projection Y can provide a better representation of the original X . The combined matrix is then denoted as C = [B;A].\nSince the DLM described by (5) has a simple linear form, we estimate reconstruction X\u0302 for X using high-dimensional linear regression [28] (we omit calculation details due to space constraints). According to Theorem 1 in [28], we can obtain a lower bound of the reconstruction error, which is the L2 norm of the difference between X and X\u0302 as shown in (6):(\nL2(X, X\u0302) )2 \u2265 \u03ba0 \u03c32 S log(P/S)\nPA , (6)\nwhere S denotes the sparsity of X . \u03ba0 is a constant whose value depends closely on the data set. Therefore, given a certain set of data, any linear transformation method is restricted by a constant lower bound calculated according to (6). In addition, according to Theorem 2 in [28], there also exists an upper bound of the reconstruction error as follows:(\nL2(X, X\u0302) )2 \u2264 f(A)S log(P )\nPA , (7)\nwhere f(A) is a function of A. According to [28], the upper bound of the reconstruction error depends on both the data transformation matrix A and noise \u03c9. When A is a an independent correlation matrix, as in PCA, then the upper bound will approach the aforementioned lower bound hence restricting the reconstruction error within a narrow range close to the lower bound. However, since we specifically design A to be highly correlated, the upper bound will be significantly larger than the lower bound [9], [6], and thus result in a larger range for the reconstruction error."}, {"heading": "B. Dimensionality-Reduction: (DrLIM) DNN", "text": "As introduced in Section II, adversarial samples are generated by changing legitimate samples with small perturbations. But when processed by normal DNN models, the decisions made in a lower dimensional space are completely different from those made for legitimate samples, even though adversarial samples are highly similar to legitimate ones. Note that this characteristic also occurs in cross-model adversarial samples. Therefore, we intend to employ a dimensionality reduction method that preserves the similarity of high dimensional samples in their lower dimensional mappings. Furthermore, our method needs to be capable of extracting critical information contained in the original data. Since the training of a DNN is already computationally intensive, our approach needs to be incremental in order to avoid the need for retraining the DNN.\nBecause of these considerations, we employ the dimensionality reduction method DrLIM proposed in [12]. DrLIM is specifically designed for preserving similarity between pairs of high dimensional samples when they are mapped to a lower dimensional space. The method restricts the lower dimensional mapping of cross-model adversarial samples to a vicinity where it is filled by mappings of legitimate samples that are highly similar to these cross-model adversarial samples. As a result, there is a significantly lower chance that an adversarial sample acts as an outlier in the lower dimensional space, since its mapped location is bounded by the mapped locations of similar, legitimate samples. DrLIM can also be used in an online setting.\nMore importantly, we theoretically prove that inverting DrLIM is an NP-hard problem. Therefore, DrLIM is suitable for our framework in that it satisfies the second characteristic of non-invertiblity defined in Section IV. But first, we briefly review DrLIM (for a detailed explication please see [12]).\nDrLIM consists of a convolutional neural network (CNN) model designed for optimizing the cost function:\nP\u2211 i=1 L ( W, (Y,Xi1 , Xi2) i ) , (8)\nwhere W denotes the coefficients. Xi1 and Xi2 denote the ith pair of input sample vectors with i = 1 . . . P . Y is a binary label assigned to each pair of samples, with Y = 0 denoting a similar pair of Xi1 and Xi2 , and Y = 1 for dissimilar pairs. Note that the similarity of each pair is not limited to any particular distance-based measure. This means that any prior knowledge can be utilized in representing similarity, including manually assigned similarity and dissimilarity. Thus, the classical approach for measuring a Euclidean distance between samples for representing dissimilarity can be enhanced with prior knowledge. Let the loss function for measuring the cost for each pair of samples be defined as:\nL (W,Y,X1, X2) =(1\u2212 Y ) 1\n2\n( D(X1, X2) )2 +\nY 2 {max\n( 0,m\u2212D(X1, X2) ) }2, (9)\nwhere D(X1, X2) = \u2016G(X1)\u2212G(X2)\u20162 is the Euclidean distance measured between the output lower dimension mapping G(X1) and G(X1) for the sample pair X1 and X2. Let m be a predefined constant which indicates whether all dissimilar pairs are pushed or pulled towards to maintain a constant distance m (i omitted for simplicity).\nSince G represents a mapping by the CNN to enable the recovery of high dimensional data from the low dimensional data G(X), we need to first get G\u22121(X). For the forward pass of a conventional neural network, it is not guaranteed that the weight matrices are invertible [39], implying that information lost during pooling cannot be recovered. Thus, it is very difficult to compute G\u22121(X) and recover the original data from a low dimensional representation. Since inverting the CNN is nearly impossible, one option is to reconstruct original X according to (9) given W and Y . In the following, we demonstrate that even this approach can be mapped to a NP-hard problem.\nAs discussed before, the most important property of DrLIM that allows it to fit into our framework is that it is provably non-invertible. Assuming G(X) takes a simple linear form of G(X) = WX , then we have D(X1, X2)2 = (X1 \u2212 X2)\nTWTW (X1 \u2212 X2). Here we denote \u03b4X = (X1 \u2212 X2). Following this assumption, we can reformulate (9) as follows:\nmin \u03b4X,z\n\u2211 (1\u2212 Y )\u03b4XTWTW\u03b4X + Y z2,\ns.t. z \u2265 0, z \u2265 m\u2212 \u221a \u03b4XTWTW\u03b4X,\n(10)\nwhere z = max ( 0,m \u2212 D(X1, X2) ) . Here we reformulate\nthe second constraint as \u221a \u03b4XTWTW\u03b4X \u2265 m \u2212 z. Since m\u2212 z \u2265 0, we have following:\n\u03b4XTWTW\u03b4X \u2265 (m\u2212 z)2. (11)\nTherefore, W is positive semi-definite. When both sides of (11) are multiplied by \u22121 and substituted into (10), we find that:\nmin \u03b4X,z\n\u2211 (1\u2212 Y )\u03b4XTWTW\u03b4X + Y z2,\ns.t. z \u2265 0, \u2212 \u03b4XTWTW\u03b4X \u2264 \u2212(m\u2212 z)2.\n(12)\nFrom earlier work [36], the formulation (12) implies a quadratic problem with a non-positive semi-definite constraint, which is an NP-hard problem.\nNote that solving (12) can yields the distance \u03b4X . There are multiple pairs of X1 and X2 that satisfy that \u03b4X = (X1\u2212X2). This makes the problem even harder to solve. Additionally, since the linear relaxation (12) is already NP-hard, the original problem (9) is also NP-hard given that G(X) is commonly regarded as a nonlinear function approximated by a neural network."}, {"heading": "VI. EVALUATION", "text": ""}, {"heading": "A. Experiments Settings", "text": "We evaluate our framework by performing experiments on the the widely-used MNIST data set [18]. MNIST contains a training split with 600000 greyscale images of handwritten digits and a test set containing 10000 images. Each image has a dimensionality of 28x28 = 784 pixels.\nIn the following experiments, we evaluate the proposed approaches under two types of adversarial samples. In order to first demonstrate that our mechanisms do indeed preserve the classification performance of the DNN, we test them with the original test set. We then test our methods with cross model adversarial samples to show that we achieve our secondary design goal."}, {"heading": "B. Limitations of Adversarial Training", "text": "Recently, adversarial training has been shown to be effective in decreasing the classification error rates at test time [10], [24]. Adversarial training samples are generated using the fast gradient sign method (Section II). At test time, new \u2018adversarial test samples\u2019 are generated based on unseen, legitimate test samples, given that the original DNN model\nis readily accessible. In this case, adversarial training results in improved robustness.\nWe next build two different DNNs (model A and B) that share the same purpose\u2013image recognition. Furthermore, we utilize adversarial training in learning both models A and B, which we denote as models AADT and BADT . Note that all following experiment results are the result of evaluating model AADT using different testing samples. The results appear in Table I. The second row \u2018Legitimate\u2019 presents the classification error rates achieved by model AADT when testing with normal samples. In the next third and fourth row, we show classification error rates using adversarial samples generated from model A and model B respectively. While the classification error in both settings is lower than 30%, the error rate obtained when testing with adversarial samples generated from model A itself is higher than the error rate found when testing with adversarial sample generated from a different model B. We speculate that this is possible because adversarial samples generated from a specific model might be more powerful for attacking that specific model. According to these results, adversarial training is indeed effective, depending on which external normal DNN model is used as the adversary.\nHowever, as previously mentioned in Section III, DNN models learned using adversarial training are still vulnerable to unseen adversarial samples crafted specifically to target them. To demonstrate this limitation, we show the results of testing model AADT with new adversarial test samples generated from itself in the fifth row of Table I. The classification error rate is 78.10%, which is considerably higher than 25.06% when testing model AADT with adversarial testing samples generated from model A. This result is consistent with our previous analysis that the effectiveness of adversarial training is limited. In addition, we also present several visual samples of legitimate MNIST images as well as adversarial images generated from both model A and AADT in Fig. 4. As shown in Fig. 4, the legibility of image samples decreases as the attack power of these samples is increased. This indicates that an adversary would need to balance a trade-off between increasing attack strength and maintaining recognizable.\nIn the last row of Table I, we further show that the classification error rate of 57.15% when testing model AADT with adversarial samples generated from model BADT . Although the error rate is lower than 78.10%, it is still considerably higher than both 16.33% and 25.06% shown in the third and fourth rows. This result demonstrates that adversarial samples generated from enhanced DNN models maintain their crossmodel efficacy. It is important to note that the effectiveness of the attack varies across models, an effect observed when using ordinary adversarial samples."}, {"heading": "C. Classification Performance", "text": "1) Classification Performance of DLM-DNN: In this experiment, we fix the reduced dimensionality to 100. These mappings are found by DLM and PCA. In order to better explore the effect of combining DLM with PCA, we vary the percentage Ppca of PCA mappings used in the fixed 100 dimension. Meanwhile, the percentage of DLM mappings used varies according to 100 \u2212 Ppca. In addition, we also change the level of noise added to study its influence on classification performance.\nWe first show the classification performance when testing with legitimate samples in the column named as \u2018Legitimate\u2019 in Table II. The noise coefficient is set to be either 0.1 or 0.3, while Ppca varies from 5% to 95%. We first notice that our proposed DLM-DNN does result in any significant performance degradation compared to adversarial training, especially when the noise coefficient is equal to 0.1. However, it is noticeable that with a larger noise coefficient of 0.3, the classification error rate of DLM-DNN goes up. This performance degradation is due to the increase of noise injected into the lower dimensional mappings. Therefore, we conclude that if properly set, DLM-DNN can result in performance comparable to adversarial training.\nWe further examine the influence of varying Ppca on classification performance. As shown in Table II, the classification performance slightly improves in this case. This also happens when the noise coefficient is set to 0.3. To better explain the performance boost, we further show the variance in preservation rates as a function of different dimensionalities\nusing PCA in Fig. 5. We show that PCA preserves over 90% of the critical information within samples from the MNIST data set when mapping original data to the 100 dimensional space. However, according to Fig. 5, when Ppca = 5%, information preserved by PCA is insufficient for representing the original samples well. Meanwhile, the 95% mapping obtained from DLM is not as effective as PCA mappings for representing the original samples. Therefore, this combination leads to the highest classification error rates for both noise coefficient equal to 0.1 and 0.3. However, as Ppca reaches 25%, enough information is preserved resulting in only a slight decrease in classification error. Meanwhile, when Ppca varies from 25% to 95%, the benefit of preserving any further information diminishes as with only a negligible decrease in the error rate.\nWe next evaluate the classification performance of DLMDNN when confronted with adversarial samples. We list the classification error rates in the column noted as \u2018Adversarial\u2019. According to Table II, the error rates obtained by the DLMDNN are considerably lower than that of a standard DNN, 0.8981. Again, when Ppca is properly set, the DLM-DNN achieves results comparable to adversarial training. The highest error rate is obtained when Ppca = 5%, for either noise coefficient is set to 0.1 or 0.3. This is consistent from our previous analysis for testing with legitimate samples. Interesting enough, as Ppca ranges from 25% to 95%, classification error goes up. This observation might imply that the impact of adversarial samples is mitigated to a larger degree when more random disturbances are added.\n2) DrLIM-DNN Classification Performance: In this experiment we demonstrate the classification performance of DrLIMDNN. The training set used for evaluation includes 5 classes from the MNIST data, and each class contains 2000 samples. For testing, each of the 5 classes contains 1000 testing samples.\nThe adversarial testing samples are generated based on this testing set.\nFor training the DrLIM, we label a pair of image samples as similar when they have the same label. This simplifies the training of DrLIM utilizing strong prior knowledge. We further set the reduced dimension to 30 during the experiments. Classification performance is shown in the bottom part of Table II. According to these results, using DrLIM-DNN results in a slightly higher error rate (0.0384) when testing with legitimate samples, but achieves a significant improvement in performance (0.1380) when testing adversarial samples. Especially in the latter case, DrLIM-DNN shows higher robustness when compared to adversarial training.\nAs previously introduced in Section V, DrLIM is designed with the objective of preserving similarity between a pair of high dimensional samples when mapped to lower dimensional space. In order to better illustrate this property, we first map 200 legitimate examples of MNIST image samples into 2D dimension using DrLIM. According to the visualization of the 2D mapping shown in Fig. 6(a), DrLIM effectively preserves the similarity of legitimate samples in the mapped 2D space. We notice some outliers and hence highlight them and their neighbours by showing their corresponding images. For example, in Fig. 6(a), one of the outliers is a blue dot representing 9 surrounded by two red nots representing 4. Meanwhile the other outlier is a red 4 neighbouring to two blue 9s. As shown in Fig. 6(a), these outliers are hard to recognize, even for human.\nSince the point of DrLIM is to preserve the similarity in a lower dimensional space, we further visualize the 2D mapping of 200 adversarial samples in Fig. 6(b). Note that these adversarial samples are generated from those 200 legitimate samples selected before. The 2D mapping in this case is not as clear as that for legitimate samples, especially since the overlapping is more obvious. However, the similarity between pairs of samples are still reasonably well-preserved. In order to explore more of these outliers, in Table III, we show the probabilities of making wrong classification decisions when\ntesting a normal DNN and a DrLIM-DNN with these outliers. As shown in Table III, these outliers cause a normal DNN to make wrong classification results with over 97% confidence. However, when processed with DrLIM-DNN, although these outliers are not mapped to ideal regions, the probabilities of being wrongly classified is significantly reduced to lower than 66%. This result indicates that a DrLIM-DNN is effective for responding to unfamiliar samples with lower confidence. Therefore, DrLIM-DNN will not suffer as much as a normal DNN would when confronted with highly confusing adversarial samples.\nAs our experimental results show, DrLIM-DNN provides the best performance when tested against adversarial samples. In particular, the DrLIM-DNN achieves a classification error rate 44.93% (smaller than the classification error rate obtained using adversarial training). In addition, DrLIM-DNN also shows its effectiveness for avoiding making strongly confident predictions for adversarial samples. With respect to this property, there has been limited research that shows similar results. However, since the DrLIM requires also building a CNN model, the computational cost is much higher compared to both DLM-DNN and adversarial training. Therefore, to best utilize the advantages of DrLIM-DNN, it might be reasonable to use a smaller data set with a fewer number of classes."}, {"heading": "D. Reconstruction Performance", "text": "As previously introduced in Section V, both DLM-DNN and DrLIM-DNN are non-invertible for different reasons. More importantly, we have proven that recovering the original data from a low dimensional space induced by DrLIM is an NP-hard problem. In this subsection, we mainly focus on inverting the proposed dimensional reduction method DLM by approximating it with a linear transformation matrix. We obtain the linear transformation matrix by solving a linear regression problem. In case the original data is sparse, we further employ a linear regression with L1 regularization. First, we demonstrate that when configuring DLM as pure PCA, the approach is not robust given that it may be effectively inverted and thus allow for reconstruction of adversarial samples. Next, we examine the reconstruction error obtained from inverting DLM, taking a percentage of PCA mappings less than 100%.\nWe evaluate the reconstruction performance when inverting one extreme case of DLM, where DLM uses only PCA mappings. In this way, the original samples are only processed by PCA before being input to a DNN model. We refer to this method as PCA-DNN for comparison. Therefore, an adversary can easily recover the adversarial images given their lower dimensional mappings. To examine this extreme case, we first configure DLM as pure PCA and map legitimate testing samples to a 100-dimensional space. Then we reconstruct these legitimate samples by inverting PCA, as explained in Section V. To demonstrate the effectiveness of inverting PCA, we show reconstructed legitimate samples in Fig. 7(a). In this case, the reconstruction was successful and the samples are haradly different from legitimate ones. Now we assume that an adversary has acquired the lower dimensional mappings generated by PCA. According to Section II, this adversarial can easily generate their corresponding lower dimensional adversarial mappings. Following the inversion procedure of Section V, this adversary can further reconstruct adversarial samples based on these lower dimensional adversarial mappings. We also show the reconstructed adversarial samples in Fig. 7(b). While Fig. 7(a) and Fig. 7(b) are difficult to differentiate, we use the reconstructed adversarial samples to test a normal DNN model and a DLM-DNN under different settings. According to the testing results shown in Table IV, the reconstructed adversarial samples maintain their attack power against a normal DNN model. The classification error rate in this case is larger than 60%. In contrast, these adversarial samples can be effectively defended by a DLM-DNN as shown in Table II.\nWe finally investigate the reconstruction errors when inverting DLM-DNN. Note that since we have shown that using PCA is not secure for defending against reconstructed adversarial samples, the reconstruction errors for this extreme case will not included in following discussion. We present reconstruction errors when varying percentages of PCA mappings used and when varying sub-space dimensionality in Fig 8. Our experiment shows that inverting a DLM-DNN leads to high reconstruction errors, regardless of how many PCA mappings are used what dimensionality is used. Recall the theoretical analysis of DrLIM-DLM in Section V, we demonstrate that our proposed methods effectively build an adversary-resistant DNN."}, {"heading": "VII. RELATED WORK", "text": "In this work, we build adversary-resilient DNN architectures using non-invertible dimension reduction methods. Related research commonly falls into the area of adversarial machine learning or dimensionality reduction methods. In this section we introduce several state-of-the-art adversarial machine learning technologies. These technologies can be categorized as either data augmentation or DNN modelcomplexity enhancement. Finally, we present several prevalent dimensionality reduction methods and study their suitability under our proposed framework."}, {"heading": "A. Adversarial Machine Learning", "text": "Approaches to the robustness issue mainly focus on adversarial training and enhancing DNN complexity.2\nAdversarial training approaches usually augment the training set by integrating adversarial samples [35]. More formally, this data augmentation approach can be theoretically interpreted regularizing the training objective function. This regularization is designed to penalize certain gradients using the derivative of the loss with respect to the input variables. The penalized gradients indicate a direction in the input space that the objective function is most sensitive to. For example, recent work[10] introduced such an objective function, directly combining a regularization term with the original cost function. Instead of training a DNN with a mixture of adversarial and legitimate samples, other approaches [23] use only adversarial samples. Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].\nHowever, since adversarial training still falls into the category of data augmentation, which, as explained earlier in this paper, cannot possibly hope to cover the entire infinite space of possible adversarial samples. In other words, adversarial training is still vulnerable to unforeseen adversarial samples. Therefore, we looked to improving the robustness of DNNs by enhancing the model complexity.\nRecent work [26], denoted as distillation enhances a normal DNN by enabling it to utilize other informative inputs. More specifically, for each input training sample, instead of using its original hard label which only indicates whether it belongs to a particular class, the enhanced DNN utilizes a soft label. This means that probability distribution over all classes is used as a corresponding target vector. In order to use this more informative soft label, two DNNs must be designed\n2For details of robustness issues, please see Section II.\nsuccessively and then stacked together. One DNN is designed for generating more informative input samples while the other DNN is trained using these more informative samples in the usual way. However, as discussed in section III, once the architecture of distillation is disclosed to attackers, it is always possible to find adversarial samples based on this very defense mechanism. If an attacker [3] generates adversarial samples by artificially reducing the magnitude of the input to the final softmax function and modifies the objective function used for generating adversarial samples [25], the defensively distilled networks are open to assault. A denoising autoencoder [11] approach was also proposed to filter out adversarial perturbations. However, it too suffers from the same problem as [26]. Again, once the architecture of the stacked DNN and auto-encoder is disclosed, attackers can still generate adversarial samples by treating this new model as a normal feed-forward DNN only with more hidden layers. Indeed, adversarial samples of this stacked DNN [11] show even smaller distortion compared with adversarial samples generated from the original DNN. Thus, a successful method for increasing model complexity requires the property that even if the model architecture is disclosed, it would not be possible to generate adversarial samples based on this information."}, {"heading": "B. Dimensionality Reduction Methods", "text": "From the previous analysis in Section IV, we see that dimensionality reduction has definite advantages for the design goals of our proposed architecture using non-invertible data transformations. As such, we investigated various dimensional reduction methods including both linear and nonlinear methods.\nRecall from Section V, dimensionality reduction methods that preserve the similarity between high dimensional samples in the mapped lower dimensional space can be helpful in defending against adversarial samples. This is due to the fact that adversarial samples are highly similar to legitimate samples yet can be wrongly classified in a lower dimensional space. One popular linear dimension reduction method with aforementioned characteristic is multi-dimensional scaling (MDS) [17]. MDS is computationally efficient and has been widely adopted in various machine learning tasks. More importantly, MDS generates lower dimensional mappings based on similarity, which is usually represented by Euclidean distance, between high dimensional data points. Given this property, an adversarial sample will not be mapped that far away from its similar, legitimate neighbors. However, MDS have three shortcomings which restrict its usage in our proposed framework. First, since MDS is built only with distance information between samples, critical feature information contained in data samples cannot be preserved. Second, MDS cannot provide a mapping from high dimensional input to a lower dimensional output when being applied to unseen data samples. When stacking DNN on top of MDS, whenever unseen samples appears, the entire combined architecture needs to be retrained, which is computationally undesirable.\nWe also investigated two nonlinear dimensional reduction methods: locally linear embedding (LLE) [30] and t-SNE [19]. Specially, these two methods are similar to MDS in that they are also capable of preserving the similarity between\nhigh dimensional data in a lower dimensional space. LLE obtains low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs from a weight matrix used for reconstructing input data from their neighbors. The primary drawback of LLE is that it only works well when input samples are locally very similar and well registered. In [12], the authors demonstrate that when this is not the case, LLE will be unable to learn a proper global embedding. The authors also show that when testing the LLE with distorted data (e.g, pictures with slightly changed pixels), the performance degenerates. This results makes LLE less suitable for our framework since our aim is to retain as much of the original classificationn performance of the DNN as possible. On the other hand, tSNE minimizes KL-divergence between two probability distributions, one representing the probability of being neighbors in a high dimensional space while the other represents the probability of being neighbors in a lower dimensional space. However, similar to MDS, t-SNE is not suitable for online processing.\nThe dimensionality reduction methods used in our proposed framework take these aforementioned issues into account. In particular, DLM is more computationally efficient in that it can easily preserve feature information of the original data samples. Meanwhile, DrLIM not only preserves similarity and important feature information of original input space simultaneously, but is an incremental algorithm. More importantly, both DLM and DrLIM are non-invertible which we justify in theoretically and empirically in Sections V and VI."}, {"heading": "VIII. CONCLUSION", "text": "We proposed a new framework for constructing deep neural network models that are robust to adversarial samples. Our framework design is based on an analysis of both the \u201cblindspot\u201d of DNNs and the limitations of currently proposed solutions.\nUsing our proposed framework, we developed two adversary-resilient DNN architectures that leverage noninvertible data transformation mechanisms. Using the first proposed approach for processing the data fed into DNN models, we empirically showed that crafting an adversarial sample for this architecture will incur significant distortion and thus lead to easily detectable adversarial samples. In contrast, under the second architecture, we theoretically demonstrated that it is impossible for an adversary to craft an adversarial sample to attack it. This implies that our proposed framework no longer suffers from attacks that rely on generating modelspecific adversarial samples.\nFurthermore, we demonstrated that recently studied adversarial training methods are not sufficient defense mechanisms. Applying our new framework to the MNIST data set, we empirically demonstrate that our new framework significantly reduces the error rates in classifying adversarial samples. Furthermore, our new framework has the same classification performance for legitimate samples with negligible degradation. Future work will entail investigating the performance of our framework in a wider variety of applications."}], "references": [{"title": "Deep learning with non-medical training used for chest pathology identification", "author": ["Y. Bar", "I. Diamant", "L. Wolf", "H. Greenspan"], "venue": "In SPIE Medical Imaging, pages 94140V\u201394140V. International Society for Optics and Photonics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep learning. Book in preparation for", "author": ["I.G.Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1607.04311,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Relaxations and randomized methods for nonconvex qcqps", "author": ["A. d?Aspremont", "S. Boyd"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1936}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "arXiv preprint arXiv:1202.2160,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Compressive sensing. In Handbook of mathematical methods in imaging, pages 187\u2013228", "author": ["M. Fornasier", "H. Rauhut"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "[cs],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Learning long-range vision for autonomous off-road driving", "author": ["R. Hadsell", "P. Sermanet", "J. Ben", "A. Erkan", "M. Scoffier", "K. Kavukcuoglu", "U. Muller", "Y. LeCun"], "venue": "Journal of Field Robotics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in cognitive sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesv\u00e1ri"], "venue": "CoRR, abs/1511.03034,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Nonmetric multidimensional scaling: a numerical method", "author": ["J.B. Kruskal"], "venue": "Psychometrika, 29(2):115\u2013129,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1964}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "S.-i. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "stat, 1050:25,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Improving back-propagation by adding an adversarial gradient", "author": ["A. N\u00f8kland"], "venue": "[cs],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Unifying adversarial training algorithms with flexible deep data gradient regularization", "author": ["A.G. Ororbia II", "C.L. Giles", "D. Kifer"], "venue": "[cs],", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "arXiv preprint arXiv:1511.04508,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Malware classification with recurrent networks", "author": ["R. Pascanu", "J.W. Stokes", "H. Sanossian", "M. Marinescu", "A. Thomas"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Minimax rates of estimation for high-dimensional linear regression over-balls", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1988}, {"title": "Recognizing functions in binaries with neural networks", "author": ["E.C.R. Shin", "D. Song", "R. Moazzezi"], "venue": "In 24th USENIX Security Symposium (USENIX Security", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1958}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Nonlinear Optimization: Complexity Issues", "author": ["S.A. Vavasis"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1991}, {"title": "Deep learning of feature representation with multiple instance learning for medical image analysis", "author": ["Y. Xu", "T. Mo", "Q. Feng", "P. Zhong", "M. Lai", "I. Eric", "C. Chang"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Droid-sec: Deep learning in android malware detection", "author": ["Z. Yuan", "Y. Lu", "Z. Wang", "Y. Xue"], "venue": "In ACM SIGCOMM Computer Communication Review,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}], "referenceMentions": [{"referenceID": 32, "context": "Aside from its highly publicized victories in Go [33], there have been numerous successful applications of deep neural network (DNN) learning in image and speech recognition.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 36, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 26, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 134, "endOffset": 138}, {"referenceID": 3, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 140, "endOffset": 143}, {"referenceID": 37, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 145, "endOffset": 149}, {"referenceID": 31, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 194, "endOffset": 198}, {"referenceID": 34, "context": "However, recent work[35], [22] uncovered a potential flaw in DNN-powered systems that could be readily exploited by an attack.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "However, recent work[35], [22] uncovered a potential flaw in DNN-powered systems that could be readily exploited by an attack.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "To mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "To mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "To mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "A recent unification of previous approaches [24] showed that they were all special cases of a general, regularized objective function,DataGrad.", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "Hence, a newly trained DNN would only be robust with respect to previously observed adversarial samples (or for those relatively near to training samples of the underlying manifold if one uses the general approximation to DataGrad developed in [24]).", "startOffset": 244, "endOffset": 248}, {"referenceID": 9, "context": "Demonstration of an adversarial example generated from a panda picture [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "Technically speaking, our framework is similar to a distillation process, which trains a DNN model using the knowledge transferred from a different model [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 33, "context": "Standard neural network [34] with two hidden layers, where neurons", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "This architecture is often referred to as a feed-forward neural network [2]", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "1There are many types of activations to choose from, including the hyperbolic tangent, the logistic sigmoid, or the linear rectified function, etc [2]", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "This bottomup propagation of information is also referred to as feedforward inference [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "Parameter gradients are calculated using back-propagation of errors [31].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "Since the gradients of the weights represent their influence on the final cost, there have been multiple algorithms developed for finding optimal weights more accurately and efficiently [21].", "startOffset": 186, "endOffset": 190}, {"referenceID": 34, "context": "Even though a well trained model is capable of recognizing out-of-sample patterns, a deep neural architecture can be easily fooled by introducing perturbations to the input variables that are often indistinguishable to the human eye [35].", "startOffset": 233, "endOffset": 237}, {"referenceID": 9, "context": "These socalled \u201cblind spots\u201d, or adversarial samples, exist because the input space of DNN is unbounded [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 9, "context": "More specifically, it was studied in [10] that attackers can find the most powerful blind spots via effective optimization procedures.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "Furthermore, according to [35], DNN models that share the same design goal, for example recognizing the same image set, all approximate a common highly complex, nonlinear function.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "In [10], the efficient and straightforward fast gradient sign method was proposed for calculating adversarial perturbation as shown in in (1):", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "To resolve the blind spot issue, there have been many data augmentation methods proposed for deep learning tasks [10], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "To resolve the blind spot issue, there have been many data augmentation methods proposed for deep learning tasks [10], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "The same mechanism has also been employed for defending against adversarial samples, also known as adversarial training [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Given the high dimensionality of data distributions that DNNs typically learn from, the input space is generally considered infinite [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "Adversarial training can be formally described as adding a regularization term known as DataGrad to a DNN\u2019s training cost function [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "This is due to the fact that, in most adversarial training approaches [24], [10], adversarial samples can be generated efficiently for a particular type of DNN.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "This is due to the fact that, in most adversarial training approaches [24], [10], adversarial samples can be generated efficiently for a particular type of DNN.", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "The fast gradient sign method [10] can generate a large pool of adversarial samples quickly while DataGrad [24] focuses on dynamically generating them per every parameter update.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "The fast gradient sign method [10] can generate a large pool of adversarial samples quickly while DataGrad [24] focuses on dynamically generating them per every parameter update.", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "DataGrad, as mentioned [24], could be viewed as taking advantage of adversarial perturbations to better explore the underlying data manifold\u2013however, while this leads to improved generalization, it does not offer any guarantees in covering all possible blind-spots.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Therefore, several ideas [26], [11] have been proposed to enhance the complexity of DNN architecture, aiming to improve the tolerance of complex DNN models with respect to adversarial samples generated from simple DNN models.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Therefore, several ideas [26], [11] have been proposed to enhance the complexity of DNN architecture, aiming to improve the tolerance of complex DNN models with respect to adversarial samples generated from simple DNN models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "[26] develops a defensive distillation mechanism, which trains a DNN from data samples that are distilled by another DNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed stacking an auto-encoder together with a normal DNN, similar to [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[11] proposed stacking an auto-encoder together with a normal DNN, similar to [26].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "Recall that many previously proposed solutions, especially adversarial training methods [10], [24], can be classified as forms of data augmentation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "Recall that many previously proposed solutions, especially adversarial training methods [10], [24], can be classified as forms of data augmentation.", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "More importantly, developing adversarial training methods has required the invention of efficient methods for generating adversarial samples [10], [35], consequently providing more useful tools for attackers.", "startOffset": 141, "endOffset": 145}, {"referenceID": 34, "context": "More importantly, developing adversarial training methods has required the invention of efficient methods for generating adversarial samples [10], [35], consequently providing more useful tools for attackers.", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "This belongs to a class of NP-hard problems, according to [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 15, "context": "PCA [16] is one of the most widely adopted dimensional reduction methods, especially since it is computationally efficient and easy to implement.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "According to the Eckart-Young Theorem [7], the optimal solution is obtained when W consists of the q largest eigenvalues of XX .", "startOffset": 38, "endOffset": 41}, {"referenceID": 27, "context": "we estimate reconstruction X\u0302 for X using high-dimensional linear regression [28] (we omit calculation details due to space constraints).", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "According to Theorem 1 in [28], we can obtain a lower bound of the reconstruction error, which is the L2 norm of the difference between X and X\u0302 as shown in (6): (", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "In addition, according to Theorem 2 in [28], there also exists an upper bound of the reconstruction error as follows: (", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "According to [28], the upper bound of the reconstruction error depends on both the data transformation matrix A and noise \u03c9.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "However, since we specifically design A to be highly correlated, the upper bound will be significantly larger than the lower bound [9], [6], and thus result in a larger range for the reconstruction error.", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "However, since we specifically design A to be highly correlated, the upper bound will be significantly larger than the lower bound [9], [6], and thus result in a larger range for the reconstruction error.", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "Because of these considerations, we employ the dimensionality reduction method DrLIM proposed in [12].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "But first, we briefly review DrLIM (for a detailed explication please see [12]).", "startOffset": 74, "endOffset": 78}, {"referenceID": 38, "context": "For the forward pass of a conventional neural network, it is not guaranteed that the weight matrices are invertible [39], implying that information lost during pooling cannot be recovered.", "startOffset": 116, "endOffset": 120}, {"referenceID": 35, "context": "From earlier work [36], the formulation (12) implies a quadratic problem with a non-positive semi-definite constraint, which is an NP-hard problem.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "We evaluate our framework by performing experiments on the the widely-used MNIST data set [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Recently, adversarial training has been shown to be effective in decreasing the classification error rates at test time [10], [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Recently, adversarial training has been shown to be effective in decreasing the classification error rates at test time [10], [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 34, "context": "Adversarial training approaches usually augment the training set by integrating adversarial samples [35].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "For example, recent work[10] introduced such an objective function, directly combining a regularization term with the original cost function.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "Instead of training a DNN with a mixture of adversarial and legitimate samples, other approaches [23] use only adversarial samples.", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].", "startOffset": 189, "endOffset": 193}, {"referenceID": 28, "context": "Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].", "startOffset": 195, "endOffset": 199}, {"referenceID": 25, "context": "Recent work [26], denoted as distillation enhances a normal DNN by enabling it to utilize other informative inputs.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "If an attacker [3] generates adversarial samples by artificially reducing the magnitude of the input to the final softmax function and modifies the objective function used for generating adversarial samples [25], the defensively distilled networks are open to assault.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "If an attacker [3] generates adversarial samples by artificially reducing the magnitude of the input to the final softmax function and modifies the objective function used for generating adversarial samples [25], the defensively distilled networks are open to assault.", "startOffset": 207, "endOffset": 211}, {"referenceID": 10, "context": "A denoising autoencoder [11] approach was also proposed to filter out adversarial perturbations.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "However, it too suffers from the same problem as [26].", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "Indeed, adversarial samples of this stacked DNN [11] show even smaller distortion compared with adversarial samples generated from the original DNN.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "One popular linear dimension reduction method with aforementioned characteristic is multi-dimensional scaling (MDS) [17].", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "We also investigated two nonlinear dimensional reduction methods: locally linear embedding (LLE) [30] and t-SNE [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "We also investigated two nonlinear dimensional reduction methods: locally linear embedding (LLE) [30] and t-SNE [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "In [12], the authors demonstrate that when this is not the case, LLE will be unable to learn a proper global embedding.", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Deep neural networks have proven to be quite effective in a wide variety of machine learning tasks, ranging from improved speech recognition systems to advancing the development of autonomous vehicles. However, despite their superior performance in many applications, these models have been recently shown to be susceptible to a particular type of attack possible through the generation of particular synthetic examples referred to as adversarial samples. These samples are constructed by manipulating real examples from the training data distribution in order to \u201cfool\u201d the original neural model, resulting in misclassification (with high confidence) of previously correctly classified samples. Addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. In this paper, we present an analysis of this fundamental flaw lurking in all neural architectures to uncover limitations of previously proposed defense mechanisms. More importantly, we present a unifying framework for protecting deep neural models using a non-invertible data transformation\u2013developing two adversary-resilient architectures utilizing both linear and nonlinear dimensionality reduction. Empirical results indicate that our framework provides better robustness compared to stateof-art solutions while having negligible degradation in accuracy.", "creator": "LaTeX with hyperref package"}}}