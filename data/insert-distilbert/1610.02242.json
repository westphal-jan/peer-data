{"id": "1610.02242", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Temporal Ensembling for Semi-Supervised Learning", "abstract": "in this paper, we present perfectly a simple and efficient method for training deep neural networks both in a semi - supervised setting where only a small given portion of training data is labeled. sometimes we introduce temporal hypothesis ensembling, where accordingly we form a consensus averaging prediction of the unknown labels under multiple instances of the network - in - training database on different assigned epochs, and most importantly, under different regularization and input augmentation conditions. this ensemble prediction can be expected to be a really better predictor for the unknown labels than the output of the network chosen at the most recent training epoch, and can thus be used as a target for training. using our method, we set new records for two standard semi - supervised experimental learning benchmarks, reducing the classification error rate from 18. 55 63 % to 12. 89 % in cifar - 10 with db 4000 labels and from 18. 44 % to 6. 83 % unchanged in cb svhn 2 with 500 labels.", "histories": [["v1", "Fri, 7 Oct 2016 12:15:42 GMT  (21kb,D)", "http://arxiv.org/abs/1610.02242v1", null], ["v2", "Mon, 7 Nov 2016 13:27:40 GMT  (99kb,D)", "http://arxiv.org/abs/1610.02242v2", "This version was submitted to ICLR 2017. The two methods are now closer to each other and use similar parameters. Also added more experimental results and a test with corrupted labels. Code released"], ["v3", "Wed, 15 Mar 2017 14:22:41 GMT  (101kb,D)", "http://arxiv.org/abs/1610.02242v3", "Final ICLR 2017 version. Includes new results for CIFAR-100 with additional unlabeled data from Tiny Images dataset"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["samuli laine", "timo aila"], "accepted": true, "id": "1610.02242"}, "pdf": {"name": "1610.02242.pdf", "metadata": {"source": "CRF", "title": "Temporal Ensembling for Semi-Supervised Learning", "authors": ["Samuli Laine", "Timo Aila"], "emails": ["slaine@nvidia.com", "taila@nvidia.com"], "sections": [{"heading": "1 Introduction", "text": "It has long been known that an ensemble of multiple neural networks generally yields better predictions than a single network in the ensemble. This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks. We extend this idea by forming ensemble predictions during training, using the outputs of a single network on different training epochs and under different regularization and input augmentation conditions. Our training still operates on a single network, but the predictions made on different epochs correspond to an ensemble prediction of a large number of individual sub-networks because of dropout regularization.\nThis ensemble prediction can be exploited for semi-supervised learning, where only a small portion of training data is labeled. If we compare the ensemble prediction to the current output of the network being trained, the ensemble prediction is likely to be closer to the correct, unknown labels of the unlabeled inputs. Therefore the labels inferred this way can be used as training targets for the unlabeled inputs. Our method relies heavily on dropout regularization and versatile input augmentation. Indeed, without neither, there would be much less reason to place confidence in whatever labels are inferred for the unlabeled training data.\nAs a stepping stone towards temporal ensembling we introduce \u03a0-model, a simple two-prong consistency-enforcing network superstructure that already surpasses prior state-of-the-art results by a considerable margin. We then simplify this back to a standard single-branch network and move the consistency enforcement logic to the training loop, which enables temporal ensembling over multiple training epochs. This simplifies the network, speeds up training, and improves the benchmark results even further.\nOur \u03a0-model can be seen as a simplification of the \u0393-model of the ladder network [13], a previously presented network architecture for semi-supervised learning. Our full temporal ensembling model also has connections to the bootstrapping method of Reed et al. [14] targeted for training with noisy labels.\nar X\niv :1\n61 0.\n02 24\n2v 1\n[ cs\n.N E\n] 7\nO ct\n2 01\n6"}, {"heading": "2 \u03a0-Model for Enforcing Consistency", "text": "Our first step towards temporal ensembling is the realization that even for unlabeled training data, we can train a network to be consistent over multiple augmentations of the same input. Our \u03a0-model achieves this by duplicating the network while sharing all the trainable parameters \u03b8 between the two branches. We feed the training input under two different augmentations and dropout conditions to these two branches. We then define a loss function that penalizes differences in the output of the two branches.\nThis approach is similar to the \u0393-model of the ladder network [13], but conceptually simpler. In the \u03a0-model, the comparison is done directly on network outputs, i.e., after softmax activation, and there is no auxiliary mapping between the two branches unlike the learned denoising functions in the ladder network architecture. Further contrasting to \u0393-model, we employ richer augmentation to both branches of the network. Instead of having one \u201cclean\u201d and one \u201ccorrupted\u201d branch as in \u0393-model, we apply augmentation and noise to the inputs for both branches.\nWe calculate the unsupervised loss term by taking the mean square difference between the outputs of the two branches. This is weighted by a constant factor and added to the standard supervised crossentropy loss computed for the inputs where labels are known, similar to \u0393-model. As shown in Section 4, the \u03a0-model combined with a good convolutional network architecture already provides a significant improvement over prior art in classification accuracy."}, {"heading": "3 Temporal Ensembling", "text": "Analyzing how the \u03a0-model works, we could equally well split the evaluation of the two branches in two separate phases, first classifying the training set under one set of augmentations and recording these predictions, and then training the network on the same inputs under different augmentations using the just obtained predictions as training targets. As the training targets obtained this way are based on a single evaluation of augmented inputs, they can be expected to be noisy. Temporal ensembling alleviates this by aggregating the predictions of multiple network invocations into an ensemble prediction. Furthermore, it unifies the labeled and unlabeled inputs to use the same asymmetrical loss function, albeit still with a weighting parameter to balance between known and inferred labels. Finally, temporal ensembling lets us dispose of the notion of having multiple copies of the same network as in our \u03a0-model, or in the \u0393-model [13].\nWe shall describe our method (Algorithm 1) in the context of traditional image classification networks. Let the training data consist of total of N inputs, out of which M are labeled. The input stimuli, available for all training data, are denoted xi, where i \u2208 {1 . . . N}. Let set L contain the indices of the labeled inputs, |L| = M . For every i \u2208 L, we have a known correct label yi \u2208 {1 . . . C}, where C is the number of different classes. The corresponding one-hot vector of length C is denoted y\u0302i.\nBefore every training epoch, we generate a target vector y\u0303i and a training weightwi for every training input, including the unlabeled inputs. For the labeled inputs we always set y\u0303i\u2208L = y\u0302i and wi\u2208L = 1. For the unlabeled inputs we initially set y\u0303i/\u2208L = (0, \u00b7 \u00b7 \u00b7 , 0) and wi/\u2208L = 0. During training, we augment each training input via a stochastic augmentation function g, and feeding the result into our network f yields output zi = f(g(xi)), where zi is the softmax output vector, i.e., |zi| = C and\u2211 j zi,j = 1. As the loss function associated with input i we use the cross entropy between network\noutput zi and target y\u0303i weighted with training weight wi, i.e., lossi = \u2212wi \u2211 j y\u0303i,j log(zi,j).\nThe evaluation of network f is stochastic due to dropout regularization, and it naturally also varies as the training proceeds. Note that for labeled inputs, our loss corresponds exactly to traditional supervised training with cross-entropy loss. For unlabeled inputs, initially having wi/\u2208L = 0 also yields lossi/\u2208L = 0.\nLet us now turn into how y\u0303i andwi are determined for unlabeled inputs as the training progresses. At every training epoch, we compute network output zi for each training input once as part of the usual training process. We accumulate these into ensemble outputs Zi by updating Zi \u2190 \u03b1Zi+ (1\u2212\u03b1)zi after each minibatch, \u03b1 being a momentum term that controls how far the ensemble reaches into training history. Z is initialized to all zeros, and even though this causes a startup bias, we need to explicitly correct for it only when calculating the training weights, as explained below.\nAlgorithm 1: Our proposed temporal ensembling method for generating training targets y\u0303 and loss function weights w in a classification network. Note that the updates of Z, y\u0303i/\u2208L, and wi/\u2208L could equally well be done inside the minibatch loop\u2014in this pseudocode they occur between epochs for clarity. N is the number of training inputs, including both labeled and unlabeled inputs, M is the number of labeled training inputs, and C is the number of classes. Require: xi = training stimuli Require: L = set of training input indices with known labels Require: y\u0302i = one-hot target vectors for labeled inputs i \u2208 L Require: \u03b1 = ensembling momentum, 0 \u2264 \u03b1 < 1 Require: f\u03b8(x) = neural network with trainable parameters \u03b8 Require: g(x, t) = input augmentation function (potentially time-dependent) Require: W(t) = total ensemble weight ramp-up function Z \u2190 0[N\u00d7C] . initialize ensemble predictions y\u0303 \u2190 0[N\u00d7C] . initialize target vectors w\u2190 0[N ] . initialize training weights t \u2190 0 . initialize epoch counter y\u0303i\u2208L \u2190 y\u0302i . set target vectors for labeled inputs wi\u2208L \u2190 1 . set training weights for labeled inputs while training not finished do t\u2190 t+ 1 . increase epoch counter for each minibatch B do zi\u2208B \u2190 f\u03b8(g(xi\u2208B , t)) . evaluate network outputs for augmented inputs loss \u2190 \u2212 1|B| \u2211 i\u2208B wi \u2211 j y\u0303i,j log(zi,j) . calculate mean weighted loss\nupdate \u03b8 using, e.g., ADAM . update network parameters end for Z \u2190 \u03b1Z + (1\u2212 \u03b1)z . accumulate ensemble predictions y\u0303i/\u2208L \u2190 Z2i /( \u2211 j Z 2 i,j) . construct target vectors for unlabeled inputs\nwi/\u2208L \u2190 maxj Zi,j/(1\u2212 \u03b1t) \u00b7W(t) \u00b7 MN\u2212M . construct training weights for unlabeled inputs end while return \u03b8\nBecause of dropout regularization and stochastic augmentation, Z thus contains a weighted average of the outputs of an ensemble of networks f from previous training epochs, with recent epochs having larger weight than distant epochs. For generating the training targets y\u0303i for the unlabeled inputs, we have found that it is beneficial to sharpen each Zi vector by raising each component to a power and renormalizing, i.e., setting y\u0303i/\u2208L = Zki / \u2211 j Z k i,j . Based on our experiments, k = 2 yields the best results. Note that setting k = 1 would correspond to using bias-corrected Zi as-is, and as k approaches infinity, y\u0303i approaches a one-hot vector with only the largest component surviving. Also note that the normalization removes the need to explicitly correct for startup bias in the accumulation of Z.\nFinally, we need to determine per-input training weights wi/\u2208L that govern the individual unlabeled inputs\u2019 contribution to the total loss, and hence also their influence to the parameter gradients. We experimented with various functions for this, but ultimately the best results were obtained by picking the largest component of the ensemble output Zi and correcting for startup bias. The intuition behind this is that we place higher weights on samples that the network is confident about. In addition, we want to gradually ramp up the total weight of unlabeled inputs, and in order to make it easier to experiment with different numbers of labeled inputs, make the total weight of unlabeled inputs vs. labeled inputs independent of label count. Thus we set wi/\u2208L = maxj Zi,j/(1\u2212 \u03b1t) \u00b7W(t) \u00b7 MN\u2212M , where t is the current epoch index, 1/(1\u2212 \u03b1t) corrects for the startup bias in Z, W(t) is a timedependent ramp-up function for unlabeled data weight, and factor MN\u2212M normalizes the total weight of unlabeled inputs to the same scale as that of labeled inputs regardless of the number of labeled inputs."}, {"heading": "4 Results", "text": "Our network structure is given in Table 3, and the test setup and all training parameters are detailed in Appendix A. We test the \u03a0-model and temporal ensembling in CIFAR-10 and SVHN image classification tasks, and report the mean and standard deviation of 10 runs using different random seeds."}, {"heading": "4.1 CIFAR-10", "text": "CIFAR-10 is a dataset consisting of 32\u00d7 32 pixel RGB images from ten classes. Table 1 demonstrates a 4.4 and 5.7 percentage point reduction in classification error rate with 4000 labels (400 per class) compared to earlier methods for the \u03a0-model and temporal ensembling, respectively. Let us now look into the causes behind this improvement.\nIt is unfortunately difficult to make a true apples-to-apples comparison between semi-supervised methods because it is often not specified whether dataset augmentation was enabled or how many training examples were used (73257 or 604388 in SVHN). Of our comparison methods the \u0393-model [13] is the only one that explicitly says that augmentation was not used for CIFAR-10. Meanwhile, in purely supervised training the de facto standard way of augmenting the CIFAR-10 dataset includes horizontal flips and random translations. We choose to follow this approach primarily because our temporal ensembling requires augmentation, and also because otherwise it would not be possible to compare the best semi-supervised and fully supervised results. After all, the latter should indicate the upper bound of obtainable accuracy. This decision to use augmentation distorts our comparison at least with the original \u0393-model, which we estimate would have benefited approximately 2.0\u20132.5 percentage points from augmentation, based on measurements with the \u03a0-network.\nWhen all labels are used for training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation [7, 10] at 6.04% and without augmentation [15] at 7.29%. This baseline is 2 percentage points better than the Conv-Large network used in the original \u0393-model, and we have observed that this improvement translates to the semi-supervised setting. Together with augmentation, this update, i.e., replacing Conv-Large with the network in Table 3, might have reduced the \u0393-model\u2019s error rate to \u223c16%, compared to the 14.22% measured for our \u03a0-model.\nTemporal ensembling helps 1.33 percentage points on top of the \u03a0-model, suggesting that maintaining a longer history is beneficial. Additionally, the training was approximately 25% faster."}, {"heading": "4.2 SVHN", "text": "The street view house numbers (SVHN) dataset consists of 32\u00d7 32 pixel RGB images of real-world house numbers, and the task is to classify the centermost digit. In SVHN we chose to use only the official 73257 training examples because otherwise the dataset would have been too easy. Even with this choice the error rate with all labels is only 2.88% (vs. 1.69% with 604388 examples).\nTable 2 compares our method to the previous state-of-the-art. With the most commonly used 1000 labels we observe an improvement of 2.6 percentage points, from 8.11% to 5.49%. Temporal ensembling did not provide a benefit over \u03a0-model in this case, and we suspect the reason is that the error rate is starting to approach that of the fully labeled case. We therefore also investigated the behavior with 500 labels, where we obtained an error rate less than half of Salimans et al. [16] with the \u03a0-model, and a further improvement of 1.31 percentage points with temporal ensembling."}, {"heading": "5 Related Work", "text": "There is a vast body of previous work on semi-supervised learning [26]. In here we will concentrate on the ones that are most directly connected to our work.\n\u0393-model is a subset of a ladder network [13] that introduces lateral connections into an encoderdecoder type network architecture, targeted at semi-supervised learning. In \u0393-model, all but the highest lateral connections in the ladder network are removed, and after pruning the unnecessary stages, the remaining network consists of two parallel, identical branches. One of the branches takes the original training inputs, whereas the other branch is given the same input corrupted with noise. The unsupervised loss term is computed as the squared difference between the (pre-activation) output of the clean branch and a denoised (pre-activation) output of the corrupted branch. The denoised estimate is computed from the output of the corrupted branch using a parametric nonlinearity that has 10 auxiliary trainable parameters per unit. Our \u03a0-model differs from the \u0393-model in removing the parametric nonlinearity and denoising, having two corrupted paths, and comparing the outputs of the network instead of pre-activation data of the final layer.\nIn bootstrap aggregating, or bagging, multiple networks are trained independently based on subsets of training data [1]. This results in an ensemble that is more stable and accurate than the individual networks. Our approach can be seen as pulling the predictions from an implicit ensemble trained with slightly different variations of the input, even though we only operate on a single network and the variability is a result of evaluating it at different points during training instead of training on different subsets of data.\nThe general technique of inferring new labels from partially labeled data is often referred to as bootstrapping or self-training, and it was first proposed by Yarowsky [25] in the context of linguistic analysis. Whitney and Sarkar [24] analyze Yarowsky\u2019s algorithm and propose a novel graph-based label propagation approach. Similarly, label propagation methods [27] infer labels for unlabeled training data by comparing the associated inputs to labeled training inputs using a suitable distance metric. Our approach differs from this in two important ways. Firstly, we never compare training inputs against each other, but instead rely on differently augmented variants of the same input to have the same label, and secondly, we let the network produce the likely labels for the unlabeled inputs instead of providing them through an outside process.\nIn addition to partially labeled data, considerable amount of effort has been put into dealing with densely but inaccurately labeled data. This can be seen as a semi-supervised learning task where part of the training process is to identify the labels that are not to be trusted. For recent work in this area, see, e.g., Sukhbaatar et al. [21] and Patrini et al. [12]. In this context of noisy labels, Reed et al. [14] presented a simple bootstrapping method that trains a classifier with the target composed of a convex combination of the previous epoch output and the known but potentially noisy labels. Our temporal ensembling differs from this by taking into account the evaluations over multiple previous epochs.\nGenerative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16]. It could be an interesting avenue for future work to incorporate a generative component to our solution. We also envision that our methods could be applied to regressiontype learning tasks as well as classification tasks with noisy labels."}, {"heading": "A Network architecture, test setup, and training parameters", "text": "Table 3 details the network architecture used in all of our tests. It is heavily inspired by ConvPoolCNN-C [19] and the improvements made by Salimans and Kingma [15]. All data layers were initialized following He et al. [3], and we applied weight normalization and mean-only batch normalization [15] to all of them. We used leaky ReLU [9] with \u03b1 = 0.1 as the non-linearity, and chose max pooling because it gave consistently better results than strided convolutions in our experiments.\nAll networks were trained using Adam [5] with a maximum learning rate of 0.003, except for temporal ensembling in the SVHN case where a maximum learning rate of 0.001 worked better. Adam momentum parameters were set to \u03b21 = 0.9 and \u03b22 = 0.999 as suggested in the paper. In \u03a0networks, the learning rate was ramped up during the first 40 epochs, and with temporal ensembling the ramp-up length was 40 epochs for CIFAR-10 and 80 epochs for SVHN tests. The learning rate was annealed to zero and \u03b21 to 0.5 during the last 50 epochs, but otherwise we did not decay them during training. The \u03a0-network was trained for 300 epochs and temporal ensembling for 500 epochs, both with minibatch size of 100. We searched the best parameters separately for each test.\nIn \u03a0-model the cross-entropy loss had a weight of 1.0 and the unsupervised mean square loss was scaled with \u03bb\u00b7M/N , where \u03bb is a free parameter and was set to 80, and the second part is the relative size of the labeled and the entire dataset. For temporal ensembling, the weighting of inferred labels versus known labels is controlled by weighting function W(t). In CIFAR-10 we had W(t) ramp up to 1.0 between training epochs 10 and 30, and in SVHN to 0.2 between epochs 10 and 70. The Z accumulation decay parameter was set to \u03b1 = 0.9.\nCIFAR-10 Following previous work in fully supervised learning, we pre-processed the images using ZCA and augmented the dataset using horizontal flips and random translations. In our \u03a0network a random translation drawn from [\u22122, 2] pixels was independently applied to both branches, and additive Gaussian noise with \u03c3 = 0.1 was added to one branch\u2014note that this is on top of the Gaussian noise that gets added inside the network. With temporal ensembling, we did not add noise outside the network, but stochastically varied the amount of noise between \u03c3 = 0.1 and \u03c3 = 0.2. Also, even larger translations of [\u22126, 6] were found to be slightly beneficial. Augmentation strength was ramped up during the first 25 epochs for temporal ensembling.\nSVHN We pre-processed the input images by normalizing the mean and variance of the entire training set to 0 and 1, respectively. In all semi-supervised tests we used only the 73257 items in the official training set, i.e., did not use the provided 531131 extra items. For our \u03a0-network the setup was otherwise identical to CIFAR-10 except that horizontal flips were not used. With temporal ensembling the random translation was drawn from [\u22122, 2] similar to the \u03a0-network. Augmentation ramp-up period was 40 epochs for temporal ensembling.\nCode Our implementation is written in Python using Theano [22] and Lasagne [2]. We expect to release the source code along with the next revision of this paper, where we also aim to harmonize the training parameters."}], "references": [{"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, 24(2)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "et al", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S.K. S\u00f8nderby"], "venue": "Lasagne: First release.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1502.01852", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "CoRR, abs/1603.09382", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D. Jimenez Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["C.-Y. Lee", "P.W. Gallagher", "Z. Tu"], "venue": "gated, and tree. CoRR, abs/1509.08985", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "CoRR, abs/1602.05473", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A. Ng"], "venue": "Proc. International Conference on Machine Learning (ICML), volume 30", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "All you need is a good init", "author": ["D. Mishkin", "J. Matas"], "venue": "Proc. International Conference on Learning Representations (ICLR)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "Proc. International Conference on Learning Representations (ICLR)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Making neural networks robust to label noise: a loss correction approach", "author": ["G. Patrini", "A. Rozza", "A. Menon", "R. Nock", "L. Qu"], "venue": "CoRR, abs/1609.03683", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Training deep neural networks on noisy labels with bootstrapping", "author": ["S.E. Reed", "H. Lee", "D. Anguelov", "C. Szegedy", "D. Erhan", "A. Rabinovich"], "venue": "CoRR, abs/1412.6596", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "CoRR, abs/1602.07868", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved techniques for training GANs", "author": ["T. Salimans", "I.J. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "CoRR, abs/1606.03498", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["S. Singh", "D. Hoiem", "D.A. Forsyth"], "venue": "CoRR, abs/1605.06465", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["J.T. Springenberg"], "venue": "Proc. International Conference on Learning Representations (ICLR)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"], "venue": "CoRR, abs/1412.6806", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15:1929\u20131958", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Training convolutional networks with noisy labels", "author": ["S. Sukhbaatar", "J. Bruna", "M. Paluri", "L. Bourdev", "R. Fergus"], "venue": "CoRR, abs/1406.2080", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proc. International Conference on Machine Learning (ICML), 28(3):1058\u20131066", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Bootstrapping via graph propagation", "author": ["M. Whitney", "A. Sarkar"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics, ACL \u201995", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 138, "endOffset": 141}, {"referenceID": 16, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 190, "endOffset": 194}, {"referenceID": 12, "context": "Our \u03a0-model can be seen as a simplification of the \u0393-model of the ladder network [13], a previously presented network architecture for semi-supervised learning.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "[14] targeted for training with noisy labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "This approach is similar to the \u0393-model of the ladder network [13], but conceptually simpler.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "Finally, temporal ensembling lets us dispose of the notion of having multiple copies of the same network as in our \u03a0-model, or in the \u0393-model [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "Conv-Large, \u0393-model [13] 20.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "47 CatGAN [18] 19.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "[16] 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Error rate (%) with # labels Model 500 1000 All (73257) DGN [6] 36.", "startOffset": 60, "endOffset": 63}, {"referenceID": 10, "context": "10 Virtual Adversarial [11] 24.", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "63 Auxiliary Deep Generative Model [8] 22.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "86 Skip Deep Generative Model [8] 16.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "[16] 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Of our comparison methods the \u0393-model [13] is the only one that explicitly says that augmentation was not used for CIFAR-10.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "When all labels are used for training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation [7, 10] at 6.", "startOffset": 154, "endOffset": 161}, {"referenceID": 9, "context": "When all labels are used for training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation [7, 10] at 6.", "startOffset": 154, "endOffset": 161}, {"referenceID": 14, "context": "04% and without augmentation [15] at 7.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "[16] with the \u03a0-model, and a further improvement of 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "There is a vast body of previous work on semi-supervised learning [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "\u0393-model is a subset of a ladder network [13] that introduces lateral connections into an encoderdecoder type network architecture, targeted at semi-supervised learning.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "In bootstrap aggregating, or bagging, multiple networks are trained independently based on subsets of training data [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 23, "context": "The general technique of inferring new labels from partially labeled data is often referred to as bootstrapping or self-training, and it was first proposed by Yarowsky [25] in the context of linguistic analysis.", "startOffset": 168, "endOffset": 172}, {"referenceID": 22, "context": "Whitney and Sarkar [24] analyze Yarowsky\u2019s algorithm and propose a novel graph-based label propagation approach.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "[21] and Patrini et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] presented a simple bootstrapping method that trains a classifier with the target composed of a convex combination of the previous epoch output and the known but potentially noisy labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16].", "startOffset": 114, "endOffset": 125}, {"referenceID": 17, "context": "Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16].", "startOffset": 114, "endOffset": 125}, {"referenceID": 15, "context": "Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16].", "startOffset": 114, "endOffset": 125}], "year": 2017, "abstractText": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63% to 12.89% in CIFAR-10 with 4000 labels and from 18.44% to 6.83% in SVHN with 500 labels.", "creator": "LaTeX with hyperref package"}}}