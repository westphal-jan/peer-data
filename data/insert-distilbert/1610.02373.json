{"id": "1610.02373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Distributed Averaging CNN-ELM for Big Data", "abstract": "increasing the scalability of machine learning to handle big or volume of data is a challenging task. the scale up approach has some limitations. in this paper, we proposed a scale out approach created for cnn - icc elm based on mapreduce on classifier level. map scaling process is the cnn - elm training for altering certain partition of data. it involves many cnn - elm based models captured that can be trained asynchronously. reduce transformation process is : the internal averaging of all cnn - elm weights selected as final simulation training yields result. this approach can save a lot of training free time than single cnn - elm models trained alone. this approach also increased the inherent scalability of machine error learning by thereby combining scale out and scale up approaches. we verified our method in extended mnist data set and not - mnist data set experiment. however, it has contained some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. further researches to use more fundamentally complex image data set are required.", "histories": [["v1", "Fri, 7 Oct 2016 18:59:23 GMT  (308kb,D)", "http://arxiv.org/abs/1610.02373v1", "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems"]], "COMMENTS": "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DC", "authors": ["arif budiman", "mohamad ivan fanany", "chan basaruddin"], "accepted": false, "id": "1610.02373"}, "pdf": {"name": "1610.02373.pdf", "metadata": {"source": "CRF", "title": "Distributed Averaging CNN-ELM for Big Data", "authors": ["Arif Budiman", "Mohamad Ivan Fanany", "Chan Basaruddin"], "emails": ["intanurma@gmail.com"], "sections": [{"heading": null, "text": "Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required.\nKeywords\u2014 deep learning, extreme learning machine, convolutional, neural network, big data, map reduce"}, {"heading": "1 Introduction", "text": "Nowadays, We are seeing a massive growth of data at a faster rate than ever before. However, the benefits of big data become meaningless if none of the processing machine can cultivate and adapt to the data quickly enough. Big data mining needs special machine learning approaches to learn huge volumes of data in an acceptable time. Volume and velocity issues are critical in overcoming big data challenges [13]. It means the data are so massive hence very difficult to be handled by a single computation task in a timely fashion. As with many new hardware and software technologies, we require a special approach to make the most of hardware and software work effectively on speed, scalability, and simplicity presents in real big data knowledge mining.\nScalability is the ability of data processing system to adapt against increased demands. It can be categorized into the following two types of scalability [23]:\n1. Vertical Scaling: Known as scale up. It involves powering more and larger computation components within a single system. It is also known as \u201dscale up\u201d and it usually involves a single instance of an operating system. I.e. Adding more power and capacity (CPU, GPU, RAM, Storage) to an existing machine. However, Scale up is limited to the maximum hardware specification of a single machine.\n1/14\nar X\niv :1\n61 0.\n02 37\n3v 1\n[ cs\n.L G\n] 7\nO ct\n2 01\n2. Horizontal Scaling: Known as scale out. The system distributes the workload across many independent computation resources which may be low end commodity machines or high end machines. All resources added together can speed up the processing capability. Thus we add more machines into one pool of resources. Scale out offers easier and dynamic scalability by adding various size of machines into the existing pool.\nTo increase the scalability of big data processing, the common approach is to distribute big data and running the process in parallel. Parallel computing is a simultaneous use of multiple computing resources to solve complex computational problems by breaking down the process into simpler series of instructions that can be executed simultaneously on different processing units and then employ an overall control management [1].\nTo overcome overhead complexities in parallel programming, Google introduced a programming model named MapReduce [3]. MapReduce is a framework for processing large data within a parallel and distributed way on many computers including low end computers in a cluster. MapReduce provides two essential functions: 1) Map function, it processes each sub problems to another nodes within the cluster; 2) Reduce function, it organizes the results from each node to be a cohesive solution [25].\nDeveloping MapReduce is simple by firstly exposing structure and process similarity and then aggregation process [25]. All the similar tasks are easily parallelized, distributed to the processors and load balanced between them. MapReduce framework does not related to specific hardware technologies. It can be employed to multiple and heterogeneous machine independent.\nFurther researches introduced MapReduce paradigm to speed up various machine learning algorithms, i.e., locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].\nCNN [14] is a popular machine learning that getting benefits from parallel computation. CNN uses a lot of convolution operations that needs many processing cores to speed up the computation time using graphics processing units (GPUs) parallelization. However, the scale up approach still has limitation mainly caused by the amount of memory available on GPUs [12,21].\nLearn from the scale up limited capability, we proposed a scale out approach based on MapReduce model to distribute the big data computation into several CNN models. We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27]. The CNN works as unsupervised convolution features learner and ELM works as supervised classifier. We employed parallel stochastic gradient descent (SGD) algorithm [29] to fine tune the weights of CNN-ELM and to average the final weights of CNN-ELM.\nOur main contributions in this paper are as follows.\n1. We studied the CNN-ELM integration using MapReduce model;\n2. We employed map processes as CNN-ELM multi classifiers learning independently (asynchronous) on different partition of training data. The reduce process is the averaging all weights (kernel weights on CNN and output weights on ELM) of all CNN-ELM classifiers. Our method enables scale out combination of highly scale up CNN-ELM members to handle very huge training data. Our idea is to place MapReduce model not intended for CNN matrix operation level but for classifier level. Many asynchronous CNN models trained together to solve very large complex problem rather than single models trained in very powerful machine.\n2/14\n3. Against ELM tenet for non iterative training, we studied the weight after fine tuning using stochastic gradient descent iteration during ELM training to check the averaging performance after some iterations.\nThe rest of this paper is organized as follows. Section 1 is to give introduction and research objectives. In Section 2, a related review of previous MapReduce framework implementations is given. Section 3 is to describe our proposed methods. Our empirical experiments result is introduced in Section 4. Finally, conclusions are drawn in Section 5."}, {"heading": "2 Literature Reviews", "text": ""}, {"heading": "2.1 Parallel SGD and weight averaging", "text": "SGD is a very popular training algorithm for various machine learning models i.e., regression, SVM, and NN. Zinkevich et.al [12] proposed a parallel model of SGD based on MapReduce that highly suitable for parallel and large-scale machine learning. In parallel SGD, the training data is accessed locally by each model and only communicated when it finished. The algorithm of parallel SGD is described below 1.\n[12]\n1: Define P = bm/kc 2: Randomly partition the training, giving P examples to each machine. 3: for all i \u2208 {1, ..., k} parallel do 4: Randomly shuffle the data on machine i 5: Initialize wi,0 = 0 6: for all p \u2208 {1, ..., P} do 7: Get the pth training on the ith machine ci,p 8: wi,p \u2190 wi,p\u22121 \u2212 \u03b7\u03b4ci(wi,p\u22121) 9: end for\n10: end for 11: Aggregate from all machines v = 1k \u2211k i=1 wi\n12: return v Algorithm 1: SimuParallelSGD(Training {x1,...,xm}; Learning Rate \u03b7; Machines k)\nThe idea of averaging was developed by Polyak et.al [20]. The averaged SGD is ordinary SGD that averages its weight over time. When optimization is finished, the averaged weight replaces the ordinary weight from SGD. It is based on the idea of averaging the trajectories, however the application requires a large amount of a priori information.\nLet we have unlimited training data : { (x(0), t(0)), (x(1), t(1)), \u00b7 \u00b7 \u00b7 , (x(\u221e), t(\u221e)) }\nwithin the same distribution. Learning objective is to construct the mapping function \u03b2\u0302 from observation data that taken randomly and its related class. However, when the number of training data m\u2192\u221e, we need to address the expected value of \u03b2\u0302(w) with w is the learning parameters. According to law of large numbers, we can make sure the consistency of expected value of learning model is \u03b2\u0302(w) approximated by the sample\naverages 1m \u2211m\ni=1 \u03b2\u0302(w)i and almost surely to the expected value as m\u2192\u221e with probability 1.\nIf the m training data is partitioned by k to be T partition, and each partition trained independently { \u03b2\u0302(w)0, \u03b2\u0302(w)1, ..., \u03b2\u0302(w)T } , we can make sure the expected value\n\u03b2\u0302(w) is approximated by 1T \u2211T i=1 \u03b2\u0302(w)i where T = bm/kc.\n3/14"}, {"heading": "2.2 MapReduce in ELM", "text": "Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10]. It used single hidden layer feedforward neural network (SLFN) architecture and generalized pseudoinverse for learning process. Similar with Neural Networks (NN), ELM used random value in hidden nodes parameters. The uniqueness of ELM is non iterative generalized pseudoinverse optimization process However, the hidden nodes parameters remain set and fixed after the training. It becomes the ELM training is fast and can avoid local minima.\nThe ELM learning result is Output weight (\u03b2) that can be computed by:\n\u03b2\u0302 = H\u2020T (1)\nwhich H\u2020 is a pseudoinverse (Moore-Penrose generalized inverse) function of H. The ELM learning objective is to find the smallest least-squares solution of linear system H\u03b2 \u2212Y that can be obtained when \u03b2\u0302 = H\u2020T.\nHidden layer matrix H is computed by activation function g of the summation matrix from the hidden nodes parameter (such as input weight a and bias b) and training input x with size N number of training data and L number of hidden nodes g(ai \u00b7 x + bi) (called random feature mapping).\nThe performance of ELM hinges on generalized inverse solution. The solution of H\u2020 uses ridge regression orthogonal projection method, by using a positive 1/\u03bb value as regularization to the auto correlation matrices HTH or HHT . Thus, we can solve Eq. 1 as follows.\n\u03b2 =\n( I\n\u03bb + HTH\n)\u22121 HTT (2)\nFurther, Eq. 2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (E2LM) [27] or Parallel ELM [6]).\nParallelization process using MapReduce approach can be divided as follows :\n1. Map. Map is the transformation of intermediate matrix multiplications for each training data and target portion.\nIf U = HTH and V = HTT, According to decomposable matrices, they can be written as :\nU = k=\u221e\u2211 k=0 U(k) (3)\nV = k=\u221e\u2211 k=0 V(k) (4)\n2. Reduce. Reduce is the aggregate process to sum the Map result. The output weights \u03b2 can be computed easily from reduce/aggregate process.\n\u03b2 =\n( I\n\u03bb + U\n)\u22121 V (5)\n4/14\nTherefore, MapReduce based ELM is more efficient for massive training data set, can be solved easily by parallel computation and has better performance [27].\nRegarding about iteration, Lee et.al [11] explained on BP Trained Weight-based ELM that the optimized input weights with BP training is more feasible than randomly assigned weights. Lee et.al implemented Average ELM however the classification accuracy was lower than basic ELM because the number of training data is so small and the network architecture is not large."}, {"heading": "2.3 MapReduce in CNN", "text": "CNN is biologically-inspired [14] from visual cortex that has a convolution arrangement of cells (a receptive field that sensitive to small sub visual field and local filter) and following by simple cells that only respond maximally to specific trigger within receptive fields. A simple CNN architecture consists of some convolution layers and following by pooling layers in the feed forward architecture. CNN has excellent performance for spatial visual classification [22].\nThe input layer exposes 2D structure with d\u00d7 d\u00d7 r of image, and r is the number of input channels. The convolution layer has c filters (or kernels) of size k \u00d7 k \u00d7 q where k < d and q can either be the same or smaller than the number of input channels r. The filters have locally connected structure which is each convolved with the image to produce c feature maps of size d\u2212 k + 1. If, at a given layer, we have the rth feature map as hr, whose filters are determined by the weights W r and bias br, then the feature map hr is obtained as :\nhrij = g((W r \u2217 x)ij + br) (6)\nEach feature map is then pooled using pooling operation either down sampling, mean or max sampling over s\u00d7 s\u00d7 s contiguous regions (Using scale s ranges between 2 for small and up to 5 for larger inputs). An additive bias and activation function (i.e. sigmoid, tanh, or reLU) can be applied to each feature map either before or after the pooling layer. At the end of the CNN layer, there may be any densely connected NN layers for supervised learning (See Fig. 1) [28]. Many variants of CNN architectures in the literatures, but the basic common building blocks are convolutional layer, pooling layer and fully connected layer [4].\nThe convolution operations need to be computed in parallel for faster computation time that can be taken from multi processor hardware, i.e., GPU [21]. Krizhevsky et. al. demonstrated a large CNN is capable of achieving record breaking results on the 1.2 million high-resolution images with 1000 different classes. However, the GPU has memory size limitation that limit the CNN network size to achieve better accuracy [12].\nCNN used back propagation algorithm that needs iterations to get the optimum solution. One iteration contains error back propagated step and following by parameter update step. The learning errors are propagated back to the previous layers using SGD optimization and continued by applying the update to kernel weight and bias parameters.\nIf \u03b4(l+1) is the error on (l + 1)th layer from a cost function J(W, b;x, t) where W is weight, b is bias parameters, and (x, t) are the training data and target.\nJ(W, b;x, t) = 1\n2 \u2016 f(z)\u2212 t \u20162 (7)\nIf the lth layer is densely connected and the (l + 1)th is output layer, then the error \u03b4(l) and the gradients for the lth layer are computed as :\n\u03b4(l) = ( (W (l))T \u03b4(l+1) ) \u00b7 f \u2032(z(l)) (8)\n5/14\nWhere f \u2032 is the derivative of the activation function.\n\u2207W (l)J(W, b;x, t) = \u03b4(l+1)(a(l))T (9) \u2207b(l)J(W, b;x, t) = \u03b4(l+1) (10)\nBut if the lth layer is a convolutional and subsampling layer then the error is computed as :\n\u03b4(l)r = pool ( (W (l)r ) T \u03b4(l+1)r ) \u00b7 f \u2032(z(l)r ) (11)\nWhere pool is the related pooling operation. To calculate the gradient to the filter maps, we used convolution operation and flip operation to the error matrix.\n\u2207 W (l) r J(W, b;x, t) = m\u2211 i=1 (a (l) i ) \u2217 rot90(\u03b4 (l+1) r , 2) (12)\n\u2207 b (l) r J(W, b;x, t) = \u2211 a,b (\u03b4(l+1))a,b (13)\nWhere a(l) is the input of lth layer, and l = 1 is the input layer. Finally, one iteration will update the parameters W and b with \u03b1 learning rate, as follows:\nW (l) ij = W (l) ij \u2212 \u03b1\n\u2202\n\u2202W (l) ij\nJ(W, b) (14)\nb (l) i = b (l) i \u2212 \u03b1\n\u2202\n\u2202b (l) i\nJ(W, b) (15)\n.\nMost CNN implementations are using GPU [21] to speed up convolution operation that required hundred numbers of core processors. Wang et. al [26] used MapReduce on Hadoop platform to take advantage of the computing power of multi core CPU to solve matrix parallel computation. However, the number of multi core CPU is far less than GPU can provide.\nGPU has limited shared memory than CPU global memory. Scherer et. al [21] explained because shared memory is very limited, so it reuses loaded data as often as possible. Comparing with CPU, the global memory in CPU can be extended larger with lower price than additional GPU cards.\n6/14"}, {"heading": "3 Proposed Method", "text": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig. 1). For better generalization accuracy, we used nonlinear optimal tanh (1.7159\u00d7 tanh( 23 \u00d7H) activation function [15]. We used the E2LM as a parallel supervised classifier to replace fully connected NN. Compared with regular ELM, we do not need input weight as hidden nodes parameter (See Fig. 2).\n.\nThe idea of backward is similar with densely connected NN back propagation error method with cost function :\nJ(\u03b2; z, t) = 1\n2 \u2016 H(z)\u03b2 \u2212T \u20162 (16)\nThen it propagated back with SGD to optimize the weight kernels of convolution layers (See Fig. 3).\n.\nDetail algorithm is explained on Algorithm 2."}, {"heading": "4 Experiment and Performance Results", "text": ""}, {"heading": "4.1 Data set", "text": "MNIST is the common data set for big data machine learning, in fact, it accepted as standard and give an excellent result. MNIST data set is a balanced data set that contains numeric (0-9) (10 target class) with size 28\u00d7 28 pixel in a gray scale image. The dataset has been divided for 60,000 examples for training data and separated 10,000 examples for testing data [16]. We extended MNIST data set 3\u00d7 larger by adding 3 types of image noises (See Fig. 4) to be 240,000 examples of training data and 40,000 examples of testing data.\nFor additional experiments, we used not-MNIST large data set [2] that has a lot of foolish images (See Fig. 5 and 6). Not-MNIST has gray scale 28\u00d7 28 image size as\n7/14\n1: Define P = bm/kc 2: Randomly partition the training, giving P examples to each machine. 3: Initialize CNN weight parameters similar for k machines 4: for all i \u2208 {1, ..., k} parallel do 5: Randomly shuffle the data on machine i 6: for all j \u2208 {1, ..., e} do 7: Reset \u03a3U = 0;\u03a3V = 0 8: for all p \u2208 {1, ..., P} do 9: Get H from pth CNN training on the ith machine ci,p\n10: Compute \u03a3U = \u03a3U +HTH 11: Compute \u03a3V = \u03a3U +HTT 12: Compute \u03b2 13: Propagate ELM Error back to CNN 14: Update Kernel Weights W (l) ij and bias b (l) ij 15: end for 16: end for 17: end for 18: Aggregate for each lth layers W\u0302 (l) = 1k \u2211k i=1W (l) i\n19: Aggregate for each lth layers b\u0302(l) = 1k \u2211k i=1 b (l) i\n20: Aggregate for each lth layers \u03b2\u0302 = 1k \u2211k i=1 \u03b2i 21: return W\u0302 , b\u0302, \u03b2\u0302\nAlgorithm 2: Distributed CNNELM(Training {x1,...,xm}; Learning Rate \u03b7; Machines k; iterations e)\n.\nattributes. We divided the set to be numeric (0-9) (360,000 data) and alphabet (A-J) symbol (540,000) data including many foolish images. The challenge with not-MNIST numeric and not-MNIST alphabet is many similarities between class 1 with class I, class 4 with class A, and another look alike foolish images."}, {"heading": "4.2 Experiment Methods", "text": "We defined the scope of works as following:\n1. We enhanced DeepLearn Toolbox [18] with Matlab parallel computing toolbox.\n8/14\n2. We used single precision for all computation in this paper.\n3. We focused on simple CNN architecture that consist of convolution layers (c), following by reLU activation layer then pooling layer (s) with down sampling in this paper.\n4. We compared the performance in testing accuracy with non partitioned sequential CNN-ELM classifier using the similar structure size.\nTo verify our method, we formulated the following research questions:\n\u2022 How is the performance following number of iterations?\n\u2022 How is the effectiveness of weight averaging CNN-ELM model for various number of training partition?\n\u2022 How is the performance consistencies of weight averaging CNN-ELM model following number of iterations?"}, {"heading": "4.3 Performance Results", "text": "In this section, we explained the research questions as follows.\n\u2022 The performance of CNN-ELM can be improved by using back propagation algorithm. However, we need to select the appropriate learning rate parameter, number of batch and number of iteration that could impact to the final performance (See Fig. 7). The wrong parameter selection especially learning rate\n9/14"}, {"heading": "5 Conclusion", "text": "The proposed CNN-ELM method gives better scale out capability for processing large data set in parallel. We can partition large data set. We can assign CNN-ELM classifier for each partition, then we just aggregated the result by averaging the weight parameters of all CNN-ELM parameters. Thus, it can safe a lot of training time rather\n10/14\nthan sequential training. However, more CNN-ELM classifiers (smaller partition) has worse performance for averaging CNN-ELM, as well as more iterations and data distribution effect.\nWe think some ideas for future research:\n11/14"}, {"heading": "6 Acknowledgment", "text": "This work is supported by Higher Education Center of Excellence Research Grant funded Indonesia Ministry of Research and Higher Education Contract No. 1068/UN2.R12/ HKP.05.00/2016"}, {"heading": "7 Conflict of Interests", "text": "The authors declare that there is no conflict of interest regarding the publication of this paper.\nReferences\n1. B. Barney.\n2. Y. Bulatov. notmnist dataset, September 2011.\n3. J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. Commun. ACM, 51(1):107\u2013113, Jan. 2008.\n4. J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, and G. Wang. Recent advances in convolutional neural networks. CoRR, abs/1512.07108, 2015.\n5. L. Guo and S. Ding. A hybrid deep learning cnn-elm model and its application in handwritten numeral recognition. page 2673\u20132680, 7 2015.\n6. Q. He, T. Shang, F. Zhuang, and Z. Shi. Parallel extreme learning machine for regression based on mapreduce. Neurocomput., 102:52\u201358, Feb. 2013.\n7. G. Huang, G.-B. Huang, S. Song, and K. You. Trends in extreme learning machines: A review. Neural Networks, 61(0):32 \u2013 48, 2015.\n8. G.-B. Huang, Z. Bai, L. L. C. Kasun, and C. M. Vong. Local receptive fields based extreme learning machine. IEEE Computational Intelligence Magazine (accepted), 10, 2015.\n9. G.-B. Huang, D. Wang, and Y. Lan. Extreme learning machines: a survey. International Journal of Machine Learning and Cybernetics, 2(2):107\u2013122, 2011.\n10. G.-B. Huang, Q. Y. Zhu, and C. K. Siew. Extreme learning machine: theory and applications. Neurocomputing, 70(1-3):489\u2013501, 2006.\n11. K. P. D.-C. P. Y.-M. J. Kheon-Hee Lee, Miso Jang and S.-Y. Min. An efficient learning scheme for extreme learning machine and its application. 2015.\n12. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc., 2012.\n13. D. Laney. 3D data management: Controlling data volume, velocity, and variety. Technical report, META Group, February 2001.\n14. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In S. Haykin and B. Kosko, editors, Intelligent Signal Processing, pages 306\u2013351. IEEE Press, 2001.\n13/14\n15. Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Mu\u0308ller. Effiicient backprop. In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK, 1998. Springer-Verlag.\n16. Y. LeCun and C. Cortes. Mnist handwritten digit database, 2010.\n17. N.-Y. Liang, G.-B. Huang, P. Saratchandran, and N. Sundararajan. A fast and accurate online sequential learning algorithm for feedforward networks. Neural Networks, IEEE Transactions on, 17(6):1411\u20131423, Nov 2006.\n18. R. B. Palm. Deep learning toolbox.\n19. S. Pang and X. Yang. Deep convolutional extreme learning machine and its application in handwritten digit classification. Hindawi, 2016.\n20. B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, 1992.\n21. D. Scherer, H. Schulz, and S. Behnke. Accelerating Large-Scale Convolutional Neural Networks with Parallel Graphics Multiprocessors, pages 82\u201391. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.\n22. P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition - Volume 2, ICDAR \u201903, pages 958\u2013, Washington, DC, USA, 2003. IEEE Computer Society.\n23. D. Singh and C. K. Reddy. A survey on platforms for big data analytics. Journal of Big Data, 2(1):1\u201320, 2014.\n24. C. tao Chu, S. K. Kim, Y. an Lin, Y. Yu, G. Bradski, K. Olukotun, and A. Y. Ng. Map-reduce for machine learning on multicore. In B. Scho\u0308lkopf, J. C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 281\u2013288. MIT Press, 2007.\n25. C. Tsang, K. Tsoi, J. H. Yeung, B. S. Kwan, A. P. Chan, C. C. Cheung, and P. H. Leong. Map-reduce as a programming model for custom computing machines. Field-Programmable Custom Computing Machines, Annual IEEE Symposium on, 00(undefined):149\u2013159, 2008.\n26. Q. Wang, J. Zhao, D. Gong, Y. Shen, M. Li, and Y. Lei. Parallelizing convolutional neural networks for action event recognition in surveillance videos. International Journal of Parallel Programming, pages 1\u201326, 2016.\n27. J. Xin, Z. Wang, L. Qu, and G. Wang. Elastic extreme learning machine for big data classification. Neurocomputing, 149, Part A:464 \u2013 471, 2015.\n28. M. D. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks, pages 818\u2013833. Springer International Publishing, Cham, 2014.\n29. M. Zinkevich, M. Weimer, L. Li, and A. J. Smola. Parallelized stochastic gradient descent. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2595\u20132603. Curran Associates, Inc., 2010.\n14/14"}], "references": [{"title": "Mapreduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Commun. ACM, 51(1):107\u2013113, Jan.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Recent advances in convolutional neural networks", "author": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang"], "venue": "CoRR, abs/1512.07108,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid deep learning cnn-elm model and its application in handwritten numeral recognition", "author": ["L. Guo", "S. Ding"], "venue": "page 2673\u20132680, 7", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel extreme learning machine for regression based on mapreduce", "author": ["Q. He", "T. Shang", "F. Zhuang", "Z. Shi"], "venue": "Neurocomput., 102:52\u201358, Feb.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Trends in extreme learning machines: A review", "author": ["G. Huang", "G.-B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, 61(0):32 \u2013 48,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Local receptive fields based extreme learning machine", "author": ["G.-B. Huang", "Z. Bai", "L.L.C. Kasun", "C.M. Vong"], "venue": "IEEE Computational Intelligence Magazine (accepted), 10,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G.-B. Huang", "D. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics, 2(2):107\u2013122,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, 70(1-3):489\u2013501,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "An efficient learning scheme for extreme learning machine and its application", "author": ["Miso Jang", "S.-Y. Min"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "3D data management: Controlling data volume, velocity, and variety", "author": ["D. Laney"], "venue": "Technical report, META Group, February", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In S. Haykin and B. Kosko, editors, Intelligent Signal Processing, pages 306\u2013351. IEEE Press,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "and C", "author": ["Y. LeCu"], "venue": "Cortes. Mnist handwritten digit database,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, 17(6):1411\u20131423, Nov", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep convolutional extreme learning machine and its application in handwritten digit classification", "author": ["S. Pang", "X. Yang"], "venue": "Hindawi,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Accelerating Large-Scale Convolutional Neural Networks with Parallel Graphics Multiprocessors, pages 82\u201391", "author": ["D. Scherer", "H. Schulz", "S. Behnke"], "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In Proceedings of the Seventh International Conference on Document Analysis and Recognition Volume 2, ICDAR \u201903, pages 958\u2013, Washington, DC, USA,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "A survey on platforms for big data analytics", "author": ["D. Singh", "C.K. Reddy"], "venue": "Journal of Big Data, 2(1):1\u201320,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Map-reduce for machine learning on multicore", "author": ["C. tao Chu", "S.K. Kim", "Y. an Lin", "Y. Yu", "G. Bradski", "K. Olukotun", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Map-reduce as a programming model for custom computing machines", "author": ["C. Tsang", "K. Tsoi", "J.H. Yeung", "B.S. Kwan", "A.P. Chan", "C.C. Cheung", "P.H. Leong"], "venue": "Field-Programmable Custom Computing Machines, Annual IEEE Symposium on, 00(undefined):149\u2013159,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallelizing convolutional neural networks for action event recognition in surveillance videos", "author": ["Q. Wang", "J. Zhao", "D. Gong", "Y. Shen", "M. Li", "Y. Lei"], "venue": "International Journal of Parallel Programming, pages 1\u201326,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Elastic extreme learning machine for big data classification", "author": ["J. Xin", "Z. Wang", "L. Qu", "G. Wang"], "venue": "Neurocomputing, 149, Part A:464 \u2013 471,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks, pages 818\u2013833", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Springer International Publishing, Cham,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola"], "venue": "In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2595\u20132603. Curran Associates, Inc.,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Volume and velocity issues are critical in overcoming big data challenges [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "It can be categorized into the following two types of scalability [23]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "To overcome overhead complexities in parallel programming, Google introduced a programming model named MapReduce [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 21, "context": "MapReduce provides two essential functions: 1) Map function, it processes each sub problems to another nodes within the cluster; 2) Reduce function, it organizes the results from each node to be a cohesive solution [25].", "startOffset": 215, "endOffset": 219}, {"referenceID": 21, "context": "Developing MapReduce is simple by firstly exposing structure and process similarity and then aggregation process [25].", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 221, "endOffset": 225}, {"referenceID": 25, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 261, "endOffset": 265}, {"referenceID": 22, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 302, "endOffset": 306}, {"referenceID": 23, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 339, "endOffset": 343}, {"referenceID": 11, "context": "CNN [14] is a popular machine learning that getting benefits from parallel computation.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "However, the scale up approach still has limitation mainly caused by the amount of memory available on GPUs [12,21].", "startOffset": 108, "endOffset": 115}, {"referenceID": 17, "context": "However, the scale up approach still has limitation mainly caused by the amount of memory available on GPUs [12,21].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 35, "endOffset": 45}, {"referenceID": 11, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 35, "endOffset": 45}, {"referenceID": 24, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 35, "endOffset": 45}, {"referenceID": 4, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 6, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 7, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 23, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 25, "context": "We employed parallel stochastic gradient descent (SGD) algorithm [29] to fine tune the weights of CNN-ELM and to average the final weights of CNN-ELM.", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "al [12] proposed a parallel model of SGD based on MapReduce that highly suitable for parallel and large-scale machine learning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "[12] 1: Define P = bm/kc 2: Randomly partition the training, giving P examples to each machine.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "al [20].", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "2 MapReduce in ELM Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10].", "startOffset": 119, "endOffset": 129}, {"referenceID": 6, "context": "2 MapReduce in ELM Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10].", "startOffset": 119, "endOffset": 129}, {"referenceID": 7, "context": "2 MapReduce in ELM Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10].", "startOffset": 119, "endOffset": 129}, {"referenceID": 14, "context": "2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (ELM) [27] or Parallel ELM [6]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (ELM) [27] or Parallel ELM [6]).", "startOffset": 234, "endOffset": 238}, {"referenceID": 3, "context": "2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (ELM) [27] or Parallel ELM [6]).", "startOffset": 255, "endOffset": 258}, {"referenceID": 23, "context": "Therefore, MapReduce based ELM is more efficient for massive training data set, can be solved easily by parallel computation and has better performance [27].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "al [11] explained on BP Trained Weight-based ELM that the optimized input weights with BP training is more feasible than randomly assigned weights.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "3 MapReduce in CNN CNN is biologically-inspired [14] from visual cortex that has a convolution arrangement of cells (a receptive field that sensitive to small sub visual field and local filter) and following by simple cells that only respond maximally to specific trigger within receptive fields.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "CNN has excellent performance for spatial visual classification [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "1) [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Many variants of CNN architectures in the literatures, but the basic common building blocks are convolutional layer, pooling layer and fully connected layer [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 17, "context": ", GPU [21].", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "However, the GPU has memory size limitation that limit the CNN network size to achieve better accuracy [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "Most CNN implementations are using GPU [21] to speed up convolution operation that required hundred numbers of core processors.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "al [26] used MapReduce on Hadoop platform to take advantage of the computing power of multi core CPU to solve matrix parallel computation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "al [21] explained because shared memory is very limited, so it reuses loaded data as often as possible.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 45}, {"referenceID": 5, "context": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 45}, {"referenceID": 15, "context": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 45}, {"referenceID": 12, "context": "7159\u00d7 tanh( 23 \u00d7H) activation function [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The dataset has been divided for 60,000 examples for training data and separated 10,000 examples for testing data [16].", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required. Keywords\u2014 deep learning, extreme learning machine, convolutional, neural network, big data, map reduce", "creator": "LaTeX with hyperref package"}}}