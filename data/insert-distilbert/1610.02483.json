{"id": "1610.02483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "Boost K-Means", "abstract": "due to its simplicity and versatility, k - means remains popular since it was proposed three decades ago. since then, continuous efforts have seemingly been taken to enhance its performance. unfortunately, providing a good trade - off between quality and efficiency compared is effectively hardly reached. late in this paper, a novel programming k - means variant is presented. different from most of k - means complexity variants, the clustering validation procedure is explicitly driven by an objective function, which is feasible for the whole l2 - space. the classic turing egg - loading chicken mating loop in k - means has been simplified to a pure stochastic optimization comparison procedure. designing k - means therefore becomes simpler, faster and cost better. the effectiveness of this new variant variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. superior performance similarity is observed across different scenarios.", "histories": [["v1", "Sat, 8 Oct 2016 04:36:42 GMT  (381kb,D)", "https://arxiv.org/abs/1610.02483v1", "11 pages, 10 figures"], ["v2", "Sun, 4 Dec 2016 07:32:37 GMT  (244kb,D)", "http://arxiv.org/abs/1610.02483v2", "11 pages, 6 figures"]], "COMMENTS": "11 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DB", "authors": ["wan-lei zhao", "cheng-hao deng", "chong-wah ngo"], "accepted": false, "id": "1610.02483"}, "pdf": {"name": "1610.02483.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wan-Lei Zhao", "Cheng-Hao Deng"], "emails": ["wlzhao@xmu.edu.cn", "cscwngo@gapps.cityu.edu.hk"], "sections": [{"heading": null, "text": "Index Terms\u2014clustering, k-means, incremental optimization\nF"}, {"heading": "1 INTRODUCTION", "text": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7]. In the last three decades, various clustering algorithms have been proposed. Among these algorithms, kmeans [8] remains a popular choice for its simplicity, efficiency and moderate but stable performance across different problems. It was known as one of top ten most popular algorithms in data mining [9]. On one hand, k-means has been widely adopted in different applications. On the other hand, continuous efforts have been devoted to enhance the performance k-means as well.\nDespite its popularity, it actually suffers from several latent issues. Although the time complexity is linear to data size, traditional k-means is still not sufficiently efficient to handle the web-scale data. In some specific scenarios, the running time of k-means could be even exponential in the worst case [10], [11]. Moreover, k-means usually only converges to local optima. As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18]. K-means has been also tailored to perform web-scale image clustering [2], [19].\nThere are in general three steps involved in the clustering procedure. Namely, 1. initialize k cluster centroids; 2. assign each sample to its closest centroid; 3. recompute cluster centroids with assignments produced in Step 2 and go back to Step 2 until convergence. This is known as Lloyd iteration procedure [8]. The iteration repeats Step 2 and Step 3 until the centroids do not change between two consecutive rounds. Given C1\u00b7\u00b7\u00b7k \u2208 Rd are cluster centroids, {xi \u2208 Rd}i=1\u00b7\u00b7\u00b7n are samples to be clustered, above proce-\n\u2022 Fujian Key Laboratory of Sensing and Computing for Smart City, and the School of Information Science and Engineering, Xiamen University, Xiamen, 361005, P. R. China. E-mail: wlzhao@xmu.edu.cn \u2022 Department of Computer Science, City University of Hong Kong. E-mail: cscwngo@gapps.cityu.edu.hk\ndure essentially minimizes the following objective function:\nmin \u2211\nq(xi)=r\n\u2016 Cr \u2212 xi \u20162. (1)\nIn Eqn. 1, function q(\u00b7) returns the closest centroid for sample xi. Unfortunately, searching an optimal solution for the above objective function is NP-hard. In general kmeans only converges to local minimum [20]. The reason that k-means maintains its popularity is mainly due to its linear complexity in terms of the number of samples to be clustered. The complexity is O(t\u00b7k\u00b7n\u00b7d), given t as the number of iterations to converge. Compared with other well-known clustering algorithms such as DBSCAN [21] and Mean shift [22], this complexity is considerably low. However, the efficiency of traditional k-means cannot cope with the massive growth of data in Internet. In particular, in the case that the size of data (m), the number of clusters (k) and the dimension (d) are all very large, k-means becomes unbearably slow. The existing efforts [16], [18] in enhancing the scalability of k-means for web-scale tasks often come with price of lower clustering quality. On the other hand, k-means++ proposed in [12], [17] focuses on enhancing the clustering quality by a careful design of the initialization procedure. However, k-means is slow down as a few rounds of scanning over the dataset is still necessary in the initialization.\nIn this paper, a novel variant of k-means is proposed, which aims to make a better trade-off between clustering quality and efficiency. Inspired by the work in [1], a novel objective function is derived from Eqn. 1. With the development of this objective function, the traditional k-means iteration procedure has been revised to a simpler form, in which the costly initial assignment becomes unnecessary. In addition, driven by the objective function, sample is moved from one cluster to another cluster when we find this movement leads to higher objective function score, which is known as incremental clustering [1], [23]. These modifications lead to several advantages.\n\u2022 K-means clustering without initial assignment results in better quality as well as higher speed effi-\nar X\niv :1\n61 0.\n02 48\n3v 2\n[ cs\n.L G\n] 4\nD ec\n2 01\n6\n2 ciency. \u2022 K-means iteration driven by an explicit objective\nfunction converges to considerably lower clustering distortion in faster pace. \u2022 Different from traditional k-means, it is not necessary to assign a sample to its closest centroid in each iteration, which also leads to higher speed.\nIn addition, when clustering in hierarchical bisecting fashion, the proposed method achieves the highest scalability among all top-down hierarchical clustering methods. Extensive experiments are conducted to contrast the performance of proposed method with k-means and its variants including tasks document clustering [1], nearest neighbor search (NNS) with product quantization [4] and image clustering.\nThe remainder of this paper is organized as follows. The reviews about representative works on improving the performance of traditional k-means are presented in Section 2. In Section 3, the clustering objective functions are derived based on Eqn. 1. Based on the objective function, Section 4 presents the clustering method. Extensive experiement studies over proposed clustering method are presented in Section 5. Section 6 concludes the paper."}, {"heading": "2 RELATED WORKS", "text": "Clustering is a process of partitioning a set of samples into a number of groups without any supervised training. Due to its versatility in different contexts, it has been studied in the last three decades [24]. As the introduction of Web 2.0, millions of data in Internet has been generated on a daily basis. Clustering becomes one of the basic tools to process such big volume of data. As a consequence, traditional clustering methods have been shed with new light. People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data. In general, boosting the performance of traditional k-means becomes the major trend due to its simplicity and relative higher efficiency over other clustering methods.\nIn general, there are two major ways to enhance the performance of k-means. For the first kind, the aim is to improve the clustering quality. One of the important work comes from Ostrovsky et al. [12], [17]. The motivation is based on the observation that k-means converges to a better local optima if the initial cluster centroids are carefully selected. According to [12], k-means iteration also converges faster due to the careful selection on the initial cluster centroids. However, in order to adapt the initial centroids to the data distribution, k rounds of scanning over the data are necessary. Although the number of scanning rounds has been reduced to a few in [17], the extra computational cost is still inevitable.\nIn each k-means iteration, the processing bottleneck is the operation of assigning each sample to its closest centroid. The iteration becomes unbearably slow when both the size and the dimension of the data are very large. Noticed that this is a nearest neighbor search problem, Kanungo et al. [14] proposed to index dataset in a KD Tree [25] to speed-up the sample-to-centroid nearest neighbor search. However, this is only feasible when the dimension of data is in few tens. Similar scheme has been adopted by Dan\net al. [26]. Unfortunately, due to the curse of dimensionality, this method becomes ineffective when the dimension of data grows to a few hundreds. A recent work [18] takes similar way to speed-up the nearest neighbor search by indexing dataset with inverted file structure. During the iteration, each centroid is queried against all the indexed data. Attributing to the efficiency of inverted file structure, one to two orders of magnitude speed-up is observed. However, inverted file indexing structure is only effective for sparse vectors.\nAlternatively, the scalability issue of k-means is addressed by subsampling over the dataset during k-means iteration. Namely, methods in [16], [27] only pick a small portion of the whole dataset to update the cluster centroids each time. For the sake of speed efficiency, the number of iterations is empirically set to small value. It is therefore possible that the clustering terminates without a single pass over the whole dataset, which leads to higher speed but also higher clustering distortion. Even though, when coping with high dimensional data in big size, the speed-up achieved by these methods is still limited.\nApart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29]. Specifically, the clustering solution is obtained via a sequence of repeated bisections. The clustering complexity of k-means is reduced from O(t\u00b7k\u00b7n\u00b7d) to O(t\u00b7log(k)\u00b7n\u00b7d). This is particularly significant when n, d and k are all very large. In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23]. Moreover, the update process is explicitly driven by an objective function (called as criterion function in [1], [29]). Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length. The clustering method is solely based on Cosine distance, which makes the clustering results unpredictable when dealing with data in the general l2-space.\nIn this paper, a new objective function is derived directly from Eqn. 1, which makes it suitable for the whole l2space. In other word, objective function proposed in [1] is the special case of our proposed form. Based on the proposed objective function, conventional egg-chicken kmeans iteration is revised to a simpler form. On one hand, when applying the revised iteration procedure in direct kway clustering, k-means is able to reach to considerably lower clustering distortion within only a few rounds. On the other hand, as the iteration procedure is undertaken in topdown hierarchical clustering manner (specifically bisecting), it shows faster speed while maintaining relatively lower clustering distortion in comparison to traditional k-means and most of its variants."}, {"heading": "3 CLUSTERING OBJECTIVE FUNCTIONS", "text": "In this section, the clustering objective functions upon which our k-means method is built are presented. Basically, two objective functions that aim to optimize the clustering results from different aspects are derived. Furthermore, we also show that these two objective functions can be reduced to a single form.\n3"}, {"heading": "3.1 Preliminaries", "text": "In order to facilitate the discussions that are followed, several variables are defined. Throughout the paper, the size of input data is given as n, while the number of clusters to be produced is given as k. The partition formed by a clustering method is represented as {S1, \u00b7 \u00b7 \u00b7 , Sr \u00b7 \u00b7 \u00b7 , Sk}. Accordingly, the sizes of clusters are given as n1, \u00b7 \u00b7 \u00b7 , nr, \u00b7 \u00b7 \u00b7 , nk. The composite vector of a cluster is defined as Dr = \u2211 xi\u2208Sr xi. The cluster centroid Cr1 is defined by its members,\nCr = \u2211nr i=1 xi nr = Dr nr\n(2)\nThe inner-product of Cr is given by C \u2032rCr = ( \u2211nr i=1 xi) \u2032( \u2211nr\ni=1 xi) n2r , which is expanded as following form.\nC \u2032rCr = 1\nn2r [(x\u20321x1 + \u00b7 \u00b7 \u00b7+ x\u20321xi + \u00b7 \u00b7 \u00b7+ x\u20321xnr )+\n(x\u20322x1 + \u00b7 \u00b7 \u00b7+ x\u20322xi + \u00b7 \u00b7 \u00b7+ x\u20322xnr )+ \u00b7 \u00b7 \u00b7 (x\u2032ix1 + \u00b7 \u00b7 \u00b7+ x\u2032ixi + \u00b7 \u00b7 \u00b7+ x\u2032ixnr )+ \u00b7 \u00b7 \u00b7 (x\u2032nrx1 + \u00b7 \u00b7 \u00b7+ x \u2032 nxi + \u00b7 \u00b7 \u00b7+ x\u2032nrxnr )]\n= 1 n2r ( nr\u2211 i=1 x2i + 2 nr\u2211 i,j=1&i<j < xi, xj >)\nRe-arrange the above equation, we have nr\u2211\ni,j=1&i<j\n< xi, xj >= 1\n2 (nr 2\u00b7C \u2032rCr \u2212 nr\u2211 i=1 x2i ). (3)\nThe sum of pairwise l2-distance within one cluster is given as\nS = (nr \u2212 1) nr\u2211 i=1 x2i \u2212 2\u00b7 nr\u2211 i,j=1&i<j < xi, xj > . (4)\nPlug Eqn. 3 into Eqn. 4, we have\nS = (nr \u2212 1) nr\u2211 i=1 x2i \u2212 (nr2\u00b7C \u2032rCr \u2212 nr\u2211 i=1 x2i )\n= (nr \u2212 1) nr\u2211 i=1 x2i \u2212 nr2\u00b7C \u2032rCr + nr\u2211 i=1 x2i\n= nr nr\u2211 i=1 x2i \u2212 nr2\u00b7C \u2032rCr.\n(5)\nEqn. 5 is rewritten as\nS = nr nr\u2211 i=1 x2i \u2212D\u2032rDr. (6)"}, {"heading": "3.2 Objective Functions", "text": "In this section, two objective functions (also known as criterion functions [1]) are developed. In addition, with the support of the results obtained in Section 3.1, these objective functions will be reduced to simple forms, which\n1. We refer to as column vector across the paper.\nenable them to be carried out efficiently in the incremental optimization procedure.\nAccording to [1], objective functions are categorized into two groups. One group of the functions considers the tightness of clusters, while another focuses on alienating different clusters. In this paper, we focus on producing a clustering solution defined over the elements within each cluster. It therefore does not consider the relationship between the elements assigned to different clusters.\nThe first objective function we consider is to minimize the distance of each element to its cluster centroid, which is nothing more than the objective function of k-means.\nMin. I1 = \u2211\nq(xi)=r\n\u2016 Cr \u2212 xi \u20162\n= k\u2211 r=1 \u2211 xi\u2208Sr d(xi, Cr).\n(7)\nThe above equation is simplified as\nMin. I1 = k\u2211 r=1 ( nr\u2211 i=1 x\u2032ixi + nrC \u2032 rCr \u2212 2 nr\u2211 i=1 x\u2032iCr)\n= k\u2211 r=1 ( nr\u2211 i=1 x\u2032ixi + D\u2032rDr nr \u2212 2D \u2032 rDr nr )\n= k\u2211 r=1 ( nr\u2211 i=1 x\u2032ixi \u2212 D\u2032rDr nr )\n= k\u2211 r=1 nr\u2211 i=1 x\u2032ixi \u2212 k\u2211 r=1 D\u2032rDr nr\n=E \u2212 k\u2211 r=1 D\u2032rDr nr\n(8)\nSince the input data are fixed, E is a constant. As a result, minimizing Eqn. 8 is equivalent to maximizing following function\nMax. I\u22171 = k\u2211 r=1 D\u2032rDr nr . (9)\nAlthough objective function in Eqn. 9 is in the same form as the first objective function in [1], they are derived from different initial objectives. More importantly, in our case, there is no constraint that input data should be in unit length.\nThe second internal objective function that we will study minimizes the sum of the average pairwise distance between the elements assigned to each cluster, weighted according to the size of each cluster.\nMin. I2 = k\u2211 r=1 nr( 2 nr\u00b7(nr \u2212 1) \u2211 di,dj\u2208Sr&i>j d(xi, xj)) (10)\nPlug Eqn. 6 in, we have\nMin. I2 = k\u2211 r=1 nr( 2 nr\u00b7(nr \u2212 1) (nr nr\u2211 i=1 x\u2032ixi \u2212D\u2032rDr))\n= k\u2211 r=1 2nr nr \u2212 1 nr\u2211 i=1 x\u2032ixi \u2212 2 k\u2211 r=1 D\u2032rDr nr \u2212 1\n(11)\n4 In Eqn. 11, nrnr\u22121 is close to 1, the above criterion function can be approximated as\nMin. I2 \u2248 2E \u2212 2 k\u2211 r=1 D\u2032rDr nr . (12)\nSimilar as Eqn. 8, since the input data are fixed, E is a constant. As as result, minimizing Eqn. 12 is equivalent to maximizing function\nMax. I\u22172 \u2248 k\u2211 r=1 D\u2032rDr nr . (13)\nNoticed that similar optimization objectives have been discussed under Cosine similarity measure in [1]. As it is shown that above two objective functions are the same in the paper. This is different from the result obtained in our case (general l2-space). As shown above, in l2-space, the objective functions for I\u22171 and I\u22172 are only approximately the same. The advantage that two objective functions are reduced to the same form is that, when we try to optimize one objective function, we optimize another in the mean time. Specifically, when we minimize the distances from elements to their cluster centroid, the average intra-cluster distance is minimized in the meantime. Since these two objective functions can be simplified to the same form, only objective function I\u22171 is discussed in the rest of paper.\nAlthough objective function in Eqn. 9 is derived from Eqn. 1, the former is much easier to operate in the incremental k-means procedure. As it will be shown in the next section, it is quite convenient to evaluate whether Eqn. 9 attains a higher score (implies lower distortion in terms of Eqn. 1) when a sample xi is moved from one cluster to another."}, {"heading": "4 K-MEANS DRIVEN BY OBJECTIVE FUNCTION", "text": "In this section, with the objective function developed in Section 3, two iterative clustering procedures are presented. Namely, one produces k clusters directly (called as direct k-way k-means), while another produces k clusters by bisecting input data sequentially k-1 times (called as bisecting k-means). Both clustering strategies are built upon incremental clustering [1], [23] and driven by objective function I\u22171 (Eqn. 9)."}, {"heading": "4.1 Clustering Algorithm", "text": "The basic idea of incremental clustering is that one sample xi is moved from cluster Su to Sv as soon as this movement leads to higher score of objective function I\u22171 . To facilitate our discussion, the new function value as sample xi is moved from Su to Sv is formulated as following.\nI\u22171 (xi) = (Dv + xi)\n\u2032(Dv + xi)\nnv + 1 + (Du \u2212 xi)\u2032(Du \u2212 xi) nu \u2212 1\n= D\u2032vDv + 2x \u2032 iDv + x \u2032 ixi nv + 1 + D\u2032uDu \u2212 2x\u2032iDu + x\u2032ixi nu \u2212 1 =2x\u2032i Dv\nnv + 1 \u2212 2x\u2032i Du nu \u2212 1 + D\u2032vDv nv + 1 + D\u2032uDu nu \u2212 1 + x\u2032ixi nv + 1 + x\u2032ixi nu \u2212 1\n(14) In each iteration of the clustering, sample xi is randomly selected. The algorithm checks whether moving xi from its\ncurrent cluster to any other cluster will lead to higher I\u22171 (i.e., \u2206I\u22171 > 0). If it is the case, xi is moved to another cluster. The clustering procedure is detailed in Alg. 1.\nAs seen from Step 3 of Alg. 1, the initialization of our method is different from most of the current practice of kmeans, there is no assignment of each sample to its closest initial centroid. On the contrary, each sample xi is assigned with a random cluster label (ranges from 1 to k). This allows to calculate an initial score of I\u22171 and the composite vector D of each cluster. It is possible to do the initial assignment following the way of k-means or k-means++ [12]. However, as will be revealed in Section 5, initialization under either kmeans manner or k-means++ manner improves the clustering quality slightly. However, extra computation is required in such kind of initial assignment.\nDuring each iteration, each sample xi \u2208 X is checked in random order. The optimization in Step 8-10 seeks the movement of xi that leads to highest increase of function score. From the optimization point of view, the algorithm reduces the clustering distortion greedily. From another point of view, the seeking process is comparable to the sample-to-centroid assignment in traditional k-means. They are actually on the same computational complexity level.\nWhereas it is not necessary that we must seek the best movement for xi. As we discover by experiment, it is feasible that moving xi to another cluster as long as we find \u2206I\u22171 (xi) is greater than 0. On one hand, this will speed-up the iteration. On the other hand such kind of scheme usually takes more rounds to reach to the same level of distortion. However, we discover that such kind of less greedy scheme results in lower clustering distortion if the iteration loops for sufficient number of times.\nMoving xi from one cluster to another (Step 9) is very convenient to take. It includes the operation that updates the cluster label of xi and the operation that updates the composite vector for cluster Sv and Su, viz., Dv = Dv + xi, Du = Du \u2212 xi.\nNote that this incremental updating scheme is essentially different from online learning vector quantization (LVQ) [30], in which the cluster centroids are updated incrementally. In the above iteration procedure, no cluster centroids are explicitly produced. As a result, there is no need to update cluster centroid. The clustering iteration is explicitly driven by an objective function rather than driven by the discrepancy between cluster centroids and their cluster members. As revealed later in the experiment, compared to LVQ, Alg 1 is more efficient and leads to considerably lower distortion.\nAlgorithm 1. Direct k-way Boost k-means\n1: Input: matrix Xn\u00d7d 2: Output: S1, \u00b7 \u00b7 \u00b7, Sr, \u00b7 \u00b7 \u00b7Sk 3: Assign xi \u2208 X with a random cluster label; 4: Calculate D1, \u00b7 \u00b7 \u00b7, Dr, \u00b7 \u00b7 \u00b7Dk and I\u22171 ; 5: while not convergence do 6: for each xi \u2208 X (in random order) do 7: Seek Sv that maximizes \u2206I\u22171 (xi); 8: if \u2206I\u22171 (xi) > 0 then 9: Move xi from current cluster to Sv ;\n10: end if 11: end for\n5 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 0\n(a) initialization 0 5 10 15 20 25 30 35 40\n0\n5\n10\n15\n20\n25\n30 1\n(b) iter=1 0 5 10 15 20 25 30 35 40\n0\n5\n10\n15\n20\n25\n30\n(c) iter=10\nFig. 1. Illustration of direct k-way k-means clustering with Alg. 1. The clustering process starts from the state that samples are all assigned with random label. The final cluster centroids in (c) form a convex partition over the 2D space, which are called as Voronoi diagram. According to Lloyd \u2019s condition, all the samples belonging to one cluster fall into the same Voronoi cell.\n12: end while end\nFigure 1 illustrates three iterations of Alg. 1 in 2D case. As shown in the figure, the initial clutsering result is random and messy. Samples belonging to different clusters are totally mixed up. However, only after one round of iteration, the clustering result becomes much more compact. The clustering terminates at the 10th round, where Lloyd\u2019s condition is reached. The optimality of this procedure is analyzed in Appendix A and its convergence is proved in Appendix B.\nOverall, method presented in Alg. 1 is different from traditional k-means in three major aspects. Firstly, no initial assignment is required. Moreover, the egg-chicken loop in the traditional k-means has been replaced by a simpler stochastic optimization procedure. Furthermore, unlike traditional k-means, it is not necessary to seek the best movement for each sample in the iteration.\nThe method presented in Alg. 1 is on the same complexity level as traditional k-means (i.e., O(t\u00b7n\u00b7d\u00b7k)), which is unbearably slow when dealing with large-scale data.\nThe method is revised into a top-down hirarchical clustering version for large-scale clustering. Specifically, at each time, one intermediate cluster is selected and bisected into two smaller clusters by calling Alg. 1. The details of this method are given in Alg. 2.\nAs shown in Alg. 2, priority queue Q pops out one cluster for bisecting each time. As discussed in [29], there are basically two ways to organize the priority queue. One can prioritze the cluster with biggest size or the one with highest average intra-cluster distance to split. Similar as [29], we find splitting the biggest cluster usually demonstrates more stable performance. As a result, the queue is sorted in descending order by the cluster sizes in our practice. Algorithm 2. Bisecting Boost k-means\n1: Input: matrix Xn\u00d7d 2: Output: S1, \u00b7 \u00b7 \u00b7, Sr, \u00b7 \u00b7 \u00b7Sk 3: Treat X as one cluster S1; 4: Push S1 into a priority queue Q; 5: i = 1; 6: while i < k do 7: Pop cluster Si from queue Q 8: Call Alg. 1 to bisect Si into {Si, Si+1};\n9: Push Si, Si+1 into queue Q; 10: i = i + 1; 11: end while\nend\nIt is possible to partition the intermediate cluster into more than two clusters each time. In the following, we are going to show that this bisecting scheme achieves highest scalability among all alternative top-down secting schemes."}, {"heading": "4.2 Scalability Analysis", "text": "In this section, the computation complexity of Alg. 2 is studied by considering the total number of comparisons required in the series of bisecting clustering. The number of iterations in each bisecting is assumed to be a constant by taking the average number of iterations.\nIn order to facilitate the analysis while without loss of generality, we assume that each intermediate cluster in Alg. 2 is partitioned evenly. In addition, we generalize Alg. 2 to an s-secting algorithm. Namely, an intermediate cluster is partitioned to s (s \u2265 2) clusters. Now we consider the size of series of intermediate clusters that are produced when performing sequential secting. Given q is the depth of splitting, it is easy to see d logs ke = q+ 1. The sizes of all intermediate clusters are given as following.\nn, n s , n\ns , ..\ufe38 \ufe37\ufe37 \ufe38\ns\n, n s2 , n\ns2 , ...\ufe38 \ufe37\ufe37 \ufe38\ns2\n, ....., n sq , n\nsq , ...\ufe38 \ufe37\ufe37 \ufe38\nsq\nAs a result, the number of samples to be visited during the clustering procedure is\nn+ n s \u2217 s1 + n s2 \u2217 s2 + n s3 \u2217 s3.....+ n sq \u2217 sq\n=n+ n+ n+ n+ ...+ n\ufe38 \ufe37\ufe37 \ufe38 q\n=n \u2217 (1 + q) \u2248n \u2217 logs k.\n(15)\nConsidering that one sample has to compare with s \u2212 1 centroids each time, the total number of comparisons is\nn \u2217 (s\u2212 1) \u2217 logs k. (16)\nGiven n and k are fixed, Eqn. 16 increases monotonically with respect to s. As a result, the number of comparisons\n6 A B\nC\nA B\n(a) the 1st round bisecting (b) the 2nd round bisecting\nFig. 2. Illutration of two consecutive bisecting in the bisecting clustering where Lloyd \u2019s condition breaks.\nreaches to the minimum (i.e., n log2 k) when s = 2. To this end, it is clear that bisecting is the most efficient secting scheme.\nCompared with Alg. 1, the complexity of Alg. 2 is reduced to O(t\u0304\u00b7n\u00b7d\u00b7log(k)), where t\u0304 is the average number of iterations in each bisecting. Compared with t in traditional k-means, t\u0304 is much smaller given the scale of clustering problem is much smaller in terms of both the size of input data and the number of clusters to be produced. As a result, the complexity of Alg. 1 has been largely reduced since term n\u00b7d has been multiplied by a much smaller factor t\u0304\u00b7log(k).\nAlthough Alg. 2 is efficient, the clustering result produced by Alg. 2 unfortunately does not satisfy with Lloyd\u2019s condition. This problem is illustrated in Figure 2. As one of the clusters is further partitioned into two (from Figure 2(a) to Figure 2(b)), the partition over 2D space is formed by centroids changes. Cluster C claims bordering points from cluster B. However, points from cluster B cannot be reassigned to cluster C if no further intervention is involved. This is actually an underfitting issue and exists for any hierarchical clustering method. Fortunately, this issue can be alleviated by adopting Alg. 1 as a refinement procedure after Alg. 2 outputs k clusters. To do so, extra time is required. It therefore becomes a problem of balancing between efficiency and quality.\nAccording to our observation, it is possible to further speed-up the proposed boost k-means. After a few iterations, both k-means and boost k-means will be trapped in a local minima. Only samples that bordering between different clusters should be shuffled from one cluster to another. As a result, given a sample, it is no need to search for the best movement among k clusters. Instead, sample only needs to compare with top-k0 (k0 k) centroids to search the suitable movement. We find that, this simple modification typically leads to 7\u223c8 times speed-up while without significant performance degradation."}, {"heading": "5 EXPERIMENTS", "text": "In this section, the effectiveness of proposed clustering method, namely boost k-means (BKM) is studied under different scenarios. In the first experiment, dataset SIFT1M [5] is adopted to evaluate the clustering quality. In the second experiment, BKM is tested on the nearest neighbor search task based on product quantizer (PQ) [5] in which this method is adopted for quantizer training. In the third experiment, BKM has been applied to traditional document clustering. Following the practice of [1], [29], 15 document datasets2 have been adopted. In the last experiment, the\n2. Available at http://glaros.dtc.umn.edu/gkhome/fetch/sw/cluto/datasets.tar.gz\nscalability of BKM has been tested on large-scale image clustering task, for which the number of images we use is as large as 10 million.\nIn our study, the performance from traditional k-means is treated as comparison baseline. In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison. For Mini-Batch, our configuration makes sure that the iteration covers 10% of the input data. The configuration is fixed across all the experiments. For RBK, we select the objective function that maximizes the average Cosine similarity between samples within one cluster, which is the special case of ours given the input data is l2normalized. LVQ is similar to k-means except that in each round, a cluster centroid is upated as soon as a sample is assigned. The updating rate starts from 0.01 and decreases at a pace of 4\u00d7 10\u22124 in one iteration.\nAs shown in Table 1, there are variants of k-means depending on cluster initialization and data partitioning strategies (e.g., direct k-way or bisecting). This is also true for the proposed BKM. In the table, \u2018initial assignment\u2019 refers to the operation of assigning each sample to its closest initial centroid. When the initial assignment is based on random seeding like traditional k-means, it is denoted as \u2018rnd\u2019. When it is based on probability distribution seeding as k-means++, it is denoted as \u2018kpp\u2019. Initialization without initial assignment is denoted as \u2018non\u2019. In the experiments, all the variants out of these different configurations on k-means as well as BKM are considered. Performance evaluation is separately conducted for k-way and bisecting clustering method. Noted that BsBKM(rnd) is the same as RBK if the input data is l2-normalized. The experiment in this section is conducted using 1 million SIFT features [31]. The features are clustered into 10,000 partitions and the average distortion error is calculated for performance evaluation.\nIn addition, we also study the performance trend of BKM when Steps 7-10 in Alg. 1 are modified to moving the sample as soon as \u2206I1(xi) > 0. The variants under this modification are denoted as BKM(xxx)+Fast. 3 All the methods considered in the paper are implemented in C++ and the simulations are conducted on a PC with 2.4GHz Xeon CPU and 32G memory setup."}, {"heading": "5.1 Evaluation of Clustering Distortion", "text": "Since k-means and most of its variants share the same objective function (Eqn. 1), it is straightforward to evaluate the clustering performance by checking to what degree the objective is reached. The average distortion (given in Eqn. 17) is adopted for evaluation [2], which takes average over Eqn. 1,\nE = \u2211 q(xi)=r \u2016 Cr \u2212 xi \u20162\nn . (17)\nFor above equation, the lower the distortion value, the better quality of the clustering result is.\nThe first experiment mainly studies the behavior of the proposed BKM under different initializations. The average distortion curves produced by variants direct k-way BKM\n3. Note that this is not applicable for bisecting BKM.\n7\nare given in Figure 3(a) as a function of numbers of iteration. Traditional k-means is treated as baseline for performance comparison. The result shows that clustering distortion of BKM drops faster than traditional k-means. The average distortion from traditional k-means is around 40,450 after 130 iterations. In contrast, BKM without initial assignment (BKM(non)) is able to reach to the same distortion level after only 7 iterations. Moreover, we find that initializing BKM as traditional k-means way (BKM(rnd)) or as k-means++ (BKM(kpp)) allows the iteration to start from a low distortion level. Nevertheless the advantage over BKM(non) fades away after 15 iterations. In comparison to BKM(non), the extra cost of adopting initial assignment in BKM is relatively high.\nThe second experiment studies the performance trend of Alg. 1 in case when Steps 7-10 do not seek the best movement (BKM(xxx)+Fast). As shown in Figure 3(b), the distortion drops slower than BKM(non) which seeks the best movement. However, lower distortion is achievable by BKM(rnd)+Fast when reaching to sufficiently number of iterations (e.g., 20 iterations). This indicates that when the optimization scheme is more greedy, it is likely to get trapped in a worse local optima. This observation applies\nto BKM under different kinds of initialization. Noted that the time cost for BKM(xxx)+Fast is lower than that of BKM that seeks the best movement in each iteration. Whereas, BKM(xxx)+Fast usually needs a few more number of iterations to reach to the similar distortion level. Overall, as investigated in Section 5.4, BKM(xxx)+Fast is %5 faster than BKM(xxx).\nFigure 3(c) studies the trend of average distortion among the proposed BKM (specifically BKM(non)), traditional kmeans, k-means++, Mini-Batch and LVQ. For all the methods presented, their distortion decreases steadily as the iteration continues. A big performance gap is observed between Mini-Batch and other k-means variants. In addition k-means and k-means++ share similar distortion curve. BKM(non) outperforms k-means and k-means++ by requiring only 7 iterations. Most of the methods including k-means and kmeans++ take more than 120 iterations to finally converge. On the other hand, little distortion is observed after 20 iterations, which implies the possibility of terminating the iteration at 20. Although similiar as BKM, LVQ updates the intermediate clusters incrementally, updating cluster centroid directly turns out to be inefficient, which leads to considerably poor performance.\nSince k-means and its variants are all sensitive to initialization, the performance fluctuates from one run to another. The candelstick chart shown in Figure 3(d) further confirms the significance of the improvement achieved by BKM. This chart is plotted with 128 clustering runs (k = 1, 024) on SIFT100K [5] for each method. As shown in the figure, although the performance flucturates for all the methods, the variations are minor. Similar as previous observation, there is no significant difference between traditional k-means and k-means++. In contrast, the performance gap between BKM and traditional k-means is much more significant than the performance variations across different runs.\nTable 2 shows the average distortion of different kmeans variants under bisecting strategy. The result from kmeans (after 130 iterations) is presented for the comparison. As shown from the table, the average distortion from all bisecting methods are on the level of 4.5\u00d7104. Methods built upon Alg. 1 always perform better. The average distortion from all bisecting clustering methods are much higher than that of k-means. They are actually only close to the distortion level of k-means after one iteration. However, the merit of clustering with bisecting strategy is that it is more than 20 times faster than k-means of a single iteration. The relatively poor clustering quality produced by bisecting strategy is mainly due to the issue of underfitting (as discussed in Section 4.2). The clustering results can be further refined by Alg. 1 as shown on the 3rd row of Table 2.\nAs learned from above experiments, on one hand initial assignment under k-means manner or under k-means++ manner is able to improve the performance of BKM slightly.\n8\nOn the other hand, the initial assignment slows down the method considerably. A trade-off has to be made. In the following experiments, only the results from two representative configurations of BKM, namely BKM(non) and BKM(rnd)+Fast are presented. We leave the other possible configurations to the readers."}, {"heading": "5.2 Nearest Neighbor Search by Product Quantizer (PQ)", "text": "In this section, BKM is applied for visual vocabulary training using product quantization [5]. Following the practice of [5], 100K SIFT features are used for product quantizer training, while SIFT1M set [5] is encoded with the trained product quantizers as the reference set for nearest neighbor search (NNS). The obtained recall@top-k is averaged over 1,000 queries for each method. In the experiment, two different settings are tested for product quantizer. Namely, the 128-dimensional SIFT vector is encoded with 8 and 16 product quantizers respectively. For clarity, the evaluations are separately conducted for direct k-way and bisecting kmeans.\nRecall@top-100 for direct k-way are presented in Figure 4(a)-(d) under two different settings (m = 8 and m = 16). As seen from the figures, the performances from k-means, k-means++ and BKM(non) are all very close to each other under different settings. The product quantizer trained with bisecting clustering methods shows only 0.1- 1.3% lower performance than that of direct k-way methods. This basically indicates that product quantizer itself is tolerant to clustering quality. The performance of Mini-Batch and RBK is around 2-6% lower than the other methods. The poor performance of RBK basically indicates the optimization objective function under Cosine similarity is not directly feasible for general l2-space."}, {"heading": "5.3 Document Clustering", "text": "In this section, the performance of proposed method is evaluated under the context of document clustering. Following in [1], 15 document datasets are used for evaluation. The documents has been represented with TF/IDF model and normalized to unit length. Similar to [1], entropy is adopted for the evaluation as following\nEntropy = k\u2211 r=1 nr n 1 log c \u2217 c\u2211 i=1 nir nr \u2217 log nir nr , (18)\nwhere c is the number of classes. Eqn. 18 evaluates to what degree that elements from the same class are put in one cluster. The lower the value, the better the result is. In the experiment, each method performs clustering for 10 runs, and the run with the lowest entropy is presented in Table 3. The presented entropy are averaged over 15 datasets.\nIn general, methods based on BKM perform considerably better. Furthermore, methods with bisecting strategy demonstrate slightly better performance than that of direct k-way in the document clustering task, which shares similar observation as [29]. Overall, BsBKM(non) shows the best performance. While the performance of RBK is close to BsBKM(non). These two methods are quite similar except that no initial assignment is involved in BsBKM(non). This indicates the advantage of no initial assignment in this scenario, which allows clustering to converge to a better local optima.\n9 0 200 400 600 800 1000 1200 1400 1600 1800\n10000 100000 1x106 1x107\nT im\ne co\nst (\nm in\n)\nn\nk-means++ k-means BKM(non) BKM(rnd)+Fast\n(a) direct k-way\n0\n20\n40\n60\n80\n100\n120\n140\n10000 100000 1x106 1x107\nn\nBsKM++ BsKM BsBKM(non) RBK Mini-Batch\n(b) bisecting\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1024 2048 4096 8192\nT im\ne co\nst (\nm in\n)\nk\nk-means++ k-means BKM(non) BKM(rnd)+Fast\n(c) direct k-way\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1024 2048 4096 8192\nk\nBsKM++ BsKM BsBKM(non) RBK Mini-Batch\n(d) bisecting\nFig. 5. Scalability test by varying the scale of input data: (a)-(b) and by varying the number of clusters: (c)-(d)."}, {"heading": "5.4 Scalability Test on Image Clustering", "text": "In this section, the scalability of the proposed k-means is tested on image clustering. The experiment is conducted on 10 million Flickr images (Flickr10M), which are a subset of YFCC100M [32]. Hessian-Affine [33] keypoints are extracted from each image and are described by RootSIFT feature [34]. Finally, the RootSIFT features from each image are pooled by VLAD [35] with a small visual vocabulary of size 64. The resulting 8,192-dimensional feature is further mapped to 512 dimensions by PCA. Following [35], the final VLAD vector is normalized to unit length. In the direct k-way clustering case, we set the number of maximum iterations for all methods to 20. While for the bisecting case, there is no threshold on the number of iterations. The results reported in this section have been averaged over 10 runs for each method.\nIn the first experiment, clustering methods are tested in the way that the scale of input images varies from 10K to 10M. A fixed number of clusters, i.e., 1,024 is used regardless of the size of dataset. The time costs for direct k-way and bisecting methods are presented in Figure 5(a)(b). Accordingly, the average distortion of all the methods are presented in Figure 6(a).\nAs shown in the figures, BKM exhibits slightly faster speed over k-means and its variants across different scales of input data under both direct k-way and bisecting cases. The speed-up becomes more significant as the scale of input data increases. The higher efficiency of these methods is mainly attributed to no requirement of initial assignment. Compared with BKM(non), BKM(rnd)+Fast takes extra time. However, the cost of initial assigment is compensated later for not seeking the best movement. Compared with direct k-way clustering, methods with bisecting strategy achieve much higher scalability. In particular, BsBKM(non) shows the highest scalability. It only takes less than 94 minutes to cluster 10 million vectors (in 512 dimensions) into 1,024 clus-\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n104 105 106 107\nA ve\nra ge\nd is\nto rt\nio n\nn\nk-means++ k-means BKM(non) BKM(rnd)+Fast BsKM++ BsKM BsBKM(non) RBK Mini-Batch\n(a) k=1024, vary n\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n1024 2048 4096 8192\nA ve\nra ge\nd is\nto rt\nio n\nk\nk-means++ k-means BKM(non) BKM(rnd)+Fast BsKM++ BsKM BsBKM(non) RBK Mini-Batch\n(b) n=106, vary k\nFig. 6. Average distortion from all 9 methods under two different scalability testings on Flickr10M (best viewed in color).\nters. The efficiency of Mini-Batch is close to BsBKM(non). However, as shown in Figure 6(a), the clustering quality is poor in most of the cases. Overall, BKM(rnd)+Fast achieves the highest speed efficiency and lowest distortion among all direct k-way clustering methods. While in the bisecting case, BsBKM(non) shows the best performance in terms of both speed efficiency and clustering quality. Similar to the experiments in Section 5.1, the average distortion introduced by bisecting clustering is much higher than direct k-way due to the problem of under-fitting.\nIn addition, the scalability of clustering methods is tested in the way that the number of clusters by varying from 1,024 to 8,192, while the scale of input data is fixed to 1 million. Figure 5(c)-(d) show the time cost of all 9 methods. Accordingly, the average distortion from all these 9 methods are presented in Figure 6(b). As shown in the figures, for all direct k-way clustering methods, the time cost increases linearly as the number of clusters increases. Mini-Batch is no longer efficient as k increases. In contrast, the time cost of all bisecting methods remains steady across different cluster numbers. In terms of clustering quality, as seen from Figure 6(b), in both direct k-way and bisecting cases, clustering driven by the proposed optimization procedure (Alg. 1) performs considerably better. A clear trend is observed from Figure 6(b), methods based on Alg. 1 shows increasingly higher performance than the rest as k grows. Overall, clustering driven by proposed optimization process shows higher speed and better quality. The highest speed is achieved by BsBKM(non), for which only 8 minutes are required to cluster 1 million high dimensional data into 8,192 clusters. Due to extra cost in initial assignment, bisecting with traditional k-means and k-means++ still shows around 35% slower speed than BsBKM(non).\nAs a summary, clustering based on Alg. 1 shows superior performance in terms of both speed efficiency and quality under different scenarios. This is mainly due to the nature of incremental updating scheme, which allows the cluster structures to be fine-tuned in a more efficient way. When the proposed Alg. 1 is performed under bisecting manner (i.e., BsBKM(non)), it shows two orders of magnitude faster than traditional k-means."}, {"heading": "6 CONCLUSION", "text": "We have presented a novel k-means variant. Firstly, a clustering objective function that is feasible for the whole l2-\n10\nspace is developed. Supported by the objective function, the traditional k-means clustering has been modified to simpler form. In this novel k-means variant, we interestingly find that neither the costly initial assignment nor the seeking of closest centroid for each sample in the iteration are necessary. This leads to higher speed and considerably lower clustering distortion. Furthermore when the proposed clustering method is undertaken in the ways of top-down bisecting, it achieves the highest scalability and best quality among all hierarchical k-means variants. Extensive experiments have been conducted in different contexts and on various datasets. Superior performance over most of the kmeans variants is observed across different scenarios."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work is supported by National Natural Science Foundation of China under grants 61572408. The authors would like to express their sincere thanks to Prof. George Karypis from University of Minnesota, USA for his detailed explanation about the implementation of repeated bisecting kmeans."}, {"heading": "APPENDIX A: OPTIMALITY OF INCREMENTAL KMEANS", "text": "As shown in Eqn. 13 and Eqn. 9, two optimal objectives are quite similar. In this section, we show that optimal solution with respect to objective function (Eqn. 9) can be reached with incremental updating scheme presented in Alg. 1. 6.0.0.1 Proof: : For contradiction, let Ao = {S1, S2, \u00b7 \u00b7 \u00b7 , Sk} be an optimal solution and assume that there exists one element d and clusters Si and Sj such that d \u2208 Si. Now consider the clustering solution A\u2217 = {S1, S2, \u00b7 \u00b7 \u00b7 , {Si \u2212 d}, \u00b7 \u00b7 \u00b7 , {Sj + d}, \u00b7 \u00b7 \u00b7 , Sk}. Let Di, Ci, and Dj , Cj be the composite and centroid vectors of cluster Si \u2212 d and Sj , respectively. Let e = I1(Ao)\u2212 I1(A\u2217), then\ne = (Di + d)\n\u2032(Di + d) ni + 1 + D\u2032jDj nj \u2212 ( D\u2032iDi ni + (Dj + d) \u2032(Dj + d) nj + 1 )\n= ( (Di + d)\n\u2032(Di + d) ni + 1 \u2212 D\u2032iDi ni )\u2212 ( (Dj + d) \u2032(Dj + d) nj + 1 \u2212 D\u2032jDj nj )\n= 2nid\n\u2032Di + nid \u2032d\u2212D\u2032iDi\nni(ni + 1) \u2212\n2njd \u2032Dj + njd \u2032d\u2212D\u2032jDj nj(nj + 1)\nLet\u2019s define \u00b5i = D\u2032iDi\nni(ni+1) , \u00b5j = D\u2032jDj nj(nj+1)\nare the average pairwise inner product in cluster Si and Sj respectively. In addition, \u03b4i and \u03b4j are given as the average inner-products between d and elements in Si and Sj respectively, viz \u03b4i = d\u2032Di ni , and \u03b4j = d\u2032Dj nj . Above Equation is rewritten as\ne = ( 2ni\u03b4i\nni + 1 +\nd\u2032d\nni + 1 \u2212\nni\u00b5i\nni + 1 )\u2212 (\n2nj\u03b4j nj + 1 +\nd\u2032d\nnj + 1 \u2212\nnj\u00b5j\nnj + 1 )\n\u2248 (2\u03b4i \u2212 2\u03b4j + d\u2032d\nni + 1 )\u2212 (\u00b5i \u2212 \u00b5j +\nd\u2032d\nnj + 1 )\n(19)\nGiven the fact that (2\u03b4i \u2212 2\u03b4j + d \u2032d\nni+1 ) < (\u00b5i \u2212 \u00b5j + d \u2032d nj+1\n), we have I1(Ao) < I1(A\u2217), which is contradicting."}, {"heading": "APPENDIX B: CONVERGENCE OF INCREMENTAL KMEANS", "text": "Si and Sj are two clusters. d is initially part of Si, and Di is the composite of Si exclude d, Ci is the centroid of Si exclude d, Dj , Cj is the composite and centroid of cluster Sj , the move condition of d from Si to Sj should satisfied\n(Di + d) \u2032(Di + d) ni + 1 + D\u2032jDj nj < D\u2032iDi ni + (Dj + d) \u2032(Dj + d) nj + 1 (20)\nThis equation can be rewritten as:\n(Di + d) \u2032(Di + d) ni + 1 \u2212 D\u2032iDi ni < (Dj + d) \u2032(Dj + d) nj + 1 \u2212 D\u2032jDj nj\nD\u2032iDi + 2d \u2032Di + d 2 ni + 1 \u2212 D\u2032iDi ni < D\u2032jDj + 2d \u2032Dj + d 2 nj + 1 \u2212 D\u2032jDj nj\n2nid \u2032Di + nid 2 \u2212D\u2032iDi ni(ni + 1) < 2njd \u2032Dj + njd 2 \u2212D\u2032jDj nj(nj + 1)\n2 ni\nni + 1\nd\u2032Di\nni \u2212\nD\u2032iDi\nni(ni + 1) +\nd2\nni + 1 < 2\nnj\nnj + 1\nd\u2032Dj\nnj\n\u2212 D\u2032jDj\nnj(nj + 1) +\nd2\nnj + 1\nNow if we assume that both ni and nj are sufficiently large, then nini+1 and nj nj+1\nwill be close to 1. Under these assumptions, we can get\n2 d\u2032Di\nni \u2212\nD\u2032iDi\nni(ni + 1) +\nd2\nni + 1 < 2\nd\u2032Dj\nnj \u2212\nD\u2032jDj\nnj(nj + 1) +\nd2\nnj + 1 .\nNow \u00b5i = D\u2032iDi\nni(ni+1) , \u00b5j = D\u2032jDj nj(nj+1)\nare defined as the average pairwise inner product in cluster Si and Sj respectively. \u03b4i and \u03b4j are given as the average inner-products between d and elements in Si and Sj respectively, viz \u03b4i = d \u2032Di ni , and \u03b4j = d\u2032Dj nj , the following inequation holds.\n2\u03b4i \u2212 2\u03b4j + d\u2032d\nni + 1 < \u00b5i \u2212 \u00b5j +\nd\u2032d\nnj + 1 . (21)"}], "references": [{"title": "Empirical and theoretical comparisons of selected criterion functions for document clustering", "author": ["Y. Zhao", "G. Karypis"], "venue": "Machine Learning, vol. 55, pp. 311\u2013331, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Web-scale image clustering revisited", "author": ["Y. Avrithis", "Y. Kalantidis", "E. Anagnostopoulos", "I.Z. Emiris"], "venue": "ICCV, pp. 1502\u20131510, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Trans. PAMI, vol. 22, pp. 888\u2013905, Aug. 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "ICCV, pp. 1470\u20131477, Oct. 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Trans. PAMI, vol. 33, pp. 117\u2013128, Jan. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D.G. Lowe"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, pp. 2227\u20132240, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Additive quantization for extreme vector compression", "author": ["A. Babenko", "V. Lempitsky"], "venue": "CVPR, pp. 931\u2013938, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory, vol. 28, pp. 129\u2013137, 1982.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1982}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "P.S. Yu", "Z.-H. Zhou", "M. Steinbach", "D.J. Hand", "D. Steinberg"], "venue": "Knowledge and Information System, vol. 14, pp. 1\u201337, Dec. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "NIPS, pp. 10\u201318, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Discrete and Computational Geometry, vol. 45, no. 4, pp. 596\u2013616, 2011.  11", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM- SIAM Symposium on Discrete Algorithms, pp. 1027\u20131035, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast and accurate k-means for large datasets", "author": ["M. Shindler", "A. Wong", "A.W. Meyerson"], "venue": "NIPS, pp. 2375\u20132383, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Neanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Trans. PAMI, vol. 24, pp. 881\u2013892, Jul. 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Using the triangle inequality to accelerate", "author": ["C. Elkan"], "venue": "ICML, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "In Proceedings of the 19th international conference on World wide web, pp. 1177\u20131178, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 7, pp. 622\u2013633, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable k-means by ranked retrieval", "author": ["A. Broder", "L. Garcia-Pueyo", "V. Josifovski", "S. Vassilvitskii", "S. Venkatesan"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pp. 233\u2013242, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Web scale photo hash clustering on a single machine", "author": ["Y. Gong", "M. Pawlowski", "F. Yang", "L. Brandy", "L. Boundev", "R. Fergus"], "venue": "CVPR, pp. 19\u201327, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergence properties of the kmeans algorithm", "author": ["L. Bottou", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pp. 585\u2013592, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H. peter Kriegel", "J. Sander", "X. Xu"], "venue": "In Proceedings of Knowledge Discovery and Data Mining, pp. 226\u2013231, 1996.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Mean shift: A robust approach toward feature space analysis", "author": ["D. Comaniciu", "P. Mee"], "venue": "Trans. PAMI, vol. 24, pp. 603\u2013619, May 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computer Survey, vol. 31, pp. 264\u2013323, Sep. 1999.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of ACM, vol. 18, pp. 509\u2013 517, Sep. 1975.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1975}, {"title": "Accelerating exact k-means algorithms with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 277\u2013281, ACM, 1999.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast and exact out-of-core k-means clustering", "author": ["A. Goswami", "R. Jin", "G. Agrawal"], "venue": "Proceedings of the Fourth IEEE International Conference on Data Mining, pp. 83\u201390, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Hierarchical clustering algorithms for document datasets", "author": ["Y. Zhao", "G. Karypis"], "venue": "Data Mining and Knowledge Discovery, vol. 10, no. 2, pp. 141\u2013168, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "eds., Self-Organizing Maps", "author": ["T. Kohonen", "M.R. Schroeder", "T.S. Huang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV, vol. 60, pp. 91\u2013110, Nov. 2004.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "YFCC100M: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, vol. 59, no. 2, pp. 64\u201373, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Scale and affine invariant interest point detectors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IJCV, vol. 60, pp. 63\u201386, Oct. 2004.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Three things everyone should know to improve object retrieval", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "CVPR, pp. 2911\u20132918, Jun. 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Aggregating local descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "Trans. PAMI, vol. 34, pp. 1704\u20131716, Sep. 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 242, "endOffset": 245}, {"referenceID": 6, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 247, "endOffset": 250}, {"referenceID": 7, "context": "Among these algorithms, kmeans [8] remains a popular choice for its simplicity, efficiency and moderate but stable performance across different problems.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "It was known as one of top ten most popular algorithms in data mining [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "In some specific scenarios, the running time of k-means could be even exponential in the worst case [10], [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "In some specific scenarios, the running time of k-means could be even exponential in the worst case [10], [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 1, "context": "K-means has been also tailored to perform web-scale image clustering [2], [19].", "startOffset": 69, "endOffset": 72}, {"referenceID": 18, "context": "K-means has been also tailored to perform web-scale image clustering [2], [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "This is known as Lloyd iteration procedure [8].", "startOffset": 43, "endOffset": 46}, {"referenceID": 19, "context": "In general kmeans only converges to local minimum [20].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "Compared with other well-known clustering algorithms such as DBSCAN [21] and Mean shift [22], this complexity is considerably low.", "startOffset": 68, "endOffset": 72}, {"referenceID": 21, "context": "Compared with other well-known clustering algorithms such as DBSCAN [21] and Mean shift [22], this complexity is considerably low.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "The existing efforts [16], [18] in enhancing the scalability of k-means for web-scale tasks often come with price of lower clustering quality.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "The existing efforts [16], [18] in enhancing the scalability of k-means for web-scale tasks often come with price of lower clustering quality.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "On the other hand, k-means++ proposed in [12], [17] focuses on enhancing the clustering quality by a careful design of the initialization procedure.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "On the other hand, k-means++ proposed in [12], [17] focuses on enhancing the clustering quality by a careful design of the initialization procedure.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Inspired by the work in [1], a novel objective function is derived from Eqn.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "In addition, driven by the objective function, sample is moved from one cluster to another cluster when we find this movement leads to higher objective function score, which is known as incremental clustering [1], [23].", "startOffset": 209, "endOffset": 212}, {"referenceID": 0, "context": "Extensive experiments are conducted to contrast the performance of proposed method with k-means and its variants including tasks document clustering [1], nearest neighbor search (NNS) with product quantization [4] and image clustering.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "Extensive experiments are conducted to contrast the performance of proposed method with k-means and its variants including tasks document clustering [1], nearest neighbor search (NNS) with product quantization [4] and image clustering.", "startOffset": 210, "endOffset": 213}, {"referenceID": 22, "context": "Due to its versatility in different contexts, it has been studied in the last three decades [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "[12], [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[12], [17].", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "According to [12], k-means iteration also converges faster due to the careful selection on the initial cluster centroids.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "Although the number of scanning rounds has been reduced to a few in [17], the extra computational cost is still inevitable.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "[14] proposed to index dataset in a KD Tree [25] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[14] proposed to index dataset in a KD Tree [25] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A recent work [18] takes similar way to speed-up the nearest neighbor search by indexing dataset with inverted file structure.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Namely, methods in [16], [27] only pick a small portion of the whole dataset to update the cluster centroids each time.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "Namely, methods in [16], [27] only pick a small portion of the whole dataset to update the cluster centroids each time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Apart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29].", "startOffset": 182, "endOffset": 185}, {"referenceID": 26, "context": "Apart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29].", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "Apart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23].", "startOffset": 51, "endOffset": 54}, {"referenceID": 27, "context": "In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23].", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "Moreover, the update process is explicitly driven by an objective function (called as criterion function in [1], [29]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 27, "context": "Moreover, the update process is explicitly driven by an objective function (called as criterion function in [1], [29]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length.", "startOffset": 47, "endOffset": 50}, {"referenceID": 26, "context": "Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length.", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "In other word, objective function proposed in [1] is the special case of our proposed form.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "In this section, two objective functions (also known as criterion functions [1]) are developed.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "According to [1], objective functions are categorized into two groups.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "9 is in the same form as the first objective function in [1], they are derived from different initial objectives.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "Noticed that similar optimization objectives have been discussed under Cosine similarity measure in [1].", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "Both clustering strategies are built upon incremental clustering [1], [23] and driven by objective function I\u2217 1 (Eqn.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "It is possible to do the initial assignment following the way of k-means or k-means++ [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "Note that this incremental updating scheme is essentially different from online learning vector quantization (LVQ) [30], in which the cluster centroids are updated incrementally.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "As discussed in [29], there are basically two ways to organize the priority queue.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "Similar as [29], we find splitting the biggest cluster usually demonstrates more stable performance.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "In the first experiment, dataset SIFT1M [5] is adopted to evaluate the clustering quality.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "In the second experiment, BKM is tested on the nearest neighbor search task based on product quantizer (PQ) [5] in which this method is adopted for quantizer training.", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Following the practice of [1], [29], 15 document datasets2 have been adopted.", "startOffset": 26, "endOffset": 29}, {"referenceID": 27, "context": "Following the practice of [1], [29], 15 document datasets2 have been adopted.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "The experiment in this section is conducted using 1 million SIFT features [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "17) is adopted for evaluation [2], which takes average over Eqn.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "k-means boost k-means Initial assigment k-way bisecting k-way bisecting Random k-means [8] BsKM BKM(rnd) BsBKM(rnd) Probability based [12] k-means++ [12] BsBKM++ BKM(kpp) BsBKM(kpp) None - BKM(non) BsBKM(non)", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "k-means boost k-means Initial assigment k-way bisecting k-way bisecting Random k-means [8] BsKM BKM(rnd) BsBKM(rnd) Probability based [12] k-means++ [12] BsBKM++ BKM(kpp) BsBKM(kpp) None - BKM(non) BsBKM(non)", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "k-means boost k-means Initial assigment k-way bisecting k-way bisecting Random k-means [8] BsKM BKM(rnd) BsBKM(rnd) Probability based [12] k-means++ [12] BsBKM++ BKM(kpp) BsBKM(kpp) None - BKM(non) BsBKM(non)", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "This chart is plotted with 128 clustering runs (k = 1, 024) on SIFT100K [5] for each method.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "In this section, BKM is applied for visual vocabulary training using product quantization [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "Following the practice of [5], 100K SIFT features are used for product quantizer training, while SIFT1M set [5] is encoded with the trained product quantizers as the reference set for nearest neighbor search (NNS).", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "Following the practice of [5], 100K SIFT features are used for product quantizer training, while SIFT1M set [5] is encoded with the trained product quantizers as the reference set for nearest neighbor search (NNS).", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Following in [1], 15 document datasets are used for evaluation.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Similar to [1], entropy is adopted for the evaluation as following", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "The assymetric distance calculation (ADC) [5] is adopted for nearest neighbor search.", "startOffset": 42, "endOffset": 45}, {"referenceID": 27, "context": "Furthermore, methods with bisecting strategy demonstrate slightly better performance than that of direct k-way in the document clustering task, which shares similar observation as [29].", "startOffset": 180, "endOffset": 184}, {"referenceID": 30, "context": "The experiment is conducted on 10 million Flickr images (Flickr10M), which are a subset of YFCC100M [32].", "startOffset": 100, "endOffset": 104}, {"referenceID": 31, "context": "Hessian-Affine [33] keypoints are extracted from each image and are described by RootSIFT feature [34].", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "Hessian-Affine [33] keypoints are extracted from each image and are described by RootSIFT feature [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "Finally, the RootSIFT features from each image are pooled by VLAD [35] with a small visual vocabulary of size 64.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "Following [35], the final VLAD vector is normalized to unit length.", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. The performance of k-means has been enhanced from different perspectives over the years. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is driven by an explicit objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. The procedure of k-means becomes simpler and converges to a considerably better local optima. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.", "creator": "LaTeX with hyperref package"}}}