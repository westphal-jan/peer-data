{"id": "1610.02906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "A General Framework for Content-enhanced Network Representation Learning", "abstract": "this particular paper investigates the problem of network cluster embedding, which firstly aims at learning low - dimensional vector representation of nodes in networks. most existing network embedding methods rely solely on the network structure, i. e., the linkage relationships between nodes, but ignore the rich content key information constraints associated with it, namely which is common in real situation world networks and beneficial to everyone describing the identifying characteristics of a node. in this paper, we could propose content - enhanced network embedding ( cene ), which is capable of jointly theoretically leveraging the internal network structure and the content information. our approach integrates text modeling and grid structure modeling in a general logical framework by treating such the content information as a special kind than of node. experiments on several real world net - works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information representations and joint learning.", "histories": [["v1", "Mon, 10 Oct 2016 13:27:01 GMT  (714kb,D)", "http://arxiv.org/abs/1610.02906v1", null], ["v2", "Tue, 11 Oct 2016 02:06:55 GMT  (513kb,D)", "http://arxiv.org/abs/1610.02906v2", null], ["v3", "Mon, 17 Oct 2016 13:55:02 GMT  (513kb,D)", "http://arxiv.org/abs/1610.02906v3", null]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.LG", "authors": ["xiaofei sun", "jiang guo", "xiao ding", "ting liu"], "accepted": false, "id": "1610.02906"}, "pdf": {"name": "1610.02906.pdf", "metadata": {"source": "CRF", "title": "A General Framework for Content-enhanced Network Representation Learning", "authors": ["Xiaofei Sun", "Jiang Guo", "Xiao Ding", "Ting Liu"], "emails": ["tliu}@ir.hit.edu.cn"], "sections": [{"heading": "Introduction", "text": "Network embedding, which aims at learning lowdimensional vector representations of a network, has attracted increasing interest in recent years. It has been shown highly effective in many important tasks in network analysis involving predictions over nodes and edges, such as node classification (Tsoumakas and Katakis 2006; Sen et al. 2008), recommendation (Tu, Liu, and Sun 2014; Yu et al. 2014) and link prediction (Liben-Nowell and Kleinberg 2007).\nVarious approaches have been proposed toward this goal, typically including Deepwalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015), GraRep (Cao, Lu, and Xu 2015), and node2vec (Grover and Leskovec 2016). These models have been proven effective in several real world networks. Most of the previous approaches utilize information only from the network structure, i.e., the linkage relationships between nodes, while paying scant attention to the content of each node, which is common in real-world networks. In a typical social network with users as vertices, the user-generated contents (e.g., texts, images) will serve as rich extra information which should be important for node representation and beneficial to downstream applications.\nFigure 1 shows an example network from Quora, a community question answering website. Users in Quora can follow each other, creating directed connections in the network.\nMore importantly, users are expected to ask or answer questions, which can be treated as users\u2019 contents. These contents are critical for identifying the characteristics of users, and thus will significantly benefit tasks like node classification (e.g. gender, location and profession). For example, we can infer from the contents of user A and user C (Figure 1) that they are likely to be female users (gender). Besides, user B is supposed to be a programmer (profession) and user D probably lives in New York (location).\nTo cope with this challenge, Yang et al. (2015) presented text-associated DeepWalk (TADW), which incorporates textual features into network embeddings through matrix factorization. This approach typically suffers from high computational cost and not scalable to large-scale networks. Besides, contents in TADW are simply incorporated as unordered text features instead of being explicitly modeled. Therefore, deeper semantics contained in the contents cannot be well captured.\nPresent work. We present a general framework for learning Content-Enhanced Network Embedding (CENE) that is capable of jointly leveraging the network structure and the contents. We consider textual contents in this study, however, our approach can be flexibly scaled to other modalities of content. Specifically, each piece of content information (e.g., a tweet one posts in twitter, a question one follows in Quora) is formalized as a document, and we integrate each document into the network by creating a special\nar X\niv :1\n61 0.\n02 90\n6v 1\n[ cs\n.S I]\n1 0\nO ct\n2 01\n6\nkind of node, whose representation will be computed compositionally from words. The resulting augmented network will consist of two kinds of links: the node-node link and the node-content link. By optimizing the joint objective, the knowledge contained in the contents will be effectively distilled into node embeddings.\nTo summarize, we make the following contributions:\n\u2022 We propose a novel network embedding model that captures both textual contents and network structure. Experiments on the tasks of node classification using two real world datasets demonstrate its superiority over various baseline methods.\n\u2022 We collect a network dataset which contains node attributes and rich textual contents. It will be made publicly available for research purpose."}, {"heading": "Related Work", "text": ""}, {"heading": "Text Embedding", "text": "In order to obtain text embeddings (e.g., sentence, paragraph), a simple and intuitive approach would be averaging the embeddings of each word in the text (Mitchell and Lapata 2010; Ferrone and Zanzotto 2013; Iyyer et al. 2015). More sophisticated models have been designed to utilize the internal structure of sentences or documents to assist the composition. For example, Socher et al. (2013) and Socher et al. (2014) use recursive neural networks over parse trees to obtain sentence representations. To alleviate the dependency on syntatic parsing, convolutional neural networks (CNN) (Blunsom, Grefenstette, and Kalchbrenner 2014; Johnson and Zhang 2015) are employed which use simple bottom-up hierarchical structures for composition. Another alternative model is the LSTM-based recurrent neural network (RNN) (Kiros et al. 2015), which is a variant of RNN that uses long short-term memory cells for capturing longterm dependencies."}, {"heading": "Network Embedding", "text": "Hoff, Raftery, and Handcock (2002) first propose to learn latent space representation of vertices in a network. Some earlier works focus on the feature vectors and the leading eigenvectors are regarded as the network representations, e.g., MDS (Borg and Groenen 2005), IsoMap (Tenenbaum, De Silva, and Langford 2000), LLE (Roweis and Saul 2000), and Laplacian Eigenmaps (Belkin and Niyogi 2001).\nRecent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global\nstructural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure.\nYang et al. (2015) present the first work that combines structure and content information for learning network embeddings. They show that DeepWalk is equivalent to matrix factorization (MF) and text features of vertices can be incorporated via factorizing a text-associated matrix. This method, however, suffers from the high computation cost of MF and has difficulties scaling to large-scale networks. Pan et al. (2016) instead combines DeepWalk with Doc2Vec (Le and Mikolov 2014), along with partial labels of nodes that constitutes a semi-supervised model. However, Doc2Vec is far from being expressive of the contents. Besides, it cannot generalize to other modalities of contents like images."}, {"heading": "Problem Definition", "text": "Definition 1. (Network) Let G = (V,E,C) denote a network, where V is the set of vertices, representing the nodes of the network; E \u2286 (V \u00d7 V ) is the set of edges, representing the relations between the nodes; and C denotes the contents of nodes. C = {(v, doc)|v \u2208 V ; doc = {Si}}, where Si denotes i-th sentence of doc and is composed of word sequence < w1, w2, . . . , wn >. Without loss of generality, we assume the structure of network to be a directed graph.1 Definition 2. (Network Embedding) Given a network denoted as G = (V,E,C), the aim of network embedding is to allocate a low dimensional real-valued vector representation ev \u2208 Rd for each vertex v \u2208 V , where d |V |. Let \u03b8 = (e1, e2, . . . , e|V |) denotes the embedded vectors in the latent space. \u03b8 is supposed to maintain as much topological information of the original network as possible.\nAs ev can be regarded as a feature vector of vertex v, it is straightforward to use it as the input of subsequent tasks like node classification. Another notable trait is that this kind of embedding is not task-specific so that it can be applied to different kinds of tasks without retraining."}, {"heading": "Method", "text": ""}, {"heading": "General Framework", "text": "To maintain the structural information of a network, we describe a general framework that minimizes the following objective:\nLg = \u2211\n(u,v)\u2208SP\nlog p(u, v; \u03b8) + \u2211\n(u,v)\u2208SN\nlog (1\u2212 p(u, v; \u03b8))\n(1) where SP is the set of positive vertex pairs and SN is negative pair set. For example, in Deepwalk based algorithms\n1Undirected networks can be readily converted to directed ones by replacing each undirected edge with two oppositely directed edges.\n(Deepwalk, walklet, node2vec), SP is the set of adjacent vertex pairs in the routes generated through random walking, and SN is the union of all negative sampling sets. p(u, v; \u03b8) is the joint probability between vertex u and v, which means the probability of pair (u, v) existing in SP and correspondingly 1 \u2212 p(u, v; \u03b8) is the probability that (u, v) does not exist.\nTo further utilize the content information, a simple way is to concatenate the content embedding with the node embedding, both of which are trained independently. Formally, let eu = eu \u2295 fe(Cu) be the representation of node u, where Cu = {(u, c)|(u, c) \u2208 C} is the set of all contents of node u. This method, however, requires each node in the network to be associated with some contents, which is too rigid for real world networks.\nIn this paper, we introduce contents (documents) as a special kind of nodes, and then the augmented network can be represented as: Gaug = (Vn, Vc, Enn, Enc), where Vn = V is vertex set; Vc = {c|(u, c) \u2208 C, u \u2208 V } is the content set; Enn = E is the set of edges between vertices; and Enc = Vn\u00d7Vc is the set of edges between vertices and contents. In this way, different nodes can also interact through connection with a same content node (e.g., two Twitter users retweet the same post), which significantly alleviates the structural sparsity in Enn. The resulting framework structure is illustrated in Figure 2.\nNext, we will describe the loss functions involving nodenode links and node-content links respectively, following the notation in Eq.1."}, {"heading": "Node-Node Link", "text": "For node-node links, we specify SP as Enn. Inspired by the idea of negative sampling (Mikolov et al. 2013b), we sample a set SNunn = {v\u2032|(u, v\u2032) /\u2208 Enn} for each edge (u, v). Then SN = \u222a\nu\u2208Vnn SNunn.\nLnn = \u2211\n(u,v)\u2208Enn\n[log p(v, u; \u03b8)\u2212 \u2211\nv\u2032\u2208SNunn\nlog p(v\u2032, u; \u03b8)]\n(2) Here p(v, u) (we omit \u03b8 for simplicity) is computed using\na logistic function:\np(v, u) = \u03c3(eu \u00b7 ev) = 1\n1 + exp(\u2212eu \u00b7 ev) (3)\nHowever, Eq.3 is a symmetrical operation, which means p(v, u) = p(u, v), and this is not suitable for directed networks. So we splited \u2200eu = (einu , eoutu ) where einu \u2208 Rd/2 and eoutu \u2208 Rd/2. Then p(v, u) can be computed as:\np(v, u) = \u03c3(einv \u00b7 eoutu ) = 1\n1 + exp(\u2212einv \u00b7 eoutu ) (4)"}, {"heading": "Node-Content Link", "text": "The node-content loss is similar to Eq.2. Let SNunc = {c\u2032|(u, c\u2032) /\u2208 Enc} denote the negative sampling set for edge (u, c), then the loss can be written as:\nLnc = \u2211\n(u,c)\u2208Enc\n[log p(c, u; \u03b8)\u2212 \u2211\nc\u2032\u2208SNunc\nlog p(c, u; \u03b8)]\n(5) where\np(c, u) = \u03c3(eu, fe(c)) = \u03c3(e out u \u2295 einu , fe(c)) (6)\nInstead of allocating an arbitrary embedding for each document c, here, we use a composition function fe(\u00b7) to compute the content representation in order to fully capture the semantics of texts. In this paper, we further decompose each document into sentences, and model node-sentence link separately (Figure 2). We investigate three typical composition models for learning sentence representations (Figure 3).\nWord Embedding Average (WAvg). This approach simply takes the average of word vectors as the sentence embedding. Despite its obliviousness to word order, it has proved surprisingly effective in text categorization tasks (Joulin et al. 2016).\nfe(c) = 1 |c| \u2211 i ewi (7)\nRecurrent Neural Network (RNN). Here we use the gated recurrent unit (GRU) proposed by Cho et al. (2014). GRU is a simplified version of the LSTM unit proposed earlier (Hochreiter and Schmidhuber 1997), with fewer parameters while still preserving the ability of capturing long-term dependencies. Instead of simply using the hidden representation at the final state as the sentence representation, we apply mean pooling over all history hidden states:\n\u2212\u2192 hi = GRU(ewi , hi\u22121); fe(c) = 1 |c| \u2211 i \u2212\u2192 hi (8)\nBidirectional Recurrent Neural Network (BiRNN). In practice, even with GRU, RNN still cannot capture very\nlong-term dependencies well. Hence, we further adopt a bidirectional variant (Schuster and Paliwal 1997) that processes a sentence in both directions with two separate hidden layers. The hidden state vectors from two directions\u2019 GRU units at each position are then concatenated, and finally passed through a mean pooling layer. \u2190\u2212 hi = GRU(ewi , hi+1); fe(c) = 1\n|c| \u2211 i ( \u2212\u2192 hi \u2295 \u2190\u2212 hi) (9)"}, {"heading": "Joint Learning", "text": "Finally, we optimize the following joint objective function, which is a weighted combination of the node-node loss (Eq.2) and the node-content loss (Eq.5):\nL = \u03b1 \u2217 Lnn + (1\u2212 \u03b1) \u2217 Lnc (10) where \u03b1 \u2208 [0, 1] is a parameter to balance the importances of the two objectives. With the \u03b1 increasing, more structure information (node-node link) will be taken into consideration. All parameters, including \u03b8 and parameters in fe(\u00b7) are jointly optimized.\nWe use stochastic gradient descent (SGD) with learning rate decay for optimization. The gradients are computed with back-propagation. In our implementation, we approximate the effect of \u03b1 through instance sampling (node-node and node-content) in each training epoch. More details are shown in Algorithm 1.\nAlgorithm 1: Joint Training. 1 Input: Vn, Vc, Enn, Enc, balance weight \u03b1, content\nembedding method fe(\u00b7), \u03b7, MaxStep 2 for step\u2190 0 to MaxStep do 3 Random generate x \u223c N(0, 1); 4 if x < \u03b1 then 5 Get negative sampling set SNunn; 6 random select (u, v) from Enn; 7 Lookup embedding of u and v from \u03b8; 8 Lookup embedding of v\u2032 \u2208 SNunn from \u03b8; 9 else\n10 Get negative sampling set SNunc; 11 Lookup embedding of u from \u03b8; 12 Calculate embedding of c and \u2200c\u2032 \u2208 SNunc with fe; 13 end 14 Perform SGD on the corresponding loss;\n\u03b7 \u2190 \u03b7 1\u2212 step\nMaxStep\n;"}, {"heading": "15 end", "text": ""}, {"heading": "Experiments Setup", "text": ""}, {"heading": "Dataset", "text": "We conduct experiments on two real world datasets: DBLP (Tang et al. 2008) and Zhihu. An overview of these networks is given in Table 1.\nDBLP We use the DBLP2 dataset to construct the citation network. Two popular conferences: SIGIR and KDD, are chosen as the two categories for node classification.3 Here\n2cn.aminer.org/citation (V1 is used) 3Note that SIGIR mainly focuses on information retrieval while\nKDD is more related to data mining.\neach paper is regarded as a node, and every directed edge between two nodes indicate a citation. We use the abstract of each paper as the contents. Note that only 16.7% nodes on DBLP have contents and we keep all nodes of DBLP for experiments.\nZhihu Zhihu4 is a Chinese community social-network based Q&A site, which aims at building a knowledge repository of questions and answers created and organized by users. We first collected the users\u2019 following lists, following questions list and their profiles. Then, we construct the Zhihu network with users as vertices, and edges indicating the following relationships. The question titles that each user follows are used as their associated contents.\nWe select the top three frequent attributes for our experiments: gender, location and profession. Three cities: Beijing, Shanghai and Guangzhou of China are chosen as location categories, and the four most popular professions:financial industry, legal profession, architect and clinical treatment are chosen as profession categories."}, {"heading": "Baseline", "text": "We consider the following network embedding methods for experimental comparison:"}, {"heading": "Structure-Based Method", "text": "\u2022 DeepWalk (DW) (Perozzi, Al-Rfou, and Skiena 2014). DeepWalk learns vertex embeddings by using the skipgram model over vertex sequences generated through random walking on the network.\n\u2022 LINE (Tang et al. 2015). LINE takes both 1-order and 2-order proximity into account and the concatenation of these two representations is used as the final embedding.\n\u2022 Word2vec (W2V). We include an additional baseline that uses Word2vec (Mikolov et al. 2013a) to directly learn vertex embeddings from node-node links. Specifically, we treat each vertex u as the word and all its neighbors as its context. Here we use the word2vecf toolkit.5"}, {"heading": "Content-Based Method", "text": "\u2022 Doc2vec (D2V) (Le and Mikolov 2014). Doc2vec is an extension of word2vec that learns document representation by predicting the surrounding words in contexts sam-\n4www.zhihu.com 5bitbucket.org/yoavgo/word2vecf\npled from the document. Here we use the Gensim implementation6.\n\u2022 Word Average (WAvg). Similar to the WAvg setting in our model (CENE), we are also interested to see how well word average performs when trained separately."}, {"heading": "Combined Method", "text": "\u2022 Naive Combination (NC). We concatenate the two best-\nperforming network embeddings learned using structurebased methods and content-based methods respectively.\n\u2022 TADW (Yang et al. 2015). TADW integrates content information into network embeddings by factorizing a textassociated matrix."}, {"heading": "Evaluation", "text": "We evaluate our network embeddings on the node classification task. Following the metric used in previous studies (Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015), we randomly sample a portion (TR, from 10% to 90%) of the labeled vertices as training data, with the rest of the vertices for testing. We use the scikit-learn (Pedregosa et al. 2011) to train logistic regression classifiers. For each TR, the experiments are executed independently for 40 times and we report the averaged Micro-F1 measures."}, {"heading": "Training Protocols", "text": "The initial learning rate is set to \u03b70 = 0.025 for CENEWAvg and \u03b70 = 0.01 for CENERNN and CENEBiRNN. The dimension of the embeddings for both nodes and contents is set to 200. Word embeddings are pretrained using the whole set of contents associated with the network, with dimension of 200. In addition, the negative sampling size SNunn is 15 for all methods, and SNunc is 25 for CENE; the total number of samples T is 10 billion for LINE (1st) and LINE (2nd) as shown in Tang et al. (2015); window size win = 5, walk length t = 40 and number of walks per vertex \u03b3 = 50 for DeepWalk."}, {"heading": "Results and Analysis", "text": ""}, {"heading": "Classification tasks", "text": "The classification results are shown in Table 2 (DBLP), Table 3 (Zhihu-Gender), Table 4 (Zhihu-Location) and Table\n6radimrehurek.com/gensim/models/doc2vec. html\n5 (Zhihu-Profession). The proposed CENE consistently and significantly outperforms both structure-based and contentbased methods on all different datasets and most training rations, demonstrating the efficacy of our approach.\nBesides, we have the following interesting observations:\n1. For most tasks, simple concatenation of structure-based methods and content-based methods yeilds improvements, showing the importance of both network structure and contents.\n2. Despite the simplicity, CENEWAvg obtains promising results in general, outperforming most of the baseline methods by a significant margin. Furthermore, CENERNN and CENEBiRNN perform better than WAvg in most cases.\n3. BiRNN works better than RNN in DBLP, while RNN is better in Zhihu. The main factor here is the average sentence length in DBLP (25) and Zhihu (11). As discussed earlier (the Introduction part), BiRNN is more powerful for longer sentences.\n4. Content-based methods work generally better than structure-based methods on Zhihu, but worse on DBLP. This observation implies that structural relationships are more indicative than contents in DBLP, that is, papers tend to cite papers within the same area. Zhihu, however, is an interest-driven network, and thus contents are more important for node representation.\n5. TADW performs poorly on Zhihu. This is mainly because TADW is originally designed for networks where each node has only one document. However, nodes on Zhihu networks may follow multiple questions and the contents are relatively independent.\nWe further conduct experiments on another DBLP 7 dataset used in TriDNR (Pan et al. 2016) to directly compare with it. We examine both the original semi-supervised version of TriDNR and an unsupervised version, in which the label-node relationship is discarded. Table 6 shows that CENERNN and CENEBiRNN even outperform the semisupervised TriDNR, which is really promising.\nConventional structure-based methods perform poorly on small-degree nodes (e.g., a Zhihu user may neither follow nor be followed). However, the introduction of content nodes would greatly alleviate the structural sparsity. Figure 4a shows the classification performance of CENE over\n7github.com/shiruipan/TriDNR\nnodes with different degrees on Zhihu-Gender, compared with DeepWalk. Figure 4b shows the curve of the absolute differences. We can see CENE has a significantly larger impact on small-degree nodes, which verifies our hypothesis."}, {"heading": "Parameter Sensitivity", "text": "CENE has two hyperparameters: iteration number k and balance weight \u03b1. We fix the training portion to 50% and test the classification F1 score with different k and \u03b1.\nFigure 5 shows F1 scores with TR ranging from 10%, 50% to 90%, on four different tasks. For all tasks, all of the three curves converge stably when k approximates 100.\nFigure 6 shows the effect of \u03b1. Note that if \u03b1 = 0, only content information will be used, and when \u03b1 = 1, our model will be degenerated into a structure-based one (W2V). With \u03b1 increasing, the performance of CENE in-\ncreases at first but decreases when \u03b1 is big enough. There is an abrupt decrease when \u03b1 grows from 0.9 to 1.0, indicating the importance of content information. Another notable phenomenon is that for the location attribute on Zhihu, performance keeps dropping as \u03b1 increases. This observation makes sense since one of the critical advantage of social networks is to break up the regional limitation, so the network structure provides little hint or even noise for identifying users\u2019 real locations."}, {"heading": "Conclusion", "text": "In this paper, we present CENE, a novel network embdding method which leverages both structure and textual content information in a network by regarding contents as a special kind of nodes. Experiments on the task of node classification with two real world datasets demonstrate the effectiveness of our model. Three content embedding methods are investigated, and we show that deeper models (RNN and BiRNN) are more competent for text modeling. For future work, we will extend our methods to networks with more diverse contents such as images."}, {"heading": "Acknowledgments", "text": "This work was supported by the National Basic Research Program (973 Program) of China via Grant 2014CB340503, the National Natural Science Foundation of China (NSFC) via Grant 61472107 and 61133012."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Belkin", "M. Niyogi 2001] Belkin", "P. Niyogi"], "venue": "In NIPS,", "citeRegEx": "Belkin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2001}, {"title": "Modern multidimensional scaling: Theory and applications", "author": ["Borg", "I. Groenen 2005] Borg", "P.J. Groenen"], "venue": null, "citeRegEx": "Borg et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Borg et al\\.", "year": 2005}, {"title": "Grarep: Learning graph representations with global structural information", "author": ["Lu Cao", "S. Xu 2015] Cao", "W. Lu", "Q. Xu"], "venue": "In Proc. of CIKM,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Linear compositional distributional semantics and structural kernels", "author": ["Ferrone", "L. Zanzotto 2013] Ferrone", "F.M. Zanzotto"], "venue": "In Joint Symposium on Semantic Processing.,", "citeRegEx": "Ferrone et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ferrone et al\\.", "year": 2013}, {"title": "Node2vec: Scalable feature learning for networks", "author": ["Grover", "A. Leskovec 2016] Grover", "J. Leskovec"], "venue": "In ACM SIGKDD,", "citeRegEx": "Grover et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grover et al\\.", "year": 2016}, {"title": "Long short-term memory. Neural computation 9(8):1735\u20131780", "author": ["Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Latent space approaches to social network analysis", "author": ["Raftery Hoff", "P.D. Handcock 2002] Hoff", "A.E. Raftery", "M.S. Handcock"], "venue": "Journal of the american Statistical association", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer"], "venue": "In Proc. of ACL", "citeRegEx": "Iyyer,? \\Q2015\\E", "shortCiteRegEx": "Iyyer", "year": 2015}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Johnson", "R. Zhang 2015] Johnson", "T. Zhang"], "venue": "In Proc. of NAACL,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "2016. Bag of tricks for efficient text classification", "author": ["Joulin"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "Joulin,? \\Q2016\\E", "shortCiteRegEx": "Joulin", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Kiros"], "venue": null, "citeRegEx": "Kiros,? \\Q2015\\E", "shortCiteRegEx": "Kiros", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Q. Mikolov 2014] Le", "T. Mikolov"], "venue": "In Proc. of ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "The link-prediction problem for social networks. JASIST 1019\u20131031", "author": ["Liben-Nowell", "D. Kleinberg 2007] Liben-Nowell", "J. Kleinberg"], "venue": null, "citeRegEx": "Liben.Nowell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liben.Nowell et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Composition in distributional models of semantics. Cognitive science 34:1388\u20131429", "author": ["Mitchell", "J. Lapata 2010] Mitchell", "M. Lapata"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Asymmetric transitivity preserving graph embedding", "author": ["Ou"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Ou,? \\Q2016\\E", "shortCiteRegEx": "Ou", "year": 2016}, {"title": "Tri-party deep network representation", "author": ["Pan"], "venue": "In IJCAI", "citeRegEx": "Pan,? \\Q2016\\E", "shortCiteRegEx": "Pan", "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Pedregosa"], "venue": null, "citeRegEx": "Pedregosa,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa", "year": 2011}, {"title": "Deepwalk: Online learning of social representations", "author": ["Al-Rfou Perozzi", "B. Skiena 2014] Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Perozzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Walklets: Multiscale graph embeddings for interpretable network classification", "author": ["Kulkarni Perozzi", "B. Skiena 2016] Perozzi", "V. Kulkarni", "S. Skiena"], "venue": "arXiv preprint arXiv:1605.02115", "citeRegEx": "Perozzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Perozzi et al\\.", "year": 2016}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "S.T. Saul 2000] Roweis", "L.K. Saul"], "venue": "Science", "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "M. Paliwal 1997] Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Collective classification in network data. AI magazine 29(3):93", "author": ["Sen"], "venue": null, "citeRegEx": "Sen,? \\Q2008\\E", "shortCiteRegEx": "Sen", "year": 2008}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher"], "venue": "TACL", "citeRegEx": "Socher,? \\Q2014\\E", "shortCiteRegEx": "Socher", "year": 2014}, {"title": "Arnetminer: Extraction and mining of academic social networks", "author": ["Tang"], "venue": "In KDD\u201908,", "citeRegEx": "Tang,? \\Q2008\\E", "shortCiteRegEx": "Tang", "year": 2008}, {"title": "Line: Large-scale information network embedding", "author": ["Tang"], "venue": "In Proc. of WWW,", "citeRegEx": "Tang,? \\Q2015\\E", "shortCiteRegEx": "Tang", "year": 2015}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["Qu Tang", "J. Mei 2015] Tang", "M. Qu", "Q. Mei"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["De Silva Tenenbaum", "J.B. Langford 2000] Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Multi-label classification: An overview", "author": ["Tsoumakas", "G. Katakis 2006] Tsoumakas", "I. Katakis"], "venue": "Dept. of Informatics,", "citeRegEx": "Tsoumakas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2006}, {"title": "Inferring correspondences from multiple sources for microblog user tags", "author": ["Liu Tu", "C. Sun 2014] Tu", "Z. Liu", "M. Sun"], "venue": "In Chinese National Conference on Social Media Processing,", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Structural deep network embedding", "author": ["Cui Wang", "D. Zhu 2016] Wang", "P. Cui", "W. Zhu"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Comprehend deepwalk as matrix factorization", "author": ["Yang", "C. Liu 2015] Yang", "Z. Liu"], "venue": "arXiv preprint arXiv:1501.00358", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Network representation learning with rich text information", "author": ["Yang"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Personalized entity recommendation: A heterogeneous information network approach", "author": ["Yu"], "venue": "In Proc. of the WSDM,", "citeRegEx": "Yu,? \\Q2014\\E", "shortCiteRegEx": "Yu", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Various approaches have been proposed toward this goal, typically including Deepwalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015), GraRep (Cao, Lu, and Xu 2015), and node2vec (Grover and Leskovec 2016).", "startOffset": 127, "endOffset": 145}, {"referenceID": 34, "context": "To cope with this challenge, Yang et al. (2015) presented text-associated DeepWalk (TADW), which incorporates textual features into network embeddings through matrix factorization.", "startOffset": 29, "endOffset": 48}, {"referenceID": 8, "context": ", sentence, paragraph), a simple and intuitive approach would be averaging the embeddings of each word in the text (Mitchell and Lapata 2010; Ferrone and Zanzotto 2013; Iyyer et al. 2015). More sophisticated models have been designed to utilize the internal structure of sentences or documents to assist the composition. For example, Socher et al. (2013) and Socher et al.", "startOffset": 169, "endOffset": 355}, {"referenceID": 8, "context": ", sentence, paragraph), a simple and intuitive approach would be averaging the embeddings of each word in the text (Mitchell and Lapata 2010; Ferrone and Zanzotto 2013; Iyyer et al. 2015). More sophisticated models have been designed to utilize the internal structure of sentences or documents to assist the composition. For example, Socher et al. (2013) and Socher et al. (2014) use recursive neural networks over parse trees to obtain sentence representations.", "startOffset": 169, "endOffset": 380}, {"referenceID": 29, "context": "LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process.", "startOffset": 5, "endOffset": 23}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network.", "startOffset": 131, "endOffset": 428}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process.", "startOffset": 131, "endOffset": 663}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure.", "startOffset": 131, "endOffset": 1040}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure. Yang et al. (2015) present the first work that combines structure and content information for learning network embeddings.", "startOffset": 131, "endOffset": 1122}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure. Yang et al. (2015) present the first work that combines structure and content information for learning network embeddings. They show that DeepWalk is equivalent to matrix factorization (MF) and text features of vertices can be incorporated via factorizing a text-associated matrix. This method, however, suffers from the high computation cost of MF and has difficulties scaling to large-scale networks. Pan et al. (2016) instead combines DeepWalk with Doc2Vec (Le and Mikolov 2014), along with partial labels of nodes that constitutes a semi-supervised model.", "startOffset": 131, "endOffset": 1524}, {"referenceID": 3, "context": "Here we use the gated recurrent unit (GRU) proposed by Cho et al. (2014). GRU is a simplified version of the LSTM unit proposed earlier (Hochreiter and Schmidhuber 1997), with fewer parameters while still preserving the ability of capturing long-term dependencies.", "startOffset": 55, "endOffset": 73}, {"referenceID": 29, "context": "\u2022 LINE (Tang et al. 2015).", "startOffset": 7, "endOffset": 25}, {"referenceID": 34, "context": "\u2022 TADW (Yang et al. 2015).", "startOffset": 7, "endOffset": 25}, {"referenceID": 29, "context": "Following the metric used in previous studies (Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015), we randomly sample a portion (TR, from 10% to 90%) of the labeled vertices as training data, with the rest of the vertices for testing.", "startOffset": 46, "endOffset": 99}, {"referenceID": 27, "context": "In addition, the negative sampling size SN nn is 15 for all methods, and SN nc is 25 for CENE; the total number of samples T is 10 billion for LINE (1st) and LINE (2nd) as shown in Tang et al. (2015); window size win = 5, walk length t = 40 and number of walks per vertex \u03b3 = 50 for DeepWalk.", "startOffset": 181, "endOffset": 200}], "year": 2016, "abstractText": "This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world networks with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning. Introduction Network embedding, which aims at learning lowdimensional vector representations of a network, has attracted increasing interest in recent years. It has been shown highly effective in many important tasks in network analysis involving predictions over nodes and edges, such as node classification (Tsoumakas and Katakis 2006; Sen et al. 2008), recommendation (Tu, Liu, and Sun 2014; Yu et al. 2014) and link prediction (Liben-Nowell and Kleinberg 2007). Various approaches have been proposed toward this goal, typically including Deepwalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015), GraRep (Cao, Lu, and Xu 2015), and node2vec (Grover and Leskovec 2016). These models have been proven effective in several real world networks. Most of the previous approaches utilize information only from the network structure, i.e., the linkage relationships between nodes, while paying scant attention to the content of each node, which is common in real-world networks. In a typical social network with users as vertices, the user-generated contents (e.g., texts, images) will serve as rich extra information which should be important for node representation and beneficial to downstream applications. Figure 1 shows an example network from Quora, a community question answering website. Users in Quora can follow each other, creating directed connections in the network. How does the shape", "creator": "LaTeX with hyperref package"}}}