{"id": "1610.03120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Correlation-Based Method for Sentiment Classification", "abstract": "many the classical classic continuous supervised classification algorithms are efficient, but time - consuming, complicated however and not interpretable, which makes it difficult to analyze entirely their results that limits the possibility to improve them based on real observations. in this paper, we propose merely a new and simplified a simple classifier to predict a sentiment label outside of a short text. this model keeps the capacity of human emotional interpret - ability and can be extended to integrate both nlp techniques in a more interpretable way. our model is based on a correlation metric which measures the degree of association between given a sentiment label and a word. ten correlation metrics are proposed and evaluated intrinsically. and then a classifier based on each metric is uniquely proposed, evaluated and compared to the classic empirical classification algorithms which have proved their performance in many studies. our model outperforms practically these algorithms with several modified correlation evaluation metrics.", "histories": [["v1", "Mon, 10 Oct 2016 22:35:21 GMT  (904kb,D)", "http://arxiv.org/abs/1610.03120v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["hussam hamdan"], "accepted": false, "id": "1610.03120"}, "pdf": {"name": "1610.03120.pdf", "metadata": {"source": "CRF", "title": "Correlation-Based Method for Sentiment Classification", "authors": ["Hussam Hamdan"], "emails": ["Hussam.Hamdan@lip6.fr"], "sections": [{"heading": "1 Introduction", "text": "Polarity classification is the main task of Sentiment Analysis. The interest in polarity detection has more and more increased with the growing of user- generated data which may be interesting for many who want to understand the opinions of the users or customers towards an issue or a service.\nMuch research has been done on the document level sentiment analysis, but recently the focus has been on the short text which can be a sentence or a clause. For example, the sentiment analysis in Twitter where the tweet cannot exceed 140 characters, and also in on-line reviews where we seek to extract the polarity towards an aspect or a topic of interest. In SemEval workshops, many tasks have been proposed to deal with the polarity de-\ntection in short texts. Sentiment Analysis in Twitter has started since 2013 where the participants have been asked to give a polarity label to a tweet or to a topic discussed within the tweet. Aspect Based Sentiment Analysis task has also started since 2014 where one of its objectives is to give a sentiment label to a known aspect of restaurant or laptop.\nOn-line Short text is different from long text. The main characteristic is that the length is very short, no longer than 140 characters in Twitter; it also suffers from Sparseness, it contains several words which don not provide enough words co-occurrence. In addition to many misspellings, non-standard terms and noise.\nMany supervised classification algorithms have been used to classify a short text, such as k-nearest neighbors (k-NN), Naive-Bayes, Maximum Entropy (ME), Logistic Regression (LR) and Support Vector Machines (SVM). The simple algorithms like Naive-Bayes and the more complicated ones like SVM and LR seem to give fair results in many studies. These algorithms try to assign a weight for each feature, then to predict the new text according to its final score, the process of weighting computing is time-consuming and their resulting weights are not understandable which makes it difficult for human interpret-ability and limits the ability to extend and integrate other techniques into the model. Normally, a hand-crafted feature extraction is done to improve the performance, but in an intuitive way, the weights assigned to some features can not be interpretable in many cases, therefore if we want to extract some features to improve the performance we cannot guarantee the effect of the new features.\nIn other hand, the results of some unsupervised sentiment classifiers can be human interpretable. For example, some unsupervised methods compute the Pair-Wise Mutual Information PMI between the phrase and some positive and negative\nar X\niv :1\n61 0.\n03 12\n0v 1\n[ cs\n.C L\n] 1\n0 O\nct 2\n01 6\nindicators to attribute the text to the most associated sentiment label.\nIn this study, we propose to use and extend the methods which have been used for unsupervised sentiment classification but in a supervised manner, thus instead of calculating the word association with the sentiment labels using some seed positive and negative words on a big un labelled corpus, we propose to use the labeled corpus for computing the word associations in order to predict the sentiment label, in this way if the classifier gives a wrong results we can understand the reason of the wrong decision and improve the performance in a wise way. Therefor, our objective is to build a simple supervised classifier which gives a good performance, but keeps the results understanding by human to integrate other techniques, rules and real observations. In this paper, we study several correlation metrics which compute the association between a term and each sentiment label using labeled data, then we use these scores to predict the polarity of a new text. The final classification model is simple, it just sums the scores given by a correlation-base metric for each sentiment label and then assign the text to the sentiment label that gets the highest score.\nThe paper is organized as follows. We begin with a description of related work in Section 2. Next, we describe the correlation metrics in Section 3. The proposed model is presented in Section 4. The data sets, experiments and evaluations will be discussed in Section 5. The conclusion and future work in Section 6."}, {"heading": "2 Related Work", "text": "The correlation metrics have been widely used in supervised text classification for feature selection. In sentiment Analysis, some metrics have been used in two scenarios: (1) to measure the prior polarity of the words in order to use it for sentiment classification or sentiment lexicon construction, (2) to compute a term weight in order to integrate it in document representation for SVM classifier. The first scenario has been used in unsupervised context, but the second is supervised.\nThe early work (Turney, 2002) estimated the sentiment orientation (SO) of the extracted phrases using the Pointwise Mutual Information (PMI). The sentiment orientation of a phrase is computed based on its association with the positive reference word \u201dexcellent\u201d and the negative\nreference word \u201dpoor\u201d. Authors in (Turney and Littman, 2003) used SO to compute the sentiment orientation of a given word. They computed the orientation of the word from the strength of its association with a set of positive words (good, nice, excellent, positive,fortunate, correct, and superior), minus the strength of its association with a set of negative words (bad, nasty, poor, negative, unfortunate, wrong, and inferior). In (Mohammad, 2012) Authors collected a set of 775,000 tweets to generate a large word-sentiment association lexicon; a tweet was considered positive if it has one of 32 positive hashtagged seed words, and negative if it has one of 36 negative hashtagged seed words; the association score for a term was calculated using SO. Authors in (Mohammad et al., 2013) used similar method on the sentiment140 corpus (Go et al., 2009), a collection of 1.6 million tweets that contains positive and negative emoticons; the tweets are labeled positive or negative according to the emoticons. I In the second scenario, the researchers have used the correlation metrics to weighting the terms. While (Pang et al., 2002) reported that binary weight schema provides good accuracy with SVM, recent research has focused on more complex weighting schema which called supervised weighting metrics ,as they exploit the categorical information, or correlation metrics as they measure the correlation between a word and a category or class. Some metrics have been adopted from information retrieval such as DelatIDF (Martineau and Finin, 2009) (Paltoglou and Thelwall, 2010), later on several metrics have been proposed involving those adopted from information theory and widely used in text classification such as IG (information gain), MI (Mutual Information) (Deng et al., 2014). Recently, Wu and Gu (Wu and Gu, 2014) also tested several methods adopted from information retreival and information theory, they showed that existing methods suffer from the problem of over-weighting and introduced normalized formula of some existing metrics. They also proposed a new metric called natural entropy (ne) also inspired from information theory. Some research has used some metrics as Z score for feature extraction as in (Hamdan et al., 2014)."}, {"heading": "3 Correlation Metrics", "text": "The correlation metrics measure the degree of the association of the term with positive, negative and\nneutral sentiment. These metrics are supervised, they need a training documents to compute the association between terms and sentiment labels. Before describing the ten supervised metrics used in this study, we present the notation used to define these metrics, we use the words term and feature interchangeably: N : Number of documents in the corpus; Nc : Number of documents in the class c; Nc\u0304 : Number of documents out of the class c; df : Number of documents containing the feature f in whole corpus; dfc : Number of documents in class c containing the feature f; dfc\u0304: Number of documents out of class c containing the feature f; p(c) :The probability of the class c; p(c|f) : The probability of class c given the feature f; p(f) :The probability of the feature f in the corpus; df\u0304 : The number of documents which don\u2019t contain the feature f in the corpus; p(c\u0304|f):The complimentary probability of class c given the feature f; p(c, f)The joint probability of the class c and the feature f; p(c, f\u0304): The probability of class c in the documents which don\u2019t contain the feature f; df\u0304 c\u0304 :The number of document out of class c and don\u2019t contain the feature f; df\u0304c : The number of document don\u2019t contain the feature f in the class c. The Ten weighting metrics are:\nNatural Entropy (ne) The more uneven the distribution of documents where a feature occurs, the larger the weight of this feature is. Thus, the entropy of the feature can express the uncertainty of the class given the feature. One minus this degree of uncertainty boosts the features that unevenly distributed between the category and other categories.\nne(f, c) = 1 + (p(c|f).log(p(c|f)) + p(c\u0304|f).log(p(c\u0304|f)))\nThis metrics was firstly proposed in (Wu and Gu, 2014), ne score is always between 0 and 1, and it assigns a high score for the words unevenly distributed over the classes, but it cannot\ndiscriminate the positive words from the negative ones. Therefore, authors in (Hamdan et al., 2015) have used the dfc and dfc\u0304 for discriminating the positive words from the negative ones; if dfc>dfc\u0304 then the word is considered positive else it is considered negative. In this work a negative value is given for a negative word.\nPairwise Mutual Information (pmi) PMI is a measure of association used in information theory and statistics (Church and Hanks, 1990 03).\npmi(f, c) = log( p(c,f)p(c).p(f))\nOdds Ratio (orr) Orr gives a positive score to features that occur more often in one category than in the other, and a negative score if it occurs more in the other. A score of zero means the the odds for a feature to occur in one category is exactly the same as the odds for it to occur in the other (Shaw, 1995).\norr(f, c) = log(p(f |c).(1\u2212p(f |c\u0304))p(f |c\u0304).(1\u2212p(f |c)))\nCategorical Proportional Difference (cpd) Cpd is a ratio that considers the number of documents of a category in which the feature occurs and the number of documents from other categories in which the feature also occurs (Simeon and Hilderman, 2008).\ncpd(f, c) = dfc\u2212dfc\u0304df\nKullback-Leibler Divergence (kl) Kl is a non-symmetric measure of the difference between the distribution of the category and the distribution of the category given the feature. A measure of how dissimilar the two distributions are. Useful feature value imply a high degree of dissimilarity.\nkl(f, c) = p(c|f).log(p(c|f)p(c) )\nRelevance Frequency (rf) The basic idea of rf is to boost the higher frequency terms in the positive category than in the negative one, that helps in selecting the positive samples from the negative ones (Lan et al., 2009).\nrf(f, c) = log(2 + dfcmax(1,dfc\u0304))\nMultinomial Z Score (zd) Suppose that the feature follows multinomial distribution over the classes, zd calculates Z transformation for a feature in each class, zd boosts the highly unevenly distributed features among the classes, it gives high positive score for a feature in the class where it is highly frequent and negative score in the class where it rarely appears (Hamdan et al., 2014).\nzd(f, c) = dfc\u2212p(f).Nc\u221a Nc.p(f).(1\u2212p(f))\nDelta BM25 IDF (dbidf) Dbidf is a variant of delta-idf measure, BM25 idf variant is used instead of classical idf. dbidf was published in Sentiment Analysis literature (Paltoglou and Thelwall, 2010).\ndbidf(f, c) = log( (Nc\u2212dfc+0.5).dfc\u0304+0.5(Nc\u0304\u2212dfc\u0304+0.5).dfc+0.5)\nWeighted Log Likelihood Ratio(wllr) Wllr is a measure of how dissimilar the distribution of the feature given the category and the distribution of the feature given the other categories (Nigam et al., 2000).\nwllr(f, c) = p(f |c).log(p(f |c)p(f |c\u0304))\nNGL Coefficient (ngl) Ngl is a variant of the Chi square metric.It measures the lack of independence between the feature and the category. The higher value, the closer relationship the feature and the class have (Ng et al., 1997).\nngl(f, c) = \u221a N(dfc.df\u0304c\u0304\u2212dfc\u0304.df\u0304c)\u221a\ndf.df\u0304 .Nc.Nc\u0304"}, {"heading": "4 Correlation-Based Model", "text": "In this section, we introduce our model which directly relates each short text instance to a sentiment label. How a short text instance, denoted by s, is related to a sentiment label sl can be measured based on the correlation of the overlapping words among the words in the training set and the sentence s. Thus, for each correlation metric which measures the degree of association of each word to each sentiment label, we compute the final polarity score of a short text as the sum score of its words. Three sum scores are computed, one for each sentiment label, then the sentence will\nbe assigned to the sentiment label which has the highest sum score. For calculating the sum score of a sentence s in a sentiment label or class i, we sum the scores of the sentence words. The correlation between s and sl is calculated based on scores as:\nsumscore(s, i) = \u2211\nt\u2208smetricScore(t, i)\nmetricScore(t,i) stands for the score of correlation between the term t and the sentiment label i, this score is given by any of the ten correlation metrics presented in the previous section. Finally, s is assigned to a sentiment label which have the highest score. For example, let\u2019s assume that s=\u201d happy day\u201d the PMi metric gives three scores for the word happy 0,0.2,1.2 in negative, neutral and positive class respectively and the three scores for day 0.3,0.9,0.2. The model produce three score, one for each sentiment label, positive score=1.2+0.2=1.4, negative score=0+0.3=0.3, neutral score=0.2+0.9=1.1 while the positive score is the highest one. Therefore, the sentence is classified as positive."}, {"heading": "5 Experiments and Evaluations", "text": "For testing our proposed model, we conduct two types of evaluations: intrinsic and extrinsic. For intrinsic evaluation, we test the performance of the proposed correlation metrics, therefore we compare the degree of prior polarity with a manually annotated list of Twitter terms. For testing our model, an extrinsic evaluation is done using two short text data set, where we calculate the correlation scores based on the training set of each data set."}, {"heading": "5.1 Training and Testing Data", "text": "We have used two data sets, the first one is extracted from Twitter which has been provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloaded these tweets using the given script. We obtained 9646 tweets, the whole training data set is used for training. The test data set provided in SemEval-2015 containing about 2390 tweets (Rosenthal et al., 2015) is used for evaluating our system. The second data set is extracted from Laptop re-\nviews, provided by SemEval 2015 ABSA organizers (Pontiki et al., 2015) where each review is composed of several sentences, the participants are asked to detect the polarity of each senetce. Table 1 shows the distribution of each label in each data set."}, {"heading": "5.2 Correlation Metrics Evaluation", "text": "This evaluation is done for measuring the performance of the correlation metrics in determining strength of association of Twitter terms with positive sentiment. A ranked list of twitter terms obtained from human annotations was provided by SemEval-2015 organizers (Rosenthal et al., 2015), the original ranked list contains 1315 terms, we used only 552 terms which exist in our training data set. We first compute the score of each term as the score of correlation with positive sentiment minus the score of correlation with the negative and neural ones, and then comparing this ranked list to the human annotated one. Kendall\u2019s Tau and Spearman will be used as metrics to compare the ranked lists. Table 2 shows the results for each metric with Kendall metric and Spearman. Kl gives the highest correlation 64% followed by cpd and rf 44% then orr metric with 43% with Kendall\u2019s Tau.\nNote that several metrics give the same results such ne, pmi, zd. Therefore, we measure the correlation between each pairs of metrics to discover how the metrics which based on different mathematical calculations can give different scores for each word but maintains the same ranking. Table 3 shows the correlation between each two pairs. Note that the metrics which give the same kendall score are so correlated as zd,ngl and ne,pmi,orr,kl. In other way, it seems that the metrics constitute some groups, each group tend to give the same ranking."}, {"heading": "5.3 Correlation-Based Model Evaluation", "text": "For evaluation our correlation-based model Section 3, we used two data sets extracted from Twitter and Laptop reviews. We have tested the model using one of each correlation metrics in addition to the evaluation using the SVM classifier and Logistic Regression which have achieved a robust baseline in SemEal2015 competition.\nTable 4 shows the results using our proposed model with the ten metrics, SVM and LR, the second column contains the f-score of positive and negatives sentiment labels, the metric of SemEval evaluation. But the third column contains the accuracy in Laptop reviews data set which was the standard evaluation metric in SemEval-2015 for this task.\nIn Twitter set, the performance of kl is the best, it achieves 57.30% with a gain of 4.95% over SVM and 3.92% over LR, note that kl was the best metric in the metric evaluation, kl gives a significant improvement of SVm and LR. orr metric gives a better performance over SVM and LR while pmi outperforms SVM but not LR. Other metrics are under the baseline of SVM and LR.\nFigure 1 demonstrates the distribution of words in Twitter set in positive class using the ten correlation metrics, each metric distributes the words in a different way. While some metrics as dbidf gives good ranking score according to the metric evaluation, it is bad in model evaluation, the reason may be that the scores given by dbidf have a wide range which may negatively affect the sum of scores. We also remark that some metrics produce some small negative scores such kl but others produce more large negatives ones such zd, pmi, ngl. In our experiments we choose a threshold to be 0 for kl because its negative values so small to be considered but the other metrics is evaluated without such threshold. It should note that the seeking for an appropriate threshold for each metric may lead to more robust model.\nThe third column in Table 4 is the accuracy score as it has been considered as evaluation measure in SemeEval-2015, the accuracy is followed by the number of correct sentences over the total number of sentences. The results show that orr metric outperforms other metrics, it achieves 72.4% with 2.43% over SVM and 0.75% over LR. ne metric also outperforms SVM and LR, rf is similar to LR and pmi gives a fair result but lower than SVM nad LR. kl metric which is the best one for Twitter set seems not efficient in Laptop and\nin general our model is not so as efficient in Laptop set as in twitter data set. One possible reason that the metrics need a sufficient amount of training data to produce a reliable scores.\nTherefore, the correlation based methods seem to be promising in the sentiment analysis of short text. They are simple, fast and interpretable methods. Some metrics like orr and pmi give good results in both data sets and outperforms the two classic models SVM and LR. In average the orr metric seems to be the best metric.\nTable 5 shows the ten high words produced for the best metrics with there scores. Some metrics produce the same ranked list but with different scores. For example, kl, pmi and orr produce the same list but with different scores, zd and ngl do the same. Therefore, the proposed model can be modified to take into account this observation, a normalization factor can be added to make the metrics which produce the same rank give the same results. A deeper research may led to classify the metrics into some groups where each group produces the same ranked list and a combination of different groups may improve the classification results."}, {"heading": "6 Conclusion", "text": "In this paper, we presented ten correlation metrics for measuring the correlation between the words and the sentiment labels. Then, we proposed a classification model based on a correlation metric. We have evaluated the metrics intrinsically using a ranked list of twitter terms, and the proposed model was evaluated in sentiment classification task using two short text data sets extracted from twitter and laptop reviews. The proposed\nmodel seem to give good results and outperforms some classic supervised algorithms with some correlation metrics. The results given by the model can be interpretable and analyzed in order to improve it in a wise way. In the next work, we plan to\nexplore how we can combine the NLP techniques in our model to take into account the negation, the reversal words, and the modifiers. We also plan to combine noisy labeled or unlabeled data which may help to get more robust correlation scores."}], "references": [{"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Patrick Hanks"], "venue": null, "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "A study of supervised term weighting scheme for sentiment analysis", "author": ["Deng et al.2014] Zhi-Hong Deng", "Kun-Hu Luo", "Hong-Liang Yu"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Go et al.2009] Alec Go", "Richa Bhayani", "Lei Huang"], "venue": null, "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "The impact of z score on twitter sentiment analysis", "author": ["Hamdan et al.2014] Hussam Hamdan", "Patrice Bellot", "Frederic Bechet"], "venue": "Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Hamdan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hamdan et al\\.", "year": 2014}, {"title": "Sentiment lexiconbased features for sentiment analysis in short text", "author": ["Hamdan et al.2015] Hussam Hamdan", "Patrice Bellot", "Frederic Bechet"], "venue": "Proceeding of the 16th International Conference on Intelligent Text Processing and Computa-", "citeRegEx": "Hamdan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hamdan et al\\.", "year": 2015}, {"title": "Supervised and traditional term weighting methods for automatic text categorization", "author": ["Lan et al.2009] Man Lan", "Chew Lim Tan", "Jian Su", "Yue Lu"], "venue": null, "citeRegEx": "Lan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2009}, {"title": "Delta TFIDF: An improved feature space for sentiment analysis", "author": ["Martineau", "Finin2009] Justin Martineau", "Tim Finin"], "venue": null, "citeRegEx": "Martineau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martineau et al\\.", "year": 2009}, {"title": "NRCCanada: Building the state-of-the-art in sentiment analysis of tweets", "author": ["Svetlana Kiritchenko", "Xiaodan Zhu"], "venue": "Proceedings of the International Workshop on Semantic Evaluation,", "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "emotional tweets. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume", "author": ["Saif Mohammad"], "venue": "Proceedings of the Sixth In-", "citeRegEx": "Mohammad.,? \\Q2012\\E", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "SemEval-2013 task 2: Sentiment analysis in twitter", "author": ["Nakov et al.2013] Preslav Nakov", "Sara Rosenthal", "Zornitsa Kozareva", "Veselin Stoyanov", "Alan Ritter", "Theresa Wilson"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Nakov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2013}, {"title": "Feature selection, perceptron learning, and a usability case study for text categorization", "author": ["Ng et al.1997] Hwee Tou Ng", "Wei Boon Goh", "Kok Leong Low"], "venue": "In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and", "citeRegEx": "Ng et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1997}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["Nigam et al.2000] Kamal Nigam", "Andrew Kachites McCallum", "Sebastian Thrun", "Tom Mitchell"], "venue": null, "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "A study of information retrieval weighting schemes for sentiment analysis", "author": ["Paltoglou", "Thelwall2010] Georgios Paltoglou", "Mike Thelwall"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Paltoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paltoglou et al\\.", "year": 2010}, {"title": "Thumbs up?: Sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "SemEval-2015 task 12: Aspect based sentiment analysis", "author": ["Dimitrios Galanis", "Haris Papageogiou", "Suresh Manandhar", "Ion Androutsopoulos"], "venue": "Proceedings of the 9th International Workshop on Semantic Evalu-", "citeRegEx": "Pontiki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "SemEval-2015 task 10: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Svetlana Kiritchenko", "Saif M. Mohammad", "Alan Ritter", "Veselin Stoyanov"], "venue": "In Proceedings of the 9th International Workshop on Semantic", "citeRegEx": "Rosenthal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "Term-relevance computations and perfect retrieval performance", "author": ["W.M. Shaw", "Jr."], "venue": null, "citeRegEx": "Shaw and Jr.,? \\Q1995\\E", "shortCiteRegEx": "Shaw and Jr.", "year": 1995}, {"title": "Categorical proportional difference: A feature selection method for text categorization", "author": ["Simeon", "Hilderman2008] Mondelle Simeon", "Robert Hilderman"], "venue": "In Proceedings of the 7th Australasian Data Mining Conference - Volume", "citeRegEx": "Simeon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Simeon et al\\.", "year": 2008}, {"title": "Measuring praise and criticism: Inference of semantic orientation from association", "author": ["Turney", "Littman2003] Peter D. Turney", "Michael L. Littman"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2003}, {"title": "Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews", "author": ["Peter D. Turney"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Turney.,? \\Q2002\\E", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "Reducing over-weighting in supervised term weighting for sentiment analysis", "author": ["Wu", "Gu2014] Haibing Wu", "Xiaodong Gu"], "venue": "COLING", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "The early work (Turney, 2002) estimated the sentiment orientation (SO) of the extracted phrases using the Pointwise Mutual Information (PMI).", "startOffset": 15, "endOffset": 29}, {"referenceID": 8, "context": "In (Mohammad, 2012) Authors collected a set of 775,000 tweets to generate a large word-sentiment association lexicon; a tweet was considered positive if it has one of 32 positive hashtagged seed words, and negative if it has one of 36 negative hashtagged seed words; the association score for a term was calculated using SO.", "startOffset": 3, "endOffset": 19}, {"referenceID": 7, "context": "Authors in (Mohammad et al., 2013) used similar method on the sentiment140 corpus (Go et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": ", 2013) used similar method on the sentiment140 corpus (Go et al., 2009), a collection of", "startOffset": 55, "endOffset": 72}, {"referenceID": 13, "context": "(Pang et al., 2002) reported that binary weight schema provides good accuracy with SVM, recent research has focused on more complex weighting schema which called supervised weighting metrics ,as they exploit the categorical information,", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "on several metrics have been proposed involving those adopted from information theory and widely used in text classification such as IG (information gain), MI (Mutual Information) (Deng et al., 2014).", "startOffset": 180, "endOffset": 199}, {"referenceID": 3, "context": "Some research has used some metrics as Z score for feature extraction as in (Hamdan et al., 2014).", "startOffset": 76, "endOffset": 97}, {"referenceID": 4, "context": "Therefore, authors in (Hamdan et al., 2015) have used the dfc and dfc\u0304 for discriminating the positive words from the negative ones; if dfc>dfc\u0304 then the word is considered positive else it is considered negative.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "The basic idea of rf is to boost the higher frequency terms in the positive category than in the negative one, that helps in selecting the positive samples from the negative ones (Lan et al., 2009).", "startOffset": 179, "endOffset": 197}, {"referenceID": 3, "context": "Suppose that the feature follows multinomial distribution over the classes, zd calculates Z transformation for a feature in each class, zd boosts the highly unevenly distributed features among the classes, it gives high positive score for a feature in the class where it is highly frequent and negative score in the class where it rarely appears (Hamdan et al., 2014).", "startOffset": 346, "endOffset": 367}, {"referenceID": 11, "context": "tribution of the feature given the category and the distribution of the feature given the other categories (Nigam et al., 2000).", "startOffset": 107, "endOffset": 127}, {"referenceID": 10, "context": "(Ng et al., 1997).", "startOffset": 0, "endOffset": 17}, {"referenceID": 9, "context": "We have used two data sets, the first one is extracted from Twitter which has been provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013).", "startOffset": 155, "endOffset": 175}, {"referenceID": 15, "context": "The test data set provided in SemEval-2015 containing about 2390 tweets (Rosenthal et al., 2015) is used for evaluating our system.", "startOffset": 72, "endOffset": 96}, {"referenceID": 14, "context": "views, provided by SemEval 2015 ABSA organizers (Pontiki et al., 2015) where each review is composed of several sentences, the participants are asked to detect the polarity of each senetce.", "startOffset": 48, "endOffset": 70}, {"referenceID": 15, "context": "A ranked list of twitter terms obtained from human annotations was provided by SemEval-2015 organizers (Rosenthal et al., 2015), the original ranked list contains 1315 terms, we used only 552 terms which exist in our training", "startOffset": 103, "endOffset": 127}], "year": 2016, "abstractText": "The classic supervised classification algorithms are efficient, but time-consuming, complicated and not interpretable, which makes it difficult to analyze their results that limits the possibility to improve them based on real observations. In this paper, we propose a new and a simple classifier to predict a sentiment label of a short text. This model keeps the capacity of human interpret-ability and can be extended to integrate NLP techniques in a more interpretable way. Our model is based on a correlation metric which measures the degree of association between a sentiment label and a word. Ten correlation metrics are proposed and evaluated intrinsically. And then a classifier based on each metric is proposed, evaluated and compared to the classic classification algorithms which have proved their performance in many studies. Our model outperforms these algorithms with several correlation metrics.", "creator": "LaTeX with hyperref package"}}}