{"id": "1610.03759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Language Models with Pre-Trained (GloVe) Word Embeddings", "abstract": "in this work we implement a deeper training type of a language model ( lm ), whether using recurrent neural network ( rnn ) and glove word embeddings, introduced by pennigton et al. in [ 1 ]. mainly the implementation is following regarding the general idea of training rnns for lm tasks first presented differently in [ module 2 ], but is rather using gated recurrent unit ( gru ) [ 3 ] matrices for a memory coding cell, and not the more commonly used lstm [ 4 ].", "histories": [["v1", "Wed, 12 Oct 2016 15:53:02 GMT  (115kb,D)", "https://arxiv.org/abs/1610.03759v1", null], ["v2", "Sun, 5 Feb 2017 11:24:05 GMT  (119kb,D)", "http://arxiv.org/abs/1610.03759v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["victor makarenkov", "bracha shapira", "lior rokach"], "accepted": false, "id": "1610.03759"}, "pdf": {"name": "1610.03759.pdf", "metadata": {"source": "CRF", "title": "Language Models with Pre-Trained (GloVe) Word Embeddings", "authors": ["Victor Makarenkov", "Lior Rokach", "Bracha Shapira"], "emails": ["makarenk@post.bgu.ac.il,", "liorrk@bgu.ac.il,", "bshapira@bgu.ac.il"], "sections": [{"heading": null, "text": "Language Models with Pre-Trained (GloVe) Word Embeddings\nVictor Makarenkov, Lior Rokach, Bracha Shapira makarenk@post.bgu.ac.il, liorrk@bgu.ac.il, bshapira@bgu.ac.il\nDepartment of Software and Information Systems Engineering Ben-Gurion University of the Negev"}, {"heading": "1 Introduction", "text": "In this work we present a step-by-step implementation of training a Language Model (LM) , using Recurrent Neural Network (RNN) and pre-trained GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4]. The implementation presented is based on using keras1 [5]."}, {"heading": "2 Motivation", "text": "Language Modeling is an important task in many Natural Language Processing (NLP) applications. These application include clustering, information retrieval, machine translation, spelling and grammatical errors correction. In general, a language model defined as a function that puts a probability measure over strings drawn from some vocabulary. In this work we consider a RNN based language model task, which aims at predicting the next n-th word in a sequence, given the previous n\u2212 1 words. Put otherwise, finding the word with maximum value for P (wn|w1, ..., wn\u22121) . The n parameter is the ContextWindowSize argument in the algorithm described further. To maximize the effectiveness and performance of the model we use word embeddings into a continuous vector space. The model of embedding we use is the GloVe [1] model, with dimensionality size equal to 300 or 50. We use both pre-trained2 on 42 billion tokens and 1.9 million vocabulary size, and specifically trained for this work vector models, which we trained on SIGIR-2015 and ICTIR-2015 conferences\u2019 proceedings. The model itself is trained as a RNN, with internal GRU for memorizing the prior sequence of words. It was shown lately, that RNNs outperform most language modeling based tasks [2, 3] when tuned and trained correctly.\n1 https://keras.io/ 2 downloaded from: http://nlp.stanford.edu/projects/glove/\nar X\niv :1\n61 0.\n03 75\n9v 2\n[ cs\n.C L\n] 5\nF eb\n2 01\n7"}, {"heading": "3 Short Description", "text": "In this work we use 300-dimensional and 50-dimensional, GloVe word embeddings. In order to embed the words in a vector space, GloVe model is trained by examining word co-occurrence matrix Xij within a huge text corpus. Despite the huge size of the Common Crawl corpus, some words may not exist with the embeddings, so we set these words to random vectors, and use the same embeddings consistently if we encounter the same unseen word again in the text. The RNN is further trained to predict the next word in its embedding form, that is, predicts the next n-dimensional vector, given the ContextWindowSize previous words. We divide the TextF ile into 70% and 30% for training and testing purposes."}, {"heading": "4 Pseudo Code", "text": "input : Input: glove-vectors : Pre-Trained-Word-Embeddings, Text-File, ContextWindowSize=10, hidden-unites=300 output: A Language Model trained on Text-File with word-embeddings representation for w \u2208 Text-File do if w \u2208 OOV-file then\ntokenized-file.append(OOV-file.get-vector(w)) end if w \u2208 glove-vectors then\ntokenized-file.append(glove-vectors.get-vector(w)) end else\nvector \u2190 random-vector() tokenized-file.append(vector) OOV-file.append(vector)\nend end NN \u2190 CreateSingleHiddenLayerDenseRNN(unit=GRU, inputs=300, outputs=300, hidden-unites) NN.setDropout(0.8) NN.setActivationFunction(Linear) NN.setLossFunction(MSE) NN.setOptimizer(rmsprop) Xtrain \u2190 tokenized-file.getInterval(0.0,0.7) Xtest \u2190 tokenized-file.getInterval(0.7,1.0) Ytrain \u2190 Xtrain.Shift(ContextWindowSize) Ytest \u2190 Xtest.Shift(ContextWindowSize) NN.Fit(Xtrain, Ytrain) NN.Predict(Xtest, Ytest)\nAlgorithm 1: Training a language model on word embeddings"}, {"heading": "5 Detailed Explanation", "text": "As stated earlier, GloVe model is trained by examining word co-occurrence matrix of two words i and j: Xij within a huge text corpus. While training the main idea is stating that wiTwj + bi + bj = log(Xij) where wi and wj are the trained vectors, bi and bj are the scalar bias terms associated with words i and j. The most important parts of the training process in GloVe are: 1) A weighting function f for elimination of very common words (like stop words) which add noise and not overweighted, 2) rare words are not overweighted 3) the co-occurrence strength, when modeled as a distance, should be smoothed with a log function. Thus, the final loss function for a GloVe model is J = \u03a3i,j\u2208V f(Xij)(wi\nTwj+bi+bj\u2212log(Xij))2 where V is a complete vocabulary, and f(x) = (x/xmax)\u03b1 if x < xmax, and f(x) = 1 otherwise. The model that is used in this work was trained with xmax = 100 and \u03b1 = 0.75.\nTraining of the RNN is somewhat blurred between supervised and unsupervised techniques. That is, no extra labeled data is given, but part of the input is used as labels. In this unfolded training paradigm, which is illustrated on Figure 1, the output is shifted in a way to create a labels for the input train dataset. In this way the RNN can actually learn to predict a next word (vector) in a sequence."}, {"heading": "6 Evaluation", "text": ""}, {"heading": "6.1 Pre-trained Vector Models", "text": "In order to evaluate our implementation of the language model, we train several different language models and compare the predicted error distribution with a\nrandom word prediction. The error is measured with a cosine distance3 between two vectors: 1 \u2212 x\u00b7y||x||\u00b7||y|| . On figure 2 we see the error distribution of the RNN with 30 hidden units. The training was performed on 5000 tokens long text file, the first entry at English wikipedia, Anarchism.\nThe machine that was used to run the evaluation had the following characteristics: 1.7 GHz, Core i7 with 8 GB memory, OS X version 10.11.13.\nThe time it took to train the model with 30 epochs was 125 seconds . The time took to make the predictions on a test set is 0.5 seconds.\nOn figure 3 we see the error distribution of the RNN with 300 hidden units The time it took to train the model with 300 epochs was 1298 seconds . The time took to make the predictions on a test set is 0.49 seconds."}, {"heading": "6.2 Self Trained Vector Model", "text": "In addition, in order to further evaluate the current approach, we specifically trained a narrow domain-specific, vector model. We used ICTIR-2015 and SIGIR2015 conferences proceedings as a corpora, and produced 50-dimensional vectors. The vector model is based on 1,500,000 tokens total, and resulted in 17,000 long vocabulary. The language model for the evaluation was built on a paper published in the ICTIR-2015 proceedings [7]. Consider figure 4. The predicted words\u2019 errors distribution differs even more than in the general case, where the vectors were trained on the general Common-Crawl corpora. That is, the performance of the language model, for the task of word prediction is higher.\nThe time took to train this model is 10 seconds. The time took to compute the predictions for evaluations is 0.04 seconds. 3 implemented on python, with scipy package"}, {"heading": "7 Instructions for running the code", "text": "The implementation of the model training was written in this work in the Python language, version 2.7. The library that was used is Keras, which in the course of this implementation was based on Theano framework. Instead of Theano, the Google\u2019s Tensorflow can be also used behind the scenes of the Keras in this implementation. In order to train the model yourself, you need to follow the next steps:\n1. Download pre-trained GloVe vectors from http://nlp.stanford.edu/projects/glove/ 2. Obtain a text to train the model on. In our example we use a Wikipdia\nAnarchism entry. 3. Open and adjust the LM_RNN_GloVe.py file parameters inside the main\nfunction: (a) file_2_tokenize_name (example = \"/Users/macbook/corpora/text2tokenize.txt\") (b) tokenized_file_name (example = \"/Users/macbook/corpora/tokenized2vectors.txt\") (c) glove_vectors_file_name (example = \"/Users/macbook/corpora/glove.42B.300d.txt\") (d) extra_vocab_filename (example = \"/Users/macbook/corpora/extra_vocab.txt\").\nThis argument has also a default value in the get_vector function 4. Run the following methods:\n(a) tokenize_file_to_vectors(glove_vectors_file_name, file_2_tokenize_name, tokenized_file_name) (b) run_experiment(tokenized_file_name)"}, {"heading": "8 Discussion", "text": "In this work we implemented and tested the training of a LM based on RNN. To emphasize the strength of such an approach, we have chosen one of the most\npowerful and prominent techniques for word embeddings - the GloVe embeddings. Although there are other approaches, such as the popular word2vec [8] technique, the GloVe embeddings was shown to outperform it on several tasks [1], partially because of the reasons described in section 5. By training the model with two different settings, one of which is order of magnitude more complex than the other we show the power of such LM. The distributions shown on figures 2 and 3 clearly indicate much smaller error on the task of next word prediction.\nThe main limitation of this implementation is the fixed window size, of the prefix in the LM. This approach does not fully show the full power of RNNbased LM. For dynamic size prefix LM please consider the DyNet [6] package for example. DyNet supports a dynamic computation graph and shares the learned parameters across multiple variable length instances during the training."}, {"heading": "9 The source at GitHub", "text": "The code was submitted publicly to the GitHub repository of the author, and is available under vicmak username, proofseer project4.\n4 https://github.com/vicmak/ProofSeer"}], "references": [{"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR abs/1409.2329", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "DyNet: The Dynamic Neural Network Toolkit arXiv preprint arXiv:1701.03980", "author": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn", "Kevin Duh", "Manaal Faruqui", "Cynthia Gan", "Dan Garrette", "Yangfeng Ji", "Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Theoretical categorization of query performance predictors", "author": ["V. Makarenkov", "B. Shapira", "L. Rokach"], "venue": "Proceedings of the 2015 International Conference on The Theory of Information Retrieval. ICTIR \u201915,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "startOffset": 204, "endOffset": 207}, {"referenceID": 0, "context": "The model of embedding we use is the GloVe [1] model, with dimensionality size equal to 300 or 50.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "It was shown lately, that RNNs outperform most language modeling based tasks [2, 3] when tuned and trained correctly.", "startOffset": 77, "endOffset": 83}, {"referenceID": 2, "context": "It was shown lately, that RNNs outperform most language modeling based tasks [2, 3] when tuned and trained correctly.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "The language model for the evaluation was built on a paper published in the ICTIR-2015 proceedings [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Although there are other approaches, such as the popular word2vec [8] technique, the GloVe embeddings was shown to outperform it on several tasks [1], partially because of the reasons described in section 5.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Although there are other approaches, such as the popular word2vec [8] technique, the GloVe embeddings was shown to outperform it on several tasks [1], partially because of the reasons described in section 5.", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "For dynamic size prefix LM please consider the DyNet [6] package for example.", "startOffset": 53, "endOffset": 56}], "year": 2017, "abstractText": "In this work we present a step-by-step implementation of training a Language Model (LM) , using Recurrent Neural Network (RNN) and pre-trained GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4]. The implementation presented is based on using keras1 [5].", "creator": "LaTeX with hyperref package"}}}