{"id": "1610.04325", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "Hadamard Product for Low-rank Bilinear Pooling", "abstract": "bilinear analysis models provide rich representations compared to linear models. they have nevertheless been greatly applied in various visual tasks, such as object recognition, segmentation, and visual question - answering, to get state - of - the - art performances taking advantage favorably of the expanded representation representations. however, bilinear representations tend to be high - dimensional, limiting the applicability to computationally complex dynamic tasks. we usually propose low - rank bilinear neural networks using hadamard product ( element - wise multiplication ), commonly also implemented in many scientific computing frameworks. we show that our structured model outperforms compact bilinear patterns pooling in visual question - answering tasks, having a better parsimonious property.", "histories": [["v1", "Fri, 14 Oct 2016 04:29:52 GMT  (80kb,D)", "http://arxiv.org/abs/1610.04325v1", "13 pages, 1 figure, &amp; appendix included"], ["v2", "Tue, 1 Nov 2016 05:31:27 GMT  (80kb,D)", "http://arxiv.org/abs/1610.04325v2", "13 pages, 1 figure, &amp; appendix included"], ["v3", "Tue, 14 Feb 2017 05:22:01 GMT  (83kb,D)", "http://arxiv.org/abs/1610.04325v3", "13 pages, 1 figure, &amp; appendix. ICLR 2017 accepted"], ["v4", "Sun, 26 Mar 2017 16:22:47 GMT  (83kb,D)", "http://arxiv.org/abs/1610.04325v4", "13 pages, 1 figure, &amp; appendix. ICLR 2017 accepted"]], "COMMENTS": "13 pages, 1 figure, &amp; appendix included", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["jin-hwa kim", "kyoung-woon on", "woosang lim", "jeonghee kim", "jung-woo ha", "byoung-tak zhang"], "accepted": true, "id": "1610.04325"}, "pdf": {"name": "1610.04325.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jin-Hwa Kim"], "emails": ["jhkim@bi.snu.ac.kr", "kwon@bi.snu.ac.kr", "jeonghee.kim@navercorp.com", "jungwoo.ha@navercorp.com", "btzhang@bi.snu.ac.kr"], "sections": [{"heading": null, "text": "Bilinear models provide rich representations compared to linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear neural networks using Hadamard product (element-wise multiplication), commonly implemented in many scientific computing frameworks. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, having a better parsimonious property."}, {"heading": "1 INTRODUCTION", "text": "Bilinear models (Tenenbaum & Freeman, 2000) provide richer representations than linear models. To exploit this advantage, fully-connected layers in neural networks can be replaced with bilinear pooling. The outer product of two vectors (or Kroneker product for matrices) is involved in bilinear pooling, as a result of this, all pairwise interactions among given features are considered. Recently, a successful application of this technique is used for fine-grained visual recognition (Lin et al., 2015).\nHowever, bilinear pooling produces a high-dimensional feature of quadratic expansion, which may constrain a model structure and computational resources. For example, an outer product of two feature vectors, both of which have 1K-dimensionality, produces a million-dimensional feature vector. Therefore, for classification problems, the choice of the number of target classes is severely constrained, because the number of parameters for a standard linear classifier is determined by multiplication of the size of the high-dimensional feature vector and the number of target classes.\nCompact bilinear pooling (Gao et al., 2015) reduces the quadratic expansion of dimensionality by two orders of magnitude, retaining the performance of the full bilinear pooling. For example, compact bilinear pooling approximates 4K \u00d7 4K features using only 16K pooled features. This approximation uses sampling-based computation, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013), which utilizes an useful property that \u03a8(x\u2297 y, h, s) = \u03a8(x, h, s) \u2217\u03a8(y, h, s), which means the projection of outer product of two vectors is the convolution of two projected vectors.\nar X\niv :1\n61 0.\n04 32\n5v 1\n[ cs\n.C V\nHere, \u03a8 is the proposed projection function, and, h and s are randomly sampled parameters by the algorithm.\nNevertheless, compact bilinear pooling embraces two shortcomings. One comes from the sampling approach. Compact bilinear pooling relies on a favorable property, E[\u3008\u03a8(x, h, s),\u03a8(y, h, s)\u3009] = \u3008x, y\u3009, which provides a basis to use projected features instead of original features. Yet, calculating the exact expectation is computationally intractable, so, the random parameters, h and s are fixed during training and evaluation. This practical choice leads to the second. The projected dimension of compact bilinear pooling should be large enough to minimize the bias from the fixed parameters. Practical choices are 10K and 16K for 512 and 4096-dimensional inputs, respectively (Gao et al., 2015; Fukui et al., 2016). Though, these compacted dimensions are reduced ones by two orders of magnitude compared with full bilinear pooling, such high-dimensional features could be a bottleneck for computationally complex models.\nWe propose low-rank bilinear neural networks using Hadamard product, which is commonly used in various scientific computing frameworks as one of tensor operations. The proposed method factors a three-dimensional weight tensor for bilinear pooling into three two-dimensional weight matrices, which enforces the rank of the weight tensor to be low-rank. As a result, two input feature vectors linearly projected by two weight matrices, respectively, are computed by Hadamard product, then, followed by a linear projection using the third weight matrix. For example, the projected vector z is represented by WTz (W T xx \u25e6WTyy), where \u25e6 denotes Hadamard product or element-wise multiplication.\nWe also explore to add non-linearity using non-linear activation functions into the low-rank bilinear neural networks, and shortcut connections inspired by deep residual learning (He et al., 2015). Then, we show that it becomes one-learning block of Multimodal Residual Networks (Kim et al., 2016b) as a low-rank bilinear model, yet, this interpretation has not be done.\nOur contributions are as follows: First, we propose low-rank bilinear neural networks to approximate full bilinear model. Second, Multimodal Low-rank Bilinear Attention Networks (MLB) having an attention mechanism using low-rank bilinear pooling is proposed for visual question-answering tasks. MLB achieves a new state-of-the-art performance, and has a better parsimonious property. Finally, ablation studies to explore alternative choices, e.g. network depth, non-linear functions, and shortcut connections, are conducted."}, {"heading": "2 LOW-RANK BILINEAR MODEL", "text": "Bilinear models use a quadratic expansion of linear transformation considering every pairs of features.\nfi = N\u2211 j=1 M\u2211 k=1 wijkxjyk + bi = x TWiy + bi (1)\nwhere x and y are input vectors, Wi \u2208 RN\u00d7M is a weight matrix for the output fi, and bi is a bias for the output fi. Notice that the number of parameters is L\u00d7 (N \u00d7M + 1) including a bias vector b, where L is the number of output features.\nPirsiavash et al. (2009) suggest a low-rank bilinear method to reduce the rank of the weight matrix Wi to have less number of parameters for regularization. They rewrite the weight matrix as Wi = UiV T i where U \u2208 RN\u00d7d and V \u2208 RM\u00d7d, which imposes a restriction on the rank of Wi to be at most d \u2264 min(N,M). Based on this idea, fi can be rewritten as follows:\nfi = x TWiy + bi = x TUiV T i y + bi = 1 T (UTi x \u25e6VTi y) + bi (2) where 1 \u2208 Rd denotes a column vector of ones, and \u25e6 denotes Hadamard product. Still, we need two third-order tensors, U and V, for a feature vector f , whose elements are {fi}. To reduce the order of the weight tensors by one, we replace 1 with P \u2208 Rd\u00d7c and bi with b \u2208 Rc to get a projected feature vector f \u2208 Rc. Then, we get:\nf = PT (UTx \u25e6VTy) + b (3) where d and c are hyperparameters to decide the dimension of joint embeddings and the output dimension of low-rank bilinear models, respectively."}, {"heading": "3 LOW-RANK BILINEAR NEURAL NETWORKS", "text": "A low-rank bilinear model in Equation 3 can be implemented using two linear mappings without biases for embedding two input vectors, Hadamard product to learn joint representations in a multiplicative way, and a linear mapping with a bias to project the joint representations into an output vector for a given output dimension. Now, we discuss possible variations of low-rank bilinear models inspired by studies of neural networks."}, {"heading": "3.1 FULL MODEL", "text": "In Equation 3, linear projections, U and V , can have their own bias vectors. As a result, linear models for each input vectors, x and y, are integrated in a additive form, called as full model for linear regression in statistics:\nf = PT ( (UTx + bx) \u25e6 (VTy + by) ) + b\n= PT (UTx \u25e6VTy) + U\u2032Tx + V\u2032Ty + b\u2032. (4)\nHere, U\u2032T = diag(by) \u00b7UT , V\u2032T = diag(bx) \u00b7VT , and b\u2032 = b + PT (bx \u25e6 by)."}, {"heading": "3.2 NONLINEAR ACTIVATION", "text": "Applying non-linear activation functions may help to increase representative capacity of model. The first candidate is to apply non-linear activation functions right after linear mappings for input vectors.\nf = PT ( \u03c3(UTx) \u25e6 \u03c3(VTy) ) + b (5)\nwhere \u03c3 denotes an arbitrary non-linear activation function, which maps any real values into a finite interval, e.g. sigmoid or tanh. If two inputs come from different modalities, statistics of two inputs may be quite different from each other, which may result an interference. Since the gradient with respect to each input is directly dependent on the other input in Hadamard product of two inputs.\nAdditional applying an activation function after the Hadamard product is not appropriate, since activation functions doubly appear in calculating gradients. However, applying the activation function only after the Hadamard product would be alternative choice (We explore this option in Section 5) as follows:\nf = PT\u03c3 ( UTx \u25e6VTy ) + b. (6)"}, {"heading": "3.3 SHORTCUT CONNECTION", "text": "When we apply two previous techniques, full model and non-linear activation, linear models of two inputs are nested by the non-linear activation functions. To avoid this unfortunate situation, we add shortcut connections as explored in residual learning (He et al., 2015).\nf = PT ( \u03c3(UTx) \u25e6 \u03c3(VTy) ) + hx(x) + hy(y) + b (7)\nwhere hx and hy are shortcut mappings. For linear projection, the shortcut mappings are linear mappings. Notice that this formulation is a generalized form of the one-block layered MRN (Kim et al., 2016b). Though, notice that, the shortcut connections are not used in our proposed model, as explained in Section 6."}, {"heading": "4 MULTIMODAL LOW-RANK BILINEAR ATTENTION NETWORKS", "text": "In this section, we apply low-rank bilinear pooling to propose an attention mechanism for visual question-answering tasks, based on the interpretation of previous section. We assumed that inputs are a question embedding vector q and a set of visual feature vectors F over S \u00d7 S lattice space."}, {"heading": "4.1 LOW-RANK BILINEAR POOLING IN ATTENTION MECHANISM", "text": "Attention mechanism uses an attention probability distribution \u03b1 over S \u00d7 S lattice space. If \u03b1 is a coefficient vector for the linear combination of multiple visual features over the same lattice\nspace, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015). Here, using the low-rank bilinear pooling, the attention probability distribution \u03b1 for the soft attention is defined as\n\u03b1 = softmax ( PT\u03b1 ( \u03c3(UTqq \u00b7 1T ) \u25e6 \u03c3(VTFFT ) )) (8)\nwhere \u03b1 \u2208 RG\u00d7S2 , P\u03b1 \u2208 Rd\u00d7G, \u03c3 is a hyperbolic tangent function, Uq \u2208 RN\u00d7d, q \u2208 RN , 1 \u2208 RS2 , VF \u2208 RM\u00d7d, and F \u2208 RS\n2\u00d7M . If G > 1, multiple glimpses are explicitly expressed as in Fukui et al. (2016), conceptually similar to Jaderberg et al. (2015). And, the softmax function applies to each row vector of \u03b1. The bias terms in the equations in this section are omitted for simplicity. Refer to full model in Section 3.1."}, {"heading": "4.2 MULTIMODAL LOW-RANK BILINEAR ATTENTION NETWORKS", "text": "Attended visual feature v\u0302 is a linear combination of visual feature vectors Fi with coefficients \u03b1g,i. Each attention probability distribution \u03b1g is for a glimpse g. For G > 1, v\u0302 is the concatenation of resulting vectors v\u0302g .\nv\u0302 =\nGn\ng=1 S2\u2211 s=1 \u03b1g,sFs (9)\nwhere f denotes concatenation of vectors.\nThe posterior probability distribution of answers is an output of a softmax function, whose input is the result of another low-rank bilinear pooling of q and v\u0302 as\np(a|q,F; \u0398) = softmax ( PTo ( \u03c3(WTqq) \u25e6 \u03c3(VTv\u0302 v\u0302) )) (10)\nand the predicted answer a\u0302 is\na\u0302 = arg max a\u2208\u2126\np(a|q,F; \u0398) (11)\nwhere \u2126 is a set of candidate answers and \u0398 is an aggregation of entire model parameters."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we conduct six experiments to select the proposed model, Multimodal Low-rank Bilinear Attention Networks (MLB). Each experiment controls other factors except one factor to assess the effect on accuracies. Based on MRN (Kim et al., 2016b), we start our assessments with an initial option of G = 1 and shortcut connections of MRN, called as Multimodal Attention Residual Networks (MARN). Notice that we use one embeddings for each visual feature for better performance, based on our preliminary experiment (not shown). We attribute this choice to the attention mechanism for visual features, which provides more capacity to learn visual features. We use the same hyper-parameters of MRN (Kim et al., 2016b), without any explicit mention of this.\nThe VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used. Validation is performed on the VQA test-dev split, and model comparison is based on the results of the VQA test-standard split. For the comprehensive reviews of VQA tasks, please refer to Wu et al. (2016a) and Kafle & Kanan (2016a).\nNumber of Learning Blocks Kim et al. (2016b) argue that three-block layered MRN shows the best performance among one to four-block layered models, taking advantage of residual learning. However, we speculate that an introduction of attention mechanism makes deep networks hard to optimize. Therefore, we explore the number of learning blocks of MARN, which have an attention mechanism using low-rank bilinear pooling.\nNumber of Glimpses Fukui et al. (2016) show that the attention mechanism of two glimpses was an optimal choice. In a similar way, we assess one, two, and four-glimpse models.\nNon-Linearity We assess three options applying non-linearity on low-rank bilinear pooling, vanilla, before Hadamard product as in Equation 5, and after Hadamard product as in Equation 6.\nAnswer Sampling VQA (Antol et al., 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al., 2016) dataset has a single answer for each question. Since difficult or ambiguous questions may have divided answers, the probabilistic sampling from the distribution of answers can be utilized to optimize for the multiple answers. An instance 1 can be found in Fukui et al. (2016). We simplify the procedure as follows:\np(a1) = { |a1|/\u03a3i|ai|, if |a1| \u2265 3 0, otherwise\n(12)\np(a0) = 1\u2212 p(a1) (13) where |ai| denotes the number of unique answer ai in a set of multiple answers, a0 denotes a mode, which is the most frequent answer, and a1 denotes the secondly most frequent answer. We define the divided answers as having at least three answers which are the secondly frequent one, for the evaluation metric of VQA (Antol et al., 2015),\naccuracy(ak) = min (|ak|/3, 1) . (14) 1https://github.com/akirafukui/vqa-mcb/blob/5fea8/train/multi_att_2_\nglove/vqa_data_provider_layer.py#L130\nThe rate of the divided answers is approximately 16.40%, and only 0.23% of questions have more than two divided answers in VQA dataset. We assume that it eases the difficulty of convergence without severe degradation of performance.\nShortcut Connection The performance contribution of shortcut connections for residual learning is explored. This experiment is conducted based on the observation of the competitive performance of single-block layered model. Since the usefulness of shortcut connections is linked to the network depth (He et al., 2015).\nData Augmentation The data augmentation with Visual Genome (Krishna et al., 2016) question answer annotations is explored. Visual Genome (Krishna et al., 2016) originally provides 1.7 Million visual question answer annotations. After aligning to VQA, the valid number of question-answering pairs for training is 837,298, which is for distinct 99,280 images."}, {"heading": "6 RESULTS", "text": "The six experiments are conducted sequentially to narrow down architectural choices. Each experiment determines experimental variables one by one. Refer to Table 1, which has six sectors divided by mid-rules."}, {"heading": "6.1 SIX EXPERIMENT RESULTS", "text": "Number of Learning Blocks Though, MRN (Kim et al., 2016b) has the three-block layered architecture, MARN shows the best performance with two-block layered models (63.92%). For the multiple glimpse models in the next experiment, we choose one-block layered model for its simplicity to extend, and competitive performance (63.79%).\nNumber of Glimpses Compared with the results of Fukui et al. (2016), four-glimpse MARN (64.61%) is better than other comparative models. However, for a parsimonious choice, two-glimpse MARN (64.53%) is chosen for later experiments. We speculate that multiple glimpses are one of key factors for the competitive performance of MCB (Fukui et al., 2016), based on a large margin in accuracy, compared to one-glimpse MARN (63.79%).\nNon-Linearity The results confirm that activation functions are useful to improve performances. Surprisingly, there is no empirical difference between two options, before-Hadamard product and after-Hadamard product. This result may build a bridge to relate with studies on multiplicative integration with recurrent neural networks (Wu et al., 2016c).\nAnswer Sampling Sampled answers (64.80%) result better performance than mode answers (64.53%). It confirms that the distribution of answers from annotators can be used to improve the performance. However, the number of multiple answers is usually limited due to the cost of data collection.\nShortcut Connection Though, MRN (Kim et al., 2016b) effectively uses shortcut connections to improve model performance, one-block layered MARN shows better performance without the shortcut connection. In other words, the residual learning is not used in our proposed model, MLB. It seems that there is a trade-off between introducing attention mechanism and residual learning. We leave a careful study on this trade-off for future work.\nData Augmentation Data augmentation using Visual Genome (Krishna et al., 2016) question answer annotations significantly improves the performance by 0.76% in accuracy for VQA test-dev split. Especially, the accuracy of others (ETC)-type answers is notably improved from the data augmentation."}, {"heading": "6.2 COMPARISON WITH STATE-OF-THE-ART", "text": "The comparison with other single models on VQA test-standard is shown in Table 2. The overall accuracy of our model is approximately 1.9% above the next best model (Noh & Han, 2016) on the Open-Ended task of VQA. The major improvements are from yes-or-no (Y/N) and others (ETC)type answers. In Table 3, we also report the accuracy of our ensemble model to compare with other ensemble models on VQA test-standard, which won 1st to 5th places in VQA Challenge 20162. We beat the previous state-of-the-art with a margin of 0.42%."}, {"heading": "7 RELATED WORKS", "text": ""}, {"heading": "7.1 COMPACT BILINEAR POOLING", "text": "Compact bilinear pooling (Gao et al., 2015) approximates full bilinear pooling using a samplingbased computation, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013):\n\u03a8(x\u2297 y, h, s) = \u03a8(x, h, s) \u2217\u03a8(y, h, s) (15) = FFT\u22121(FFT(\u03a8(x, h, s) \u25e6 FFT(\u03a8(y, h, s)) (16)\n2http://visualqa.org/challenge.html\nwhere \u2297 denotes outer product, \u2217 denotes convolution, \u03a8(v, h, s)i := \u2211 j:hj=i\nsj \u00b7 vj , FFT denotes Fast Fourier Transform, d denotes an output dimension, x, y, h, s \u2208 Rn, x and y are inputs, and h and s are random variables. hi is sampled from {1, ..., d}, and si is sampled from {\u22121, 1}, then, both random variables are fixed for further usage. Although the dimensions of x and y are different from each other, one can easily generalize compact bilinear pooling for multimodal learning (Fukui et al., 2016).\nMCB (Fukui et al., 2016) for VQA tasks needs to set the dimension of output d to 16,000, to reduce the bias induced by the fixed random variables h and s. As a result, the majority of model parameters (16K \u00d7 3K = 48M) are concentrated on the last fully connected layer, which makes a fan-out structure. So, the total number of parameters of MCB is highly sensitive to the number of classes, which is approximately 69.2M for MCB+att, and 70.5M for MCB+att+GloVe. Yet, the total number of parameters of our proposed model (MLB) is 51.9M, which is more robust to the number of classes having d = 1,200, which has a similar role in model architecture."}, {"heading": "7.2 MULTIMODAL RESIDUAL NETWORKS", "text": "MRN (Kim et al., 2016b) is an implicit attentional model which does not have any explicit attention mechanism. Instead, they suggest a novel method to visualize implicit attention induced by Hadamard product, even though input visual features are the outputs of a fully-connected layer, which do not have explicit spatial information.\nF (k)(q,v) = \u03c3(W(k)q q) \u25e6 \u03c3(W (k) 2 \u03c3(W (k) 1 v)) (17)\nHL(q,v) = Wq\u2032q + L\u2211 l=1 WF(l)F (l)(Hl\u22121,v) (18)\nwhere W\u2217 are parameter matrices, L is the number of learning blocks, H0 = q, Wq\u2032 = \u03a0Ll=1W (l) q\u2032 , and WF(l) = \u03a0Lm=l+1W (m) q\u2032 .\nHowever, an explicit attention mechanism allows the use of lower-level visual features than fullyconnected layers, and, more importantly, spatially selective learning. Recent state-of-the-art methods use a variant of an explicit attention mechanism in their models (Lu et al., 2016; Noh & Han, 2016; Fukui et al., 2016). Note that shortcut connections of MRN are not used in the proposed Multimodal Low-rank Bilinear (MLB) model. Since, it does not have any performance gain due to not stacking multiple layers in MLB. We leave the study of residual learning for MLB for future work, which may leverage the excellency of bilinear models as suggested in Wu et al. (2016a)."}, {"heading": "8 CONCLUSIONS", "text": "We suggest a low-rank bilinear pooling method to replace compact bilinear pooling, which has a fan-out structure, and needs complex computations. Low-rank bilinear pooling has a flexible structure and a better parsimonious property using linear mapping and Hadamard product, compared to compact bilinear pooling. We achieve new state-of-the-art results on the VQA dataset using a similar architecture of Fukui et al. (2016), replacing compact bilinear pooling with low-rank bilinear pooling. We believe our method could be applicable to other bilinear learning tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Patrick Emaase for helpful comments and editing. This work was supported by Naver Corp. and partly by the Korea government (IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR)."}, {"heading": "A EXPERIMENT DETAILS", "text": ""}, {"heading": "A.1 PREPROCESSING", "text": "We follow the preprocessing procedure of Kim et al. (2016b). Here, we remark some details of it, and changes."}, {"heading": "A.1.1 QUESTION EMBEDDING", "text": "The 90.45% of questions for the 2K-most frequent answers are used. The vocabulary size of questions is 15,031. GRU (Cho et al., 2014) is used for question embedding. Based on earlier studies (Noh et al., 2015; Kim et al., 2016b), a word embedding matrix and a GRU are initialized with Skip-thought Vector pre-trained model (Kiros et al., 2015). As a result, question vectors have 2,400 dimensions.\nFor efficient computation of variable-length questions, Kim et al. (2016a) is used for the GRU. Moreover, for regularization, Bayesian Dropout (Gal, 2015) which is implemented in Le\u0301onard et al. (2015) is applied while training.\nA.2 VISION EMBEDDING\nResNet-152 networks (He et al., 2015) are used for feature extraction. The dimensionality of an input image is 3\u00d7448\u00d7448. The outputs of the last convolution layer is used, which have 2, 048\u00d714\u00d714 dimensions."}, {"heading": "A.3 HYPERPARAMETERS", "text": "The hyperparameters used in MLB of Table 2 are described in Table 4. The batch size is 200, and the number of iterations is fixed to 250K. For data augmented models, a simplified early stopping is used, starting from 250K to 350K-iteration for every 25K iterations (250K, 275K, 300K, 325K, and 350K; at most five points) to avoid exhaustive submissions to VQA test-dev evaluation server."}, {"heading": "A.4 MODEL SCHEMA", "text": "Figure 1 shows a schematic diagram of MLB, where \u25e6 denotes Hadamard product, and \u03a3 denotes a linear combination of visual feature vectors using coefficients, which is the output of softmax function. If G > 1, the softmax function is applied to each row vectors of an output matrix (Equation 8), and we concatenate the resulting vectors of the G linear combinations (Equation 9)."}, {"heading": "A.5 ENSEMBLE OF SEVEN MODELS", "text": "The test-dev results for individual models consisting of our ensemble model is presented in Table 5."}, {"heading": "LinearLinear", "text": ""}, {"heading": "MODEL GLIMPSE ALL Y/N NUM ETC", "text": ""}, {"heading": "B OTHER RELATED WORKS", "text": ""}, {"heading": "B.1 HIGHER-ORDER BOLTZMANN MACHINES", "text": "A similar model can be found in a study of Higher-Order Boltzmann Machines (Memisevic & Hinton, 2007; 2010). They suggest a factoring method for the three-way energy function to capture correlations among input, output, and hidden representations.\n\u2212E(y,h;x) = \u2211 f (\u2211 i xiw x if )(\u2211 j yjw y jf )(\u2211 k hkw h kf ) + \u2211 k whkhk + \u2211 j wyj yj\n= ( xTWx \u25e6 yTWy \u25e6 hTWh ) 1+ hTwh + yTwy (19)\nSetting aside of bias terms, the I \u00d7 J \u00d7K parameter tensor of unfactored Higher-Order Boltzmann Machines is replaced with three matrices, Wx \u2208 RI\u00d7F , Wy \u2208 RJ\u00d7F , and Wh \u2208 RK\u00d7F ."}, {"heading": "B.2 MULTIPLICATIVE INTEGRATION WITH RECURRENT NEURAL NETWORKS", "text": "Most of recurrent neural networks, including vanilla RNNs, Long Short Term Memory networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (Cho et al., 2014), share a common expression as follows:\n\u03c6(Wx + Uh + b) (20)\nwhere \u03c6 is a non-linear function, W \u2208 Rd\u00d7n, x \u2208 Rn, U \u2208 Rd\u00d7m, h \u2208 Rm, and b \u2208 Rd is a bias vector. Note that, usually, x is an input state vector and h is an hidden state vector in recurrent neural networks.\nWu et al. (2016c) propose a new design to replace the additive expression with a multiplicative expression using Hadamard product as\n\u03c6(Wx \u25e6Uh + b). (21)\nMoreover, a general formulation of this multiplicative integration can be described as\n\u03c6(\u03b1 \u25e6Wx \u25e6Uh + Wx \u25e6 \u03b21 + Uh \u25e6 \u03b22 + b) (22)\nwhich is reminiscent of full model in Section 3.1."}], "references": [{"title": "Learning to Compose Neural Networks for Question Answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "arXiv preprint arXiv:1601.01705,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "International Conference on Computer Vision,", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Finding frequent items in data streams", "author": ["Moses Charikar", "Kevin Chen", "Martin Farach-Colton"], "venue": "In International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "Charikar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2002}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": "arXiv preprint arXiv:1606.01847,", "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Compact Bilinear Pooling", "author": ["Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1511.06062,", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A Focused Dynamic Attention Model for Visual Question Answering", "author": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "venue": "arXiv preprint arXiv:1604.01485,", "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "Spatial Transformer Networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "arXiv preprint arXiv:1610.01465,", "citeRegEx": "Kafle and Kanan.,? \\Q2016\\E", "shortCiteRegEx": "Kafle and Kanan.", "year": 2016}, {"title": "Answer-Type Prediction for Visual Question Answering", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kafle and Kanan.,? \\Q2016\\E", "shortCiteRegEx": "Kafle and Kanan.", "year": 2016}, {"title": "TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing", "author": ["Jin-Hwa Kim", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "In Proceedings of KIIS Spring Conference,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Multimodal Residual Learning for Visual QA", "author": ["Jin-Hwa Kim", "Sang-Woo Lee", "Dong-Hyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "arXiv preprint arXiv:1606.01455,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Skip-Thought Vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Recurrent Library for Torch", "author": ["Nicholas L\u00e9onard", "Sagar Waghmare", "Yang Wang", "Jin-Hwa Kim"], "venue": "arXiv preprint arXiv:1511.07889,", "citeRegEx": "L\u00e9onard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "L\u00e9onard et al\\.", "year": 2015}, {"title": "Bilinear CNN Models for Fine-grained Visual Recognition", "author": ["Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji"], "venue": "In IEEE International Conference on Computer Vision, pp", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": "https://github.com/VT-vision-lab/VQA_LSTM_CNN,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1606.00061,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "arXiv preprint arXiv:1605.02697,", "citeRegEx": "Malinowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2016}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey E Hinton"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Memisevic and Hinton.,? \\Q2007\\E", "shortCiteRegEx": "Memisevic and Hinton.", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order Boltzmann machines", "author": ["Roland Memisevic", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Memisevic and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic and Hinton.", "year": 2010}, {"title": "Training Recurrent Answering Units with Joint Loss Minimization for VQA", "author": ["Hyeonwoo Noh", "Bohyung Han"], "venue": "arXiv preprint arXiv:1606.03647,", "citeRegEx": "Noh and Han.,? \\Q2016\\E", "shortCiteRegEx": "Noh and Han.", "year": 2016}, {"title": "Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": "arXiv preprint arXiv:1511.05756,", "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Bilinear classifiers for visual recognition", "author": ["Hamed Pirsiavash", "Deva Ramanan", "Charless C. Fowlkes"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pirsiavash et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pirsiavash et al\\.", "year": 2009}, {"title": "Separating style and content with bilinear models", "author": ["Joshua B Tenenbaum", "William T Freeman"], "venue": "Neural computation,", "citeRegEx": "Tenenbaum and Freeman.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum and Freeman.", "year": 2000}, {"title": "Visual Question Answering: A Survey of Methods and Datasets", "author": ["Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "On Multiplicative Integration with Recurrent Neural Networks. 2016c", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Xu and Saenko.,? \\Q2016\\E", "shortCiteRegEx": "Xu and Saenko.", "year": 2016}, {"title": "Show, Attend and Tell : Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Aaron Courville", "Richard S Zemel", "Yoshua Bengio"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "arXiv preprint arXiv:1511.02274,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Simple Baseline for Visual Question Answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "2016a) is used for the GRU. Moreover, for regularization, Bayesian Dropout (Gal, 2015) which is implemented in L\u00e9onard et al", "author": ["Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2015\\E", "shortCiteRegEx": "Kim", "year": 2015}, {"title": "2016c) propose a new design to replace the additive expression with a multiplicative expression using Hadamard product", "author": ["Wu"], "venue": null, "citeRegEx": "Wu,? \\Q2016\\E", "shortCiteRegEx": "Wu", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Recently, a successful application of this technique is used for fine-grained visual recognition (Lin et al., 2015).", "startOffset": 97, "endOffset": 115}, {"referenceID": 7, "context": "Compact bilinear pooling (Gao et al., 2015) reduces the quadratic expansion of dimensionality by two orders of magnitude, retaining the performance of the full bilinear pooling.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": "This approximation uses sampling-based computation, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013), which utilizes an useful property that \u03a8(x\u2297 y, h, s) = \u03a8(x, h, s) \u2217\u03a8(y, h, s), which means the projection of outer product of two vectors is the convolution of two projected vectors.", "startOffset": 77, "endOffset": 119}, {"referenceID": 7, "context": "Practical choices are 10K and 16K for 512 and 4096-dimensional inputs, respectively (Gao et al., 2015; Fukui et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 5, "context": "Practical choices are 10K and 16K for 512 and 4096-dimensional inputs, respectively (Gao et al., 2015; Fukui et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 8, "context": "We also explore to add non-linearity using non-linear activation functions into the low-rank bilinear neural networks, and shortcut connections inspired by deep residual learning (He et al., 2015).", "startOffset": 179, "endOffset": 196}, {"referenceID": 28, "context": "Pirsiavash et al. (2009) suggest a low-rank bilinear method to reduce the rank of the weight matrix Wi to have less number of parameters for regularization.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "To avoid this unfortunate situation, we add shortcut connections as explored in residual learning (He et al., 2015).", "startOffset": 98, "endOffset": 115}, {"referenceID": 2, "context": "space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 35, "context": ", 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015).", "startOffset": 111, "endOffset": 128}, {"referenceID": 2, "context": "space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015). Here, using the low-rank bilinear pooling, the attention probability distribution \u03b1 for the soft attention is defined as \u03b1 = softmax ( P\u03b1 ( \u03c3(Uqq \u00b7 1 ) \u25e6 \u03c3(V FF ) )) (8) where \u03b1 \u2208 RG\u00d7S2 , P\u03b1 \u2208 Rd\u00d7G, \u03c3 is a hyperbolic tangent function, Uq \u2208 RN\u00d7d, q \u2208 R , 1 \u2208 RS2 , VF \u2208 RM\u00d7d, and F \u2208 R \u00d7M . If G > 1, multiple glimpses are explicitly expressed as in Fukui et al. (2016), conceptually similar to Jaderberg et al.", "startOffset": 52, "endOffset": 566}, {"referenceID": 2, "context": "space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015). Here, using the low-rank bilinear pooling, the attention probability distribution \u03b1 for the soft attention is defined as \u03b1 = softmax ( P\u03b1 ( \u03c3(Uqq \u00b7 1 ) \u25e6 \u03c3(V FF ) )) (8) where \u03b1 \u2208 RG\u00d7S2 , P\u03b1 \u2208 Rd\u00d7G, \u03c3 is a hyperbolic tangent function, Uq \u2208 RN\u00d7d, q \u2208 R , 1 \u2208 RS2 , VF \u2208 RM\u00d7d, and F \u2208 R \u00d7M . If G > 1, multiple glimpses are explicitly expressed as in Fukui et al. (2016), conceptually similar to Jaderberg et al. (2015). And, the softmax function applies to each row vector of \u03b1.", "startOffset": 52, "endOffset": 615}, {"referenceID": 1, "context": "The VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 17, "context": ", 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used.", "startOffset": 114, "endOffset": 136}, {"referenceID": 1, "context": "The VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used. Validation is performed on the VQA test-dev split, and model comparison is based on the results of the VQA test-standard split. For the comprehensive reviews of VQA tasks, please refer to Wu et al. (2016a) and Kafle & Kanan (2016a).", "startOffset": 17, "endOffset": 382}, {"referenceID": 1, "context": "The VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used. Validation is performed on the VQA test-dev split, and model comparison is based on the results of the VQA test-standard split. For the comprehensive reviews of VQA tasks, please refer to Wu et al. (2016a) and Kafle & Kanan (2016a).", "startOffset": 17, "endOffset": 408}, {"referenceID": 14, "context": "Number of Learning Blocks Kim et al. (2016b) argue that three-block layered MRN shows the best performance among one to four-block layered models, taking advantage of residual learning.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "Number of Glimpses Fukui et al. (2016) show that the attention mechanism of two glimpses was an optimal choice.", "startOffset": 19, "endOffset": 39}, {"referenceID": 5, "context": "Since Fukui et al. (2016) only report the accuracy of the ensemble model on the test-standard, the test-dev results of their single models are included in the last sector.", "startOffset": 6, "endOffset": 26}, {"referenceID": 5, "context": "76 MCB+Att (Fukui et al., 2016) 69.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "8 MCB+Att+GloVe (Fukui et al., 2016) 70.", "startOffset": 16, "endOffset": 36}, {"referenceID": 5, "context": "6 MCB+Att+Glove+VG (Fukui et al., 2016) 70.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "Answer Sampling VQA (Antol et al., 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 17, "context": ", 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al., 2016) dataset has a single answer for each question.", "startOffset": 91, "endOffset": 113}, {"referenceID": 1, "context": "Answer Sampling VQA (Antol et al., 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al., 2016) dataset has a single answer for each question. Since difficult or ambiguous questions may have divided answers, the probabilistic sampling from the distribution of answers can be utilized to optimize for the multiple answers. An instance 1 can be found in Fukui et al. (2016). We simplify the procedure as follows:", "startOffset": 21, "endOffset": 423}, {"referenceID": 1, "context": "We define the divided answers as having at least three answers which are the secondly frequent one, for the evaluation metric of VQA (Antol et al., 2015), accuracy(ak) = min (|ak|/3, 1) .", "startOffset": 133, "endOffset": 153}, {"referenceID": 37, "context": "iBOWIMG (Zhou et al., 2015) 55.", "startOffset": 8, "endOffset": 27}, {"referenceID": 26, "context": "97 DPPnet (Noh et al., 2015) 57.", "startOffset": 10, "endOffset": 28}, {"referenceID": 20, "context": "69 Deeper LSTM+Normalized CNN (Lu et al., 2015) 58.", "startOffset": 30, "endOffset": 47}, {"referenceID": 22, "context": "48 Ask Your Neuron (Malinowski et al., 2016) 58.", "startOffset": 19, "endOffset": 44}, {"referenceID": 36, "context": "32 SAN (Yang et al., 2015) 58.", "startOffset": 7, "endOffset": 26}, {"referenceID": 0, "context": "42 D-NMN (Andreas et al., 2016) 59.", "startOffset": 9, "endOffset": 31}, {"referenceID": 10, "context": "83 FDA (Ilievski et al., 2016) 59.", "startOffset": 7, "endOffset": 30}, {"referenceID": 33, "context": "56 DMN+ (Xiong et al., 2016) 60.", "startOffset": 8, "endOffset": 28}, {"referenceID": 21, "context": "33 HieCoAtt (Lu et al., 2016) 62.", "startOffset": 12, "endOffset": 29}, {"referenceID": 8, "context": "Since the usefulness of shortcut connections is linked to the network depth (He et al., 2015).", "startOffset": 76, "endOffset": 93}, {"referenceID": 17, "context": "Data Augmentation The data augmentation with Visual Genome (Krishna et al., 2016) question answer annotations is explored.", "startOffset": 59, "endOffset": 81}, {"referenceID": 17, "context": "Visual Genome (Krishna et al., 2016) originally provides 1.", "startOffset": 14, "endOffset": 36}, {"referenceID": 5, "context": "We speculate that multiple glimpses are one of key factors for the competitive performance of MCB (Fukui et al., 2016), based on a large margin in accuracy, compared to one-glimpse MARN (63.", "startOffset": 98, "endOffset": 118}, {"referenceID": 5, "context": "Number of Glimpses Compared with the results of Fukui et al. (2016), four-glimpse MARN (64.", "startOffset": 48, "endOffset": 68}, {"referenceID": 17, "context": "Data Augmentation Data augmentation using Visual Genome (Krishna et al., 2016) question answer annotations significantly improves the performance by 0.", "startOffset": 56, "endOffset": 78}, {"referenceID": 5, "context": "26 MCB (Fukui et al., 2016) 66.", "startOffset": 7, "endOffset": 27}, {"referenceID": 1, "context": "29 Human (Antol et al., 2015) 83.", "startOffset": 9, "endOffset": 29}, {"referenceID": 7, "context": "Compact bilinear pooling (Gao et al., 2015) approximates full bilinear pooling using a samplingbased computation, Tensor Sketch Projection (Charikar et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": ", 2015) approximates full bilinear pooling using a samplingbased computation, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013): \u03a8(x\u2297 y, h, s) = \u03a8(x, h, s) \u2217\u03a8(y, h, s) (15) = FFT\u22121(FFT(\u03a8(x, h, s) \u25e6 FFT(\u03a8(y, h, s)) (16) http://visualqa.", "startOffset": 103, "endOffset": 145}, {"referenceID": 5, "context": "Although the dimensions of x and y are different from each other, one can easily generalize compact bilinear pooling for multimodal learning (Fukui et al., 2016).", "startOffset": 141, "endOffset": 161}, {"referenceID": 5, "context": "MCB (Fukui et al., 2016) for VQA tasks needs to set the dimension of output d to 16,000, to reduce the bias induced by the fixed random variables h and s.", "startOffset": 4, "endOffset": 24}, {"referenceID": 21, "context": "Recent state-of-the-art methods use a variant of an explicit attention mechanism in their models (Lu et al., 2016; Noh & Han, 2016; Fukui et al., 2016).", "startOffset": 97, "endOffset": 151}, {"referenceID": 5, "context": "Recent state-of-the-art methods use a variant of an explicit attention mechanism in their models (Lu et al., 2016; Noh & Han, 2016; Fukui et al., 2016).", "startOffset": 97, "endOffset": 151}, {"referenceID": 5, "context": ", 2016; Noh & Han, 2016; Fukui et al., 2016). Note that shortcut connections of MRN are not used in the proposed Multimodal Low-rank Bilinear (MLB) model. Since, it does not have any performance gain due to not stacking multiple layers in MLB. We leave the study of residual learning for MLB for future work, which may leverage the excellency of bilinear models as suggested in Wu et al. (2016a).", "startOffset": 25, "endOffset": 396}, {"referenceID": 5, "context": "We achieve new state-of-the-art results on the VQA dataset using a similar architecture of Fukui et al. (2016), replacing compact bilinear pooling with low-rank bilinear pooling.", "startOffset": 91, "endOffset": 111}], "year": 2017, "abstractText": "Bilinear models provide rich representations compared to linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear neural networks using Hadamard product (element-wise multiplication), commonly implemented in many scientific computing frameworks. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, having a better parsimonious property.", "creator": "LaTeX with hyperref package"}}}