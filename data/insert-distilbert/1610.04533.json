{"id": "1610.04533", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "A Comprehensive Comparative Study of Word and Sentence Similarity Measures", "abstract": "sentence similarity is considered mostly the basis of many natural language tasks such as information retrieval, question of answering and text summarization. the semantic meaning between compared relative text fragments is guessed based on the words semantic confidence features and their relationships. this article reviews a set of word and sentence similarity measures and compares them on benchmark descriptive datasets. on the studied datasets, results showed that hybrid semantic measures perform better than both functional knowledge and corpus based credibility measures.", "histories": [["v1", "Wed, 17 Feb 2016 19:33:47 GMT  (444kb)", "http://arxiv.org/abs/1610.04533v1", "7 pages,4 figures"]], "COMMENTS": "7 pages,4 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["issa atoum", "ahmed otoom", "narayanan kulathuramaiyer"], "accepted": false, "id": "1610.04533"}, "pdf": {"name": "1610.04533.pdf", "metadata": {"source": "CRF", "title": "A Comprehensive Comparative Study of Word and Sentence Similarity Measures", "authors": ["Issa Atoum", "Ahmed Otoom", "Narayanan Kulathuramaiyer"], "emails": ["Issa.Atoum@wise.edu.jo", "aotoom@rjaf.mil.jo", "nara@fit.unimas.my"], "sections": [{"heading": null, "text": "language tasks such as information retrieval, question answering and text summarization. The semantic meaning between compared text fragments is based on the words\u2019 semantic features and their relationships. This article reviews a set of word and sentence similarity measures and compares them on benchmark datasets. On the studied datasets, results showed that hybrid semantic measures perform better than both knowledge and corpus based measures.\nGeneral Terms Semantic Similarity, Natural Language Processing, Computational Linguistics, Text Similarity\nKeywords Word Similarity, Sentence Similarity, Corpus Measures, Knowledge Measures, Hybrid Measures, Text Similarity"}, {"heading": "1. INTRODUCTION", "text": "Semantic similarity finds a resemblance between the related textual terms. Words are considered semantically similar or related if they have common relationships. For example, food and salad are semantically similar; obviously salad is a type of food. Also, fork and food are related; undoubtedly a fork is used to take food. Resnik illustrated that word similarity is a subcase of word relatedness[1].\nThe word similarity is the foundation of the sentence similarity measures. A Sentence similarity method measures the semantics of group of terms in the text fragments. It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10]. Furthermore, semantic similarity is also used in other domains; in medical domain to extract protein functions from biomedical literature [11] and in software quality[12]\u2013[14] to find common software attributes.\nGenerally, sentence similarity methods can be classified as corpus based, knowledge based and hybrid methods. Corpus based methods depend on building word frequencies from specific corpus. In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20]. In other words, if the model\n(i.e. corpus model) was built for news text, it usually performs poorly in another domain such computer science text.\nThe knowledge based methods usually employ dictionary information such as path and/or depth lengths between compared words to signify relatedness. These methods suffer from the limited number of general dictionary words that might not suit specific domains. Most knowledge based measures depend on WordNet[21]. WordNet is a hand crafted lexical knowledge of English that contains more than 155,000 words organized into a taxonomic ontology of related terms known as synsets. Each synset (i.e. a concept) is linked to different synsets via a defined relationship between concepts. The most common relationships in WordNet are the \u2018is-a\u2019 and \u2018part \u2013of\u2019 relationships.\nHybrid methods combine the corpus based methods with knowledge based methods and they generally perform better.\nTo the best of authors knowledge, there are a few works that compares sentences [22] [10]. This article compares state of the art word and sentence measures on benchmark datasets. It is found that hybrid measures are generally better than knowledge and corpus based measures."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1 Word Similarity Methods", "text": ""}, {"heading": "2.1.1 Corpus based Methods", "text": "These methods depend on word features extracted from a corpus. The first category of these methods is based on the information content (IC) of the least common subsumer (LCS) of compared term synsets [23]\u2013[25]. The second category, a group known as distributional methods, depends on distribution of words within a text context. Words co-occurrences are represented as vectors of grammatical dependencies. The distributional method, LSA similarity [16], [26] transforms text to low dimensional matrix and it finds the most common words that can appear together in the processed text. Corpus based methods are domain dependent because they are limited to their base corpora."}, {"heading": "2.1.2 Knowledge based Methods", "text": "Knowledge based methods use information from dictionaries (such as WordNet) to get similarity scores. Classical knowledge based methods use the shortest path measure [27] , while others extend the path measure with depth of the LCS of compared words [28], [29] . Leacock Chodorow [30] proposed a similarity measure based on number of nodes in a taxonomy\nand shortest path between compared terms. Hirst and St-Onge [31] considered all types of WordNet relations; the path length and its change in direction. Some methods [23]\u2013[25] have the ability to use intrinsic information rather than information content. Knowledge based methods suffer from limited hand crafted ontologies."}, {"heading": "2.1.3 Hybrid Methods", "text": "Hybrid based methods associate functions from corpus and knowledge based methods. Zhou et al. [32] proposed a similarity measure as a function of the IC and the path length of compared words. Rodriguez and Egenhofer [33] used the weighted sum between synsets paths, neighboring concepts and their features in a knowledge fusion model . Dong et al. [34] proposed a weighted edge approach to give different weights of words that share the same LCS and have the same graph distance; words with lower edge weights are more similar than words with higher edge weights. Atoum and Bong [35] proposed a hybrid measure of distance based/knowledge based method[29] and information content method [23]. They called their model Joint Distance and Information Content Word Similarity Measure (JDIC).\nIn this category also, web based methods depend on the web resources to calculate the similarity. Turney et al. [9] used a measure called Point-Wise Mutual Information and Information Retrieval (PMI-IR) that is based on the number of hits returned by a web search engine. Bollegala et al. [36] used a WordNet metric and Support Vector Machines on text snippets returned by a Web search engine to learn semantically related and unrelated words."}, {"heading": "2.2 Sentence Similarity Methods", "text": ""}, {"heading": "2.2.1 Corpus based Methods", "text": "These methods are based on a corpus features. The first category, traditional information retrieval methods, Term Frequency \u2013Inverse Document Frequency (TF-IDF) methods [37]\u2013[39], assume that documents have common words. However, these methods are not valid for sentences because sentences may have null common words[29], [40] . For example, the sentences \u201cmy boy went to school\u201d and \u201ckids learn math\u201d do not have any common word although they are semantically related to education.\nBased on the TF-IDF idea, the second category, word cooccurrence methods are proposed. They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45]. After these vectors are processed a similarity measure such as the cosine measure is used to calculate the final similarity between compared text fragments.\nThe third category, string similarity methods (mini corpus based methods) depend on strings edit distance and the word order in a sentence [43]\u2013[45].The fourth category, the gigantic corpus methods. They use the internet resources as their baseline; Wikipedia [46], Google Tri-grams[6][47] , and search engine documents [48]. These methods are more practical to text rather than sentences.\nCorpus based methods (second and fourth category) suffer from these problems; once the vector space model is built for a domain it can be hardly used in another domain [19]. In addition, adding new instance of existing model becomes\ninfeasible, as it requires rebuilding the whole model, (i.e. computationally costly). They also have the problem of high sparse vectors especially for short sentences and generally they are not practical [20]."}, {"heading": "2.2.2 Knowledge based Methods", "text": "knowledge based methods use semantic dictionary information such word relationships [31][40][49], information content [1], [23] to get word semantic features. Li et al. [20] proposed a sentence similarity based on the aspects that a human interprets sentences; objects the sentence describes, properties of these objects and behaviors of these objects.\nTian et al. [19] proposed sentence similarity based on WordNet IC and part of speech tree kernels. Huang and Sheng [45] proposed a sentence similarity measure for paraphrase recognition and text entailment based on WordNet IC and string edit distance. Lee [50] built semantic vectors from WordNet information and part of speech tags. Abdalgader and Skabar [51] proposed a sentence similarity measure based on word sense disambiguation and the WordNet synonym expansion. Tsatsaronis et al. [40] measured the semantic relatedness between compared texts based on their implicit semantic links extracted from a thesaurus. Li et al. [52] proposed a sentence similarity measure based on word and verb vectors and the words order.\nGenerally, the knowledge based methods are limited to the human crafted dictionaries. Due to this, not all words are available in the dictionary and even if a few words exits they usually do not have the required semantic information. As an example, WordNet has a limited number of verbs and adverbs synsets compared to the list of available nouns synsets in the same ontology."}, {"heading": "2.2.3 Hybrid Methods", "text": "Hybrid methods are a combinations of the previous mentioned methods. Croft et al. [4] applied their measure on photographic description data based semantic vectors of path and term frequency. Li et al. [29] proposed a sentence similarity based on WordNet information, IC of Brown Corpus, and sentence words orders. Later, [52] proposed a word similarity based on a new information content formula and Lin word similarity[23].\nHo et al. [6] incorporated a modified version of word sense disambiguation of [53] in their similarity measure. Feng et al. [54] used direct( words relationships) and indirect (reasoning) relevance between sentences to estimate sentence similarity. Liu et al. [55] proposed a sentence similarity based on Dynamic Time Wrapping (DTW) approach. They calculated the similarity between sentences by aligning sentences parts of speech using DTW distance measure. Ho et al. [6] showed that DTW is computationally costly and time proportionately with the sentence\u2019s length.\nA combination of eight knowledge base measures and three corpus based measures is proposed in [39], [56]. The final word similarity measure is the average of all eight measures. The sentence similarity measure is derived using word overlapping over an IDF function of words in related segments.\nHybrid approaches shows promising results on standard benchmark datasets. Table 1 shows the summary of different word and sentence similarity measures."}, {"heading": "3. EXPERIMENTAL EVALUATION", "text": ""}, {"heading": "3.1 Word Similarity Methods", "text": "To evaluate the performance of word similarity methods, the\nRubenstein Goodenough[57] and Miller Charles[58] word\npairs benchmark datasets are selected. Rubenstein and\nGoodenough investigated synonymy judgements of 65 noun\npairs categorized by human experts on the scale from 0.0 to 4.0.\nMiller and Charles selected 30 word pairs out of the 65 pairs of\nnouns and organized them under three similarity levels.\nThe experiments were run with WordNet 3.0 [59] for\nknowledge based measures and Brown Dictionary [60] for\ncorpus based measures. The similarity measures are\nimplemented using python custom code. Figure 1 and Figure 2\nrespectively summarizes the Pearson correlation of different\nsimilarity measures against human means on the Miller and\nGoodenough datasets.\nResults showed it cannot be argued what is the best word method unless the method is used in real application or tested on a benchmark dataset. However, hybrid methods (e.g. JDIC) perform better than other corpus and knowledge based methods."}, {"heading": "3.2 Sentence Similarity Methods", "text": "To evaluate the performance of the sentence similarity methods, the dataset constructed by [29] ( the STSS-65 dataset) is selected1. It consists of sentences pairs that were originally\nconstructed manually to evaluate a short similarity measure\n1 http://semanticsimilarity.net/benchmark-datasets.\nnamed STASIS. In STSS-65 dataset, the corresponding words in [57] are replaced with the words definitions from the Collins Cobuild Dictionary [61]. Instead of keeping all the 65 pairs Li et al. [29] decided to keep only the most accurate annotated and balanced sentence pairs. Note that in this dataset, the pair number 17 has been used with different Human scores namely (0.13,0.063,0.048) in different research works e.g., [4], [29], [50]. The human score 0.13 was first used in the main work of [29], but later [62] published the dataset on 2009 with the figure 0.048 (0.19 from 4). The 0.13 figure is used in this article as first used by the original work of [29].\nFor all experiments WordNet Version 3.0 is used. For Mihalcea [11] measure the PMI-IR measure is replaced with Normalized Search engine Index Distance (NSID) [63] as Turner 's PMI is not available. Also, Wikipedia dataset of December 2013 were used for LSA measure and Open America National Corpus (OANC) to replace BNC Corpus.\nTable 2 shows the Pearson correlation and Spearman\u2019s rank coefficient between different measures and Human participants\u2019 ratings. On the first hand, the Pearson correlation is either calculated or taken from respected works. On the other hand, the Spearman\u2019s rank figure is calculated using published similarity figures of the respected works. The computed similarity scores are sorted in an ascending order, and the\nranking of similarities is compared against the benchmark dataset using Spearman\u2019s rank correlation.\nTable 2 shows that Ming [50] and Mihalcea measures have the lowest Pearson and Spearman Coefficients. To investigate this result, Mihalcea [11] is taken as an example. Each of the 8 different measures (of Mihalcea) has its strengths and weakness. One of them, Wikipedia measure has relatively high similarity (>0.5) while the path measure has relatively low similarity (<0.1). Therefore, once the average all the measures is computed the final similarity score will be no longer be near the human similarity rating score. More precisely, from Mihalcea\u2019s study got score values in range (0.07-0.5) for all compared benchmark sentence pairs. The authors findings resemble Ho et al. [6] findings. They showed that simple average similarity can never be a good similarity measure.\nMany sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] . For example the works of [39], [56] are based on 8 different knowledge based measures and 3 corpus based measures which makes their implementation difficult. Further difficulties in other works includes the need of processing gigantic data processing [47].\n[47] used the Web 1T 5-gram dataset; a compressed text file of approximately 24 GB compressed composed of more than 1 million tri-grams extracted from 1 trillion tokens. Nevertheless, [47] [10] are considered comprehensive datasets and can be accessed easily once indexed.\nFigure 3 shows the similarity measure versus Pearson Correlation over the STS-65 dataset. Table 2 shows that hybrid methods (e.g. [6], [55], [40]) perform better than knowledge based (e.g. [29]) and corpus based (e.g. [10]) methods. Islam et al. Tri-gram measure [47] is an exception. This finding is explained by studying details in Table 3. Table 3 shows the STS-65 benchmark dataset word pairs (second column) that correspond to the list of sentences (i.e. sentences used in similarity measures). The human mean score rating (third column), in the range of 0.01-0.96, represent dissimilar to very similar sentences. It is found that [47] overestimates the human rating scores especially the dissimilar sentence pairs. Conversely, this finding was not clear at the Pearson correlation level shown in Table 2.\nFigure 4 shows the STS-65 dataset human scores versus the scores of [47] and [40]. It is clear that [47] overestimates sentence pairs 1-29(30% of the original dataset). However, the same method works well for pairs that are semantically similar as per human scores (30-65). On the other hand, although [40] has less Pearson correlation, as shown in Figure 3, it is relatively better than [47] in sentence pairs 1-29. Therefore, the Pearson correlation (in this case) is not a good measure to compare sentence measures that are relatively dissimilar. It is concluded that another measure should take into consideration this case instead of using an average as in the case of Pearson correlation."}, {"heading": "4. CONCLUSION", "text": "This article studies a set of word and sentence similarity measures. The study showed that word similarity is not enough to select a good sentence similarity measure. Hybrid sentence\nmethods are generally better than corpus and knowledge based methods. In the future, it is planned to test more word and sentence methods on other datasets. Furthermore, more work will concentrate on an approach to choose between Spearman and Pearson correlations.\nNo Corresponding\nword pairs Mean Li 2006\nTsatsaronis\n2010\nIslam 2008 Ho 2010\nCroft 2013 Islam 2012\n56 coast-shores 0.59 0.76 0.93 0.47 0.49 0.80 0.42\n57 woodland-forest 0.63 0.7 0.61 0.26 0.34 1.00 0.47\n58 implement-tool 0.59 0.75 0.74 0.51 0.56 0.80 0.67\n59 cock-rooster 0.86 1.00 1.00 0.94 0.87 1.00 0.53\n60 boy-lad 0.58 0.66 0.93 0.6 0.57 0.80 0.62\n61 pillow-cushion 0.52 0.66 0.35 0.29 0.26 0.80 0.49\n62 cemetery-graveyard 0.77 0.73 0.73 0.51 0.59 1.00 0.48\n63 automobile-car 0.56 0.64 0.79 0.52 0.38 1.00 0.64\n64 midday-noon 0.96 1.00 0.93 0.93 0.86 1.00 0.87\n65 gem-jewel 0.65 0.83 0.82 0.65 0.61 1.00 0.88"}, {"heading": "5. REFERENCES", "text": "[1] P. Resnik, \u201cUsing information content to evaluate semantic\nsimilarity in a taxonomy,\u201d in Proceedings of the 14th international joint conference on Artificial intelligence (IJCAI\u201995), 1995, vol. 1, pp. 448\u2013453.\n[2] A. Islam and D. Inkpen, \u201cUnsupervised Near-Synonym\nChoice using the Google Web 1T,\u201d ACM Trans. Knowl. Discov. Data, vol. V, no. June, pp. 1\u201319, 2012.\n[3] B. Chen, \u201cLatent topic modelling of word co-occurence\ninformation for spoken document retrieval,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2009, 2009, no. 2, pp. 3961\u20133964.\n[4] D. Croft, S. Coupland, J. Shell, and S. Brown, \u201cA fast and\nefficient semantic short text similarity metric,\u201d in Computational Intelligence (UKCI), 2013 13th UK Workshop on, 2013, pp. 221\u2013227.\n[5] S. Memar, L. S. Affendey, N. Mustapha, S. C. Doraisamy,\nand M. Ektefa, \u201cAn integrated semantic-based approach in concept based video retrieval,\u201d Multimed. Tools Appl., vol. 64, no. 1, pp. 77\u201395, Aug. 2011.\n[6] C. Ho, M. A. A. Murad, R. A. Kadir, and S. C. Doraisamy,\n\u201cWord sense disambiguation-based sentence similarity,\u201d in Proceedings of the 23rd International Conference on Computational Linguistics: Posters, 2010, no. August, pp. 418\u2013426.\n[7] A. Islam and D. Inkpen, \u201cReal-word Spelling Correction\nUsing Google Web IT 3-grams,\u201d in Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, 2009, pp. 1241\u20131249.\n[8] M. Jarmasz and S. Szpakowicz, \u201cRoget\u2019s Thesaurus and\nSemantic Similarity,\u201d Recent Adv. Nat. Lang. Process. III Sel. Pap. from RANLP 2003, vol. 111, 2004.\n[9] P. Turney, \u201cMining the Web for Synonyms: PMI-IR versus\nLSA on TOEFL,\u201d in Proceedings of the 12th European Conference on Machine Learning, 2001, pp. 491\u2013502.\n[10] J. O\u2019Shea, Z. Bandar, K. Crockett, and D. McLean, \u201cA\nComparative Study of Two Short Text Semantic Similarity Measures,\u201d in Agent and Multi-Agent Systems: Technologies and Applications, vol. 4953, N. Nguyen, G. Jo, R. Howlett, and L. Jain, Eds. Springer Berlin Heidelberg, 2008, pp. 172\u2013181.\n[11] J.-H. Chiang and H.-C. Yu, \u201cLiterature extraction of\nprotein functions using sentence pattern mining,\u201d IEEE Trans. Knowl. Data Eng., vol. 17, no. 8, pp. 1088\u20131098, 2005.\n[12] I. Atoum and C. H. Bong, \u201cMeasuring Software Quality in\nUse: State-of-the-Art and Research Challenges,\u201d ASQ.Software Qual. Prof., vol. 17, no. 2, pp. 4\u201315, 2015.\n[13] S. T. W. Wendy, B. C. How, and I. Atoum, \u201cUsing Latent\nSemantic Analysis to Identify Quality in Use ( QU ) Indicators from User Reviews,\u201d in The International Conference on Artificial Intelligence and Pattern Recognition (AIPR2014), 2014, pp. 143\u2013151.\n[14] I. Atoum, C. H. Bong, and N. Kulathuramaiyer, \u201cBuilding\na Pilot Software Quality-in-Use Benchmark Dataset,\u201d in 9th International Conference on IT in Asia, 2015.\n[15] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R.\nHarshman, \u201cIndexing by latent semantic analysis,\u201d J. Am. Soc. Inf. Sci., vol. 41, no. 6, pp. 391\u2013407, Sep. 1990.\n[16] T. K. Landauer, P. W. Foltz, and D. Laham, \u201cAn\nintroduction to latent semantic analysis,\u201d Discourse Process., vol. 25, no. 2\u20133, pp. 259\u2013284, 1998.\n[17] W. Guo and M. Diab, \u201cA Simple Unsupervised Latent\nSemantics Based Approach for Sentence Similarity,\u201d in Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, 2012, pp. 586\u2013590.\n[18] J. Xu, P. Liu, G. Wu, Z. Sun, B. Xu, and H. Hao, \u201cA Fast\nMatching Method Based on Semantic Similarity for Short Texts,\u201d in Natural Language Processing and Chinese Computing, Y. Zhou, Guodong and Li, Juanzi and Zhao, Dongyan and Feng, Ed. Chongqing, China: Springer Berlin Heidelberg, 2013, pp. 299\u2013309.\n[19] Y. Tian, H. Li, Q. Cai, and S. Zhao, \u201cMeasuring the\nsimilarity of short texts by word similarity and tree kernels,\u201d in IEEE Youth Conference on Information Computing and Telecommunications (YC-ICT), 2010, pp. 363\u2013366.\n[20] L. Li, X. Hu, B.-Y. Hu, J. Wang, and Y.-M. Zhou,\n\u201cMeasuring sentence similarity from different aspects,\u201d in International Conference on Machine Learning and Cybernetics, 2009, 2009, vol. 4, pp. 2244\u20132249.\n[21] C. Fellbaum, \u201cWordNet: An electronic lexical database.\n1998,\u201d WordNet is available from http//www. cogsci. princeton. edu/wn, no. 2000, pp. 231\u2013243, 2010.\n[22] P. Achananuparp, X. Hu, and X. Shen, \u201cThe Evaluation of\nSentence Similarity Measures,\u201d in Data Warehousing and Knowledge Discovery, vol. 5182, I.-Y. Song, J. Eder, and T. Nguyen, Eds. Springer Berlin Heidelberg, 2008, pp. 305\u2013316.\n[23] D. Lin, \u201cAn information-theoretic definition of similarity,\u201d\nin Proceedings of the 15th international conference on Machine Learning, 1998, vol. 1, pp. 296\u2013304.\n[24] P. Resnik, \u201cDisambiguating Noun Groupings with Respect\nto WordNet Senses,\u201d in Natural Language Processing Using Very Large Corpora SE - 6, 1995, vol. 11, pp. 77\u2013 98.\n[25] J. J. Jiang and D. W. Conrath, \u201cSemantic similarity based\non corpus statistics and lexical taxonomy,\u201d in Proceedings of the 10th Research on Computational Linguistics International Conference (ROCLING X), 1997, pp. 19\u201333.\n[26] S. Deerwester and S. Dumais, \u201cIndexing by latent\nsemantic analysis,\u201d J. Am. Soc. Inf. Sci., vol. 41, no. 6, pp. 391\u2013407, Sep. 1990.\n[27] R. Rada, H. Mili, E. Bicknell, and M. Blettner,\n\u201cDevelopment and application of a metric on semantic nets,\u201d IEEE Trans. Syst. Man Cybern., vol. 19, no. 1, pp. 17\u201330, 1989.\n[28] Z. Wu and M. Palmer, \u201cVerbs semantics and lexical\nselection,\u201d in Proceedings of the 32nd annual meeting on Association for Computational Linguistics, 1994, pp. 133\u2013 138.\n[29] Y. Li, D. McLean, Z. A. Bandar, J. D. O\u2019Shea, and K.\nCrockett, \u201cSentence similarity based on semantic nets and corpus statistics,\u201d IEEE Trans. Knowl. Data Eng., vol. 18, no. 8, pp. 1138\u20131150, Aug. 2006.\n[30] C. Leacock and M. Chodorow, \u201cCombining local context\nand WordNet similarity for word sense identification,\u201d WordNet An Electron. Lex. database, vol. 49, no. 2, pp. 265\u2013283, 1998.\n[31] G. Hirst and D. St-Onge, \u201cLexical chains as\nrepresentations of context for the detection and correction of malapropisms,\u201d in WordNet: An electronic lexical database, vol. 305, C. Fellbaum, Ed. Cambridge, MA: The MIT Press, 1998, pp. 305\u2013332.\n[32] Z. Zhou, Y. Wang, and J. Gu, \u201cA New Model of\nInformation Content for Semantic Similarity in WordNet,\u201d in Second International Conference on Future Generation Communication and Networking Symposia, 2008, vol. 1, pp. 85\u201389.\n[33] M. a. Rodriguez and M. J. J. Egenhofer, \u201cDetermining\nsemantic similarity among entity classes from different ontologies,\u201d IEEE Trans. Knowl. Data Eng., vol. 15, no. 2, pp. 442\u2013456, Mar. 2003.\n[34] L. Dong, P. K. Srimani, and J. Z. Wang, \u201cWEST:\nWeighted-Edge Based Similarity Measurement Tools for Word Semantics,\u201d in IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010, vol. 1, pp. 216\u2013223.\n[35] I. Atoum and C. H. Bong, \u201cJoint Distance and Information\nContent Word Similarity Measure,\u201d in Soft Computing Applications and Intelligent Systems SE - 22, vol. 378, S. Noah, A. Abdullah, H. Arshad, A. Abu Bakar, Z. Othman, S. Sahran, N. Omar, and Z. Othman, Eds. Kuala Lumpur: Springer Berlin Heidelberg, 2013, pp. 257\u2013267.\n[36] D. Bollegala, Y. Matsuo, M. Ishizuka, M. D. Thiyagarajan,\nand N. Navaneethakrishnanc, \u201cA Web Search EngineBased Approach to Measure Semantic Similarity between Words,\u201d IEEE Trans. Knowl. Data Eng., vol. 23, no. 7, pp. 977\u2013990, Jul. 2011.\n[37] J. Allan, C. Wade, and A. Bolivar, \u201cRetrieval and Novelty\nDetection at the Sentence Level,\u201d in Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, 2003, pp. 314\u2013321.\n[38] T. C. Hoad and J. Zobel, \u201cMethods for identifying\nversioned and plagiarized documents,\u201d J. Am. Soc. Inf. Sci. Technol., vol. 54, no. 3, pp. 203\u2013215, 2003.\n[39] C. Akkaya, J. Wiebe, and R. Mihalcea, \u201cSubjectivity word\nsense disambiguation,\u201d in Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, 2009, pp. 190\u2013199.\n[40] G. Tsatsaronis, I. Varlamis, and M. Vazirgiannis, \u201cText\nrelatedness based on a word thesaurus,\u201d J. Artif. Intell. Res., vol. 37, pp. 1\u201338, 2010.\n[41] C. Burgess, K. Livesay, and K. Lund, \u201cExplorations in\ncontext space: Words, sentences, discourse,\u201d Discourse Process., vol. 25, no. 2\u20133, pp. 211\u2013257, 1998.\n[42] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent dirichlet\nallocation,\u201d J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, Mar. 2003.\n[43] A. Islam and D. Inkpen, \u201cSemantic text similarity using\ncorpus-based word similarity and string similarity,\u201d ACM Trans. Knowl. Discov. from Data, vol. 2, no. 2, pp. 10:1\u2013 10:25, Jul. 2008.\n[44] F. Mandreoli, R. Martoglia, and P. Tiberio, \u201cA Syntactic\nApproach for Searching Similarities Within Sentences,\u201d in Proceedings of the Eleventh International Conference on Information and Knowledge Management, 2002, pp. 635\u2013 637.\n[45] G. Huang and J. Sheng, \u201cMeasuring Similarity between\nSentence Fragments,\u201d in 4th International Conference on Intelligent Human-Machine Systems and Cybernetics, 2012, pp. 327\u2013330.\n[46] L. C. Wee and S. Hassan, \u201cExploiting Wikipedia for\nDirectional Inferential Text Similarity,\u201d in Fifth International Conference on Information Technology: New Generations, 2008, pp. 686\u2013691.\n[47] A. Islam, E. Milios, and V. Ke\u0161elj, \u201cText similarity using\ngoogle tri-grams,\u201d in Advances in Artificial Intelligence, vol. 7310, L. Kosseim and D. Inkpen, Eds. Springer, 2012, pp. 312\u2013317.\n[48] N. Malandrakis, E. Iosif, and A. Potamianos,\n\u201cDeepPurple: Estimating Sentence Semantic Similarity Using N-gram Regression Models and Web Snippets,\u201d in Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, 2012, pp. 565\u2013570.\n[49] N. Seco, T. Veale, and J. Hayes, \u201cAn Intrinsic Information\nContent Metric for Semantic Similarity in WordNet,\u201d in Proceedings of the 16th European Conference on Artificial Intelligence, 2004, no. Ic, pp. 1\u20135.\n[50] M. C. Lee, \u201cA novel sentence similarity measure for\nsemantic-based expert systems,\u201d Expert Syst. Appl., vol. 38, no. 5, pp. 6392\u20136399, 2011.\n[51] K. Abdalgader and A. Skabar, \u201cShort-text similarity\nmeasurement using word sense disambiguation and synonym expansion,\u201d in AI 2010: Advances in Artificial Intelligence, Springer Berlin / Heidelberg, 2011, pp. 435\u2013 444.\n[52] Y. Li, H. Li, Q. Cai, and D. Han, \u201cA novel semantic\nsimilarity measure within sentences,\u201d in Proceedings of 2012 2nd International Conference on Computer Science and Network Technology, 2012, pp. 1176\u20131179.\n[53] D. Yang and D. M. W. Powers, \u201cMeasuring semantic\nsimilarity in the taxonomy of WordNet,\u201d in Proceedings of the Twenty-eighth Australasian conference on Computer Science - Volume 38, 2005, pp. 315\u2013322.\n[54] J. Feng, Y. Zhou, and T. Martin, \u201cSentence similarity\nbased on relevance,\u201d in Proceedings of IPMU, 2008, pp. 832\u2013839.\n[55] X. Liu, Y. Zhou, and R. Zheng, \u201cSentence Similarity based\non Dynamic Time Warping,\u201d in International Conference on Semantic Computing (ICSC 2007), 2007, pp. 250\u2013256.\n[56] R. Mihalcea, C. Corley, and C. Strapparava, \u201cCorpus-\nbased and knowledge-based measures of text semantic similarity,\u201d Assoc. Adv. Artif. Intell., vol. 6, pp. 775\u2013780, 2006.\n[57] H. Rubenstein and J. B. Goodenough, \u201cContextual\ncorrelates of synonymy,\u201d Commun. ACM, vol. 8, no. 10, pp. 627\u2013633, Oct. 1965.\n[58] G. A. Miller and W. G. Charles, \u201cContextual correlates of\nsemantic similarity,\u201d Lang. Cogn. Process., vol. 6, no. 1, pp. 1\u201328, 1991.\n[59] P. University, \u201cAbout WordNet,\u201d Princeton University,\n2010. [Online]. Available: http://wordnet.princeton.edu.\n[60] W. N. Francis and H. Kucera, \u201cBrown corpus manual,\u201d\nLett. to Ed., vol. 5, no. 2, p. 7, 1979.\n[61] J. M. Sinclair, Collins COBUILD English dictionary for\nadvanced learners. HarperCollins, 2001.\n[62] J. O\u2019Shea, Z. Bandar, K. Crockett, and D. McLean, \u201cPilot\nShort Text Semantic Similarity Benchmark Data Set: Full Listing and Description,\u201d 2008.\n[63] R. Cilibrasi and P. M. B. Vit\u00e1nyi, \u201cThe Google Similarity\nDistance,\u201d CoRR, vol. abs/cs/041, 2004.\n[64] M. Mohler and R. Mihalcea, \u201cText-to-text Semantic\nSimilarity for Automatic Short Answer Grading,\u201d in Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, 2009, pp. 567\u2013575."}], "references": [{"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["P. Resnik"], "venue": "Proceedings of the 14th international joint conference on Artificial intelligence (IJCAI\u201995), 1995, vol. 1, pp. 448\u2013453.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Unsupervised Near-Synonym Choice using the Google Web 1T", "author": ["A. Islam", "D. Inkpen"], "venue": "ACM Trans. Knowl. Discov. Data, vol. V, no. June, pp. 1\u201319, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent topic modelling of word co-occurence information for spoken document retrieval", "author": ["B. Chen"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2009, 2009, no. 2, pp. 3961\u20133964.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast and efficient semantic short text similarity metric", "author": ["D. Croft", "S. Coupland", "J. Shell", "S. Brown"], "venue": "Computational Intelligence (UKCI), 2013 13th UK Workshop on, 2013, pp. 221\u2013227.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "An integrated semantic-based approach in concept based video retrieval", "author": ["S. Memar", "L.S. Affendey", "N. Mustapha", "S.C. Doraisamy", "M. Ektefa"], "venue": "Multimed. Tools Appl., vol. 64, no. 1, pp. 77\u201395, Aug. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Word sense disambiguation-based sentence similarity", "author": ["C. Ho", "M.A.A. Murad", "R.A. Kadir", "S.C. Doraisamy"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, 2010, no. August, pp. 418\u2013426.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Real-word Spelling Correction Using Google Web IT 3-grams", "author": ["A. Islam", "D. Inkpen"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, 2009, pp. 1241\u20131249.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Roget\u2019s Thesaurus and Semantic Similarity", "author": ["M. Jarmasz", "S. Szpakowicz"], "venue": "Recent Adv. Nat. Lang. Process. III Sel. Pap. from RANLP 2003, vol. 111, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL", "author": ["P. Turney"], "venue": "Proceedings of the 12th European Conference on Machine Learning, 2001, pp. 491\u2013502.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "A Comparative Study of Two Short Text Semantic Similarity Measures", "author": ["J. O\u2019Shea", "Z. Bandar", "K. Crockett", "D. McLean"], "venue": "Agent and Multi-Agent Systems: Technologies and Applications, vol. 4953, N. Nguyen, G. Jo, R. Howlett, and L. Jain, Eds. Springer Berlin Heidelberg, 2008, pp. 172\u2013181.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Literature extraction of protein functions using sentence pattern mining", "author": ["J.-H. Chiang", "H.-C. Yu"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 17, no. 8, pp. 1088\u20131098, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Measuring Software Quality in Use: State-of-the-Art and Research Challenges", "author": ["I. Atoum", "C.H. Bong"], "venue": "ASQ.Software Qual. Prof., vol. 17, no. 2, pp. 4\u201315, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Using Latent Semantic Analysis to Identify Quality in Use ( QU ) Indicators from User Reviews", "author": ["S.T.W. Wendy", "B.C. How", "I. Atoum"], "venue": "The International Conference on Artificial Intelligence and Pattern Recognition (AIPR2014), 2014, pp. 143\u2013151.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Building a Pilot Software Quality-in-Use Benchmark Dataset", "author": ["I. Atoum", "C.H. Bong", "N. Kulathuramaiyer"], "venue": "9th International Conference on IT in Asia, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "T. Landauer", "G. Furnas", "R. Harshman"], "venue": "J. Am. Soc. Inf. Sci., vol. 41, no. 6, pp. 391\u2013407, Sep. 1990.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "An introduction to latent semantic analysis", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Discourse Process., vol. 25, no. 2\u20133, pp. 259\u2013284, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A Simple Unsupervised Latent Semantics Based Approach for Sentence Similarity", "author": ["W. Guo", "M. Diab"], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, 2012, pp. 586\u2013590.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A Fast Matching Method Based on Semantic Similarity for Short Texts", "author": ["J. Xu", "P. Liu", "G. Wu", "Z. Sun", "B. Xu", "H. Hao"], "venue": "Natural Language Processing and Chinese Computing, Y. Zhou, Guodong and Li, Juanzi and Zhao, Dongyan and Feng, Ed. Chongqing, China: Springer Berlin Heidelberg, 2013, pp. 299\u2013309.  8", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Measuring the similarity of short texts by word similarity and tree kernels", "author": ["Y. Tian", "H. Li", "Q. Cai", "S. Zhao"], "venue": "IEEE Youth Conference on Information Computing and Telecommunications (YC-ICT), 2010, pp. 363\u2013366.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Measuring sentence similarity from different aspects", "author": ["L. Li", "X. Hu", "B.-Y. Hu", "J. Wang", "Y.-M. Zhou"], "venue": "International Conference on Machine Learning and Cybernetics, 2009, 2009, vol. 4, pp. 2244\u20132249.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "WordNet: An electronic lexical database. 1998", "author": ["C. Fellbaum"], "venue": "WordNet is available from http//www. cogsci. princeton. edu/wn, no. 2000, pp. 231\u2013243, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "The Evaluation of Sentence Similarity Measures", "author": ["P. Achananuparp", "X. Hu", "X. Shen"], "venue": "Data Warehousing and Knowledge Discovery, vol. 5182, I.-Y. Song, J. Eder, and T. Nguyen, Eds. Springer Berlin Heidelberg, 2008, pp. 305\u2013316.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "Proceedings of the 15th international conference on Machine Learning, 1998, vol. 1, pp. 296\u2013304.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "author": ["P. Resnik"], "venue": "Natural Language Processing Using Very Large Corpora SE - 6, 1995, vol. 11, pp. 77\u2013 98.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "Proceedings of the 10th Research on Computational Linguistics International Conference (ROCLING X), 1997, pp. 19\u201333.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais"], "venue": "J. Am. Soc. Inf. Sci., vol. 41, no. 6, pp. 391\u2013407, Sep. 1990.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "Development and application of a metric on semantic nets", "author": ["R. Rada", "H. Mili", "E. Bicknell", "M. Blettner"], "venue": "IEEE Trans. Syst. Man Cybern., vol. 19, no. 1, pp. 17\u201330, 1989.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics, 1994, pp. 133\u2013 138.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "Sentence similarity based on semantic nets and corpus statistics", "author": ["Y. Li", "D. McLean", "Z.A. Bandar", "J.D. O\u2019Shea", "K. Crockett"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 18, no. 8, pp. 1138\u20131150, Aug. 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["C. Leacock", "M. Chodorow"], "venue": "WordNet An Electron. Lex. database, vol. 49, no. 2, pp. 265\u2013283, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Lexical chains as representations of context for the detection and correction of malapropisms", "author": ["G. Hirst", "D. St-Onge"], "venue": "WordNet: An electronic lexical database, vol. 305, C. Fellbaum, Ed. Cambridge, MA: The MIT Press, 1998, pp. 305\u2013332.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "A New Model of Information Content for Semantic Similarity in WordNet", "author": ["Z. Zhou", "Y. Wang", "J. Gu"], "venue": "Second International Conference on Future Generation Communication and Networking Symposia, 2008, vol. 1, pp. 85\u201389.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Determining semantic similarity among entity classes from different ontologies", "author": ["M. a. Rodriguez", "M.J.J. Egenhofer"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 15, no. 2, pp. 442\u2013456, Mar. 2003.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "WEST: Weighted-Edge Based Similarity Measurement Tools for Word Semantics", "author": ["L. Dong", "P.K. Srimani", "J.Z. Wang"], "venue": "IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010, vol. 1, pp. 216\u2013223.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint Distance and Information Content Word Similarity Measure", "author": ["I. Atoum", "C.H. Bong"], "venue": "Soft Computing Applications and Intelligent Systems SE - 22, vol. 378, S. Noah, A. Abdullah, H. Arshad, A. Abu Bakar, Z. Othman, S. Sahran, N. Omar, and Z. Othman, Eds. Kuala Lumpur: Springer Berlin Heidelberg, 2013, pp. 257\u2013267.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "A Web Search Engine- Based Approach to Measure Semantic Similarity between Words", "author": ["D. Bollegala", "Y. Matsuo", "M. Ishizuka", "M.D. Thiyagarajan", "N. Navaneethakrishnanc"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 23, no. 7, pp. 977\u2013990, Jul. 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Retrieval and Novelty Detection at the Sentence Level", "author": ["J. Allan", "C. Wade", "A. Bolivar"], "venue": "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, 2003, pp. 314\u2013321.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Methods for identifying versioned and plagiarized documents", "author": ["T.C. Hoad", "J. Zobel"], "venue": "J. Am. Soc. Inf. Sci. Technol., vol. 54, no. 3, pp. 203\u2013215, 2003.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Subjectivity word sense disambiguation", "author": ["C. Akkaya", "J. Wiebe", "R. Mihalcea"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, 2009, pp. 190\u2013199.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Text relatedness based on a word thesaurus", "author": ["G. Tsatsaronis", "I. Varlamis", "M. Vazirgiannis"], "venue": "J. Artif. Intell. Res., vol. 37, pp. 1\u201338, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Explorations in context space: Words, sentences, discourse", "author": ["C. Burgess", "K. Livesay", "K. Lund"], "venue": "Discourse Process., vol. 25, no. 2\u20133, pp. 211\u2013257, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, Mar. 2003.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}, {"title": "Semantic text similarity using corpus-based word similarity and string similarity", "author": ["A. Islam", "D. Inkpen"], "venue": "ACM Trans. Knowl. Discov. from Data, vol. 2, no. 2, pp. 10:1\u2013 10:25, Jul. 2008.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "A Syntactic Approach for Searching Similarities Within Sentences", "author": ["F. Mandreoli", "R. Martoglia", "P. Tiberio"], "venue": "Proceedings of the Eleventh International Conference on Information and Knowledge Management, 2002, pp. 635\u2013 637.  9", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Measuring Similarity between Sentence Fragments", "author": ["G. Huang", "J. Sheng"], "venue": "4th International Conference on Intelligent Human-Machine Systems and Cybernetics, 2012, pp. 327\u2013330.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting Wikipedia for Directional Inferential Text Similarity", "author": ["L.C. Wee", "S. Hassan"], "venue": "Fifth International Conference on Information Technology: New Generations, 2008, pp. 686\u2013691.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Text similarity using google tri-grams", "author": ["A. Islam", "E. Milios", "V. Ke\u0161elj"], "venue": "Advances in Artificial Intelligence, vol. 7310, L. Kosseim and D. Inkpen, Eds. Springer, 2012, pp. 312\u2013317.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "DeepPurple: Estimating Sentence Semantic Similarity Using N-gram Regression Models and Web Snippets", "author": ["N. Malandrakis", "E. Iosif", "A. Potamianos"], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, 2012, pp. 565\u2013570.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "An Intrinsic Information Content Metric for Semantic Similarity in WordNet", "author": ["N. Seco", "T. Veale", "J. Hayes"], "venue": "Proceedings of the 16th European Conference on Artificial Intelligence, 2004, no. Ic, pp. 1\u20135.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "A novel sentence similarity measure for semantic-based expert systems", "author": ["M.C. Lee"], "venue": "Expert Syst. Appl., vol. 38, no. 5, pp. 6392\u20136399, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Short-text similarity measurement using word sense disambiguation and synonym expansion", "author": ["K. Abdalgader", "A. Skabar"], "venue": "AI 2010: Advances in Artificial Intelligence, Springer Berlin / Heidelberg, 2011, pp. 435\u2013 444.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel semantic similarity measure within sentences", "author": ["Y. Li", "H. Li", "Q. Cai", "D. Han"], "venue": "Proceedings of 2012 2nd International Conference on Computer Science and Network Technology, 2012, pp. 1176\u20131179.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Measuring semantic similarity in the taxonomy of WordNet", "author": ["D. Yang", "D.M.W. Powers"], "venue": "Proceedings of the Twenty-eighth Australasian conference on Computer Science - Volume 38, 2005, pp. 315\u2013322.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "Sentence similarity based on relevance", "author": ["J. Feng", "Y. Zhou", "T. Martin"], "venue": "Proceedings of IPMU, 2008, pp. 832\u2013839.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2008}, {"title": "Sentence Similarity based on Dynamic Time Warping", "author": ["X. Liu", "Y. Zhou", "R. Zheng"], "venue": "International Conference on Semantic Computing (ICSC 2007), 2007, pp. 250\u2013256.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "Corpusbased and knowledge-based measures of text semantic similarity", "author": ["R. Mihalcea", "C. Corley", "C. Strapparava"], "venue": "Assoc. Adv. Artif. Intell., vol. 6, pp. 775\u2013780, 2006.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2006}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM, vol. 8, no. 10, pp. 627\u2013633, Oct. 1965.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1965}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Lang. Cogn. Process., vol. 6, no. 1, pp. 1\u201328, 1991.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1991}, {"title": "About WordNet", "author": ["P. University"], "venue": "Princeton University, 2010. [Online]. Available: http://wordnet.princeton.edu.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Brown corpus manual", "author": ["W.N. Francis", "H. Kucera"], "venue": "Lett. to Ed., vol. 5, no. 2, p. 7, 1979.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1979}, {"title": "Pilot Short Text Semantic Similarity Benchmark Data Set: Full Listing and Description", "author": ["J. O\u2019Shea", "Z. Bandar", "K. Crockett", "D. McLean"], "venue": "2008.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "The Google Similarity Distance", "author": ["R. Cilibrasi", "P.M.B. Vit\u00e1nyi"], "venue": "CoRR, vol. abs/cs/041, 2004.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2004}, {"title": "Text-to-text Semantic Similarity for Automatic Short Answer Grading", "author": ["M. Mohler", "R. Mihalcea"], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, 2009, pp. 567\u2013575.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Resnik illustrated that word similarity is a subcase of word relatedness[1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 205, "endOffset": 208}, {"referenceID": 9, "context": "It has an important role in many applications such as machine translation [2], information retrieval [3]\u2013[5], word sense disambiguation [6], spell checking [7], thesauri generation [8], synonymy detection [9], and question answering [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 10, "context": "Furthermore, semantic similarity is also used in other domains; in medical domain to extract protein functions from biomedical literature [11] and in software quality[12]\u2013[14] to find common software attributes.", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Furthermore, semantic similarity is also used in other domains; in medical domain to extract protein functions from biomedical literature [11] and in software quality[12]\u2013[14] to find common software attributes.", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "Furthermore, semantic similarity is also used in other domains; in medical domain to extract protein functions from biomedical literature [11] and in software quality[12]\u2013[14] to find common software attributes.", "startOffset": 171, "endOffset": 175}, {"referenceID": 9, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 19, "context": "In this category, Latent Semantic Analysis (LSA) [10], [15], [16], and Latent Dirichlet Allocation (LDA) [3], [17], [18] have shown positive outcomes, however they are rather domain dependent [19], [20].", "startOffset": 198, "endOffset": 202}, {"referenceID": 20, "context": "Most knowledge based measures depend on WordNet[21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "To the best of authors knowledge, there are a few works that compares sentences [22] [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "To the best of authors knowledge, there are a few works that compares sentences [22] [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "The first category of these methods is based on the information content (IC) of the least common subsumer (LCS) of compared term synsets [23]\u2013[25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "The first category of these methods is based on the information content (IC) of the least common subsumer (LCS) of compared term synsets [23]\u2013[25].", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "The distributional method, LSA similarity [16], [26] transforms text to low dimensional matrix and it finds the most common words that can appear together in the processed text.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "The distributional method, LSA similarity [16], [26] transforms text to low dimensional matrix and it finds the most common words that can appear together in the processed text.", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "Classical knowledge based methods use the shortest path measure [27] , while others extend the path measure with depth of the LCS of compared words [28], [29] .", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "Classical knowledge based methods use the shortest path measure [27] , while others extend the path measure with depth of the LCS of compared words [28], [29] .", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Classical knowledge based methods use the shortest path measure [27] , while others extend the path measure with depth of the LCS of compared words [28], [29] .", "startOffset": 154, "endOffset": 158}, {"referenceID": 29, "context": "Leacock Chodorow [30] proposed a similarity measure based on number of nodes in a taxonomy", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "Hirst and St-Onge [31] considered all types of WordNet relations; the path length and its change in direction.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "Some methods [23]\u2013[25] have the ability to use intrinsic information rather than information content.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Some methods [23]\u2013[25] have the ability to use intrinsic information rather than information content.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "[32] proposed a similarity measure as a function of the IC and the path length of compared words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Rodriguez and Egenhofer [33] used the weighted sum between synsets paths, neighboring concepts and their features in a knowledge fusion model .", "startOffset": 24, "endOffset": 28}, {"referenceID": 33, "context": "[34] proposed a weighted edge approach to give different weights of words that share the same LCS and have the same graph distance; words with lower edge weights are more similar than words with higher edge weights.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Atoum and Bong [35] proposed a hybrid measure of distance based/knowledge based method[29] and information content method [23].", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "Atoum and Bong [35] proposed a hybrid measure of distance based/knowledge based method[29] and information content method [23].", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "Atoum and Bong [35] proposed a hybrid measure of distance based/knowledge based method[29] and information content method [23].", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": "[9] used a measure called Point-Wise Mutual Information and Information Retrieval (PMI-IR) that is based on the number of hits returned by a web search engine.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[36] used a WordNet metric and Support Vector Machines on text snippets returned by a Web search engine to learn semantically related and unrelated words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The first category, traditional information retrieval methods, Term Frequency \u2013Inverse Document Frequency (TF-IDF) methods [37]\u2013[39], assume that documents have common words.", "startOffset": 123, "endOffset": 127}, {"referenceID": 38, "context": "The first category, traditional information retrieval methods, Term Frequency \u2013Inverse Document Frequency (TF-IDF) methods [37]\u2013[39], assume that documents have common words.", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "However, these methods are not valid for sentences because sentences may have null common words[29], [40] .", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "However, these methods are not valid for sentences because sentences may have null common words[29], [40] .", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 75, "endOffset": 79}, {"referenceID": 40, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 142, "endOffset": 146}, {"referenceID": 44, "context": "They model words cooccurrences as vectors of semantic features; LSA[10][16][26], Hyperspace Analogues to Language (HAL) [41], and LDA [7] [17][18][45].", "startOffset": 146, "endOffset": 150}, {"referenceID": 42, "context": "The third category, string similarity methods (mini corpus based methods) depend on strings edit distance and the word order in a sentence [43]\u2013[45].", "startOffset": 139, "endOffset": 143}, {"referenceID": 44, "context": "The third category, string similarity methods (mini corpus based methods) depend on strings edit distance and the word order in a sentence [43]\u2013[45].", "startOffset": 144, "endOffset": 148}, {"referenceID": 45, "context": "They use the internet resources as their baseline; Wikipedia [46], Google Tri-grams[6][47] , and search engine documents [48].", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "They use the internet resources as their baseline; Wikipedia [46], Google Tri-grams[6][47] , and search engine documents [48].", "startOffset": 83, "endOffset": 86}, {"referenceID": 46, "context": "They use the internet resources as their baseline; Wikipedia [46], Google Tri-grams[6][47] , and search engine documents [48].", "startOffset": 86, "endOffset": 90}, {"referenceID": 47, "context": "They use the internet resources as their baseline; Wikipedia [46], Google Tri-grams[6][47] , and search engine documents [48].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "Corpus based methods (second and fourth category) suffer from these problems; once the vector space model is built for a domain it can be hardly used in another domain [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "They also have the problem of high sparse vectors especially for short sentences and generally they are not practical [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": "2 Knowledge based Methods knowledge based methods use semantic dictionary information such word relationships [31][40][49], information content [1], [23] to get word semantic features.", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "2 Knowledge based Methods knowledge based methods use semantic dictionary information such word relationships [31][40][49], information content [1], [23] to get word semantic features.", "startOffset": 114, "endOffset": 118}, {"referenceID": 48, "context": "2 Knowledge based Methods knowledge based methods use semantic dictionary information such word relationships [31][40][49], information content [1], [23] to get word semantic features.", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "2 Knowledge based Methods knowledge based methods use semantic dictionary information such word relationships [31][40][49], information content [1], [23] to get word semantic features.", "startOffset": 144, "endOffset": 147}, {"referenceID": 22, "context": "2 Knowledge based Methods knowledge based methods use semantic dictionary information such word relationships [31][40][49], information content [1], [23] to get word semantic features.", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "[20] proposed a sentence similarity based on the aspects that a human interprets sentences; objects the sentence describes, properties of these objects and behaviors of these objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed sentence similarity based on WordNet IC and part of speech tree kernels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "Huang and Sheng [45] proposed a sentence similarity measure for paraphrase recognition and text entailment based on WordNet IC and string edit distance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 49, "context": "Lee [50] built semantic vectors from WordNet information and part of speech tags.", "startOffset": 4, "endOffset": 8}, {"referenceID": 50, "context": "Abdalgader and Skabar [51] proposed a sentence similarity measure based on word sense disambiguation and the WordNet synonym expansion.", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "[40] measured the semantic relatedness between compared texts based on their implicit semantic links extracted from a thesaurus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[52] proposed a sentence similarity measure based on word and verb vectors and the words order.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] applied their measure on photographic description data based semantic vectors of path and term frequency.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[29] proposed a sentence similarity based on WordNet information, IC of Brown Corpus, and sentence words orders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Later, [52] proposed a word similarity based on a new information content formula and Lin word similarity[23].", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "Later, [52] proposed a word similarity based on a new information content formula and Lin word similarity[23].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "[6] incorporated a modified version of word sense disambiguation of [53] in their similarity measure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 52, "context": "[6] incorporated a modified version of word sense disambiguation of [53] in their similarity measure.", "startOffset": 68, "endOffset": 72}, {"referenceID": 53, "context": "[54] used direct( words relationships) and indirect (reasoning) relevance between sentences to estimate sentence similarity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[55] proposed a sentence similarity based on Dynamic Time Wrapping (DTW) approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] showed that DTW is computationally costly and time proportionately with the sentence\u2019s length.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "A combination of eight knowledge base measures and three corpus based measures is proposed in [39], [56].", "startOffset": 94, "endOffset": 98}, {"referenceID": 55, "context": "A combination of eight knowledge base measures and three corpus based measures is proposed in [39], [56].", "startOffset": 100, "endOffset": 104}, {"referenceID": 56, "context": "1 Word Similarity Methods To evaluate the performance of word similarity methods, the Rubenstein Goodenough[57] and Miller Charles[58] word pairs benchmark datasets are selected.", "startOffset": 107, "endOffset": 111}, {"referenceID": 57, "context": "1 Word Similarity Methods To evaluate the performance of word similarity methods, the Rubenstein Goodenough[57] and Miller Charles[58] word pairs benchmark datasets are selected.", "startOffset": 130, "endOffset": 134}, {"referenceID": 58, "context": "0 [59] for knowledge based measures and Brown Dictionary [60] for corpus based measures.", "startOffset": 2, "endOffset": 6}, {"referenceID": 59, "context": "0 [59] for knowledge based measures and Brown Dictionary [60] for corpus based measures.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "2 Sentence Similarity Methods To evaluate the performance of the sentence similarity methods, the dataset constructed by [29] ( the STSS-65 dataset) is selected.", "startOffset": 121, "endOffset": 125}, {"referenceID": 56, "context": "In STSS-65 dataset, the corresponding words in [57] are replaced with the words definitions from the Collins Cobuild Dictionary [61].", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "[29] decided to keep only the most accurate annotated and balanced sentence pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": ", [4], [29], [50].", "startOffset": 2, "endOffset": 5}, {"referenceID": 28, "context": ", [4], [29], [50].", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": ", [4], [29], [50].", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "13 was first used in the main work of [29], but later [62] published the dataset on 2009 with the figure 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 60, "context": "13 was first used in the main work of [29], but later [62] published the dataset on 2009 with the figure 0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "13 figure is used in this article as first used by the original work of [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "For Mihalcea [11] measure the PMI-IR measure is replaced with Normalized Search engine Index Distance (NSID) [63] as Turner 's PMI is not available.", "startOffset": 13, "endOffset": 17}, {"referenceID": 61, "context": "For Mihalcea [11] measure the PMI-IR measure is replaced with Normalized Search engine Index Distance (NSID) [63] as Turner 's PMI is not available.", "startOffset": 109, "endOffset": 113}, {"referenceID": 49, "context": "Ming Che Lee 2011[50] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 38, "context": "2009 [39] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 53, "context": "2008 [54] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "2013 (LSS) [4] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 28, "context": "2006(STASIS)[29] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "2008 (LSA)[10] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 54, "context": "2007 [55] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 42, "context": "2008 [43] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "Tsatsaronis et al 2010 (Omiotis)[40] 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "2010 (SPDSTS)[6] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 46, "context": "2012(TriGrams) [47] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 49, "context": "Table 2 shows that Ming [50] and Mihalcea measures have the lowest Pearson and Spearman Coefficients.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "To investigate this result, Mihalcea [11] is taken as an example.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "[6] findings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 46, "context": "Many sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] .", "startOffset": 103, "endOffset": 107}, {"referenceID": 62, "context": "Many sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] .", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "Many sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] .", "startOffset": 137, "endOffset": 140}, {"referenceID": 49, "context": "Many sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] .", "startOffset": 142, "endOffset": 146}, {"referenceID": 53, "context": "Many sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] .", "startOffset": 148, "endOffset": 152}, {"referenceID": 62, "context": "Many sentence similarity approaches have been proposed but many of them might be difficult to implement[47], [64] or has poor performance[4], [50], [54], [64] .", "startOffset": 154, "endOffset": 158}, {"referenceID": 38, "context": "For example the works of [39], [56] are based on 8 different knowledge based measures and 3 corpus based measures which makes their implementation difficult.", "startOffset": 25, "endOffset": 29}, {"referenceID": 55, "context": "For example the works of [39], [56] are based on 8 different knowledge based measures and 3 corpus based measures which makes their implementation difficult.", "startOffset": 31, "endOffset": 35}, {"referenceID": 46, "context": "Further difficulties in other works includes the need of processing gigantic data processing [47].", "startOffset": 93, "endOffset": 97}, {"referenceID": 46, "context": "[47] used the Web 1T 5-gram dataset; a compressed text file of approximately 24 GB compressed composed of more than 1 million tri-grams extracted from 1 trillion tokens.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "Nevertheless, [47] [10] are considered comprehensive datasets and can be accessed easily once indexed.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "Nevertheless, [47] [10] are considered comprehensive datasets and can be accessed easily once indexed.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "[6], [55], [40]) perform better than knowledge based (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 54, "context": "[6], [55], [40]) perform better than knowledge based (e.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "[6], [55], [40]) perform better than knowledge based (e.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "[29]) and corpus based (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10]) methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "Tri-gram measure [47] is an exception.", "startOffset": 17, "endOffset": 21}, {"referenceID": 46, "context": "It is found that [47] overestimates the human rating scores especially the dissimilar sentence pairs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 46, "context": "Figure 4 shows the STS-65 dataset human scores versus the scores of [47] and [40].", "startOffset": 68, "endOffset": 72}, {"referenceID": 39, "context": "Figure 4 shows the STS-65 dataset human scores versus the scores of [47] and [40].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "It is clear that [47] overestimates sentence pairs 1-29(30% of the original dataset).", "startOffset": 17, "endOffset": 21}, {"referenceID": 39, "context": "On the other hand, although [40] has less Pearson correlation, as shown in Figure 3, it is relatively better than [47] in sentence pairs 1-29.", "startOffset": 28, "endOffset": 32}, {"referenceID": 46, "context": "On the other hand, although [40] has less Pearson correlation, as shown in Figure 3, it is relatively better than [47] in sentence pairs 1-29.", "startOffset": 114, "endOffset": 118}, {"referenceID": 49, "context": "8 1 Worst Human Participant Ming Che Lee 2011[50] Mihalcea et al.", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "2009 [39] Feng et al.", "startOffset": 5, "endOffset": 9}, {"referenceID": 53, "context": "2008 [54] Croft et al.", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "2013 (LSS) [4] Li et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 28, "context": "2006(STASIS)[29] Human Participants Means O\u2019shea et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "2008 (LSA)[10] Liu et al.", "startOffset": 10, "endOffset": 14}, {"referenceID": 54, "context": "2007 [55] Islam et al.", "startOffset": 5, "endOffset": 9}, {"referenceID": 42, "context": "2008 [43] Tsatsaronis et al 2010 [40] Ho et al.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "2008 [43] Tsatsaronis et al 2010 [40] Ho et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "2010 [6] Islam et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 46, "context": "2012 [47] Pearson Correlation", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "Corresponding Sentence pairs [47] [40]", "startOffset": 29, "endOffset": 33}, {"referenceID": 39, "context": "Corresponding Sentence pairs [47] [40]", "startOffset": 34, "endOffset": 38}], "year": 2016, "abstractText": "Sentence similarity is considered the basis of many natural language tasks such as information retrieval, question answering and text summarization. The semantic meaning between compared text fragments is based on the words\u2019 semantic features and their relationships. This article reviews a set of word and sentence similarity measures and compares them on benchmark datasets. On the studied datasets, results showed that hybrid semantic measures perform better than both knowledge and corpus based measures. General Terms Semantic Similarity, Natural Language Processing, Computational Linguistics, Text Similarity", "creator": "Microsoft\u00ae Word 2013"}}}