{"id": "1610.04631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification", "abstract": "linear discriminant analysis ( lda ) is a widely - used supervised dimensionality reduction method in computer vision and pattern recognition. in null mean space based lda ( nlda ), a well - known lda extension, between - class distance is maximized in the null space of the within - value class scatter matrix. unfortunately however, there are some limitations in nlda. firstly, for example many data sets, null space of within - class scatter matrix does not exist, thus nlda is nevertheless not applicable to those datasets. secondly, nlda uses arithmetic mean terms of fewer between - class distances and gives equal consideration to all between - class distances, which makes larger between - class distances can dominate the result and thus limits the performance of nlda. in this paper, we propose a harmonic mean based linear discriminant analysis, multi - class discriminant analysis ( mcda ), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between - class distance. more closely importantly, mcda gives higher priority to maximize small between - class distances. mcda can be extended to multi - label dimension reduction. results on 7 single - label data sets and 4 multi - label data sets show that mcda has approximately consistently better performance than 10 other single - label approaches and 4 other multi - label approaches in terms of classification accuracy, macro and 360 micro average f1 score.", "histories": [["v1", "Fri, 14 Oct 2016 20:36:57 GMT  (1784kb)", "http://arxiv.org/abs/1610.04631v1", "IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016"], ["v2", "Mon, 24 Oct 2016 16:38:29 GMT  (221kb,D)", "http://arxiv.org/abs/1610.04631v2", "IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016"]], "COMMENTS": "IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["shuai zheng", "feiping nie", "chris ding", "heng huang"], "accepted": false, "id": "1610.04631"}, "pdf": {"name": "1610.04631.pdf", "metadata": {"source": "CRF", "title": "A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification", "authors": ["Shuai Zheng", "Feiping Nie", "Chris Ding", "Heng Huang"], "emails": ["zhengs123@gmail.com,", "feipingnie@gmail.com,", "chqding@uta.edu,", "heng@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n04 63\n1v 1\n[ cs\n.C V\n] 1\n4 O\nct 2\n01 6\nKeywords-Dimensionality Reduction; Linear Discriminant Analysis; Image Classification\nI. INTRODUCTION\nResearchers and engineers nowadays have larger and larger data with very high dimension to be processed everyday [1]. For example, in image classification, a small image of size 100 \u00d7 100 will have 10, 000 dimension. In biology science, high-dimensional gene expression data is used to predict tumor [2]. However, there is always an underlying low-dimensional structure which can capture the underlying main attributes of high-dimensional data. What\u2019s more, high-dimensional data costs a lot of computing resources. Dimensionality reduction algorithms have been proposed to extract important features from high-dimensional data.\nDimensionality reduction is important in many applications of statistics, pattern recognition and machine learning. Many methods have been proposed for dimensionality reduction, such as principal component analysis (PCA) [3] and linear discriminant analysis (LDA) [4]. LDA is a popular supervised dimensionality reduction. To be specific, let X \u2208 \u211cp\u00d7n be the data matrix, and X = (x1, \u00b7 \u00b7 \u00b7 ,xn),\nwhere p is data dimension, n is number of data points. Let G \u2208 \u211cp\u00d7k be the transformation matrix to a k-dimensional subspace. The between-class scatter matrix Sb, within-class scatter matrix Sw and total scatter matrix St is defined as:\nSb =\nK\u2211\nk=1\nnk(mk \u2212m)(mk \u2212m) T , (1)\nSw =\nK\u2211\nk=1\n\u2211\nxi\u2208k\n(xi \u2212mk)(xi \u2212mk) T , (2)\nSt = Sb + Sw, (3)\nwhere K is total class number, nk is number of points in class k, mk = \u2211 xi\u2208k\nxi/nk is the mean of class k, m =\u2211n i=1 xi/n is the mean of entire data set. Sb, Sw and St are semi-positive definite matrices. Classical LDA finds a transformation matrix G by solving the problem:\nmax G\nTr GTSbG\nGTSwG . (4)\nWhen GTSwG = 0, i.e., there exist null space of Sw, the above formulation has some difficulty. In null space based LDA (NLDA) [5], the between-class distance is maximized in the null space of within-class scatter matrix Sw,\nmax G\nTr(GTSbG), s.t. G TSwG = 0, G TG = I (5)\nwhich is based on the idea that the null space of Sw contains sufficient discriminant information. The dimension of the null space of Sw is at least p\u2212(n\u2212K). When p \u2264 (n\u2212K), the null space of Sw may not exist. In the era of big data, we usually have sufficient amount of training data, so p is usually less than (n \u2212 K). Thus, NLDA is not applicable to many problems. What\u2019s more, NLDA gives equal consideration to all between-class distances, which makes larger between-class distances could dominate the objective function and thus limits the performance of NLDA.\nIn this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), to overcome the limitations of NLDA. MCDA minimizes within-class distance and maximizes weighted pairwise between-class distance. More importantly, MCDA\ngives higher priority to maximize small pairwise betweenclass distance."}, {"heading": "II. LIMITATIONS OF NLDA", "text": "NLDA has some limitations. Firstly, null space of withinclass scatter matrix Sw \u2208 \u211cp\u00d7p may not exist for many data sets, where p is data dimension. Rank of Sw is at most n\u2212K , where n is sample number, K is class number. Thus the null space dimension of Sw is at least p\u2212(n\u2212K). When p \u2264 (n\u2212K), the null space of Sw may not exist.\nSecondly, NLDA gives equal consideration to all betweenclass distances, which makes larger between-class distances could dominate the objective function and thus limits the performance of NLDA. NLDA solves the problem of Eq.(5), which maximizes the distance between mk (the center of class k) and m (the center of all data). However, it gives equal consideration to all between-class distances. Figure 1 shows two solutions on a toy data. This toy data has 3 classes and each class contains 10 points. Figure 1a shows the NLDA solution, where GTSwG = 0, ensures the solution is in the null space of within-class scatter matrix Sw. Data points of the same class overlap with each other. The maximized sum of squared between-class distance Tr(GTSbG) = 0.6401. However, as we can see from the figure, even though the sum of squared between-class distance is maximized, class 1 and class 3 are not discriminated. Figure 1b gives a better solution, where all the 3 classes are separated evenly. Solution of Figure 1b is also in the null space of within-class scatter matrix Sw, but the sum of squared between-class distance Tr(GTSbG) = 0.4445, which is not maximized from the view of NLDA."}, {"heading": "III. A HARMONIC MEAN BETWEEN-CLASS DISTANCE OBJECTIVE", "text": "As we can see from the demonstration in Figure 1, pairwise between-class distance plays an important role in the result. Figure 1b is a better solution than Figure 1a, because all 3 classes in the solution are clearly separated and no two classes are too close to each other. In order to achieve this goal, we introduce the use of pair-wise between-class\ndistance. To incorporate pairwise between-class distance into our objective, we define pairwise between-class scatter matrix Bk1k2 for class k1 and k2 as:\nBk1k2 = (mk1 \u2212mk2)(mk1 \u2212mk2) T . (6)\nWe wish to maximize the between class distances. Instead of the traditional approach of maxTr(GTSbG) in Eq.(5), we maximize all pairs of the pairwise between-class distance:\nmax G\nK\u22121\u2211\nk1=1\nK\u2211\nk2=k1+1\nnk1nk2Tr(G TBk1k2G), s.t. G TG = I\n(7)\nwhere nk1 is the number of points in class k1, nk2 is the number of points in class k2. The key point of this new approach is that Sb is a global summation, insensitive to individual class variations, but Bk1k2 is more sensitive to individual class variations.\nThe weight nk1nk2 controls the relative importance of between class distance between class k1 and k2. The reason why we use nk1nk2 is because the importance of separating two classes is based on the number of points in these two classes respectively.\nIn above approach, although Bk1k2 is more sensitive than Sb, the addition of all pairs, or the arithmetic mean of pairwise between-class distance, is not robust: one large between-class distances could dominate the objective function.\nIt is well-known that the harmonic mean is more robust than arithmetic mean. Thus, we propose to minimize the inverse of harmonic mean of pairwise between-class distance:\nmin G\nK\u22121\u2211\nk1=1\nK\u2211\nk2=k1+1\nnk1nk2 Tr(GTBk1k2G) s.t. GTG = I. (8)\nClearly, the difficult case is when two classes are close in feature space; In this case, their between-class distances is small. The objective of Eq.(8) gives higher weight to this pair of classes, therefore correctly emphasize is critical part of the discrimination task. In contrast, the objective of Eq.(7) gives small weight to this difficult pair, thus incorrectly deemphasize the critical part of the discrimination task. This is the key reason we propose the harmonic mean objective."}, {"heading": "IV. MULTI-CLASS DISCRIMINANT ANALYSIS (MCDA)", "text": "In this section, we propose a generalized, efficient and stable LDA formulation, Multi-Class Discriminant Analysis (MCDA).\nSince in NLDA, for many applications, the null space of within-class scatter matrix does not exist, which means there is no such transformation matrix G, such that GTSwG = 0, or Tr(GTSwG) = 0. A revised version of the objective is to minimize Tr(GTSwG), so we have:\nmin G\nTr(GTSwG) s.t. GTG = I. (9)\nAlgorithm 1 Gradient descent algorithm for MCDA.\nInput: Data matrix X \u2208 \u211cp\u00d7n with n data points and p dimension; class indicator matrix Y \u2208 \u211cn\u00d7K , K is number of classes; subspace dimension k Output: Projection matrix G \u2208 \u211cp\u00d7k\n1: Initialize G 2: Compute Sw and Bk1k2 using Eq.(2) and Eq.(6) 3: while Objective value Eq.(10) dose not converge do 4: Compute gradient using Eq.(11) 5: Update G using gradient 6: Use SVD to enforce constraint GTG = I (every few\niterations) 7: end while\nIn MCDA, we use Eq.(8) to maximize between-class distance, because Eq.(8) is a robust between-class distance objective and it gives higher priority to maximize small between-class distances. To summarize, the objective of Multi-Class Discriminant Analysis (MCDA) is proposed as\nmin G\nJMCDA = \u03b3Tr(G TSwG) +\nK\u22121\u2211\nk1=1\nK\u2211\nk2=k1+1\nnk1nk2 Tr(GTBk1k2G) ,\ns.t. GTG = I, (10)\nwhere \u03b3 controls the weight between minimizing withinclass distance and maximizing between-class distance, constraint GTG = I ensures the columns of solution G are linearly independent. When \u03b3 \u2192 \u221e, Eq.(10) focuses on minimizing within-class distance only, which is equal to finding the null space of within-class scatter matrix Sw. When \u03b3 \u2192 0, Eq.(10) focuses on maximizing pairwise between-class distance."}, {"heading": "V. ALGORITHM", "text": "We use gradient descent to solve Eq.(10). The gradient is:\n\u2202JMCDA\n\u2202G = 2\u03b3SwG\u2212\nK\u22121\u2211\nk1=1\nK\u2211\nk2=k1+1\n2nk1nk2Bk1k2G (TrGTBk1k2G)2 . (11)\nIn order to enforce the constraint requirement GTG = I , we use SVD decomposition to make G orthonormal every few iterations. If we apply SVD in every iteration, it will be computational expensive. Thus, we can apply SVD every few iterations, which is on projection matrix G (subspace dimension is very small), and is very fast. Algorithm 1 summarizes the steps to solve Eq.(10). The objective is optimized in an iterative fashion. There is no need to do Eigen decomposition or matrix inverse for scatter matrices.\nWhen initializing matrix G, if subspace dimension k <= K\u22121, we can use classical LDA Eq.(4) solution to initialize G; when k > K \u2212 1, we can use trace ratio LDA Eq.(12) solution to initialize G. This ensures that our approach can find a better solution than other LDA formulations (see experiments part for comparison).\nVI. ILLUSTRATION\nTo show the effectiveness of proposed MCDA, Figure 2 visualizes two real data sets, Caltech and MSRC, in 2-D subspace using PCA, LDA, null space LDA (NLDA) and MCDA. In this example, we take 4 classes from Caltech101 and MSRC, 30 data points in each class. We extract 432- dimensional HOG feature. Figure 2a and 2e show the Caltech and MSRC data projected in 2-D PCA subspace. As we can see, data points from 4 classes are mixed together. Figure 2b and 2f show the data projected in 2-D LDA subspace. Figure 2c and 2g shows the Caltech and MSRC data projected in 2-D NLDA subspace. Data points have been projected on 4 different data points, each of which includes 30 overlapped points, which is because the withinclass distance in this subspace is 0 now. However, class 1 and class 4 in both Figure 2c and 2g are still very close. This is due to the limitations of NLDA we discussed in Figure 1. By using MCDA proposed in Eq.(10), there are no two classes becoming too close in Figure 2d and 2h. MCDA takes weighted harmonic mean of pairwise betweenclass distance into account and gives higher priority to small between-class distances."}, {"heading": "VII. CONNECTION WITH TRACE RATIO", "text": "Trace ratio was proposed in [6], [7], [8] to solve the following problem:\nmax G Tr(GTSbG) Tr(GTStG) s.t. GTG = I. (12)\nSince Eq.(12) maximize Tr(GTSbG), it maximizes the arithmetic mean of between-class distance as well and also suffers from the robustness problem as discussed in Eq.(7).\nTrace ratio problem can be reduced to NLDA when the reduced subspace dimension k is not larger than the dimension of null space of Sw. Using St = Sb + Sw, Eq.(12) is equivalent to maxG\nTr(GTSbG) Tr(GTSbG)+Tr(GTSwG) =\nmaxG 1\n1+Tr(GTSwG)/Tr(GTSbG) . Since Sb, Sw and St are\nall semi-positive definite, the maximum objective value can be achieved when Tr(GTSwG) = 0. This means that the optimal solution G is in the null space of within-class scatter matrix Sw."}, {"heading": "VIII. MULTI-LABEL MCDA", "text": "In image and video annotation, each image is usually associated with several different conceptual classes. Let\u2019s take two sample images from MSRC data in Figure 4 as an example. Figure 4a is annotated using 3 words: sky, plane and grass; Figure 4b is annotated using 3 words: car, building, road. In machine learning, such problem that requires each data point to be assigned to multiple different classes\nis called multi-label classification problem. In contrast, in\ntraditional single-label classification, which is also called single-label multi-class classification, each data point is only classified into one category. Multi-label multi-class problem is more generalized than single-label multi-class problem.\nAn important difference between single-label classification and multi-label classification is that class memberships in single-label classification are mutually exclusive, while class memberships in multi-label classification are overlapped with 2 or more classes. Class memberships can be inferred from label correlations, which can be used to improve classification. It has stimulated many multi-label learning algorithms [9] [10] [11] [12].\nHowever, Linear Discriminant Analysis (LDA) by nature is derived for single-label classification. Wang proposed a multi-label formulation of scatter matrices for multi-label data in [12]. Multi-label class indicator matrix Y \u2208 \u211cn\u00d7K is defined as\nYik =\n{ 1, if point i is in class k.\n0, otherwise. (13)\nFor data point i, \u2211\nk Yik > 1, which means that data i belongs to more than 1 class. Multi-label between-class scatter matrix S\u0303b and within-class scatter matrix S\u0303w are\ndefined as follows [12]:\nS\u0303b =\nK\u2211\nk=1\n(\nn\u2211\ni=1\nYik)(mk \u2212m)(mk \u2212m) T , (14)\nS\u0303w =\nK\u2211\nk=1\nn\u2211\ni=1\nYik(xi \u2212mk)(xi \u2212mk) T , (15)\nwhere mk is the mean of class k and m is global mean, defined as follows:\nmk = \u2211n i=1 Yikxi\u2211n i=1 Yik , m = \u2211K k=1 \u2211n i=1 Yikxi\u2211K\nk=1 \u2211n i=1 Yik . (16)\nEq.(14,15) is also equivalent to Eq.(28, 29, 30) in [13]. Using Eq.(15) and Eq.(16), the objective of Multi-label MCDA is proposed as\nmin G\n\u03b3Tr(GT S\u0303wG) + K\u22121\u2211\nk1=1\nK\u2211\nk2=k1+1\nnk1nk2\nTr(GT B\u0303k1k2G) , (17)\ns.t. GTG = I,\nwhere B\u0303k1k2 is between-class scatter matrix for class k1 and k2:\nB\u0303k1k2 = (mk1 \u2212mk2)(mk1 \u2212mk2) T . (18) nk1 = n\u2211\ni=1\nYik1 , nk2 = n\u2211\ni=1\nYik2 . (19)"}, {"heading": "IX. EXPERIMENTS", "text": "In this section, we first study the convergence of Algorithm 1, and the effect of reduced dimension number of MCDA. Then we compare classification accuracy of MCDA with 10 other subspace learning algorithms using reduced dimension number K \u2212 1 (K is class number). Finally, we experiment the classification accuracy and macro and micro average F score for multi-label data sets.\nSingle-label Dataset 7 single-label datasets are used in this experiment. Caltech101[14] contains 101 object categories. We then use VLFeat [15] to extract HOG feature. Caltech07-HOG contains 7 categories randomly selected from Caltech101 and each category has 30 images. Caltech20-HOG contains 20 categories randomly selected from Caltech101 and each category has 30 images. MSRC [16] is from MSRC data base v1 and contains 7 classes with 30 images in each class. We use the HOG and GIST feature of MSRC. Other datasets include face datasets ATT\n[17], digit datasets MNIST [18] and handwritten alphabets Binalpha. Table I summarizes the attributes of single-label datasets. Figure 3 shows sample images from the data.\nMulti-label Dataset 4 multi-label datasets are used in this experiment. Barcelona data set contains 139 images with 4 categories, i.e., building, flora, people and sky. Each image has at least two labels. Yeast is a multi-label data from [19]. MSRC [16] is MSRC multi-label data base v2 provided by Microsoft Research Cambridge, which has 591 images annotated by 23 classes. Scene is a multi-label image data from [20]. Table II summarizes the attributes of those datasets."}, {"heading": "A. Convergence of Algorithm 1", "text": "We take the first 4 single-label datasets, Caltech07HOG, Caltech20-HOG, MSRC-HOG, MSRC-GIST, as examples to check the convergence of the Algorithm 1. In order to find a reasonable guess for \u03b3, the first part \u03b3Tr(GTSwG) of Eq.(10) and the second part\u2211K\u22121\nk1=1 \u2211K k2=k1+1\nnk1nk2 Tr(GTBk1k2G) should be in similar scale.\nTo get an approximate value, we set G = I in Eq.(10), if \u03b3Tr(GTSwG) = \u2211K\u22121 k1=1 \u2211K k2=k1+1\nnk1nk2 Tr(GTBk1k2G) , we have\n\u03b3 = 1\nTrSw\nK\u22121\u2211\nk1=1\nK\u2211\nk2=k1+1\nnk1nk2 TrBk1k2 .\nFigure 5 shows the objective value of Eq.(10) while using the above \u03b3. We can see that all the 4 objective values converge quickly in about 50 iterations."}, {"heading": "B. Effect of subspace dimension", "text": "Standard LDA can find subspace dimension from 1 to K \u2212 1. MCDA does not have dimension limit. So in this part, we study MCDA subspace classification performance with respect to subspace dimension and we compare the performance with standard LDA and trace ratio. For standard LDA, we only compute reduced dimension from 1 to K\u22121. After using dimension reduction, KNN classifier (knn = 3) is applied to perform classification. The classification accuracy is the average of 5-fold cross validation results. Figure 6 shows the classification accuracy of MCDA, LDA and Trace Ratio. We can see from the result that MCDA has higher classification accuracy than LDA and Trace Ratio when using the same number of dimension on all 4 data sets."}, {"heading": "C. Single-label classification experiment", "text": "On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24],\nOrthogonal Centroid Method(OCM) [25] and Orthogonal Least Squares LDA(OLSLDA)[26].\nIn experiment, we reduce raw data to K\u22121 dimension. We use 5-fold cross validation to select training and testing data. After selecting training and testing data, we tune parameters based on the selected training data only. We tune weight parameter \u03b3 in Eq.(10) from 10\u221210 to 1010 and use the best result for the selected training data only. Solution projection matrix G was solved based on only the training set. Then we apply G on both training and testing data and then KNN is applied to do the classification. In our experiment, we set the nearest neighbors number knn = 3. The final classification accuracy is the average of 5-fold cross validation results, and is reported in Table III. MCDA outperforms all other 10 algorithms. Note, parameters of other algorithms have also been tuned to the best value, such as the regularization parameter of regularized LDA. For Caltech20-HOG and BinAlpha, null space of Sw does not exist."}, {"heading": "D. Multi-label classification experiment", "text": "We compare the performance of Multi-label MCDA with 4 other multi-label algorithms on 4 multi-label datasets in terms of macro accuracy (Table IV), macro-averaged F1score (Table V) and micro-averaged F1-score (Table VI). F1-score is defined as: F1 = 2\u00d7 precision\u00d7recallprecision+recall . Macroaverage is the average based on the overall testing dataset, while micro-average is the average which gives equal weight to each class. Macro-averaged and micro-averaged F1-score are widely used as a metric to evaluate classification performance [27].\nThe algorithms for multi-label dataset we compared in this section include Multi-label informed Latent Semantic Indexing (MLSI) [9], Multi-label Dimensionality reduction via Dependence Maximization (MDDM) [10], Multi-Label Least Square (MLLS) [11], Multi-label Linear Discriminant Analysis (MLDA) [12].\nThe experiment used 5-fold cross validation to evaluate the classification performance of different algorithms when dimension is K \u2212 1. K-Nearest Neighbour (KNN) classifier is then used after each algorithm. As we can see from Table IV, V) and VI, Multi-label MCDA clearly outperforms other 4 algorithms."}, {"heading": "X. RELATED WORK", "text": "Researchers and engineers nowadays have larger and larger data with very high dimension to be processed everyday [1]. Many big data technologies including cloud computing, dimension reduction, accelerating algorithms have been proposed [28] [29] [30] [31] [32] [33]. Trace ratio problem has been studied thoroughly in recent years. Many dimension reduction algorithms can be reduced to a trace ratio objective. But trace ratio problem does not have closed-form solution. Thus how to solve trace ratio efficiently becomes an interesting research topic. Wang [6] proposed an efficient iterative algorithm to get an approximate solution. Shen [34] proposed a formulation for solving the trace ratio problem directly. Nie proposed a Trace Ratio criteria for feature selection[35]. Each feature subset has a feature score, which is computed by trace ratio. They propose an iterative algorithm to find the global optimal feature subset. A number of LDA reformulation ideas have be proposed in recent years, such as PCA+LDA [36], regularized LDA(RLDA) [23], null space LDA (NLDA) [5], Orthogonal Centroid Method (OCM) [25], Uncorrelated LDA(ULDA)[24], Orthogonal LDA (OLDA)[24], etc. Ye introduced a unified framework for generalized LDA in [37]. The unified framework consists of four steps:\n1) Compute the eigenvalues {\u03bbi}di=1 and eigenvectors {ui} d i=1 of total covariance matrix St, where d is the\ndimension of data. So St = \u2211d i=1 \u03bbiuiu T i .\n2) Given a transfer function \u03c6: \u03bb\u0303i = \u03c6(\u03bbi). Construct S\u0303t = \u2211d i=1 \u03bb\u0303iuiu T i . 3) Compute the eigenvectors of matrix S\u0303+t Sb that correspond to the largest q eigenvalues, where q is the rank of Sb and S\u0303 + t means pseudo-inverse of S\u0303t. Construct\nmatrix G using these q eigenvectors. 4) Optional: compute the QR decomposition of G = QR.\nThe final projection is given as G or Q. In RLDA, the transfer function is \u03c6(\u03bbi) = \u03bbi + \u00b5. In ULDA, \u03c6(\u03bbi) = \u03bbi and the optional QR decomposition is not applied. In OLDA, \u03c6(\u03bbi) = \u03bbi +\u00b5 and the optional QR decomposition is applied. In OCM, the optimal transformation is the top eigenvectors of Sb and the transfer function is \u03c6(\u03bbi) = 1."}, {"heading": "XI. CONCLUSIONS", "text": "In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), which makes use of weighted harmonic mean of pairwise between-class distance and gives higher priority to maximize small between-class distances. MCDA has been extended to multi-label dimension reduction. Extensive experiments on 7 single-label datasets and 4 multi-label datasets show that MCDA outperforms 10 other single-label algorithms and 4 multi-label algorithms consistently in terms of classification accuracy, macro and micro average F1 score."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is partially supported by NSF1356628, NSF1633753."}], "references": [{"title": "A survey of dimension reduction techniques", "author": ["I.K. Fodor"], "venue": "2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Dimension reduction for classification with gene expression microarray data", "author": ["J.J. Dai", "L. Lieu", "D. Rocke"], "venue": "Statistical applications in genetics and molecular biology, vol. 5, no. 1, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A new lda-based face recognition system which can solve the small sample size problem", "author": ["L.-F. Chen", "H.-Y.M. Liao", "M.-T. Ko", "J.-C. Lin", "G.-J. Yu"], "venue": "Pattern recognition, vol. 33, no. 10, pp. 1713\u20131726, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Trace ratio vs. ratio trace for dimensionality reduction", "author": ["H. Wang", "S. Yan", "D. Xu", "X. Tang", "T. Huang"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Trace ratio problem revisited", "author": ["Y. Jia", "F. Nie", "C. Zhang"], "venue": "Neural Networks, IEEE Transactions on, vol. 20, no. 4, pp. 729\u2013735, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Neighborhood minmax projections.", "author": ["F. Nie", "S. Xiang", "C. Zhang"], "venue": "in IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Multi-label informed latent semantic indexing", "author": ["K. Yu", "S. Yu", "V. Tresp"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2005, pp. 258\u2013265.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Multilabel dimensionality reduction via dependence maximization", "author": ["Y. Zhang", "Z.-H. Zhou"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 4, no. 3, p. 14, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting shared subspace for multi-label classification", "author": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2008, pp. 381\u2013389.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label linear discriminant analysis", "author": ["H. Wang", "C. Ding", "H. Huang"], "venue": "Computer Vision\u2013ECCV 2010. Springer, 2010, pp. 126\u2013139.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised orthogonal discriminant analysis via label propagation", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang"], "venue": "Pattern Recognition, vol. 42, no. 11, pp. 2615\u20132627, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding, vol. 106, no. 1, pp. 59\u201370, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "Proceedings of the international conference on Multimedia. ACM, 2010, pp. 1469\u20131472.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Locus: Learning object classes with unsupervised segmentation", "author": ["J. Winn", "N. Jojic"], "venue": "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, vol. 1. IEEE, 2005, pp. 756\u2013763.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Applications of Computer Vision, 1994., Proceedings of the Second IEEE Workshop on. IEEE, 1994, pp. 138\u2013142.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A kernel method for multilabelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "Advances in neural information processing systems, 2001, pp. 681\u2013687.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern recognition, vol. 37, no. 9, pp. 1757\u20131771, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A semi-definite positive linear discriminant analysis and its applications", "author": ["D. Kong", "C. Ding"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 942\u2013947.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient and robust feature extraction by maximum margin criterion", "author": ["H. Li", "T. Jiang", "K. Zhang"], "venue": "Neural Networks, IEEE Transactions on, vol. 17, no. 1, pp. 157\u2013165, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularized linear discriminant analysis and its application in microarrays", "author": ["Y. Guo", "T. Hastie", "R. Tibshirani"], "venue": "Biostatistics, vol. 8, no. 1, pp. 86\u2013100, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems", "author": ["J. Ye"], "venue": "Journal of Machine Learning Research, 2005, pp. 483\u2013502.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Lower dimensional representation of text data based on centroids and least squares", "author": ["H. Park", "M. Jeon", "J.B. Rosen"], "venue": "BIT Numerical mathematics, vol. 43, no. 2, pp. 427\u2013 448, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Orthogonal vs. uncorrelated least squares discriminant analysis for feature extraction", "author": ["F. Nie", "S. Xiang", "Y. Liu", "C. Hou", "C. Zhang"], "venue": "Pattern Recognition Letters, vol. 33, no. 5, pp. 485\u2013491, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation: From precision, recall and f-measure to roc., informedness, markedness & correlation", "author": ["D. Powers"], "venue": "Journal of Machine Learning Technologies, vol. 2, no. 1, pp. 37\u201363, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis and modeling of social influence in high performance computing workloads", "author": ["S. Zheng", "Z.-Y. Shae", "X. Zhang", "H. Jamjoom", "L. Fong"], "venue": "European Conference on Parallel Processing. Springer Berlin Heidelberg, 2011, pp. 193\u2013204.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Virtual machine migration in an over-committed cloud", "author": ["X. Zhang", "Z.-Y. Shae", "S. Zheng", "H. Jamjoom"], "venue": "2012 IEEE Network Operations and Management Symposium. IEEE, 2012, pp. 196\u2013203.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Tidewatch: Fingerprinting the cyclicality of big data workloads", "author": ["D. Williams", "S. Zheng", "X. Zhang", "H. Jamjoom"], "venue": "IEEE INFOCOM 2014-IEEE Conference on Computer Communications. IEEE, 2014, pp. 2031\u20132039.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Kernel alignment inspired linear discriminant analysis", "author": ["S. Zheng", "C. Ding"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2014, pp. 401\u2013416.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "A closed form solution to multi-view low-rank regression.", "author": ["S. Zheng", "X. Cai", "C.H. Ding", "F. Nie", "H. Huang"], "venue": "in AAAI,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Accelerating deep learning with shrinkage and recall", "author": ["S. Zheng", "A. Vishnu", "C. Ding"], "venue": "arXiv preprint arXiv:1605.01369, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "A convex programming approach to the trace quotient problem", "author": ["C. Shen", "H. Li", "M.J. Brooks"], "venue": "Computer Vision\u2013 ACCV 2007. Springer, 2007, pp. 227\u2013235.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Trace ratio criterion for feature selection.", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "in AAAI,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Subspace linear discriminant analysis for face recognition", "author": ["W. Zhao", "R. Chellappa", "P.J. Phillips"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Researchers and engineers nowadays have larger and larger data with very high dimension to be processed everyday [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "In biology science, high-dimensional gene expression data is used to predict tumor [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Many methods have been proposed for dimensionality reduction, such as principal component analysis (PCA) [3] and linear discriminant analysis (LDA) [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "Many methods have been proposed for dimensionality reduction, such as principal component analysis (PCA) [3] and linear discriminant analysis (LDA) [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "In null space based LDA (NLDA) [5], the between-class distance is maximized in the null space of within-class scatter matrix Sw, max G Tr(GSbG), s.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "CONNECTION WITH TRACE RATIO Trace ratio was proposed in [6], [7], [8] to solve the following problem:", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "CONNECTION WITH TRACE RATIO Trace ratio was proposed in [6], [7], [8] to solve the following problem:", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "CONNECTION WITH TRACE RATIO Trace ratio was proposed in [6], [7], [8] to solve the following problem:", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Wang proposed a multi-label formulation of scatter matrices for multi-label data in [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Multi-label between-class scatter matrix S\u0303b and within-class scatter matrix S\u0303w are defined as follows [12]:", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "(28, 29, 30) in [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Caltech101[14] contains 101 object categories.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "We then use VLFeat [15] to extract HOG feature.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "MSRC [16] is from MSRC data base v1 and contains 7 classes with 30 images in each class.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "[17], digit datasets MNIST [18] and handwritten alphabets Binalpha.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[17], digit datasets MNIST [18] and handwritten alphabets Binalpha.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "Yeast is a multi-label data from [19].", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "MSRC [16] is MSRC multi-label data base v2 provided by Microsoft Research Cambridge, which has 591 images annotated by 23 classes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "Scene is a multi-label image data from [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 184, "endOffset": 187}, {"referenceID": 20, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 259, "endOffset": 263}, {"referenceID": 22, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 287, "endOffset": 291}, {"referenceID": 23, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 316, "endOffset": 320}, {"referenceID": 23, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 344, "endOffset": 348}, {"referenceID": 24, "context": "Orthogonal Centroid Method(OCM) [25] and Orthogonal Least Squares LDA(OLSLDA)[26].", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "Orthogonal Centroid Method(OCM) [25] and Orthogonal Least Squares LDA(OLSLDA)[26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "Macro-averaged and micro-averaged F1-score are widely used as a metric to evaluate classification performance [27].", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "Linear Discriminant Analysis (LDA) is a widelyused supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score. Keywords-Dimensionality Reduction; Linear Discriminant Analysis; Image Classification", "creator": "LaTeX with hyperref package"}}}