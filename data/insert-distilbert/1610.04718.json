{"id": "1610.04718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "Generalization of metric classification algorithms for sequences classification and labelling", "abstract": "the article deals with the wider issue of modification of metric classification algorithms. in particular, besides it studies the algorithm k - nearest neighbours for developing its application parallel to sequential aggregate data. a method of generalization of simplified metric descriptive classification algorithms is proposed. as a part of it, yet there just has been developed an algorithm for solving the problem of classification and labelling of sequential data. the advantages available of the internationally developed algorithm of classification in comparison with the existing one are also discussed in the article. there is a comparison of the effectiveness of the proposed algorithm with the algorithm of crf in the task of chunking in sorting the open data set conll2000.", "histories": [["v1", "Sat, 15 Oct 2016 11:04:44 GMT  (174kb,D)", "https://arxiv.org/abs/1610.04718v1", null], ["v2", "Fri, 16 Dec 2016 05:32:34 GMT  (174kb,D)", "http://arxiv.org/abs/1610.04718v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["roman samarev", "rey vasnetsov", "elizaveta smelkova"], "accepted": false, "id": "1610.04718"}, "pdf": {"name": "1610.04718.pdf", "metadata": {"source": "CRF", "title": "Generalization of metric classification algorithms for sequences classification and labelling", "authors": ["Roman Samarev", "Andrey Vasnetsov", "Elizaveta Smelkova"], "emails": ["samarev@acm.org", "vasnetsov93@gmail.com", "lizuzu@mail.ru"], "sections": [{"heading": "1. INTRODUCTION", "text": "The classification and labelling of sequences have a wide range of practical applications. In the research of the genome, classification of protein sequences in the existing categories is used to determine the function of a new protein[6, 17]. The study of the sequence of events to access Internet-based resources may allow distinguishing a user-person from a search bot[4]. The classification and labelling of sequences are used in Natural Language Processing (NLP)[20, 7]. In General, a sequence is an ordered list of events S =< s1, s2, . . . , sn > , where an event si, in general, can be associated with any data. However, in this work, we assume that the events are presented in the form of a feature vector of a fixed length, in which a feature can be represented by a string, the real number or integer, a boolean variable or a variable of an enumerated type. Each element of a sequence, among other things, may have a label; in this case, we speak of the labelled sequence. Let L be a set of labels, then we have a function Label : si \u2192 l, l \u2208 L which maps events to labels. In addition to the labels of the elements of a sequence, the sequence itself can also have the class: Class : si \u2192 c, c \u2208 Cl where Cl is the set of possible classes. Thus, when working with sequences, there are two tasks: a sequence classification that is, the reproducing of the unknown function Class,\nand a sequence labelling, that is, the reproducing of the unknown function Label. . In the scientific literature[14], this type of classification is also called a strong sequence classification. Both tasks are supposed to have a training set. It is a set of correctly classified or labelled sequences. A lot of different methods have been proposed for solving the problem of classification. They can be divided into the following groups:\n\u2022 A methods based on fixed size feature vectors. This method requires a conversion of a sequence in a feature vector, and this conversion plays a major role in the classification.\n\u2022 A method based on an evaluation of the measure of distance between sequences. The example of such a distance is Levenshtein distance[8].\n\u2022 A classification using statistical models.\nHowever, for the solutions of strong classification tasks, only the last group of methods is suitable. The following ones should be mentioned[14]:\n\u2022 Condition Random Fields, CRF.\n\u2022 Hidden Markov Models.\n\u2022 Markov SVM[2].\n\u2022 Recurrent Neural Networks, RNN.\nThese methods are based on statistical modelling of any features of sequence elements and the prediction of their labels based on these models. In some cases, this approach may be ineffective, as, for example, despite the fact that the label signs have different categorical values, they have a different degree of proximity. For example, the letters \u201cA\u201d and \u201cE\u201d in some sense (as they sound) closer to each other than letters \u201cA\u201d and \u201cX\u201d[10], and the words \u201cred\u201d and \u201cblue\u201d are closer to each other than the words algebra and stool. It is often convenient to set such proximity with the help of a distance function. To use such a distance function in the classification process you need to have another type of algorithms, so-called metric classification algorithms, for example, the algorithm k-Nearest Neighbours (kNN). The correctness of the prediction of the elements labels often depends on the context in which this element is met. So, for example, the meaning of a word often depends on the context in which it is used. But metric algorithms accept as input data a feature vector of a fixed length and do not have\nar X\niv :1\n61 0.\n04 71\n8v 2\n[ cs\n.L G\n] 1\n6 D\nec 2\n01 6\nan internal state, so their direct application to the problem of labelling sequential data is not effective enough. The article proposes a method of modifying metric classification algorithms that allows the algorithms to consider the context of the element of a sequence, thereby improving the classification accuracy."}, {"heading": "2. GENERALIZATION OF CLASSIFICATION ALGORITHMS FOR SEQUENCES", "text": "To substantiate the possibility of modification of metric classification algorithms we consider the relation between the existing classification algorithms and methods applied to them to enable work with sequences. One of the promising directions for solving strong classification is recurrent neural networks (RNN), which are different from the usual neural networks in that fact that they have feedback. This feedback means the connection between more logically remote elements to less remote ones. The feedback connection allows storing and reproducing a number of sequences of reactions to a single impulse. Due to the fact that the kNN algorithm has no internal state, the use of generalization method which has been applied to the RNN is not possible.\nFigure 1 shows a diagram of the relationship between the algorithms of HMM, CRF, Naive Bayes and Logit Regression[18]. The diagram shows that the HMM algorithm is a generalization of the naive Bayes classifier on the sequence of input data. Indeed, in the working process of the HMM algorithm for each vertex, actually, the Bayes classification algorithm with its own set of input data is applied. In this case for classification is used the Viterbi algorithm, which finds a path in the graph that maximizes the probability of a sequence.\nBased on a review of the existing sequence classification algorithms we offer a method of generalization of metric classification algorithms to deal with sequential data. The method includes an algorithm for creating a graph model, the choice of the function being optimized, the modified Viterbi algorithm. Lets consider the proposed method on the example of the kNN algorithm. The kNN algorithm in the process of its work relies on the hypothesis of compactness: close objects tend to lie in the same class, the close ones refer to the objects having the smallest among all the other distances in certain, pre-determined metric. This hy-\npothesis is naturally generalized to the problem of sequence classification.\nThe hypothesis of compactness of sequences: each element of a sequence, as a rule, belongs to the class which minimizes the total distance of all elements of this sequence to the known elements of the appropriate class. Thus, the classification algorithm of sequences, which is a generalization of kNN, is supposed to search for a sequence of classes which will minimize the total distance of each element of the classified sequence to the nearest element of the corresponding class in the training set. Since transitions between the classes in accordance with the training set, form a kind of graph structure we will call the described algorithm as Structured k-Nearest Neighbours (SkNN). The relationship of the algorithms is shown in figure 2."}, {"heading": "2.1 Structured k-Nearest Neighbours definitions", "text": "To consider SkNN models we introduce the following definitions:\n1. k - a number of considered closest elements.\n2. L - a set of possible labels of classes of features of sequences.\n3. Cl - a set of possible classes of sequences.\n4. S - a set of features of sequences.\n5. X - a set of sequences of a training set, X = {(s)n : n \u2208 N, s \u2208 S}\n6. G = (V,E) - graph describing the structure of the classifier.\n7. fvl : V 7\u2192 L - function that determines the correspondence of vertex and labels. The function is injective.\n8. fsl : S 7\u2192 L - function determines the class label for each element of training set.\n9. fsv : S 7\u2192 V - function determines to which vertex the element of the training set belongs to. All the first elements of sequences belong to Vinit.\n10. C \u2208 V - the current state of machine. At time \u03c4 = 0, C = Vinit.\nThe kNN algorithm is based on a directed graph G = (V,E), its each vertex corresponds to either the beginning of the sequence (we call this vertex conventionally Vinit \u2208 V ), or to a possible class of feature of sequence Vl, l \u2208 L, or to the end of the sequence Vend \u2208 V .\nThe following conditions must be met: Condition 1. Edge Einti,x is present in the graph G if and only if the training set has 2 consecutive elements \u03b1 \u2208 S, and classes of elements \u03b2 \u2208 S, \u03b1 : L\u03b1 = x and \u03b2 : L\u03b2 = y.\nCondition 2. Edge Einti,x is present if and only if a training set has a sequence the first element of which has class x.\nCondition 3. Edge Ex,end is present if and only if a training set has a sequence the last element of which has class x.\nCreating the SkNN model from a training set is carried out with the help of the algorithm 1.\nData: Training set X Result: Constructed model of SkNN. for seq \u2208 Dataset do\nC = Vinit for inst \u2208 seq, inst \u2208 S do\nl := fsl(inst) L.add(l) fsv(inst) := C; newC := fvl(l); E.add(EC,newC); C := newC;\nend E.add(EC,Vend);\nend Algorithm 1: Construction of SkNN model from training set.\nFor labeling sequences using the SkNN algorithm you can apply a modified Viterbi algorithm, which is different from the original algorithm in the fact that it minimizes the total distance. We give an example of Viterbi algorithm 2.\nFunction n dist(inst,X, k) finds k closest in some metric elements to inst in the set X and returns the weighted average distance to them."}, {"heading": "2.2 Types of features in SkNN", "text": "In the current implementation of the model, as input for the algorithm SkNN, you can use sequences of elements, each of which consists of a fixed number of features. In the general case, the requirements for the type and composition of the characteristics of the element are imposed solely by a distance function (metrics), which should be able to determine the distance between any pair of elements. It is a distinctive feature and advantage of the SkNN algorithm, as other algorithms such as HMM, CRF, Structured SVM, RNN are able to work only with the description of the element as a tuple of features of numeric and enumerated types. For SkNN, however, it is possible to use any type of features, while\nData: Model of SkNN, unlabelled sequence sequence Result: Classified sequence. for vi \u2208 V do\nT1[i, 1]\u2190 0 T2[i, 1]\u2190 Vinit\nend for insti \u2208 sequence do\nfor vj \u2208 V do T1[j, i]\u2190 min\nk (T1[k, i\u2212 1] +\nn dist(insti, inst : fsv(inst) = vj ,K)) T2[j, i]\u2190 argmin\nk (T1[k, i\u2212 1] +\nn dist(insti, inst : fsv(inst) = vj ,K)) end\nend T \u2190 sizeof(sequence) ZT \u2190 argmin\nk (T1[k, T ])\nyi \u2208 Y for i\u2190 {T, T \u2212 1, ..., 2} do\nZi\u22121 \u2190 T2[zi, i] yi\u22121 \u2190 Vzi\u22121\nend return Y\nAlgorithm 2: Modified Viterbi algorithm.\nthe semantics of the feature is not lost, but it can be represented as a distance function. For example, it is possible to introduce metric which works with elements of the ontology. In this case, the distance function can return, for example, the minimal path from one concept to the other, according to the ontology. Using a weighted summation of different features, it is possible to create metric that simultaneously takes into account the characteristics of arbitrary type, for example, features derived from the ontology and the word vector, obtained by using the algorithm word2vec[12].\n2.2.1 Obtaining graph structure using clustering\nIn a large number of existing tasks of sequence classification it is required not to label each individual element of a sequence but the entire sequence as a whole. In this case, input data does not usually have label information of each single element and the SkNN algorithm is reduced to a conventional kNN algorithm which is applied for each element separately. It is evident that this approach does not give reasonable results, so it is necessary to mine structure automatically in input data. As the basis of SkNN is the comparison of elements in some metric, then it becomes possible to use clustering for automatic mining of graph structure in sequences. To obtain such a structure, you can perform the following actions:\n\u2022 There is an initial structure in which each class of sequences is represented by a single vertex. Features of sequences included in the corresponding class are associated with vertex.\n\u2022 For each vertex we hold clustering of related features of sequences.\n\u2022 We split each clustered vertex into several vertices in such a way that each new vertex corresponds to one cluster of old vertices and it contains features of the corresponding cluster. In this case, edges are added in the graph in accordance with condition 1.\nIt is worth noting that this approach is not necessary to use solely in conjunction with SkNN. However, due to the fact that the input for clustering algorithms is a matrix of distances between objects (or a distance function / similarity), the SkNN algorithm that uses the same distance function will be native to clusterization."}, {"heading": "2.3 Sequence Classification", "text": "As it has been mentioned above, in a large number of existing tasks of sequence classification it is required not to classify each individual element of a sequence but the entire sequence as a whole. SkNNs Constructing Algorithm can be applied in this case without additional modifications. You just need to perform the following actions:\n1. For sequences of each class apply a clustering algorithm to obtain a graph structure.\n2. Combine graphs obtained on the previous step. In order to have it:\n(a) Add vertices init and end.\n(b) Add edges from init to the initial vertices of the graphs, obtained in the first step, and edges of the end vertices to the end.\nThe result is a graph consisting of non-overlapped subgraphs. Example of general view of this sub-graphs present in figure 3.\nApplying the obtained graph in the SkNN algorithm for sequence classification, due to the fact that in the graph there is no path from a sub-graph of one class to the subgraph of a different class, all the class labels will belong to the same sub-graph, and hence, one initial class of sequence. Thus, for sequence classification, it is enough to determine which of the initial classes the received labels belong to."}, {"heading": "3. EXPERIMENTS", "text": "To check the work of the algorithm and to compare it with the existing popular algorithms in practice, we conducted a series of experiments using open datasets[9]. It should be noted that the field of possible application of the SkNN algorithm is quite big: as standard benchmark datasets we used sets of such fields as recognition of handwritten symbols (UJI Pen Characters [11]) and processing of texts in natural language (CoNLL200[19])."}, {"heading": "3.1 Recognition of hand-written symbols", "text": "For the first experiment we selected a dataset representing handwritten symbols. This dataset was chosen because it contains a simple data type - a sequence of two-dimensional coordinates on the plane. Simplicity of work with such data makes it easy to visualize, which simplifies debugging of the algorithm. For testing the algorithm we compiled a training set consisting of 200 sequences of writing handwritten digits of eight different authors. Test set consisted of 40 sequences of 3 other authors. For the experiment we used the normalized Euclidean distance, as the simplest example of a metric. The experiment showed that the accuracy of the SkNN is equal to 92.5%, which is significantly higher than random guessing (10% in this case).\nIt is worth noting, even though the accuracy of the algorithm SkNN does not exceed the accuracy of the algorithms, which are intended for solving this very problem[16], but it shows the result that can be compared with them."}, {"heading": "3.2 Chunking", "text": "The second experimental dataset CoNLL200 represents sequences of words in sentences in English with the corresponding parts of speech derived (POS-tag) with the help of program Brill tagger[1] and chunk tag which has been obtained from corpora WSJ[15]. Group labels represent such parts of the sentence as verb groups, e.t.c.. In the experiment the accuracy of the SkNN algorithm was compared with the accuracy of the CRF algorithm implemented in the software utility CRF++ . This algorithm is one of the most popular and accurate algorithms today in the field of NLP. Dataset parameters: the size of training corpora 9 thousand sentences, 220 thousand words, the size of the test set - 400 sentences, 10 thousand words. As a hyperparameter for both algorithms there is a size of context window which is added to the current feature of sequence. The accuracy of the algorithms was compared with equal values of this hyperparameter.\nAnother parameter for the SkNN algorithm is metric which is used for distance calculation. The experiment was carried\nout using different metrics which were most successfully applied in practice in the field NLP[3]. Metrics which have been used:\n1. Modified Value Difference Metric (MVDM) [10].\n2. Overlap.\n3. Weighted Overlap.\n(a) Using Information Gain[5].\n(b) Using Information Gain Ratio[13].\nThe results of the experiments are shown in table 1."}, {"heading": "4. CONCLUSIONS", "text": "This article studied the possibility of modification of metric classification algorithms on the example of the algorithm k-Nearest Neighbours for their application to sequences. The scientific novelty lies in the fact that new method of generalization of metric classification algorithm is proposed. With using of this method new algorithm for sequence classification SkNN was offered and it demonstrated its effectiveness in problems of classification and labelling in the field of NLP. A comparison of the algorithm with the other often used algorithms shows that with the accuracy of the algorithm it is reasonable to continue working towards its effective realization and, for example, the implementation of this algorithm for computing clusters, which will significantly increase its performance."}, {"heading": "5. REFERENCES", "text": "[1] S. Acedan\u0301ski. A morphosyntactic brill tagger for\ninflectional languages. In International Conference on Natural Language Processing, pages 3\u201314. Springer, 2010.\n[2] Y. Altun, I. Tsochantaridis, T. Hofmann, et al. Hidden markov support vector machines. In ICML, volume 3, pages 3\u201310, 2003.\n[3] W. Daelemans and A. Van den Bosch. Memory-based language processing. Cambridge University Press, 2005.\n[4] M. Feily, A. Shahrestani, and S. Ramadass. A survey of botnet and botnet detection. In 2009 Third International Conference on Emerging Security Information, Systems and Technologies, pages 268\u2013273. IEEE, 2009.\n[5] J. T. Kent. Information gain and a general measure of correlation. Biometrika, 70(1):163\u2013173, 1983.\n[6] A. Kocsor, A. Kerte\u0301sz-Farkas, L. Kaja\u0301n, and S. Pongor. Application of compression-based distance measures to protein sequence classification: a methodological study. Bioinformatics, 22(4):407\u2013412, 2006.\n[7] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the eighteenth international conference on machine learning, ICML, volume 1, pages 282\u2013289, 2001.\n[8] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. In Soviet physics doklady, volume 10, page 707, 1966.\n[9] M. Lichman. Uci machine learning repository, 2013.\n[10] F. Liu, B. Vanschoenwinkel, Y.-f. Chen, and B. Manderick. A modified value difference metric kernel for context-dependent classification tasks. In 2006 International Conference on Machine Learning and Cybernetics, pages 3432\u20133437. IEEE, 2006.\n[11] D. Llorens, F. Prat, A. Marzal, J. M. Vilar, M. J. Castro, J.-C. Amengual, S. Barrachina, A. Castellanos, S. E. Boquera, J. Go\u0301mez, et al. The ujipenchars database: a pen-based database of isolated handwritten characters. In LREC, 2008.\n[12] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.\n[13] T. Mori. Information gain ratio as term weight: the case of summarization of ir results. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137. Association for Computational Linguistics, 2002.\n[14] N. Nguyen and Y. Guo. Comparisons of sequence labeling algorithms and extensions. In Proceedings of the 24th international conference on Machine learning, pages 681\u2013688. ACM, 2007.\n[15] D. B. Paul and J. M. Baker. The design for the wall street journal-based csr corpus. In Proceedings of the workshop on Speech and Natural Language, pages 357\u2013362. Association for Computational Linguistics, 1992.\n[16] R. Ramos-Garijo, S. Mart\u0301\u0131n, A. Marzal, F. Prat, J. M. Vilar, and D. Llorens. An input panel and recognition engine for on-line handwritten text recognition. Frontiers in Artificial Intelligence and Applications, 163:223, 2007.\n[17] P. Sonego, A. Kocsor, and S. Pongor. Roc analysis: applications to the classification of biological sequences and 3d structures. Briefings in bioinformatics, 9(3):198\u2013209, 2008.\n[18] C. Sutton and A. McCallum. An introduction to conditional random fields for relational learning, volume 2. Introduction to statistical relational learning. MIT Press, 2006.\n[19] E. F. Tjong Kim Sang and S. Buchholz. Introduction to the conll-2000 shared task: Chunking. In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7, pages 127\u2013132. Association for Computational Linguistics, 2000.\n[20] G. Zhou and J. Su. Named entity recognition using an hmm-based chunk tagger. In proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 473\u2013480. Association for Computational Linguistics, 2002."}], "references": [{"title": "A morphosyntactic brill tagger for inflectional languages", "author": ["S. Aceda\u0144ski"], "venue": "International Conference on Natural Language Processing, pages 3\u201314. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["Y. Altun", "I. Tsochantaridis", "T. Hofmann"], "venue": "Hidden markov support vector machines. In ICML, volume 3, pages 3\u201310", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Memory-based language processing", "author": ["W. Daelemans", "A. Van den Bosch"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "A survey of botnet and botnet detection", "author": ["M. Feily", "A. Shahrestani", "S. Ramadass"], "venue": "2009 Third International Conference on Emerging Security Information, Systems and Technologies, pages 268\u2013273. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Information gain and a general measure of correlation", "author": ["J.T. Kent"], "venue": "Biometrika, 70(1):163\u2013173", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1983}, {"title": "Application of compression-based distance measures to protein sequence classification: a methodological study", "author": ["A. Kocsor", "A. Kert\u00e9sz-Farkas", "L. Kaj\u00e1n", "S. Pongor"], "venue": "Bioinformatics, 22(4):407\u2013412", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the eighteenth international conference on machine learning, ICML, volume 1, pages 282\u2013289", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Binary codes capable of correcting deletions", "author": ["V.I. Levenshtein"], "venue": "insertions and reversals. In Soviet physics doklady, volume 10, page 707", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1966}, {"title": "B", "author": ["F. Liu"], "venue": "Vanschoenwinkel, Y.-f. Chen, and B. Manderick. A modified value difference metric kernel for context-dependent classification tasks. In 2006 International Conference on Machine Learning and Cybernetics, pages 3432\u20133437. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "et al", "author": ["D. Llorens", "F. Prat", "A. Marzal", "J.M. Vilar", "M.J. Castro", "J.-C. Amengual", "S. Barrachina", "A. Castellanos", "S.E. Boquera", "J. G\u00f3mez"], "venue": "The ujipenchars database: a pen-based database of isolated handwritten characters. In LREC", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Information gain ratio as term weight: the case of summarization of ir results", "author": ["T. Mori"], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137. Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Comparisons of sequence labeling algorithms and extensions", "author": ["N. Nguyen", "Y. Guo"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 681\u2013688. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "The design for the wall street journal-based csr corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language, pages 357\u2013362. Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "S", "author": ["R. Ramos-Garijo"], "venue": "Mart\u0301\u0131n, A. Marzal, F. Prat, J. M. Vilar, and D. Llorens. An input panel and recognition engine for on-line handwritten text recognition. Frontiers in Artificial Intelligence and Applications, 163:223", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Roc analysis: applications to the classification of biological sequences and 3d structures", "author": ["P. Sonego", "A. Kocsor", "S. Pongor"], "venue": "Briefings in bioinformatics, 9(3):198\u2013209", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "An introduction to conditional random fields for relational learning", "author": ["C. Sutton", "A. McCallum"], "venue": "volume 2. Introduction to statistical relational learning. MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Introduction to the conll-2000 shared task: Chunking", "author": ["E.F. Tjong Kim Sang", "S. Buchholz"], "venue": "In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Named entity recognition using an hmm-based chunk tagger", "author": ["G. Zhou", "J. Su"], "venue": "proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 473\u2013480. Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "In the research of the genome, classification of protein sequences in the existing categories is used to determine the function of a new protein[6, 17].", "startOffset": 144, "endOffset": 151}, {"referenceID": 15, "context": "In the research of the genome, classification of protein sequences in the existing categories is used to determine the function of a new protein[6, 17].", "startOffset": 144, "endOffset": 151}, {"referenceID": 3, "context": "The study of the sequence of events to access Internet-based resources may allow distinguishing a user-person from a search bot[4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 18, "context": "The classification and labelling of sequences are used in Natural Language Processing (NLP)[20, 7].", "startOffset": 91, "endOffset": 98}, {"referenceID": 6, "context": "The classification and labelling of sequences are used in Natural Language Processing (NLP)[20, 7].", "startOffset": 91, "endOffset": 98}, {"referenceID": 12, "context": "In the scientific literature[14], this type of classification is also called a strong sequence classification.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "The example of such a distance is Levenshtein distance[8].", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "The following ones should be mentioned[14]:", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "\u2022 Markov SVM[2].", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "For example, the letters \u201cA\u201d and \u201cE\u201d in some sense (as they sound) closer to each other than letters \u201cA\u201d and \u201cX\u201d[10], and the words \u201cred\u201d and \u201cblue\u201d are closer to each other than the words algebra and stool.", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Figure 1 shows a diagram of the relationship between the algorithms of HMM, CRF, Naive Bayes and Logit Regression[18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "Using a weighted summation of different features, it is possible to create metric that simultaneously takes into account the characteristics of arbitrary type, for example, features derived from the ontology and the word vector, obtained by using the algorithm word2vec[12].", "startOffset": 269, "endOffset": 273}, {"referenceID": 9, "context": "It should be noted that the field of possible application of the SkNN algorithm is quite big: as standard benchmark datasets we used sets of such fields as recognition of handwritten symbols (UJI Pen Characters [11]) and processing of texts in natural language (CoNLL200[19]).", "startOffset": 211, "endOffset": 215}, {"referenceID": 17, "context": "It should be noted that the field of possible application of the SkNN algorithm is quite big: as standard benchmark datasets we used sets of such fields as recognition of handwritten symbols (UJI Pen Characters [11]) and processing of texts in natural language (CoNLL200[19]).", "startOffset": 270, "endOffset": 274}, {"referenceID": 14, "context": "It is worth noting, even though the accuracy of the algorithm SkNN does not exceed the accuracy of the algorithms, which are intended for solving this very problem[16], but it shows the result that can be compared with them.", "startOffset": 163, "endOffset": 167}, {"referenceID": 0, "context": "The second experimental dataset CoNLL200 represents sequences of words in sentences in English with the corresponding parts of speech derived (POS-tag) with the help of program Brill tagger[1] and chunk tag which has been obtained from corpora WSJ[15].", "startOffset": 189, "endOffset": 192}, {"referenceID": 13, "context": "The second experimental dataset CoNLL200 represents sequences of words in sentences in English with the corresponding parts of speech derived (POS-tag) with the help of program Brill tagger[1] and chunk tag which has been obtained from corpora WSJ[15].", "startOffset": 247, "endOffset": 251}, {"referenceID": 2, "context": "out using different metrics which were most successfully applied in practice in the field NLP[3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Modified Value Difference Metric (MVDM) [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "(a) Using Information Gain[5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "(b) Using Information Gain Ratio[13].", "startOffset": 32, "endOffset": 36}], "year": 2016, "abstractText": "The article deals with the issue of modification of metric classification algorithms. In particular, it studies the algorithm k-Nearest Neighbours for its application to sequential data. A method of generalization of metric classification algorithms is proposed. As a part of it, there has been developed an algorithm for solving the problem of classification and labelling of sequential data. The advantages of the developed algorithm of classification in comparison with the existing one are also discussed in the article. There is a comparison of the effectiveness of the proposed algorithm with the algorithm of CRF in the task of chunking in the open data set CoNLL2000.", "creator": "LaTeX with hyperref package"}}}