{"id": "1610.05011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Interactive Attention for Neural Machine Translation", "abstract": "conventional attention - based neural machine translation ( nmt ) conducts dynamic alignment in generating the target sentence. by repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder ( bahdanau et al., december 2015 ), the attention mechanism has greatly enhanced state - of - the - art nmt. in this paper, finally we propose a new attention mechanism, called the interactive attention, which models the interaction effects between the decoder and the representation of source sentence during translation by both reading and in writing operations. interactive enhanced attention can keep track of the interaction history and therefore improve the translation performance. experiments on nist chinese - english translation task show hope that interactive attention can achieve significant improvements efficiently over both the previous attention - based nmt baseline and some state - of - the - art variants of attention - based nmt ( i. e., comparative coverage models ( tu et al., 2016 ) ). and neural machine translator with our interactive attention can outperform the open source attention - based nmt system groundhog by 4. 22 bleu points and the open source phrase - based system moses by 3. 94 bleu points averagely on multiple test sets.", "histories": [["v1", "Mon, 17 Oct 2016 08:33:20 GMT  (562kb)", "http://arxiv.org/abs/1610.05011v1", "Accepted at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fandong meng", "zhengdong lu", "hang li", "qun liu"], "accepted": false, "id": "1610.05011"}, "pdf": {"name": "1610.05011.pdf", "metadata": {"source": "CRF", "title": "Interactive Attention for Neural Machine Translation", "authors": ["Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu"], "emails": ["fandongmeng@tencent.com", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com", "qliu@computing.dcu.ie"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n05 01\n1v 1\n[ cs\n.C L\n] 1\n7 O\nct 2\n01 6\nInteractive Attention for Neural Machine Translation\nFandong Meng1\u2217 Zhengdong Lu2 Hang Li2 Qun Liu3,4\n1AI Platform Department, Tencent Technology Co., Ltd. fandongmeng@tencent.com\n2Noah\u2019s Ark Lab, Huawei Technologies {Lu.Zhengdong,HangLi.HL}@huawei.com\n3ADAPT Centre, School of Computing, Dublin City University 4Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS\nqliu@computing.dcu.ie Abstract\nConventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets."}, {"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector (Sutskever et al., 2014).\nHowever, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a). This may let the decoder tend to ignore past attention information, and lead to over-translation and undertranslation (Tu et al., 2016). To address this problem, Tu et al. (2016) proposed to maintain tag vec-\n\u2217The majority of this work was completed when the first author studied at Institute of Computing Technology, Chinese Academy of Sciences.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\ntors in source representation to keep track of the attention history, which encourages the attentionbased NMT system to consider more untranslated source words. Inspired by neural turing machines (Graves et al., 2014), we propose INTERACTIVE ATTENTION model from the perspective of memory reading-writing, which provides a conceptually simpler and practically more effective mechanism for attention-based NMT. The NMT with INTERACTIVE ATTENTION is called NMTIA, which can keep track of the interaction history with the representation of source sentence by both reading and writing operations during translation. This interactive mechanism may be helpful for the decoder to automatically distinguish which parts have been translated and which parts are under-translated.\nWe test the efficacy of NMTIA on NIST Chinese-English translation task. Experiment results show that NMTIA can significantly outperform both the conventional attention-based NMT baseline (Bahdanau et al., 2015) and coverage models (Tu et al., 2016). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points.\nRoadMap: In the remainder of this paper, we will start with a brief overview of attention-based neural machine translation in Section 2. Then in Section 3, we will detail the INTERACTIVE ATTENTIONbased NMT (NMTIA). In Section 4, we report our empirical study of NMTIA on a Chinese-English translation task, followed by Section 5 and 6 for related work and conclusion."}, {"heading": "2 Background", "text": "Our work is built upon the attention-based NMT (Bahdanau et al., 2015), which takes a sequence of vector representations of the source sentence generated by a RNN or bi-directional RNN as input, and then jointly learns to align and translate by reading the vector representations during translation with a RNN decoder. Therefore, we take an overview of the attention-based NMT in this section before detail the NMTIA in next section."}, {"heading": "2.1 Attention-based Neural Machine Translation", "text": "Figure 1 shows the framework of attention-based NMT. Formally, given an input source sequence x= {x1, x2, \u00b7 \u00b7 \u00b7 , xN} and the previously generated target sequence y<t = {y1, y2, \u00b7 \u00b7 \u00b7 , yt\u22121}, the probability of the next target word yt is\np(yt|y<t,x) = softmax(f(ct, yt\u22121, st)) (1)\nwhere f(\u00b7) is a non-linear function, and st is the state of decoder RNN at time step t which is calculated as\nst = g(st\u22121, yt\u22121, ct) (2)\nwhere g(\u00b7) can be any activation function, here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit (GRU) (Cho et al., 2014). In the remainder of the paper, we will also use GRU to stand for the operator. And ct is a distinct source representation for time t, calculated as a weighted sum of the source annotations:\nct =\nN\u2211\nj=1\nat,jhj (3)\nFormally, hj = [ \u2212\u2192 hj T , \u2190\u2212 hj\nT ]T is the annotation of xj , which is computed by a bi-directional RNN (Schuster and Paliwal, 1997) with GRU and contains information about the whole input sequence with a strong focus on the parts surrounding xj . And its weight at,j is computed by\nat,j = exp(et,j)\u2211N k=1 exp(et,k)\n(4)\nwhere et,j = vTa tanh(Wast\u22121+Uahj) scores how well st\u22121 and hj match. This is called automatic alignment (Bahdanau et al., 2015) or attention model (Luong et al., 2015a), but it is essentially reading with content-based addressing defined in (Graves et al., 2014). With the attention model, it releases the need to summarize the entire sentence with a single fixed-length vector (Sutskever et al., 2014; Cho et al., 2014). Instead, it lets the decoding network focus on one particular segment in source sentence at one moment, and therefore better resolution."}, {"heading": "2.2 Improved Attention Model", "text": "The alignment model at,j scores how well the output at position t matches the inputs around position j based on st\u22121 and hj . Intuitively, it should be beneficial to directly exploit the information of yt\u22121 when reading from the representation of source sentence, which is not implemented in the original attention-based NMT (Bahdanau et al., 2015). As illustrated in Figure 2, we add this implementation into the attention model, inspired by the latest implementation of attention-based NMT1. This kind of attention model can find a more effective alignment path by using both previous hidden state st\u22121 and the previous context word yt\u22121. Then, the calculation of e(t, j) becomes\net,j = v T a tanh(Was\u0303t\u22121 +Uahj) (5)\nwhere s\u0303t\u22121 = GRU(st\u22121, eyt\u22121) is an intermediate state tailored for reading from the representation of source sentence with the information of yt\u22121 (its word embedding being eyt\u22121) added. And the calculation of update-state st becomes\nst = GRU(s\u0303t\u22121, ct) (6) 1https://github.com/nyu-dl/dl4mt-tutorial/tree/master/session2"}, {"heading": "3 Interactive Attention", "text": "In this section, we will elaborate on the proposed INTERACTIVE ATTENTION-based NMT, called NMTIA. Figure 3 shows the framework of NMTIA with two rounds of interactive read-write operations (indicated by the yellow and red arrows respectively), which adopts the same prediction model (Eq. 1) with improved attention-based NMT. With annotations H={h1,h2, . . . ,hN} of the source sentence x={x1, x2, \u00b7 \u00b7 \u00b7 , xN}, we take H as a memory, which contains N cells with the jth cell being hj . As illustrated in Figure 3, INTERACTIVE ATTENTION in NMTIA contains two key parts at each time step t: 1) attentive reading from H, and 2) attentive writing to H. Since the content in H changes with time, we will add time stamp on H (hence H(t)) and its cells (hence h(t)j ).\nAt time t, the state st\u22121 first meets the prediction yt\u22121 to form an \u201cintermediate\u201d state s\u0303t\u22121, which can be calculated as follows\ns\u0303t\u22121 = GRU(st\u22121, eyt\u22121) (7)\nwhere eyt\u22121 is the word-embedding associated with the previous prediction word yt\u22121. This \u201cintermediate\u201d state s\u0303t\u22121 is used to read the source memory H(t\u22121)\nct = Read(s\u0303t\u22121,H (t\u22121)) (8)\nAfter that, s\u0303t\u22121 is combined with ct to update the new state\nst = GRU(s\u0303t\u22121, ct) (9)\nFinally, the new state st is used to update the source memory by writing to it to finish the interaction in a round of state-update\nH(t) = Write(st,H (t\u22121)) (10)\nThe details of Read and Write in Eq. 8 and 10 will be described later in next section. From the whole framework of NMTIA, we can see that the new attention mechanism can timely update the representation of source sentence along with the update-chain of the decoder RNN state. This may let the decoder keep track of the attention history during translation. Clearly, INTERACTIVE ATTENTION can subsume the coverage models in (Tu et al., 2016) as special cases while conceptually simpler. Moreover, with the attentive writing, INTERACTIVE ATTENTION potentially can modify and add more on the source representation than just history of attention, and is therefore a more powerful model for machine translation, as empirically verified in Section 4."}, {"heading": "3.1 Read and Write of Interactive Attention", "text": "Attentive Read Formally, H(t \u2032) \u2208 Rn\u00d7m is the memory in time t\u2032 after the decoder RNN state update, where n is the number of memory cells and m is the dimension of vector in each cell. Before the state s update at time t, the output of reading ct is given by\nct =\nn\u2211\nj=1\nwRt (j)h (t\u22121) j (11)\nwhere wRt \u2208 R n specifies the normalized weights assigned to the cells in H(t\u22121). We can use contentbased addressing to determine wRt as described in (Graves et al., 2014) or (quite similarly) use the reading mechanism such as the attention model in Section 2. In this paper, we adopt the latter one.2\nAttentive Write Inspired by the writing operation of neural turing machines (Graves et al., 2014), we define two types of operation on writing to the memory: FORGET and UPDATE. FORGET is similar\n2 Wang et al. (2016) verified the former one for the read operation on the external memory.\nto the forget gate in GRU, which determines the content to be removed from memory cells. More specifically, the vector Ft \u2208 Rm specifies the values to be forgotten or removed on each dimension in memory cells, which is then assigned to each cell through normalized weights wWt . Formally, the memory (\u201cintermediate\u201d) after FORGET operation is given by\nh\u0303 (t) i = h (t\u22121) i (1\u2212w W t (i) \u00b7 Ft), i = 1, 2, \u00b7 \u00b7 \u00b7 , n (12)\nwhere\n\u2022 Ft = \u03c3(WF , st) is parameterized with WF \u2208 Rm\u00d7m, and \u03c3 stands for the Sigmoid activation function;\n\u2022 wWt \u2208 R n specifies the normalized weights assigned to the cells in H(t), and wWt (i) specifies\nthe weight associated with the ith cell in the same parametric form as wRt .\nUPDATE is similar to the update gate in GRU, deciding how much current information should be written to the memory as the added content\nh (t) i = h\u0303 (t) i +w W t (i) \u00b7Ut, i = 1, 2, \u00b7 \u00b7 \u00b7 , n (13)\nwhere Ut = \u03c3(WU , st) is parameterized with WU \u2208 Rm\u00d7m, and Ut \u2208 Rm. In our experiments, the weights for reading (i.e., wRt ) and writing (i.e., w W t ) at time t are shared when conducting interaction with the source memory."}, {"heading": "3.2 Optimization", "text": "The parameters to be optimized include the embedding of words on source and target languages, the parameters for the encoder, the decoder and other operations of NMTIA. The optimization is conducted via the standard back-propagation (BP) aiming to maximize the likelihood of the target sequence. In practice, we use the standard stochastic gradient descent (SGD) and mini-batch with learning rate controlled by AdaDelta (Zeiler, 2012)."}, {"heading": "4 Experiments", "text": "We report our empirical study of NMTIA on Chinese-to-English translation task in this section. The experiments are designed to answer the following questions:\n\u2022 Can NMTIA achieve significant improvements over the conventional attention-based NMT?\n\u2022 Can NMTIA outperform the attention-based NMT with coverage model (Tu et al., 2016)?"}, {"heading": "4.1 Data and Metric", "text": "Our training data consist of 1.25M sentence pairs extracted from LDC corpora3, with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, which is used to monitor the training process and decide the early stop condition. And the NIST 2003 (MT03), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are used as our test sets. The numbers of sentences in NIST MT02, MT03, MT04, MT05 and MT06 are 878, 919, 1788, 1082, and 1664 respectively. We use the case-insensitive 4-gram NIST BLEU4 as our evaluation metric, with statistical significance test (sign-test (Collins et al., 2005)) between the proposed models and the baselines.\n3The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl"}, {"heading": "4.2 Training Details", "text": "In training the neural networks, we limit the source and target vocabulary to the most frequent 30K words for both Chinese and English, covering approximately 97.7% and 99.3% of two corpus respectively. All the out-of-vocabulary words are mapped to a special token UNK. We initialize the recurrent weight matrices as random orthogonal matrices. All the bias vectors are initialized to zero. For other parameters, we initialize them by sampling each element from the Gaussian distribution of mean 0 and variance 0.012. The parameters are updated by SGD and mini-batch (size 80) with learning rate controlled by AdaDelta (Zeiler, 2012) (\u01eb = 1e\u22126 and \u03c1 = 0.95). We train the NMT systems with the sentences of length up to 50 words in training data, and set the dimension of word embedding to 620 and the size of the hidden layer to 1000, following the settings in (Bahdanau et al., 2015). We also use dropout for our baseline NMT systems and NMTIA to avoid over-fitting (Hinton et al., 2012). In our experiments, dropout was applied on the output layer with dropout rate setting to 0.5.\nInspired by the effort on easing the training of very deep architectures (Hinton and Salakhutdinov, 2006), we use a simple pre-training strategy to train our NMTIA. First we train a regular attention-based NMT model (Bahdanau et al., 2015). Then we use the trained NMT model to initialize the parameters of NMTIA except for those related to the operations of INTERACTIVE ATTENTION. After that, we fine-tune all the parameters of NMTIA."}, {"heading": "4.3 Comparison Systems", "text": "We compare our NMTIA with four systems:\n\u2022 Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default configuration. The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy (Koehn et al., 2003). The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data with the SRILM toolkit (Stolcke and others, 2002),\n\u2022 Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015).\n\u2022 RNNsearch\u22c6: our in-house implementation of NMT system with the improved conventional attention model as described in Section 2.2.\n\u2022 Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which improve the attention mechanism through modeling a soft coverage on the source representation\n5http://www.statmt.org/moses/ 6https://github.com/lisa-groundhog/GroundHog\nby maintain a coverage vector to keep track of the attention history during translation."}, {"heading": "4.4 Main Results", "text": "The main results of different models are given in Table 1. Before proceeding to more detailed comparisons, we first observe that\n\u2022 RNNsearch\u22c6 outperforms Groundhog, which is implemented with the conventional attention model as described in Section 2.1, by 2.38 BLEU points averagely on four test sets;\n\u2022 RNNsearch\u22c6 only exploit sentences of length up to 50 words with 30K vocabulary, but can achieve averagely 2.10 BLEU points higher than the open source phrase-based system Moses, which is trained with full training data.\nClearly from Table 1, NMTIA can achieve significant improvements over RNNsearch\u22c6 by 1.84 BLEU points averagely on four test sets. We conjecture it is because our INTERACTIVE ATTENTION mechanism can keep track of the interaction history between the decoder and the representation of source sentence during translation, which may be helpful for the decoder to automatically distinguish which parts have been translated and which parts are under-translated."}, {"heading": "4.5 INTERACTIVE ATTENTION Vs. Coverage Model", "text": "Tu et al. (2016) proposed two coverage models to let the NMT system to consider more about untranslated source words. Basically, they maintain a coverage vector for each hidden state for source to keep track of the attention history and feed the coverage vector to the attention model to help adjust future attention. Although we do not maintain a coverage vector, our INTERACTIVE ATTENTION can potentially do similar things, therefore subsuming coverage models as special cases. We hence compare our INTERACTIVE ATTENTION model with the coverage model in (Tu et al., 2016). There are two coverage models proposed in (Tu et al., 2016), including linguistic coverage model and neural network based coverage model (NN-Cover). Since the neural network based coverage model generally yields better results, we mainly compare with the neural network based coverage model. Although the coverage models are originally implemented on Groundhog in (Tu et al., 2016), they can be easily adapted to the \u201cRNNsearch\u22c6\u201d. Following the setting in (Tu et al., 2016), we conduct the comparison with the training sentences of length up to 80 words. Clearly from Table 2, our NMTIA-80 outperforms the NN-Cover-80 by +1.04 BLEU scores averagely on four test sets.\nA more detailed comparison between conventional attention model (RNNsearch\u22c6-80), neural network based coverage model (NN-Cover-80) (Tu et al., 2016) and NMTIA-80 suggests that our NMTIA80 is quite consistent on outperforming the conventional attention model and the coverage model. Figure 4 shows the BLEU scores of generated translations on the test sets with respect to the length of the\nsource sentences. In particular, we test the BLEU scores on sentences longer than {0, 10, 20, 30, 40, 50, 60} in the merged test set of MT03, MT04, MT05 and MT06. Clearly, on sentences with different length, NMTIA-80 always yields consistently higher BLEU scores than the conventional attentionbased NMT and the enhanced version with the neural network based coverage model. We conjecture that with the attentive writing (described in Section 3.1), INTERACTIVE ATTENTION potentially can modify and add more on the source representation than just history of attention, and is therefore a more powerful model for machine translation.\nWe also provide some actual translation examples (see Appendix) to show that our INTERACTIVE ATTENTION can get better performance then baselines, especially on solving under-translation problem. We think the interactive mechanism of NMTIA is helpful for the decoder to automatically distinguish which parts have been translated and which parts are under-translated."}, {"heading": "5 Related Work", "text": "Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements. These works are different with our INTERACTIVE ATTENTION approach, as we use a rather generic attentive reading while at the same time performing attentive writing.\nOur work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation. Tang et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form. They let the decoder utilize a mixture of word-generating and phrase-generating component, to generate a sequence of multiple words all at once. Wang et al. (2016) extended the NMT decoder by maintaining an external memory, which is operated by reading and writing opera-\ntions of neural turing machines (Graves et al., 2014), while keeping a read-only copy of the original source annotations along side the \u201cread-write\u201d memory. These powerful extensions have been verified on Chinese-English translation tasks. Our INTERACTIVE ATTENTION is different from previous works. We take the annotations of source sentence as a memory instead of using an external memory, and we design a mechanism to directly read from and write to it during translation. Therefore, the original source annotations are not accessible in later steps. More specially, our model inherited the notation and some simple operations for writing from (Graves et al., 2014), while NMTIA extends it to \u201cunbounded\u201d memory for representing the source. In addition, although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those in (Graves et al., 2014; Wang et al., 2016), our model can also achieve good performance."}, {"heading": "6 Conclusion", "text": "We propose a simple yet effective INTERACTIVE ATTENTION approach, which models the interaction between the decoder and the representation of source sentence during translation by using reading and writing operations. Our empirical study on Chinese-English translation shows that INTERACTIVE ATTENTION can significantly improve the performance of NMT."}, {"heading": "Acknowledgements", "text": "Liu is partially supported by the Science Foundation Ireland (Grant 13/RC/2106) as part of the ADAPT Centre at Dublin City University. We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions."}, {"heading": "APPENDIX: Actual Translation Examples", "text": "In appendix we give some example translations from RNNsearch\u22c6-80, NN-Cover-80 and NMTIA-80, and compare them against the reference. We highlight some correct translation segments (or undertranslated by baseline systems) in blue color and wrong ones in red color.\nExample Translations\nsrc !\"#$%#&#'#()#*+#,#-#./#01#2#34#56# 7# 89#1:#\n;<#=>#?#@#A#B6#CDE#F#\nref North Korea said the nuclear stalemate is a bilateral topic of discussion with United States only. The interference of other countries will only complicate the issue.\nRNNsearch ! -80 north korea claimed that the nuclear stalemate was only involved in bilateral issues in\nthe united states , and other countries will find it more complicated .\nNN-Cover-80 the north korea said that it had only involved bilateral talks in the united states and other countries would interfere with the issue . NMTIA -80 north korea claimed that this nuclear stalemate was only related to the us bilateral\nagenda , and interference in other countries could only complicate the problem . !\nsrc GH#IJ#KL#<M#NO#P#Q#7#RS1#TU#VW@#2#X#Y#Z[#V\nW1#\\#]#^#_#`a#IJ#>bc#de#<M#F#\nref Four days after Pyongyang made the above move, five permanent members of the UN\nSecurity Council have all taken preventive diplomatic actions on this crisis.\nRNNsearch ! -80 pyongyang has taken these actions four days ago , and the five permanent members of the un security council have taken precautions against this crisis . NN-Cover-80 in a four - day operation , the five permanent members of the un security council have\ntaken preventive diplomatic actions for the crisis .\nNMTIA -80 in the four days after pyongyang took the above action , the five permanent members of the un security council have taken preventive diplomatic actions for this crisis .\n!\nsrc fgh#ij#kl#mn#o#l#p#qO#r#sC#tu#vw#_#W#x#fy#\nz<#{|#}w#F#\nref The Philippine government originally planned to hold preliminary discussions with the Philippine communists on the resumption of formal peace talks later this month.\nRNNsearch ! -80 the philippine government originally planned to hold a preliminary meeting with\n<UNK> on friday .\nNN-Cover-80 the philippine government plans to resume formal peace talks with <UNK> later this\nmonth .\nNMTIA -80 the philippine government originally planned to hold a preliminary discussion on the\nresumption of formal peace talks later this month . !\nsrc 9#~ # # # #2#7# # # #2# _# #W # # # #T\nU# #1 E#7# #_# #W # # # #ij#e #RS1#F# #\nref\nHe said: \"Obviously, the first thing we need to do is to internationalize the security force in Iraq. The other thing is to turn the transitional government over to the United Nations.\"\nRNNsearch ! -80 he said : \" obviously , the first thing we need to do is to <UNK> iraqi security forces\nto the united nations . \"\nNN-Cover-80 he said : \" obviously , we need the first thing to internationalize the security forces in iraq , and another thing is to hand over the transitional government to the united\nnations . \"\nNMTIA -80 he said : \" obviously , the first thing we need to do is to internationalize the security forces in iraq , and the other is to send the transitional government to the united\nnations . \"\n! !"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 876\u2013885, San Diego, California, June.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Clause restructuring for statistical machine translation", "author": ["Michael Collins", "Philipp Koehn", "Ivona Ku\u010derov\u00e1."], "venue": "Proceedings of ACL, pages 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Implicit distortion and fertility models for attention-based encoder-decoder NMT model", "author": ["Shi Feng", "Shujie Liu", "Mu Li", "Ming Zhou."], "venue": "CoRR, abs/1601.03317.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."], "venue": "Science, 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov.,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1\u201310, Beijing, China, July.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of NAACL, pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of ACL on interactive poster and demonstration sessions, pages 177\u2013180, Prague, Czech Republic, June.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Towards zero unknown word in neural machine translation", "author": ["Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 11\u201319, Beijing, China, July.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural transformation machine: A new architecture for sequence-to-sequence learning", "author": ["Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu."], "venue": "CoRR, abs/1506.06442.", "citeRegEx": "Meng et al\\.,? 2015", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE Transactions on, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of ACL, pages 1683\u20131692, Berlin, Germany, August.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "SRILM-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Proceedings of ICSLP, volume 2, pages 901\u2013904.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Neural machine translation with external phrase memory", "author": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip L.H. Yu."], "venue": "CoRR, abs/1606.01792.", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL, pages 76\u201385, Berlin, Germany, August. Association for Computational Linguistics.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Memory-enhanced decoder for neural machine translation", "author": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."], "venue": "CoRR, abs/1606.04199.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT.", "startOffset": 110, "endOffset": 133}, {"referenceID": 21, "context": ", coverage models (Tu et al., 2016)).", "startOffset": 18, "endOffset": 35}, {"referenceID": 19, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 0, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 12, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 8, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 13, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 20, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 22, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 11, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 21, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 17, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 25, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 16, "context": "Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN.", "startOffset": 110, "endOffset": 138}, {"referenceID": 8, "context": "Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015).", "startOffset": 170, "endOffset": 189}, {"referenceID": 19, "context": "This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector (Sutskever et al., 2014).", "startOffset": 174, "endOffset": 198}, {"referenceID": 0, "context": "However, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 144, "endOffset": 188}, {"referenceID": 12, "context": "However, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 144, "endOffset": 188}, {"referenceID": 21, "context": "This may let the decoder tend to ignore past attention information, and lead to over-translation and undertranslation (Tu et al., 2016).", "startOffset": 118, "endOffset": 135}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector (Sutskever et al., 2014). However, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a). This may let the decoder tend to ignore past attention information, and lead to over-translation and undertranslation (Tu et al., 2016). To address this problem, Tu et al. (2016) proposed to maintain tag vec\u2217The majority of this work was completed when the first author studied at Institute of Computing Technology, Chinese Academy of Sciences.", "startOffset": 8, "endOffset": 1302}, {"referenceID": 5, "context": "Inspired by neural turing machines (Graves et al., 2014), we propose INTERACTIVE ATTENTION model from the perspective of memory reading-writing, which provides a conceptually simpler and practically more effective mechanism for attention-based NMT.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "Experiment results show that NMTIA can significantly outperform both the conventional attention-based NMT baseline (Bahdanau et al., 2015) and coverage models (Tu et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 21, "context": ", 2015) and coverage models (Tu et al., 2016).", "startOffset": 28, "endOffset": 45}, {"referenceID": 0, "context": "2 Background Our work is built upon the attention-based NMT (Bahdanau et al., 2015), which takes a sequence of vector representations of the source sentence generated by a RNN or bi-directional RNN as input, and then jointly learns to align and translate by reading the vector representations during translation with a RNN decoder.", "startOffset": 60, "endOffset": 83}, {"referenceID": 1, "context": "st = g(st\u22121, yt\u22121, ct) (2) where g(\u00b7) can be any activation function, here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 155, "endOffset": 173}, {"referenceID": 16, "context": "Formally, hj = [ \u2212\u2192 hj T , \u2190\u2212 hj T ] is the annotation of xj , which is computed by a bi-directional RNN (Schuster and Paliwal, 1997) with GRU and contains information about the whole input sequence with a strong focus on the parts surrounding xj .", "startOffset": 105, "endOffset": 133}, {"referenceID": 0, "context": "This is called automatic alignment (Bahdanau et al., 2015) or attention model (Luong et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 12, "context": ", 2015) or attention model (Luong et al., 2015a), but it is essentially reading with content-based addressing defined in (Graves et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": ", 2015a), but it is essentially reading with content-based addressing defined in (Graves et al., 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 19, "context": "With the attention model, it releases the need to summarize the entire sentence with a single fixed-length vector (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 114, "endOffset": 156}, {"referenceID": 1, "context": "With the attention model, it releases the need to summarize the entire sentence with a single fixed-length vector (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 114, "endOffset": 156}, {"referenceID": 0, "context": "Intuitively, it should be beneficial to directly exploit the information of yt\u22121 when reading from the representation of source sentence, which is not implemented in the original attention-based NMT (Bahdanau et al., 2015).", "startOffset": 199, "endOffset": 222}, {"referenceID": 21, "context": "Clearly, INTERACTIVE ATTENTION can subsume the coverage models in (Tu et al., 2016) as special cases while conceptually simpler.", "startOffset": 66, "endOffset": 83}, {"referenceID": 5, "context": "We can use contentbased addressing to determine w t as described in (Graves et al., 2014) or (quite similarly) use the reading mechanism such as the attention model in Section 2.", "startOffset": 68, "endOffset": 89}, {"referenceID": 5, "context": "Attentive Write Inspired by the writing operation of neural turing machines (Graves et al., 2014), we define two types of operation on writing to the memory: FORGET and UPDATE.", "startOffset": 76, "endOffset": 97}, {"referenceID": 5, "context": "Attentive Write Inspired by the writing operation of neural turing machines (Graves et al., 2014), we define two types of operation on writing to the memory: FORGET and UPDATE. FORGET is similar 2 Wang et al. (2016) verified the former one for the read operation on the external memory.", "startOffset": 77, "endOffset": 216}, {"referenceID": 24, "context": "In practice, we use the standard stochastic gradient descent (SGD) and mini-batch with learning rate controlled by AdaDelta (Zeiler, 2012).", "startOffset": 124, "endOffset": 138}, {"referenceID": 21, "context": "The experiments are designed to answer the following questions: \u2022 Can NMTIA achieve significant improvements over the conventional attention-based NMT? \u2022 Can NMTIA outperform the attention-based NMT with coverage model (Tu et al., 2016)? 4.", "startOffset": 219, "endOffset": 236}, {"referenceID": 3, "context": "We use the case-insensitive 4-gram NIST BLEU4 as our evaluation metric, with statistical significance test (sign-test (Collins et al., 2005)) between the proposed models and the baselines.", "startOffset": 118, "endOffset": 140}, {"referenceID": 24, "context": "The parameters are updated by SGD and mini-batch (size 80) with learning rate controlled by AdaDelta (Zeiler, 2012) (\u01eb = 1e\u22126 and \u03c1 = 0.", "startOffset": 101, "endOffset": 115}, {"referenceID": 0, "context": "We train the NMT systems with the sentences of length up to 50 words in training data, and set the dimension of word embedding to 620 and the size of the hidden layer to 1000, following the settings in (Bahdanau et al., 2015).", "startOffset": 202, "endOffset": 225}, {"referenceID": 7, "context": "We also use dropout for our baseline NMT systems and NMTIA to avoid over-fitting (Hinton et al., 2012).", "startOffset": 81, "endOffset": 102}, {"referenceID": 6, "context": "Inspired by the effort on easing the training of very deep architectures (Hinton and Salakhutdinov, 2006), we use a simple pre-training strategy to train our NMTIA.", "startOffset": 73, "endOffset": 105}, {"referenceID": 0, "context": "First we train a regular attention-based NMT model (Bahdanau et al., 2015).", "startOffset": 51, "endOffset": 74}, {"referenceID": 10, "context": "3 Comparison Systems We compare our NMTIA with four systems: \u2022 Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default configuration.", "startOffset": 69, "endOffset": 89}, {"referenceID": 15, "context": "The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy (Koehn et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 9, "context": "The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy (Koehn et al., 2003).", "startOffset": 158, "endOffset": 178}, {"referenceID": 0, "context": "The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data with the SRILM toolkit (Stolcke and others, 2002), \u2022 Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015).", "startOffset": 252, "endOffset": 275}, {"referenceID": 21, "context": "\u2022 Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which improve the attention mechanism through modeling a soft coverage on the source representation http://www.", "startOffset": 73, "endOffset": 90}, {"referenceID": 21, "context": "Table 2: BLEU-4 scores (%) of the conventional attention-based model (RNNsearch-80), the neural network based coverage model (NN-Cover-80) (Tu et al., 2016) and our INTERACTIVE ATTENTION model (NMTIA-80).", "startOffset": 139, "endOffset": 156}, {"referenceID": 21, "context": "\u201c-80\u201d means the models are trained with the sentences of length up to 80 words, which is consistent with the setting in (Tu et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 21, "context": "We hence compare our INTERACTIVE ATTENTION model with the coverage model in (Tu et al., 2016).", "startOffset": 76, "endOffset": 93}, {"referenceID": 21, "context": "There are two coverage models proposed in (Tu et al., 2016), including linguistic coverage model and neural network based coverage model (NN-Cover).", "startOffset": 42, "endOffset": 59}, {"referenceID": 21, "context": "Although the coverage models are originally implemented on Groundhog in (Tu et al., 2016), they can be easily adapted to the \u201cRNNsearch\u201d.", "startOffset": 72, "endOffset": 89}, {"referenceID": 21, "context": "Following the setting in (Tu et al., 2016), we conduct the comparison with the training sentences of length up to 80 words.", "startOffset": 25, "endOffset": 42}, {"referenceID": 21, "context": "A more detailed comparison between conventional attention model (RNNsearch-80), neural network based coverage model (NN-Cover-80) (Tu et al., 2016) and NMTIA-80 suggests that our NMTIA80 is quite consistent on outperforming the conventional attention model and the coverage model.", "startOffset": 130, "endOffset": 147}, {"referenceID": 21, "context": "Coverage Model Tu et al. (2016) proposed two coverage models to let the NMT system to consider more about untranslated source words.", "startOffset": 15, "endOffset": 32}, {"referenceID": 12, "context": "5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016).", "startOffset": 92, "endOffset": 151}, {"referenceID": 2, "context": "5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016).", "startOffset": 92, "endOffset": 151}, {"referenceID": 4, "context": "5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016).", "startOffset": 92, "endOffset": 151}, {"referenceID": 5, "context": "Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 23, "context": ", 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al.", "startOffset": 25, "endOffset": 65}, {"referenceID": 14, "context": ", 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al.", "startOffset": 25, "endOffset": 65}, {"referenceID": 20, "context": ", 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation.", "startOffset": 42, "endOffset": 80}, {"referenceID": 22, "context": ", 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation.", "startOffset": 42, "endOffset": 80}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance.", "startOffset": 9, "endOffset": 69}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs.", "startOffset": 9, "endOffset": 289}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements.", "startOffset": 9, "endOffset": 462}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements. These works are different with our INTERACTIVE ATTENTION approach, as we use a rather generic attentive reading while at the same time performing attentive writing. Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation. Tang et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form.", "startOffset": 9, "endOffset": 1048}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements. These works are different with our INTERACTIVE ATTENTION approach, as we use a rather generic attentive reading while at the same time performing attentive writing. Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation. Tang et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form. They let the decoder utilize a mixture of word-generating and phrase-generating component, to generate a sequence of multiple words all at once. Wang et al. (2016) extended the NMT decoder by maintaining an external memory, which is operated by reading and writing opera-", "startOffset": 9, "endOffset": 1291}, {"referenceID": 5, "context": "tions of neural turing machines (Graves et al., 2014), while keeping a read-only copy of the original source annotations along side the \u201cread-write\u201d memory.", "startOffset": 32, "endOffset": 53}, {"referenceID": 5, "context": "More specially, our model inherited the notation and some simple operations for writing from (Graves et al., 2014), while NMTIA extends it to \u201cunbounded\u201d memory for representing the source.", "startOffset": 93, "endOffset": 114}, {"referenceID": 5, "context": "In addition, although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those in (Graves et al., 2014; Wang et al., 2016), our model can also achieve good performance.", "startOffset": 112, "endOffset": 152}, {"referenceID": 22, "context": "In addition, although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those in (Graves et al., 2014; Wang et al., 2016), our model can also achieve good performance.", "startOffset": 112, "endOffset": 152}], "year": 2016, "abstractText": "Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.", "creator": "gnuplot 4.6 patchlevel 3"}}}