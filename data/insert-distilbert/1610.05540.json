{"id": "1610.05540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "SYSTRAN's Pure Neural Machine Translation Systems", "abstract": "since the first online demonstration of neural machine translation ( nmt ) by apple lisa, nmt development has now recently moved moving from laboratory to production systems as demonstrated by several entities announcing roll - out of nmt engines to replace their existing technologies. nmt systems have spanned a normally large number of training configurations and the training process of such systems is usually in very long, often a few weeks, so role of experimentation is critical and important to share. in this work, we present our approach to production - ready systems simultaneously with release of online demonstrators covering a large variety of languages ( 12 languages, for 32 language pairs ). we explore different practical choices : an efficient and evolutive open - source framework ; data preparation ; unique network architecture ; additional locally implemented features ; tuning for production ; etc. we discuss about evaluation methodology, here present our first findings and afterward we finally outline further work.", "histories": [["v1", "Tue, 18 Oct 2016 11:32:42 GMT  (302kb)", "http://arxiv.org/abs/1610.05540v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["josep crego", "jungi kim", "guillaume klein", "anabel rebollo", "kathy yang", "jean senellart", "egor akhanov", "patrice brunelle", "aurelien coquard", "yongchao deng", "satoshi enoue", "chiyo geiss", "joshua johanson", "ardas khalsa", "raoum khiari", "byeongil ko", "catherine kobus", "jean lorieux", "leidiana martins", "dang-chuan nguyen", "alexandra priori", "thomas riccardi", "natalia segal", "christophe servan", "cyril tiquet", "bo wang", "jin yang", "dakun zhang", "jing zhou", "peter zoldan"], "accepted": false, "id": "1610.05540"}, "pdf": {"name": "1610.05540.pdf", "metadata": {"source": "CRF", "title": "SYSTRAN\u2019s Pure Neural Machine Translation Systems", "authors": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aur\u00e9lien Coquard", "Yongchao Deng", "Satoshi Enoue", "Chiyo Geiss", "Joshua Johanson", "Bo Wang", "Jin Yang", "Dakun Zhang", "Jing Zhou", "Peter Zoldan"], "emails": ["firstname.lastname@systrangroup.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n05 54\n0v 1\n[ cs\n.C L\n] 1\n8 O\nct 2\n01 6\nSince the first online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work.\nOur ultimate goal is to share our expertise to build competitive production systems for \u201dgeneric\u201d translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems."}, {"heading": "1 Introduction", "text": "Neural MT has recently achieved state-of-theart performance in several large-scale translation tasks. As a result, the deep learning approach to MT has received exponential attention, not only by the MT research community but by a growing number of private entities, that begin to include NMT engines in their production systems.\nIn the last decade, several opensource MT toolkits have emerged\u2014Moses (Koehn et al., 2007) is probably the best-known out-of-the-box MT system\u2014coexisting with commercial alternatives, though lowering the entry barriers and bringing new opportunities on both research and business areas. Following this direction, our NMT system is based on the opensource project seq2seq-attn1 initiated by the Harvard NLP group2 with the main contributor Yoon Kim. We are contributing to the project by sharing several features described in this technical report, which are available to the MT community.\nNeural MT systems have the ability to directly model, in an end-to-end fashion, the association from an input text (in a source language) to its translation counterpart (in a target language). A major strength of Neural MT lies in that all the necessary knowledge, such as syntactic and semantic information, is learned by taking the global sentence context into consideration when modeling translation. However, Neural MT engines are known to be computationally very expensive, sometimes needing for several weeks to accomplish the training phase, even making use of cutting-edge hardware to accelerate computations. Since our interest is for a large variety of languages, and that based on our long experience with machine translation, we do not believe that a onefits-all approach would work optimally for lan-\n1https://github.com/harvardnlp/seq2seq-attn 2http://nlp.seas.harvard.edu\nguages as different as Korean, Arabic, Spanish or Russian, we did run hundreds of experiments, and particularily explored language specific behaviors. One of our goal would indeed be to be able to inject existing language knowledge in the training process.\nIn this work we share our recipes and experience to build our first generation of productionready systems for \u201cgeneric\u201d translation, setting a starting point to build specialized systems. We also report on extending the baseline NMT engine with several features that in some cases increase performance accuracy and/or efficiency while for some others are boosting the learning curve, and/or model speed. As a machine translation company, in addition to decoding accuracy for \u201cgeneric domain\u201d, we also pay special attention to features such as:\n\u2022 Training time\n\u2022 Customization possibility: user terminology, domain adaptation\n\u2022 Preserving and leveraging internal format tags and misc placeholders\n\u2022 Practical integration in business applications: for instance online translation box, but also translation batch utilities, post-editing environment...\n\u2022 Multiple deployment environments: cloudbased, customer-hosted environment or embedded for mobile applications\n\u2022 etc\nMore important than unique and uniform translation options, or reaching state-of-the-art research systems, our focus is to reveal language specific settings, and practical tricks to deliver this technology to the largest number of users.\nThe remaining of this report is as follows: Section 2 covers basic details of the NMT system employed in this work. Description of the translation resources are given in section 3. We report on the different experiments for trying to improve the system by guiding the training process in section 4 and section 5, we discuss about performance. In section 6 and 7, we report on evaluation of the models and on practical findings. And we finish by describing work in progress for the next release."}, {"heading": "2 System Description", "text": "We base our NMT system on the encoder-decoder framework made available by the open-source project seq2seq-attn. With its root on a number of established open-source projects such as Andrej Karpathy\u2019s char-rnn,3 Wojciech Zaremba\u2019s standard long short-term memory (LSTM)4 and the rnn library from Element-Research,5 the framework provides a solid NMT basis consisting of LSTM, as the recurrent module and faithful reimplementations of global-general-attention model and input-feeding at each time-step of the RNN decoder as described by Luong et al. (2015).\nIt also comes with a variety of features such as the ability to train with bidirectional encoders and pre-trained word embeddings, the ability to handle unknown words during decoding by substituting them either by copying the source word with the most attention or by looking up the source word on an external dictionary, and the ability to switch between CPU and GPU for both training and decoding. The project is actively maintained by the Harvard NLP group6.\nOver the course of the development of our own NMT system, we have implemented additional features as described in Section 4, and contributed back to the open-source community by making many of them available in the seq2seq-attn repository. seq2seq-attn is implemented on top of the popular scientific computing library Torch.7 Torch uses Lua, a powerful and light-weight script language, as its front-end and uses the C language where efficient implementations are needed. The combination results in a fast and efficient system both at the development and the run time. As an extension, to fully benefit from multi-threading, optimize CPU and GPU interactions, and to have finer control on the objects for runtime (sparse matrix, quantized tensor, ...), we developed a C-based decoder using the C APIs of Torch, called C-torch, explained in detail in section 5.4.\nThe number of parameters within an NMT model can grow to hundreds of millions, but there are also a handful of meta-parameters that need to be manually determined. For some of the meta-\n3https://github.com/karpathy/char-rnn 4https://github.com/wojzaremba/lstm 5https://github.com/Element-Research/rnn 6http://nlp.seas.harvard.edu 7http://torch.ch\nparameters, many previous work presents clear choices on their effectiveness, such as using the attention mechanism or feeding the previous prediction as input to the current time step in the decoder. However, there are still many more meta-parameters that have different optimal values across datasets, language pairs, and the configurations of the rest of the meta-parameters. In table 1, we list the meta-parameter space that we explored during the training of our NMT systems.\nIn appendix B, we detail the parameters used for the online system of this first release."}, {"heading": "3 Training Resources", "text": "Training \u201cgeneric\u201d engines is a challenge, because there is no such notion of generic translation which is what online translation service users are expecting from these services. Indeed online translation is covering a very large variety of use cases, genres and domains. Also available opensource corpora are domain specific: Europarl (Koehn, 2005), JRC (Steinberger et al., 2006) or MultiUN (Chen and Eisele, 2012) are legal texts, ted talk are scientific presentations, open subtitles (Tiedemann, 2012) are colloquial, etc. As a result,\nthe training corpora we used for this release were built by doing a weighted mix all of the available sources. For languages with large resources, we did reduce the ratio of the institutional (Europal, UN-type), and colloquial types \u2013 giving the preference to news-type, mix of webpages (like Gigaword).\nOur strategy, in order to enable more experiments was to define 3 sizes of corpora for each language pair: a baseline corpus (1 million sentences) for quick experiments (day-scale), a medium corpus (2-5M) for real-scale system (week-scale) and a very large corpora with more than 10M segments.\nThe amount of data used to train online systems are reported in table 2, while most of the individual experimental results reported in this report are obtained with baseline corpora.\nNote that size of the corpus needs to be considered with the number of training periods since the neural network is continuously fed by sequences of sentence batches till the network is considered trained. In Junczys-Dowmunt et al. (2016), authors mention using corpus of 5M sentences and training of 1.2M batches each having 40 sentences \u2013 meaning basically that each sentence of the full corpus is presented 10 times to the training. In Wu et al. (2016), authors mention 2M steps of 128 examples for English\u2013French, for a corpus of 36M sentences, meaning about 7 iterations on the complete corpus. In our framework, for this release, we systematically extended the training up to 18 epochs and for some languages up to 22 epochs.\nSelection of the optimal system is made after the complete training by calculating scores on independent test sets. As an outcome, we have seen different behaviours for different language pairs with similar training corpus size apparently connected to the language pair complexity. For instance, English\u2013Korean training perplexity still decreases significantly between epoch 13 and 19 while Italian\u2013English perplexity decreases marginally after epoch 10. For most languages, in our set-up, optimal systems are achieved around epoch 15.\nWe did also some experiment on the corpus size. Intuitively, since NMT systems do not have the memorizing capacity of PBMT engines, the fact that the training use 10 times 10M sentence corpus, or 20 times 5M corpus should not make a huge difference. In one of the experiment, we\ncompared training on a 5M corpus trained over 20 epochs for English to/from French, and the same 5M corpus for only 10 epochs, followed by 10 additional epochs on additional 5M corpus. The 10M being completely homogeneous. In both directions, we observe that the 5 \u00d7 10 + 5 \u00d7 10 training is completing with a score improvement of 0.8 \u2212 1.2 compared to the 5\u00d7 20 showing that the additional corpus is managing to bring a meaningful improvement. This observation leads to a more general question about how much corpus is needed to actually build a high quality NMT engine (learn the language), the role and timing of diversity in the training and whether the incremental gain could not be substituted by terminology feeding (learn the lexicon)."}, {"heading": "4 Technology", "text": "In this section we account for several experiments that improved different aspects of our translation engines. Experiments range from preprocessing techniques to extend the network with the ability to handle named entities, to use multiple word features and to enforce the attention module to be more like word alignments. We also report on different levels of translation customization."}, {"heading": "4.1 Tokenization", "text": "All corpora are preprocessed with an in-house toolkit. We use standard token separators (spaces, tabs, etc.) as well as a set of language-dependent linguistic rules. Several kinds of entities are recognized (url and number) replacing its content by the appropriate place-holder. A postprocess is used to detokenize translation hypotheses, where the original raw text format is regenerated following equivalent techniques.\nFor each language, we have access to language specific tokenization and normalization rules. However, our preliminary experiments showed that there was no obvious gain of using these language specific tokenization patterns, and that some of the hardcoded rules were actually degrading the performance. This would need more investigation, but for the release of our first batch systems, we used a generic tokenization model for most of the languages except Arabic, Chinese and German. In our past experiences with Arabic, separating segmentation of clitics was beneficial, and we retained the same procedure. For German and Chinese, we used in-house compound splitter and\nword segmentation models, respectively.\nIn our current NMT approach, vocabulary size is an important factor that determines the efficiency and the quality of the translation system; a larger vocabulary size correlates directly to greater computational cost during decoding, whereas low coverage of vocabulary leads to severe out-of-vocabulary (OOV) problems, hence lowering translation quality.\nIn most language pairs, our strategy combines a vocabulary shortlist and a placeholder mechanism, as described in Sections 4.2 and 4.3. This approach, in general, is a practical and linguisticallyrobust option to addressing the fixed vocabulary issue, since we can take the full advantage of internal manually-crafted dictionaries and customisized user dictionaries (UDs).\nA number of previous work such as characterlevel (Chung et al., 2016), hybrid word-characterbased (Luong and Manning, 2016) and subwordlevel (Sennrich et al., 2016b) address issues that arise with morphologically rich languages such as German, Korean and Chinese. These approaches either build accurate open-vacabulary word representations on the source side or improve translation models\u2019 generative capacity on the target side. Among those approaches, subword tokenization yields competitive results achieving excellent vocabulary coverage and good efficiency at the same time.\nFor two language pairs: enko and jaen, we used source and target sub-word tokenization (BPE, see (Sennrich et al., 2016b)) to reduce the vocabulary size but also to deal with rich morphology and spacing flexibility that can be observed in Korean. Although this approach is very seducing by its simplicity and also used systematically in (Wu et al., 2016) and (Junczys-Dowmunt et al., 2016), it does not have significant side effects (for instance generation of impossible words) and is not optimal to deal with actual word morphology - since the same suffix (josa in Korea) depending on the frequency of the word ending it is integrated with, will be splitted in multiple representations. Also, in Korean, these josa, are is an \u201cagreement\u201d with the previous syllabus based on their final endings: however such simple information is not explicitely or implicitely reachable by the neural network.\nThe sub-word encoding algorithm Byte Pair Encoding (BPE) described by Sennrich et al. (2016b)\nwas re-implemented in C++ for further speed optimization."}, {"heading": "4.2 Word Features", "text": "Sennrich and Haddow (2016) showed that using additional input features improves translation quality. Similarly to this work, we introduced in the framework the support for an arbitrary number of discrete word features as additional inputs to the encoder. Because we do not constrain the number of values these features can take at the same time, we represent them with continuous and normalized vectors. For example, the representation of a feature f at time-step t is:\nx (t) i =\n{\n1 nf\nif f takes the ith value\n0 otherwise (1)\nwhere nf is the number of possible values the feature f can take and with x(t) \u2208 Rnf .\nThese representations are then concatenated with the word embedding to form the new input to the encoder.\nWe extended this work by also supporting additional features on the target side which will be predicted by the decoder. We used the same input representation as on the encoder side but shifted the features sequence compared to the words sequence so that the prediction of the features at time-step t depend on the word at time-step t they annotate. Practically, we are generating feature at time t+ 1 for the word generated at time t.\nTo learn these target features, we added a linear layer to the decoder followed by the softmax function and used the mean square error criterion to learn the correct representation.\nFor this release, we only used case information as additional feature. It allows us to work with a lowercased vocabulary and treat the recasing as a separate problem. We observed that the use of this simple case feature in source and target does improve the translation quality as illustrated in Figure 1. Also, we compared the accuracy of the induced recasing with other recasing frameworks (SRI disamb, and in-house recasing tool based on n-gram language models) and observed that the prediction of case by the NN was higher than using external recaser, which was expected since NN has access to source in addition to the source sentence context, and target sentence history."}, {"heading": "4.3 Named Entities (NE)", "text": "SYSTRAN\u2019s RBMT and SMT translation engines utilize number and named entity (NE) module to recognize, protect, and translate NE entities. Similarly, we used the same internal NE module to recognize numbers and named entities in the source sentence and temporarily replaced them with their corresponding placeholders (Table 3).\nBoth the source and the target side of the training dataset need to be processed for NE placeholders. To ensure the correct entity recognition, we cross-validate the recognized entity across parallel dataset, that is: a valid entity recognition in a sentence should have the same type of entity in its parallel pair and the word or phrase covered by the entities need to be aligned to each other. We used fast align (Dyer et al., 2013) to automatically align source words to target words.\nIn our datasets, generally about one-fifth of the training instances contained one or more NE placeholders. Our training dataset consists of sentences with NE placeholders as well as sentences without them to be able to handle both instantiated and recognized entity types.\nPer source sentence, a list of all entities, along with their translations in the target language, if available, are returned by our internal NE recognition module. The entities in the source sentence is then replaced with their corresponding NE placeholders. During beam search, we make sure that an entity placehoder is translated by itself in the target sentence. When the entire target sentence is produced along with the attention weights that provide soft alignments back to the original source tokens, Placeholders in the target sentences are replaced with either the original source string or its translation.\nThe substitution of the NE placeholders with their correct values needs language pair-specific considerations. In Figure 4, we show that even the handling of Arabic numbers cannot be straightforward as copying the original value in the source text."}, {"heading": "4.4 Guided Alignments", "text": "We re-reimplemented Guided alignment strategy described in Chen et al. (2016). Guided alignment enforces the attention weights to be more like alignments in the traditional sense (e.g. IBM4 viterbi alignment) where the word alignments explicitly indicate that source words aligned to a target word are translation equivalents.\nSimilarly to the previous work, we created an additional criterion on attention weights, Lga, such that the difference in the attention weights and the reference alignments is treated as an error and directly and additionally optimize the output\nof the attention module.\nLga(A,\u03b1) = 1\nT \u00b7\n\u2211\nt\n\u2211\ns\n(Ast \u2212 \u03b1st) 2\nThe final loss function for the optimization is then:\nLtotal = wga \u00b7Lga(A,\u03b1) + (1\u2212wga) \u00b7Ldec(y, x)\nwhere A is the alignment matrix, \u03b1 attention weights, s and t indicating the indices in the source and target sentences, and wga the linear combination weight for the guided alignment loss.\nChen et al. (2016) also report that decaying wga, thereby gradually reducing the influence from guided alignment, over the course of training found to be helpful on certain datasets. When guided alignment decay is enabled, wga is gradually reduced at this rate after each epoch from the beginning of the training.\nWithout searching for the optimal parameter values, we simply took following configurations from the literature: mean square error (MSE) as loss function, 0.5 as the linear-combination weight for guided alignment loss and the cross-entropy loss for decoder, and 0.9 for decaying factor for guided alignment loss.\nFor the alignment, we again utilized fast align tool. We stored alignments in sparse format8 to save memory usage, and for each minibatch a dense matrix is created for faster computation.\nApplying such a constraint on attention weights can help locate the original source word more\n8Compressed Column Storage (CCS)\naccurately, which we hope to benefits better NE placeholder substitutions and especially unknown word handling.\nIn Figure 2, we see the effects of guided alignment and its decay rate on our English-to-French generic model. Unfortunately, this full comparative run was disrupted by a power outage and we did not have time to relaunch, however we can still clearly observe that up to the initial 5 epochs, guided alignment, with or without decay, provides rather big boosts over the baseline training. After 5 epochs, the training with decay slow down compare to the training without, which is rather intuitive: the guided alignment is indeed in conflict with the attention learning. What would remain to be seen, is if at the training the training, the baseline and the guided alignment with decay are converging."}, {"heading": "4.5 Politeness Mode", "text": "Many languages have ways to express politeness and deference towards people being reffered to in sentences. In Indo-European languages, there are two pronouns corresponding to the English You; it is called the T-V distinction between the informal Latin pronoun tu (T) and the polite Latin pronoun Vos (V). Asian languages, such as Japanese and Korean, make an extensive use of honorifics (respectful words), words that are usually appended to the ends of names or pronouns to indicate the relative ages and social positions of the speakers. Expressing politeness can also impact the vocabu-\nlary of verbs, adjectives, and nouns used, as well as sentence structures.\nFollowing the work of Sennrich et al. (2016a), we implemented a politeness feature in our NMT engine: a special token is added to each source sentence during training, where the token indicates the politeness mode observed in the target sentence. Having such an ability to specify the politeness mode is very useful especially when translating from a language where politeness is not expressed, e.g. English, into where such expressions are abundant, e.g. Korean, because it provides a way of customizing politeness mode of the translation output. Table 5 presents our English-to-Korean NMT model trained with politeness mode, and it is clear that the proper verb endings are generated according to the user selection. After a preliminary evaluation on a small testset from English to Korean, we observed 70 to 80% accuracy of the politeness generation (Table 6). We also noticed that 86% of sentences (43 out of 50) have exactly the same meaning preserved across different politeness modes.\nThis simple approach, however, comes at a small price, where sometimes the unknown replacement scheme tries to copy the special token in the target generation. A more appropriate approach that we plan to switch to in our future trainings is to directly feed the politeness mode into the sentential representation of the decoder."}, {"heading": "4.6 Customization", "text": "Domain adaptation is a key feature for our customers\u2014it generally encompasses terminology, domain and style adaptation, but can also be seen as an extension of translation memory for human post-editing workflows.\nSYSTRAN engines integrate multiple techniques for domain adaptation, training full new\nin-domain engines, automatically post-editing an existing translation model using translation memories, extracting and re-using terminology. With Neural Machine Translation, a new notion of \u201cspecialization\u201d comes close to the concept of incremental translation as developed for statistical machine translation like (Ortiz-Mart\u0131\u0301nez et al., 2010)."}, {"heading": "4.6.1 Generic Specialization", "text": "Domain adaptation techniques have successfully been used in Statistical Machine Translation. It is well known that a system optimized on a specific text genre obtains higher accuracy results than a \u201cgeneric\u201d system. The adaptation process can be done before, during or after the training process. Our preliminary experiments follow the latter approach. We incrementally adapt a Neural MT \u201cgeneric\u201d system to a specific domain by running additional training epochs over newly available indomain data.\nAdaptation proceeds incrementally when new in-domain data becomes available, generated by human translators while post-editing, which is similar to the Computer Aided Translation framework described in (Cettolo et al., 2014).\nWe experiment on an English-to-French translation task. The generic model is a subsample of the corpora made available for the WMT15 translation task (Bojar et al., 2015). Source and target NMT vocabularies are the 60k most frequent words of source and target training datasets. The in-domain data is extracted from the European Medical Agency (EMEA) corpus. Table 7 shows some statistics of the corpora used in this experiment.\nOur preliminary results show that incremental adaptation is effective for even limited amounts of in-domain data (nearly 50k additional words). Constrained to use the original \u201cgeneric\u201d vocabulary, adaptation of the models can be run in a few seconds, showing clear quality improvements on in-domain test sets.\nFigure 3 compares the accuracy (BLEU) of\ntwo systems: full is trained after concatenation of generic and in-domain data; adapt is initially trained over generic data (showing a BLEU score of 29.01 at epoch 0) and adapted after running several training epochs over only the in-domain training data. Both systems share the same \u201dgeneric\u201d NMT vocabularies. As it can be seen the adapt system improves drastically its accuracy after a single additional training epoch, obtaining a similar BLEU score than the full system (separated by .91 BLEU). Note also that each additional epoch using the in-domain training data takes less than 50 seconds to be processed, while training the full system needs more than 17 hours.\nResults validate the utility of the adaptation approach. A human post-editor would take advantage of using new training data as soon as it becomes available, without needing to wait for a long full training process. However, the comparison is not entirely fair since full training would allow to include the in-domain vocabulary in the new full model, what surely would result in an additional accuracy improvement."}, {"heading": "4.6.2 Post-editing Engine", "text": "Recent success of Pure Neural Machine Translation has led to the application of this technology to various related tasks and in particular to the Automatic Post-Editing (APE). The goal of this task is to simulate the behavior of a human post-editor, correcting translation errors made by a MT system.\nUntil recently, most of the APE approaches have been based on phrase-based SMT systems, either monolingual (MT target to human post-edition) (Simard et al., 2007) or source-\naware (Be\u0301chara et al., 2011). For many years now SYSTRAN has been offering a hybrid Statistical Post-Editing (SPE) solution to enhance the translation provided by its rule-based MT system (RBMT) (Dugast et al., 2007).\nFollowing the success of Neural PostEditing (NPE) in the APE task of WMT\u201916 (Junczys-Dowmunt and Grundkiewicz, 2016), we have run a series of experiments applying the neural approach in order to improve the RBMT system output. As a first experiment, we compared the performance of our English-to-Korean SPE system trained on technical (IT) domain data to two NPE systems trained on the same data: monolingual NPE and multi-source NPE, where the input language and the MT hypothesis sequences have been concatenated together into one input sequence (separated by a special token).\nFigure 4 illustrates the accuracy (BLEU) results of four different systems at different training epochs. The RBMT system performs poorly, confirming the importance of post-editing. Both NPE systems clearly outperform SPE. It can also be observed that adding source information even in a simplest way possible (NPE multi-source), without any source-target alignment, considerably improves NPE translation results.\nThe system performing NPE multi-source obtains similar accuracy results than pure NMT. What can be seen is that NPE multi-source essentially employs the information from the original sentence to produce translations. However, notice that benefits of utilizing multiple inputs from different sources is clear at earlier epochs while once the model parameters converge, the differ-\nence in performances of NMT and NPE multisource models become negligible.\nFurther experiments are currently being conducted aiming at finding more sophisticated ways of combining the original source and the MT translation in the context of NPE."}, {"heading": "5 Performance", "text": "As previously outlined, one of the major drawbacks of NMT engines is the need for cutting-edge hardware technology to face the enormous computational requirements at training and runtime.\nRegarding training, there are two major issues: the full training time and the required computation power, i.e. the server investment. For this release, most of our trainings have been running on single GTX GeForce 1080 GPU (about $2.5k) while in (Wu et al., 2016), authors mention using 96 K80 GPU for a full week for training one single language pair (about $250k). On our hardware, full training on 2x5M sentences (see section 3) took a bit less than one month.\nA reasonnable target is to maintain training time for any language pair under one week and keeping reasonable investment so that the full research community can have competitive trainings but also indirectly so that all of our customers can benefit from the training technology. To do so, we need to better leverage multiple GPUs on a single server which is on-going engineering work. We also need to continue on exploring how to learn more with less data. For instance, we are convinced that injecting terminology as part of the training data should be competitive with continuing adding full sentences.\nAlso, shortening training cycle can also be achieved by better control of the training cycle. We have shown that multiple features are boosting the training pace, and if going to bigger network is clearly improving performance. For instance, we are using a bidirectional 4 layer RNN in addition to our regular RNN, but in Wu et al. (2016), authors mention using bidirectional RNN only for the first layer. We need to understand more these phenomena and restrict to the minimum to reduce the model size.\nFinally, work on specialization described in sections 4.6.1 and 5.2 are promising for long term maintenance: we could reach a point where we do not need to retrain from scratch but continuously improve existing model and use teacher models to\nboost initial trainings. Regarding runtime performance, we have been exploring the following areas and are reaching today throughputs compatible with production requirement not only using GPUs but also using CPU and we report our different strategies in the following sub-sections."}, {"heading": "5.1 Pruning", "text": "Pruning the parameters of a neural network is a common technique to reduce its memory footprint. This approach has been proven efficient for the NMT tasks in See et al. (2016). Inspired by this work, we introduced similar pruning techniques in seq2seq-attn. We reproduced that models parameters can be pruned up to 60% without any performance loss after retraining as shown in Table 8.\nWith a large pruning factor, neural network\u2019s weights can also be represented with sparse matrices. This implementation can lead to lower computation time but more importantly to a smaller memory footprint that allows us to target more environment. Figures 5 and 6 show experiments involving sparse matrices using Eigen9. For example, when using the float precision, a multiplication with a sparse matrix already begins to take less memory when 35% of its parameters are pruned.\nRelated to this work, we present in Section 5.4 our alternative Eigen-based decoder that allows us to support sparse matrices."}, {"heading": "5.2 Distillation", "text": "Despite that surprisingly accurate, NMT systems need for deep networks in order to perform well. Typically, a 4-layer LSTM with 1000 hidden units per layer (4 x 1000) are used to obtain state-ofthe-art results. Such models require cutting-edge\n9http://eigen.tuxfamily.org\nhardware for training in reasonable time while inference becomes also challenging on standard setups, or on small devices such as mobile phones. Though, compressing deep models into smaller networks has been an active area of research.\nFollowing the work in (Kim and Rush, 2016), we experimented sequence-level knowledge distillation in the context of an English-to-French NMT task. Knowledge distillation relies on training a smaller student network to perform better by learning the relevant parts of a larger teacher network. Hence, \u2019wasting\u2019TM parameters on trying to model the entire space of translations. Sequencelevel is the knowledge distillation variant where the student model mimics the teacher\u2019s actions at the sequence-level.\nThe experiment is summarized in 3 steps:\n\u2022 train a teacher model on a source/reference training set,\n\u2022 use the teacher model to produce translations for the source training set,\n\u2022 train a student model on the new source/translation training set.\nFor our initial experiments, we produced 35- best translations for each of the sentences of the source training set, and used a normalized n-gram matching score computed at the sentence level, to select the closest translation to each reference sentence. The original training source sentences and their translated hypotheses where used as training data to learn a 2 x 300 LSTM network.\nResults showed slightly higher accuracy results for a 70% reduction of the number of parameters and a 30% increase on decoding speed. In a second experiment, we learned a student model with the same structure than the teacher model. Surprisingly, the student clearly outperformed the teacher model by nearly 1.5 BLEU.\nWe hypothesize that the translation performed over the target side of the training set produces a sort of language normalization which is by construction very heterogeneous. Such normalization eases the translation task, being learned by not so deep networks with similar accuracy levels."}, {"heading": "5.3 Batch Translation", "text": "To increase translation speed of large texts, we support batch translation that works in addition to the beam search. It means that for a beam of size\nK and a batch of size B, we forward K \u00d7 B sequences into the model. Then, the decoder output is split across each batch and the beam path for each sentence is updated sequentially.\nAs sentences within a batch can have large variations in size, extra care is needed to mask accordingly the output of the encoder and the attention softmax over the source sequences.\nFigure 7 shows the speedup obtained using batch decoding in a typical setup."}, {"heading": "5.4 C++ Decoder", "text": "While Torch is a powerful and easy to use framework, we chose to develop an alternative C++ implementation for the decoding on CPU. It increases our control over the decoding process and open the path to further memory and speed improvements while making deployment easier.\nOur implementation is graph-based and use Eigen for efficient matrix computation. It can load and infer from Torch models.\nFor this release, experiments show that the decoding speed is on par or faster than the Torch-based implementation especially in a multithreaded context. Figure 8 shows the better use of parallelization of the Eigen-based implementation."}, {"heading": "6 Evaluation", "text": "Evaluation of machine translation has always been a challenge and subject to many papers and dedicated workshops (Bojar et al., 2016). While automatic metrics are now used as standard in the\nresearch world and have shown good correlation with human evaluation, ad-hoc human evaluation or productivity analysis metrics are rather used in the industry (Blain et al., 2011).\nAs a translation solution company, even if automatic metrics are used through all the training process (and we give scores in the section 6.1), we care about human evaluation of the results. Wu et al. (2016) mention human evaluation but simultaneously cast a doubt on the referenced human to translate or evaluate. In this context, the claim \u201calmost indistinguishable with human translation\u201d is at the same time strong but also very vague. On our side, we have observed during all our experiments and preparation of specialized models, unprecedented level of quality, and contexts where we could claim \u201csuper human\u201d translation quality.\nHowever, we need to be very carefully defining the tasks, the human that are being compared to, and the nature of the evaluation. For evaluating technical translation, the nature of the evaluation is somewhat easy and really depending on the user expectation: is the meaning properly conveyed, or is the sentence faster to post-edit than to translate. Also, to avoid doubts about integrity or competency of the evaluation we sub-contracted the task to CrossLang, a company specialized in machine translation evaluation. The test protocol was defined collaboratively and for this first release, we decided to perform ranking of different systems, and we present in the section 6.2 the results obtained on two very different language pairs: English to/from French, and English to Korean.\nFinally, in the section 6.3, we also present some\nqualitative evaluation results showing specificities of the Neural Machine Translation."}, {"heading": "6.1 Automatic Scoring and system comparison", "text": "Figure 9 plots automatic accuracy results, BLEU, and Perplexities for all language pairs. It is remarkable the high correlation between perplexity and BLEU scores, showing that language pairs with lower perplexity yield higher BLEU scores. Note also that different amounts of training data were used for each system (see Table 2). BLEU scores were calculated over an internal test set.\nFrom the beginning of this report we have used \u201cinternal\u201d validation and test sets, what makes it difficult to compare the performance of our systems to other research engines. However, we must keep in mind that our goal is to account for improvements in our production systems. We focus on human evaluations rather than on any automatic evaluation score."}, {"heading": "6.2 Human Ranking Evaluation", "text": "To evaluate translation outputs and compare with human translation, we have defined the following protocol.\n1. For each language pair, 100 sentences \u201cin domain\u201d (*) are collected,\n2. These sentences are sent to human translation (**), and translated with candidate model and using available online translation services (***).\n3. Without notification of the mix of human and machine translation, a team of 3 professional translators or linguists fluent in both source and target languages is then asked to rank 3 random outputs for each sentence based on their preference as translation. Preference includes accuracy as a priority, but also fluency of the generated translation. They have the choice to give them 3 different ranks, or can also decide to give 2 or the 3 of them the same rank, if they cannot decide.\n(*) for Generic domain, sentences from recent news article were selected online, for Technical (IT) sentences part of translation memory defined in section 4.6 were kept apart from the training. (**) for human translation, we did use translation agency (human-pro), and online collaborative translation platform (human-casual).\n(***) we used Naver Translator10 (Naver), Google Translate11 (Google) and Bing Translator12 (Bing).\nFor this first release, we experimented on the evaluation protocol for 2 different extremely different categories of language pairs. On one hand, English\u2194French which is probably the most studied language pairs for MT and for which resources are very large: (Wu et al., 2016) mention about 36M sentence pairs used in their NMT training and the equivalent PBMT is completed by a webscale target side language models13. Also, as English is a low inflected language, the current phrase-based technology for target language English is more competitive due to the relative simplicity of the generation and weight of gigantic language models.\nOn the other hand, English\u2194Korean is one of the toughest language pair due to the far distance between English and Korean language, the small availability of training corpus, and the rich agglutinative morphology of Korean. For a real comparison, we ran evaluation against Naver Translation service from English into Korean, where Naver is the main South Korean search engine.\nTables 9 and 10 describe the different evaluations and their results.\nSeveral interesting outcomes:\n\u2022 Our vanilla English 7\u2192 French model outperforms existing online engines and our best of breed technology.\n\u2022 For French 7\u2192 English, if the model slightly outperforms (human-casual) and our best of breed technology, it stays behind Google Translate, and more significantly behind Microsoft Translator.\n\u2022 The generic English 7\u2192 Korean model shows closer results with human translation and outperforms clearly existing online engines.\n\u2022 The \u201cin-domain\u201d specialized model surprisingly outperforms the reference human translation.\nWe are aware that far more evaluations are necessary and we will be launching a larger evalu-\n10http://translate.naver.com 11http://translate.google.com 12http://translator.bing 13In 2007, Google already mentions using 2 trillion words in their language models for machine translation (Brants et al., 2007).\nation plan for our next release. Informally, we do observe that the biggest performance jump are observed on complicated language pairs, like English-Korean or Japanese-English showing that NMT engines are better able to handle major structure differences, but also on the languages with lower resources like Farsi-English demonstrating that NMT is able to learn better with less, and we will explore this even more.\nFinally, we are also launching in parallel, a reallife beta-testing program with our customers so that we can also obtain formal feedback from their use-case and related to specialized models."}, {"heading": "6.3 Qualitative Evaluation", "text": "In the table 11, we report the result of error analysis for NMT, SMT and RBMT for English-French language pair. This evaluation confirms the translation ranking performed in the previous but also exhibits some interesting facts:\n\u2022 The most salient error comes from missing words or parts of sentence. It is interesting to see though that half of these \u201comissions\u201d are considered okay by the reviewers and were most of the time not considered as errors - it indeed shows the ability of the system not only to translate but to summarize and get to the point as we would expect from human translation. Of course, we need to fix the cases, where the \u201comissions\u201d are not okay.\n\u2022 Another finding is that the engine is badly managing quotes, and we will make sure to specifically teach that in our next release. Other low-hanging fruit are the case generation, which seems sometimes to get off-track, and the handling of Named Entity that we have already introduced in the system but not connected for the release.\n\u2022 On the positive side, we observe that NMT is drastically improving fluency, reduces slightly meaning selection errors, and handle better morphology although it does not have yet any specific access to morphology (like sub-word embedding). Regarding meaning selection errors, we will focussing on teaching more expressions to the system which is still a major structural weakness compared to PBMT engines."}, {"heading": "7 Practical Issues", "text": "Translation results from an NMT system, at first glance, is incredibly fluent that your are diverted from its downsides. Over the course of the training and during our internal evaluation, we found out multiple practical issues worth sharing:\n1. translating very long sentences\n2. translating user input such as a short word or the title of a news article\n3. cleaning the corpus\n4. alignment\nNMT is greatly impacted by the train data, on which NMT learns how to generate accurate and fluent translations holistically. Because the maximal length of a training instance was limited to a certain length during the training of our models, NMT models are puzzled by sentences that exceed this length, not having encountered such a training data. Hard splitting of longer sentences has some side-effects since the model consider both parts as full sentence. As a consequence, whatever is the limit we set for sentence length, we do also need to teach the neural network how to handle longer sentences. For that, we are exploring several options including using separate model based on source/target alignment to find optimal breaking point, and introduce special <TO BE CONTINUED> and <CONTINUING> tokens. Likewise, very short phrases and incomplete or fragmental sentences were not included in our training data, and consequently NMT systems fail to correctly translate such input texts (e.g. Figure 10). Here also, to enable this, we do simply need to teach the model to handle such input by injecting additional synthetic data.\nAlso, our training corpus includes a number of resources that are known to contain many noisy data. While NMT seems more robust than other technologies for handling noise, we can still perceive noise effect in translation - in particular for recurring noise. An example is in English-toKorean, where we see the model trying to systematically convert amount currency in addition to the translation. As demonstrated in Section 5.2, preparing the right kind of input to NMT seems to result in more efficient and accurate systems, and such a procedure should also be directly applied to the training data more aggressively.\nFinally,let us note that source/target alignment is a must for our users, but this information is missing from NMT output due to soft alignment. To hide this issue from the end-users, multiple alignment heuristic are showing tradictional target-source alignment."}, {"heading": "8 Further Work", "text": "In this section we outline further experiments currently being conducted. First we extend NMT decoding with the ability to make use of multiple models. Both external models, particularly an n-gram language model, as well as decoding with multiple networks (ensemble). We also work on using external word embeddings, and on modelling unknown words within the network."}, {"heading": "8.1 Extending Decoding", "text": ""}, {"heading": "8.1.1 Additional LM", "text": "As proposed by (Gu\u0308lc\u0327ehre et al., 2015), we conduct experiments to integrate an n-gram language model estimated over a large dataset on our Neural MT system. We followed a shallow fusion integration, similar to how language models are used in a standard phrase-based MT decoder.\nIn the context of beam search decoding in NMT, at each time step t, candidate words x are hypothesized and assigned a score according to the neural network, pNMT (x). Sorted according to their respective scores, the K-best candidates, are reranked using the score assigned by the language model, pLM(x). The resulting probability of each candidate is obtained by the weighted sum of each log-probability log p(x) = log pLM (x) + \u03b2 log pNMT (x). Where \u03b2 is a hyper-parameter that needs to be tuned.\nThis technique is specially useful for handling out-of-vocabulary words (OOV). Deep networks are technically constrained to work with limited vocabulary sets (in our experiments we use target vocabularies of 60k words), hence suffering from important OOV problems. In contrast, n-gram language models can be learned for very large vocabularies.\nInitial results show the suitability of the shallow integration technique to select the appropriate OOV candidate out of a dictionary list (external resource). The probability obtained from a language model is the unique modeling alternative for those word candidates for which the neural network produces no score."}, {"heading": "8.1.2 Ensemble Decoding", "text": "Ensemble decoding has been verified as a practical technique to further improve the performance compared to a single Encoder-Decoder model (Sennrich et al., 2016b; Wu et al., 2016; Zhou et al., 2016). The improvement comes from the diversity of prediction from different neural network models, which are learned by random initialization seeds and shuffling of examples during training, or different optimization methods towards the development set(Cho et al., 2015). As a consequence, 3-8 isolated models will be trained and ensembled together, considering the cost of memory and training speed. Also, (Junczys-Dowmunt et al., 2016) provides some methods to accelerate the training by choosing different checkpoints as the final models.\nWe implement ensemble decoding by averaging the output probabilities for each estimation of target word x with the formula:\npensx = 1 M \u2211M m=1 p m x\nwherein, pmx represents probabilities of each x, and M is the number of neural models."}, {"heading": "8.2 Extending word embeddings", "text": "Although NMT technology has recently accomplished a major breakthrough in Machine Translation field, it still remains constrained due to the limited vocabulary size and to the use of bilingual training data. In order to reduce the negative impact of both phenomena, experiments are currently being held on using external word embedding weights.\nThose external word embeddings are not learned by the NMT network from bilingual data only, but by an external model (e.g. word2vec (Mikolov et al., 2013)). They can therefore be estimated from larger monolingual corpora, incorporating data from different domains.\nAnother advantage lies in the fact that, since external word embedding weights are not modified during NMT training, it is possible to use a different vocabulary for this fixed part of the input during the application or re-training of the model (provided that the weights for the words in new vocabulary come from the same embedding space as the original ones). This may allow a more efficient adaptation to the data coming from a different domain with a different vocabulary."}, {"heading": "8.3 Unknown word handling", "text": "When an unknown word is generated in the target output sentence, a general encoder-decoder with attentional mechanism utilizes heuristics based on attention weights such that the source word with the most attention is either directly copied as-is or looked up in a dictionary.\nIn the recent literature (Gu et al., 2016; Gulcehre et al., 2016), researchers have attempted to directly model the unknown word handling within the attention and decoder networks. Having the model learn to take control of both decoding and unknown word handling will result in the most optimized way to address the single unknown word replacement problem, and we are implementing and evaluating the previous approaches within our framework."}, {"heading": "9 Conclusion", "text": "Neural MT has progressed at a very impressive rate, and it has proven itself to be competitive against online systems trained on train data whose size is several orders of magnitude larger. There is no doubt that Neural MT is definitely a technology that will continue to have a great impact on academia and industry. However, at its current status, it is not without limitations; on language pairs that have abundant amount of monolingual and bilingual train data, phrase-based MT still perform better than Neural MT, because Neural MT is still limited on the vocabulary size and deficient utilization of monolingual data.\nNeural MT is not an one-size-fits-all technology such that one general configuration of the model universally works on any language pairs. For example, subword tokenization such as BPE provides an easy way out of the limited vocabulary problem, but we have discovered that it is not always the best choice for all language pairs. Attention mechanism is still not at the satisfactory status and it needs to be more accurate for better controlling the translation output and for better user interactions.\nFor upcoming releases, we have begun to making even more experiments with injection of various linguistic knowledges, at which SYSTRAN possesses the foremost expertise. We will also apply our engineering know-hows to conquer the practical issues of NMT one by one."}, {"heading": "Acknowledgments", "text": "We would like to thank Yoon Kim, Prof. Alexander Rush and the rest of members of the Harvard NLP group for their support with the open-source code, their pro-active advices and their valuable insights on the extensions.\nWe are also thankful to CrossLang and Homin Kwon for their thorough and meaningful definition of the evaluation protocol, and their evaluation team as well as Inah Hong, Weimin Jiang and SunHyung Lee for their work."}, {"heading": "A Remarkable Results", "text": "In this section, we highlight a serie of \u201cremarkable\u201d translations (positively remarkable and also few negatively outstanding) that we found out during evaluation for a variety of languages.\nLanguage Pair Source NMT Google Translate (2016/10/15) en7\u2192fr The report also said that although three-quarters of Chinese say their country is playing a larger role in the world than it did a decade ago, most want their government to focus on domestic issues rather than helping other nations.\nLe rapport dit aussi que, bien que les trois quarts des Chinois disent que leur pays joue un ro\u0302le plus important dans le monde qu\u2019il ne l\u2019a fait il y a dix ans, la plupart veulent que leur gouvernement se concentre sur les questions nationales pluto\u0302t que d\u2019aider d\u2019autres nations. Le rapport indique e\u0301galement que, bien que les trois quarts des Chinois disent que leur pays joue un ro\u0302le plus important dans le monde qu\u2019il y a dix ans, la plupart veulent que leur gouvernement de se concentrer sur les questions inte\u0301rieures pluto\u0302t que d\u2019aider les autres nations.\nfr 7\u2192en Fene\u0302tre a\u0300 rue se fermait mal, tre\u0300s tre\u0300s bruyant et mal dormi. Petit de\u0301jeuner banal. The room was very clean and the room was very clean. street window will not close, very very noisy and slept badly. mundane breakfast. en7\u2192ko Forty Republican senators and congressmen have revoked their support for the Republican presidential nominee \u2014 with nearly 30 of them calling on him to quit the race altogether in recent days. 40 \u1106\u1167\u11bc\u110b\u1174 \u1100\u1169\u11bc\u1112\u116a\u1103\u1161\u11bc \u1109\u1161\u11bc\u110b\u116f\u11ab\u110b\u1174\u110b\u116f\u11ab\u1103\u1173\u11af\u1100\u116a \u1112\u1161 \u110b\u116f\u11ab\u110b\u1174\u110b\u116f\u11ab\u1103\u1173\u11af\u110b\u1175 \u1100\u1169\u11bc\u1112\u116a\u1103\u1161\u11bc \u1103\u1162\u1110\u1169\u11bc\u1105\u1167\u11bc \u1112\u116e\u1107\u1169 \u110c\u1161\u110b\u1166 \u1103\u1162\u1112\u1161\u11ab \u1100\u1173\u1103\u1173\u11af\u110b\u1174 \u110c\u1175\u110c\u1175\u1105\u1173\u11af \u110e\u1165\u11af\u1112\u116c\u1112\u1162\u11bb \u1100\u1169, \u1100\u1165\u110b\u1174 30 \u1106\u1167\u11bc\u110b\u1174 \u1109\u1161\u1105\u1161\u11b7\u1103\u1173\u11af\u110b\u1175 \u110e\u116c\u1100\u1173\u11ab \u1106\u1167\u110e\u1175\u11af \u1103\u1169\u11bc\u110b\u1161\u11ab \u1100\u1173 \u1100\u1167\u11bc\u110c\u116e\u1105\u1173\u11af \u1100\u1173\u1106\u1161\u11ab\u1103\u116e\u1103\u1169\u1105\u1169\u11a8 \u1100\u1173\u110b\u1166\u1100\u1166\u110b\u116d\u110e\u1165\u11bc\u1112\u1162\u11bb\u1109\u1173\u11b8\u1102\u1175\u1103\u1161. (From Naver NMT14): 40\u110b\u1167\u1106\u1167\u11bc\u110b\u1174\u1100\u1169\u11bc \u1112\u116a\u1103\u1161\u11bc \u110b\u1174\u110b\u116f\u11ab\u1103\u1173\u11af\u1100\u116a \u1112\u1161\u110b\u116f\u11ab \u110b\u1174\u110b\u116f\u11ab\u1103\u1173\u11af\u110b\u1173\u11ab \u1100\u1165 \u110b\u1174 30\u1106\u1167\u11bc\u110b\u1174 \u1100\u1169\u11bc\u1112\u116a\u1103\u1161\u11bc \u1103\u1162\u1110\u1169\u11bc\u1105\u1167\u11bc \u1112\u116e\u1107\u1169\u1103\u1173\u11af \u110b\u1173\u11af\u110e\u1165\u11af\u1112\u116c\u1112\u1162\u11bb\u1100\u1169, \u1100\u1173\u1103\u1173\u11af\u110c\u116e\u11bc\u1100\u1165\u110b\u1174 30\u1106\u1167\u11bc\u110b\u1175 \u110e\u116c\u1100\u1173\u11ab\u110b\u1166 \u1100\u1173\u1105\u1173\u11af \u110b\u1173\u11ab\u1110\u116c \u110c\u1175\u11a8\u110c\u1165\u11ab\u110b\u1166 \u1100\u1173\u1106\u1161\u11ab\u1103\u116e \u1100\u1166 \u1112\u1161\u11af\u1100\u1165\u11ba\u110b\u1173\u11af\u110e\u1169\u11a8\u1100\u116e\u1112\u1162\u11bb\u1103\u1161. en7\u2192it all this leads to feelings of insecurity and conflict thus reinforcing negative spirals.\ntutto cio\u0300 porta a sentimenti di insicurezza e di conflitto, rafforzando cos\u0131\u0300 la spirale negativa.\ntutto cio\u0300 genera elementi di insicurezza e di conflitto rinforzando quindi spirali negative.\nit 7\u2192fr Mi sembra che cio\u0300 non abbia nulla a che vedere con una penuria alimentare: si tratta di una vera carestia, di una vera e propria catastrofe che non e\u0300 naturale ma strutturale, provocata da un regime folle, criminale, psicopatico, peggiore dei peggiori regimi esistiti nell\u2019Unione Sovietica e nella Romania di Ceaucescu. Il me semble que cela n\u2019a rien a\u0300 voir avec une pe\u0301nurie alimentaire: il s\u2019agit d\u2019une ve\u0301ritable famine, d\u2019une ve\u0301ritable catastrophe qui n\u2019est pas naturelle mais structurelle, provoque\u0301e par un re\u0301gime insense\u0301, criminel, psychopathe, pire que les pires re\u0301gimes de l\u2019union sovie\u0301tique et de la Roumanie de Ceaucescu. Il me semble que cela n\u2019a rien a\u0300 voir avec une pe\u0301nurie alimentaire: il est une ve\u0301ritable famine, une catastrophe qui est pas naturel, mais une structure, provoque\u0301e par un re\u0301gime criminel fou, psychopathe, le pire des pires re\u0301gimes existait l\u2019Union sovie\u0301tique et en Roumanie de Ceaucescu. it 7\u2192fr Ebbene io sono in totale disaccordo: lo Stato non deve passare in secondo piano; deve invece intervenire di piu\u0300 e garantire una maggiore vigilanza in ambito economico. je suis tout a\u0300 fait d\u2019accord: l\u2019e\u0301tat ne doit pas passer sous silence; il doit pluto\u0302t agir et assurer une plus grande vigilance dans le domaine e\u0301conomique. Eh bien, je suis en de\u0301saccord total: l\u2019Etat ne doit pas prendre un sie\u0300ge arrie\u0300re; Il doit pluto\u0302t agir plus et assurer une plus grande supervision dans l\u2019e\u0301conomie. es7\u2192en \u00bfEs posible un Bangladesh ma\u0301s amable con los nin\u0303os? Is Bangladesh more friendly with\nchildren?\nCan a kinder Bangladesh with children?\nen7\u2192de On the other hand, businesses are constantly complaining about the shortage of specialist staff, using this supposed shortage as an excuse to use cheap labour. Andererseits beschweren sich Unternehmen sta\u0308ndig u\u0308ber den Mangel an Fachpersonal, indem sie diese angebliche Knappheit als Vorwand benutzen, billige Arbeitskra\u0308fte einzusetzen. Auf der anderen Seite beschweren sich Unternehmen sta\u0308ndig u\u0308ber den Mangel an Fachkra\u0308ften, wobei diese angebliche Mangelhaftigkeit als Ausrede fu\u0308r billige Arbeitskra\u0308fte verwendet wird. zh7\u2192en \u671d\u95fb\u5929\u4e0b,\u73b0\u5728\u53c8\u5230\u4e86\u6211\u4eec\u7684\u4e92\u52a8 \u65f6\u95f4\u5566. Morning News, now it\u2019s our inter-\nactive time.\n(From Google GNMT): North Korea heard the world, and now to our interaction time friends.\nLanguage Pair Source NMT Google Translate (2016/10/15) br 7\u2192en Face a\u0300s deciso\u0303es de Nice, t\u0131\u0301nhamos de ter em conta esta situac\u0327a\u0303o e de adotar uma metodologia que contemplasse as transformac\u0327o\u0303es necessa\u0301rias. Faced with the Nice decisions, we had to take this situation into account and adopt a methodology that would address the necessary changes. Nice view of the decisions we had to take account of this situation and adopt a methodology that took into consideration the necessary changes. fr 7\u2192br Une information pre\u0301sente\u0301e au moment opportun signifie la transparence, laquelle cre\u0301e la confiance et e\u0301vite a\u0300 l\u2019entreprise de subir des pertes. A informac\u0327a\u0303o apresentada no momento oportuno significa transpare\u0302ncia, que cria confianc\u0327a e evita que a empresa sofra perdas. Informac\u0327a\u0303o apresentada em uma transpare\u0302ncia meio oportuna, que cria confianc\u0327a e evita a empresa a sofrer perdas."}, {"heading": "B Online System Parameters", "text": "All systems were trained with 4 LSTM layers, size of word embedding vectors was 500, dropout was set to 0.3 and we used bidirectional RNN (BRNN). Column Guided Alignment indicates wether the network was trained with guided alignments and on which epoch the feature was stopped."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "Demoed at NIPS 2014: http://lisa.iro.umontreal.ca/mt-demo/", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Statistical post-editing for a statistical mt system", "author": ["Yanjun Ma", "Josef van Genabith"], "venue": "In MT Summit,", "citeRegEx": "B\u00e9chara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2011}, {"title": "Qualitative analysis of post-editing for high quality machine translation. MT Summit XIII: the Thirteenth Machine Translation Summit [organized", "author": ["Blain et al.2011] Fr\u00e9d\u00e9ric Blain", "Jean Senellart", "Holger Schwenk", "Mirko Plitt", "Johann Roturier"], "venue": null, "citeRegEx": "Blain et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blain et al\\.", "year": 2011}, {"title": "Results of the wmt16 metrics shared task", "author": ["Bojar et al.2016] Ond\u0159ej Bojar", "Yvette Graham", "Amir Kamran", "Milo\u0161 Stanojevi\u0107"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2016}, {"title": "Large language models in machine translation", "author": ["Ashok C Popat", "Peng Xu", "Franz J Och", "Jeffrey Dean"], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Translation project adaptation for mt-enhanced computer assisted translation", "author": ["Nicola Bertoldi", "Marcello Federico", "Holger Schwenk", "Loic Barrault", "Chrstophe Servan"], "venue": "Machine Translation,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Multiun v2: Un documents with multilingual alignments", "author": ["Chen", "Eisele2012] Yu Chen", "Andreas Eisele"], "venue": "In LREC,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Guided alignment training for topic-aware neural machine translation. CoRR, abs/1607.01628v1", "author": ["Chen et al.2016] Wenhu Chen", "Evgeny Matusov", "Shahram Khadivi", "Jan-Thorsten Peter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.2007", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation. CoRR, abs/1603.06147", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Statistical Post-Editing on SYSTRAN\u2019s Rule-Based Translation System", "author": ["Dugast et al.2007] Lo\u0131\u0308c Dugast", "Jean Senellart", "Philipp Koehn"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Dugast et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dugast et al\\.", "year": 2007}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu et al.2016] Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "HueiChi Lin", "Fethi Bougares", "Holger Schwenk andYoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Pointing the unknown words", "author": ["Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Junczys-Dowmunt", "Roman Grundkiewicz"], "venue": null, "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "author": ["T. Dwojak", "H. Hoang"], "venue": null, "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Sequence-level knowledge distillation", "author": ["Kim", "Rush2016] Yoon Kim", "Alexander M. Rush"], "venue": "CoRR, abs/1606.07947", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models. CoRR, abs/1604.00788", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Online learning for interactive statistical machine translation", "author": ["Ismael Garc\u0131\u0301a-Varea", "Francisco Casacuberta"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North", "citeRegEx": "Ortiz.Mart\u0131\u0301nez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ortiz.Mart\u0131\u0301nez et al\\.", "year": 2010}, {"title": "Compression of neural machine translation models via pruning", "author": ["See et al.2016] Abigail See", "Minh-Thang Luong", "Christopher D Manning"], "venue": "In the proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning", "citeRegEx": "See et al\\.,? \\Q2016\\E", "shortCiteRegEx": "See et al\\.", "year": 2016}, {"title": "Linguistic input features improve neural machine translation. CoRR, abs/1606.02892", "author": ["Sennrich", "Haddow2016] Rico Sennrich", "Barry Haddow"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Controlling politeness in neural machine translation via side constraints", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Statistical phrase-based postediting", "author": ["Simard et al.2007] Michel Simard", "Cyril Goutte", "Pierre Isabelle"], "venue": "Proceedings of NAACL", "citeRegEx": "Simard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "The jrc-acquis: A multilingual aligned parallel corpus with 20+", "author": ["Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "D\u00e1niel Varga"], "venue": null, "citeRegEx": "Steinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann"], "venue": "In LREC,", "citeRegEx": "Tiedemann.,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation. Transactions of the Association for Computational Linguistics, 4:371\u2013383", "author": ["Zhou et al.2016] Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Since the first online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies.", "startOffset": 81, "endOffset": 104}, {"referenceID": 20, "context": "With its root on a number of established open-source projects such as Andrej Karpathy\u2019s char-rnn,3 Wojciech Zaremba\u2019s standard long short-term memory (LSTM)4 and the rnn library from Element-Research,5 the framework provides a solid NMT basis consisting of LSTM, as the recurrent module and faithful reimplementations of global-general-attention model and input-feeding at each time-step of the RNN decoder as described by Luong et al. (2015).", "startOffset": 423, "endOffset": 443}, {"referenceID": 19, "context": "Also available opensource corpora are domain specific: Europarl (Koehn, 2005), JRC (Steinberger et al.", "startOffset": 64, "endOffset": 77}, {"referenceID": 29, "context": "Also available opensource corpora are domain specific: Europarl (Koehn, 2005), JRC (Steinberger et al., 2006) or MultiUN (Chen and Eisele, 2012) are legal texts, ted talk are scientific presentations, open subtitles (Tiedemann, 2012) are colloquial, etc.", "startOffset": 83, "endOffset": 109}, {"referenceID": 30, "context": ", 2006) or MultiUN (Chen and Eisele, 2012) are legal texts, ted talk are scientific presentations, open subtitles (Tiedemann, 2012) are colloquial, etc.", "startOffset": 114, "endOffset": 131}, {"referenceID": 15, "context": "In Junczys-Dowmunt et al. (2016), authors mention using corpus of 5M sentences and training of 1.", "startOffset": 3, "endOffset": 33}, {"referenceID": 15, "context": "In Junczys-Dowmunt et al. (2016), authors mention using corpus of 5M sentences and training of 1.2M batches each having 40 sentences \u2013 meaning basically that each sentence of the full corpus is presented 10 times to the training. In Wu et al. (2016), authors mention 2M steps of 128 examples for English\u2013French, for a corpus of 36M sentences, meaning about 7 iterations on the complete corpus.", "startOffset": 3, "endOffset": 250}, {"referenceID": 9, "context": "A number of previous work such as characterlevel (Chung et al., 2016), hybrid word-characterbased (Luong and Manning, 2016) and subwordlevel (Sennrich et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 15, "context": ", 2016) and (Junczys-Dowmunt et al., 2016), it does not have significant side effects (for instance generation of impossible words) and is not optimal to deal with actual word morphology - since the same suffix (josa in Korea) depending on the frequency of the word ending it is integrated with, will be splitted in multiple representations.", "startOffset": 12, "endOffset": 42}, {"referenceID": 25, "context": "The sub-word encoding algorithm Byte Pair Encoding (BPE) described by Sennrich et al. (2016b)", "startOffset": 70, "endOffset": 94}, {"referenceID": 11, "context": "fast align (Dyer et al., 2013) to automatically align source words to target words.", "startOffset": 11, "endOffset": 30}, {"referenceID": 6, "context": "We re-reimplemented Guided alignment strategy described in Chen et al. (2016). Guided alignment enforces the attention weights to be more like alignments in the traditional sense (e.", "startOffset": 59, "endOffset": 78}, {"referenceID": 25, "context": "Following the work of Sennrich et al. (2016a), we implemented a politeness feature in our NMT engine: a special token is added to each source sentence during training, where the token indicates the politeness mode observed in the target sentence.", "startOffset": 22, "endOffset": 46}, {"referenceID": 23, "context": "With Neural Machine Translation, a new notion of \u201cspecialization\u201d comes close to the concept of incremental translation as developed for statistical machine translation like (Ortiz-Mart\u0131\u0301nez et al., 2010).", "startOffset": 174, "endOffset": 204}, {"referenceID": 5, "context": "Adaptation proceeds incrementally when new in-domain data becomes available, generated by human translators while post-editing, which is similar to the Computer Aided Translation framework described in (Cettolo et al., 2014).", "startOffset": 202, "endOffset": 224}, {"referenceID": 28, "context": "Until recently, most of the APE approaches have been based on phrase-based SMT systems, either monolingual (MT target to human post-edition) (Simard et al., 2007) or source-", "startOffset": 141, "endOffset": 162}, {"referenceID": 1, "context": "aware (B\u00e9chara et al., 2011).", "startOffset": 6, "endOffset": 28}, {"referenceID": 10, "context": "For many years now SYSTRAN has been offering a hybrid Statistical Post-Editing (SPE) solution to enhance the translation provided by its rule-based MT system (RBMT) (Dugast et al., 2007).", "startOffset": 165, "endOffset": 186}, {"referenceID": 24, "context": "This approach has been proven efficient for the NMT tasks in See et al. (2016). Inspired by this work, we introduced similar pruning techniques in seq2seq-attn.", "startOffset": 61, "endOffset": 79}, {"referenceID": 3, "context": "Evaluation of machine translation has always been a challenge and subject to many papers and dedicated workshops (Bojar et al., 2016).", "startOffset": 113, "endOffset": 133}, {"referenceID": 2, "context": "research world and have shown good correlation with human evaluation, ad-hoc human evaluation or productivity analysis metrics are rather used in the industry (Blain et al., 2011).", "startOffset": 159, "endOffset": 179}, {"referenceID": 4, "context": "bing In 2007, Google already mentions using 2 trillion words in their language models for machine translation (Brants et al., 2007).", "startOffset": 110, "endOffset": 131}, {"referenceID": 13, "context": "As proposed by (G\u00fcl\u00e7ehre et al., 2015), we conduct experiments to integrate an n-gram language model estimated over a large dataset on our Neural MT system.", "startOffset": 15, "endOffset": 38}, {"referenceID": 31, "context": "Ensemble decoding has been verified as a practical technique to further improve the performance compared to a single Encoder-Decoder model (Sennrich et al., 2016b; Wu et al., 2016; Zhou et al., 2016).", "startOffset": 139, "endOffset": 199}, {"referenceID": 8, "context": "The improvement comes from the diversity of prediction from different neural network models, which are learned by random initialization seeds and shuffling of examples during training, or different optimization methods towards the development set(Cho et al., 2015).", "startOffset": 246, "endOffset": 264}, {"referenceID": 15, "context": "Also, (Junczys-Dowmunt et al., 2016) provides some methods to accelerate the training by choosing different checkpoints as the final models.", "startOffset": 6, "endOffset": 36}, {"referenceID": 22, "context": "word2vec (Mikolov et al., 2013)).", "startOffset": 9, "endOffset": 31}, {"referenceID": 12, "context": "In the recent literature (Gu et al., 2016; Gulcehre et al., 2016), researchers have attempted to directly model the unknown word handling within the attention and decoder networks.", "startOffset": 25, "endOffset": 65}, {"referenceID": 14, "context": "In the recent literature (Gu et al., 2016; Gulcehre et al., 2016), researchers have attempted to directly model the unknown word handling within the attention and decoder networks.", "startOffset": 25, "endOffset": 65}], "year": 2016, "abstractText": "Since the first online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work. Our ultimate goal is to share our expertise to build competitive production systems for \u201dgeneric\u201d translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems.", "creator": "LaTeX with hyperref package"}}}