{"id": "1610.05551", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Weighted Positive Binary Decision Diagrams for Exact Probabilistic Inference", "abstract": "recent work chiefly on weighted model gene counting has been very successfully applied to the problem of probabilistic inference in bayesian networks. the probability distribution is encoded into a boolean normal form and compiled identical to a target language, in order to represent local structure expressed among conditional probabilities more well efficiently. we frequently show that further improvements are possible, by exploiting the knowledge that is lost during the encoding phase iteration and incorporating it differently into a compiler inspired by satisfiability fuzzy modulo theories. constraints among variables are regularly used as a background theory, which allows us to optimize the shannon decomposition. we propose a wholly new language, called weighted positive binary decision diagrams, that reduces the cost of probabilistic inference by using this decomposition variant to induce an arithmetic circuit of reduced size.", "histories": [["v1", "Tue, 18 Oct 2016 11:58:28 GMT  (258kb)", "http://arxiv.org/abs/1610.05551v1", "30 pages"]], "COMMENTS": "30 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["giso h dal", "peter j f lucas"], "accepted": false, "id": "1610.05551"}, "pdf": {"name": "1610.05551.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["gdal@cs.ru.nl", "peterl@cs.ru.nl"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n05 55\n1v 1\n[ cs\n.A I]\n1 8\nRecent work on weighted model counting has been very successfully applied to the problem of probabilistic inference in Bayesian networks. The probability distribution is encoded into a Boolean normal form and compiled to a target language, in order to represent local structure expressed among conditional probabilities more efficiently. We show that further improvements are possible, by exploiting the knowledge that is lost during the encoding phase and incorporating it into a compiler inspired by Satisfiability Modulo Theories. Constraints among variables are used as a background theory, which allows us to optimize the Shannon decomposition. We propose a new language, called Weighted Positive Binary Decision Diagrams, that reduces the cost of probabilistic inference by using this decomposition variant to induce an arithmetic circuit of reduced size.\nKeywords: knowledge compilation, probabilistic inference, weighted model counting, Bayesian networks, binary decision diagrams."}, {"heading": "1. Introduction", "text": "Bayesian networks, BNs for short, have been a subject of great interest partly due to their contribution in solving real-life problems that involve uncertainty. Bayesian networks are probabilistic graphical models that represent joint probability distributions concisely by factoring them into conditional probabilities based on independence assumptions, in order to perform inference more efficiently [1]. Further representational and computational advances have been made by exploiting causal independence [2], as well as contextual independence [3] and determinism [4] expressed in conditional probability tables (CPTs). In order to capture these independencies local to CPTs, Bayesian networks have been represented as weighted Boolean formulas [5, 6], reducing inference to Weighted Model Counting (WMC), or weighted #SAT [5]. By representing a Bayesian network as Boolean formula f in conjunctive normal form (CNF), it can be compiled into a more concise normal form, or language, that renders inference a polytime operation in the size of the representation [7].\nA joint probability space with n Boolean variables has 2n interpretations. It is therefore necessary to be able to reason with sets of interpretations, requiring a symbolic representation [8]. Symbolic inference unifies the work of probabilistic inference and the extensive research done in the field of model checking, verification and satisfiability [9]. Ordered Binary Decision Diagrams (OBDDs) are based on Shannon decompositions and have been a very\nPreprint submitted to Elsevier October 19, 2016\ninfluential symbolic representation that reduces compilation to the problem of finding the variable ordering resulting in the optimal factoring.\nWe focus on the disadvantage of the approaches in recent work that encode a BN as an independent CNF f , motivated by the ability to use off-the-shelf SAT-solvers. While maintaining this ability, we exploit the knowledge that is lost during the encoding without requiring this independence.\nOur contributions are the following. We propose a weighted variant of OBDDs, called Weighted Positive Binary Decision Diagrams (WPBDDs), which are based on positive Shannon decompositions, allowing constraints in BNs to be represented more concisely. We use probabilities as symbolic edge weights, reducing the search space exponentially. An optimized compilation algorithm is introduced, inspired by the field of Satisfiability Modulo Theories (SMT), namely a lazy SMT-solver [10]. It provides the means to view constraints among variables in the encoding as background theory T which supports the SAT-solver, allowing constant time conditioning. We compile the conditional probability tables of a BN explicitly, but leave implicit the domain closure implied by the encoding. This approach allows us to remove by up to a third of the clauses in the encoding.\nA comparison is provided with the state-of-the-art CUDD (CU Decision Diagram [11]) and SDD (Sentential Decision Diagram [12]) compilers and we show that WPBDDs induce arithmetic circuits that are 60% reduced in size on average compared to a corresponding OBDD circuits at representing over 30 publicly available BNs. We show an inference speedup of over 2.6 times on average compared to Weighted Model Counting with OBDDs, and an average speedup between 5 and 1000 times compared to different implementations of the Junction Tree algorithm, making WPBDDs a valuable addition to the field of exact probabilistic inference.\nAfter preliminaries and background (Section 3), we introduce WPBDDs (Section 4). The process of using a BN to perform exact inference by WMC is explained (Section 5) in addition to its optimization (Section 6). We conclude with experimental results (Section 7), and review achievements (Section 8). First we summarize related work."}, {"heading": "2. Related Work", "text": "Probabilistic inference is a hard computational problem that can be achieved by marginalizing out non-evidence variables from a joint distribution, requiring an exponential number of operations in the worst case. Efforts toward efficient exact probabilistic inference attempt to find a concise factorization, e.g., BNs represent a joint probability distribution as a multiplicative factorization, exploiting independencies among variables. Further improvements to this factorization have been made by using propositional and first order logic [13, 14, 15]. Representing probability distributions as Boolean functions empowers probabilistic inference with the tools developed for VLSI-CAD design, by using symbolic representations and Boolean algebra for minimization. Symbolic Probabilistic Inference (SPI) [9] is a good example of this, which is currently more commonly referred to as inference by WMC or #SAT [16].\nA BN can be viewed as a constraint satisfaction problem (CSP) and translated to a satisfiability (SAT) instance in conjunctive normal form (CNF), a form commonly used in satisfiability solving. Various encodings of CSPs have been proposed, namely log, direct [17], order [18], compact order [19], log-support encoding [20], etc. In the context of BNs, a probability distribution can be considered a pseudo Boolean function f : {0, 1}n \u2192 R, with arity n, which can be uniquely written as an exponentially sized multi-linear polynomial\n2\n[21, 22]. Others have used the direct encoding [6], or a combination between the direct and order encoding [23], where a BN is viewed as a set of discrete real valued functions, where each function represents a distinct CPT.\nInference by WMC is motivated by linear time complexity in the size of the representation [24], where the common goal is to exploit local structure [25]. Choosing a representation or language to compile to is therefore a critical task, where one must deal with the balance between the functions a language can represent concisely versus its algorithmic properties. Initial attempts include probability trees [3, 26, 27] and recursive (factored) probability trees [28], which focus on concisely representing each CPT independently, allowing their usage in inference algorithms directly. Probabilistic Decision Graphs (PDG) have even shown that the smallest PDG is at least as small as the smallest Junction tree for the same distribution [29].\nCurrent WMC approaches to inference divide into search and compilation methods [30]. Typical search algorithms are based on DPLL-style SAT solvers that do an exhaustive run to count all satisfying models [23]. Recording SAT evaluation paths (i.e. resolution steps) as a compiled structure (e.g. an OBDD), yields one possible factoring. We refer to finding the optimal factoring given all variable orderings, as exact compilation.\nCompilation performance has been improved by clause learning [31], formula caching [32], bounding [33], and using canonical languages. Representational advances include symmetry detection [34, 35], support for causal independence [36] and using read-once functions[37].\nRepresentations relevant in the context of BN compilation are AND/OR Multi-Valued Decision Diagrams (AOMDD) [38], Sentential Decision Diagrams (SDD) [12], Zero-suppressed Binary decision diagrams (ZBDD) [21] and Ordered Binary Decision Diagrams (OBDD) [39], which view probabilities as auxiliary literals, resulting in an intractably large search space. Multi-Terminal BDDs [40] represent multi-valued functions, but would require too many terminal nodes considering the size of a probability distribution. Variants of Edge-Value BDDs [41, 42] focus on real valued functions. When multiple CPTs have probabilities in common, we lose the ability to distinguish from which CPT the probabilities originated. We therefore cannot determine on which variables they depend, resulting in an inconsistent model count with regard to the distribution. Our approach to maintain consistency is to represent probabilistic edges weights symbolically. This differentiates our approach from Multi-Terminal BDDs [40] and Edge-Value BDDs [41]. And unlike SDDs [43], we are not obligated to view probabilities as auxiliary literals, reducing the search space to a fraction of its former size. A common characteristic with ZBDDs, is the ability to represent mutual-exclusive constraints more concisely [44]. The intuitive difference is that ZBDD optimize only the positive cofactor, while we optimize both the positive and negative cofactor of decomposition nodes, a matter we will elaborate on in upcoming discussions."}, {"heading": "3. Preliminaries and Background", "text": "We provide here a description of what Bayesian networks are and introduce a running example (Section 3.1), show how to encode a BN onto the Boolean domain (Section 3.2), and describe an influential representation that will serve for comparison with ours (Section 3.3).\n3.1. Bayesian Networks\nProbabilistic inference is an important computational problem in Artificial Intelligence. A full joint probability distribution defined over n Boolean variables is of size O(2n). Finding\n3\nthe minimal representation of a function describing a probability distribution reduces memory and inference complexity, which is the motivation for this paper.\nA Bayesian Network (BN) is a graphical representation that is used to compactly represent a joint distribution as a product of factors, by taking advantage of conditional independence (CI). A BN is an directed acyclic graph (DAG) that models variables X as nodes, the dependencies among them as edges, and their joint probability distribution as\nP (X) = P (x1, ..., xn) =\nn\u220f\ni=1\nP (xi | pa(xi)),\nwhere P (xi | pa(xi)) represents the conditional probability of variable xi given its parents pa(xi). Conditional probability tables (CPTs) are associated with edges and capture the degree to which variables are related. BNs reduce the size of representing a probability distribution to O(n2k), where k is the maximum number of parents of any node.\nExample 1. Figure 1 shows BN B defined over variables X = {a, b} (Figure 1b), its CPTs (Figure 1c) and corresponding full joint probability distribution (Figure 1a).\nFigure 1b includes a factored form that can greatly be improved when CPTs exhibit local structure, which comes in two forms. Context-specific independence (CSI) is expressed when probabilities in a CPT show uniformity regardless of the value of one or more variables they have in common, with or without a certain context. Determinism is expressed when probabilities in a CPT are equal to 0 or 1, which can be used to simplify the Boolean formula that represents it.\nIn order to exploit more of the problem structure than a BN, we compile it to a target language that is more capable of doing so. The compilation process is not just about reformulating into a different language, it is also about finding the minimal representation given that language. The goal is to derive an arithmetic circuit corresponding to that representation with improved complexity compared to the standard factorization.\n3.2. Encoding Bayesian Networks\nIn order to exploit local structure, we encode BNs into conjunctive normal form (CNF), which is the most common representation used in satisfiability solving. It consists of a conjunction of clauses, where each clause is a disjunction of literals. A literal is a propositional Boolean variable or its negation. A Bayesian Network defined over variables X can be seen as a multi-linear function f : X \u2192 R. Using a weighted adaptation of the sparse or direct\n4\nencoding [6, 17, 45], we encode f into a Boolean function E(f) = f e by representing it as a weighted CNF:\nE(f) = f c \u2227 fm, (1)\nwhere constraint clauses f c support the mapping M(f) = fm that encodes probabilities and introduces a Boolean variable for each unique variable-value pair. Details are discussed below.\n3.2.1. Encoding Constraints\nThe mapping function M introduces for each x \u2208 X atoms A(x) = {x1, . . . , xn}, where xi signifies x being equal to its i\nth value. To maintain consistency among variables we add to f c an at-least-once (ALO) constraint clause for each x, to ensure x is assigned a value:\n(x1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xn) (2)\nAs values of a variable are mutually exclusive, we add to f c the following at-most-once (AMO) constraint clauses:\nn\u2227\ni=1\n xi =\u21d2 \u2227\nxj\u2208A(x)\\{xi}\nxj\n\n =\nn\u2227\ni=1\nn\u2227\nj=i+1\n(xi \u2228 xj), (3)\nwhere xi indicates the negation of xi.\n3.2.2. Encoding CPTs\nThe BN\u2019s factored form is preserved by using a weighted adaptation of the direct encoding. The mapping function M adds a clause for every probability P (x|U), where x depends on variables U = {u1, . . . , ur}:\n(x \u2227 u1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 ur \u21d2 \u03c9i) = (x \u2228 u1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 ur \u2228 \u03c9i), (4)\nwhich we henceforth shall view as a weighted clause (x\u2228u1\u2228 \u00b7 \u00b7 \u00b7 \u2228ur) : \u03c9i, where its symbolic weight \u03c9i represents probability P (x|U). We introduce a symbolic weight for every unique probability per CPT, and thus allow multiple models to be associated with it:\nI1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 Is, (5)\nwhere each Ij has weight \u03c9i and forms the implicate of model m associated with P (x|U ), i.e., Ij = \u03c9i : (x\u2227u\n1 \u2227 . . .\u2227ur), where the variables x\u222aU have been appropriately mapped by M, i.e., x \u2208 A(x) and each uk \u2208 A(uk).\nExample 2. Assume a BN as given in Example 1. The following clauses form f c:\nVariable ALO (Eq. 2) AMO (Eq. 3)\na (a1 \u2228 a2) (a1 \u2228 a2) b (b1 \u2228 b2 \u2228 b3) (b1 \u2228 b2) \u2227 (b1 \u2228 b3) \u2227 (b2 \u2228 b3)\nThe variables that make up the search space during compilation therefore are A({a, b}) = {a1, a2, b1, b2, b3}. We encode equal probabilities P (x|U) as unique symbolic weights per CPT:\n5\nP (a1) P (a2)\n\u03c91 \u03c91\na P (b1|a) P (b2|a) P (b3|a)\n1 \u03c92 \u03c92 \u03c93 2 \u03c92 \u03c92 \u03c93\nIn accordance with Equations 4 and 5, fm consists of the following clauses, accompanied by their respective symbolic weights:\n(a1) : \u03c91 \u2227 (a2) : \u03c92 \u2227 (a1 \u2228 b1) : \u03c92 \u2227 (a1 \u2228 b2) : \u03c92 \u2227 (a1 \u2228 b1) : \u03c92 \u2227 (a1 \u2228 b2) : \u03c92 \u2227 (a1 \u2228 b3) : \u03c93 \u2227 (a2 \u2228 b3) : \u03c93.\n3.3. Ordered Binary Decision Diagrams\nA Boolean function f defined over a set of variables X is a function that maps each complete assignment of its variables to either true (1) or false (0). The conditioning of f on instantiated variable xi is defined as the projection:\nf|xi\u2190b(x1, . . . , xn) = f(x1, . . . , xi\u22121, b, xi+1, . . . , xn), (6)\nwith b \u2208 {0, 1}. We will use shorthand notations f|xi and f|xi for f|xi\u21901 and f|xi\u21900, respectively. Shannon\u2019s theorem is used to find a more compact way to represent f by factoring it.\nTheorem 1. [46] Shannon\u2019s expansion allows a Boolean function f : {0, 1}n \u2192 {0, 1} defined over variables X, to be written in terms of its inputs:\nf = x \u2227 f|x \u2228 x \u2227 f|x,\nwith x \u2208 X and where f|x is called the positive cofactor of f with respect to x, and f|x the negative cofactor. Applying Shannon\u2019s theorem is known as an (additive) decomposition step. The decomposition of f is defined as the recursive application of Shannon\u2019s expansion theorem to cofactors, removing one variable at a time, until no variables are left.\nNote that all proofs of theorems, lemmas, etc., can be found in the Appendix. The Shannon decomposition is key to one of the most influential representations in Artificial Intelligence (AI), namely Ordered Binary Decision Diagrams (OBDD)[47].\nDefinition 1. [48] A Binary Decision Diagram (BDD) represents Boolean function f defined over variables X as a rooted, directed acyclic graph, where each node v represents a Shannon decomposition on variable var(v) \u2208 X. A BDD is ordered (OBDD) if variables appear in the same order on all paths from the root. It is a canonical representation if it is reduced by applying the following rules:\n1. Merge rule: All isomorphic subgraphs are merged.\n2. Delete rule: All nodes are removed whose children are isomorphic.\nOne can compile the described encoding of BNs to OBDDs to perform inference by WMC. Although other languages have been used in this context, we focus more on OBDDs as it is commonly used in comparisons with related work.\n6"}, {"heading": "4. Weighted Positive Binary Decision Diagrams", "text": "Consider variable x and its corresponding mapped atoms A(x) = {x1, x2}. Decision diagrams that are based on Shannon decompositions produce an unnecessarily large arithmetic circuit by redundantly representing constraints f c provided as part of encoding E . They also do not take advantage of the symmetric relation x1 = x2 and x1 = x2 in the presence of ALO constraints. To ameliorate this, we propose a new canonical language called Weighted Positive Binary Decision Diagrams (WPBDD), which are based on positive Shannon decompositions and implicit conditioning. We will elaborate on these concepts through an intermediate unweighted variant of WPBDDs (PBDD).\n4.1. Explicit and Implicit Conditioning\nWhen encoded function f e contains a unit clause xi (a clause consisting of a single literal), it can be simplified using unit propagation:\n1. Every clause containing xi is removed, excluding the unit clause. 2. Literal xi is removed from every clause containing it.\nWhen conditioning f e on literal xi we are guaranteed, due to constraints f c, to obtain unit clauses containing negated literals xj, with xj \u2208 A(x)\\xi (i.e., if x is equal to its i th value, it cannot be equal to its jth value).\nExample 3. We will show by example what unit clauses are obtained by conditioning on positive literals. Consider the constraint clauses provided by Example 2, regarding only variable b:\nf c = (b1 \u2228 b2 \u2228 b3)\u2227 (b1 \u2228 b2)\u2227 (b1 \u2228 b3)\u2227 (b2 \u2228 b3)\nAccording to Shannon\u2019s expansion the following holds:\nf c = b1 \u2227 f c |b1 \u2228 b1 \u2227 f c |b1\nNow specifically look at the unit clauses that result from conditioning on positive literal b1, i.e., instantiating b1 and performing unit propagation.\nf c|b1 = (1 \u2228 b2 \u2228 b3) \u2227 (0 \u2228 b2) \u2227 (0 \u2228 b3) \u2227 (b2 \u2228 b3)\n= (1) \u2227 (b2) \u2227 (b3) \u2227 (b2 \u2228 b3) = (1) \u2227 (b2) \u2227 (b3) = (b2) \u2227 (b3).\nThus, conditioning on bi will result in unit clauses containing negated literals bj , with bj \u2208 A(b) \\ {bi}.\nWe distinguish between two types of conditioning based on the previous observation, and describe them in the following definition.\n7\nDefinition 2. Let f e be an encoded representation of function f , given encoding E, where f is defined over variables X. We define f e\u2016xi as the conditioning of f e on literals {xi, x1, . . . , xi\u22121, xi+1, . . . , xn} in any order, i.e., as the explicit conditioning of f e on literal xi \u2208 A(x), and its implicit conditioning on literals xj \u2208 A(x)\\xi, with x \u2208 X:\nf e\u2016xi = f e |xi,x1,...,xi\u22121,xi+1,...,xn ,\nIt follows from Definition 2 and the constraints provided by encoding E , that the relation between f e\u2016xi and f e |xi is given by the following equality:\nf e|xi =\n  \u2227\nxj\u2208A(x)\\xi\nxj\n\n \u2227 f e\u2016xi . (7)\nImplicit conditioning on unit clauses takes advantage of deterministic behavior expressed in f c, while other representations would have to explicitly condition these unit clauses out. The advantage is two-fold. The size of the encoding can be reduced by removing constraint clauses f c generated by Equation 2 and 3, and integrating them directly into the compilation process through theory T . As will be shown later, by separating constraint clauses from f e as theory T allows them to be conditioned in constant time, as opposed to quadratic time. Secondly, the size of the compiled structure is reduced by not having to represent redundant constraint information with our variant on the Shannon expansion, introduced in the following section.\n4.2. Positive Shannon Decomposition\nWe propose positive Shannon decompositions that use background knowledge to improve upon Shannon decompositions by combining it with implicit conditioning.\nLemma 1. A positive Shannon expansion allows an encoded Boolean function E(f) = f e, where f is defined over X, to be written in terms of its inputs:\nf e = f c \u2227 (\nxi \u2227 f e \u2016xi \u2228 f e|xi\n)\n,\nwhere f e = f c \u2227 fm, and xi \u2208 A(x), with x \u2208 X. The positive cofactor f e \u2016xi incorporates implicit conditioning (Definition 2). The negative cofactor f e|xi and the decomposition of f e are as defined by Theorem 1.\nAs intuition might confirm, logical representations will grow by the reintroduction of constraints at every expansion. We introduce a reduced expansion by removing constraint clauses f c that introduces additional models, in turn allowing us to find more concise representations. These models can easily be removed by a post decomposition conjoin with f c.\nTheorem 2. A reduced positive Shannon expansion allows an encoded Boolean function E(f) = f e, where f is defined over X, to be written in terms of its inputs under constraints f c:\nf e |= xi \u2227 f e \u2016xi \u2228 f e|xi,\nwhere f e = f c \u2227 fm, and xi \u2208 A(x), with x \u2208 X. Cofactors and decomposition are as defined by Lemma 1.\n8\nBy using the reduced form of the expansion, we are able represent constraints more concisely in corresponding logical circuits (Figure 2), as well as in the soon to be introduced representation that utilizes it.\nUsing encoding E we can infer that the delete rule used to reduce OBDDs will never be applied to constraint clauses f c. OBDDs are therefore not capable of capturing local structure along one dimension. To ameliorate this, we introduce positive OBDDs (PBDD) as an unweighted intermediate representation, that are based on the positive Shannon decomposition and substitutes the delete rule with the collapse rule, which as opposed to deleting literals, applies the distributive law to involved literals in the induced logical circuit.\nDefinition 3. A positive OBDD (PBDD) represents Boolean function E(f) = f e, where f is defined over variables X, as an ordered BDD where each node v represents a positive Shannon decomposition on variable var(v) \u2208 A(X). It is a canonical representation if reduced by applying the following rules:\n1. Merge rule: All isomorphic subgraphs are merged.\n2. Collapse rule: remove direct descendant u of node v iff f\u2016xi = f\u2016xj , where var(v) = xi and var(u) = xj, with xi, xj \u2208 A(x) and x \u2208 X.\nA function essentially depends on a variable if it appears in its prime implicate. The variable set S, on which f e essentially depends, is called the support of f e. We will use this support set to identify to what variables the collapsed rule has been applied in order to produce its corresponding arithmetic circuit, a trick similarly utilized with Zero-Suppressed BDDs (ZBDD) [44]. Note that a Boolean function E(f) = f e, where f is defined over variables X, essentially depends on A(X), because f c is a prime implicate that mentions all A(X). The canonical property of PBDDs follows from the fact that a binary tree can be reconstructed from a PBDD and its support set, by apply its reduction rules reversely.\nFigure 3 shows the difference in representational size between an OBDD, a ZBDD and a PBDD representing the same function with constraints on A(x) = {x1, x2}, where g is a Boolean function that does not essentially depend on literals {x1, x2}. The positive Shannon decomposition does not only reduce the size the corresponding logical circuit, it also reduces the size of the representation. More generally, OBDDs require an exponential number of nodes in the product of each constraint variable\u2019s dimension, where ZBDDs require a linear number, and where PBDDs require only 1 node.\n9\nFigure 3 shows that there are functions where the corresponding minimal PBDD and OBDD differ exponentially in size. Not represented in the figure, the collapse rule additionally removes the nodes that share the same positive cofactor with their parents (Figure 4).\nThe semantics of nodes whose child has been removed changes. A missing node, inferable by the support set S and variable order, indicates the application of the collapse rule, i.e., the distributive law on xi and xj . There is no ambiguity regarding the delete rule as it can never be applied on A(X) due to constraint clauses f c. Note that the delete rule can however be applied in case one optimizes Boolean variables of the BN by representing them with only one literal in f e, as opposed to two. This will delete literals from the induced circuit and result in an inconsistent model count with regard to the probability distribution. It is precisely for this reason why the collapse rule uses the distributive law on involved literals to simplify the induced circuit, as apposed to deleting them from it. The combination of the merge and collapse rule allow for more fine grained control in exploiting CSI, because it allows independence given a subset of the values to be expressed more efficiently when dealing with multi-valued variables.\nProposition 1. An OBDD representing Boolean function f and an PBDD representing E(f) induce isomorphic logical circuits under Boolean identity, given an appropriate ordering.\nProposition 2. Given an ordering on A(X), the size of PBDD \u03d5 is less than the size of OBDD \u03c8 when both representing E(f) = f e, where f is defined over variables X.\n10\n4.3. Adding Probabilities as Weights\nEncoding E represents a BN as a weighted propositional formula. We extend PBDDs to weighted PBDDs (WPBDD) using an intuitive scheme, taking advantage of the fact that probabilities are fully implied by the variables in the BN. Traditionally, an empty clause would result in a contradiction, i.e., the instantiation is unsatisfiable. We implicitly assign weight \u03c9 of empty clause c to the edge it is associated with, and remove c from the expression. When multiple empty clauses are associated with an edge, we simply assign the conjunction (multiplication) of their weights to the edge. To maintain canonicity, we only assign weights on the side of the positive cofactor.\nThe collapse rule previously introduced can easily be extended for the non-binary and weighted case as shown in Figure 5.\nDefinition 4. A weighted PBDD (WPBDD) representing Boolean function E(f) = f e, where f is defined over variables X, is a PBDD where each node v is a tuple \u3008xi,W, f\ne \u2016xi , f e|xi\u3009 that\nrepresents a weighted reduced positive Shannon expansion:\nf e |= xi \u2227 (W \u2227 f e \u2016xi ) \u2228 f|xi ,\nwhere xi \u2208 A(X), and W is a conjunction of weights \u03c9i that correspond to f e \u2016xi . Positive and negative cofactors are as described by Lemma 1. It is a canonical representation if reduced by applying the following rules:\n1. Merge rule: All isomorphic subgraphs are merged.\n2. Collapse rule: remove direct descendant u of node v iff W \u2227 f\u2016xi = W \u2227 f\u2016xj , where var(v) = xi and var(u) = xj, with xi, xj \u2208 A(x) and x \u2208 X.\nExample 4. Consider the CPTs from Example 1, where per CPT, equal probabilities are represented by unique symbolic weights \u03c9i.\nP (a1) P (a2)\n\u03c91 \u03c91\na P (b1|a) P (b2|a) P (b3|a)\n1 \u03c92 \u03c92 \u03c93 2 \u03c92 \u03c92 \u03c93\nFigure 6 shows the minimization of a WPBDD using variable ordering a1 < a2 < b1 < b2 < b3, that represents the BN with 3 probabilities instead of 8. Figure 7 shows the comparison of this WPBDD with an OBDD representing the same function, given variable ordering a1 < a2 < \u03c91 < b1 < b2 < b3 < \u03c92 < \u03c93, which results in a minimal OBDD that obeys the partial ordering used for the WPBDD.\n11"}, {"heading": "5. Symbolic Inference", "text": "We perform Bayesian inference through a three phase process: encoding, compiling and model counting:\nComposition Input Output\nEncoder Encoding E f Theory T + fe Compiler T -solver + SAT T + fe WPBDD \u03d5 Counter T -solver + WMC T + \u03d5 P (x|e)\nA BN represented by f is first encoded by the encoder as Boolean function f e using encoding E as defined in Section 3.2. The encoder also provides background theory T representing f c, i.e., the constraints among variables that support mapping M.\nThe compiler uses a lazy SMT-solver to record evaluation paths and as a WPBDD, given f e and theory T . A lazy SMT-solver combines a SAT-solver with a theory-solver (or T -solver) for some theory T . Traditionally, the role of a theory-solver is to purely report back on the satisfiability of T . We have extended the theory-solver to provide more information in order to support implicit conditioning in the SAT-solver.\n12\nThe counter computes the probability of x given evidence e, by translating the provided WPBDD into an arithmetic circuit and using the extended capabilities of the T -solver to properly instantiate the variables.\n5.1. Compilation\nThe order of decomposition greatly influences representation size. Compilation therefore reduces to finding the optimal variable ordering. Satisfiability (SAT) is key during compilation. Normally, when CNF f contains an empty clause we derive a contradiction. Note that if all contradicting clauses are weighted, we supersede the contradiction and introduce their weights into our representation at the corresponding edge. In order to obtain a minimal BDD representation, the search space of all variable instantiations is traversed with a DPLL-style algorithm, in order to find partial instantiations that describe equal (sub)functions [5], as depicted in Figure 8.\nAlgorithm 1 shows a Depth-first search/dynamic programming (DFS+DP) approach that uses a lazy SMT-solver (solver) to compile f e into a WPBDD, given some ordering on the variables (ordering), where \u22a4 and \u22a5 denote the terminal nodes representing true and false, respectively.\nThe lazy SMT-solver is unique in the sense that there exists a link beyond satisfiability feedback between the T -solver and the SAT-solver (solver.theory and solver.sat, re-\n13\nAlgorithm 1 Compiler\n1 struct node { 2 literal l; 3 node *t, *e; 4 set W; 5 }; 6 7 enum satisfy_t { 8 satisfiable = 0, 9 unsatisfied = 1,\n10 unsatisfiable = 3 11 }; 12 13 satisfy_t solver::condition(literal l){ 14 solver.theory.condition(l); 15 solver.sat.condition(l); 16 solver.sat.condition( 17 solver.theory.unit_clauses()); 18 19 return solver.theory.state() | 20 solver.sat.state(); 21 } 22 23 node* condition(literal l) 24 solver.condition(l); 25 if(not negated(l)) 26 n->W += solver.sat.weights(); 27 28 node *n; 29 switch(solver.state()){ 30 case unsatisfied: 31 n = compile(new node,i+1); 32 break;\n33 case satisfiable: 34 n = \u22a4; 35 break; 36 case unsatisfiable: 37 n = \u22a5; 38 break; 39 } 40 solver.undo(); 41 42 return n; 43 } 44 45 node* compile(n,i=0){ 46 n->l = ordering[i]; 47 n->t = condition(n->l); 48 n->e = condition(not n->l); 49 50 apply_collapse_rule(n); 51 apply_merge_rule(n); 52 53 return n; 54 } 55 56 wpbdd* compiler(T ,f\ne){ 57 solver.theory.init(T ); 58 solver.sat.init(fm); 59 60 return compile(new node); 61 }\nspectively). The traditional role of the theory-solver is to solely report back on satisfiability. To implement implicit conditioning, we have extended it to also provide its unit clauses, that are used by the SAT-solver to further condition f e. This connection is possible because f e and theory T both essentially depend on the same variables. The SMT-solver reports f e to be satisfiable only when both the theory and SAT-solver agree on this. Storing intermediate states as an undo mechanism for the solver is infeasible, thus it has the ability to dynamically undo any conditioning (solver.undo).\nThe compiler uses the satisfiability state of the SMT-solver (solver.state) to build the WPBDD and achieves a canonical form by applying to each subfunction the merge rule (as describe by [49]) and collapse rule (apply merge rule(n) and apply collapse rule(n), respectively).\n5.2. Inference by Weighted Model Counting\nIn order to perform inference by WMC, a WPBDD must be converted into a logical (refactored) form using Definition 4. Recall that a missing variable along a path implies the use of the distributive law, identifiable by using the variable ordering and support set S. The logical form can easily be translated into an arithmetic circuit according to Table 1. Note that x\u2228y reduces to x+y, when x and y originate from the same dimension, i.e., x, y \u2208 A(z), with z \u2208 X.\n14\nOne of the reasons for using the positive Shannon decomposition is to prevent constraints among variables to be represented twice in the described process of symbolic inference: once as part of the compiled representation, and again when we substitute literals with their appropriate weight in order to perform model counting. During this later phase, theory T is used to prohibit inconsistent network instantiations, preventing a state where multiple values are assigned to one variable. To perform inference, all weights \u03c9i are set to the probability they represent, and all other literals are set to 1. By conditioning T on the evidence using the theory-solver, literals are found that conflict with the evidence in the form of unit clauses. These must be set to 0.\nExample 5. Let f c = (a1 \u2228 a2) \u2227 (a1 \u2228 a2) represent the constraint clauses for variable a of Example 1. When computing P (a1) we condition f c on evidence a1 yielding f c a1\n= a2, thus evidence a1 implies a1 = 1 (true) and a2 = 0 (false). This process is shown in Figure 9.\n6. Optimizations\n6.1. Encoding\nThe constraint clauses f c of encoded function E(f) = f e, where f is defined over variables X, introduce predictable symmetries into the encoding (demonstrated by Example 3). By incorporating these constraints directly into the compilation process through theory T , the constraint clauses f c generated by Equation 2 and 3 become obsolete and can thus be removed from f e. This reduces the number of clauses in the encoding by:\n15\n\u2211 x\u2208X 1\n\ufe38\ufe37\ufe37\ufe38\nALO clause\n+\n( n\n2\n)\n\ufe38\ufe37\ufe37 \ufe38\nAMO clauses\n,\nwhere we sum over every x \u2208 X, with n the domain size of x, i.e., |A(x)|. Both the atleast-once (AMO) and at-most-once (AMO) clauses contribute to reducing the number of clauses in the encoding to the number of probabilities in the CPTs of the BN. This gives an advantage over related work using the direct encoding, as it puts less strain on the SAT-solver by requiring it to only process M(f).\n6.2. Compiler\nThe compiler uses a lazy SMT-solver, consisting of a theory- and SAT-solver. In the way we have build the compiler, it naturally allows for optimization by providing the ability to substitute the SAT-solver with any other state-of-the-art solver. We have optimized the SMT-solver by using the structure expressed by the encoding, and the fact that theory T and f e are defined over the same variables.\nWe have optimized the theory-solver such that it now supports constant time conditioning of constraint clauses, which can take up one third of the encoding as shown by experimental results later. All one needs is the function V : A(X) \u2192 X that maps literal xi back to x, where xi \u2208 A(x). For each x we maintain a counter that is initialized to the domain size of x, i.e., |A(x)|. Conditioning on negated literal xi will decrease the counter corresponding to x by 1. If the counter reaches 0, we derive contradiction (i.e., unsatisfiable as x has no value). Conditioning on positive literal xi will cause any following conditioning on xj \u2208 A(x)\\xi as redundant (i.e., x can only have one value). The SAT-solver will be bypassed completely as a result and the compiler will continue with the next variable in the ordering, saving additional time.\nWe have simplified the SAT-solver considerably by taking advantage of the structure of M(f). We use an one-to-many map Q : A(X) \u2192 O from literal l \u2208 A(X) to the clauses O it occurs in, i.e., Q(l) = {c1, . . . , cn}. For each clause ci, we maintain if it is satisfied with a counter, initialized to the number of literals it consists of. When conditioning on positive literal l we decrease counters associated with Q(l) by 1. The clauses of which the counters have reached 0 are marked as satisfied, and their corresponding weights are set aside to be introduced into the representation later. Conditioning on negated literal l will mark clauses Q(l) as satisfied. This is possible because M(f) only contains negated literals, and we are able to assume that Q(l) \u2229Q(l) = \u2205. When all clauses in fm are satisfied, we derive f to be satisfiable given the evaluated instantiation. In combination with the SAT-solver being bypassed in the case of the previously mentioned redundant variables, this allows for conditioning in linear time, in the number of clauses that l occurs in."}, {"heading": "7. Experimental Results", "text": "We have developed a tool chain, that can encode a Bayesian network into CNF, compile it to various different representations, and perform inference using the arithmetic circuits they induce. Using over 30 publicly available Bayesian networks, we provide empirical results on encoding size, representation size and compilation time comparisons to other well known\n16\nrepresentations and compilers. We also compare the time it takes to perform exact inference compared to the classic Junction tree algorithm.\nStatistics related to the encoding are shown in Table 2, which include Example 1 as BN example. The number of clauses produced by encoding E is equal to |f c|+ |P |, for constraint clauses f c and mappingM, disregarding determinism. We can reduce the size of the encoding by up to a third, by moving constraint clauses to the theory solver, additionally allowing us to perform constant time conditioning on them. We can also see that the majority of the BNs will benefit greatly by the techniques in this paper by looking at the amount of equal and deterministic probabilities they contain.\nWe have developed a compiler that supports compilation of Bayesian networks to OBDDs\n17\nand ZBDDs (using the CUDD1 3.0.0 library), SDDs [43] (using the SDD2 1.1.1 library), and WPBDDs. Each decision diagram is created with the same ordering, within the same framework, i.e., doing the same amount of work in the same order. Quite literally, the only differences are the inserted appropriate function calls to different libraries, and the output representation. This will have comparative implications to whether a particular compilation will succeed given resource constraints as time and memory. At the same time, we did not tune the algorithms to ensure that our algorithm stood out favorably, ensuring fair comparison.\nThe framework divides the compilation process in two for efficiency. The logical representation of each CPT is first compiled separately, and then conjoined to represent the full distribution. The later is essentail for producing a logical circuit with a consistent model count in order to perform for inference. All results regarding the WPBDD compiler have been produced with a hybrid approach, where CPTs are compiled in a topdown fashion, and conjoined bottom-up. We found that a fully bottomup approach is only favorable when BNs have large CPTs like mildew, where we got a 5x speedup compared to the topdown approach. In practice, large CPTs are usually avoid as they increase the complexity of inference.\nMany strategies were explored in order to find a good variable ordering for each BN. Using simulated annealing in combination with an upper bound function yielded best results by far. The variable orderings were used to induce orderings based on literals, by saying that literal x1 must come before y1 if variable x comes before y in the variable ordering, where x, y \u2208 X, x1 \u2208 A(x) and y1 \u2208 A(y). The weights are introduced into the ordering as literals precisely when the WPBDD would introduce them as edge weights.\nTables 3 and 4 show a comparative study between representation size and compilation time of supported representations, where WPBDDnc is a WPBDD where the collapse rule has not been applied, in order to show the impact that this rule has. SDDs and SDDrs are compiled using a balanced and right-aligned vtree ordering, respectively. A left-to-right traversal of these vtrees produces the ordering also used for the other representations. Table 3 indicates compilation failure due to a 24Gb RAM memory limitation or an one hour time limit by symbols - and *, respectively. The progress each failed compilation made before is indicated in Table 4. All experiments were run using an Intel Xeon E5620 CPU.\nTable 3 shows a size comparison of each representation by the only commom size metricperators in the logical circuit that each decision diagram induces. We can see that WPBDDs have 60% less logical operators than the corresponding OBDDs on average at both stages of compilation, reducing inference time and system requirements considerably. Also, a WPBDD is reduced by 15% on average by applying the collapse rule when compiling CPTs, and 6% reduction on average with fully compiled networks. This statistic is fully determined by the amount of local structure in the BN and the ordering used during compilation, and can greatly be improved upon utilizing techniques as dynamic compilation in the future.\nObserve that there is a close relation between then size of OBDDs and SDDrs, as mentioned in [43]. We can see that the size of each SDDr is marginally smaller than its corresponding OBDD in Table 3. We assume that this is because SDDs have multi-valued logical-OR operators, which allow for more concise representations. SDDs consist of binary logical-AND, and n-ary logical-OR operators. We have included OR operators in size computations as n\u22121 binary logical-OR operators.\n1Available at http://vlsi.colorado.edu/\u223cfabio/ 2Available at http://reasoning.cs.ucla.edu/sdd/\n18\n(symbols - and * indicate compilation failure due to memory or an one hour time limitation, respectively)\n19\n(In case of compilation failure, the percentage of successfully conjoined/processed\nvariables is shown, of which the reason is documented in Table 3)\n20\nIn order to evaluate the WMC approach to exact inference with other methods, we have chosen to compare to the Junction tree algorithm using the publicly available Dlib3 C++ library (version 18.18), and the HUGIN4 library (through the C++ API version 8.4). We exhaustively go through all possible probabilistic queries. Table 5 and 6 show how much time T spent by each method on an identical set of queries. We have excluded time spent on reading or processing the Bayesian network, as well as creating the join tree, purely focusing on inference time. We went through all possible queries up to poker, and limited others by a reasonable amount of time. Time Ttotal indicates the total time spent on compilation and inference, and shows that it occasionally depends on how many queries you intend to answer which language must be chosen to get results faster. In theory, the speedup S of WPBDDs vs other logical representations coincides with the sizes difference of the arithmetic circuits they induce (see Table 3), as inference has linear complexity in the size of induced circuits. This is confirmed with an average speedup of over 2.6x compared to OBDDs. We achieved an average speedup of over 5x compared to HUGIN (Note that we were not able to process all BNs as we used the LITE (free) version, which comes with limitations). We also achieved a staggering speedup compared to the Junction tree algorithm by Dlib, confirmed by an exceptional amount of cache misses reported by cachegrind (Valgrind tool), and other profile information by GNU Gprof and GNU Perf on resource usage. Collectively, compile\n3Available at http://dlib.net/ 4http://www.hugin.com\n21\nand inference results show that WPBDDs make a valuable addition in the field of exact probabilistic inference."}, {"heading": "8. Conclusion", "text": "To reduce the cost of Bayesian inference through Weighted Model Counting (WMC), we proposed a new canonical language called Weighted Positive Binary Decision Diagrams that represent probability distributions more concisely. We have provided theoretical results in addition to practical results on compilation size with regard to 30+ Bayesian networks, where we have seen WPBDD induced logical circuits reduced by 60% on average in comparison to OBDD induced circuits. The introduced reduction rule is responsibly for a 15% reduction on average among compiled CPTs. These results can be improved upon even further in the future by finding a better variable ordering, which is made easier by WPBDDs, as they do not consider probabilities as auxiliary literals, reducing the search space considerably. We have evaluated the cost of inference compared to OBDD induced circuits, yielding a 2.5x speedup on average, and to the Junction tree algorithm, approaching a speedup of 1000x. The language thus gives computational benefits during model counting as well as compilation."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Recent work on weighted model counting has been very successfully applied to the prob-<lb>lem of probabilistic inference in Bayesian networks. The probability distribution is encoded<lb>into a Boolean normal form and compiled to a target language, in order to represent local<lb>structure expressed among conditional probabilities more efficiently. We show that further<lb>improvements are possible, by exploiting the knowledge that is lost during the encoding phase<lb>and incorporating it into a compiler inspired by Satisfiability Modulo Theories. Constraints<lb>among variables are used as a background theory, which allows us to optimize the Shannon<lb>decomposition. We propose a new language, called Weighted Positive Binary Decision Dia-<lb>grams, that reduces the cost of probabilistic inference by using this decomposition variant to<lb>induce an arithmetic circuit of reduced size.<lb>", "creator": "LaTeX with hyperref package"}}}