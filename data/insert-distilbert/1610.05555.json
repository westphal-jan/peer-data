{"id": "1610.05555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Online Contrastive Divergence with Generative Replay: Experience Replay without Storing Data", "abstract": "conceived in the early 1990s, experience replay ( er ) has been shown to be a successful mechanism to inherently allow online learning algorithms to reuse past experiences. traditionally, er can be applied to all machine learning paradigms ( i. e., simply unsupervised, supervised, and reinforcement lifelong learning ). recently, er has contributed to improving the performance of deep reinforcement learning. yet, its application to many practical settings is simply still limited by the memory requirements of individual er, necessary to explicitly store previous observations. to significantly remedy this issue, we explore a novel hybrid approach, online contrastive divergence with generative replay ( ocd _ gr ), which uses the generative capability of restricted boltzmann machines ( rbms ) instead of recorded past experiences. the rbm is trained online, and does not require the system to store any of the observed data points. we compare ocd _ gr to er on 9 real - world datasets, considering a worst - case scenario ( data points arriving in sorted order ) possibly as well as a more realistic one ( sequential random - order data points ). our results show that in 64. 28 % of the cases ocd _ gr outperforms er and ars in the remaining 35. 72 % it has an almost equal performance, while having a considerably reduced space complexity ( i. e., memory usage ) at most a comparable time complexity.", "histories": [["v1", "Tue, 18 Oct 2016 12:06:14 GMT  (746kb,D)", "http://arxiv.org/abs/1610.05555v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["decebal constantin mocanu", "maria torres vega", "eric eaton", "peter stone", "antonio liotta"], "accepted": false, "id": "1610.05555"}, "pdf": {"name": "1610.05555.pdf", "metadata": {"source": "CRF", "title": "Online Contrastive Divergence with Generative Replay: Experience Replay without Storing Data", "authors": ["Decebal Constantin Mocanu", "Maria Torres Vega", "Eric Eaton", "Antonio Liotta"], "emails": ["d.c.mocanu@tue.nl"], "sections": [{"heading": null, "text": "Keywords generative replay \u00b7 experience replay \u00b7 unsupervised learning \u00b7 online learning \u00b7 density estimation \u00b7 restricted Boltzmann machines \u00b7 deep learning\nDecebal Constantin Mocanu Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, the Netherlands Tel.: +31 40-247 5394 E-mail: d.c.mocanu@tue.nl\nMaria Torres Vega Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, the Netherlands\nEric Eaton Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USA\nPeter Stone Department of Computer Science, The University of Texas at Austin, Austin, USA\nAntonio Liotta Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, the Netherlands\nar X\niv :1\n61 0.\n05 55\n5v 1\n[ cs\n.L G\n] 1\n8 O\nct 2\n01 6"}, {"heading": "1 Introduction", "text": "Experience Replay (ER) (Lin, 1992) (dubbed interleaved learning in McClelland et al (1995)) has been shown to be a successful mechanism in helping online learning algorithms to reuse past experiences. In ER, the data acquired during the online learning process is stored explicitly and presented repeatedly to the online learning algorithm, such as reinforcement learning (RL) (Adam et al, 2012), deep reinforcement learning (DRL) (Mnih et al, 2015), or supervised learning (McClelland et al, 1995). The ER process enables the learner to achieve good performance from limited training data, and helps to break temporal correlations in the observations which go against the i.i.d assumptions of many stochastic gradient-based algorithms (Schaul et al, 2015). Since ER uses recorded data in chunks, it has sometimes been deemed a batch learning approach (Kalyanakrishnan and Stone, 2007). In general, ER focuses on the reuse of observed data in its raw form as stored in memory, replaying it to the online learner. However, this causes ER to scale poorly, as the memory requirements increase as the environment and system requirements increase. One common practice is to limit the available memory of the ER mechanism and to either 1.) discard the oldest experiences as the memory buffer becomes full and/or 2.) prioritize the experiences (Schaul et al, 2015).\nFrom a biological sense of memory (i.e., hippocampal replay in McClelland et al (1995)), the human brain does not store all observations explicitly, but instead it dynamically generates approximate reconstructions of those experiences for recall. This idea has also been applied to online learning through model-based learning as an alternative to ER. Such approaches indirectly reuse experiences by first modeling the environment, and then using that model to generate new data. This procedure is used by Dyna and other model-based learning approaches (Sutton et al, 2008). Building a model will generally require less memory than storing the raw data, and can diminish the effects of noise in the observations. However, model learning incurs additional computational costs and, more importantly, will introduce modeling errors that can significantly decrease performance (Sutton et al, 2008). For this reason, it is necessary to look for alternatives that are able to scale effectively (which is one of the biggest issues in ER) and yield performance results that are comparable with those obtained under ER, without increased computational complexity.\nAt the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016). Due to these capabilities RBMs and models derived from them have been successfully applied to various problems also as standalone models. Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008). However, in all of the above settings RBMs have been used offline using offline training algorithms. This reduces drastically their capabilities to tackle real-world problems which can not be handled on server clouds using GPU computing, and require fast training algorithms capable of continuous learning when the the environment is changing. For example, in the world of wireless sensor networks which is by definition an environment with low-resources (e.g., memory, computational power, low energy) devices to perform anomaly detection in time series directly in the wireless nodes would help improving the network capacity on various components (e.g., lifetime, avoid data traffic congestions), as exemplified in Bosman et al (2017). We then hypothesize that due to their density estimation capabilities RBMs which have been used recently to\nestimate the similarity between data distributions in various domains (e.g., image quality assessment (Mocanu et al, 2014), Markov decision processes (Ammar et al, 2014)), could be used to perform anomaly detection directly on any wireless node if they would have available an online training algorithm which has low memory requirements (best case is to store none of the historical data). Still, to our knowledge, there are no dedicated algorithms to train RBMs in a fully online manner\u2014the only currently available solution is to employ ER mechanisms with memory, by following the successful examples from other deep learning models (e.g., Mnih et al (2015)).\nIn this paper, we combine the generative capabilities of RBMs with the biological inspiration behind experience replay, yielding a novel algorithm to train RBMs in online settings, which we call Online Contrastive Divergence with Generative Replay (OCDGR). In comparison with state-of-the-art ER techniques, OCDGR acts more like the experience replay concept in a biological sense. Instead of explicitly storing past observations in memory, it generates new training data dynamically to represent historical observations, using the generative capabilities of the RBM itself. In contrast to model-based learning approaches, which learn models for the environment (Sutton et al, 2008), OCDGR relies on the underlying RBM, which models only the observed data distribution\u2014a substantially easier problem. OCDGR derived methods may have a wide applicability to a variety of tasks (e.g., regression, classification, reinforcement learning, anomaly detection), but in this paper we focus on demonstrating the benefits of OCDGR over current ER approaches on the RBMs main task (i.e., distribution estimation), this being a must have for any further developments. Thus, using 9 real-world datasets we show how OCDGR outperforms ER in training RBMs, while having reduced memory requirements and an equivalent time complexity.\nThe remainder of this paper is organized as follows. Section 2 presents background knowledge about experience replay and restricted Boltzmann machines for the benefit of the non-specialist reader. Section 3 introduces our proposed method, while Section 4 describes the experiments performed and assesses the results. Finally, Section 5 concludes the paper and presents further research directions."}, {"heading": "2 Background and Related Work", "text": "In this section, we first discuss related work on ER. Next, background informations on RBMs and their offline training methods are presented.\n2.1 Experience Replay (ER)\nExperience replay was first introduced for RL in Lin (1992) and for supervised learning in McClelland et al (1995). Later on, a number of methods have been proposed for ER, aiming to model the environment and to optimize the performance of online learning. A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance. Recently, ER and its variants have contributed to improving deep reinforcement learning (DRL) (Mnih et al, 2015). Narasimhan et al (2015) proposed a form of re-sampling in the context of DRL, separating the learner experience into two parts: one for the positive rewards and one for the negative. Ororbia et al (2015) proposed an online semi-supervised learning algorithm using deep hybrid Boltzmann machines and denoising autoencoders. However, all of these approaches require\nmemory to explicitly store past observations for recall, making them less suitable to the online learning setting.\n2.2 Restricted Boltzmann Machines\nIn this paper, we use the generative capabilities of Restricted Boltzmann Machines (RBMs) to dynamically generate new training data during the online learning process, instead of explicitly storing and recalling past observations. We next review the mathematical details of RBMs. They were introduced by Smolensky (1987) as a powerful model to learn a probability distribution over its inputs. Formally, RBMs are generative stochastic neural networks with two binary layers: the hidden layer h = [h1, h2, .., hnh ] \u2208 {0, 1}nh , and the visible layer v = [v1, v2, .., vnv ] \u2208 {0, 1}nv , where nh and nv are the numbers of hidden neurons and visible neurons, respectively. In comparison with the original Boltzmann machine (Ackley et al, 1985), the RBM architecture (Figure 1) is restricted to be a complete bipartite graph between the hidden and visible layers, disallowing intra-layer connections between the units. The energy function of an RBM for any state {v,h} is computed by summing over all possible interactions between neurons, weights, and biases as follows:\nE(v,h) = \u2212aTv \u2212 bTh\u2212 hTWv , (1)\nwhere W \u2208 Rnh\u00d7nv is the weighted adjacency matrix for the bipartite connections between the visible and hidden layers, and a \u2208 Rnv and b \u2208 Rnh are vectors containing the biases for the visible and hidden neurons, respectively. For convenience, we can bundle the RBM\u2019s free parameters together into \u0398 = {W,a,b}. Functionally, the visible layer encodes the data, while the hidden layer increases the learning capacity of the RBM model by enlarging the class of distributions that can be represented to an arbitrary complexity (Taylor et al, 2011). Due the binary state of the neurons, the free energy of the visible units may be computed as (Bengio, 2009):\nF(v) = \u2212aTv \u2212 \u2211\nj log(1 + exp (bj +Wj:v)) , (2)\nwhere Wj: represents the jth row of the matrix W. The activations of the hidden or visible layers are generated by sampling from a sigmoid S(\u00b7) according to: P (h = 1|v, \u0398) = S(b+Wv) and P (v = 1|h, \u0398) = S(a+WTh) .\n2.3 Offline RBM Training via Contrastive Divergence\nThe RBM parameters can be learned effectively by following the log-likelihood gradient computed over a training set D, with nv-dimensional binary instances. The log-likelihood gradient is given by:\nEP\u0302 [\u2202(logP (v))\n\u2202\u03b8\n] = \u2212EP\u0302 [\u2202F(v) \u2202\u03b8 ] + EP [\u2202F(v) \u2202\u03b8 ] , (3)\nwhere P\u0302 represents the empirical distribution of D and EP is the expectation computed under the model distribution (Bengio, 2009). However, sampling from P to compute the free energy and running long Monte-Carlo Markov Chains (MCMC) to obtain an estimator of the log-likelihood gradient is usually intractable. Due to this intractability, Hinton (2002) proposed an approximation method called Contrastive Divergence (CD), which solves the above problem by making two approximations. The first approximation is to replace the average over all possible inputs from the second term of Equation 3 by a single sample. The second approximation is to run each MCMC chain for only a specific number of steps (nCD ), starting from a data point v0 \u2208 D, as follows:\nv0 P (h|v0)7\u2212\u2192 h0 P (v|h 0)7\u2212\u2192 v1 P (h|v 1)7\u2212\u2192 h1 799K vnCD 7\u2212\u2192 hnCD .\nThe free parameters can then be updated afterwards via:\n\u2206\u0398 = \u2202F(v0) \u2202\u03b8 \u2212 \u2202F(v nCD ) \u2202\u03b8 , (4)\nyielding the following update rules for the free parameters of binary RBMs:\n\u2206Wji \u221d v0i h0j \u2212 vnCDi hnCDj for 1 \u2264 i \u2264 nv, 1 \u2264 j \u2264 nh \u2206ai \u221d v0i \u2212 vnCDi for 1 \u2264 i \u2264 nv (5) \u2206bj \u221d h0j \u2212 hnCDj for 1 \u2264 j \u2264 nh .\nSeveral other variants of contrastive divergence have been proposed to train RBMs offline. Examples of these are: persistent contrastive divergence (Tieleman, 2008), fast persistent contrastive divergence (Tieleman and Hinton, 2009), parallel tempering (Desjardins et al, 2010), and the replace of the Gibbs sampling with a transition operator to obtain a faster mixing rate and an improved learning accuracy without affecting the computational costs (Br\u00fcgge et al, 2013). Yet, in this paper we use the original CD (Hinton, 2002), as it is easily adaptable to online settings, and at the same time it is widely used and allows for a direct comparison with other results reported in the literature."}, {"heading": "3 Online Contrastive Divergence with Generative Replay", "text": "This section presents our novel algorithm to train RBMs online: Online Contrastive Divergence with Generative Replay (OCDGR). Our approach adapts the standard CD algorithm (see Section 2.3) to the online learning setting, and uses dynamic generation of data as a replay mechanism. We show how an RBM trained via OCDGR can have the same functionality as training via ER. However, OCDGR provides the significant advantage of not needing to explicitly store past observations in memory, substantially reducing its space complexity. To our knowledge, this capability is unique, since other state-of-the-art experience replay mechanisms require a memory dataset to store historical data.\n3.1 Intuition and Formalism\nOur algorithm is motivated by the fact that hippocampal replay (McClelland et al, 1995) in the human brain does not recall previous observations explicitly, but instead it generates approximate reconstructions of those past experiences for recall. At the same time, RBMs can generate good samples of the incorporated data distribution via Gibbs sampling (Bengio, 2009). Intuitively, by using those generated samples (instead of previous observations from stored memory as in ER) during the online training process, any RBM model can retain knowledge of past observations while learning new ones.\nBefore entering into the technical details of our proposed method, we mention that further on we use the following notations: Bt represents a batch of observed data between time t\u2212 1 and t, while B\u0302t represents samples generated by the RBM model using the free parameters \u0398t\u22121 (i.e., the parameters values at time t\u2212 1). Figure 2 summarizes the main differences between the OCDGR (Figure 2b) and ER mechanisms (Figure 2a) for training RBMs online, showing how ER explicitly stores previous observations in memory for recall, while OCDGR dynamically generates samples from its current model of the input data distribution. Clearly, the memory used by ER increases linearly with the amount of data observed (up to a fixed limit for memory-bounded ER methods), while by contrast, OCDGR maintains the same memory footprint throughout the training process. Also, note that OCDGR has the Markov property that P (\u0398t) depends only upon\u0398t\u22121 and Bt, while the ER mechanism with memory does not, since for ER, P (\u0398t) is dependent upon {\u0398t\u22121,B1,B2, . . . ,Bt}. This is an important aspect for an algorithm which runs for an indefinite amount of time, as may occur in many real-time systems. Formally, OCDGR is a continuous-time Markov chain with finite (countable) state space X , given by a family {RBM t = RBM (t)}t>0 of X such that:\n1. t 7\u2192 RBM (t) are right-continuous step functions, and\n2. \u2200s, s1, ..., sk \u2208 X , and every sequence of times t1 < t2 < ... < tk < tk+1, it holds that:\nP ( RBM (tk+1) = s | RBM (tk) = sk, ...,RBM (t1) = s1 )\n= P ( RBM (tk+1) = s | RBM (tk) = sk ) .\nThe second condition is the natural continuous-time analogue of the Markov property, and it requires that the future is conditionally independent of the past given the present RBM. A continuous time Markov chain is a non-lattice semi-Markov model, so it has no concept of periodicity. Consequently, the long-runtime averages equals the limiting probabilities, and it has an equilibrium distribution.\n3.2 Algorithm\nOCDGR is presented as Algorithm 1. As input, the algorithm accepts various meta-parameters, two of them being specific for OCDGR, while the others are common to all RBM models (Algorithm 1, line 2). The two meta-parameters specific for OCDGR are the number of Gibbs sampling steps for the generation of the new training data points (nGs), and the number of new data points generated by the RBM with Gibbs sampling (nB\u0302). The common RBMs meta-parameters include the number of hidden neurons (nh), the number of visible neurons (nv) (which is given by the dimensionality of the data), the number of CD steps (nCD ), the number of training epochs (nE), the number of data points stored in a mini-batch before the RBM parameters are updated (nB), the learning rate (\u03b1), the momentum (\u03c1), and the weight decay (\u03be). Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012).\nThe algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135). Each time step, the algorithm observes a new data instance, collecting nB new data points into a mini-batch Bt (lines 8\u201310). After observing nB new data points, OCDGR updates the RBM\u2019s parameters (line 11\u201341). The update procedure proceeds in two phases: Dynamic generation of historical data (lines 14\u201322) As it has been shown in Desjardins et al (2010) and in Cho et al (2010) that RBMs can sample uniformly from the state space, we generate nB\u0302 new training data points to represent past observations based on the data distribution modeled by the RBM at time t \u2212 1; these generated data points are collected into the set B\u0302t. To obtain good data points with high representational power, we perform Gibbs sampling starting from random values of the hidden neurons drawn from a uniform distribution U(0, 1). CD update with generative replay (lines 26\u201341) In the second phase, we update the RBM\u2019s weights and biases (\u0398) using standard CD for a number of epochs, computing the update only over the most recent mini-batch composed by the union of Bt and B\u0302t. This most recent mini-batch consists of 1.) the data points observed between time t\u2212 1 and time t and 2.) the data points generated by the RBM at time t. Note that line 39 of Algorithm 1 contains the general form of the update equation, in which \u03a8+ (statistics collected from the data) and \u03a8\u2212 (statistics collected from the model) can be computed for each free parameter type using Equation 5. Finally, the data points observed between t\u2212 1 and t, and the data points generated with the RBM at time t are deleted from memory, and OCDGR advances to the next discrete time step t+ 1 (lines 42\u201346).\nIt is easy to observe that an RBM trained with OCDGR acts at any time step t as a generative replay mechanism to provide repetition of approximated past experiences, providing a\n1 %% Initialization of the various parameters 2 Set nh, nv , nGs, nCD , nE , nB , nB\u0302 , \u03b1, \u03c1, \u03be 3 Initialize RBM parameters \u03980(i.e., W0, a0, b0)\u223c N (0, \u03c3) 4 Set \u2206\u0398nE0 = 0 5 Set t = 1, Bt = \u2205 6 %% A continuous loop to handle sequential incoming data 7 while system is running do 8 Observe a new data point d 9 Add d to Bt\n10 if Bt contains nB observed data points then 11 Set B\u0302t = \u2205 12 %% Generate new data points with the RBM 13 if t > 1 then 14 for i = 1 : nB\u0302 do 15 %% Run Gibbs sampling 16 Initialize h \u223c U(0, 1) 17 for k = 1 : nGs do 18 Infer P (v = 1|h, \u0398t\u22121) 19 Infer P (h = 1|v, \u0398t\u22121) 20 end 21 Add v to B\u0302t 22 end 23 end 24 %% Update parameters 25 Set \u03980t = \u0398t\u22121 and \u2206\u0398 0 t = \u2206\u0398 nE t\u22121 26 for e(epoch) = 1 : nE do 27 %% Create a training batch from Bt and B\u0302t 28 Set V = Bt \u222a B\u0302t 29 Infer P (H = 1|V, \u0398e\u22121t ) 30 %% Collect positive statistics \u03a8+ 31 Compute \u03a8+ from V and H 32 for k = 1 : nnCD do 33 Infer P (V = 1|H, \u0398e\u22121t ) 34 Infer P (H = 1|V, \u0398e\u22121t ) 35 end 36 %% Collect negative statistics \u03a8\u2212 37 Compute \u03a8\u2212 from V and H 38 %% Perform parameters update 39 \u2206\u0398et = \u03c1\u2206\u0398 e\u22121 t + \u03b1[(\u03a8 + \u2212 \u03a8\u2212)/(nB + nB\u0302)\u2212 \u03be\u0398 e\u22121 t ] 40 \u0398et = \u0398 e\u22121 t +\u2206\u0398 e t 41 end 42 Set \u0398t = \u0398 nE t 43 %% Clean the memory 44 Delete B\u0302t, Bt from memory 45 %% Advance to the next time step 46 Set t = t+ 1, Bt = \u2205 47 end 48 end Algorithm 1: Online Contrastive Divergence with Generative Replay. Note that OCDGR only stores the last variant of \u0398et and \u2206\u0398et in memory. Still, we notate them as being indexed by t for a better illustration of the time and training epochs dimensions.\nmemory-free alternative to ER. In our experiments, we demonstrate empirically that OCDGR can be used successfully to train RBMs in an online setting.\n3.3 Computational Complexity\nThe primary difference between ER and OCDGR at each discrete time step t is that ER has to recall random data from memory, while OCDGR generates the data via Gibbs sampling. For ER, the memory recall time depends upon the hardware platform and the programming environment (e.g., Matlab, C++), and so is not easily quantifiable. For OCDGR, the dynamic generation of historical data phase using Gibbs sampling requires, on one side, a small number of matrix multiplication (which may be parallelized) and are linearly dependent by nGs and, on the other side, the computation of the sigmoid functions for the visible and hidden neurons. This yields a per-update time ofO(2nGsnvnh + nGsnh + nGsnv), which in the typical case of nGs = 1, reduces to O(nvnh)."}, {"heading": "4 Experiments and Results", "text": "Firstly, we considered a toy scenario (i.e., an artificially generated dataset) to illustrate the OCDGR behavior. Secondly, we evaluated OCDGR performance on the MNIST dataset1 of handwritten digits, and on the UCI evaluation suite (Germain et al, 2015). Thus, overall, the evaluation was performed on 9 datasets coming from different domains, which are detailed in Table 1.\nTo simulate the online learning setting, each training instance was fed to the RBM training algorithm only once in a sequential manner in one of two orders: 1.) a worst-case scenario, in which the data instances are presented in order of the classes, and 2.) a more realistic scenario, in which the instances are ordered randomly.\nThe update procedure of the RBM\u2019s free parameters was triggered each time after the system had observed and collected 100 data points (i.e., nB = 100). To find the best metaparameters specific for OCDGR (i.e., nB\u0302 , nGs) we conducted a random search. Based on this small experiment, before each update procedure took place, we generated another nB\u0302 = 300 data points according with Algorithm 1, lines 14\u201322, with nGs set to 1. Moreover, another reason to set a small number of steps for Gibbs sampling when new data points are generated (i.e., nGs = 1) is given by the fact that if we use samples from the model for both components of the gradient (i.e., Equation 3), these will cancel out in expectation. Except when specified otherwise, the other meta-parameters used usually in the RBM training process were set to standard values, such as nE = 10, nCD = 1, \u03b1 = 0.05, \u03c1 = 0.9 (except the first 5 training epochs in which \u03c1 = 0.5), and \u03be = 0.0002, following (Hinton, 2012). Please note that even\n1 http://yann.lecun.com/exdb/mnist/. Last visit on 26 September 2016.\na higher number of contrastive divergence steps (i.e., nCD ) may lead to a better performance on some specific datasets, i.e., MNIST, it leads also to an increasing amount of computations. As our goal was to propose a fast algorithm to train RBMs in an online manner also on low-resources devices, we preferred to perform most of our experiments using just 1 step contrastive divergence.\n4.1 Illustration of OCDGR\u2019s behavior (toy scenario)\nTo easy visualize the quality of the samples generated by RBMs trained with OCDGR (RBMOCD) we have considered a toy scenario with artificially generated data and an RBM with 100 visible neurons and 50 hidden neurons. For training we have created 10000 data points (each data point being a binary vector of 100 elements) split in 10 classes of 1000 data points each, as following. For Class 1 p(vi = 1) = 0.3 \u21d4 1 \u2264 i \u2264 10 and p(vi = 1) = 0\u21d4 11 \u2264 i \u2264 100, and so on up to Class 10 for which p(vi = 1) = 0.3\u21d4 91 \u2264 i \u2264 100 and p(vi = 1) = 0 \u21d4 1 \u2264 i \u2264 90. During training, we have firstly observed all data instances belonging to Class 1, and after that we have generated 1000 samples with the trained RBMOCD. Next, we have continued the training procedure using all data points belonging to Class 2, and then we have generated another 1000 samples, and further on we repeated this procedure until all 10 classes have been considered. To classify the samples generated by RBMOCD we used k-nearest neighbors. Figure 3 shows that OCDGR behaves as expected and as new classes are observed the RBMOCD enlarges its encoded distribution.\n4.2 Evaluation\nWe compared our proposed method, RBMOCD , against 1.) RBMs trained using Experience Replay with a Memory Limit (RBMER-ML) and 2.) RBMs trained using Experience Replay with Infinite Memory (RBMER-IM ). For a fair comparison, in the case of RBMER-ML we limited the number of data points stored in memory to occupy approximately the same number of bytes as the parameters ofRBMOCD . We highlight that by allowing RBMER-ML to have an experiences memory of the same size with the RBMOCD parameters means that, in fact, RBMER-ML needs a double memory size than RBMOCD , as it needs also some memory to store its own parameters. In contrast, we allow RBMER-IM to store all observed data points\nin memory. To train both experience replay models, i.e., RBMER-ML and RBMER-IM , we use a similar algorithm to Algorithm 1. The only main difference is that instead of generating new samples using the RBM models themselves (Algorithm 1, lines 11-23), we retrieve those samples from the replay memory. Moreover, same as for RBMOCD, in the case of RBMER-ML and RBMER-IM models, we used 300 randomly chosen data points from the memory of past experiences and the same meta-parameters values. To quantify the generative performance of the trained networks, we used Annealed Importance Sampling (AIS) with the same parameters as in the original paper (Salakhutdinov and Murray, 2008) to estimate the partition function of the RBMs and to calculate their log probabilities. On each dataset, after all training data points were given to the learner, we computed the average log probabilities on the entire test set."}, {"heading": "4.2.1 Worst Case Scenario: Sorted Order", "text": "In the first scenario, we have used the binarized MNIST dataset. During training, the data instances were ordered sequentially in ascending order of the digits (0, 1, . . . , 9), making it a difficult scenario for online learning. For each algorithm, we considered various numbers of hidden neurons (nh \u2208 {25, 250, 500}), and 784 visible neurons (i.e., 28\u00d7 28 binary image). Figure 4a shows that RBMOCD outperforms RBMER-ML in all cases, independent of the number of hidden neurons. Moreover, it outperforms even RBMER-IM when it has enough representational power (i.e., 250 and 500 hidden neurons). It is interesting to see that while the generative power of RBMOCD increases with the number of hidden neurons, RBMER-IM is not significantly affected when given more hidden neurons. Further, the RBMER-ML model loses its generative power when the number of hidden neurons is increased. These results may be explained by the fact that having more hidden neurons helps RBMOCD to better model the data distribution. In contrast, in the case of experience replay mechanisms with memory, a larger RBM would need more past-experience training data to avoid forgetting the distribution of the first observed data points, especially in the case of RBMER-ML. This\nsituation does not occur forRBMOCD , due to the fact the data points generated randomly by the RBM itself using Gibbs sampling approximate well the distribution of the past-experience data. For the sake of clarification, we mention that even if at a first look an inter-class standard deviation of 30\u2212 40 nats for all online trained models is striking, in fact, it is same as the one obtained for the offline trained RBMs."}, {"heading": "4.2.2 Realistic Scenario: Random Order", "text": "In the second more realistic scenario, the training instances were presented sequentially in random order. As this is an usually encountered situation, herein, besides the MNIST dataset, we have used also the UCI evaluation suite (Germain et al, 2015). The latter one contains contains 8 real-world binary datasets from various domains, specially selected to evaluate the performance of density estimation models.\nMNIST dataset. Figure 4b shows that RBMOCD outperforms both RBMER-ML and RBMER-IM , when it has enough representational power (i.e., 250 and 500 hidden neurons). As in the previous experiment, the generative performance of RBMOCD increases as the number of hidden neurons increases, but the gain in performance is even higher in this situation, culminating with \u2212114.53 nats in the case of an RBM with 500 hidden neurons. Thus, RBMOCD outperforms RBMER-IM by 35.39 nats at the same number of hidden neurons. In fact, RBMOCD with 500 hidden neurons outperforms by 11.01 nats even the state-of-the-art results reported by Salakhutdinov and Murray (Salakhutdinov and Murray, 2008) (see Table 2, third row) for an RBM with 500 hidden neurons trained completely offline with standard one-step contrastive divergence. Besides the improved average performance on the entire test set, also observe that as the number of hidden neurons increases in the case of RBMOCD , the standard deviation (computed on the average log probabilities from each digit class) decreases. The smaller standard deviation implies that the model represents all classes well, without imbalance.\nTo better understand RBMOCD\u2019s behavior, we performed an additional experiment in the realistic scenario. We again trained RBMOCD, RBMER-ML, and RBMER-IM models, each with 500 hidden neurons. However, in this experiment, we measured performance on the MNIST test set during the training phase after every 1,000 observed data points. Figure 5 shows an interesting behavior for all three models. The RBMOCD has a very stable learning curve which increases over time. In contrast, RBMER-ML and RBMER-IM show unstable learning curves. This behavior can be explained by the fact that when the probability of selecting for replay any past observed data point decreases below a certain threshold, then the subset of the selected data points for replay no longer represents well the distribution of the past-experience data, and the models become over-fitted. To avoid this situation, the number of selected data points from the replay memory would need to increase linearly with the number of observations. However, this solution is infeasible as it will lead to a linear increase in the computational complexity of RBMER-ML and RBMER-IM over time, leading to non-realistic online learning algorithms. In contrast, RBMOCD is a Markov chain and it is not affected by this situation, explaining why RBMOCD outperforms RBMER-ML and RBMER-IM after observing approximately 8,000 instances.\nIn our final experiment on the MNIST dataset, we varied the number of contrastive divergence steps, training an RBMOCD with 500 hidden neurons using 3 steps and 10 steps of contrastive divergence. Similarly to the RBM\u2019s behavior reported by Salakhutdinov and Murray (Salakhutdinov and Murray, 2008), further CD steps improved the generative performance of these models. In the case when nCD = 3, the average log probabilities on the MNIST test set was -108.96; for nCD = 10, it was -104.31.\nUCI evaluation suite. Herein, we have trainedRBMOCD , RBMER-ML, and RBMER-IM with nh = 500, and nv set to the number of features of each dataset. The results reflected in Table 2 show that RBMOCD outperforms RBMER-ML and RBMER-IM on 5 out of 8 datasets, while on the other 3 has a very close generative performance to the top performer. Overall, we may observe that as the size of the dataset increases, or as data distribution became more complex, RBMOCD starts having a clear advantage over RBMER-ML or RBMER-IM .\nIn all experiments performed, we observed that our MATLAB implementations of all algorithms ran in approximately the same time. Given the same RBM configuration, RBMER-IM was slightly slower then RBMOCD , which was slightly slower then RBMER-ML. However,\nthe differences were on the order of few milliseconds, with one note that for RBMER-IM , the difference increased with the number of data points saved in memory."}, {"heading": "5 Conclusion", "text": "We have proposed a novel method, Online Contrastive Divergence with Generative Replay (OCDGR), to train RBMs in online settings. Unlike current experience replay mechanisms which directly recall recorded observations from memory, OCDGR uses the generative capabilities of RBMs to dynamically simulate past experiences. As a consequence, it does not need to store past observations in memory, substantially reducing its memory requirements. We demonstrated that RBMs trained online with OCDGR outperform RBMs trained online using experience replay mechanisms with memory, with few exceptions, in which their performance is comparable. We highlight that in some cases they even outperform RBMs having a similar number of hidden neurons, but trained offline with standard contrastive divergence.\nIn future work, we intend to understand better the effect of the various OCDGR metaparameters (especially the relation between the number of generated samples and the number of observed samples) on the RBM\u2019s generative performance, and to extend OCDGR to other suitable generative models, e.g., deep Boltzmann machines, autoencoders. Other interesting research directions, which we hope to investigate, is to use to use RBMs trained with OCDGR to perform anomaly detection in low-resources devices, to control DRL algorithms by generating RL atomic operations instead of using experience replay mechanisms with memory to store them, to perform online supervised learning by generating input-output pairs for the online training of deep networks.\nAcknowledgements This research has been partly funded by the European Union\u2019s Horizon 2020 project INTER-IoT (grant number 687283). A portion of this work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (CNS-1330072, CNS-1305287, IIS-1637736, IIS-1651089), ONR (21C184-01), and AFOSR (FA9550-14-1-0087)."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["H Ackley", "E Hinton", "J Sejnowski"], "venue": "Cognitive Science", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Experience replay for real-time reinforcement learning control", "author": ["S Adam", "L Busoniu", "R Babuska"], "venue": "IEEE Transactions on Systems,", "citeRegEx": "Adam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Adam et al\\.", "year": 2012}, {"title": "Automatically mapped transfer between reinforcement learning tasks via three-way restricted boltzmann machines", "author": ["HB Ammar", "DC Mocanu", "M Taylor", "K Driessens", "K Tuyls", "G Weiss"], "venue": "Notes in Computer Science,", "citeRegEx": "Ammar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2013}, {"title": "An automated measure of mdp similarity for transfer in reinforcement learning", "author": ["HB Ammar", "E Eaton", "ME Taylor", "DC Mocanu", "K Driessens", "G Weiss", "K Tuyls"], "venue": "Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Y Bengio"], "venue": "Found Trends Mach Learn 2(1):1\u2013127,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Spatial anomaly detection", "author": ["HH Bosman", "G Iacca", "A Tejada", "HJ W\u00f6rtche", "A Liotta"], "venue": null, "citeRegEx": "Bosman et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bosman et al\\.", "year": 2017}, {"title": "The flip-the-state transition operator for restricted", "author": ["A Fischer", "C Igel"], "venue": null, "citeRegEx": "Br\u00fcgge et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Br\u00fcgge et al\\.", "year": 2013}, {"title": "Parallel tempering for", "author": ["G Desjardins", "A Courville", "Y Bengio", "P Vincent", "O Delalleau"], "venue": null, "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "The rate adapting poisson model for information", "author": ["PV Gehler", "AD Holub", "M Welling"], "venue": null, "citeRegEx": "Gehler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gehler et al\\.", "year": 2010}, {"title": "MADE: masked autoencoder", "author": ["M Germain", "K Gregor", "I Murray", "H Larochelle"], "venue": "Machine Learning, ACM,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G Hinton"], "venue": "Learning, ICML 2015,", "citeRegEx": "Hinton,? \\Q2015\\E", "shortCiteRegEx": "Hinton", "year": 2015}, {"title": "Batch reinforcement learning in a complex", "author": ["S Kalyanakrishnan", "P Stone"], "venue": null, "citeRegEx": "Kalyanakrishnan and Stone,? \\Q2007\\E", "shortCiteRegEx": "Kalyanakrishnan and Stone", "year": 2007}, {"title": "Classification using discriminative restricted boltzmann", "author": ["H Larochelle", "Y Bengio"], "venue": null, "citeRegEx": "Larochelle and Bengio,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio", "year": 2008}, {"title": "Self-improving reactive agents based on reinforcement learning, planning", "author": ["LJ Lin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Human-level control through deep", "author": ["D Kumaran", "D Wierstra", "S Legg", "D Hassabis"], "venue": null, "citeRegEx": "Kumaran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2015}, {"title": "A (2014) Deep learning for objective quality assessment", "author": ["DC Mocanu", "G Exarchakos", "Liotta"], "venue": null, "citeRegEx": "Mocanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mocanu et al\\.", "year": 2014}, {"title": "A (2016) A topological insight", "author": ["DC Mocanu", "E Mocanu", "PH Nguyen", "M Gibescu", "Liotta"], "venue": null, "citeRegEx": "Mocanu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mocanu et al\\.", "year": 2016}, {"title": "Language understanding for text-based games", "author": ["K s10994-016-5570-z Narasimhan", "T Kulkarni", "R Barzilay"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders", "author": ["AG Ororbia", "CL Giles", "D Reitter"], "venue": null, "citeRegEx": "Ororbia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ororbia et al\\.", "year": 2015}, {"title": "Restricted boltzmann machines modeling human choice", "author": ["T Osogami", "M Otsuka"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Osogami and Otsuka,? \\Q2014\\E", "shortCiteRegEx": "Osogami and Otsuka", "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R Salakhutdinov", "I Murray"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov and Murray,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R Salakhutdinov", "A Mnih", "G Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ACM,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Prioritized experience replay", "author": ["T Schaul", "J Quan", "I Antonoglou", "D Silver"], "venue": null, "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1987\\E", "shortCiteRegEx": "Smolensky", "year": 1987}, {"title": "Dyna-style planning with linear function approximation and prioritized sweeping", "author": ["RS Sutton", "C Szepesv\u00e1ri", "A Geramifard", "M Bowling"], "venue": "Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Sutton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["GW Taylor", "GE Hinton", "ST Roweis"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T Tieleman"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Tieleman,? \\Q2008\\E", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["T Tieleman", "G Hinton"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Since ER uses recorded data in chunks, it has sometimes been deemed a batch learning approach (Kalyanakrishnan and Stone, 2007).", "startOffset": 94, "endOffset": 127}, {"referenceID": 23, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016).", "startOffset": 55, "endOffset": 72}, {"referenceID": 4, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016).", "startOffset": 127, "endOffset": 141}, {"referenceID": 20, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016).", "startOffset": 361, "endOffset": 413}, {"referenceID": 19, "context": "Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008).", "startOffset": 58, "endOffset": 84}, {"referenceID": 12, "context": "Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008).", "startOffset": 251, "endOffset": 280}, {"referenceID": 10, "context": "Since ER uses recorded data in chunks, it has sometimes been deemed a batch learning approach (Kalyanakrishnan and Stone, 2007). In general, ER focuses on the reuse of observed data in its raw form as stored in memory, replaying it to the online learner. However, this causes ER to scale poorly, as the memory requirements increase as the environment and system requirements increase. One common practice is to limit the available memory of the ER mechanism and to either 1.) discard the oldest experiences as the memory buffer becomes full and/or 2.) prioritize the experiences (Schaul et al, 2015). From a biological sense of memory (i.e., hippocampal replay in McClelland et al (1995)), the human brain does not store all observations explicitly, but instead it dynamically generates approximate reconstructions of those experiences for recall.", "startOffset": 95, "endOffset": 688}, {"referenceID": 4, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016). Due to these capabilities RBMs and models derived from them have been successfully applied to various problems also as standalone models. Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008). However, in all of the above settings RBMs have been used offline using offline training algorithms. This reduces drastically their capabilities to tackle real-world problems which can not be handled on server clouds using GPU computing, and require fast training algorithms capable of continuous learning when the the environment is changing. For example, in the world of wireless sensor networks which is by definition an environment with low-resources (e.g., memory, computational power, low energy) devices to perform anomaly detection in time series directly in the wireless nodes would help improving the network capacity on various components (e.g., lifetime, avoid data traffic congestions), as exemplified in Bosman et al (2017). We then hypothesize that due to their density estimation capabilities RBMs which have been used recently to", "startOffset": 128, "endOffset": 1573}, {"referenceID": 11, "context": "A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance.", "startOffset": 99, "endOffset": 132}, {"referenceID": 11, "context": "A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance. Recently, ER and its variants have contributed to improving deep reinforcement learning (DRL) (Mnih et al, 2015). Narasimhan et al (2015) proposed a form of re-sampling in the context of DRL, separating the learner experience into two parts: one for the positive rewards and one for the negative.", "startOffset": 99, "endOffset": 375}, {"referenceID": 11, "context": "A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance. Recently, ER and its variants have contributed to improving deep reinforcement learning (DRL) (Mnih et al, 2015). Narasimhan et al (2015) proposed a form of re-sampling in the context of DRL, separating the learner experience into two parts: one for the positive rewards and one for the negative. Ororbia et al (2015) proposed an online semi-supervised learning algorithm using deep hybrid Boltzmann machines and denoising autoencoders.", "startOffset": 99, "endOffset": 555}, {"referenceID": 23, "context": "They were introduced by Smolensky (1987) as a powerful model to learn a probability distribution over its inputs.", "startOffset": 24, "endOffset": 41}, {"referenceID": 4, "context": "Due the binary state of the neurons, the free energy of the visible units may be computed as (Bengio, 2009):", "startOffset": 93, "endOffset": 107}, {"referenceID": 4, "context": "where P\u0302 represents the empirical distribution of D and EP is the expectation computed under the model distribution (Bengio, 2009).", "startOffset": 116, "endOffset": 130}, {"referenceID": 4, "context": "where P\u0302 represents the empirical distribution of D and EP is the expectation computed under the model distribution (Bengio, 2009). However, sampling from P to compute the free energy and running long Monte-Carlo Markov Chains (MCMC) to obtain an estimator of the log-likelihood gradient is usually intractable. Due to this intractability, Hinton (2002) proposed an approximation method called Contrastive Divergence (CD), which solves the above problem by making two approximations.", "startOffset": 117, "endOffset": 354}, {"referenceID": 26, "context": "Examples of these are: persistent contrastive divergence (Tieleman, 2008), fast persistent contrastive divergence (Tieleman and Hinton, 2009), parallel tempering (Desjardins et al, 2010), and the replace of the Gibbs sampling with a transition operator to obtain a faster mixing rate and an improved learning accuracy without affecting the computational costs (Br\u00fcgge et al, 2013).", "startOffset": 57, "endOffset": 73}, {"referenceID": 27, "context": "Examples of these are: persistent contrastive divergence (Tieleman, 2008), fast persistent contrastive divergence (Tieleman and Hinton, 2009), parallel tempering (Desjardins et al, 2010), and the replace of the Gibbs sampling with a transition operator to obtain a faster mixing rate and an improved learning accuracy without affecting the computational costs (Br\u00fcgge et al, 2013).", "startOffset": 114, "endOffset": 141}, {"referenceID": 4, "context": "At the same time, RBMs can generate good samples of the incorporated data distribution via Gibbs sampling (Bengio, 2009).", "startOffset": 106, "endOffset": 120}, {"referenceID": 10, "context": "Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012). The algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135).", "startOffset": 91, "endOffset": 105}, {"referenceID": 10, "context": "Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012). The algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135). Each time step, the algorithm observes a new data instance, collecting nB new data points into a mini-batch Bt (lines 8\u201310). After observing nB new data points, OCDGR updates the RBM\u2019s parameters (line 11\u201341). The update procedure proceeds in two phases: Dynamic generation of historical data (lines 14\u201322) As it has been shown in Desjardins et al (2010) and in Cho et al (2010) that RBMs can sample uniformly from the state space, we generate nB\u0302 new training data points to represent past observations based on the data distribution modeled by the RBM at time t \u2212 1; these generated data points are collected into the set B\u0302t.", "startOffset": 91, "endOffset": 563}, {"referenceID": 10, "context": "Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012). The algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135). Each time step, the algorithm observes a new data instance, collecting nB new data points into a mini-batch Bt (lines 8\u201310). After observing nB new data points, OCDGR updates the RBM\u2019s parameters (line 11\u201341). The update procedure proceeds in two phases: Dynamic generation of historical data (lines 14\u201322) As it has been shown in Desjardins et al (2010) and in Cho et al (2010) that RBMs can sample uniformly from the state space, we generate nB\u0302 new training data points to represent past observations based on the data distribution modeled by the RBM at time t \u2212 1; these generated data points are collected into the set B\u0302t.", "startOffset": 91, "endOffset": 587}, {"referenceID": 20, "context": "To quantify the generative performance of the trained networks, we used Annealed Importance Sampling (AIS) with the same parameters as in the original paper (Salakhutdinov and Murray, 2008) to estimate the partition function of the RBMs and to calculate their log probabilities.", "startOffset": 157, "endOffset": 189}, {"referenceID": 20, "context": "01 nats even the state-of-the-art results reported by Salakhutdinov and Murray (Salakhutdinov and Murray, 2008) (see Table 2, third row) for an RBM with 500 hidden neurons trained completely offline with standard one-step contrastive divergence.", "startOffset": 79, "endOffset": 111}, {"referenceID": 20, "context": "On the MNIST dataset the offline RBM results are taken from Salakhutdinov and Murray (2008), while on the UCI evaluation suite the offline RBMs results are taken from Germain et al (2015).", "startOffset": 60, "endOffset": 92}, {"referenceID": 20, "context": "On the MNIST dataset the offline RBM results are taken from Salakhutdinov and Murray (2008), while on the UCI evaluation suite the offline RBMs results are taken from Germain et al (2015).", "startOffset": 60, "endOffset": 188}, {"referenceID": 20, "context": "Similarly to the RBM\u2019s behavior reported by Salakhutdinov and Murray (Salakhutdinov and Murray, 2008), further CD steps improved the generative performance of these models.", "startOffset": 69, "endOffset": 101}], "year": 2016, "abstractText": "Conceived in the early 1990s, Experience Replay (ER) has been shown to be a successful mechanism to allow online learning algorithms to reuse past experiences. Traditionally, ER can be applied to all machine learning paradigms (i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has contributed to improving the performance of deep reinforcement learning. Yet, its application to many practical settings is still limited by the memory requirements of ER, necessary to explicitly store previous observations. To remedy this issue, we explore a novel approach, Online Contrastive Divergence with Generative Replay (OCDGR), which uses the generative capability of Restricted Boltzmann Machines (RBMs) instead of recorded past experiences. The RBM is trained online, and does not require the system to store any of the observed data points. We compare OCDGR to ER on 9 real-world datasets, considering a worst-case scenario (data points arriving in sorted order) as well as a more realistic one (sequential random-order data points). Our results show that in 64.28% of the cases OCDGR outperforms ER and in the remaining 35.72% it has an almost equal performance, while having a considerably reduced space complexity (i.e., memory usage) at a comparable time complexity.", "creator": "LaTeX with hyperref package"}}}