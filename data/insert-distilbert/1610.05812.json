{"id": "1610.05812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "abstract": "morden state - of - the - engine art additive speech recognition systems usually readily employ speech neural networks for acoustic modeling. however, compared to the conventional gaussian mixture models, deep neural network ( dnn ) based acoustic detection models usually have much wider larger number sizes of model parameters, making enabling it challenging for their applications in resource constrained platforms such as mobile devices. in this paper, we might study successfully the application of the recently proposed highway deep neural network ( hdnn ) for training small - footprint acoustic instrument models. using hdnn is a type of depth - gated feedforward neural network, which introduces two special type of gate functions to visually facilitate the input information / flow through different layers. our study demonstrates that hdnns are actually more compact than plain dnns for discrete acoustic modeling, i. e., so they can achieve comparable impulse recognition accuracy with much less model parameters than plain dnn - length based acoustic models. furthermore, hdnns are more controllable than plain dnns. the gate functions of a hdnn largely control the behavior of the whole network with very small number of model implementation parameters. and finally, hdnns are more adaptable than plain dnns. for example, even simply slowly updating up the gate functions using the adaptation data can result in considerable gains. clearly we demonstrate these aspect by experiments using the previously publicly available ami meeting speech transcription corpus, which has around 80 hours of training data. \u2026 moreover, we also investigate the knowledge distillation algorithm technique to further improve the small - footprint hdnn acoustic models.", "histories": [["v1", "Tue, 18 Oct 2016 22:06:01 GMT  (118kb,D)", "https://arxiv.org/abs/1610.05812v1", "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963"], ["v2", "Mon, 24 Oct 2016 21:12:56 GMT  (131kb,D)", "http://arxiv.org/abs/1610.05812v2", "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963"], ["v3", "Wed, 25 Jan 2017 15:45:22 GMT  (131kb,D)", "http://arxiv.org/abs/1610.05812v3", "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963"], ["v4", "Tue, 25 Apr 2017 19:48:41 GMT  (942kb,D)", "http://arxiv.org/abs/1610.05812v4", "9 pages, 6 figures. Accepted to IEEE/ACM Transactions on Audio, Speech and Language Processing, 2017. arXiv admin note: text overlap witharXiv:1608.00892,arXiv:1607.01963"]], "COMMENTS": "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["liang lu", "steve renals"], "accepted": false, "id": "1610.05812"}, "pdf": {"name": "1610.05812.pdf", "metadata": {"source": "CRF", "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "authors": ["Liang Lu", "Steve Renals"], "emails": ["llu@ttic.edu,", "s.renals@ed.ac.uk"], "sections": [{"heading": null, "text": "IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017 1\nSmall-footprint Highway Deep Neural Networks for Speech Recognition\nLiang Lu Member, IEEE, Steve Renals Fellow, IEEE\nAbstract\u2014State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data.\nIndex Terms\u2014Deep learning, Highway networks, Smallfootprint models, Speech recognition\nI. INTRODUCTION\nDEEP Learning has significantly advanced the state-of-the-art in speech recognition over the past few years [1]\u2013 [3]. Most speech recognisers now employ the neural network and hidden Markov model (NN/HMM) hybrid architecture, first investigated in the early 1990s [4], [5]. Compared to those models, current neural network acoustic models tend to be larger and deeper, made possible by faster computing such as general-purpose graphic processing units (GPGPUs). Furthermore, more complex neural architectures such as recurrent neural networks (RNNs) with long short-term memory (LSTM) units and convolutional neural networks (CNNs) have received intensive research, resulting in a range of flexible and powerful neural network architectures that have been applied to a range of tasks in speech, image and natural language processing.\nDespite their success, neural network models have been criticized as lacking structure, being resistant to interpretation, and possessing limited adaptablity. Furthermore accurate neural network acoustic models reported in the research literature\nManuscript received -; revised - Liang Lu is with Toyota Technological Institute at Chicago, and Steve Renals is with The University of Edinburgh, UK; email: llu@ttic.edu, s.renals@ed.ac.uk\nThe research was supported by EPSRC Programme Grant grant EP/I031022/1 Natural Speech Technology (NST) and the European Union under H2020 project SUMMA, grant agreement 688139.\nhave tended to be much larger than conventional Gaussian mixture models, thus making it challenging to deploy them on resource constrained embedded or mobile platforms when cloud computing solutions are not appropriate (due to the unavailability of an internet connection or for privacy reasons). Recently, there has been considerable work to reduce the size of neural network acoustic models while limiting any reduction in recognition accuracy, such as the use of lowrank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13]. Smaller footprint models may also bring advantages in requiring less training data, and in being potentially more adaptable to changing target domains, environments or speakers, owing to having fewer model parameters.\nIn this paper, we present a comprehensive study of smallfootprint acoustic models using highway deep neural networks (HDNNs), building on our previous studies [14]\u2013[16]. HDNNs are multi-layer networks which have shortcut connections between hidden layers [17]. Compared to regular multi-layer networks with skip connections, HDNNs are additionally equipped with two gate functions \u2013 transform and carry gates \u2013 which control and facilitate the information flow throughout the whole network. In particular, the transform gate scales the output of a hidden layer, and the carry gate is used to pass through a layer input directly after element-wise rescaling. These gate functions are central to training very deep networks [17] and to speeding up convergence [14]. We show that for speech recognition, recognition accuracy can be retained by increasing the depth of the network, while the number of hidden units in each hidden layer can be significantly reduced. As a result, HDNNs are much thinner and deeper with many fewer model parameters. Besides, in contrast to training regular multi-layer networks of the same depth and width, which typically requires careful pretraining, we demonstrate that HDNNs may be trained using standard stochastic gradient descent without any pretraining [14]. To further reduce the number of model parameters, we propose a variant of HDNN architecture by sharing the gate units across all the hidden layers. Furthermore, The authors in [17] only studied the constrained carry gate setting for HDNNs, while in this work we provide detailed comparisons of different gate functions in the context of speech recognition.\nWe also investigate the roles of the two gate functions in HDNNs using both cross-entropy (CE) training and sequence training, and We present a different way to investigate and understand the effect of gate units in neural networks from the point of view of regularization and adaptation. Our key observation is that the gate functions can manipulate the\nar X\niv :1\n61 0.\n05 81\n2v 4\n[ cs\n.C L\n] 2\n5 A\npr 2\n01 7"}, {"heading": "2 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "behavior of all the hidden layers, and they are robust to overfitting. For instance, if we do not update the model parameters in the hidden layers and/or the softmax layer during sequence training, and only update the gate functions, then we are able to retain most of the improvement by sequence training. Moreover, the regularization term in the sequence training objective is not required when only updating the gate functions. Since the size of the gate functions are relatively small, we can achieve a considerable gain by only fine tuning these parameters for unsupervised speaker adaptation, which is a strong advantage of this model. Finally, we investigate teacher-student training, and its combination with sequence training, as well as speaker adaptation to further improve the accuracy of the small-size HDNN acoustic models. Our teacher-student training experiments also provide more results to understand this technique in the sequence training and adaptation setting.\nOverall, a small-footprint HDNN acoustic model with 5 million model parameters achieved slightly better accuracy compared to a DNN system with 30 million parameters, while the HDNN model with 2 million parameters achieved only slightly lower accuracy compared to that DNN system. Finally, the recognition accuracy of a much smaller HDNN model (less than 0.8 million model parameters) can be significantly improved by teacher-student style training, narrowing the gap between this model and the much larger DNN system."}, {"heading": "II. HIGHWAY DEEP NEURAL NETWORKS", "text": ""}, {"heading": "A. Deep neural networks", "text": "We focus on feed-forward deep neural networks (DNNs) in this study. Although recurrent neural networks with long shortterm memory units (LSTM-RNNs) and convolutional neural networks (CNNs) can obtain higher recognition accuracy with fewer model parameters compared to DNNs [18], [19], they are computationally more expensive for applications on resource constrained platforms. Moreover, their accuracy can be transferred to a DNN by teacher-student training [?], [20], [21].\nA multi-layer network with L hidden layers is represented as\nh1 = \u03c3(x, \u03b81) (1) hl = \u03c3(hl\u22121, \u03b8l), for l = 2, . . . , L (2) y = g(hL, \u03b8c) (3)\nwhere: x is an input feature vector; \u03c3(h(l\u22121)t , \u03b8l) denotes the transformation of the input h(l\u22121)t with the parameter \u03b8l followed by a nonlinear activation function \u03c3, e.g., sigmoid; g(\u00b7, \u03b8c) is the output function that is parameterized by \u03b8c in the output layer, which usually uses the softmax to obtain the posterior probability of each class given the input feature. To facilitate our discussion later on, we denote \u03b8h={\u03b81, \u00b7 \u00b7 \u00b7 , \u03b8L} as the set of neural network parameters.\nGiven target labels, the network is usually trained by gradient descent to minimize a loss function such as cross-entropy. However, as the number of hidden layers increases, the error surface becomes increasingly non-convex, and it becomes\nmore likely to find a poor local minimum using gradientbased optimization algorithms with random initialization [22]. Furthermore the variance of the back-propagated gradients may become small in the lower layers if the model parameters are not initialized properly [23]."}, {"heading": "B. Highway networks", "text": "There have been a variety of training algorithms, and model architectures, proposed to enable very deep multi-layer networks including pre-training [24], [25], normalised initialisation [23], deeply-supervised networks [26], and batch normalisation [27]. Highway deep neural networks (HDNNs) [17] were proposed to enable very deep networks to be trained by augmenting the hidden layers with gate functions:\nhl = \u03c3(hl\u22121, \u03b8l) \u25e6 T (hl\u22121,WT ) + hl\u22121 \u25e6 C(hl\u22121,Wc) (4)\nwhere: hl denotes the hidden activations of l-th layer; T (\u00b7) is the transform gate that scales the original hidden activations; C(\u00b7) is the carry gate, which scales the input before passing it directly to the next hidden layer; and \u25e6 denotes elementwise multiplication. The outputs of T (\u00b7) and C(\u00b7) are constrained to be within [0, 1], and we use a sigmoid function for each, parameterized by WT and Wc respectively. Following our previous work [14], we tie the parameters in the gate functions across all the hidden layers, which can significantly save model parameters. Untying the gate functions did not result in any gain in our preliminary experiments. In this work, we do not use any bias vector in the two gate functions. Since the parameters in T (\u00b7) and C(\u00b7) are layer-independent, we denote \u03b8g = (WT ,Wc), and we will look into the specific roles of these model parameters in sequence training and model adaptation experiments.\nWithout the transform gate, i.e. T (\u00b7) = 1, the highway network is similar to a network with skip connections \u2013 the main difference is that the input is firstly scaled by the carry gate. If the carry gate is set to zero, i.e. C(\u00b7) = 0, the second term in (4) is dropped,\nhl = \u03c3(hl\u22121, \u03b8l) \u25e6 T (hl\u22121,WT ), (5)\nresulting in a model that is similar to dropout regularization [28], which may be written as\nhl = \u03c3(hl\u22121, \u03b8l) \u25e6 , i \u223c p( i), (6)\nwhere p( i) is a Bernoulli distribution for the i-th element in as originally proposed in [28]; it was shown later that using a continuous distribution with well designed mean and variance works as well or better [29]. From this perspective, the transform gate may work as a regularizer, but with the key difference that T (\u00b7) is a deterministic function, while i is drawn stochastically from a predefined distribution in dropout. The network in (5) is also related to LHUC (Learning Hidden Unit Contribution) adaptation for multilayer acoustic models [30], [31], which may be represented as\nhsl = a(r s l ) \u25e6 \u03c3(hsl\u22121, \u03b8l) (7)\n3 where: rsl is a speaker dependent vector for l-th hidden layer, and hsl is the speaker adapted hidden activations; s is the speaker index; and a(\u00b7) is a nonlinear function. The model in (5) can be seen as an extension of LHUC in which rsl is parameterized as WThl\u22121. We shall investigate the update of WT for speaker adaptation in the experimental section.\nAlthough there are more computational steps for each hidden layer compared to regular DNNs due to the gate functions, the training speed will be improved if the size of the weight matrices are smaller. Furthermore, the matrices can be packed together as\nW\u0303l = [ W>l ,W > T ,W > c ]> , (8)\nwhere W>l is the weight matrix in the l-th layer, and we then compute W\u0303lhl\u22121. This approach, applied at the minibatch level, allows more efficient matrix computation when using GPUs."}, {"heading": "C. Related models", "text": "Both HDNNs and LSTM-RNNs [32] employ gate functions. However, the gates in LSTMs are designed to control the information flow through time and to model along temporal dependencies; for HDNNs, the gates are used to facilitate the information flow through the depth of the model. Combinations of the two architectures have been explored recently: highway LSTMs [33] employ highway connections to train a stacked LSTM with multiple layers; recurrent highway networks [34] share gate functions to control the information flow in both time and model depth. On the other hand, the residual network (ResNet) [35] was recently proposed to train very deep networks, advancing the state-of-the-art in computer vision. ResNets are closely related to highway networks in the sense that they also rely on skip connections for training very deep networks; however, gate functions are not employed in ResNets (which can save some computational cost). Finally, adapting approaches developed for visual object recognition [36], very deep CNN architectures have been investigated for speech recognition [37]."}, {"heading": "III. TRAINING", "text": ""}, {"heading": "A. Cross-entropy training", "text": "The most common criterion used to train neural networks for classification is the cross-entropy (CE) loss function,\nL(CE)(\u03b8) = \u2212 \u2211 j y\u0302jt log yjt, (9)\nwhere j is the index of the hidden Markov model (HMM) state, yt is the output of the neural network (3) at time t, and y\u0302t = {y1t, \u00b7 \u00b7 \u00b7 , yJt} denotes the ground truth label that is a one-hot vector, where J is the number of HMM states. Note that the loss function is defined for one training example here for simplicity of notation. Supposing that y\u0302jt = \u03b4ij , where \u03b4ij is the Kronecker delta function and i is the ground truth class at the time step t, the CE loss becomes\nL(CE)(\u03b8) = \u2212 log yit. (10)\nIn this case, minimizing L(CE)(\u03b8) corresponds to minimizing the negative log posterior probability of the correct class, and is equal to maximizing the probability yit; this will also result in minimizing the posterior probabilities of other classes since they sum to one."}, {"heading": "B. Teacher-Student training", "text": "Instead of using the ground truth labels, the teacher-student training approach defines the loss function as\nL(KL)(\u03b8) = \u2212 \u2211 j y\u0303jt log yjt, (11)\nwhere y\u0303jt is the output of the teacher model, which works as a pseudo-label. Minimizing this loss function is equivalent to minimizing the Kullback-Leibler (KL) divergence between the posterior probabilities of each class from the teacher and student models [8]. Here, y\u0303jt is no longer a one-hot vector; instead, the competing classes will have small but nonzero posterior probabilities for each training example. Hinton et al. [38] suggested that the small posterior probabilities are valuable information that encode correlations among different classes. However, their roles may be very small in the loss function as these probabilities are close to zero due to the softmax function. To address this problem, a temperature parameter, T \u2208 R+, may be used to flatten the posterior distribution,\nyjt = exp (zjt/T )\u2211 i exp (zit/T ) , (12)\nzt = WL+1hLt + bL+1, (13)\nwhere WL+1, bL+1 are parameters in the softmax layer. Following [38], we applied the same temperature to the softmax functions in both the teacher and student networks in our experiments.1\nA particular advantage of teacher-student training is that unlabelled data can be used easily. However, when ground truth labels are available, the two loss functions can be interpolated to give a hybrid loss parametrised by q \u2208 R+\nL\u0303(\u03b8) = L(KL)(\u03b8) + qL(CE)(\u03b8). (14)"}, {"heading": "C. Sequence training", "text": "While the previous two loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which usually yields a significant improvement in speech recognition accuracy [39]\u2013[41]. Given a sequence of acoustic frames, X = {x1, . . . ,xT }, of length T , and a sequence of labels, Y , then the loss function from the state-level minimum Bayesian risk criterion (sMBR) [42], [43] is defined as\nL(sMBR)(\u03b8) = \u2211 W\u2208\u03a6 p(X | W)kP (W)A(Y , Y\u0302 )\u2211\nW\u2208\u03a6 p(X | W)kP (W) , (15)\nwhere: A(Y , Y\u0302 ) measures the state-level distance between the ground truth and predicted labels; \u03a6 denotes the hypothesis\n1Only increasing the temperature in the teacher network resulted in much higher error rates in pilot experiments."}, {"heading": "4 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "space represented by a denominator lattice; W is the wordlevel transcription; and k is the acoustic score scaling parameter. In this paper, we focus on the sMBR criterion for sequence training since it can achieve comparable or slightly better results than training using the maximum mutual information (MMI) or minimum phone error (MPE) criteria [40].\nOnly applying the sequence training criterion without regularization may lead to overfitting [40], [41]. To address this problem, we interpolate the sMBR loss function with the CE loss [41], with smoothing parameter p \u2208 R+,\nL(\u03b8) = L(sMBR)(\u03b8) + pL(CE)(\u03b8). (16)\nA motivation for this interpolation is that the acoustic model is usually first trained using CE, and then fine tuned using sMBR for a few iterations. However, in the case of teacher-student training for knowledge distillation, the model is first trained with the KL loss function (11). Hence, we apply the following interpolation when switching from the KL loss function (11) to the sequence-level loss function in the case of teacher-student training:\nL\u0302(\u03b8) = L(sMBR)(\u03b8) + pL(KL)(\u03b8). (17)\nAgain, p \u2208 R+ is the smoothing parameter, and we have used the same ground truth labels Y when measure the sMBR loss as in the the standard sequence training ."}, {"heading": "D. Adaptation", "text": "Adaption of deep neural networks is challenging due to the large number of unstructured model parameters and the small amount of adaptation data. However, the HDNN architecture is more structured as the parameters in the gate functions are layer-independent, and can control the behavior of all the hidden layers. This motivates the investigation of the adaptation of highway gates by only fine tuning these model parameters. Although the number of parameters in the gate functions is still large compared to the amount of per-speaker adaptation data, the size of the gate functions may be controlled by reducing the number of hidden units, but maintaining the accuracy by increasing the depth [14]. Moreover, speaker adaptation can be applied to teacher-student training to further improve the accuracy of the compact HDNN acoustic models."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. System setup", "text": "Our experiments were performed on the individual headset microphone (IHM) subset of the AMI meeting speech transcription corpus [44], [45].2 The amount of training data is around 80 hours, corresponding to roughly 28 million frames. We used 40-dimensional fMLLR adapted features vectors normalised at a per-speaker level, which were then spliced by a context window of 15 frames (i.e. \u00b17) for all the systems. The number of tied HMM states is 3972, and all the DNN systems were trained using the same alignment. The results reported in this paper were obtained using the CNTK toolkit [46] with the Kaldi decoder [47], and the networks were trained\n2http://corpus.amiproject.org\nusing the cross-entropy (CE) criterion without pre-training unless specified otherwise. We set the momentum to be 0.9 after the 1st epoch, and we used the sigmoid activation for the hidden layers. The weights in each hidden layer were randomly initialized with a uniform distribution in the range of [\u22120.5, 0.5] and the bias parameters were initialized to be 0 for CNTK systems. We used a trigram language model for decoding."}, {"heading": "B. Baseline results", "text": "Table I shows the CE and sequence training results for baseline DNN and HDNN models of various size. The DNN systems were all trained using Kaldi with RBM pretraining (without pretraining, training thin and deep DNN models did not converge using CNTK). However, we were able to train HDNNs with random initialization without pretraining, demonstrating that the gate functions in HDNNs facilitate the information flow through the layers. For sequence training, we performed the sMBR update for 4 iterations, and set p = 0.2 in Eq. (16) to avoid overfitting. Table I shows that the HDNNs achieved consistently lower WERs compared to the DNNs; the margin of the gain also increases as the number of hidden units becomes smaller. As the number of hidden units decreases, the accuracy of DNNs degrades rapidly, and the accuracy loss cannot be compensated by increasing the depth of the network. The results also show that sequence training improves the recognition accuracy comparably for both DNN and HDNN systems, and the improvements are consistent for both dev and eval sets. Overall, the HDNN model with around 6 million model parameters has a similar accuracy to the regular DNN system with 30 million model parameters."}, {"heading": "C. Transform and Carry gates", "text": "We then evaluated the specific role of the transform and carry gates in the highway architectures. The results are shown in Table II, where we disabled each of the gates in turn. We can see that using only one of the two gates, the HDNN can still achieve lower WER compared to the regular DNN baseline, but the best results are obtained when both gates are active, indicating that the two gating functions are\n5\ncomplementary. Figure 1 shows the convergence curves of training HDNNs with and without the transform and carry gates. We observe faster convergence when both gates are active, with considerably slower convergence when using only the transform gate. This indicates that the carry gate, which controls the skip connections, is more important to the convergence rate. We also investigated constrained gates, in which C(\u00b7) = 1\u2212T (\u00b7) [17], which reduces the computational cost since the matrix-vector multiplication for the carry gate is not required. We evaluated this configuration with 10-layer neural networks, and the results are also shown in Table II: this approach does not improve recognition accuracy in our experiments.\nTo look into the relative importance of the gate functions to other type of model parameters in the feature extractor and classification layer, we also performed a set of ablation experiments with sequence training, where we removed the update of different sets of model parameters (after CE training). These results are given in Table III, which shows that only updating the parameters in the gates \u03b8g can retain most of the improvement given by sequence training, while updating \u03b8g and \u03b8c can achieve the accuracies close to the optimum. Although \u03b8g only accounts for a small fraction of the total number of parameters (e.g., \u223c 10% for the HDNN-H512L10 system and \u223c 7% for the HDNN-H256L10 system), the results demonstrate that it plays an important role in manipulating the behavior of the neural network feature extractor.\nComplementary to the above experiments, we then investigated the effect of the regularization term for sequence training\nof HDNNs (16). We performed the experiments with and without the CE regularization for two system settings, i.e.: i) update all the model parameters; ii) update only the gate functions. Our motivation was to validate if only updating the gate parameters is more resistant to overfitting. The results are given in Table IV, from which we see that by removing the CE regularization term, we achieved slightly lower WER when updating the gate functions only. However, when updating all model parameters, the regularization term was an important stabilizer for the convergence. Figure 2 shows the convergence curves for the two system settings. Overall, although the gate functions can largely control the behavior of the highway networks, they are not prone to overfitting when other model parameters are switched off."}, {"heading": "D. Adaptation", "text": "The previous experiments show that the gate functions can largely control the behavior of a multi-layer neural network feature extractor with a relatively small number of model parameters. This observation inspired us to study speaker adaptation using the gate functions. Our first experiments explored unsupervised speaker adaptation, in which we decoded the evaluation set using the speaker-independent models, and then used the resulting pseudo-labels to fine-tune the gating parameters (\u03b8g) in the second pass. The evaluation set contained around 8.6 hours of audio, with 63 speakers, an average of"}, {"heading": "6 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "around 8 minutes of speech per speaker, which corresponds to about 50 000 frames. This is a relatively small amount of adaptation data, given the size of \u03b8g (0.5 million parameters in the HDNN-H512L10 system). We set the learning rate to be 2\u00d7 10\u22124 per sample, and we updated \u03b8g for 5 adaptation epochs.\nTable V shows the adaptation results, from which we observe a small but consistent reduction in WER for different model configurations (both CE and sMBR trained) when using fMLLR speaker adapted features. The results indicate that updating all the model parameters yields smaller improvements. With speaker adaptation and sequence training, the HDNN system with 5 million model parameters (HDNN-H512L10) works slightly better than the DNN baseline with 30 million parameters (24.1% from row 5 of Table V vs. 24.6% from row 1 of Table I), while the HDNN model with 2 million parameters (HDNN-H256L10) has only a slightly higher WER compared to the baseline (25.0% from row 6 of Table V vs. 24.6% from row 1 of Table I). In Figure 3 we show the adaptation results for a different number of iterations. We observe that the best results can be achieved after 2 or 3 adaptation iterations; further updating the gate functions \u03b8g does not result in overfitting. For validation we performed experiments with 10 adaptation iterations, and again we did not observe overfitting. This observation is in line with the sequence training experiments, demonstrating that the gate functions are relatively resistant to overfitting.\nIn order to evaluate the impact of the accuracy of the\nlabels to this adaptation method as well as the memorization capacity of the highway gate units, we performed a set of diagnostic experiments, in which we used the oracle labels for adaptation. We obtained the oracle labels from a forced alignment using the DNN model trained with the CE criterion and word level transcriptions. We used this fixed alignment for all the adaptation experiments in order to compare the different seed models. Figure 4 shows the adaptation results with oracle labels, suggesting that an increased reduction in WER may be achieved when the supervision labels are more accurate. In the future, we shall investigate the model for domain adaptation,\n7 where the amount of adaptation data is usually relatively larger, and the ground truth labels are available."}, {"heading": "E. Teacher-Student training", "text": "After sequence training and adaptation, the HDNN with 2 million model parameters has a similar accuracy to the DNN baseline with 30 million model parameters. However, the model HDNN-H128L10 which has fewer than 0.8 million model parameters has a substantially higher WER compared to the DNN baseline (28.7% from row 9 of Table V vs. 24.6% from row 1 of Table I). We investigated if the accuracy of the small HDNN model can be further improved using teacherstudent training. We first compare the teacher-student loss function (11) and the hybrid loss function (14). We used a CE trained DNN-H2048L6 as the teacher model, and used the HDNN-H128L10 as the student model. Figure 5 shows the convergence curves when training the model with the different loss functions, while Table VI shows the WERs. We observe that teacher-student training without the ground truth labels can achieve a significantly lower frame error rate on the cross validation set (Figure 5) which corresponds to a moderate WER reduction (Table VI: 31.3% vs. 32.0% on the eval set). However, using the hybrid loss function (14) does not result in further improvement, and when q > 0 during training convergence is slower (Figure 5). We interpret this result as indicating that the probabilities of uncorrected classes may play a lesser role, which supports the argument that they encode useful information for training the student model [38]. This hypothesis encouraged us to investigate the use of a high temperature to flatten the posterior probability distribution of the labels from the teacher model. The results are shown in Table VI; contrary to our expectation, using high temperatures results in higher WERs. In the following experiments, we fixed q = 0 and T = 1.\nWe then improved the teacher model by sMBR sequence training, and used this model to supervise the training of\nnumber of iterations 0 1 2 3 4 5 6 7 8 9 10\nW or\nd Er\nro r R\nat e\n(% )\n18\n20\n22\n24\n26\n28 30 HDNN-H512L10-CE HDNN-H256L10-CE HDNN-H256L10-sMBR HDNN-H512L10-sMBR\nFig. 4. Supervised adaptation results with oracle labels.\nthe student model. We found that the sMBR-based teacher model can significantly improve the performance of the student model (similar to the results reported in [8]). In fact, the error rate is lower than that achieved by the student model trained independently with sMBR (28.8% from row 2 of Table VII vs. 29.4% from row 9 of Table I on the eval set). Note that, since the sequence training criterion does not maximize the frame accuracy, training the model with this criterion often reduces the frame accuracy (see Figure 6 of [48]). Interestingly, we observed the same pattern in the case of teacher-student training of HDNNs. Figure 6 shows the convergence curves of using CE and sMBR based teacher models, where we see that the student model achieves much higher frame error rate on the cross validation set when supervised by sMBR-based teacher model, although the loss function (11) is at the frame level.\nWe then investigated whether the accuracy of the student model can be further improved by the sequence level criterion. Here, we set the smoothing parameter p = 0.2 in (17) and the default learning rate to be 10\u22125 following our previous work [15]. Table VII shows sequence training results for student models supervised by both CE and sMBR-based teacher models. Surprisingly, the student model supervised by the CE-based DNN model can be significantly improved by sequence training \u2013 the WER obtained by this approach\n8 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017\nis lower compared to the model trained independently with sMBR (28.4% from row 1 of Table VII vs. 29.4% from row 9 of Table I on the eval set). However, this configuration did not improve the student model supervised by an sMBR-based teacher model. After inspection, we found that this was due to overfitting. We then increased the value of p to enable stronger regularization and reduced the learning rate. Lower WERs were obtained as the table shows; however, the improvement is less significant as the sequence level information has already been integrated into the teacher model."}, {"heading": "F. Teacher-Student training with adaptation", "text": "We then performed similar adaptation experiments to section IV-D for HDNNs trained by the teacher-student approach. We applied the second-pass adaptation approach for the standalone HDNN model, i.e., we decoded the evaluation utterances to obtain the hard labels first, and then used these labels to adapt the model using the CE loss (10). However, when using the teacher-student loss (11) only one-pass decoding is required because the pseudo-labels for adaptation are provided by the teacher, which does not need a word level transcription. This is a particular advantage of the teacherstudent training technique. However, for resource-constrained application scenarios, the student model should be adapted offline, because otherwise the teacher model needs to be accessed to generate the labels. This requires another set of unlabelled speaker-dependent data for adaptation, which is usually not expensive to collect.\nSince the standard AMI corpus does not have an additional set of speaker-dependent data, we only show online adaptation results. We used the teacher-student trained model from row 1 of Table VII as the speaker-independent (SI) model because its pipeline is much simpler. The baseline system used the same network as the SI model, but it was trained independently. During adaptation, we updated the SI model using 5 iterations with a fixed learning rate of 2 \u00d7 10\u22124 per sample following our previous setup [15]. We also compared the CE loss (10) and the teacher-student loss (11) for adaptation (Table VIII). When using the CE loss function for both SI models, slightly better results wer obtained when updating the gates only, while updating all the model parameters gave smaller improvements, possibly due to overfitting. Interestingly, this is not the case for the teacher-student loss, where updating all the model parameters yielded lower WER. These results are also in line with the argument in [38] that the soft targets can work as a regularizer and can prevent the student model from overfitting."}, {"heading": "G. Summary", "text": "We summarize our key results in Table IX. Overall, the HDNN acoustic model can slightly outperform the sequence trained baseline using around 5 million model parameters after adapting the gate functions; using fewer than 2 million model parameters it performed slightly worse. If fewer than 0.8 million parameters are used, then the gap is much larger\n9\ncompared to the DNN baseline. With adaptation and teacherstudent training, we can close the gap by around 50%, with difference in WER falling from roughly 5% absolute to 2.5% absolute."}, {"heading": "V. CONCLUSIONS", "text": "Highway deep neural networks are structured, depth-gated feedforward neural networks. In this paper, we studied sequence training and adaptation of these networks for acoustic modeling. In particular, we investigated the roles of the parameters in the hidden layers, gate functions and classification layer in the case of sequence training. We show that the gate functions, which only account for a small fraction of the whole parameter set, are able to control the information flow and adjust the behavior of the neural network feature extractors. We demonstrate this in both sequence training and adaptation experiments, in which considerable improvements were achieved by only updating the gate functions. Using these techniques, we obtained comparable or slightly lower WERs with much smaller acoustic models compared to a strong baseline set by a conventional DNN acoustic model with sequence training. Since the number of model parameters is still relatively large compared to the amount of data typically used for speaker adaptation, this adaptation technique may be more applicable to domain adaptation, where the expected amount of adaptation data is larger.\nFurthermore, we also investigated teacher-student training for small-footprint acoustic models using HDNNs. We observed that the accuracy of the student acoustic model could be improved under the supervision of a high accuracy teacher model, even without additional unsupervised data. In particular, the student model supervised by an sMBRbased teacher model achieved lower WER compared to the model trained independently using the sMBR-based sequence training approach. Unsupervised speaker adaptation further improved the recognition accuracy by around 5% relative for a model with fewer then 0.8 million model parameters. However, we did not obtain improvements either using a hybrid loss function which interpolates the CE and teacher-student loss functions, or using a higher temperature to smooth the pseudolabels. In the future, we shall evaluate this model in low resource conditions where the amount of training data is much smaller."}, {"heading": "VI. ACKNOWLEDGEMENT", "text": "We thank the NVIDIA Corporation for the donation of a Titan X GPU, and the anonymous reviewers for insightful comments and suggestions that helped to improve the quality of this paper."}, {"heading": "10 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "[23] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256. [24] Geoffrey E Hinton and Ruslan R Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006. [25] Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al., \u201cGreedy layer-wise training of deep networks,\u201d in Proc. NIPS, 2007, vol. 19, p. 153. [26] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu, \u201cDeeply-supervised nets,\u201d arXiv preprint arXiv:1409.5185, 2014. [27] S Ioffe and C Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d in ICML, 2015. [28] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv preprint arXiv:1207.0580, 2012. [29] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014. [30] Pawel Swietojanski and Steve Renals, \u201cLearning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models,\u201d in Proc. SLT. IEEE, 2014, pp. 171\u2013176. [31] P Swietojanski, J Li, and S Renals, \u201cLearning hidden unit contributions for unsupervised acoustic model adaptation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1450\u2013 1463, 2016. [32] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. [33] Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, and James Glass, \u201cHighway Long Short-Term Memory RNNs for Distant Speech Recognition,\u201d Proc. ICASSP, 2015. [34] Julian Georg. Zilly, Rupesh Kumar Srivastava, Koutnik Jan, and Ju\u0308rgen Schmidhuber, \u201cRecurrent highway networks,\u201d arXiv preprint arXiv:1607.03474, 2016. [35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015. [36] Karen Simonyan and Andrew Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d in Proc. ICLR, 2015. [37] Yanmin Qian, Mengxiao Bi, Tian Tan, and Kai Yu, \u201cVery deep convolutional neural networks for noise robust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016. [38] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistilling the knowledge in a neural network,\u201d in Proc. NIPS Deep Learning and Representation Learning Workshop, 2015. [39] Brian Kingsbury, Tara N Sainath, and Hagen Soltau, \u201cScalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization,\u201d in Proc. INTERSPEECH, 2012. [40] K Vesely\u0301, A Ghoshal, L Burget, and D Povey, \u201cSequence-discriminative training of deep neural networks,\u201d in Proc. INTERSPEECH, 2013. [41] Hang Su, Gang Li, Dong Yu, and Frank Seide, \u201cError back propagation for sequence training of context-dependent deep networks for conversational speech transcription,\u201d in Proc. ICASSP. IEEE, 2013, pp. 6664\u20136668. [42] Matthew Gibson and Thomas Hain, \u201cHypothesis spaces for minimum bayes risk training in large vocabulary speech recognition.,\u201d in Proc. INTERSPEECH. Citeseer, 2006. [43] Brian Kingsbury, \u201cLattice-based optimization of sequence classification criteria for neural-network acoustic modeling,\u201d in Proc. ICASSP. IEEE, 2009, pp. 3761\u20133764. [44] Steve Renals, Thomas Hain, and Herve\u0301 Bourlard, \u201cRecognition and understanding of meetings the AMI and AMIDA projects,\u201d in Proc. ASRU. IEEE, 2007, pp. 238\u2013247. [45] S Renals and P Swietojanski, \u201cDistant speech recognition experiments using the AMI Corpus,\u201d in New Era for Robust Speech Recognition \u2013 Exploting Deep Learning, S Watanabe, M Delcroix, F Metze, and JR Hershey, Eds. Springer, 2016. [46] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, et al., \u201cAn introduction to computational networks and the computational network toolkit,\u201d Tech. Rep., Tech. Rep. MSR, Microsoft Research, 2014. [47] D Povey, A Ghoshal, G Boulianne, L Burget, O Glembek, N Goel, M Hannemann, P Motl\u0131cek, Y Qian, P Schwarz, J Silovsky\u0301, G Semmer, and K Vesely\u0301, \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. ASRU, 2011. [48] Georg Heigold, Erik McDermott, Vincent Vanhoucke, Andrew Senior, and Michiel Bacchiani, \u201cAsynchronous stochastic optimization for sequence training of deep neural networks,\u201d in Proc. ICASSP. IEEE, 2014, pp. 5587\u20135591. Liang Lu a Research Assistant Professor at the Toyota Technological Institute at Chicago. He received his Ph.D. degree from the University of Edinburgh in 2013, where he then worked as a Postdoctoral Research Associate until 2016 before moving to Chicago. He has a broad research interest in the field of speech and language processing. He received the best paper award for his work on the low-resource pronunciation modeling at the 2013 IEEE ASRU workshop. Steve Renals (M\u201991 \u2014 SM\u201911 \u2014 F\u201914) received the B.Sc. degree from the University of Sheffield, Sheffield, U.K., and the M.Sc. and Ph.D. degrees from the University of Edinburgh. He is Professor of Speech Technology at the University of Edinburgh, having previously had positions at ICSI Berkeley, the University of Cambridge, and the University of Sheffield. He has research interests in speech and language processing. He is a fellow of ISCA."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath", "Brain Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "Interspeech, 2011, pp. 437\u2013440.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "The IBM 2016 English Conversational Telephone Speech Recognition System", "author": ["George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J. Kuo"], "venue": "Proc. INTERSPEECH, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist speech recognition: a hybrid", "author": ["Herve A Bourlard", "Nelson Morgan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["Steve Renals", "Nelson Morgan", "Herv\u00e9 Bourlard", "Michael Cohen", "Horacio Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 2, no. 1, pp. 161\u2013174, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "Proc. INTERSPEECH, 2013, pp. 2365\u20132369.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning smallsize DNN with output-distribution-based criteria", "author": ["Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong"], "venue": "Proc. INTER- SPEECH, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "Proc. NIPS, 2014, pp. 2654\u20132662.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero Adriana", "Ballas Nicolas", "Kahou Samira Ebrahimi", "Chassang Antoine", "Gatta Carlo", "Bengio Yoshua"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Quoc Le", "Tam\u00e1s Sarl\u00f3s", "Alex Smola"], "venue": "Proc. ICML, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara N Sainath", "Sanjiv Kumar"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "ACDC: A Structured Efficient Linear Layer", "author": ["Marcin Moczulski", "Misha Denil", "Jeremy Appleyard", "Nando de Freitas"], "venue": "Proc. ICLR, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Small-footprint deep neural networks with highway connections for speech recognition", "author": ["Liang Lu", "Steve Renals"], "venue": "Proc. INTERSPEECH, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence training and adaptation of highway deep neural networks", "author": ["Liang Lu"], "venue": "Proc. SLT, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge distillation for small-footprint highway networks", "author": ["Liang Lu", "Michelle Guo", "Steve Renals"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "Proc. INTERSPEECH, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-Rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 22, no. 10, pp. 1533\u20131545, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Transferring knowledge from a RNN to a DNN", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "Proc. INTERSPEECH, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence student-teacher training of deep neural networks", "author": ["Jeremy HM Wong", "Mark JF Gales"], "venue": "Proc. INTERSPEECH. 2016, International Speech Communication Association.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Dumitru Erhan", "Pierre-Antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": "International Conference on artificial intelligence and statistics, 2009, pp. 153\u2013160.  10  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Proc. NIPS, 2007, vol. 19, p. 153.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Deeply-supervised nets", "author": ["Chen-Yu Lee", "Saining Xie", "Patrick Gallagher", "Zhengyou Zhang", "Zhuowen Tu"], "venue": "arXiv preprint arXiv:1409.5185, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S Ioffe", "C Szegedy"], "venue": "ICML, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1929}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["Pawel Swietojanski", "Steve Renals"], "venue": "Proc. SLT. IEEE, 2014, pp. 171\u2013176.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hidden unit contributions for unsupervised acoustic model adaptation", "author": ["P Swietojanski", "J Li", "S Renals"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1450\u2013 1463, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}, {"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yao", "Sanjeev Khudanpur", "James Glass"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent highway networks", "author": ["Julian Georg. Zilly", "Rupesh Kumar Srivastava", "Koutnik Jan", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for noise robust speech recognition", "author": ["Yanmin Qian", "Mengxiao Bi", "Tian Tan", "Kai Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "Proc. NIPS Deep Learning and Representation Learning Workshop, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization", "author": ["Brian Kingsbury", "Tara N Sainath", "Hagen Soltau"], "venue": "Proc. INTERSPEECH, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K Vesel\u00fd", "A Ghoshal", "L Burget", "D Povey"], "venue": "Proc. INTERSPEECH, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["Hang Su", "Gang Li", "Dong Yu", "Frank Seide"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6664\u20136668.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Hypothesis spaces for minimum bayes risk training in large vocabulary speech recognition", "author": ["Matthew Gibson", "Thomas Hain"], "venue": "Proc. INTERSPEECH. Citeseer, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Proc. ICASSP. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Recognition and understanding of meetings the AMI and AMIDA projects", "author": ["Steve Renals", "Thomas Hain", "Herv\u00e9 Bourlard"], "venue": "Proc. ASRU. IEEE, 2007, pp. 238\u2013247.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Distant speech recognition experiments using the AMI Corpus", "author": ["S Renals", "P Swietojanski"], "venue": "New Era for Robust Speech Recognition \u2013 Exploting Deep Learning, S Watanabe, M Delcroix, F Metze, and JR Hershey, Eds. Springer, 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Dong Yu", "Adam Eversole", "Mike Seltzer", "Kaisheng Yao", "Zhiheng Huang", "Brian Guenter", "Oleksii Kuchaiev", "Yu Zhang", "Frank Seide", "Huaming Wang"], "venue": "Tech. Rep., Tech. Rep. MSR, Microsoft Research, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "The Kaldi speech recognition toolkit", "author": ["D Povey", "A Ghoshal", "G Boulianne", "L Burget", "O Glembek", "N Goel", "M Hannemann", "P Motl\u0131cek", "Y Qian", "P Schwarz", "J Silovsk\u00fd", "G Semmer", "K Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "DEEP Learning has significantly advanced the state-ofthe-art in speech recognition over the past few years [1]\u2013 [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "DEEP Learning has significantly advanced the state-ofthe-art in speech recognition over the past few years [1]\u2013 [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "and hidden Markov model (NN/HMM) hybrid architecture, first investigated in the early 1990s [4], [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "and hidden Markov model (NN/HMM) hybrid architecture, first investigated in the early 1990s [4], [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "In this paper, we present a comprehensive study of smallfootprint acoustic models using highway deep neural networks (HDNNs), building on our previous studies [14]\u2013[16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 15, "context": "In this paper, we present a comprehensive study of smallfootprint acoustic models using highway deep neural networks (HDNNs), building on our previous studies [14]\u2013[16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "HDNNs are multi-layer networks which have shortcut connections between hidden layers [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "These gate functions are central to training very deep networks [17] and to speeding up convergence [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "These gate functions are central to training very deep networks [17] and to speeding up convergence [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Besides, in contrast to training regular multi-layer networks of the same depth and width, which typically requires careful pretraining, we demonstrate that HDNNs may be trained using standard stochastic gradient descent without any pretraining [14].", "startOffset": 245, "endOffset": 249}, {"referenceID": 16, "context": "Furthermore, The authors in [17] only studied the constrained carry gate setting for HDNNs, while in this work we provide detailed comparisons of different gate functions in the context of speech recognition.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "term memory units (LSTM-RNNs) and convolutional neural networks (CNNs) can obtain higher recognition accuracy with fewer model parameters compared to DNNs [18], [19], they are computationally more expensive for applications on resource constrained platforms.", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "term memory units (LSTM-RNNs) and convolutional neural networks (CNNs) can obtain higher recognition accuracy with fewer model parameters compared to DNNs [18], [19], they are computationally more expensive for applications on resource constrained platforms.", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "Moreover, their accuracy can be transferred to a DNN by teacher-student training [?], [20],", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "However, as the number of hidden layers increases, the error surface becomes increasingly non-convex, and it becomes more likely to find a poor local minimum using gradientbased optimization algorithms with random initialization [22].", "startOffset": 229, "endOffset": 233}, {"referenceID": 22, "context": "Furthermore the variance of the back-propagated gradients may become small in the lower layers if the model parameters are not initialized properly [23].", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "There have been a variety of training algorithms, and model architectures, proposed to enable very deep multi-layer networks including pre-training [24], [25], normalised initial-", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "There have been a variety of training algorithms, and model architectures, proposed to enable very deep multi-layer networks including pre-training [24], [25], normalised initial-", "startOffset": 154, "endOffset": 158}, {"referenceID": 22, "context": "isation [23], deeply-supervised networks [26], and batch normalisation [27].", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "isation [23], deeply-supervised networks [26], and batch normalisation [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "isation [23], deeply-supervised networks [26], and batch normalisation [27].", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Highway deep neural networks (HDNNs) [17] were proposed to enable very deep networks to be trained by augmenting the hidden layers with gate functions:", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The outputs of T (\u00b7) and C(\u00b7) are constrained to be within [0, 1], and we use a sigmoid function for each, parameterized by WT and Wc respectively.", "startOffset": 59, "endOffset": 65}, {"referenceID": 13, "context": "Following our previous work [14], we tie the parameters in the gate functions across all the hidden layers, which can significantly save model parameters.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "tion [28], which may be written as", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "where p( i) is a Bernoulli distribution for the i-th element in as originally proposed in [28]; it was shown later that using a continuous distribution with well designed mean and", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "variance works as well or better [29].", "startOffset": 33, "endOffset": 37}, {"referenceID": 29, "context": "models [30], [31], which may be represented as", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "models [30], [31], which may be represented as", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "Both HDNNs and LSTM-RNNs [32] employ gate functions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 32, "context": "Combinations of the two architectures have been explored recently: highway LSTMs [33] employ highway connections to train a stacked LSTM with multiple layers; recurrent highway networks [34] share gate functions to control the information flow in both time and model depth.", "startOffset": 81, "endOffset": 85}, {"referenceID": 33, "context": "Combinations of the two architectures have been explored recently: highway LSTMs [33] employ highway connections to train a stacked LSTM with multiple layers; recurrent highway networks [34] share gate functions to control the information flow in both time and model depth.", "startOffset": 186, "endOffset": 190}, {"referenceID": 34, "context": "residual network (ResNet) [35] was recently proposed to train very deep networks, advancing the state-of-the-art in computer vision.", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "adapting approaches developed for visual object recognition [36], very deep CNN architectures have been investigated for speech recognition [37].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "adapting approaches developed for visual object recognition [36], very deep CNN architectures have been investigated for speech recognition [37].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Minimizing this loss function is equivalent to minimizing the Kullback-Leibler (KL) divergence between the posterior probabilities of each class from the teacher and student models [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 37, "context": "[38] suggested that the small posterior probabilities are valuable information that encode correlations among different classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Following [38], we applied the same temperature to the softmax functions in both the teacher and student networks in our experiments.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "While the previous two loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which usually yields a significant improvement in speech recognition accuracy [39]\u2013[41].", "startOffset": 206, "endOffset": 210}, {"referenceID": 40, "context": "While the previous two loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which usually yields a significant improvement in speech recognition accuracy [39]\u2013[41].", "startOffset": 211, "endOffset": 215}, {"referenceID": 41, "context": ",xT }, of length T , and a sequence of labels, Y , then the loss function from the state-level minimum Bayesian risk criterion (sMBR) [42], [43] is defined as", "startOffset": 134, "endOffset": 138}, {"referenceID": 42, "context": ",xT }, of length T , and a sequence of labels, Y , then the loss function from the state-level minimum Bayesian risk criterion (sMBR) [42], [43] is defined as", "startOffset": 140, "endOffset": 144}, {"referenceID": 39, "context": "In this paper, we focus on the sMBR criterion for sequence training since it can achieve comparable or slightly better results than training using the maximum mutual information (MMI) or minimum phone error (MPE) criteria [40].", "startOffset": 222, "endOffset": 226}, {"referenceID": 39, "context": "Only applying the sequence training criterion without regularization may lead to overfitting [40], [41].", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "Only applying the sequence training criterion without regularization may lead to overfitting [40], [41].", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "To address this problem, we interpolate the sMBR loss function with the CE loss [41], with smoothing parameter p \u2208 R,", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Although the number of parameters in the gate functions is still large compared to the amount of per-speaker adaptation data, the size of the gate functions may be controlled by reducing the number of hidden units, but maintaining the accuracy by increasing the depth [14].", "startOffset": 268, "endOffset": 272}, {"referenceID": 43, "context": "Our experiments were performed on the individual headset microphone (IHM) subset of the AMI meeting speech transcription corpus [44], [45].", "startOffset": 128, "endOffset": 132}, {"referenceID": 44, "context": "Our experiments were performed on the individual headset microphone (IHM) subset of the AMI meeting speech transcription corpus [44], [45].", "startOffset": 134, "endOffset": 138}, {"referenceID": 45, "context": "The results reported in this paper were obtained using the CNTK toolkit [46] with the Kaldi decoder [47], and the networks were trained", "startOffset": 72, "endOffset": 76}, {"referenceID": 46, "context": "The results reported in this paper were obtained using the CNTK toolkit [46] with the Kaldi decoder [47], and the networks were trained", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "We also investigated constrained gates, in which C(\u00b7) = 1\u2212T (\u00b7) [17], which reduces the computational cost since the matrix-vector multiplication for the carry gate is not required.", "startOffset": 64, "endOffset": 68}, {"referenceID": 37, "context": "may play a lesser role, which supports the argument that they encode useful information for training the student model [38].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "We found that the sMBR-based teacher model can significantly improve the performance of the student model (similar to the results reported in [8]).", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "2 in (17) and the default learning rate to be 10\u22125 following our previous work [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "During adaptation, we updated the SI model using 5 iterations with a fixed learning rate of 2 \u00d7 10\u22124 per sample following our previous setup [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 37, "context": "These results are also in line with the argument in [38] that the soft targets can work as a regularizer and can prevent the student model from overfitting.", "startOffset": 52, "endOffset": 56}], "year": 2017, "abstractText": "State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data.", "creator": "LaTeX with hyperref package"}}}