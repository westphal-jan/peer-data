{"id": "1610.05858", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "abstract": "extraction inclusion of metadata concepts explicitly present in patient clinical records is an obvious essential step in clinical productivity research. the 2010 i2b2 / va workshop on natural language processing challenges for clinical records presented concept for extraction ( ce ) task, with aim to identify concepts ( such domains as treatments, tests, problems ) and classify comparing them into predefined categories. state - of - the - art ce approaches heavily rely on hand crafted hardware features and domain specific resources which are hard to collect recognize and tune. for this reason, this paper employs bidirectional lstm with basic crf decoding initialized with most general purpose off - the - shelf word file embeddings for ce. the experimental results achieved on 2010 iec i2b2 / va reference standard corpora using bidirectional tools lstm crf ranks closely with top 15 ranked systems.", "histories": [["v1", "Wed, 19 Oct 2016 03:59:30 GMT  (14kb)", "http://arxiv.org/abs/1610.05858v1", "This paper \"Bidirectional LSTM-CRF for Clinical Concept Extraction\" is accepted for short paper presentation at Clinical Natural Language Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016"]], "COMMENTS": "This paper \"Bidirectional LSTM-CRF for Clinical Concept Extraction\" is accepted for short paper presentation at Clinical Natural Language Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raghavendra chalapathy", "ehsan zare borzeshi", "massimo piccardi"], "accepted": false, "id": "1610.05858"}, "pdf": {"name": "1610.05858.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rcha9612@uni.sydney.edu.au", "ezborzeshi@cmcrc.com", "Massimo.Piccardi@uts.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n05 85\n8v 1\n[ cs\n.C L\n] 1\n9 O\nct 2\n01 6"}, {"heading": "1 Introduction", "text": "Patient clinical records contain longitudinal record of patient health, disease, test\u2019s conducted and response to treatment, often useful for epidemiologic and clinical research. Thus extracting these information has been of immense value for both clinical practise and to improve quality of patient care provided while reducing healthcare costs. Concept extraction (CE) aims to identify medical concept mentions such as problems, test, treatments in clinical records (Eg: discharge summaries, progress reports) and classify them into pre-defined categories. The concepts in clinical records are often expressed with unstructured free text, rendering their extraction a daunting task for clinical Natural Language Processing (NLP) systems. The CE problem is analogous to well-studied Named Entity Recognition (NER) task in general NLP domain. Traditional approaches to extracting concepts relied on rule based systems or dictionaries (lexicon\u2019s) using string comparision to recognise concepts of interest. The concepts represent drug names, anatomical nomenclature, other specialised names and phrases which are not part of mundane English vocabulary. For instance \u201dresp status\u201d should be interpreted as \u201dresponse status\u201d. Furthermore the use of abbreviated phrases are very common among medical fraternity and many of these abbreviations have alternative meanings in other genres of English. Intrinsically, rule based systems are hard to scale, and ineffective in the presence of informal sentences and abbreviated phrases (Liu et al., 2015). Dictionary based systems perform a fast look-up from medical ontologies such as Unified Medical Language System (UMLS) to extract concepts (Kipper-Schuler et al., 2008). Although these systems achieve high precison but suffer from low recall ( i,e they may not identify significant number of concepts) due to missplelled words or medical jargons not present in dictionaries. To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998). However these state-of-the-art ML approaches follow two step process of domain specific feature engineering and classification, which are highly dedicated hand-crafted systems and require labour intensive expert knowledge. For this reason, this paper employs bidirectional LSTM-CRF intialized with general purpose off-the-shelf neural word embeddings derived from Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) for automatic feature learning thus avoiding time-consuming feature engineering, which deliver system performance comparable to the best submissions from the 2010 i2b2/VA challenge.\nMost of the research to date have formulated CE as a sequence labelling NER problem employing various supervised and semi-supervised ML algorithms employing focussed domain-dependent attributes and specialized text features (Uzuner et al., 2011). Similarly hybrid models obtained by cascading CRF and SVM algorithms along with several pattern matching rules are shown to produce effective results (Boag et al., 2015). The efficacy of including pre-processing technique (such as truecasing and annotation combination) along with CRF based NER system to improve concept extraction performace was exemplified by (Fu and Ananiadou, 2014). The best performing system for 2010 i2b2/VA concept extract task adopted unsupervised feature representations derived from unlabeled corpora using Brown clustering technique along with semi-supervised Markov HMM models (de Bruijn et al., 2011). However, the unsupervised one-hot word feature representations derived from Brown clustering fails to capture multiple aspect relation between words. Subsequently (Jonnalagadda et al., 2012) demonstrated that random indexing model with distributional word representations improve clinical concept extraction. With recent success of incorporating word embeddings derived from the entire English wikipedia in various NER task (Collobert et al., 2011), binarized word embeddings derived from domain specific copora (Eg: Monitoring in Intensive Care (MIMIC) II corpus) has improved performance of CRF based concept extraction system (Wu et al., 2015). In the broader field of machine learning, the recent years have witnessed proliferation of deep neural networks, with unprecedented results in tasks such as visual, speech and NER. One of the main advantages of neural networks is that they learn features automatically thus avoiding laborious feature engineerin. Given these promising results obtained the main goal of this paper is to employ bidirectional LSTM CRF intialized with general off-the-shelf unsupervised word embeddings derived from Glove and Word2Vec models and evaluate its performance. The experimental results obtained on 2010 i2b2/VA reference standard corpora without use of any extensive feature engineering and domain specific resources is very encouraging."}, {"heading": "3 The Proposed Approach", "text": "CE can be formulated as a joint segmentation and classification task over a predefined set of classes. As an example, consider the input sentence provided in Table 1. The notation follows the widely adopted in/out/begin (IOB) entity representation with, in this instance, HCT as the test, 2U PRBC as the treatment. In this paper, we approach the CE task by bidirectional LSTM CRF and we therefore provide a brief description hereafter. In a bidirectional LSTM CRF, each word in the input sentence is first mapped to a random real-valued vector of arbitrary dimension, d. Then, a measurement for the word, noted as x(t), is formed by concatenating the word\u2019s own vector with a window of preceding and following vectors (the\nw3(t) = [His,HCT, dropped],\n\u2018His\u2032 \u2192 xHCT \u2208 R d,\n\u2018HCT \u2032 \u2192 xHis \u2208 R d,\n\u2018dropped\u2032 \u2192 xdropped \u2208 R d,\nx(t) = [xHis, xHCT, xdropped] \u2208 R 3d\n(1)\nwhere w3(t) is the context window centered around the t-th word, \u2032HCT \u2032, and xword represents the numerical vector for word."}, {"heading": "3.1 Word Embeddings", "text": "Word embeddings are dense vector representations of natural language words that preserves the semantic and syntactic similarities between them. The vector representations could be generated by either count based such as Hellinger-PCA (Lebret and Collobert, 2013), direct prediction models such as Word2Vec comprising of Skip-gram or Common Bag of Words (CBOW) or Glove word embeddings. Glove vector representations captures complex patterns beyond word similarity through by combining efficient use of word co-occurance statistics and generate a global vector representation for any given word."}, {"heading": "3.2 Bidirectional LSTM-CRF Networks", "text": "The LSTM was designed to overcome this limitation by incorporating a gated memory-cell to capture long-range dependencies within the data (Hochreiter and Schmidhuber, 1997). In the bidirectional LSTM, for any given sentence, the network computes both a left, \u2212\u2192 h (t), and a right, \u2190\u2212 h (t), representations of the sentence context at every input, x(t). The final representation is created by concatenating them as h(t) = [ \u2212\u2192 h (t); \u2190\u2212 h (t)]. All these networks utilize the h(t) layer as an implicit feature for entity class prediction: although this model has proved effective in many cases, it is not able to provide joint decoding of the outputs in a Viterbi-style manner (e.g., an I-group cannot follow a B-brand; etc). Thus, another modification to the bidirectional LSTM is the addition of a conditional random field (CRF) (Lafferty et al., 2001) as the output layer to provide optimal sequential decoding. The resulting network is commonly referred to as the bidirectional LSTM-CRF (Lample et al., 2016)."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks, one among them is concept extraction task focused on the extraction of medical concepts from patient reports. A total of 394 training reports, 477 test reports, and 877 unannotated reports\nwere de-identified and released to challenge participants with data use agreements (Uzuner et al., 2011). However part of that data set is no longer being distributed due to Institutional Review Board (IRB) restrictions. Table 2 summarizes the basic statistics of the training and test datasets used in our experiments. We split training dataset into a training and validation sets with approximately 70% of sentences for training and the remaining for validation."}, {"heading": "4.2 Evaluation Methodology", "text": "Our models have been blindly evaluated on unseen 2010 i2b2/VA CE test data using the strict evaluation metrics. With this evaluation, the predicted entities have to match the ground-truth entities exactly, both in boundary and class. To facilitate the replication of our experimental results, we have used a publiclyavailable library for the implementation (i.e., the Theano neural network toolkit (Bergstra et al., 2010)) and we publicly release our code1. The experiments have been run over a range of values for the hyperparameters, using the validation set for selection (Bergstra and Bengio, 2012). The hyper-parameters include the number of hidden-layer nodes, H \u2208 {25, 50, 100}, the context window size, s \u2208 {1, 3, 5}, and the embedding dimension, d \u2208 {50, 100, 300, 500, 1000}. Two additional parameters, the learning and drop-out rates, were sampled from a uniform distribution in the range [0.05, 0.1]. To begin with, the embedding and initial weight matrices were all randomly initialized from the uniform distribution within range [\u22121, 1] subsequently word embeddings with d = 300 derived from Word2Vec and Glove was utilized in the experiments. Early training stopping was set to 100 epochs to mollify over-fitting, and the model that gave the best performance on the validation set was retained. The accuracy is reported in terms of micro-average F1 score computed using the CoNLL score function (Nadeau and Sekine, 2007)."}, {"heading": "4.3 Results and Analysis", "text": "Table 3 shows the performance comparison between the employed bidirectional LSTM-CRF and stateof-the-art CE systems. As an overall note, the bidirectional LSTM-CRF have not reached the same accuracy as the top system, semi-supervised Markov HMM (de Bruijn et al., 2011). However, our approach has achieved the second-best score on 2010 i2b2/VA. These results seem interesting on the ground that the bidirectional LSTM-CRF provide CE without utilizing any manually-engineered features. Given that our system learn entirely from the data, it is also robust to any new concept or unseen words additions. In our current experimental setting about 20% of tokens were either alpha-numeric or abbreviated strings whose Word2Vec or Glove pretrained vector embeddings were not available. These special strings in text were randomly initialized with d = 300 vector embeddings and input to bidirectional LSTM-CRF system. Subsequently the system was able to learn meaningfull representations with remaining 80% of pre-trained vector embeddings and produce comparable results to the state-of-the-art CE systems.\nConclusion\nThis paper has used the contemporary bidirectional LSTM-CRF, for clinical concept extraction. The most appealing feature of this sytem is their ability to provide end-to-end recognition initialized with general purpose off-the-shelf word embeddings sparing effort from laborious feature construction. To the best of our knowledge, ours is the first paper to adopt bidirectional LSTM-CRF for concept extraction from clinical records. The experimental results over the 2010 i2b2/VA reference standard corpora look promising, with the bidirectional LSTM-CRF ranking closely to the state of the art. A potential way to further improve its performance would be to initialize its training with unsupervised word embeddings such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) trained with domain specific resources such as Monitoring in Intensive Care (MIMIC) II copora. This approach has proved effective in many other domains and still dispenses with expert annotation effort; we plan this exploration for the near future.\n1https://github.com/raghavchalapathy/Bidirectional-LSTM-CRF-for-Clinical-Concept-Extraction\n[Berger et al.1996] Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39\u201371.\n[Bergstra and Bengio2012] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281\u2013305.\n[Bergstra et al.2010] James Bergstra, Olivier Breuleux, Fre\u0301de\u0301ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: A CPU and GPU math compiler in Python. In The 9th Python in Science Conference, pages 1\u20137.\n[Boag et al.2015] William Boag, Kevin Wacome, and MS Tristan Naumann. 2015. Cliner: A lightweight tool for clinical named entity recognition.\n[Collobert et al.2011] Ronan Collobert, Jason Weston, Le\u0301on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493\u20132537.\n[de Bruijn et al.2011] Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Xiaodan Zhu. 2011. Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010. Journal of the American Medical Informatics Association, 18(5):557\u2013562.\n[Fu and Ananiadou2014] Xiao Fu and Sophia Ananiadou. 2014. Improving the extraction of clinical concepts from clinical records. Proceedings of BioTxtM14.\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and Ju\u0308rgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u20131780.\n[Joachims1998] Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In European conference on machine learning, pages 137\u2013142. Springer.\n[Jonnalagadda et al.2012] Siddhartha Jonnalagadda, Trevor Cohen, Stephen Wu, and Graciela Gonzalez. 2012. Enhancing clinical concept extraction with distributional semantics. Journal of biomedical informatics, 45(1):129\u2013140.\n[Kipper-Schuler et al.2008] Karin Kipper-Schuler, Vinod Kaggal, James Masanz, Philip Ogren, and Guergana Savova. 2008. System evaluation on a named entity corpus from clinical notes. In Language Resources and Evaluation Conference, LREC, pages 3001\u20133007.\n[Lafferty et al.2001] John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282\u2013289.\n[Lample et al.2016] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL-HLT.\n[Lebret and Collobert2013] Re\u0301mi Lebret and Ronan Collobert. 2013. Word emdeddings through hellinger pca. arXiv preprint arXiv:1312.5542.\n[Liu et al.2015] Shengyu Liu, Buzhou Tang, Qingcai Chen, and Xiaolong Wang. 2015. Drug name recognition: Approaches and resources. Information, 6(4):790\u2013810.\n[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119.\n[Nadeau and Sekine2007] David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30(1):3\u201326.\n[Pennington et al.2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In EMNLP, pages 1532\u20131543.\n[Uzuner et al.2011] O\u0308zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2011. 2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552\u2013556.\n[Wu et al.2015] Yonghui Wu, Jun Xu, Min Jiang, Yaoyun Zhang, and Hua Xu. 2015. A study of neural word embeddings for named entity recognition in clinical text. In AMIA Annual Symposium Proceedings, volume 2015, page 1326. American Medical Informatics Association."}], "references": [{"title": "A maximum entropy approach to natural language processing", "author": ["Berger et al.1996] Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra"], "venue": null, "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Theano: A CPU and GPU math compiler in Python", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In The 9th Python in Science Conference,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Cliner: A lightweight tool for clinical named entity recognition", "author": ["Boag et al.2015] William Boag", "Kevin Wacome", "MS Tristan Naumann"], "venue": null, "citeRegEx": "Boag et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Boag et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2", "author": ["Colin Cherry", "Svetlana Kiritchenko", "Joel Martin", "Xiaodan Zhu"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Bruijn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bruijn et al\\.", "year": 2011}, {"title": "Improving the extraction of clinical concepts from clinical records", "author": ["Fu", "Ananiadou2014] Xiao Fu", "Sophia Ananiadou"], "venue": "Proceedings of BioTxtM14", "citeRegEx": "Fu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": "In European conference on machine learning,", "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Enhancing clinical concept extraction with distributional semantics", "author": ["Trevor Cohen", "Stephen Wu", "Graciela Gonzalez"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Jonnalagadda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jonnalagadda et al\\.", "year": 2012}, {"title": "System evaluation on a named entity corpus from clinical notes", "author": ["Vinod Kaggal", "James Masanz", "Philip Ogren", "Guergana Savova"], "venue": "In Language Resources and Evaluation Conference,", "citeRegEx": "Kipper.Schuler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kipper.Schuler et al\\.", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": null, "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Word emdeddings through hellinger pca. arXiv preprint arXiv:1312.5542", "author": ["Lebret", "Collobert2013] R\u00e9mi Lebret", "Ronan Collobert"], "venue": null, "citeRegEx": "Lebret et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A survey of named entity recognition and classification", "author": ["Nadeau", "Sekine2007] David Nadeau", "Satoshi Sekine"], "venue": "Linguisticae Investigationes,", "citeRegEx": "Nadeau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nadeau et al\\.", "year": 2007}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A study of neural word embeddings for named entity recognition in clinical text", "author": ["Wu et al.2015] Yonghui Wu", "Jun Xu", "Min Jiang", "Yaoyun Zhang", "Hua Xu"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Dictionary based systems perform a fast look-up from medical ontologies such as Unified Medical Language System (UMLS) to extract concepts (Kipper-Schuler et al., 2008).", "startOffset": 139, "endOffset": 168}, {"referenceID": 11, "context": "To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998).", "startOffset": 340, "endOffset": 400}, {"referenceID": 0, "context": "To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998).", "startOffset": 340, "endOffset": 400}, {"referenceID": 8, "context": "To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998).", "startOffset": 340, "endOffset": 400}, {"referenceID": 16, "context": "For this reason, this paper employs bidirectional LSTM-CRF intialized with general purpose off-the-shelf neural word embeddings derived from Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al.", "startOffset": 147, "endOffset": 172}, {"referenceID": 14, "context": ", 2014) and Word2Vec (Mikolov et al., 2013) for automatic feature learning thus avoiding time-consuming feature engineering, which deliver system performance comparable to the best submissions from the 2010 i2b2/VA challenge.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "Similarly hybrid models obtained by cascading CRF and SVM algorithms along with several pattern matching rules are shown to produce effective results (Boag et al., 2015).", "startOffset": 150, "endOffset": 169}, {"referenceID": 9, "context": "Subsequently (Jonnalagadda et al., 2012) demonstrated that random indexing model with distributional word representations improve clinical concept extraction.", "startOffset": 13, "endOffset": 40}, {"referenceID": 4, "context": "With recent success of incorporating word embeddings derived from the entire English wikipedia in various NER task (Collobert et al., 2011), binarized word embeddings derived from domain specific copora (Eg: Monitoring in Intensive Care (MIMIC) II corpus) has improved performance of CRF based concept extraction system (Wu et al.", "startOffset": 115, "endOffset": 139}, {"referenceID": 17, "context": ", 2011), binarized word embeddings derived from domain specific copora (Eg: Monitoring in Intensive Care (MIMIC) II corpus) has improved performance of CRF based concept extraction system (Wu et al., 2015).", "startOffset": 188, "endOffset": 205}, {"referenceID": 11, "context": "Thus, another modification to the bidirectional LSTM is the addition of a conditional random field (CRF) (Lafferty et al., 2001) as the output layer to provide optimal sequential decoding.", "startOffset": 105, "endOffset": 128}, {"referenceID": 12, "context": "The resulting network is commonly referred to as the bidirectional LSTM-CRF (Lample et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 9, "context": "23 distributonal semantics-CRF (Jonnalagadda et al., 2012) 85.", "startOffset": 31, "endOffset": 58}, {"referenceID": 17, "context": "70 binarized neural embedding CRF(Wu et al., 2015) 85.", "startOffset": 33, "endOffset": 50}, {"referenceID": 3, "context": "80 CliNER (Boag et al., 2015) 79.", "startOffset": 10, "endOffset": 29}, {"referenceID": 2, "context": ", the Theano neural network toolkit (Bergstra et al., 2010)) and we publicly release our code1.", "startOffset": 36, "endOffset": 59}, {"referenceID": 14, "context": "A potential way to further improve its performance would be to initialize its training with unsupervised word embeddings such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 16, "context": ", 2013) and GloVe (Pennington et al., 2014) trained with domain specific resources such as Monitoring in Intensive Care (MIMIC) II copora.", "startOffset": 18, "endOffset": 43}], "year": 2016, "abstractText": "Extraction of concepts present in patient clinical records is an essential step in clinical research. The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for clinical records presented concept extraction (CE) task, with aim to identify concepts (such as treatments, tests, problems) and classify them into predefined categories. State-of-the-art CE approaches heavily rely on hand crafted features and domain specific resources which are hard to collect and tune. For this reason, this paper employs bidirectional LSTM with CRF decoding initialized with general purpose off-the-shelf word embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference standard corpora using bidirectional LSTM CRF ranks closely with top ranked systems.", "creator": "LaTeX with hyperref package"}}}