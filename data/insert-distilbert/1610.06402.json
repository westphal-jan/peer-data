{"id": "1610.06402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "A Growing Long-term Episodic & Semantic Memory", "abstract": "likewise the long - term memory of most connectionist systems lies entirely in the weights of the system. since consistently the number of weights is typically fixed, this bounds limits the total amount of knowledge that can sometimes be learned and stored. though this is not normally a major problem for a directed neural network designed simply for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. ideally to address this, we describe a lifelong learning system that leverages arguably a fast, though non - differentiable, rigorous content - searching addressable memory path which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge arising over many episodes for an unbounded number of domains. this method opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned publicly over a lifetime of experiences to suggest new domains.", "histories": [["v1", "Thu, 20 Oct 2016 13:29:56 GMT  (216kb,D)", "http://arxiv.org/abs/1610.06402v1", "Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material"]], "COMMENTS": "Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["marc pickett", "rami al-rfou", "louis shao", "chris tar"], "accepted": false, "id": "1610.06402"}, "pdf": {"name": "1610.06402.pdf", "metadata": {"source": "CRF", "title": "A Growing Long-term Episodic & Semantic Memory", "authors": ["Marc Pickett", "Rami Al-Rfou", "Louis Shao", "Chris Tar"], "emails": ["pickett@google.com", "rmyeid@google.com", "overmind@google.com", "ctar@google.com"], "sections": [{"heading": null, "text": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains."}, {"heading": "1 Introduction", "text": "Over the course of several decades of experience a person typically learns a variety of disparate domains. A person can learn a new domain and still retain long-held knowledge about previously learned domains. However, many neural models experience catastrophic interference when being trained on new domains, where the new learning overrides or corrupts the network\u2019s earlier knowledge. If a machine were similarly capable of learning a variety of domains over its \u201clifetime\u201d, it would potentially allow the machine to transfer knowledge among domains and bring to bear a large box of tools when facing a new problem. The problem we address is: How can a machine store an unbounded amount of episodic and semantic memory such that it can store knowledge from previous tasks and efficiently retrieve relevant information for new tasks? To do this, we must also address the subproblems of how a machine can automatically segment its experiences into episodes and how it can encode episodes into long-term memory such that relevant semantic knowledge and analogous episodes can be efficiently recalled and applied to new experiences.\nOur current work makes the following contributions: 1. We introduce a system that stores both episodic and semantic memory in a single memory system1. 2. This system automatically separates domains without requiring an explicit signal telling it which domain it is currently experiencing. We describe how a classifier may be used with this system so that it may retrieve relevant domain information while only explicitly considering a fraction of its knowledge of previous domains. This is in contrast to other systems that either require explicit domain indicators or linearly consider each of its known domains in turn. 3. We describe how the system may be used to automatically increase its memory capacity and overcome catastrophic interference.\n1We take a liberal definition of \u201cepisodic memory\u201d, which includes memory of specific sequential events. This is a looser definition than that used by Tulving and others [33], who require that episodic memory be autobiographical, for example. We define \u201csemantic memory\u201d loosely as abstractions or summaries induced over multiple sequences.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n06 40\n2v 1\n[ cs\n.A I]\n2 0\nO ct\n2 01\n6"}, {"heading": "2 Lifelong Unsupervised Learning", "text": "By any reasonable measure, human brains well over several trillion parameters2. If an artificial neural network is going to reach human-level intelligence, it seems likely that it will also need a similar order of magnitude of parameters. Assuming typical bounds on each parameter (e.g., 32 bits), for a machine to represent a huge amount of knowledge about the world, it will need a large number of parameters. One way to achieve this, which we call a fixed-brain design, is for the machine to begin its existence with nearly all the parameters it will ever have. This a perfectly reasonable approach, as there is evidence that the total number of neurons in humans actually decreases with age, even accounting for neurogenesis [25]. However, a fixed-brain design places an upper bound on the machine\u2019s total domain knowledge, which requires the machine\u2019s designers to have some prior knowledge of an upper bound of the complexity of the machine\u2019s lifetime experience (which has the potential to be orders of magnitude greater than the lifetime experience of a single person).\nInstead, we are investigating an alternative to a fixed-brain design we call a growing-brain design, where a machine has the ability to indefinitely allocate (and deallocate) new parameters as needed from an extendable memory. At the heart of our approach we assume an unlimited associative memory to which our system can read and write real-valued vectors of some fixed width3 (e.g., 10,000 elements). We assume each of these operations takes time that is logarithmic in the number of items in the memory. A \u201cwrite\u201d operation takes a key value pair, where both the key and value are vectors, and simply stores them in memory. A \u201cread\u201d operation takes a key vector and returns a small set of vectors that are likely to be those whose keys are closest to the given key. In the absence of other information, this memory assumes that nearby keys map to nearby values. So, unlike a normal hash table, a key during retrieval need not exactly match the key that was used to originally store the value. Note that a vector can serve as its own key, which results in content addressable memory. That is, one can read from the memory using a noisy or incomplete version of a vector and retrieve a completed denoised version. Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34]. Few of these methods are differentiable, and we sacrifice the assumption of differentiability in our vector memory, which gives us flexibility in which systems we can use.\nWe make the following assumptions in our approach: 1. The memory capacity of a single vector in memory has a fixed bound that is less than the amount of information we eventually want to encode about the world. 2. Following [28], our system is unsupervised, and its goal is to compress its experiences, which is an uninterrupted stream of fixed width vectors. This includes both episodic knowledge (individual instances of sequences) and semantic knowledge (patterns among many sequences). 3. Nearly all knowledge learned about the world is stored in the associative memory. This includes both individual episodes and semantic knowledge. We allow a fixed number of learned parameters in a meta-level controller outside of the vector memory. 4. Operations on the vector memory, such as insertion, deletion, and retrieval, are not assumed to be differentiable.\nMotivated by very early infant development, we assume an unsupervised setup where our machine experiences a continuous stream of data, but has no external supervision or reward signals, and no actions to affect the environment. The machine receives a continuous stream of fixed-width vectors. In our experiments we use the sequence of 1024-bit memory states from Atari games concatenated with an 18-bit one-hot encoding of the previous action. We use the implementation available from OpenAI Gym [5]. An example of this data is shown in Figure 1 in the Supplementary Material. Since our system has no control, it\u2019s merely watching another (random) player play games. Although our stream comes from multiple runs from different Atari games, the machine is given no special signal marking the beginning of an episode, nor is it given explicit information about which game is being played at any time. Though the machine\u2019s goal is merely to remember and compress these sequences,\n2A healthy adult cortex is estimated to have roughly 20 billion neurons and 150 trillion synapses [10]. The estimates for the number of bits captured by these synaptic connections vary widely. A recent estimate gives 4.7 bits per synapse [2], yielding roughly 700 trillion bits. At 32 bits per floating point parameter, this gives roughly 5 trillion floating point parameters in the cortex. By contrast, it is currently rare for artificial neural networks to have more than 100 billion floating point parameters [32], with typical networks having much fewer. For example, a recent ResNet architecture for CIFAR-10 had only 1.7 million parameters [17].\n3Note that this memory is still technically bounded by its \u201caddress space\u201d, but this is exponential in the vector width, and is unbounded for all practical purposes.\nwe hypothesize that, in doing so, the machine will develop a model of the games that will be useful for a later time when it is given a reward signal.\nIn this setup, we want our system to be capable of learning new games indefinitely without forgetting earlier games. We would also like the system to leverage knowledge from earlier games to learn faster on new games."}, {"heading": "3 Solution Overview: Storing Program Vectors in Long-term Memory", "text": "We assume our vector memory works on floating-point vectors of a fixed size (we somewhat arbitrarily chose 64 elements for our implementations). We now discuss how such a memory can be used to store a virtually unlimited amount of sequential trace data.\nInspired by Complementary Learning Systems [24], we assume we have a large, though fixed, memory buffer (separate from the extendable vector memory) in which we can rotely store a long sequence of vectors. Our initial approach for compressing the data in this buffer was to train an LSTM Sequence to Sequence auto-encoder [31] to encode subsequences from this longer sequence (we used subsequences of length 7), then commit the 64-element-wide thought vectors for the subsequences to the vector-memory. There are several problems with this approach. One is that the amount of semantic knowledge is bounded by the weights of the LSTM auto-encoder. This means that we cannot expect the LSTM to keep learning the dynamics of new Atari games indefinitely.\nTo get around this, the number of free parameters for the LSTM models needs to be increased. One possibility was to simply increasing the number of hidden states in the LSTM, but each expansion would require us to address how to train the grown LSTM without causing catastrophic interference.\nOur next approach was to train multiple LSTM auto-encoders, where each auto-encoder attempted to compress the input then reported a loss based on the difference between the original and decoded sequences. For each sequence, we tied together the losses with a minimum operation, which had the effect of only training the model that best encoded the sequence. In our experiments, this caused the models to specialize: When we trained three models on data from three different Atari games, each model specialized on encoding a particular game. (Of course, a single large LSTM with the same number of parameters has a lower reconstruction error than three small LSTMs, but the latter approach has the advantage of a simple straightforward way to extend the capacity of the model without risking catastrophic interference.)\nOne issue with using multiple LSTM auto-encoders is that our model\u2019s semantic knowledge is stored outside the vector memory (i.e., in the weights of the LSTM auto-encoders). Since each model has 651,154 parameters, this is far too big to fit into a single vector of our vector memory (which we chose to store vectors of 64 elements). To address this, we reduced the dimensionality of the auto-encoders in a manner reminiscent of HyperNetworks [16] and learnets [3]. We trained 64-element embedding vectors for each LSTM by using a feedforward \u201cstretcher\u201d network shown in Figure 2a in the Supplementary Material, that \u201cstretches\u201d a vector of size 64 to size 651,154 using layers of 64, 128, 256, and 651,154 nodes. The final weights of the stretcher network are \u201creshaped\u201d into the weights for an 64-hidden-unit LSTM auto-encoder. All the layers of the stretcher network are fully connected except the last layer, which is sparsely connected with only 1% of the possible connections, chosen randomly (a fully connected matrix would be too big to easily train). Thus, the parameter specification of each LSTM auto-encoder is a differentiable function of its 64-element embedding and the weights of the stretcher network, and thus backpropagation adjusts the embedding and the weights of the stretcher network instead of directly changing the auto-encoder\u2019s parameters.\nWe dub the final embedding for each LSTM auto-encoder a \u201cprogram vector\u201d, with the analogy that this embedding can be interpreted as a program that can be \u201ccalled\u201d with different thought-vectors or \u201carguments\u201d to produce specific sequences. Of course, knowledge is stored in the weights of the stretcher network, which has a fixed number of parameters. We hope that the stretcher network becomes somewhat generic, only encoding very general knowledge after training on a wide variety of games4.\n4Alternatively, one can imagine using a meta-stretcher network that allows us to embed many different stretcher networks, which could add another level of generality. At some point, there will have to be a fixed controller.\nWe hypothesize that the set of practically useful LSTM auto-encoders is only a tiny fraction of the set of those possible, and that the stretcher network will learn to generate \u201csensible\u201d LSTMs with most 64-element vectors chosen from a 0, 1, Gaussian distribution."}, {"heading": "4 Preliminary Results", "text": "We trained the stretcher network by training on sequences from various Atari games and varying the number of program vectors the system is allowed to use. The strongest result we have so far is that when the number of program vectors is equal to the number of Atari games, the system tends to use the same program vector for the same domain. That is, to some extent, it automatically segments the domains without being given explicit information which Atari game the traces are coming from. (See Figure 3 in the Supplementary Material)."}, {"heading": "5 Related work", "text": "Many expandable architectures have been proposed both recently and several decades ago. Nonparametric methods, such as Case-based reasoning [1], K-means and others (see [18] for a survey), have the ability to grow their capacity linearly with the data. Most of these methods operate on static vector data, so must be adapted to operate on sequential data. Furthermore, semantic knowledge (i.e., knowledge of patterns) is usually stored only implicitly (i.e., in the data points), unlike our proposed system which stores both instances and embeddings of semantic knowledge.\nOther methods have been proposed to address catastrophic interference. For example, Complementary Learning Systems [24] and Learning without Forgetting [22] both interleave training of remembered earlier data with new data. We draw inspiration from both of these systems, and from work on Progressive Networks [27], which freezes weights of networks trained on earlier domains. Unlike the others, Progressive Networks allow a network to expand its capacity. Unlike our system, Progressive Networks do not attempt to store the semantic knowledge of earlier systems in a content addressable memory, and have the problem that their network grows quadratically in the number of domains. We hypothesize that storing semantic knowledge in a content addressable memory will help address this by allowing fast lookup of relevant \u201cprogram vectors\u201d potentially yielding linear storage and logarithmic program lookup.\nSeveral methods have been proposed for expanding the capacity of neural networks. Part of our work was initially inspired by the Cascade Correlation algorithm [11], an early example which incrementally learns new features and adds them to a feed-forward network while freezing weights for previously learned features. Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8]. Our work attempts to build on these ideas by providing a means of storing sequential instances in addition to semantic (weight) information.\nThere have been recent advances in differentiable memory, such as Neural Turing Machines [13], Memory Networks [35, 21], Differentiable Neural Computers [14], and Memory-based Deep Reinforcement Learning [23]. All of these provide the system with what is essentially a working memory that can be accessed during the course of a single episode. Unlike our system, the memory is cleared between episodes, so the only long-term memory these systems retain is in the network weights.\nEpisodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30]. Our work was originally influenced specifically by SOAR [9], but extends these by using recent developments in sequence to sequence models to encode sequences as static vectors. Episodic memory has been shown to be useful for Reinforcement Learning tasks [4]. Our system provides a mechanism by which episodes may be stored and retrieved."}, {"heading": "6 Open Challenges and Future work", "text": "The primary contribution of our work is a system for encoding an unbounded amount of episodic and semantic knowledge in a expandable content-addressable vector memory. This work is still in its infancy and there are many unresolved issues to answer the question of how a machine can store a lifetime of knowledge such that it can be usefully retrieved and transferred to new situations. We share our current approaches for addressing some of these problems in the Supplementary Material."}, {"heading": "7 Approaches for Addressing Shortcomings of the Current Approach", "text": "Below, we provide a sample of the shortcomings of the current approach and what we are doing to address them."}, {"heading": "7.1 Train simple seq2vec classifier that produces best program vector for input vector", "text": "When our current system is given a trace, it checks every program vector in its vector memory, one by one, and encodes the traces using the program vector with the smallest loss. This linear search is undesirable when there are thousands or millions of program vectors, which a continually learning system might accumulate over a lifetime. Instead, we propose a system that quickly retrieves a small subset of relevant program vectors given a trace. We do this by augmenting each program vector with another key vector. When given an input key, the vector memory retrieves the vectors whose keys are closest to the input key. We then train a classifier that takes a trace as input and generates a key. This key is then fed into the vector memory, which generates a (small) set of candidate program vectors. The classifier\u2019s loss is the difference between the key it generates and the key for the best program vector. This puts domain knowledge into the classifier, which has a fixed number of parameters. To remedy this, we initially train both the classifier and the program keys. Once the classifier begins to stabilize (and we have a \u201cuniversal\u201d classifier), we train only the program vectors\u2019 keys (so that these keys effectively contain an encoding of in what situations to use the program vectors they point to)."}, {"heading": "7.2 Train simple greedy growing number of LTM program vectors", "text": "Another direction is to automatically allocate new program vectors with experience. The simplest case of this is where we have a batch dataset. In this case, we incrementally add program vectors (initializing them to be nearby existing program vectors), then train with the new vector. We keep adding until the cost of storing the new vector is no longer offset by the reduction in reconstruction error.\nFor the online case, we can compress the incoming sequence using our existing program vectors until the buffer reaches some limit. (This buffering can be done by the vector memory.) When the buffer is full (which will take longer as the system learns more patterns), the system can add program vectors and train them on the data in the buffer, similar to the batch case, but taking the buffer as the batch. The system can also retrieve earlier memories from the vector memory, and interleave these in training along with the data in the buffer, similar to the Complementary Learning Systems model [24]."}, {"heading": "7.3 Make predictions via instance retrieval (use K-nearest neighbors to transfer knowledge)", "text": "Although the current system is an autoencoder, there are at least two minor modifications that can turn it into a prediction model. The most straightforward is to use prediction loss instead of reconstruction loss while training the stretcher network and program vectors.\nAn alternate approach is more closely related to Case-base Reasoning. This is where, for each thought vector (with its program vector), we simply use the vector memory to memorize the next consequent thought vector in the sequence. When given a new thought-with-program vector, we retrieve nearby vectors, and use the memorized consequent vectors to predict the consequent vector for the input. This could be a weighted average, or we could simply allow multiple predictions. This latter approach potentially allows for richer summaries of possible predictions, such as a disjoint distribution over divergent predictions."}, {"heading": "7.4 Smarter parsing: Try multiple parsings", "text": "Our current model doesn\u2019t do a search for parsing the data stream. It simply breaks the chunks into sequences of a particular length. The system might encode the stream more compactly if it instead explores multiple windows on which to parse. Although there have been methods for a differentiable system to learn to parse [7], our first attempts at this will be a simpler discrete search over possible parsings and segmentations. This allows us to easily add other functionality, such as top-down contextual influences of a hierarchical system and classical parsing ideas using dynamic programming and back-tracking, to the parsing process."}, {"heading": "7.5 Train \u201ccontinuation\u201d version, where sequences \u201ccall\u201d next sequence", "text": "When we use the decoder to unroll thought vectors in our current model, we expect to get back just the literal original (short) sequence used in encoding the thought vector. There are variations on this idea one may try. Most obviously is prediction or skip-thought [20], in which we try to predict the following sequence rather than recite the current sequence.\nAnother approach we would like to investigate is the idea that an unrolled sequence can call other sequences. Instead of generating only literal base-level vectors, a unrolled sequence can contain a length-two subsequence corresponding to a program vector followed by its argument (i.e., a thought vector). One simple approach for training this would be to generate the program vector and thought vector for the last subsequence of a long sequence, then extend the previous subsequence with the short sequence containing these two vectors. For example, if our window size is 4, and we want to encode the sequence of vectors A,B,C,D,E, F,G,H , then we first encode E,F,G,H . Suppose the encoder encodes this sequence as program vector P7 using thought vector \u03b8. Then we feed the system A,B,C,D, but ask it to produce a sequence with six elements: A,B,C,D, P7, \u03b8. We can also extend this idea so that sequences can call program vectors at other points rather than just pointing to their continuation."}, {"heading": "7.6 Reusable Submodules, Mixture of Experts, and Explaining Away", "text": "A pattern that is common across many Atari games is a binary \u201ccounter\u201d in the first 8 bits of memory5. However, in the current system, each short trace is encoded by only one program vector, so each program vector needs to independently represent this \u201cbit counter\u201d pattern. Although the program vectors implicitly share knowledge through the stretcher network, it would be useful if \u201cexpert\u201d program vectors were able to specialize on patterns that are used across different domains.\nOne approach for addressing this is by incrementally \u201cexplaining away\u201d elements of a trace by greedily applying program vectors that most reduce the reconstruction cost. For example, if a trace has a binary counter pattern in its first 8 bits, and if we have a program vector Pb that specializes in this pattern, then we can encode the thought vector created by encoding the trace with program vector Pb. Ideally, the decoding of this thought vector using Pb would produce exactly the binary counter subsequence. We would then subtract the decoded sequence (each vector element-wise) from the original sequence (which would essentially cause the first 8 bits of the vectors in the trace to all be near zero), and repeat until no program vectors were able to further reduce the reconstruction cost. We would then train the various \u201cexpert\u201d program vectors based on the calls that this search made.\nWe could then store the sequence of calls to the experts the same way we store other sequences. To create the original trace, we would decode the sequence of calls and allow each expert to additively modify the \u201ccanvas\u201d of the trace (where experts might \u201cadd\u201d negative numbers) in a manner reminiscent of DRAW [15].\n5These counters (and other patterns) are common in other bit indices also, but we would like to address the simpler \u201cnon-transformed\u201d case first."}], "references": [{"title": "Instance-based learning algorithms, Machine learning", "author": ["David W Aha", "Dennis Kibler", "Marc K Albert"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1991}, {"title": "Nanoconnectomic upper bound on the variability of synaptic plasticity, Elife", "author": ["Thomas M Bartol Jr.", "Cailey Bromer", "Justin Kinney", "Michael A Chirillo", "Jennifer N Bourne", "Kristen M Harris", "Terrence J Sejnowski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning feed-forward one-shot learners", "author": ["Luca Bertinetto", "Jo\u00e3o F. Henriques", "Jack Valmadre", "Philip H.S. Torr", "Andrea Vedaldi"], "venue": "CoRR abs/1606.05233", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Net2net: Accelerating learning via knowledge transfer, arXiv preprint", "author": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks, arXiv preprint", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Adanet: Adaptive structural learning of artificial neural networks, arXiv preprint", "author": ["Corinna Cortes", "Xavi Gonzalvo", "Vitaly Kuznetsov", "Mehryar Mohri", "Scott Yang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Efficiently implementing episodic memory", "author": ["Nate Derbinsky", "John E Laird"], "venue": "International Conference on Case-Based Reasoning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Do we have brain to spare", "author": ["David A Drachman"], "venue": "Neurology", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A growing neural gas network learns topologies, Advances in neural information processing systems", "author": ["Bernd Fritzke"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Draw: A recurrent neural network for image generation, arXiv preprint", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep residual learning for image recognition, arXiv preprint", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Chicken, Nonparametric statistical methods", "author": ["Myles Hollander", "Douglas A Wolfe", "Eric"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Sparse Distributed Memory", "author": ["Pentti Kanerva"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1988}, {"title": "Ask me anything: Dynamic memory networks for natural language processing, arXiv preprint", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning without forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "European Conference on Computer Vision, Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Neocortical neuron number in humans: effect of sex and age", "author": ["Bente Pakkenberg", "Hans J\u00f8rgen G Gundersen"], "venue": "Journal of Comparative Neurology", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Memory systems for cognitive agents, Proceedings of Human Memory for Artificial Agents Symposium at the Artificial Intelligence and Simulation of Behavior Convention (AISB\u201911)", "author": ["Uma Ramamurthy", "Stan Franklin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models, arXiv preprint", "author": ["Juergen Schmidhuber"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "A biologically realistic cleanup memory: Autoassociation in spiking neurons", "author": ["Terrence C Stewart", "Yichuan Tang", "Chris Eliasmith"], "venue": "Cognitive Systems Research", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Memory systems within a cognitive architecture", "author": ["Ron Sun"], "venue": "New Ideas in Psychology", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Sequence to sequence learning with neural networks, Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Modeling order in neural word embeddings at scale", "author": ["Andrew Trask", "David Gilmore", "Matthew Russell"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "What is episodic memory", "author": ["Endel Tulving"], "venue": "Current Directions in Psychological Science", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1993}, {"title": "Hashing for similarity search: A survey, arXiv preprint", "author": ["Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "This is a looser definition than that used by Tulving and others [33], who require that episodic memory be autobiographical, for example.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "This a perfectly reasonable approach, as there is evidence that the total number of neurons in humans actually decreases with age, even accounting for neurogenesis [25].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "Following [28], our system is unsupervised, and its goal is to compress its experiences, which is an uninterrupted stream of fixed width vectors.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "A healthy adult cortex is estimated to have roughly 20 billion neurons and 150 trillion synapses [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "7 bits per synapse [2], yielding roughly 700 trillion bits.", "startOffset": 19, "endOffset": 22}, {"referenceID": 22, "context": "By contrast, it is currently rare for artificial neural networks to have more than 100 billion floating point parameters [32], with typical networks having much fewer.", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "7 million parameters [17].", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "Our initial approach for compressing the data in this buffer was to train an LSTM Sequence to Sequence auto-encoder [31] to encode subsequences from this longer sequence (we used subsequences of length 7), then commit the 64-element-wide thought vectors for the subsequences to the vector-memory.", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "To address this, we reduced the dimensionality of the auto-encoders in a manner reminiscent of HyperNetworks [16] and learnets [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 0, "context": "Nonparametric methods, such as Case-based reasoning [1], K-means and others (see [18] for a survey), have the ability to grow their capacity linearly with the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "Nonparametric methods, such as Case-based reasoning [1], K-means and others (see [18] for a survey), have the ability to grow their capacity linearly with the data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "For example, Complementary Learning Systems [24] and Learning without Forgetting [22] both interleave training of remembered earlier data with new data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8].", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "There have been recent advances in differentiable memory, such as Neural Turing Machines [13], Memory Networks [35, 21], Differentiable Neural Computers [14], and Memory-based Deep Reinforcement Learning [23].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "There have been recent advances in differentiable memory, such as Neural Turing Machines [13], Memory Networks [35, 21], Differentiable Neural Computers [14], and Memory-based Deep Reinforcement Learning [23].", "startOffset": 204, "endOffset": 208}, {"referenceID": 6, "context": "Episodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30].", "startOffset": 88, "endOffset": 91}, {"referenceID": 17, "context": "Episodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "Episodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "Our work was originally influenced specifically by SOAR [9], but extends these by using recent developments in sequence to sequence models to encode sequences as static vectors.", "startOffset": 56, "endOffset": 59}], "year": 2016, "abstractText": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.", "creator": "LaTeX with hyperref package"}}}