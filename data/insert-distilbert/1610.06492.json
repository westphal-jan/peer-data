{"id": "1610.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Utilization of Deep Reinforcement Learning for saccadic-based object visual search", "abstract": "the paper focuses on emerging the problem of learning saccades enabling visual object search. the developed system combines reinforcement learning with a neural network for learning to positively predict the possible outcomes needed of its actions. we validated verified the solution successfully in three types of environment consisting either of ( pseudo ) - randomly generated matrices of consecutive digits. the experimental verification is followed by the discussion regarding elements required achieved by systems mimicking the fovea movement and possible further research directions.", "histories": [["v1", "Thu, 20 Oct 2016 16:34:08 GMT  (1814kb,D)", "http://arxiv.org/abs/1610.06492v1", "Paper submitted to special session on Machine Intelligence organized during 23rd International AUTOMATION Conference"]], "COMMENTS": "Paper submitted to special session on Machine Intelligence organized during 23rd International AUTOMATION Conference", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["tomasz kornuta", "kamil rocki"], "accepted": false, "id": "1610.06492"}, "pdf": {"name": "1610.06492.pdf", "metadata": {"source": "CRF", "title": "Utilization of Deep Reinforcement Learning for saccadic-based object visual search", "authors": ["Tomasz Kornuta", "Kamil Rocki"], "emails": ["tkornut@us.ibm.com", "kmrocki@us.ibm.com"], "sections": [{"heading": null, "text": "Keywords: visual search, saccades, q-learning, neural networks"}, {"heading": "1 Introduction", "text": "Humans do not look at a scene in a passive, fixed, steady manner. Instead, their eyes move around, activelly locating and analysing interesting parts of the scene, constantly building up its mental, three-dimensional model. Those rapid jerk-like movements of the eyeball are known as saccades and subserve vision by redirecting the fovea along with the associated visual axis to a new region of\n(a) (b) (c) (d)\nFig. 1. The desired behaviour of a system realising saccadic-based: (a) image classification/semantic description of the scene, (b) visual object search, (c) an exemplary maze of digits (20x20) with indicated: goal (red cross), current agent pose (white circle) and saccadic path (green line) along with (d) the current agent observation (7x7)\nar X\niv :1\n61 0.\n06 49\n2v 1\n[ cs\n.C V\n] 2\n0 O\nct 2\ninterest. Human eyes fixate mainly on certain elements of the scene that carry or might carry essential or usefull information, whereas saccadic movements depend not only on the objects present in the scene, but also on the task the observer has to achieve [1].\nThere are several possible applications of artificial systems mimicking the fovea movement, with three being particularly interesting, i.e. image classification, semantic description of the scene and visual object search. For example, as a result of saccadic-based analysis visualized in fig. 1a the system might classify the whole image and return a single label (\"desert\") or return the whole semantic description of the scene (\u201cwhite truck on a road in a desert\u201d), whereas as a result of the visual object search (fig. 1b) the system should indicate that the object \"white truck\" was found in the position (x,y).\nIn this paper we have focused on the the latter problem, i.e. learning of visual traces mimicking the saccadic motion enabling object search. Sec. 2 briefly presents the general idea and recent applications of Deep Reinforcement Learning, whereas sec. 3 describes principles of the operation of the developed system. In sec. 4 we introduce an environment called a maze of digits that we have used for verification of our system along with the obtained results, followed by a brief discussion of the solution and future works in sec. 5."}, {"heading": "2 Related works", "text": "Reinforcement Learning (RL), as a general method of injection of the goal and learning goal-oriented behaviurs, had several successfull applications in diverse domains. For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].\nAs Reinforcement Learning in its core relies on the idea of finding action being optimal for a given state, thus the pure RL-based systems have problems with modelling of huge number of states (or, more generally, with highly dimensional inputs). For this reason people started to combine RL with Neural Networks (NNs), using the latter as (state) approximators. Such a combination has a long history. One of the very first examples is TD-Gammon [5], where weights of a neural net were updated according to a learning rule being a form of temporaldifference (TD) learning. Originally, TD-Gammon stopped improving after about 1,500,000 games (of self-play), reaching a superhuman level.\nRecent progress in the end-to-end training of multi-layer neural networks [6] (and in the so called Deep Learning [7]) once again attracted the attention of researchers to neural nets. In particular, a lot of attention was put on the mixture of RL with NNs, known recently as Deep Reinforcement Learning (DRL). In [8] authors presented a system combining neural networks with evolutionary programming and reinforcement learning, that was able to learn policies in an end-to-end manner straight from images, enabling it to drive a virtual racing car in a TORCS simulator. One of the most successful modern examples of\nsuch a combination is DQN (Deep-Q-Network) [9]. The system was based on a Convolutional Neural Network (CNN) trained with RL, reaching super-human level in 29 classic Atari games. Yet another application of CNN combined with RL (and additionally supplemented with Monte Carlo Tree Search) is AlphaGo [10], a system the managed to beat both the European (Fan Hui) and World (Lee Se-dol) Go Champions. Finally, there are also several interesting works applying Deep Reinforcement Learning in robotics, e.g. [11] presented a setup consisting of a battery of 14 robotic manipulators learning simultaneously to grasp different objects and moreover, exchanging gained experience by sharing the policy network, whereas in [12] the authors used a similar setup for learning to robustly open doors."}, {"heading": "3 Saccadic-based visual search", "text": ""}, {"heading": "3.1 Deep Reinforcement Learning primer", "text": "Reinforcement Learning aims at learning policies controlling actions of an agent interacting with an unknown environment [13]. Such an environment is often formalized as a Markov Decision Process (MDP), described by a 4-tuple (S,A,P,R). At each time step t, the agent, being in a state st \u2208 S, chooses and executes an action at \u2208 A. In consequence in the next time step the agent receives a reward rt+1 \u2208 R and finds itself in a new state st+1. P represents the state transition model of the environment. The goal of the agent is to maximize the expected discounted reward Rt = \u2211\u221e k=0 \u03b3\nkrt+k+1, with \u03b3 denoting the discount rate. Our approach is based on Q-learning [14], a Temporal Difference (TD) learning method for estimation of state action values called Q-values by updating the current estimate of Q(st, at) (Qt in short) towards the reward Rt and estimated utility of the resulting state st+1:\nQ\u0302(st, at) = Q(st, at) + \u03b1(rt + \u03b3(max a\u2032\nQ(st+1, a \u2032)\u2212Q(st, at)), (1)\nwhere \u03b1 denotes the learning rate. In particular, when st+1 is the terminal state, then Q-value is simply equal to the final reward associated with that state, i.e. Q\u0302(st, at) = R(st+1). Such a formulation enables the agent to propagate the rewards being sparsely distributed in the environment from the terminal state to the rest of the environment.\nIn our system we have decided to combine Reinforcement Learning with a (multi-layer) neural network and use the latter for approximation of Q-values, similarly to DQN [9]. The NN is parameterized with weights and biases collectively denoted as \u03b8t. The input of NN consists of a given state st, whereas the output of the forward network pass contains the vector of Q-values Qt associated with all possible types of actions. This allows to reformulate the TD update rule from (1) as a rule for update of the parameters of the network realized by minimization of a differentiable loss function:\nL(st, at|\u03b8t) \u2248 (rt + \u03b3(max a\u2032 Q(st+1, a \u2032|\u03b8t)\u2212Q(st, at|\u03b8t))2, (2)\nwhich results in: \u03b8t+1 = \u03b8t + \u03b15\u03b8 L(\u03b8t). (3)\nFinally, we can also decouple action execution from learning by using Experience Replay. The idea behind Experience Replay is to collect the experience gained after each action execution, defined as a tuple et = (st, at, rt, st+1), and store it in a memory buffer called the Experience Replay memory. When training, instead of the most recent transition, a random minibatch et from the replay memory is used. The minibatch learning accumulates error (the average error for the whole batch) and performs only a single, aggregated update of network weights calculated on the basis of all samples in a given batch, which significantly improves the convergence."}, {"heading": "3.2 System for learning saccadic-based visual search", "text": "The data-flow diagram of our system (agent realising visual search) is presented in fig. 2. There are two major components of the system: Actor, responsible for interactions with the environment, and Learner, responsible for learning from the gathered experiences. Actor and Learner share the parameters of neural network and Experience Replay Memory.\nThe general principle of the operation of the Actor is as follows. The current state of the environment si (consisting of a single observation, in our case an image patch) in iteration i is passed to the neural network, which predicts the Q-values Qi received after the execution of four possible types of actions A = {N,E, S,W}, i.e. N (North), E (East), S (South) and W (West). This enables\nthe actor to decide which action should it perform next (an epsilon-greedy action selection) and execute that action, which results in transition to the next state of the environment si+1. The actor collects the reward ri, stores the experience ei = (si, ai, ri, si+1) in the memory and starts next iteration.\nOn the other hand, the Learner performs the following operations. In the first step of a given iteration j it samples a random minibatch ej from the Experience Replay Memory. Both vectors of states sj and sj+1 are (independently) passed to the neural network, which results in predictions Qj and Qj+1. This enables the Learner to calculate the values of Q\u0302j according to Q-learning formula (1). It is worth noting that the procedure takes into account only the values for which actions were performed (single value for each experience, as only a single action was performed), whereas other values are simply copied from predictions:\nQ\u0302(sj , ak) =\n{ Q(sj , ak) + \u03b1(rj + \u03b3(max\na\u2032 Q(sj+1, a \u2032)\u2212Q(sj , ak)), if ak = aj , Q(sj , ak), otherwise,\n(4) for ak denoting each one of the four actions possible in a given state. The resulting values Q\u0302j are next used as targets along with the states sj in training of the neural network using Stochastic Gradient Descent (SGD). The resulting parameters \u03b8j+1 are stored in the memory and the next iteration j + 1 starts."}, {"heading": "4 Experimental verification", "text": ""}, {"heading": "4.1 Partially observable environment: the maze of digits", "text": "The experimental verification of our solution was performed in a controlled environment called \"a maze of digits\". Such a maze is simply a matrix of digits (integers) from 0 to 9, with 9 denoting the goal that we want to find or reach. Those mazes can be threated as simplified, single channel images, where the goal is to find a given characteristic image patch. In fig. 1c the color of the cell indicates the associated digit, the red cross on a white cell denotes the goal, whereas the current position of the agent is represented by a white circle. The current agent path is plotted with green lines connecting the consecutive grid cells, with brightness indicating the \"age\" of a given step \u2013 the darker the colour the older the step. As it is assumed that the system can observe only a part of the environment (fig. 1d), what reformulates the problem as a Partially Observable Markov Decision Process (POMDP).\nThe experiments were performed with three different types of maze of digits: totally random mazes (fig. 3a), mazes with digits forming a circular structure with the goal located in the center of the structure (fig. 3b) and mazes with digits forming kind of a path leading to the goal (fig. 3c). The latter type of mazes reflects images containing a car on a road as presented in the introductory example (fig. 1b), where finding and following one characteristic object in the image (a road) can facilitate search for yet another one (a car)."}, {"heading": "4.2 Random mazes of digits", "text": "First we have analysed the convergence in the environment of totally random mazes of digits. In each experiment we have generated a single maze, whereas at the beginning of every episode we have placed the agent in a different, random position. As those types of mazes do not possess any clear structure nor tendency indicating how close to the target the agent is, the major goal of those experiments was to check how good the solution is in the memorization of the actions to be performed in different sections of the environment and whether the system can properly learn (approximate Q-values in different states).\nIn fig. 4 and fig. 5 we have presented the exemplary results for mazes of sizes 10x10 and 20x20 and observation windows of sizes 3x3 and 5x5 respectively. The plotted \"Current\" score represents the ratio between optimal (i.e. shortest) path from the initial agent position to the goal and the length of a saccadic path (number of steps), whereas \"Average\" is the mean running ratio calculated on the basis of all past episodes. The results prove that the system is able to generate paths leading to the goal for static environments, whereas the convergence depends both on the size of the environment and the size of the window. In\nparticular, the bigger the ratio between the size of the observation window and size of the environment, the slower the system was learning."}, {"heading": "4.3 Mazes of digits forming a circle", "text": "Natural images aren\u2019t random \u2013 the majority of our surroundings possess some kind of a structure (forests consist of trees, there are beaches near oceans and seas, houses are surrounded with roads leading to them etc.). For this reason we have performed a series of experiments with random mazes possessing kind of an underlying structure. First, we generated mazes with values of digits decreasing along with the growth of distance to the goal. Besides that, in the contrary to the previous experiments, in this case in each episode the agent was placed in a newly generated, totally random (thus unique) maze.\nThe obtained results (fig. 6) indicate that the system was able to generalize across different mazes and learn to follow the colour tendency. The convergence seems slightly worse that in the previous experiments, however it should be stressed out that the utilized metric is very restrictive and even saccades being\nquite close to the optimal might receive a quite low final score (ratio). For example, in the episode 392 (fig. 6a) despite that the agent wandered only a bit at the start and managed to succesfully reach the goal \u2013 still, the path was graded the score of 0.26. Similarly, in fig. 6b the path looks perfect, but along the way the agent made additionally two steps back, which resulted in ratio of 0.85."}, {"heading": "4.4 Mazes of digits forming a path", "text": "Next we validated out system on the random mazes possesing yet another underlying structure, i.e. with digits forming a path leading to the goal. The experiments were conducted in a similar setup as the ones with random gridworlds, i.e. in each episode a totally new maze was generated and the agent started from a totally different, random position.\nAlso in this case the system was able to learn the follow that kind of a structural tendency, however the variation of current values is much higher then previously. This is related to the fact that in many cases the system had to make additional steps, first in order to find the path and then start to follow it. For example, even though the saccadic paths from fig. 7a and fig. 7b seem quite reasonable, the resulting \"Current\" scores were 0.56 and 0.76 respectivelly."}, {"heading": "5 Discussion of results and future works", "text": "According to [15], there are three major elements of saccadic system that enable the humans to perform visual search so well, namely: parallel detection (responsible for finding potential target locations), integration of information accross fixations, and the selection of the next fixation location. In this paper we have focused on the problem of saccadic visual object search, thus we have narrowed the scope of our research to the third element. The developed solution, combining reinforcement learning with a neural network, was able to learn in an\nend-to-end manner and generalize across randomly generated, thus previously unseen environments. In particular, it was able also to generate saccadic paths matching the shapes of digits from the MNIST dataset (fig. 8). However, in order to get a fully operational saccadic vision system, there are several issues that need to be addressed.\nFirst, the system must be able to aggregate data from consecutive observations. One possible solution is to use Recurrent Neural Networks (RNNs) such as Long-Short Term Memory (LSTM) [16]. For example, in [17] the authors successfully used RNNs for classification of MNIST digits represented as a sequence of pixels feed to the network one pixel at a time.\nNext problem concerns more effective analysis and learning of insights regarding the most interesting image fragments and macro-saccadic \"jumping\" between different parts of the image instead of moving by one pixel in on of four directions. Such a continuous control can be realized in several ways, e.g. in [18] the authors used RNN emitting the location of the next image patch to be analysed. More effective analysis concerns also the avoidance of already visited places \u2013 one possible solution is to use Neural Turing Machine (NTM) [19], i.e. RNN with an external memory, for memorization of the already visited locations. This also indicates that the system should monitor its current position, e.g. in [18] the RNN was feed (aside of multi-scale image patch called glimpse) with a position encoded be a simple NN called location network.\nNext, the current solution requires the final rewards to be (manually) placed in the environment, whereas we would like to have the system to autonomously learn rewarding of patches that are more interesting then others. One possible solution includes training of a network to assign higher value to the patches that are more unique then others (i.e. have smaller inter-patch covariance). The other possibility is to use RNN to predict the patch associated with the next action/location and use cross-entropy (or surprisal) for grading of that action. The recent results on utilization of surprisal for learning of long sequences [20] prove that it is a good idea. Those problems and the mentioned possible solutions indicate the future directions of development and research.\nAcknowledgments The authors kindly acknowledge the support of DARPA through the grant \"Saccadic Vision and Hierarchical Temporal Memory\", contract no. N66001-15-C-4034."}], "references": [{"title": "Eye movements during perception of complex objects", "author": ["A.L. Yarbus"], "venue": "Eye movement and vision. Plenum Press, New York", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1967}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems 19", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement learning with experience replay for model-free humanoid walking optimization", "author": ["P. Wawrzy\u0144ski"], "venue": "International Journal of Humanoid Robotics 11(03)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM 38(3)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving large-scale neural networks for vision-based reinforcement learning", "author": ["J. Koutn\u00edk", "G. Cuccu", "J. Schmidhuber", "F. Gomez"], "venue": "Proceedings of the 15th annual conference on Genetic and evolutionary computation, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature 518(7540)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": "Nature 529(7587)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research 17(39)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "arXiv preprint arXiv:1610.00633", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning from delayed rewards", "author": ["Watkins", "C.J.C.H."], "venue": "PhD thesis, University of Cambridge England", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimal eye movement strategies in visual search", "author": ["J. Najemnik", "W.S. Geisler"], "venue": "Nature 434(7031)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A Graves"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1608.06027", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Human eyes fixate mainly on certain elements of the scene that carry or might carry essential or usefull information, whereas saccadic movements depend not only on the objects present in the scene, but also on the task the observer has to achieve [1].", "startOffset": 247, "endOffset": 250}, {"referenceID": 1, "context": "For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 2, "context": "For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].", "startOffset": 296, "endOffset": 299}, {"referenceID": 4, "context": "One of the very first examples is TD-Gammon [5], where weights of a neural net were updated according to a learning rule being a form of temporaldifference (TD) learning.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "Recent progress in the end-to-end training of multi-layer neural networks [6] (and in the so called Deep Learning [7]) once again attracted the attention of researchers to neural nets.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Recent progress in the end-to-end training of multi-layer neural networks [6] (and in the so called Deep Learning [7]) once again attracted the attention of researchers to neural nets.", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "In [8] authors presented a system combining neural networks with evolutionary programming and reinforcement learning, that was able to learn policies in an end-to-end manner straight from images, enabling it to drive a virtual racing car in a TORCS simulator.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "such a combination is DQN (Deep-Q-Network) [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Yet another application of CNN combined with RL (and additionally supplemented with Monte Carlo Tree Search) is AlphaGo [10], a system the managed to beat both the European (Fan Hui) and World (Lee Se-dol) Go Champions.", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "[11] presented a setup consisting of a battery of 14 robotic manipulators learning simultaneously to grasp different objects and moreover, exchanging gained experience by sharing the policy network, whereas in [12] the authors used a similar setup for learning to robustly open doors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] presented a setup consisting of a battery of 14 robotic manipulators learning simultaneously to grasp different objects and moreover, exchanging gained experience by sharing the policy network, whereas in [12] the authors used a similar setup for learning to robustly open doors.", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "Reinforcement Learning aims at learning policies controlling actions of an agent interacting with an unknown environment [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "Our approach is based on Q-learning [14], a Temporal Difference (TD) learning method for estimation of state action values called Q-values by updating the current estimate of Q(st, at) (Qt in short) towards the reward Rt and estimated utility of the resulting state st+1:", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "In our system we have decided to combine Reinforcement Learning with a (multi-layer) neural network and use the latter for approximation of Q-values, similarly to DQN [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 14, "context": "According to [15], there are three major elements of saccadic system that enable the humans to perform visual search so well, namely: parallel detection (responsible for finding potential target locations), integration of information accross fixations, and the selection of the next fixation location.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "One possible solution is to use Recurrent Neural Networks (RNNs) such as Long-Short Term Memory (LSTM) [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "For example, in [17] the authors successfully used RNNs for classification of MNIST digits represented as a sequence of pixels feed to the network one pixel at a time.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "in [18] the authors used RNN emitting the location of the next image patch to be analysed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "More effective analysis concerns also the avoidance of already visited places \u2013 one possible solution is to use Neural Turing Machine (NTM) [19], i.", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "in [18] the RNN was feed (aside of multi-scale image patch called glimpse) with a position encoded be a simple NN called location network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The recent results on utilization of surprisal for learning of long sequences [20] prove that it is a good idea.", "startOffset": 78, "endOffset": 82}], "year": 2016, "abstractText": "The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.", "creator": "LaTeX with hyperref package"}}}