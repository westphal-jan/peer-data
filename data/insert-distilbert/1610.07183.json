{"id": "1610.07183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Oct-2016", "title": "How to be Fair and Diverse?", "abstract": "due to the recent cases of algorithmic bias propagation in data - driven decision - making, machine learning methods typically are being put under the microscope in order to understand the root cause of these biases and how to correct them. here, we consider a basic algorithmic task that is central in machine learning : subsampling from a large data set. subsamples are used both critically as an end - goal in data summarization ( where fairness could either be a legal, political or moral requirement ) and to train algorithms ( where biases in the samples are often a source of bias in the resulting model ). consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves combined in order to ensure fairness. however, in doing so, a question that seems to be overlooked perhaps is whether it is possible to produce fair subsamples that therefore are also adequately representative of the feature space implementation of the data set - an important and classic requirement in machine learning. can diversity and fairness be simultaneously ensured? we start by noting findings that, just in some applications, guaranteeing one does not necessarily guarantee the other, and a totally new approach is required. subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples. our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds.", "histories": [["v1", "Sun, 23 Oct 2016 15:13:46 GMT  (2875kb,D)", "http://arxiv.org/abs/1610.07183v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["l elisa celis", "amit deshpande", "tarun kathuria", "nisheeth k vishnoi"], "accepted": false, "id": "1610.07183"}, "pdf": {"name": "1610.07183.pdf", "metadata": {"source": "CRF", "title": "How to be Fair and Diverse?", "authors": ["L. Elisa Celis", "Amit Deshpande", "Nisheeth K. Vishnoi"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "As more and more machine learning algorithms automate data-driven processes in education, recruitment, banking, and judiciary systems, one thing has become evident \u2013 algorithms can have biases [16]. Given that these algorithms have far-reaching social and economic consequences, it is important to ensure that they comply with non-discrimination and fairness policies based on race, gender, and other sensitive attributes. Towards this, there is an ongoing effort to understand and incorporate fairness in machine learning algorithms (e.g., see [2, 8, 5, 20]).\nWe study the problem of subsampling a large data set \u2013 a basic task in machine learning. Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are\noften a source of bias in the resulting model). A crucial requirement for either task is that the sample be diverse in the feature space; this is important both to provide a comprehensive viewpoint if the sample is the end-goal, and to make algorithms trained on such samples robust. Ensuring diversity in samples is well-studied; there are several notions of diversity and approaches for attaining it (see, e.g., [13]). However, diversity may not guarantee fairness on sensitive attributes and may propagate biases, leading to broken models and algorithmic prejudice [16, 2]. Mathematically, fairness can be viewed as a measure of diversity in the combinatorial space of sensitive attributes, as opposed to the geometric space of features.\nThis brings us to the central question of this work: How do we select samples from a large dataset that are both diverse in features and fair to sensitive attributes? Simple examples (such as those in Figure 1) show that, in certain settings, diversity does not necessarily imply fairness and vice-versa; however, both seem simultaneously achievable. While both geometric and combinatorial diversities have been studied in independent works (e.g., [13] and [9]), to the best of our knowledge, this is the first systematic study that addresses both simultaneously.\nDiversity: combinatorial and geometric. Formally, we study the following problem: given a large dataset X of n items, output a geometrically and combinatorially diverse S of size k. To make this well-defined, we need to specify two things: 1) how the data is given and 2) measures of diversity. There are two extremes \u2013\n1. Combinatorial diversity. Each data point x \u2208 X has an attribute from a small set {1, 2, . . . , p}, which leads to a combinatorial measure of diversity D(\u00b7): The diversity of a set S is the Shannon entropy of the distribution ( |S1|/k, |S2|/k, . . . , |Sp|/k), where Si is the set of elements in S with attribute value i. Intuitively, the larger the entropy, the more diverse is S with respect to the given attributes.\n2. Geometric diversity. Each data point x \u2208 X has a high-dimensional feature vector vx, which motivates a geometric measure of diversity G(\u00b7): The diversity of a set S is the (squared) volume of the k-dimensional parallelepiped formed by the vectors {vx : x \u2208 S}. Intuitively, the larger this volume, the more diverse is S in the feature space.\nar X\niv :1\n61 0.\n07 18\n3v 1\n[ cs\n.L G\n] 2\n3 O\nct 2\n01 6\nThe combinatorial notion works with much less information, and is known as the diversity index [17] in social and biological sciences. It is more suited to quantify fairness in sensitive or human-interpretable attributes that take a small set of discrete values. The geometric notion gives rise to a probability distribution known as determinantal point process (or k-DPP), and such measures have been used to quantify feature diversity in a variety of machine learning applications for images [13], videos [7], documents [14], recommendation systems [21], and sensor placement [11]. Besides quantification of diversity in feature-rich datasets, an important reason for the deployment of k-DPPs is the recent efficient algorithms to sample from these distributions [3, 1].\nOur contribution. We present an algorithmic framework that allows a user to integrate both notions of diversity, and experimentally demonstrate a marked improvement in fairness without compromising geometric diversity by much \u2013 resulting in the best of both the worlds.\nConceptually, we propose a novel generalization of kDPPs which we call P -DPP. Given the feature vectors and the partition of the dataset X = X1 \u222aX2 \u222a \u00b7 \u00b7 \u00b7 \u222aXp based on the p different values of a sensitive attribute, P - DPP samples a k-sized subset S with probability proportional to the squared volume of the parallelepiped formed by the feature vectors in S (as is done in k-DPPs) but only over sets S that satisfy |S \u2229Xi| = ki, for given kis for all 1 \u2264 i \u2264 p. Algorithmically, a polynomial time algorithm for sampling k-DPPs generalizes, albeit nontrivially, to P -DPPs with a constant number of disjoint partitions (i.e., p = O(1)), making our approach feasible [10, 18].\nWe experimentally compare the performance of sampling with P -DPPs against three natural baselines for an image summarization task. We consider an image dataset that consists of male and female scientists and artists.\nWe observe that P -DPP outperforms or matches other approaches with respect to both D(\u00b7) and G(\u00b7) in three different scenarios: 1) when we can ensure perfect fairness, see Section 3.3.1 and Figure 2(i), 2) when some sensitive attributes remain hidden, see Section 3.3.2 and Figure 2(ii), and 3) when the underlying dataset is biased, see Section 3.3.3 and Figure 2(iii).\nThese experiments give strong evidence that sampling with P -DPPs is a successful approach for data summarization. Subsampling is also an important subroutine in various machine learning tasks (see, e.g., [4, 6]), and it remains an important avenue for future work to study if P -DPPs can also help mitigate algorithmic bias in such settings."}, {"heading": "2. Preliminaries", "text": "Here we give the formal definitions and theoretical constructs used in this paper. An attribute that takes one of p different values gives a natural partition of the underlying data into p parts. The fairness of a dataset (or its subset) with respect to such an attribute can then be quantified by the fairness or diversity index. Definition 2.1. (Fairness or Diversity Index) Given a set X of n items and its partition X = X1\u222aX2\u222a. . .\u222aXp into p parts, the diversity index of any subset S \u2286 X is defined as the Shannon entropy D(S) = \u2212 ( \u2211p i=1 si log si) where si = |S\u2229Xi|\n|S| .\nFor feature-rich data, where a kernel defines the dot product of feature vectors, (sub)determinants extend this notion to define diversity over subsets. Definition 2.2. (Geometric Diversity) Given a dataset X and a positive semidefinite kernel matrix K = (K(x, y))x,y\u2208X , the geometric diversity of a subset S \u2286 X is defined as G(S) = det (KS,S), which is the determinant of the principal submatrix KS,S = (K(x, y))x,y\u2208S given by the row and column indices in S. Geometric diversity defines a distribution on subsets known as a (discrete) determinantal point process. Definition 2.3. (DPPs and k-DPPs) Given a dataset X and a positive semidefinite kernel matrix K = (K(x, y))x,y\u2208X , the DPP is a distribution over subsets S \u2286 X such that the probability Pr (S) \u221d det (KS,S). The induced probability distribution over k-sized subsets is called k-DPP. Now we define P -DPP; our generalization of k-DPP to subsets that have the same relative partition as X. Definition 2.4. (P -DPP) Given a dataset X, a positive semidefinite kernel matrix K = (K(x, y))x,y\u2208X , a partition X = X1 \u222a X2 \u222a \u00b7 \u00b7 \u00b7 \u222a Xp into p parts, and numbers k1, . . . , kp, P -DPP defines a distribution over k-sized subsets S \u2286 X such that Pr (S) \u221d det (KS,S) if |S \u2229Xi| = ki and Pr (S) = 0, otherwise. Lastly, we introduce a natural baseline which we also compare against in our experiments.\nDefinition 2.5. (ki-DPP) Given a dataset X, a positive semidefinite kernel matrix K = (K(x, y))x,y\u2208X , a partition X = X1 \u222a X2 \u222a \u00b7 \u00b7 \u00b7 \u222a Xp into p parts, and numbers k1, . . . , kp, ki-DPP defines a distribution over k1 + k2 + \u00b7 \u00b7 \u00b7 + kp-sized subsets S \u2286 X that is a product distribution: for each i, we obtain a sample Si \u2286 Xi of size ki independently with probability proportional to Pr (Si) \u221d det (KSi,Si), and combine these samples to output S = S1 \u222a S2 \u222a \u00b7 \u00b7 \u00b7 \u222a Sp. We emphasize that the difference between a ki-DPP and a P -DPP with the same parameters (k1, . . . , kp) is that the samples Si from each part in P -DPP are not independent as in ki-DPP. Indeed, this is what makes them more powerful.\nPolynomial time sampling from k-DPPs uses a linear algebraic fact that the partition function as well as the marginals of k-DPP can be computed using the characteristic polynomial of the underlying kernel matrix. A multivariate generalization of this can incorporate partition constraints (and beyond) to sample from P -DPPs in time nO(p), which is polynomial for p = O(1) [10, 18]."}, {"heading": "3. Experimental Results", "text": ""}, {"heading": "3.1. Datasets and Features", "text": "We ran our experiments on a collection of images curated using Google image search as follows: Four search terms were used: (a) \u201cScientist Male\u201d, (b) \u201cScientist Female\u201d, (c) \u201cPainter Male\u201d, and (d) \u201cPainter Female\u201d. The search was restricted to medium sized JPEG files that passed the strictest level of Safe Search filtering. The top 200 distinct images from each were collected to create the following three datasets:1 \u2022 Scientist: (a) and (b) \u2022 Artist: (c) and (d) \u2022 Scientist+Artist: (a), (b), (c), and (d). Hence, each dataset has inherent labels (a)-(d) over which we can measure the combinatorial diversity of a sample. In order to measure geometric diversity, following [12], each image was processed with the vlfeat toolbox to obtain sets of 128-dimensional SIFT descriptors [15, 19]. The descriptors are combined, subsampled to a set of 36,000 and then clustered using k-means into 256 clusters. The feature vector for an image is the normalized histogram of the nearest clusters to the descriptors in the image. Finally, the kernel value K(x, y) for any pair of images x and y is obtained by taking the dot-product of the SIFT features of x and y."}, {"heading": "3.2. Algorithms and Baselines", "text": "In each experiment, we compare four different probability distributions from which to select k samples from a dataset: 1) Our proposed P -DPP (see Def 2.4), 2) the\n1The images are available at goo.gl/hNukfP.\nclassic k-DPP (see Def 2.3), 3) ki-DPP (see Def 2.5), and 4) UNIF, which takes a uniformly random subset of size k.\nIn order to sample from k-DPP, ki-DPP and P -DPP, instead of using the polynomial time algorithms of [10, 18], we appeal to a Markov Chain Monte Carlo (MCMC) heuristic inspired by [1] as the latter seems faster in practice. The Markov chain is defined over the space of subsets of cardinality k. The algorithm first chooses a \u201cwarm start state\u201d S obtained by greedily maximizing the determinant while satisfying the partition constraints. Then, in each iteration, elements i \u2208 S and j 6\u2208 S are chosen uniformly at random. The chain moves to state T = S \\ {i} \u222a {j} with probability 12 min{1, det(KT,T )/det(KS,S)}, if it satisfies the constraints. Otherwise, it stays in state S. This is repeated for a suitable number of iterations to guarantee that samples drawn from this chain are \u201cclose\u201d to that of the desired distribution. In each experiment, given a sample XA selected by algorithm A, we report the combinatorial diversity using the fairness index D(XA) (see Def 2.1) and the geometric diversity G(XA) (see Def 2.2)."}, {"heading": "3.3. Experiments and Discussion", "text": ""}, {"heading": "3.3.1. Experiment 1: Perfect Fairness", "text": "Experimental Setup. We first consider the performance as we vary the sample size k from 20 to 100 on the Scientist dataset (see Figure 2(i)); recall that the dataset has two parts, male and female, and that the dataset is unbiased. We place fairness constraints so that P -DPP and ki-DPP select exactly 50% of their samples from the male and female parts. Hence, we have set up the experiment to guarantee optimal D(\u00b7) for P -DPP and ki-DPP, and measure the resulting degradation in G(\u00b7).\nResults. Both P -DPP and ki-DPP attain the optimal D(\u00b7) of 2. As expected, this is significantly higher than UNIF and k-DPP (paired one-sided t-tests, p < 0.05). In fact, even UNIF has significantly higher fairness than kDPP (paired one-sided t-test, p < 0.05). With respect to G(\u00b7), the performance of k-DPP and P -DPP is comparable, with neither significantly outperforming the other. This is notable as P -DPP has constraints that k-DPP need not abide by; hence, a priori, k-DPP could be significantly better. Moreover, both k-DPP and P -DPP have significantly higher G(\u00b7) than UNIF and ki-DPP (paired one-sided t-tests, p < 0.05). Outperforming UNIF is expected as random selection makes no effort to increase G(\u00b7), however the outperformance of ki-DPP is notable for two reasons: 1) ki-DPP is the only other algorithm that matched the fairness index of P -DPP, and 2) ki-DPP is also explicitly attempting to improve G(\u00b7). However, while ki-DPP improves G(\u00b7) within a part of the dataset, it does not diversify across parts; P -DPP avoids exactly this pitfall.\nConclusion. This experiment demonstrates that P - DPP can match or outperform the other approaches with respect to both fairness and diversity. This conclusion is not unique to this dataset \u2013 we also conducted the same experiment on the Artist dataset, and the results are very similar with the same significance findings holding; we omit the full details due to length constraints. Exploring whether such results are consistent on other types of datasets would be a clear direction for future work."}, {"heading": "3.3.2. Experiment 2: Hidden Attributes", "text": "Experimental Setup. We then consider the performance of the algorithms as we vary the sample size k from 10 to 50 on the Scientist+Artist dataset, but consider the case where there is a hidden underlying partition (see Figure 2(ii)). Here, we place fairness constraints so that P -DPP and ki-DPP select exactly 50% of their samples from the male (a and c) images and female (b and d) images, but do not enforce constraints across scientist (a and b) images and artist (c and d) images, allowing for disproportionality across this dimension. However, we measure the fairness with respect to all four parts.\nResults. With respect to the D(\u00b7), P -DPP and ki-DPP no longer attain the optimal fairness of 4. However, P - DPP significantly outperforms k-DPP, UNIF and ki-DPP (paired one-sided t-tests, p < 0.05), with ki-DPP being the worst performer despite the partial constraints. With respect to G(\u00b7), as in Experiment 1, the performance of k-DPP and P -DPP is comparable, and both have significantly higher G(\u00b7) than UNIF (paired one-sided t-tests, p < 0.05). For this experiment, P -DPP is also compara-\nble to ki-DPP, with a mean determinant that is higher, but not significantly so; this is largely due to the fact that for this experiment k is smaller while the dataset size is larger, and hence the drop-off in performance of ki-DPP is not as evident as it was in Experiment 1.\nConclusion. Hence, this experiment demonstrates that P -DPP can match or outperform the other approaches with respect to both fairness and diversity, even when some of the underlying attributes are unknown. This is an important consideration as we should not inadvertently boost one kind of fairness at the expense of another."}, {"heading": "3.3.3. Experiment 3: Biased Datasets", "text": "Experimental setup. Lastly, we consider the situation where the underlying dataset is biased (see Figure 2(iii)). We include all female (b and d) images, but only include a subsample of male images (a and c) in the dataset in order to create biased datasets that have between 10% to 50% male images. The subsampled images are selected uniformly at random from all male scientists and artists for each repetition in the experiment. We place fairness constraints so that P -DPP and ki-DPP select exactly 50% of their samples from the male (a and c) images and female (b and d) images, regardless of the bias in the underlying dataset. As in Experiment 2, we do not enforce constraints across scientist (a and b) images and artist (c and d) images, but measure D(\u00b7) with respect to all four attributes.\nResults. With respect to D(\u00b7), P -DPP significantly outperforms k-DPP, UNIF and ki-DPP (paired one-sided t-tests, p < 0.05). Here, we see that the bias in the un-\nderlying dataset can dramatically affect the fairness of UNIF and k-DPP as neither approach is designed to correct for such biases. However, P -DPP and ki-DPP are able to remain relatively stable throughout. With respect to G(\u00b7), P -DPP has significantly higher G(\u00b7) than UNIF and ki-DPP (paired one-sided t-tests, p < 0.05). However, now k-DPP significantly outperforms P -DPP (paired onesided t-test, p < 0.05). This is due to the fact that when the dataset is highly biased, the available selection of images in the smaller partition is limited, and hence it is more difficult for P -DPP to diversify across the feature space. Indeed, we expect this gap to close as the size (but not proportion) of the smaller part increases.\nConclusion. In this experiment we observe that, when the underlying data is highly biased, there is now a tradeoff between D(\u00b7) (for which P -DPP performs best) and G(\u00b7) (for which k-DPP performs best). Despite these differences, we note that the gap in P -DPP\u2019s geometric diversity gradually decreases, while k-DPPs fairness index drops rapidly as the bias increases, leading us to conclude that P -DPPs remain the best of both worlds, allowing for fairness and diversity."}], "references": [{"title": "Monte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal point processes", "author": ["Nima Anari", "Shayan Oveis Gharan", "Alireza Rezaei"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Selbst. Big Data\u2019s Disparate Impact", "author": ["A.D.S. Barocas"], "venue": "SSRN eLibrary,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "On the nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["Petros Drineas", "Michael W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Revisiting the nystrom method for improved large-scale machine learning", "author": ["Alex Gittens", "Michael W. Mahoney"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["Boqing Gong", "Wei-Lun Chao", "Kristen Grauman", "Fei Sha"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural  Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Semantics derived automatically from language corpora necessarily contain human", "author": ["Aylin Caliskan Islam", "Joanna J. Bryson", "Arvind Narayanan"], "venue": "biases. CoRR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Classifying without discriminating", "author": ["Faisal Kamiran", "Toon Calders"], "venue": "In Computer, Control and Communication,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "On Sampling and Greedy MAP Inference of Constrained Determinantal Point Processes", "author": ["Tarun Kathuria", "Amit Deshpande"], "venue": "ArXiv e-prints,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["Andreas Krause", "Ajit Paul Singh", "Carlos Guestrin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "k-dpps: Fixed-size determinantal point processes", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Hui Lin", "Jeff A. Bilmes"], "venue": "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Object recognition from local scaleinvariant features", "author": ["David G. Lowe"], "venue": "In ICCV, pages 1150\u20131157,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy", "author": ["C. O\u2019Neil"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Measurement of diversity", "author": ["Edward H Simpson"], "venue": "Nature,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1949}, {"title": "Generalized determinantal point processes: The linear case", "author": ["Damian Straszak", "Nisheeth Vishnoi"], "venue": "ArXiv eprints,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Vlfeat: An open and portable library of computer vision", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Fairness Constraints: A Mechanism for Fair Classification", "author": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna Gummadi"], "venue": "In 2nd Workshop on Fairness, Accountability, and Transparency in Machine Learning, Lille, France,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Solving the apparent diversity-accuracy dilemma of recommender systems", "author": ["Tao Zhou", "Zoltn Kuscsik", "Jian-Guo Liu", "Mat Medo", "Joseph Rushton Wakeling", "Yi-Cheng Zhang"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "As more and more machine learning algorithms automate data-driven processes in education, recruitment, banking, and judiciary systems, one thing has become evident \u2013 algorithms can have biases [16].", "startOffset": 193, "endOffset": 197}, {"referenceID": 1, "context": ", see [2, 8, 5, 20]).", "startOffset": 6, "endOffset": 19}, {"referenceID": 7, "context": ", see [2, 8, 5, 20]).", "startOffset": 6, "endOffset": 19}, {"referenceID": 4, "context": ", see [2, 8, 5, 20]).", "startOffset": 6, "endOffset": 19}, {"referenceID": 19, "context": ", see [2, 8, 5, 20]).", "startOffset": 6, "endOffset": 19}, {"referenceID": 12, "context": ", [13]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "However, diversity may not guarantee fairness on sensitive attributes and may propagate biases, leading to broken models and algorithmic prejudice [16, 2].", "startOffset": 147, "endOffset": 154}, {"referenceID": 1, "context": "However, diversity may not guarantee fairness on sensitive attributes and may propagate biases, leading to broken models and algorithmic prejudice [16, 2].", "startOffset": 147, "endOffset": 154}, {"referenceID": 12, "context": ", [13] and [9]), to the best of our knowledge, this is the first systematic study that addresses both simultaneously.", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": ", [13] and [9]), to the best of our knowledge, this is the first systematic study that addresses both simultaneously.", "startOffset": 11, "endOffset": 14}, {"referenceID": 16, "context": "The combinatorial notion works with much less information, and is known as the diversity index [17] in social and biological sciences.", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "The geometric notion gives rise to a probability distribution known as determinantal point process (or k-DPP), and such measures have been used to quantify feature diversity in a variety of machine learning applications for images [13], videos [7], documents [14], recommendation systems [21], and sensor placement [11].", "startOffset": 231, "endOffset": 235}, {"referenceID": 6, "context": "The geometric notion gives rise to a probability distribution known as determinantal point process (or k-DPP), and such measures have been used to quantify feature diversity in a variety of machine learning applications for images [13], videos [7], documents [14], recommendation systems [21], and sensor placement [11].", "startOffset": 244, "endOffset": 247}, {"referenceID": 13, "context": "The geometric notion gives rise to a probability distribution known as determinantal point process (or k-DPP), and such measures have been used to quantify feature diversity in a variety of machine learning applications for images [13], videos [7], documents [14], recommendation systems [21], and sensor placement [11].", "startOffset": 259, "endOffset": 263}, {"referenceID": 20, "context": "The geometric notion gives rise to a probability distribution known as determinantal point process (or k-DPP), and such measures have been used to quantify feature diversity in a variety of machine learning applications for images [13], videos [7], documents [14], recommendation systems [21], and sensor placement [11].", "startOffset": 288, "endOffset": 292}, {"referenceID": 10, "context": "The geometric notion gives rise to a probability distribution known as determinantal point process (or k-DPP), and such measures have been used to quantify feature diversity in a variety of machine learning applications for images [13], videos [7], documents [14], recommendation systems [21], and sensor placement [11].", "startOffset": 315, "endOffset": 319}, {"referenceID": 2, "context": "Besides quantification of diversity in feature-rich datasets, an important reason for the deployment of k-DPPs is the recent efficient algorithms to sample from these distributions [3, 1].", "startOffset": 181, "endOffset": 187}, {"referenceID": 0, "context": "Besides quantification of diversity in feature-rich datasets, an important reason for the deployment of k-DPPs is the recent efficient algorithms to sample from these distributions [3, 1].", "startOffset": 181, "endOffset": 187}, {"referenceID": 9, "context": ", p = O(1)), making our approach feasible [10, 18].", "startOffset": 42, "endOffset": 50}, {"referenceID": 17, "context": ", p = O(1)), making our approach feasible [10, 18].", "startOffset": 42, "endOffset": 50}, {"referenceID": 3, "context": ", [4, 6]), and it remains an important avenue for future work to study if P -DPPs can also help mitigate algorithmic bias in such settings.", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": ", [4, 6]), and it remains an important avenue for future work to study if P -DPPs can also help mitigate algorithmic bias in such settings.", "startOffset": 2, "endOffset": 8}, {"referenceID": 9, "context": "A multivariate generalization of this can incorporate partition constraints (and beyond) to sample from P -DPPs in time n, which is polynomial for p = O(1) [10, 18].", "startOffset": 156, "endOffset": 164}, {"referenceID": 17, "context": "A multivariate generalization of this can incorporate partition constraints (and beyond) to sample from P -DPPs in time n, which is polynomial for p = O(1) [10, 18].", "startOffset": 156, "endOffset": 164}, {"referenceID": 11, "context": "In order to measure geometric diversity, following [12], each image was processed with the vlfeat toolbox to obtain sets of 128-dimensional SIFT descriptors [15, 19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "In order to measure geometric diversity, following [12], each image was processed with the vlfeat toolbox to obtain sets of 128-dimensional SIFT descriptors [15, 19].", "startOffset": 157, "endOffset": 165}, {"referenceID": 18, "context": "In order to measure geometric diversity, following [12], each image was processed with the vlfeat toolbox to obtain sets of 128-dimensional SIFT descriptors [15, 19].", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "In order to sample from k-DPP, ki-DPP and P -DPP, instead of using the polynomial time algorithms of [10, 18], we appeal to a Markov Chain Monte Carlo (MCMC) heuristic inspired by [1] as the latter seems faster in practice.", "startOffset": 101, "endOffset": 109}, {"referenceID": 17, "context": "In order to sample from k-DPP, ki-DPP and P -DPP, instead of using the polynomial time algorithms of [10, 18], we appeal to a Markov Chain Monte Carlo (MCMC) heuristic inspired by [1] as the latter seems faster in practice.", "startOffset": 101, "endOffset": 109}, {"referenceID": 0, "context": "In order to sample from k-DPP, ki-DPP and P -DPP, instead of using the polynomial time algorithms of [10, 18], we appeal to a Markov Chain Monte Carlo (MCMC) heuristic inspired by [1] as the latter seems faster in practice.", "startOffset": 180, "endOffset": 183}], "year": 2016, "abstractText": "Due to the recent cases of algorithmic bias in datadriven decision-making, machine learning methods are being put under the microscope in order to understand the root cause of these biases and how to correct them. Here, we consider a basic algorithmic task that is central in machine learning: subsampling from a large data set. Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are often a source of bias in the resulting model). Consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves in order to ensure fairness. However, in doing so, a question that seems to be overlooked is whether it is possible to produce fair subsamples that are also adequately representative of the feature space of the data set \u2013 an important and classic requirement in machine learning. Can diversity and fairness be simultaneously ensured? We start by noting that, in some applications, guaranteeing one does not necessarily guarantee the other, and a new approach is required. Subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples. Our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds.", "creator": "LaTeX with hyperref package"}}}