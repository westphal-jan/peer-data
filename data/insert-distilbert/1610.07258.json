{"id": "1610.07258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization", "abstract": "we propose a new model based on the simple deconvolutional networks and sax discretization to learn the representation for multivariate time series. deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner described of unsupervised functional learning. formally we design a network structure specifically to capture the cross - channel correlation strategy with deconvolution, forcing the pooling operation to perform the dimension reduction along about each position stored in the individual channel. weighted discretization based operations on symbolic aggregate approximation is applied centred on the feature correlation vectors attempting to further extract the bag of features. we show how this representation and systematic bag of features helps on classification. suppose a full comparison with learning the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. additionally we further build the markov probability matrix from the discretized representation from the deconvolution to visualize the measured time cross series as complex networks, which show more class - specific defined statistical properties and clear structures with respect to different labels.", "histories": [["v1", "Mon, 24 Oct 2016 01:53:12 GMT  (1052kb,D)", "https://arxiv.org/abs/1610.07258v1", "Submitted to NeuroComputing. arXiv admin note: text overlap witharXiv:1505.04366by other authors"], ["v2", "Wed, 26 Oct 2016 00:17:45 GMT  (1054kb,D)", "http://arxiv.org/abs/1610.07258v2", "Submitted to NeuroComputing. arXiv admin note: text overlap witharXiv:1505.04366by other authors"], ["v3", "Sat, 26 Nov 2016 21:02:49 GMT  (1054kb,D)", "http://arxiv.org/abs/1610.07258v3", "arXiv admin note: text overlap witharXiv:1505.04366by other authors"]], "COMMENTS": "Submitted to NeuroComputing. arXiv admin note: text overlap witharXiv:1505.04366by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhiguang wang", "wei song", "lu liu", "fan zhang", "junxiao xue", "yangdong ye", "ming fan", "mingliang xu"], "accepted": false, "id": "1610.07258"}, "pdf": {"name": "1610.07258.pdf", "metadata": {"source": "CRF", "title": "Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization", "authors": ["Wei Songa", "Zhiguang Wangb", "Lu Liub", "Fan Zhangc", "Junxiao Xued", "Yangdong Yea", "Ming Fana", "MingLiang Xud"], "emails": ["iewsong@zzu.edu.cn"], "sections": [{"heading": null, "text": "We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels.\nKeywords: Multivariate time-series, Deconvolution, Symbolic Aggregate Approximation, Deep Learning, Markov Matrix, Visualization"}, {"heading": "1. Introduction", "text": "Sensors are now becoming cheaper and more prevalent in recent years to motivate the broad usage of large amount of time series data. For example, Non-invasive, continuous, high resolution vital signs data, such as Electrocardiography (ECG) and Photoplethysmograph (PPG), are commonly used in hospital settings for better monitoring of patient outcomes to optimize early care. Industrial time series help the engineers to predict and get early preparation of the potential failure. We formulate these tasks as regular multivariate time series classification/learning problem. Compared to the univariate time series, multivariate time series is more ubiquitous, hence providing more patterns and insight of the underlying phenomena to help improve the classification performance. Therefore, multivariate time series classification is becoming more and more important in a broad range of applications, such as industrial inspection and clinical monitoring.\nMultivariate time series data is not only characterized by individual attributes, but also by the relationships between the attributes [1]. Such information is not captured by the similarity between the individual sequences [2]. To deal with the classification problem on multivariate time series, several similarity measurements including Edit distance with Real Penalty\n\u2217Corresponding author: Wei Song. Email address: iewsong@zzu.edu.cn (Zhiguang Wang) 1These authors contribute equally to this work.\n(ERP) and Time Warping Edit Distance (TWED) are summarized and tested on several benchmark dataset [3]. Recently, a symbolic representation for multivariate time series classification (SMTS) is proposed. Mining core feature for early classification (MCFEC) along the sequence is proposed to capture the shapelets in each channel independently [4]. SMTS builds a tree learner with two ensembles to learn the segmentations and a high-dimensional codebook [5]. While these methods provide new perspective to handle multivariate data, some are time consuming (e.g. SMTS), some are effective but cannot address the curse of dimensionality (distance on raw data).\nInspired by recent advances in feature learning for image classification, several feature-based approaches are proposed like [5, 6]. Compared with those sequence-distance based approaches, the feature-based approaches skip the tricky handcrafted features as they learn a hierarchical feature representation from raw data automatically. However, the feature learning approach are only limited on the scenario of supervised learning and few comparison towards distance-based learning approaches (like [7]). The method described in [6] is simple but not fully automated, instead they still need to design the weighting scheme manually.\nOur work provides a new perspective to learn the hidden representations with deconvolutional networks (in the self-supervised learning way), hence to fully exploit the unlabeled data especially when the data is large. We design the network structure to capture the cross-channel correlation with convolutions, forcing the pooling operation to perform the dimension reduc-\nPreprint submitted to Journal of NeuroComputing December 5, 2016\nar X\niv :1\n61 0.\n07 25\n8v 3\n[ cs\n.L G\n] 2\n6 N\nov 2\n01 6\ntion along each position of the individual channel. Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach."}, {"heading": "2. Background and Related Work", "text": ""}, {"heading": "2.1. Deep Neural Networks", "text": "Since 2006, the techniques developed from deep neural networks (or, deep learning) have greatly impacted natural language processing, speech recognition and computer vision research [11, 12]. One successful deep learning architecture used in computer vision is convolutional neural networks (CNN) [13]. CNNs exploit translational invariance by extracting features through receptive fields [14] and learning with weight sharing, becoming the state-of-the-art approach in various image recognition and computer vision tasks [15]. Most exciting advance comes from the exploration of unsupervised learning algorithms for generative models, such as Deep Belief Networks (DBN) and Denoised Auto-encoders (DA) [16, 17]. Many deep generative models are developed based on energy-based model or autoencoders. Temporal autoencoding is integrated with Restrict Boltzmann Machines (RBMs) to improve generative models [18]. A training strategy inspired by recent work on optimizationbased learning is proposed to train complex neural networks for imputation tasks [19]. A generalized Denoised Auto-encoder extends the theoretical framework and is applied to Deep Generative Stochastic Networks (DGSN) [20, 21].\nHowever, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26]. We use deconvolution to capture both the temporal and cross-channel correlation in the multivariate time series, rather than pretrain a supervised model."}, {"heading": "2.2. Discretization and Visualization for Time Series", "text": "Time Series discretization is broadly used in symbolic approximation based approach . Aligned Cluster Analysis (ACA) is introduced as an unsupervised method to cluster the temporal patterns of human motion data [27]. It is an extension of kernel k-means clustering but requires quite computational capacity. Persist is an unsupervised discretization methods to maximize the persistence measurement of each symbol [28]. Piecewise Aggregate Approximation (PAA) methods is proposed by Keogh [29] to reduce the dimensionality of time series, which is then upgraded to Symbolic Aggregate Approximation (SAX) [8]. In SAX, each aggregation value after PAA process is mapped into the equiprobable intervals based on standard normal distribution to produce a sequence of symbolic representations. Among these symbolic approaches, SAX method has become\none of the de facto standard to discretize time series and is at the core of many effective classification algorithms.\nThe principal idea of SAX is to smooth the input time series using Piecewise Aggregation Approximation (PAA) and assign symbols to the PAA bins. The overall time series trend is extracted as a sequence of symbols.\nThe algorithm requires three parameters: window length n, number of symbols w and alphabet size a. Different parameters lead to different representations of the time series. Given a normalized time series of length L, we first reduce the dimensionality by dividing it into [L/n] non-overlapping sliding windows with skip size 1. Each sliding window is partitioned into w subwindows. Mean values are computed to reduce volume and smooth the noise. Then PAA values are mapped to a probability density function N(0, 1), which is divided into several equiprobable segments. Letters starting from A to Z are assigned to each PAA values according to their corresponding segments (Figure 1).\nIn another hand, reformulating time series as visual clues has raised much attention in computer science and physics in which the discretization method plays an important role. The typical examples are that acoustic/speech data input is typically represented by Mel-frequency cepstral coefficients (MFCCs) or Perceptual Linear Prediction (PLP) to explicitly represent the temporal and frequency information. Researchers are trying to build different network structures from time series for visual inspection or designing distance measures. Recurrence Networks were proposed to analyze the structural properties of time series from complex systems [30, 31]. They build adjacency matrices from the predefined recurrence functions to interpret the time series as complex networks. Silva et al. extended the recurrence plot paradigm for time series classification using compression distance [32]. Another way to build a weighted adjacency matrix is extracting transition dynamics from the first order Markov matrix [33]. Although these maps demonstrate distinct topological properties among different time series, it remains unclear how these topological properties relate to the original time series since they have no exact inverse operations. [10] proposed an generalized Markovian encoding to map the complex correlations in the time series into images while preserving the temporal information as well.\nTo give a intuition about how our learned feature is shaped,\nwe simply build the Markov Matrix to visualize the topology of the formed complex networks as given by [33]."}, {"heading": "3. Representation Learning Using Deconvolutional Networks", "text": "Deconvolutional networks have the similar mathematical form with convolutional networks. The difference is, deconvolutional networks contain the \u2019inverse\u2019 operation of convolution and pooling for reconstruction."}, {"heading": "3.1. Deconvolution", "text": "Convolutional layers connect multiple input activations within a filter window to a single activation. In contrary, deconvolutional layers associate a single input activation with multiple outputs (Figure 2). The output of the deconvolutional layer is an enlarged and dense feature map. In practice, we crop the boundary of the enlarged feature map to keep the size of the output map identical to the one from the preceding unpooling layer.\nThe learned filters in deconvolutional layers are actually matching the bases to reconstruct the same shape of the input, thus, similar to the convolution network, a hierarchical structure of deconvolutional layers are used to capture different level of shape details. The low level filters tend to capture detailed/finegrained features while the filters in higher layers tends to capture more abstract features. Thus, the network directly takes specific shape information into account for multi-scale feature capturing, which is often ignored in other approaches based only on convolutional layers.\nFor the deconvolution operation, the feature maps are calculated as\nH j = \u03c6(Z j) = \u03c6( \u2211\ni\n\u03b4(Xi) \u2297Wi j + b j) (1)\nwhere Xi represents the i-th element of input and H j denotes the j-th filter map after convolution and activation. The function padding function \u03b4 pads Xi with zeros to keep the output size same with the input. After deconvolution, H j will be processed through a pooling layer. The reconstruction of Xi is built based on H j in a reversed procedure of convolution. The reconstruction works in form of\nY j = \u03c6( \u2211 i \u03b4(H\u0302i) \u2297 W\u0302i j + c j) (2)\nc j is the bias term for the reconstruction Y j. H\u0302i is the feature map extracted by the unpooling layer. The gradient of each parameter in back propagation could be obtained and propagated to the preceding layers in an end-to-end manner until final convergence."}, {"heading": "3.2. Unpooling", "text": "Pooling in convolution network abstracts activations in a receptive field with a single representative value to gain the robustness to noise and translation. Although it helps classification by retaining only robust activations in upper layers, spatial information within a receptive field is lost during pooling. Such information loss may be critical for precise feature learning that is required for reconstruction and classification.\nUnpooling layers in deconvolution network perform the reverse operation of pooling and reconstruct the original size of activations as illustrated in Figure 3. To implement the unpooling operation, we record the locations of the maximum activations selected during pooling operation in the transposed variables, which are employed to place each activation back to its original pooled location. This unpooling strategy is particularly useful to reconstruct the structure of input object. Note that the output of an unpooling layer is an enlarged, yet sparse activation map, which might loss the expressiveness of the complex feature for reconstruction. To resolve the issue, the deconvolution layers is used after the unpooling operation to densify the sparse activations through convolution-like operations with multiple learned filters."}, {"heading": "3.3. Deconvolution for Multivariate Time Series", "text": "In the proposed algorithm, the deconvolution network is a key component for precise feature learning on the multivariate time series data. Contrary to the simple usual deconvolution and pooling both performed with square kernels, our algorithm generates feature maps using deep deconvolution network across the channel but pooling along each individual channel. The dense element-wise deconvolutional map is obtained by successive operations of unpooling, deconvolution, and rectification.\nFigure 4 visualizes the example network structure layer by layer, which is helpful to understand internal operations of our deconvolution network. We can observe that deconvolution\nwith multiple 3\u00d73 filters are applied to capture both the temporal and cross-channel correlation. Lower layers tend to capture overall coarse configuration of the short term signals (e.g. location and frequency), while more complex patterns are discovered in higher layers. Note that pooling/unpooling layer and deconvolution play different roles for the construction of the learned features. Pooling/unpooling captures the significant information within a single channel by tracing each individual position with strong activations back to the signal space. As a result, it effectively reconstructs the detailed structure of the multivariate signals in finer resolutions. On the other hand, learned filters in deconvolutional layers tend to capture the generic generating shapes. Through deconvolution and tied weights, the activations closely related to the generating distribution along each signal and cross the channels are amplified while noisy activations from other regions are suppressed effectively. By the combination of unpooling and deconvolution, our network is able to generates accurate reconstruction of the multivariate time series."}, {"heading": "4. Visualization and Classification", "text": "To visualize the learned representation to inspect and understand, we choose to discretize and convert the final encoding in the hidden layers of the deconvolutional networks to a Markov Matrix, hence visualizing them as complex networks [33].\nAs in Figure 5, a time series X is split into Q = 10 quantiles, each quantile qi is assigned to a node ni \u2208 N in the corresponding network G. Then, nodes ni and n j are connected in the network with the arc where the weight wi j of the arc is given by the probability that a point in quantile qi is followed by a point in quantile q j . Repeated transitions between quantiles results in arcs in the network with larger weights, hence the connection is represented by thicker lines. Note that the discretization is originally based on quantile bins. As indicated in SAX methods that time series tends to follow the Gaussian distribution, we use Gaussian mapping to replace quantile bins for discretization.\nThe deconvolution operation has a sliding window along time, which means the hidden representation should maintain a significant temporal component, thus be particularly within the application domain of SAX and bag-of-words approaches. The bag-of-words dictionary built from the SAX words is benefit from the invariance to locality. Compared with the raw vectorbased representation, these feature bags improves the classifi-\ncation performance as it fits the temporal correlation while increase the expressiveness against noise and outliers. In our experiments, we use both the raw hidden vector and the bag of SAX words for classification."}, {"heading": "5. Experiments and Results", "text": "This section first describes the settings and results of representation learning with deconvolution. Then, we analyze and evaluate the proposed representation in classification and visualization tasks.\nWe primarily use two standard datasets that are broadly appeared in the literature about multivariate time series 2. The ECG dataset contains 200 samples with two channels, among which 133 samples are normal and 67 samples are abnormal.The length of a MTS sample is between 39 and 153. The wafer datasets contain 1194 samples. 1067 samples are normal and 127 samples are abnormal.The length of a sample is between 104 and 198. We preprocess each dataset by standardization and realigning all the signals with the maximum of the length. All missing values are filled by 0. Table 1 gives the statistics summary of each dataset."}, {"heading": "5.1. Representation Learning with Deconvolution", "text": "Figure 4 summarizes the detailed configuration of the proposed network. Our network has symmetrical configuration of convolution and deconvolution network centered around the output of the 2nd Convolutional layer. The input and output layers correspond to input signals and their corresponding reconstruction. We use ReLU as activation function. The network is trained by Adadelta with learning rate 0.1 and \u03c1 = 0.95 3.\n2http://www.cs.cmu.edu/\u223cbobski/ 3Codes are available at https://github.com/cauchyturing/Deconv SAX\nFigure 6 and 7 show the reconstructions by our deconvolutional networks. While the filters trained by the deconvolution captures both the temporal and cross-channel information, combination of unpooling and deconvolution, our network is able to generates accurate reconstruction of the multivariate time series, which guarantees the expressiveness of the learned representations. As shown in Figure 8, After filtering by the deconvolution and pooling/unpooling, the final encoding of each map learned different representation independently. Diverse local patterns (shapes) of time series are captured automatically. Through the deconvolution, the filters determine the importance of each feature by considering both the single channel and cross channel information."}, {"heading": "5.2. Classification", "text": "For classification, we feed both the learned representation vector and the bag of SAX words into a linear SVM. Note that we only use training data to train the representation with deconvolutional networks, then generate the test representation using the well trained model with a single forward pass on the test set. The parameters of SAX, window length n, number of symbols w and alphabet size a is selected using Leave-One-Out cross validation in the training set with Bayesian optimization [34].\nAfter discretization and symbolization, bag of words dictionary are built by a sliding window of length n and convert\neach subsequence into w SAX words. Bag of words catch the features shared in the same structure among different instance and regardless of where they occur. The discretized features are built based on bag of words histogram of the word counts.\nWe compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].\nTable 2 summarizes the classification results4. Our model outperform all other approaches. Even wafer dataset has 6 channels, our approach is still able to capture the precise information through deconvolution to improve the classification performance. Because the datasets are small, supervised deep learning model tends to overfit the label, but in our unsupervised feature learning framework, the model takes advantage of the great expressiveness of the neural networks from the large number of weights to build precise feature set. These precise features provide a more optimal space for classification.\nAnother comparison is performed between the vector and discretized bag-of-words representation from the deconvolutional networks (Table 4). Although discretization by SAX introduces more hyperparameters, both cross validation and test error rate are better than the feature vector. As we analyzed before, the deconvolutional networks learn the representation which preserve the high order abstract temporal information. Through\n4The results with a st are reported as the error rate of 10-fold cross validation on the whole datasets (train + test).\nSAX and bag of words, these information is enhanced particularly for classification in the supervised way. Noise and outliers that are less useful for classification are removed, while the constraint of the dependency on temporal locality is weakened by bag of words. Thus, the bag of SAX words show advantage against the raw vector feature. The Bayesian optimization greatly facilitate the searching process on the hyperparameter and converge fast."}, {"heading": "5.3. Visualization and Statistical Analysis", "text": "Figure 8 has shown how the vector representation is like as time series. To fully understand the representation learned through deconvolution and the effect of discretization through SAX, we flatten and discretize each feature map (which is feed in the classifier as input) and visualize them as complex networks to further inspect other statistical properties.\nAs for the number of discretization bins (or the alphabet size in our SAX settings), we set Q = 60 for the ECG dataset and Q = 120 for the wafer dataset. We use the hierarchical force-directed algorithm as the network layout [35]. As shown in Figure 9 and 10, the demo on the both dataset show different network structures. For ECG, The normal sample tends to have round-shaped layout while the abnormal sample always has a narrow and winded structure. As for wafer, normal sample is shown as a regular closed-form shape, but the structure of the abnormal sample is open while thicker edges piercing through the border.\nTable 3 summarizes four statistics of all the complex networks generated from the deconvolutional representations: average degree, modularity class [36], Pagerank index [37] and average path length. Note that Pagerank index here denotes the largest value in its propagation distribution. For the ECG dataset, all statistics are significantly different between the graphs with different labels under the rejection threshold of 0.05. However, for the Wafer dataset, only average path length show significant difference between two labels. We think the reason is\nthat thicker edges are appearing around the network structures, which indicates the number of edge and their weights are highly skewed. This topological structure would not effect other statistics, but would be reflected in the path degree."}, {"heading": "6. Conclusion and Future Work", "text": "We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. SAX discretization is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach. We further build the Markov matrix from the discretized representation to visualize the time series as complex networks, which show more statistical properties and clear class-specific structures with respect to different labels.\nAs future work, we suppose to integrate grammar induction approach on the deconvolutional SAX words to further infer the semantics of multivariate time series. We are also interested in designing advanced intelligent interfaces to enable the interaction from human to inspect, understand and guide the feature learning and semantic inference for mechanical multivariate signals."}, {"heading": "7. Acknowledgment", "text": "This work is supported by the Natural Science Foundation of China (NSFC) under Grant No. 61472370, 61170223;The National Key Technology Research and Development Program of China under Grant No. 2013BAH23F01; The Education Department of Henan Province under Grant No.13A520453; The Department of Science & Technology of Henan Province under Grant No.142300410229."}], "references": [{"title": "Correlation based dynamic time warping of multivariate time series", "author": ["Z. Bank\u00f3", "J. Abonyi"], "venue": "Expert Systems with Applications 39 (17) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification of multivariate time series using locality preserving projections", "author": ["X. Weng", "J. Shen"], "venue": "Knowledge-Based Systems 21 (7) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Pattern recognition in time series", "author": ["J B.K.D.D. Lin", "S Williamson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Early classification on multivariate time series", "author": ["G. He", "Y. Duan", "R. Peng", "X. Jing", "T. Qian", "L. Wang"], "venue": "Neurocomputing 149 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a symbolic representation for multivariate time series classification", "author": ["M.G. Baydogan", "G. Runger"], "venue": "Data Mining and Knowledge Discovery ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "T", "author": ["Z. Wang"], "venue": "Oates, Pooling sax-bop approaches with boosting to classify multivariate synchronous physiological time series data., in: FLAIRS Conference", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series classification using multi-channels deep convolutional neural networks", "author": ["Y. Zheng", "Q. Liu", "E. Chen", "Y. Ge", "J.L. Zhao"], "venue": "in: International Conference on Web-Age Information Management, Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "S. Lonardi", "B. Chiu"], "venue": "with implications for streaming algorithms, in: Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "An improvement of symbolic aggregate approximation distance measure for time series", "author": ["Y. Sun", "J. Li", "J. Liu", "B. Sun", "C. Chow"], "venue": "Neurocomputing 138 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning 2 (1) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Tech. Rep. MSR-TR-2014-21 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Receptive fields", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "binocular interaction and functional architecture in the cat\u2019s visual cortex, The Journal of physiology 160 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1962}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18 (7) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "in: Proceedings of the 25th international conference on Machine learning, ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Training energy-based models for time-series imputation", "author": ["P. Brakel", "D. Stroobandt", "B. Schrauwen"], "venue": "The Journal of Machine Learning Research 14 (1) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research 11 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Encouraging orthogonality between weight vectors in pretrained deep neural networks", "author": ["K. Grzegorczyk", "M. Kurdziel", "P.I. W\u00f3jcik"], "venue": "Neurocomputing 202 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "An efficient and effective convolutional auto-encoder extreme learning machine network for 3d feature learning", "author": ["Y. Wang", "Z. Xie", "K. Xu", "Y. Dou", "Y. Lei"], "venue": "Neurocomputing 174 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Aligned cluster analysis for temporal segmentation of human motion", "author": ["F. Zhou", "F. Torre", "J.K. Hodgins"], "venue": "in: Automatic Face & Gesture Recognition, 2008. 8th IEEE International Conference on, IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding persisting states for knowledge discovery in time series", "author": ["F. M\u00f6rchen", "A. Ultsch"], "venue": "in: From Data and Information Analysis to Knowledge Engineering, Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "author": ["E. Keogh", "K. Chakrabarti", "M. Pazzani", "S. Mehrotra"], "venue": "Knowl- 7 Wafer Abnormal  Normal Figure 10: Visualization of the complex network generated from the discretized deconvolutional features on the Wafer dataset. edge and information Systems 3 (3) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrence networksa novel paradigm for nonlinear time series analysis", "author": ["R.V. Donner", "Y. Zou", "J.F. Donges", "N. Marwan", "J. Kurths"], "venue": "New Journal of Physics 12 (3) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrence-based time series analysis by means of complex network methods", "author": ["R.V. Donner", "M. Small", "J.F. Donges", "N. Marwan", "Y. Zou", "R. Xiang", "J. Kurths"], "venue": "International Journal of Bifurcation and Chaos 21 (04) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series classification using compression distance of recurrence plots", "author": ["D.F. Silva", "V. Souza", "M. De", "G.E. Batista"], "venue": "in: Data Mining (ICDM), 2013 IEEE 13th International Conference on, IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Duality between time series and networks", "author": ["A.S. Campanharo", "M.I. Sirer", "R.D. Malmgren", "F.M. Ramos", "L.A.N. Amaral"], "venue": "PloS one 6 (8) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient", "author": ["Y. Hu"], "venue": "high-quality force-directed graph drawing, Mathematica Journal 10 (1) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of statistical mechanics: theory and experiment 2008 (10) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Google\u2019s PageRank and beyond: The science of search engine rankings", "author": ["A.N. Langville", "C.D. Meyer"], "venue": "Princeton University Press", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Multivariate time series data is not only characterized by individual attributes, but also by the relationships between the attributes [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "Such information is not captured by the similarity between the individual sequences [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "(ERP) and Time Warping Edit Distance (TWED) are summarized and tested on several benchmark dataset [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "Mining core feature for early classification (MCFEC) along the sequence is proposed to capture the shapelets in each channel independently [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "SMTS builds a tree learner with two ensembles to learn the segmentations and a high-dimensional codebook [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Inspired by recent advances in feature learning for image classification, several feature-based approaches are proposed like [5, 6].", "startOffset": 125, "endOffset": 131}, {"referenceID": 5, "context": "Inspired by recent advances in feature learning for image classification, several feature-based approaches are proposed like [5, 6].", "startOffset": 125, "endOffset": 131}, {"referenceID": 6, "context": "However, the feature learning approach are only limited on the scenario of supervised learning and few comparison towards distance-based learning approaches (like [7]).", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "The method described in [6] is simple but not fully automated, instead they still need to design the weighting scheme manually.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks.", "startOffset": 106, "endOffset": 115}, {"referenceID": 8, "context": "Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks.", "startOffset": 106, "endOffset": 115}, {"referenceID": 5, "context": "Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks.", "startOffset": 106, "endOffset": 115}, {"referenceID": 9, "context": "Since 2006, the techniques developed from deep neural networks (or, deep learning) have greatly impacted natural language processing, speech recognition and computer vision research [11, 12].", "startOffset": 182, "endOffset": 190}, {"referenceID": 10, "context": "Since 2006, the techniques developed from deep neural networks (or, deep learning) have greatly impacted natural language processing, speech recognition and computer vision research [11, 12].", "startOffset": 182, "endOffset": 190}, {"referenceID": 11, "context": "One successful deep learning architecture used in computer vision is convolutional neural networks (CNN) [13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "CNNs exploit translational invariance by extracting features through receptive fields [14] and learning with weight sharing, becoming the state-of-the-art approach in various image recognition and computer vision tasks [15].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "CNNs exploit translational invariance by extracting features through receptive fields [14] and learning with weight sharing, becoming the state-of-the-art approach in various image recognition and computer vision tasks [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 14, "context": "Most exciting advance comes from the exploration of unsupervised learning algorithms for generative models, such as Deep Belief Networks (DBN) and Denoised Auto-encoders (DA) [16, 17].", "startOffset": 175, "endOffset": 183}, {"referenceID": 15, "context": "Most exciting advance comes from the exploration of unsupervised learning algorithms for generative models, such as Deep Belief Networks (DBN) and Denoised Auto-encoders (DA) [16, 17].", "startOffset": 175, "endOffset": 183}, {"referenceID": 16, "context": "A training strategy inspired by recent work on optimizationbased learning is proposed to train complex neural networks for imputation tasks [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "A generalized Denoised Auto-encoder extends the theoretical framework and is applied to Deep Generative Stochastic Networks (DGSN) [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 137, "endOffset": 145}, {"referenceID": 19, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 137, "endOffset": 145}, {"referenceID": 20, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 324, "endOffset": 336}, {"referenceID": 21, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 324, "endOffset": 336}, {"referenceID": 22, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 324, "endOffset": 336}, {"referenceID": 23, "context": "Aligned Cluster Analysis (ACA) is introduced as an unsupervised method to cluster the temporal patterns of human motion data [27].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "Persist is an unsupervised discretization methods to maximize the persistence measurement of each symbol [28].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "Piecewise Aggregate Approximation (PAA) methods is proposed by Keogh [29] to reduce the dimensionality of time series, which is then upgraded to Symbolic Aggregate Approximation (SAX) [8].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Piecewise Aggregate Approximation (PAA) methods is proposed by Keogh [29] to reduce the dimensionality of time series, which is then upgraded to Symbolic Aggregate Approximation (SAX) [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 26, "context": "Recurrence Networks were proposed to analyze the structural properties of time series from complex systems [30, 31].", "startOffset": 107, "endOffset": 115}, {"referenceID": 27, "context": "Recurrence Networks were proposed to analyze the structural properties of time series from complex systems [30, 31].", "startOffset": 107, "endOffset": 115}, {"referenceID": 28, "context": "extended the recurrence plot paradigm for time series classification using compression distance [32].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "Another way to build a weighted adjacency matrix is extracting transition dynamics from the first order Markov matrix [33].", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "we simply build the Markov Matrix to visualize the topology of the formed complex networks as given by [33].", "startOffset": 103, "endOffset": 107}, {"referenceID": 29, "context": "To visualize the learned representation to inspect and understand, we choose to discretize and convert the final encoding in the hidden layers of the deconvolutional networks to a Markov Matrix, hence visualizing them as complex networks [33].", "startOffset": 238, "endOffset": 242}, {"referenceID": 30, "context": "The parameters of SAX, window length n, number of symbols w and alphabet size a is selected using Leave-One-Out cross validation in the training set with Bayesian optimization [34].", "startOffset": 176, "endOffset": 180}, {"referenceID": 2, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 227, "endOffset": 230}, {"referenceID": 4, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 251, "endOffset": 254}, {"referenceID": 5, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 271, "endOffset": 274}, {"referenceID": 31, "context": "We use the hierarchical force-directed algorithm as the network layout [35].", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "Table 3 summarizes four statistics of all the complex networks generated from the deconvolutional representations: average degree, modularity class [36], Pagerank index [37] and average path length.", "startOffset": 148, "endOffset": 152}, {"referenceID": 33, "context": "Table 3 summarizes four statistics of all the complex networks generated from the deconvolutional representations: average degree, modularity class [36], Pagerank index [37] and average path length.", "startOffset": 169, "endOffset": 173}], "year": 2016, "abstractText": "We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels.", "creator": "LaTeX with hyperref package"}}}