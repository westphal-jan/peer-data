{"id": "1610.07272", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Bridging Neural Machine Translation and Bilingual Dictionaries", "abstract": "neural machine translation ( nmt ) has become the new state - of - the - art in several language pairs. basically however, it remains a challenging problem how to integrate nmt with a bilingual dictionary which mainly contains words rarely or never again seen in the bilingual training data. in this recent paper, we propose two methods namely to bridge nmt and the bilingual dictionaries. the core idea behind is to design novel models that transform directly the bilingual comprehension dictionaries into adequate sentence pairs, so that nmt can accurately distil latent bilingual mappings from the ample and repetitive phenomena. one method leverages a mixed word / character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. extensive new experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can thereby obtain correct dialogue translations if they are covered by the dictionary.", "histories": [["v1", "Mon, 24 Oct 2016 03:39:22 GMT  (1466kb,D)", "http://arxiv.org/abs/1610.07272v1", "10 pages, 2 figures"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiajun zhang", "chengqing zong"], "accepted": false, "id": "1610.07272"}, "pdf": {"name": "1610.07272.pdf", "metadata": {"source": "CRF", "title": "Bridging Neural Machine Translation and Bilingual Dictionaries", "authors": ["Jiajun Zhang", "Chengqing Zong"], "emails": ["jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary."}, {"heading": "1 Introduction", "text": "Due to its superior ability in modelling the end-to-end translation process, neural machine translation (NMT), recently proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015b; Wu et al., 2016).\nTypically, NMT adopts the encoder-decoder architecture which consists of two recurrent neural networks. The encoder network models the semantics of the source sentence and transforms the source sentence into the context vector representation, from which the decoder network generates the target translation word by word.\nOne important feature of NMT is that each word in the vocabulary is mapped into a lowdimensional real-valued vector (word embedding). The use of continuous representations enables NMT to learn latent bilingual mappings for accurate translation and explore the statistical similarity between words (e.g. desk and table) as well. As a disadvantage of the statistical models, NMT can learn good word embeddings and accurate bilingual mappings only when the words occur frequently in the parallel sentence pairs. However, low-frequency words are ubiquitous, especially when the training data is not enough (e.g. lowresource language pairs). Fortunately, in many language pairs and domains, we have handmade bilingual dictionaries which mainly contain words rarely or never seen in the training corpus. Therefore, it remains a big challenge how to bridge NMT and the bilingual dictionaries.\nRecently, Arthur et al. (2016) attempt at incorporating discrete translation lexicons into NMT. The main idea of their method is leveraging the discrete translation lexicons to positively influence the probability distribution of the output words in the NMT softmax layer. However, their approach only addresses the translation lexicons which are in the restricted vocabulary 1 of NMT. The out-ofvocabulary (OOV) words are out of their consideration.\n1NMT usually keeps only the words whose occurrence is more than a threshold (e.g. 10), since very rare words can not yield good embeddings and large vocabulary leads to high computational complexity.\nar X\niv :1\n61 0.\n07 27\n2v 1\n[ cs\n.C L\n] 2\n4 O\nct 2\n01 6\nIn this paper, we aim at making full use of all the bilingual dictionaries, especially the ones covering the rare or OOV words. Our basic idea is to transform the low-frequency word pair in bilingual dictionaries into adequate sequence pairs which guarantee the frequent occurrence of the word pair, so that NMT can learn translation mappings between the source word and the target word.\nTo achieve this goal, we propose two methods, as shown in Fig. 1. In the test sentence, the Chinese word l\u030cihua\u0304 appears only once in our training data and the baseline NMT cannot correctly translate this word. Fortunately, our bilingual dictionary contains this translation lexicon. Our first method extends the mixed word/character model proposed by Wu et al. (2016) to re-label the rare words in both of the dictionary and training data with character sequences in which characters are now frequent and the character translation mappings can be learnt by NMT. Instead of backing off words into characters, our second method is well designed to synthesize adequate pseudo sentence pairs containing the translation lexicon, allowing NMT to learn the word translation mappings.\nWe make the following contributions in this paper:\n\u2022 We propose a low-frequency to highfrequency framework to bridge NMT and the bilingual dictionaries.\n\u2022 We propose and investigate two methods to utilize the bilingual dictionaries. One ex-\ntends the mixed word/character model and the other designs a pseudo sentence pair synthesis model.\n\u2022 The extensive experiments on Chinese-toEnglish translation show that our proposed methods significantly outperform the strong attention-based NMT. We further find that most of rare words can be correctly translated, as long as they are covered by the bilingual dictionary."}, {"heading": "2 Neural Machine Translation", "text": "Our framework bridging NMT and the discrete bilingual dictionaries can be applied in any neural machine translation model. Without loss of generality, we use the attention-based NMT proposed by (Luong et al., 2015b), which utilizes stacked Long-Short Term Memory (LSTM, (Hochreiter and Schmidhuber, 1997)) layers for both encoder and decoder as illustrated in Fig. 2.\nThe encoder-decoder NMT first encodes the source sentence X = (x1, x2, \u00b7 \u00b7 \u00b7 , xTx) into a sequence of context vectors C = (h1,h2, \u00b7 \u00b7 \u00b7 ,hTx) whose size varies with respect to the source sentence length. Then, the encoder-decoder NMT decodes from the context vectors C and generates target translation Y = (y1, y2, \u00b7 \u00b7 \u00b7 , yTy) one word each time by maximizing the probability of p(yi|y<i, C). Note that xj (yi) is word embedding corresponding to the jth (ith) word in the source (target) sentence. Next, we briefly review the en-\ncoder introducing how to obtainC and the decoder addressing how to calculate p(yi|y<i, C).\nEncoder: The context vectors C = (hm1 ,h m 2 , \u00b7 \u00b7 \u00b7 ,hmTx) are generated by the encoder using m stacked LSTM layers. hkj is calculated as follows:\nhkj = LSTM(h k j\u22121,h k\u22121 j ) (1)\nWhere hk\u22121j = xj if k = 1. Decoder: The conditional probability p(yi|y<i, C) is computed in different ways according to the choice of the context C at time i. In (Cho et al., 2014), the authors choose C = hmTx , while Bahdanau et al. (2014) use different context ci at different time step and the conditional probability will become:\np(yi|y<i, C) = p(yi|y<i, ci) = softmax(Wz\u0302i)) (2)\nwhere z\u0302i is the attention output:\nz\u0302i = tahn(Wc[z l i; ci]) (3)\nThe attention model calculates ci as the weighted sum of the source-side context vectors, just as illustrated in the middle part of Fig. 2.\nci = Tx\u2211 j=1 \u03b1ijz l i (4)\nwhere \u03b1ij is a normalized item calculated as follows:\n\u03b1ij = hmj \u00b7 zli\u2211 j\u2032 h m j\u2032 \u00b7 zli\n(5)\nzki is computed using the following formula:\nzki = LSTM(z k i\u22121, z k\u22121 i ) (6)\nIf k = 1, z1i will be calculated by combining z\u0302i\u22121 as feed input (Luong et al., 2015b):\nz1i = LSTM(z 1 i\u22121, yi\u22121, z\u0302i\u22121) (7)\nGiven the sentence aligned bilingual training data Db = {(X (n) b , Y (n) b )} N n=1 , all the parameters of the encoder-decoder NMT are optimized to maximize the following conditional loglikelihood:\nL(\u03b8) = 1 N N\u2211 n=1 Ty\u2211 i=1 logp(y (n) i |y (n) <i , X (n), \u03b8) (8)"}, {"heading": "3 Incorporating Bilingual Dictionaries", "text": "The word translation pairs in bilingual dictionaries are difficult to use in neural machine translation, mainly because they are rarely or never seen in the parallel training corpus. We attempt to build a bridge between NMT and bilingual dictionaries. We believe the bridge is data transformation that can transform rarely or unseen word translation pairs into frequent ones and provide NMT adequate information to learn latent translation mappings. In this work, we propose two methods to perform data transformation from character level and word level respectively."}, {"heading": "3.1 Mixed Word/Character Model", "text": "Given a bilingual dictionary Dic = {(Dic(i)x , Dic(i)y )}Ii=1, we focus on the translation lexicons (Dicx, Dicy) if Dicx is a rare or unknown word in the bilingual corpus Db.\nWe first introduce data transformation using the character-based method. We all know that words\nare composed of characters and most of the characters are frequent even though the word is never seen. This idea is popularly used to deploy open vocabulary NMT (Ling et al., 2015; Costa-Jussa\u0300 and Fonollosa, 2016; Chung et al., 2016).\nCharacter translation mappings are much easier to learn for NMT than word translation mappings. However, given a character sequence of a source language word, NMT cannot guarantee the generated character sequence would lead to a valid target language word. Therefore, we prefer the framework mixing the words and characters, which is employed by Wu et al. (2016) to handle OOV words. If it is a frequent word, we keep it unchanged. Otherwise, we fall back to the character sequence.\nWe perform data transformation on both parallel training corpus and bilingual dictionaries. Here, English sentences and words are adopted as examples. Suppose we keep the English vocabulary V in which the frequency of each word exceeds a threshold K. For each English word w (e.g. oak) in a parallel sentence pair (Xb, Yb) or in a translation lexicon (Dicx, Dicy), if w \u2208 V , w will be left as it is. Otherwise, w is re-labelled by character sequence. For example, oak will be:\noak \u2192 \u3008B\u3009o \u3008M\u3009a \u3008E\u3009k (9)\nWhere \u3008B\u3009, \u3008M\u3009 and \u3008E\u3009 denotes respectively begin, middle and end of a word."}, {"heading": "3.2 Pseudo Sentence Pair Synthesis Model", "text": "Since NMT is a data driven approach, it can learn latent translation mappings for a word pair (Dicx, Dicy) if these exist many parallel sentences containing (Dicx, Dicy). Along this line, we propose the pseudo sentence pair synthesis model. In this model, we aim at synthesizing for a rare or unknown translation lexicon (Dicx, Dicy) the adequate pseudo parallel sentences {(Xjp , Y jp )}Jj=1 each of which contains (Dicx, Dicy).\nAlthough there are no enough bilingual sentence pairs in many languages (and many domains), a huge amount of the monolingual data is available in the web. In this paper, we plan to make use of the source-side monolingual data Dsm = {(X(m)sm }Mm=1 (M N ) to synthesize the pseudo bilingual sentence pairs Dbp = {(Xjp , Y jp )}Jj=1.\nFor constructingDbp, we resort to statistical machine translation (SMT) and apply a self-learning\nAlgorithm 1 Pseudo Sentence Pair Synthesis. Input: bilingual training data Db; bilingual dic-\ntionary Dic; source language monolingual dataDsm; pseudo sentence pair numberK for each (Dicx, Dicy); Output: pseudo sentence pairs Dbp = {(Xjp , Y jp )}Jj=1:\n1: Build an SMT system PBMT on {Db,Dic}; 2: Dbp = {}; 3: for each (Dicx, Dicy) in Dic do 4: Retrieve K monolingual sentences {Xkp }Kk=1 containing Dicx from Dsm; 5: Translate {Xkp }Kk=1 into target language sentences {Y kp }Kk=1 using PBMT; 6: Add {Xkp , Y kp }Kk=1 into Dbp; 7: end for 8: return Dbp\nmethod as illustrated in Algorithm 1. In contrast to NMT, statistical machine translation (SMT, e.g. phrase-based SMT (Koehn et al., 2007; Xiong et al., 2006)) is easy to integrate bilingual dictionaries (Wu et al., 2008) as long as we consider the translation lexicons of bilingual dictionaries as phrasal translation rules. Following (Wu et al., 2008), we first merge the bilingual sentence corpus Db with the bilingual dictionaries Dic, and employ the phrase-based SMT to train an SMT system called PBMT (line 1 in Algorithm 1).\nFor each rare or unknown word translation pair (Dicx, Dicy), we can easily retrieve the adequate source language monolingual sentences {(Xkp )}Kk=1 (Dicx \u2208 Xkp ) from the web or other data collections. PBMT is then applied to translate {(Xkp )}Kk=1 to generate target language translations {(Y kp )}Kk=1. As PBMT employs the bilingual dictionaries Dic as additional translation rules, each target translation sentence Yp \u2208 {(Y kp )}Kk=1 will contain Dicy. Then, the sentence pair (Xkp , Y k p ) will include the word translation pair (Dicx, Dicy). Finally, we can pair {(Xkp )}Kk=1 and {(Y kp )}Kk=1 to yield pseudo sentence pairs {(Xkp , Y kp )}Kk=1, which will be added into Dbp (line 2-6 in Algorithm 1).\nThe original bilingual corpusDb and the pseudo bilingual sentence pairs Dbp are combined together to train a new NMT model. Some may worry that the target parts of Dbp are SMT results but not well-formed sentences which would harm NMT training. Fortunately, Sennrich et\nal. (2015b), Cheng et al. (2016b) and Zhang and Zong (2016) observe from large-scale experiments that the synthesized bilingual data using self-learning framework can substantially improve NMT performance. Since Dbp now contains bilingual dictionaries, we expect that the NMT trained on {Db,Dbp} cannot only significantly boost the translation quality, but also solve the problem of rare word translation if they are covered by Dic.\nNote that the pseudo sentence pair synthesis model can be further augmented by the mixed word/character model to solve other OOV translations."}, {"heading": "4 Experimental Settings", "text": "In this section we describe the data sets, data preprocessing, the training and evaluation details, and all the translation methods we compare in the experiments."}, {"heading": "4.1 Dataset", "text": "We perform the experiments on Chinese-toEnglish translation. Our bilingual training data Db includes 630K2 sentence pairs (each sentence length is limited up to 50 words) extracted from LDC corpora3. For validation, we choose NIST 2003 (MT03) dataset. For testing, we use NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2006 (MT08) datasets. The test sentences are remained as their original length. As for the source-side monolingual data Dsm, we collect about 100M Chinese sentences in which approximately 40% are provided by Sogou and the rest are collected by searching the words in the bilingual data from the web. We use two bilingual dictionaries: one is from LDC (LDC2002L27) and the other is manually collected by ourselves. The combined dictionary Dic contains 86,252 translation lexicons in total."}, {"heading": "4.2 Data Preprocessing", "text": "If necessary, the Chinese sentences are word segmented using Stanford Word Segmenter4. The English sentences are tokenized using the tokenizer script from the Moses decoder5. We limit the vocabulary in both Chinese and English using a fre-\n2Without using very large-scale data, it is relatively easy to evaluate the effectiveness of the bilingual dictionaries.\n3LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07.\n4http://nlp.stanford.edu/software/segmenter.shtml 5http://www.statmt.org/moses/\nquency threshold u. We choose uc = 10 for Chinese and ue = 8 for English, resulting |Vc| = 38815 and |Ve| = 30514 for Chinese and English respectively in Db. As we focus on rare or unseen translation lexicons of the bilingual dictionary Dic in this work, we filter Dic and retain the ones (Dicx, Dicy) if Dicx /\u2208 Vc, resulting 8306 entries in which 2831 ones appear in the validation and test data sets. All the OOV words are replaced with UNK in the word-based NMT and are re-labelled into character sequences in the mixed word/character model."}, {"heading": "4.3 Training and Evaluation Details", "text": "We build the described models using the Zoph RNN6 toolkit which is written in C++/CUDA and provides efficient training across multiple GPUs. In the NMT architecture as illustrated in Fig. 2, the encoder includes two stacked LSTM layers, followed by a global attention layer, and the decoder also contains two stacked LSTM layers followed by the softmax layer. The word embedding dimension and the size of hidden layers are all set to 1000.\nEach NMT model is trained on GPU K80 using stochastic gradient decent algorithm AdaGrad (Duchi et al., 2011). We use a mini batch size of B = 128 and we run a total of 20 iterations for all the data sets. The training time for each model ranges from 2 days to 4 days. At test time, we employ beam search with beam size b = 10. We use case-insensitive 4-gram BLEU score as the automatic metric (Papineni et al., 2002) for translation quality evaluation."}, {"heading": "4.4 Translation Methods", "text": "In the experiments, we compare our method with the conventional SMT model and the baseline attention-based NMT model. We list all the translation methods as follows:\n\u2022 Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007). We use its default configuration and train a 4-gram language model on the target portion of the bilingual training data.\n\u2022 Zoph RNN: It is the baseline attention-based NMT system (Luong et al., 2015a; Zoph et al., 2016) using two stacked LSTM layers for both of the encoder and the decoder.\n6https://github.com/isi-nlp/Zoph RNN\n\u2022 Zoph RNN-mixed-dic: It is our NMT system which integrates the bilingual dictionaries by re-labelling the rare or unknown words with character sequence on both bilingual training data and bilingual dictionaries. Zoph RNN-mixed indicates that mixed word/character model is performed only on the bilingual training data and the bilingual dictionary is not used.\n\u2022 Zoph RNN-pseudo-dic: It is our NMT system that integrates the bilingual dictionaries by synthesizing adequate pseudo sentence pairs that contain the focused rare or unseen translation lexicons. Zoph RNNpseudo means that the target language parts of pseudo sentence pairs are obtained by the SMT system PBMT without using the bilingual dictionary Dic.\n\u2022 Zoph RNN-pseudo-mixed-dic: It is a NMT system combining the two methods Zoph RNN-pseudo and Zoph RNN-mixed. Zoph RNN-pseudo-mixed is similar to Zoph RNN-pseudo."}, {"heading": "5 Translation Results and Analysis", "text": "For translation quality evaluation, we attempt to figure out the following three questions: 1) Could the employed attention-based NMT outperform SMT even on less than 1 million sentence pairs? 2) Which model is more effective for integrating the bilingual dictionaries: mixed word/character model or pseudo sentence pair synthesis data? 3)\nCan the combined two proposed methods further boost the translation performance?"}, {"heading": "5.1 NMT vs. SMT", "text": "Table 1 reports the detailed translation quality for different methods. Comparing the first two lines in Table 1, it is very obvious that the attentionbased NMT system Zoph RNN substantially outperforms the phrase-based SMT system Moses on just 630K bilingual Chinese-English sentence pairs. The gap can be as large as 6.36 absolute BLEU points on MT04. The average improvement is up to 4.43 BLEU points (32.98 vs. 28.55). It is in line with the findings reported in (Wu et al., 2016; Junczys-Dowmunt et al., 2016) which conducted experiments on tens of millions or even more parallel sentence pairs. Our experiments further show that NMT can be still much better even we have less than 1 million sentence pairs."}, {"heading": "5.2 The Effect of The Mixed W/C Model", "text": "The two lines (3-4 in Table 1) presents the BLEU scores when applying the mixed word/character model in NMT. We can see that this model markedly improves the translation quality over the baseline attention-based NMT, although the idea behind is very simple.\nSpecifically, the system Zoph RNN-mixed, trained only on the bitext Db, achieves an average improvement of more than 1.0 BLEU point (34.19 vs 32.98) over the baseline Zoph RNN. It indicates that the mixed word/character model can alleviate the OOV translation problem to some ex-\ntent. For example, the number 31.3 is an OOV word in Chinese. The mixed model transforms this word into \u3008B\u30093 \u3008M\u30091 \u3008M\u3009. \u3008E\u30093 and it is correctly copied into target side, yielding a correct translation 31.3. Moreover, some named entities (e.g. person name hecker) can be well translated.\nWhen adding the bilingual dictionary Dic as training data, the system Zoph RNN-mixed-dic further gets a moderate improvement of 0.51 BLEU points (34.70 vs 34.19) on average. We find that the mixed model could make use of some rare or unseen translation lexicons in NMT, as illustrated in the first two parts of Table 2. In the first part of Table 2, the English side of the translation lexicon is a frequent word (e.g. remain). The Chinese frequent character (e.g. liu\u0301) shares the most meaning of the whole word (zhu\u0300liu\u0301) and thus it could be correctly translated into remain. We are a little surprised by the examples in the second part of Table 2, since the correct English parts are all OOV words which require each English character to be correctly generated. It demonstrates that the mixed model has some ability to predict the correct character sequence. However, this mixed model fails in many scenarios. The third part in Table 2 gives some bad cases. If the first predicted character is wrong, the final word translation will be incorrect (e.g. take-owned lane vs. overtaking lane). This is the main reason why the mixed model could not obtain large improvements."}, {"heading": "5.3 The Effect of Data Synthesis Model", "text": "The eight lines (5-12) in Table 1 show the translation performance of the pseudo sentence pair synthesis model. We can analyze the results from three perspectives: 1) the effect of the self-\nlearning method for using the source-side monolingual data; 2) the effect of the bilingual dictionary; and 3) the effect of pseudo sentence pair number.\nThe results in the odd lines (lines with Zoph RNN-pseudo) demonstrate that the synthesized parallel sentence pairs using source-side monolingual data can significantly improve the baseline NMT Zoph RNN, and the average improvement can be up to 1.62 BLEU points (34.60 vs. 32.98). This finding is also reported by Cheng et al. (2016b) and Zhang and Zong (2016).\nAfter augmenting Zoph RNN-pseudo with bilingual dictionaries, we can further obtain considerable gains. The largest average improvement can be 3.41 BLEU points when compared to the baseline NMT Zoph RNN and 2.04 BLEU points when compared to Zoph RNN-pseudo (35.86 vs. 33.82).\nWhen investigating the effect of pseudo sentence pair number (from K = 10 to K = 40), we find that the performance is largely better and better if we synthesize more pseudo sentence pairs for each rare or unseen word translation pair (Dicx, Dicy). We can also notice that improvement gets smaller and smaller when K grows."}, {"heading": "5.4 Mixed W/C Model vs. Data Synthesis Model", "text": "Comparing the results between the mixed model and the data synthesis model (Zoph RNN-mixeddic vs. Zoph RNN-pseudo-dic) in Table 1, we can easily see that the data synthesis model is much better to integrate bilingual dictionaries in NMT. Zoph RNN-pseudo-dic can substantially outperform Zoph RNN-mixed-dic by an average improvement up to 1.69 BLEU points (36.39 vs. 34.70).\nThrough a deep analysis, we find that most of rare or unseen words in test sets can be well translated by Zoph RNN-pseudo-dic if they are covered by the bilingual dictionary. Table 3 reports the hit rate of the bilingual dictionaries. 0.71 indicates that 2010 (2831 \u00d7 0.71) words among the 2831 covered rare or unseen words in the test set can\nbe correctly translated. This table explains why Zoph RNN-pseudo-dic performs much better than Zoph RNN-mixed-dic.\nThe last two lines in Table 1 demonstrate that the combined method can further boost the translation quality. The biggest average improvement over the baseline NMT Zoph RNN can be as large as 4.62 BLEU points, which is very promising. We believe that this method fully exploits the capacity of the data synthesis model and the mixed model. Zoph RNN-pseudo-dic can well incorporate the bilingual dictionary and Zoph RNN-mixed can well handle the OOV word translation. Thus, the combined method is the best.\nOne may argue that the proposed methods use bigger vocabulary and the performance gains may be attributed to the increased vocabulary size. We further conduct an experiment for the baseline NMT Zoph RNN by setting |Vc| = 4600 and |Ve| = 3400. We find that this setting decreases the translation quality by an average BLEU points 0.88 (32.10 vs. 32.98). This further verifies the superiority of our proposed methods."}, {"heading": "6 Related Work", "text": "The recently proposed neural machine translation has drawn more and more attention. Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al., 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJussa\u0300 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al., 2015; Sennrich et al., 2015a; Cheng et al., 2016b; Zhang and Zong, 2016).\nOur focus in this work is aiming to fully integrate the discrete bilingual dictionaries into NMT. The most related works lie in three aspects: 1) applying the character-based method to deal with open vocabulary; 2) making use of the synthesized data in NMT, and 3) incorporating translation lexicons in NMT.\nLing et al. (2015), Costa-Jussa\u0300 and Fonollosa (2016) and Sennrich et al. (2015b) propose purely character-based or subword-based neural machine\ntranslation to circumvent the open word vocabulary problem. Luong et al. (2015c) and Wu et al. (2016) present the mixed word/character model which utilizes character sequence to replace the OOV words. We introduce the mixed model to integrate the bilingual dictionaries and find that it is useful but not the best method.\nSennrich et al. (2015a) propose an approach to use target-side monolingual data to synthesize the bitexts. They generate the synthetic bilingual data by translating the target monolingual sentences to source language sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data. Cheng et al. (2016b) and Zhang and Zong (2016) also investigate the effect of the synthesized parallel sentences. They report that the pseudo sentence pairs synthesized using the source-side monolingual data can significantly improve the translation quality. These studies inspire us to leverage the synthesized data to incorporate the bilingual dictionaries in NMT.\nVery recently, Arthur et al. (2016) try to use discrete translation lexicons in NMT. Their approach attempts to employ the discrete translation lexicons to positively influence the probability distribution of the output words in the NMT softmax layer. However, their approach only focuses on the words that belong to the vocabulary and the outof-vocabulary (OOV) words are not considered. In contrast, we concentrated ourselves on the word translation lexicons which are rarely or never seen in the bilingual training data. It is a much tougher problem. The extensive experiments demonstrate that our proposed models, especially the data synthesis model, can solve this problem very well."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we have presented two models to bridge neural machine translation and the bilingual dictionaries in which translation lexicons are rarely or never seen in the bilingual training data. Our proposed methods focus on data transformation mechanism which guarantees the massive and repetitive occurrence of the translation lexicon.\nThe mixed word/character model tackles this problem by re-labelling the OOV words with character sequence, while our data synthesis model constructs adequate pseudo sentence pairs for each translation lexicon. The extensive experiments show that the data synthesis model substantially outperforms the mixed word/character model, and\nthe combined method performs best. All of the proposed methods obtain promising improvements over the baseline NMT. We further find that more than 70% of the rare or unseen words in test sets can get correct translations as long as they are covered by the bilingual dictionary.\nCurrently, the data synthesis model does not distinguish the original bilingual training data from the synthesized parallel sentences in which the target sides are SMT translation results. In the future work, we plan to modify the neural network structure to avoid the negative effect of the SMT translation noise."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "arXiv preprint arXiv:1606.02006", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "2016a. Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2016a] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of AAAI 2016", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Cheng et al.2016b] Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.06147", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Cohn et al.2016] Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": "Proceedings of NAACL 2016", "citeRegEx": "Cohn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Characterbased neural machine translation", "author": ["Costa-Juss\u00e0", "Jos\u00e9 AR Fonollosa"], "venue": "arXiv preprint arXiv:1603.00810", "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Implicit distortion and fertility models for attention-based encoder-decoder nmt model", "author": ["Feng et al.2016] Shi Feng", "Shujie Liu", "Mu Li", "Ming Zhou"], "venue": "arXiv preprint arXiv:1601.03317", "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "HueiChi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Tomasz Dwojak", "Hieu Hoang"], "venue": "arXiv preprint arXiv:1610.01108", "citeRegEx": "JunczysDowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "JunczysDowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Lee et al.2016] Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1610.03017", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Towards zero unknown word in neural machine translation", "author": ["Li et al.2016] Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of IJCAI 2016", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Neural machine translation with supervised attention", "author": ["Liu et al.2016] Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "venue": "arXiv preprint arXiv:1609.04186", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "arXiv preprint arXiv:1511.06114", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Interactive attention for neural machine translation", "author": ["Meng et al.2016] Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu"], "venue": "arXiv preprint arXiv:1610.05011", "citeRegEx": "Meng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "2016a. A coverage embedding model for neural machine translation", "author": ["Mi et al.2016a] Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Supervised attentions for neural machine translation", "author": ["Mi et al.2016b] Haitao Mi", "Zhiguo Wang", "Niyu Ge", "Abe Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "2016c. Vocabulary manipulation for large vocabulary neural machine translation", "author": ["Mi et al.2016c] Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2016] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proceedings of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora", "author": ["Wu et al.2008] Hua Wu", "Haifeng Wang", "Chengqing Zong"], "venue": "In Proceedings of COLING", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Xiong et al.2006] Deyi Xiong", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of ACL-COLING,", "citeRegEx": "Xiong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Zhang", "Zong2016] Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Zoph et al.2016] Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "In Proceedings of NAACL", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 31, "context": "Due to its superior ability in modelling the end-to-end translation process, neural machine translation (NMT), recently proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al.", "startOffset": 132, "endOffset": 206}, {"referenceID": 31, "context": ", 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015b; Wu et al., 2016).", "startOffset": 197, "endOffset": 306}, {"referenceID": 1, "context": ", 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015b; Wu et al., 2016).", "startOffset": 197, "endOffset": 306}, {"referenceID": 0, "context": "Recently, Arthur et al. (2016) attempt at incorporating discrete translation lexicons into NMT.", "startOffset": 10, "endOffset": 31}, {"referenceID": 33, "context": "Our first method extends the mixed word/character model proposed by Wu et al. (2016) to re-label the rare words in both of the dictionary and training data with character sequences in which characters are now frequent and the character translation mappings can be learnt by NMT.", "startOffset": 68, "endOffset": 85}, {"referenceID": 1, "context": ", 2014), the authors choose C = hmTx , while Bahdanau et al. (2014) use different context ci at different time step and the conditional probability will become:", "startOffset": 45, "endOffset": 68}, {"referenceID": 18, "context": "This idea is popularly used to deploy open vocabulary NMT (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 58, "endOffset": 130}, {"referenceID": 5, "context": "This idea is popularly used to deploy open vocabulary NMT (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 58, "endOffset": 130}, {"referenceID": 33, "context": "Therefore, we prefer the framework mixing the words and characters, which is employed by Wu et al. (2016) to handle OOV words.", "startOffset": 89, "endOffset": 106}, {"referenceID": 15, "context": "phrase-based SMT (Koehn et al., 2007; Xiong et al., 2006)) is easy to integrate bilingual dictionaries (Wu et al.", "startOffset": 17, "endOffset": 57}, {"referenceID": 34, "context": "phrase-based SMT (Koehn et al., 2007; Xiong et al., 2006)) is easy to integrate bilingual dictionaries (Wu et al.", "startOffset": 17, "endOffset": 57}, {"referenceID": 33, "context": ", 2006)) is easy to integrate bilingual dictionaries (Wu et al., 2008) as long as we consider the translation lexicons of bilingual dictionaries as phrasal translation rules.", "startOffset": 53, "endOffset": 70}, {"referenceID": 33, "context": "Following (Wu et al., 2008), we first merge the bilingual sentence corpus Db with the bilingual dictionaries Dic, and employ the phrase-based SMT to train an SMT system called PBMT (line 1 in Algorithm 1).", "startOffset": 10, "endOffset": 27}, {"referenceID": 2, "context": "(2015b), Cheng et al. (2016b) and Zhang and Zong (2016) observe from large-scale experiments that the synthesized bilingual data using self-learning framework can substantially improve NMT performance.", "startOffset": 9, "endOffset": 30}, {"referenceID": 2, "context": "(2015b), Cheng et al. (2016b) and Zhang and Zong (2016) observe from large-scale experiments that the synthesized bilingual data using self-learning framework can substantially improve NMT performance.", "startOffset": 9, "endOffset": 56}, {"referenceID": 8, "context": "Each NMT model is trained on GPU K80 using stochastic gradient decent algorithm AdaGrad (Duchi et al., 2011).", "startOffset": 88, "endOffset": 108}, {"referenceID": 27, "context": "We use case-insensitive 4-gram BLEU score as the automatic metric (Papineni et al., 2002) for translation quality evaluation.", "startOffset": 66, "endOffset": 89}, {"referenceID": 15, "context": "\u2022 Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007).", "startOffset": 60, "endOffset": 80}, {"referenceID": 36, "context": "\u2022 Zoph RNN: It is the baseline attention-based NMT system (Luong et al., 2015a; Zoph et al., 2016) using two stacked LSTM layers for both of the encoder and the decoder.", "startOffset": 58, "endOffset": 98}, {"referenceID": 2, "context": "This finding is also reported by Cheng et al. (2016b) and Zhang and Zong (2016).", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": "This finding is also reported by Cheng et al. (2016b) and Zhang and Zong (2016).", "startOffset": 33, "endOffset": 80}, {"referenceID": 6, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 9, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 19, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 23, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 32, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 30, "context": ", 2016), better objective functions for BLEU evaluation (Shen et al., 2016), better strategies for handling open vocabulary (Ling et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 18, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 12, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 16, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 17, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 10, "context": ", 2016) and exploiting large-scale monolingual data (Gulcehre et al., 2015; Sennrich et al., 2015a; Cheng et al., 2016b; Zhang and Zong, 2016).", "startOffset": 52, "endOffset": 142}, {"referenceID": 2, "context": "Cheng et al. (2016b) and Zhang and Zong (2016) also investigate the effect of the synthesized parallel sentences.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Cheng et al. (2016b) and Zhang and Zong (2016) also investigate the effect of the synthesized parallel sentences.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Very recently, Arthur et al. (2016) try to use discrete translation lexicons in NMT.", "startOffset": 15, "endOffset": 36}], "year": 2016, "abstractText": "Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.", "creator": "LaTeX with hyperref package"}}}