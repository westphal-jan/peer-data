{"id": "1610.07675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Surprisal-Driven Zoneout", "abstract": "we propose a novel method of regularization for filtering recurrent neural networks called suprisal - computer driven zoneout. in this complementary method, all states \\ textit { zoneout } ( rows maintain their previous value rather than updating ), when the \\ textit { suprisal } ( discrepancy between extending the last state's prediction and target ) is small. thus regularization is adaptive and input - driven on a per - device neuron basis. we demonstrate the effectiveness of this algorithm idea by achieving state - of - the - science art bits per character of 1. 49 32 on the hutter prize wikipedia dataset, significantly reducing the gap to the best known highly - engineered compression methods.", "histories": [["v1", "Mon, 24 Oct 2016 22:38:52 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v1", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v2", "Fri, 28 Oct 2016 19:55:16 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v2", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v3", "Mon, 31 Oct 2016 15:18:11 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v3", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v4", "Thu, 3 Nov 2016 17:09:23 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v4", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v5", "Thu, 24 Nov 2016 06:40:26 GMT  (1242kb,D)", "http://arxiv.org/abs/1610.07675v5", "To be published at the Continual Learning and Deep Networks Workshop NIPS 2016"], ["v6", "Tue, 13 Dec 2016 23:32:24 GMT  (1242kb,D)", "http://arxiv.org/abs/1610.07675v6", "Published at the Continual Learning and Deep Networks Workshop; NIPS 2016"]], "COMMENTS": "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["kamil rocki", "tomasz kornuta", "tegan maharaj"], "accepted": false, "id": "1610.07675"}, "pdf": {"name": "1610.07675.pdf", "metadata": {"source": "META", "title": "Suprisal-Driven Zoneout", "authors": ["Suprisal-Driven Zoneout", "Kamil Rocki", "Tomasz Kornuta"], "emails": ["KMROCKI@US.IBM.COM", "TKORNUT@US.IBM.COM"], "sections": [{"heading": "1. Introduction", "text": "An important part of learning is to go beyond simple memorization, to find as general dependencies in the data as possible. For sequences of information, this means looking for a concise representation of how things change over time. One common way of modeling this is with recurrent neural networks (RNNs), whose parameters can be thought of as the transition operator of a Markov chain. Training an RNN is the process of learning this transition operator. Generally speaking, temporal dynamics can have very different timescales, and intuitively it is a challenge to keep track of long-term dependencies, while accurately modeling more short-term processes as well.\nThe Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) architecture, a type of RNN, has proven to be exceptionally well suited for learning longterm dependencies, and is very widely used to model sequence data. Learned, parameterized gating mechanisms control what is retrieved and what is stored by the LSTM\u2019s state at each timestep via multiplicative interactions with LSTM\u2019s state. There have been many approaches to capturing temporal dynamics at different timescales, e.g. neural networks with kalman filters, clockwork RNNs, narx networks, and recently hierarchical multiscale neural networks.\nIt has been proven (Solomonoff, 1964) that the most gen-\nN o r t h e r n I r e l a n d ] ] = = E x t e r n a l l i n k s = = *Input\nInput clock\nSurprisal\nOperation probability\nN or th ern Ireland]] = = xternal links == * Memory E\nMemory clock (No operation) (No operation)\n(Predictable sequence) (Predictable sequence)\nt\nFigure 1.1: Illustration of the adaptive zoneout idea\neral solution to a problem is the one with the lowest Kolmogorov complexity, that is its code is as short as possible. In terms of neural networks one could measure the complexity of a solution by counting the number of active neurons. According to Redundancy-Reducing Hypothesis (Barlow, 1961) neurons within the brain can code messages using different number of impulses. This indicates that the most probable events should be assigned with codes with fewer impulses in order to minimize energy expenditure, or, in other words, that the more occurring patterns in lower level neurons should trigger sparser activations in higher level ones. Keeping that in mind, we have focused on the problem of adaptive regularization, i.e. minimization of a number of neurons being activated depending on the novelty of the current input.\nOne of the successful, recently proposed solutions, is zoneout (Krueger et al., 2016; Rocki, 2016a). Zoneout is an effective way of regularizing network by zoning them out, that is: freezing its state for a time step with some fixed probability. This mitigates the unstable behavior of a standard dropout. However, since the zoneout rate is fixed beforehand, one has to set it a priori to prefer faster convergence or higher stochasticity, whereas we would like to change it per memory cell according to learning phase, i.e. lower initially and higher later to prevent memorization/unnecessary activation. This is why we have decided to add surpisal-driven feedback (Rocki, 2016b), since it gives a measurement of current progress in learning. The provided (negative) feedback loop enables to change the zoneout rate online within the scope of a given cell. As learning pro-\nar X\niv :1\n61 0.\n07 67\n5v 1\n[ cs\n.L G\n] 2\n4 O\nct 2\n01 6\ngresses, the activations of that cell become less frequent in time and more iterations will just skip memorization, thus the proposed mechanism in fact enables different memory cells to operate on different time scales. The idea is illustrated in Fig. 1.1.\nThe main contribution of this paper is the introduction of Surprisal-Driven Adaptive Zoneout, where each neuron is encouraged to be active as rarely as possible with the most preferred state being no operation. The motivation behind this idea is that low complexity codes will provide better generalization."}, {"heading": "2. The model", "text": "We used the surprisal-feedback LSTM (Rocki, 2016b)\u2217 which keeps track of past predictions p and compares them with current observations x to compute surprisal s:\nst = \u2212 \u2211 i log pt\u22121 xt (2.1)\nNext we compute the gate activations:\nft = \u03c3(Wf \u00b7 xt + Uf \u00b7 ht\u22121 + Vf \u00b7 st + bf ) (2.2)\nit = \u03c3(Wi \u00b7 xt + Ui \u00b7 ht\u22121 + Vi \u00b7 st + bi) (2.3) ot = \u03c3(Wo \u00b7 xt + Uo \u00b7 ht\u22121 + Vo \u00b7 st + bo) (2.4)\nut = tanh(Wu \u00b7 xt + Uu \u00b7 ht\u22121 + Vu \u00b7 st + bu) (2.5) The zoneout rate is adaptive; it is a function of st, \u03c4 is a threshold parameter added for numerical stability, Wy is a h\u2192 y connection matrix:\nzt = max(\u03c4 + |st \u00b7WTy |, 1) (2.6)\nSample a binary mask Zt according to zoneout probability zt: Zt \u223c zt (2.7) New memory state depends on Zt. Intuitively, Zt = 0 means NOP, that is dashed line in Fig. 2.1.\nct = (1\u2212 ft Zt) ct\u22121 + Zt it ut (2.8)\nc\u0302t = tanh(ct) (2.9)\nht = ot c\u0302t (2.10) Outputs: yt = Wy \u00b7 ht + by (2.11) Normalize every output unit:\npit = ey i t\u2211\ni e yit\n(2.12)\n\u2217Remark: Surprisal-Driven Feedback is wrongly described by others as \u2019dynamic evaluation\u2019. Reasons : 1. It never actually sees test data during training. 2. It does not adapt weights during testing."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Datasets", "text": "Hutter Prize Wikipedia Hutter Prize Wikipedia (also known as enwik8) dataset (Hutter, 2012). This corpus contains 205 symbols including XML markups and special characters.\nLinux This dataset comprises approximately 603MB of raw Linux 4.7 kernel source code\u2020"}, {"heading": "3.2. Methodology", "text": "In both cases the first 90% of each corpus was used for training, the next 5% for validation and the last 5% for reporting test accuracy. In each iteration sequences of length 10000 were randomly selected. The learning algorithm used was Adadelta with a learning rate of 0.001. Weights were initialized using the so-called Xavier initialization (Glorot and Bengio, 2010). Sequence length for BPTT was 100 and batch size 128. In all experiments only one layer of 4000 LSTM cells was used. States were carried over for the entire sequence of 10000 emulating full BPTT. Forget bias was set initially to 1. Other parameters were set to zero. The algorithm was written in C++ and CUDA 8 and ran on GTX Titan GPU for up to 2 weeks."}, {"heading": "3.3. Results and discussion", "text": "We observed substantial improvements on enwik8 (Table 3.1) and Linux (Table 3.2) datasets. Our hypothesis is that it is due to the presence of memorizable tags and multinestedness. Patterns such as < timestamp > or long periods of spaces can be represented by a single impulse in this approach and zoning out entirely until the end of the pattern. Without adaptive zoneout that would have to be controlled entirely by gates. Fig 3.2 shows side by side comparison\n\u2020 http://olab.is.s.u-tokyo.ac.jp/\u02dckamil. rocki/data/\nof a version without and with adaptive zoneout, demonstrating that in fact the dynamic span of memory cells is greater when adaptive zoneout is used. Furthermore, we show that the activations using adaptive zoneout are in fact sparser than without it, which supports our intuition about the inner workings of the network. An especially interesting observation is the fact that adaptive zoneout seems to help separate instructions which appear mixed otherwise (see Fig 3.2).\nA similar approach to the same problem is called Hierarchical Multiscale Recurrent Neural Networks (Chung et al., 2016). The main difference is that we do not design an explicit hierarchy of levels, instead allowing each neuron to operate on arbitrary timescale depending on its zoneout rate. In fact all 4000 neurons in our experiments can operate asynchronously on different timescales even with-\nout multiple layers. Syntactic patterns in enwik8 and linux datasets are highly nested. For example (< page >, < revision >, < comment >, [[:en, ..., not mentioning parallel semantic context (movie, book, history, language). We believe that in order to learn such complex structure, we need distributed representations with every neuron operating at arbitrary time scale independent of another. Hardcoded hierarchical architecture will have problems solving such problem."}, {"heading": "4. Summary", "text": "The proposed surprisal-driven zoneout appeared to be a flexible mechanism to control the activation of a given cell. Empirically, this method performs extremely well on the enwik8 and linux datasets, setting a new state of the art by a large margin."}, {"heading": "5. Further work", "text": "We would like to explore variations of suprisal-driven zoneout on both state and cell, and more thoroughly investigate the impact of suprisal feedback on the LSTM\u2019s gating mechanisms. Another interesting direction to pursue is the connection with sparse coding - using suprisal-driven zoneout, the LSTM\u2019s cell contents are more sparsely revealed through time, potentially resulting in information being used more effectively."}, {"heading": "Acknowledgements", "text": "This work has been supported in part by the Defense Advanced Research Projects Agency (DARPA)."}], "references": [{"title": "Possible principles underlying the transformations of sensory messages", "author": ["H.B. Barlow"], "venue": null, "citeRegEx": "Barlow.,? \\Q1961\\E", "shortCiteRegEx": "Barlow.", "year": 1961}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS???10). Society for Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A.C. Courville", "C. Pal"], "venue": "CoRR, abs/1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Recurrent memory array structures", "author": ["K. Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1608.06027,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "i. Information and control,", "citeRegEx": "Solomonoff.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff.", "year": 1964}, {"title": "On multiplicative integration with recurrent neural networks. CoRR, abs/1606.06630, 2016", "author": ["Y. Wu", "S. Zhang", "Y. Zhang", "Y. Bengio", "R. Salakhutdinov"], "venue": "URL http: //arxiv.org/abs/1606.06630", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent highway networks, 2016", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Koutn?k", "J. Schmidhuber"], "venue": null, "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "The Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) architecture, a type of RNN, has proven to be exceptionally well suited for learning longterm dependencies, and is very widely used to model sequence data.", "startOffset": 34, "endOffset": 68}, {"referenceID": 8, "context": "It has been proven (Solomonoff, 1964) that the most genN o r t h e r n I r e l a n d ] ] = = E x t e r n a l l i n k s = = * Input Input clock Surprisal Operation probability N or th ern Ireland]] = = xternal links == * Memory E", "startOffset": 19, "endOffset": 37}, {"referenceID": 0, "context": "According to Redundancy-Reducing Hypothesis (Barlow, 1961) neurons within the brain can code messages using different number of impulses.", "startOffset": 44, "endOffset": 58}, {"referenceID": 5, "context": "One of the successful, recently proposed solutions, is zoneout (Krueger et al., 2016; Rocki, 2016a).", "startOffset": 63, "endOffset": 99}, {"referenceID": 2, "context": "Weights were initialized using the so-called Xavier initialization (Glorot and Bengio, 2010).", "startOffset": 67, "endOffset": 92}, {"referenceID": 4, "context": "58 Grid LSTM (Kalchbrenner et al., 2015) 1.", "startOffset": 13, "endOffset": 40}, {"referenceID": 9, "context": "45 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "44 RHN (Zilly et al., 2016) 1.", "startOffset": 7, "endOffset": 27}, {"referenceID": 1, "context": "40 HM-LSTM (Chung et al., 2016) 1.", "startOffset": 11, "endOffset": 31}, {"referenceID": 1, "context": "A similar approach to the same problem is called Hierarchical Multiscale Recurrent Neural Networks (Chung et al., 2016).", "startOffset": 99, "endOffset": 119}], "year": 2017, "abstractText": "We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state\u2019s prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.32 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.", "creator": "LaTeX with hyperref package"}}}