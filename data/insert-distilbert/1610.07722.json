{"id": "1610.07722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Sparse Hierarchical Tucker Factorization and its Application to Healthcare", "abstract": "we propose a new tensor factorization method, called the sparse hierarchical - tucker ( sparse rank h - tucker ), for sparse and high - order data tensors. sparse h - tucker is inspired by computing its namesake, the classical hierarchical tucker method, which aims to compute a tree - structured factorization of an input data set that may be nearly readily interpreted by a domain expert. however, sparse h - tucker uses a nested sampling technique to overcome a key scalability problem in hierarchical tucker, which is the creation of an unwieldy intermediate _ dense core tensor ; the result of our approach is a faster, more space - efficient, and more accurate method. we extensively test our method on a real healthcare dataset, which is collected from 30k patients and results in an 18th order sparse data tensor. unlike competing methods, sparse h - tucker can analyze the full data set on a single multi - threaded machine. notably it can also do well so more accurately and in less time than the state - of - state the - art : on a 12th order subset of the input data, sparse h - tucker is 18x more accurate and 7. 5x faster than a previously state - of - the - art method. even for analyzing low order tensors ( e. g., 4 - order ), our method requires close to an order of magnitude less time and over twenty two orders of magnitude less memory, as compared to traditional tensor factorization methods such as cp and tucker. moreover, hence we observe that sparse h - tucker scales nearly linearly in the number of non - zero tensor elements. the resulting model also provides an interpretable disease hierarchy, which is well confirmed by a clinical expert.", "histories": [["v1", "Tue, 25 Oct 2016 04:08:11 GMT  (2907kb,D)", "http://arxiv.org/abs/1610.07722v1", "This is an extended version of a paper presented at the 15th IEEE International Conference on Data Mining (ICDM 2015)"]], "COMMENTS": "This is an extended version of a paper presented at the 15th IEEE International Conference on Data Mining (ICDM 2015)", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["ioakeim perros", "robert chen", "richard vuduc", "jimeng sun"], "accepted": false, "id": "1610.07722"}, "pdf": {"name": "1610.07722.pdf", "metadata": {"source": "CRF", "title": "Sparse Hierarchical Tucker Factorization and its Application to Healthcare", "authors": ["Ioakeim Perros", "Robert Chen", "Richard Vuduc", "Jimeng Sun"], "emails": [], "sections": [{"heading": null, "text": "We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18\u00d7 more accurate and 7.5\u00d7 faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert."}, {"heading": "1 Introduction", "text": "This paper proposes a new tensor factorization method, designed to model multi-modal data, when the number of modes is high and the input data are sparse. Analyzing multi-modal data arises in data mining due to the abundance of information available that describes the same data objects [28]. We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language\nar X\niv :1\n61 0.\n07 72\n2v 1\n[ cs\n.L G\n] 2\n5 O\nprocessing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few. However, tensors also pose numerous computational scalability challenges, in all the senses of time, storage, and accuracy. This paper addresses these challenges.\nBy way of background, a tensor generalizes the concept of a matrix to more than two dimensions (rows and columns). A tensor may be dense, meaning one must assume nearly all its entries are non-zero, or sparse, meaning most entries are zero, so that tensor may be stored compactly and many computational operations may be eliminated. In data analysis, each dimension is referred to as a mode, order, or way [26]. For example, a 10th order disease tensor might be constructed so as to capture interactions across 10 different disease groups. Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11]. However, despite their known value to data analysis problems, these methods have been largely limited to the analysis of data sets with a relatively small number of modes, typically 3 to 5, and so would not apply to our hypothetical 10th order example. There are two principal challenges:\n1. Modeling assumptions. Traditional tensor models like CP or Tucker reveal strictly flat structures. By contrast, the 10 different disease groups in our hypothetical example might have natural subgroups, or even hierarchical structure; CP and Tucker ignore the possibility of such structure. Indeed, one might rightfully expect that, as the order grows, so, too, does the number of subgroups or the depth of the hierarchy.\n2. Exponential computational cost. With respect to the order of the tensor, there may be exponential costs in space and time. In the case of the Tucker method, the cause is the need to store a fully dense core tensor C as output, even if the input tensor is sparse. To see why this is problematic, consider an order d = 50 input tensor for which we wish to compute just a very lowrank approximation of, say, r = 2. Then, the dense core has size rd, which in this case is nearly 9 Petabytes, assuming 8 bytes per (double-precision floating-point) value [18].\nTo tackle the challenges above, we propose a scalable hierarchical tensor factorization for sparse high-order tensors, which we call the Sparse Hierarchical Tucker (or Sparse H-Tucker) method. Sparse H-Tucker expresses mode interactions as a binary tree, which is further parameterized in order to allow the approximation accuracy and cost to be tuned. For the same approximation error, it provides close to an order of magnitude gain in terms of the time required, when compared to a state-of-the-art CP factorization, and over two orders of magnitude gain in terms of the space required, when compared to a state-ofthe-art Tucker factorization method. At the same time, it respects sparsity in the input, achieving a near-linear scale-up in time and space with respect to the non-zeros of the input tensor. Perhaps somewhat surprisingly, this level of performance is not achieved at the cost of accuracy; on the contrary, as we\nverify experimentally, Sparse H-Tucker achieves remarkable gains in accuracy as well, particularly as the tensor density and order increase.\nAnother subtle but important challenge in dealing with high-order tensors is the lack of intuitive and generic representation for tensors and tensor operations, which may hinder end-user analysts from adopting tensor methods. As a result, most works on new tensor models are presented for a specific low-order tensor (e.g., 3 orders [37, 15]). For example, recent work by Fang et al. models the interactions of each one of the three modes with the two others through a tensor, the horizontal slices of which are further decomposed into two lowrank matrices [15]. The case of d > 3, when each horizontal slice would be a tensor, is not addressed. In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25]. This formalism has a nice visual representation as well, the basic elements of which appear in Figure 1 and are reviewed in Section 2.\nBesides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9]. These methods enable the compression of a tensor of size nd into a form that is linear in d, while preserving favorable numerical properties. However, successfully applying tensor networks to unsupervised learning has not been demonstrated in practice. One reason is that, despite their nice theoretical properties, tensor network methods target dense tensors, which is the usual case in quantum chemistry and physics applications; by contrast, data tensors are usually sparse. Also, the design of tensor networks has hereto focused on compression, rather than interpretation and pattern discovery, though their potential for the latter has been appreciated by some.1 Nevertheless, to our knowledge, this paper is the first to try to really apply tensor network modeling to a knowledge discovery application, through experimental evaluation as well as discussion on the interpretability of the model.\nAn earlier version of the present work appeared in the proceedings of IEEE\n1The intuition behind and potential applications of tensor networks in data processing appear in recent surveys by Cichocki [9, 10].\nICDM 2015 [36]. In addition to the ones of [36], our contributions in this extended version can be summarized as follows:\n\u2022 Tensor networks for data mining: We showcase how our method may be derived and understood from the vantage point of the tensor networks mathematical formalism, rather than strictly algebraic representations, which we review. This description renders the work simpler and more easily accessible. This work is the first to apply the tensor networks formalism as a modeling tool for unsupervised learning purposes, and evaluate its practicality.\n\u2022 Theoretical backing and explanations: We underpin Sparse H-Tucker with the necessary theoretical proofs and extensive discussions required to fully understand its mathematical foundations and its functionality.\n\u2022 Thorough experimental evaluation on real healthcare data: We complement our disease phenotyping case study using electronic health records (EHR), by providing experimental evaluation of the low-order scenario as well; in that case, we identify close to an order of magnitude gains in time and over two orders of magnitude gains in memory, as compared to traditional tensor factorization methods. As such, we justify the suitability of our work to low-order tensor problems as well. We also provide a detailed discussion on the interpretability of the resulting hierarchical disease model, guided by a domain expert."}, {"heading": "2 Background", "text": "This section introduces the necessary definitions and the preliminaries of matrix and tensor operations. Table 1 lists the notations used throughout the paper."}, {"heading": "2.1 Matrix factorizations", "text": "The Eckhart-Young Theorem for the Singular Value Decomposition (SVD) [16] for U\u03a3V = svd(A), where A \u2208 Rm\u00d7n defines that if k < r = rank(A) and Ak = \u2211k i=1 \u03c3iuiv T i , then: min\nrank(B)=k ||A \u2212 B||2 = ||A \u2212 Ak||2 = \u03c3k+1. In-\nstead of using the singular vectors in SVD, the CUR decomposition [30] uses representative columns C and rows R to approximate the input matrix. The relative-error guarantees of this method [14] depend on the notion of leverage score sampling2. The leverage scores \u03c0 for each j = 1, . . . , n column of A are: \u03c0j = 1/k \u2211k \u03be=1 (vj(\u03be)) 2 , where vj is the j-th right singular vector of A (out of k computed in total). Symmetrically, row sampling for matrix R is achieved by applying the above process on AT . It is proven that sampling O(klogk/ 2) columns (and rows of AT ) based on the distribution \u03c0 and defining\n2Alternative ways of lower-cost sampling (e.g. based on the row/column norms) are known to give much coarser additive error estimates [13].\nU = C+AR+ gives: ||A \u2212CUR||F \u2264 (2 + )||A \u2212Ak||F . Tensor versions of CUR approximation are given in [31, 40], but cannot handle high-order tensors due to cost limitations faced, similar to the ones of Tucker."}, {"heading": "2.2 Tensor operations and factorizations", "text": "Tensors are high-order generalizations of matrices. A fiber is a vector extracted from a tensor by fixing all modes but one and a slice is a matrix extracted from a tensor by fixing all modes but two [26]. Let a d-order tensor A \u2208 RI . The index set over which the tensor is defined is: I := I1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Id and the index set of each individual mode is I\u00b5 := {1, . . . , n\u00b5}, \u00b5 \u2208 {1, . . . , d}.\nMatricization (or reshaping, unfolding) logically reorganizes tensors into other forms, without changing the values themselves. Let the index set I(\u00b5) := I1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 I\u00b5\u22121 \u00d7 I\u00b5+1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Id. Then, the \u00b5-mode matricization is a mapping: A(\u00b5) : RI \u2192 RI\u00b5\u00d7I(\u00b5) . As a result, the mode-\u00b5 fibers of the tensor become columns of a matrix. Given U\u00b5 \u2208 RJ\u00b5\u00d7I\u00b5 , the \u00b5-mode multiplication is defined by (U\u00b5 \u25e6\u00b5 A)(\u00b5) := U\u00b5A(\u00b5) \u2208 RJ\u00b5\u00d7I (\u00b5)\n. Given matrices Uv \u2208 RJv\u00d7Iv with v = 1, . . . , d the multi-linear multiplication is defined as: (U1, . . . ,Ud) \u25e6 A := U1 \u25e61 . . .Ud \u25e6d A \u2208 RJ1\u00d7\u00b7\u00b7\u00b7\u00d7Jd .\nThe factorization of a tensor into a sum of component rank-one tensors is called the CP/PARAFAC [19, 6] factorization. If the rank of a d-order tensor\nA is equal to R, then: A = \u2211Rr=1 \u03bbr a(1)r \u2022 a(2)r \u2022 \u00b7 \u00b7 \u00b7 \u2022 a(d)r . The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others. The Tucker format is given by the following form [41, 11]: A = (U1, . . . ,Ud) \u25e6 C, where U\u00b5 \u2208 Rn\u00b5\u00d7k\u00b5 are (columnwise) orthonormal matrices and C \u2208 Rk1\u00d7\u00b7\u00b7\u00b7\u00d7kd is a core tensor, The tuple (k1, . . . , kd) with (elementwise) minimal entries for which the above relation holds is called the Tucker rank. In data analysis applications, the above relation is expected to hold only approximately. For fixed U\u00b5 matrices, the unique core tensor minimizing the approximation error is given by: C = (UT1 , . . . ,UTd ) \u25e6 A. If the core tensor is computed in the above way and each U\u00b5 contains the leading k\u00b5 left singular vectors of A(\u00b5), the factorization of tensor A is called the higher-order SVD (HOSVD) [11, 26]. HOSVD is considered as a good initialization to the higher-order orthogonal iteration (HOOI) [12], which is also an ALS-type algorithm, being the most popular way to approximate the Tucker format in real world applications."}, {"heading": "2.3 Tensor networks", "text": "A tensor network diagram, or just tensor network hereafter, provides an intuitive and concise graphical notation for representing tensors and operations on tensors [9, 10]. A scalar, vector, matrix, or tensor is represented by the \u201cballand-stick\u201d symbol that appears in Figure 1, where each circle denotes the object and each edge an order or mode. Annotated circles indicate special structure,\nsuch as being sparse. Where an open edge represents a mode, a closed edge that connects two tensors represents a contraction along the given edge. Contracting two tensors A \u2208 RI1\u00d7\u00b7\u00b7\u00b7\u00d7IN and B \u2208 RJ1\u00d7\u00b7\u00b7\u00b7\u00d7JM on common modes In = Jm yields another tensor, C \u2208 RI1\u00d7\u00b7\u00b7\u00b7\u00d7In\u22121\u00d7In+1\u00d7\u00b7\u00b7\u00b7\u00d7IN\u00d7J1\u00d7\u00b7\u00b7\u00b7\u00d7Jm\u22121\u00d7Jm+1\u00d7\u00b7\u00b7\u00b7\u00d7JM . Hierarchical Tucker and its Limitations One popular model of the tensor network family that shares structural similarities with our proposed model is the Hierarchical Tucker (H-Tucker in short) presented in [17]. Intuitively, the HTucker factorization algorithm proposed in [17] first decomposes the input tensor into the Tucker format through the HOSVD and then recursively factorizes the output tensor of this process. Such a strategy though suffers from severe scalability issues as the tensor order d increases. Despite the fact that the final form of H-Tucker requires linear storage to d, the size of the intermediate core tensor computed increases exponentially to d; and this core tensor is dense. As a result, this method faces a potential memory blow-up as it requires further decomposing an intermediate result that may not even fit into memory.\nAnother factorization scheme that is based on H-Tucker and is similar to ours was proposed in the tensor community by Ballani et al [5, 3]. However, that work exclusively targets dense tensors (does not work for sparse input), while ours focuses on sparse ones and data mining applications."}, {"heading": "3 Sparse Hierarchical Tucker", "text": ""}, {"heading": "3.1 Model", "text": "Our proposed target model is called the Sparse Hierarchical Tucker (Sparse H-Tucker). An example of this model in tensor network notation appears in Figure 2. In Sparse H-Tucker, the tensor modes are split recursively, resulting\nin a binary tree that we call the dimension tree and denote by TI . Each node of this tree contains a subset t \u2282 {1, . . . , d} of the modes and is either a leaf and singleton t = \u00b5 or the union of its two disjoint successors t1, t2 : t = t1 \u222a t2. Each tree node is associated with an output factor of the model. We denote these output factors by,\n(Bt)t\u2208I(TI) \u2208 Rkt\u00d7kt1\u00d7kt2 , (Ut)t\u2208L(TI) \u2208 RIt\u00d7kt .\nThe tensors Bt are called the transfer tensors, which correspond to the interior nodes, I(TI); the matrices Ut correspond to the leaves of the tree, L(TI), where s(t) = {t1, t2} denotes the set of successors of node t. By definition, the matrices Ut associated with the leaves of this tree structure are sparse. The tensor associated with the root node tr is a degenerate one (i.e., it is a matrix since ktr = 1), because unlike other interior nodes, only the root node connects to 2 nodes instead of 3.\nOur proposed model\u2019s tree structure is like that of the H-Tucker model [17]. However, Sparse H-Tucker preserves sparsity. By contrast, in H-Tucker, the matrices corresponding to the leaf nodes are dense, which fundamentally limits the scalability of any algorithms operating on it."}, {"heading": "3.2 Sparse H-Tucker factorization algorithm", "text": "The proposed factorization method can be conceptually divided into two phases:\n\u2022 Phase 1 computes a sampling-based low-rank approximation of all A(t) associated with each tree node except for the root. Notice that A(t) combines all modes contained in t as row indices and the rest of the modes into column indices.\n\u2022 Phase 2 uses the output of Phase 1, in order to assemble the final Sparse H-Tucker model in parallel.\nThe rationale behind these two phases is first to conduct all the preparation work in Phase 1 and then to compute the expensive steps fully in parallel in Phase 2.\nALGORITHM 1: Sparse Hierarchical Tucker factorization\nData: Input tensor A \u2208 RI , tree TI , accuracy parameter Result: (Bt)t\u2208I(TI), (Ut)t\u2208L(TI) // Phase 1\n1 {Pt, Qt,Mt} = TreeParameterization(A, tr,\u2205, ) ; // Phase 2: fully-parallelizable loop 2 foreach t \u2208 TI do 3 if t \u2208 I(TI) then 4 Compute Bt through Equation 2 ; 5 else // t \u2208 L(TI) 6 Compute sparse matrix Ut through Equation 1 ; 7 end\n8 end\nALGORITHM 2: TreeParameterization Data: Tensor A, tree node t, sampled column indices Qt, accuracy\nparameter Result: {Pt, Qt,Mt}\u2200t \u2208 TI\\tr\n1 {t1, t2} = s(t) ; 2 [Pt1 , Qt1 ,Mt1 ,A1] = NestedSampling(A, t1, Qt, ) ; 3 [Pt2 , Qt2 ,Mt2 ,A2] = NestedSampling(A, t2, Qt, ) ; 4 if t1 \u2208 I(TI) then 5 TreeParameterization(A1, t1, Qt1 , ) 6 end 7 if t2 \u2208 I(TI) then 8 TreeParameterization(A2, t2, Qt2 , ) 9 end\nAlgorithm 1 is our top-level procedure to compute the Sparse H-Tucker form. It takes as input the original tensor A, the dimension tree structure TI and a parameter which governs the accuracy of low-rank approximations. In Line 1 of Algorithm 1, we invoke Algorithm 2, by starting the recursion from the root node of the tree (tr) to parameterize the dimension tree.\nWithin Algorithm 2, Lines 2 and 3 call the function NestedSampling to compute the factors for the approximation of each A(t). If Ct and Rt contain column and row samples from A(t), respectively, and Mt is a small matrix minimizing the error of approximation, then the the product CtMtRt is an approximation of A(t). To avoid the materialization of Ct and Rt, we main-\ntain the index sets Pt, Qt denoting the row and column indices sampled from A(t) respectively. The challenges emerging so as to execute the NestedSampling function and its exact operation will be explained in Section 3.4. The recursive procedure TreeParameterization is continued until we reach the leaf nodes. 3\nIn Phase 2 of Algorithm 1, we construct the output factors of the Sparse H-Tucker model, by exploiting the sampling results from Phase 1. Since the construction over a single node is completely independent to others, we can fully parallelize this step.\nTo assemble the matrices Ut corresponding to the leaf nodes, we directly sample from the column fibers of A(t):\n((Ut)i)t\u2208L(TI) = A (t)(:, qi), qi \u2208 Qt. (1)\nSince we are sampling directly from the sparse input tensor for the construction of the (Ut)t\u2208L(TI) matrices, our leaf output factors maintain the sparsity of the input tensor. Thus, the requirement of our model for sparsity on matrices associated with leaf nodes is satisfied.\nA great advantage of the model is that the transfer tensors are directly assembled without the need of computing a huge, dense intermediate result (as in the case of the H-Tucker model). Below, we provide the equation for computing the factors (Bt)t\u2208I(TI) for the interior tree nodes. The proof of its correctness is given in the Appendix. Given nodes t, t1, t2 where {t1, t2} = s(t):\n(Bt)i,j,l = \u2211 p\u2208Pt1 \u2211 q\u2208Pt2 (Mt1)qj ,pA (t) (p,q),qi (Mt2)ql,q, (2)\nwhere qi \u2208 Qt, qj \u2208 Qt1 , ql \u2208 Qt2 ."}, {"heading": "3.3 Tensor approximation via the model\u2019s factors", "text": "Below, we describe how to approximate the input tensor through the Sparse HTucker model. First, each pair of leaves (matrices) that share a parent (tensor) are combined into a matrix Ut as follows:\n(Ut)i = kt1\u2211 j=1 kt2\u2211 l=1 (Bt)i,j,l ((Ut1)j \u2297 (Ut2)l) (3)\nwhere {t1, t2} = s(t), Bt \u2208 Rkt\u00d7kt1\u00d7kt2 , Ut \u2208 R(It1It2)\u00d7kt , Ut1 \u2208 RIt1\u00d7kt1 , and Ut2 \u2208 RIt2\u00d7kt2 . This process is followed for all interior nodes in a bottom-up fashion.\n3A remark regarding Algorithm 2 is that only for the root node\u2019s successors (i.e., when\n{t1, t2} = s(tr)), it holds that: A(t1) T\n= A(t2). To reduce redundant computations within the actual implementation, Line 3 of Algorithm 2 is executed only in the case when t 6= tr. Otherwise (t = tr), we set: Pt2 = Qt1 , Qt2 = Pt1 ,Mt2 = M T t1 .\nGiven that we have re-constructed the matrices Ut1 ,Ut2 ({t1, t2} = s(tr)), corresponding to the second level of the tree, the final input tensor approximation is given in vectorized form as follows:\nvec(A) \u2248 kt1\u2211 j=1 kt2\u2211 l=1 (Btr )j,l ((Ut1)j \u2297 (Ut2)l) (4)\nEquation 4 is a special case of Equation 3, accounting for the root node being associated with a matrix rather than a tensor.\nWe need not construct the full representation if we need only specific reconstructed entries, such as the reconstruction of a tensor\u2019s sub-block. Instead, we just have to prune the Ut matrices associated with the leaves, so that each one only contains the rows corresponding to the desired mode indices. A special case of this property is an element-wise query, when out of each Ut leaf matrix we use a single row vector for the desired element\u2019s approximation. For example, the reconstruction of A(i, j, k) cell of a 3-order tensor A requires the Ut1(i, :),Ut2(j, :),Ut3(k, :) to be used as input, if t1, t2, t3 correspond to the mode sets of the leaves.\nThe equations that govern the reconstruction of our model also apply in the H-Tucker model [17], where Equation 3 reflects a property called nestedness; we will use the same terminology hereafter."}, {"heading": "3.4 Nested sampling", "text": "Below, we describe the NestedSampling function that is called within Algorithm 2. Its role is to compute the factors required to approximate the matricizations A(t) for each subset of modes t associated with each tree node. Our approach is to form the factors approximating A(t) through the CUR decomposition based on leverage score sampling [30]. The biased samples from CUR decomposition help to boost the accuracy of our approach. More specifically, we follow the same sampling strategy as in [30], by retrieving O(k log k/ 2) rows or columns for each required approximation, where k is the rank of SVD, which is set to a small value (k = 5)4.\nHowever, a simple application of the CUR decomposition within our factorization framework would completely fail, due to challenges related to the consistency of each A(t) approximation with the whole framework. Assume calling the NestedSampling function with arguments A, t1, Qt, (as happens in Line 2 of Algorithm 2). Before even attempting to execute a nested sampling of A(t1), we have to ensure that the available set of rows and columns is consistent across the entire dimension tree. In other words, we have to ensure that the way we extract our model\u2019s transfer tensors (Equation 2) is consistent to each individual A(t) approximation.\nTo do so, we have to guarantee the validity of the nestedness property (Equation 3). The way we exploit this property towards the proof of correctness of\n4We detected no significant change in the accuracy of the final approximation by tuning k, hence we keep it fixed.\nEquation 2 is contained in the Appendix. In the following, we will explain the manner in which we guarantee that this property holds and how this relates to the column indices Qt in each NestedSampling call and tensors A1,A2 in each TreeParameterization call of Algorithm 2.\nEquation 3 dictates that we should be able to construct each column vector of (Ut)t\u2208I(TI) through linearly combined Kronecker products of column vectors of Ut1 ,Ut2 . Within our framework, where the Ut matrices contain actual fiber samples from the input tensor, this restriction is translated to enforcing the following reduction on the available column fibers for the CUR decomposition:\nQt1 \u2286 It2 \u00d7Qt (5)\nwhere {t1, t2} = s(t). The notation \u00d7 denotes the cartesian set product. Relation 5 implies that the columns sampled (Qt1) by the successor node associated with t1 will be a subset of all possible combinations formed between the unrestricted index set It2 and the fixed index set Qt (which had been previously defined by the parent node). By symmetry, Qt2 \u2286 It1 \u00d7Qt. In order to clarify this index restriction, we use a toy example in Figure 3. Given that the 1st column of A(t) is selected (which means that Qt = {1}), the available fibers for the successor node are those containing the 1st element of the 3rd mode in their multi-index. Thus, the available multi-indices for Qt1 are of the form (x, 1), where x \u2208 It2 .\nA single node\u2019s (e.g., associated with subset of modes t) index restriction\nhas to hold for all successors as we recursively visit the tree nodes in a top down fashion; thus, tensors A1,A2 are passed in each TreeParameterization call so as to avoid starting the index dropping from scratch at each NestedSampling call. Those tensors are obtained by finding the subset of tensor entries of A that obey to the rule of Relation 5."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "Our experiments were conducted on a server running the Red Hat Enterprise 6.6 OS with 64 AMD Opteron processors (1.4 GHz) and 512 GB of RAM. We used Matlab R2015a as the programming framework as well as Matlab Tensor Toolbox v2.6 [2] in order to support tensor operations. In order to promote reproducible and usable research, our code is open-sourced and publicly available 5.\nThe methods under comparison are the following:\n\u2022 Sparse H-Tucker (Sequential): Sparse H-Tucker implementation with sequential execution in Phase 2.\n\u2022 Sparse H-Tucker (Parallel): Sparse H-Tucker implementation with parallel execution in Phase 2.\n\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]6;\n\u2022 CP-ALS: Tensor Toolbox [2] implementation; and\n\u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI."}, {"heading": "4.2 Experiments on real healthcare data", "text": "Dataset and task description We used publicly available healthcare data for our experimental evaluation. The dataset is called MIMIC-II and can be found in [38] 7. It contains disease history of 29, 862 patients where an overall of 314, 647 diagnostic events are recorded over time. The task is about extracting co-occurring patterns of different diagnoses in patient records, in order to better understand the complex interactions of disease diagnoses. To acquire accurate phenotyping, we exploit the domain knowledge provided by the International Classification of Diseases (ICD) hierarchy [1] and guide the tensor construction with it. The ICD hierarchy consists of a collection of trees representing hierarchical relationships between diagnoses. As such, diagnoses belonging to the\n5http://www.cc.gatech.edu/~iperros3/src/sp_htucker.zip 6In order to enable sparse tensor input, we modified the computation of the left leading singular vectors by re-directing to the \u201dnvecs\u201d function of the \u201dsptensor\u201d Tensor Toolbox class. 7http://physionet.org/mimic2/\nsame diagnostic family reside under the same sub-tree. We map each tensor mode to a node of the top-level hierarchy. Thus, the order of our tensor will be equal to the number of top-level nodes. Furthermore, the lower-level diagnoses that are contained in each top-level node will be the elements of a tensor mode. Input tensor construction In order to end up with the previously described tensor, we sum over the number of co-occurrences for each multi-index of diagnoses. This means that each one of the input tensor cells contains the sum of the corresponding diagnoses found for all patients. For example, consider the case of a 3-order tensor T where each one of the 3 modes corresponds to a group of similar diseases. If a certain combination of diseases (i, j, k) is co-occurring in 3 patients out of the whole dataset, then T (i, j, k) = 3. Since the top-level of the ICD hierarchy contains 18 nodes, our complete tensor input is an 18-order tensor. For the purposes of our experimental evaluation, we constructed tensors from the same dataset with varying tensor order (less than 18) by limiting the subset of disease groups. Also, to each one of the modes, we added an additional element corresponding to \u201dno disease\u201d, so that we model the case when a mode does not participate in a certain co-occurrence at all. Cost-Accuracy Trade-offs At first, we would like to examine the re-construction error achieved by the methods under comparison, as a function of the cost (time/space). Since many baseline methods do not scale to higher orders, we decide to use 4-order tensor where all methods can run without memory issues.\nIn Figure 4, we observed the time-error and space-error trade-offs for the methods under comparison. Here we varied the parameters governing the quality approximation in all cases. The results were averaged over 10 runs in order to avoid fluctuations caused by random artifacts. For our implementation executing Phase 2 in parallel, we set the number of Matlab workers to 8 (through the \u201dparpool\u201d command).\nThe superiority of Sparse H-Tucker as compared to all methods in terms of the time-error trade-off is evident. In particular, it achieves close to an order of magnitude gain (8x) as compared to the CP-ALS method and 66x gain as compared to the Tucker-ALS. It is worth stressing out that even with a sequential execution of Phase 2, Sparse H-Tucker outperforms traditional tensor methods.\nAs concerns the space-approximation error trade-off, the savings of Sparse H-Tucker against Tucker and H-Tucker for the same error are remarkable: it achieves over than 2 orders of magnitude reduction on the peak memory allocation (396x). The reasons behind this stark difference lie in the fact that Sparse H-Tucker does not form any huge, dense intermediate result that increases with the tensor order. We were not able to reliably measure the peak allocation memory of our method using parallelism in Phase 2. However, we empirically noticed that the total memory required in this case is still orders of magnitude less than the one required by Tucker and H-Tucker.\nTucker-ALS method achieves the worst time-error tradeoff, while requiring the same peak memory requirements as the H-Tucker for the same low-rank parameter. Both methods form the same d-order dense tensor, either as a final output or as an intermediate result. Tucker-ALS just achieves slightly better\napproximation for the same space. Scalability We also conducted experiments in order to assess the scalability behavior of the methods under comparison, with respect to both the time and space required for increasing tensor order and number of non-zero elements. The input tensors having different density and order were constructed as explained above. For Sparse H-Tucker (parallel), we set the number of Matlab workers to 16, so as to exploit the full parallelism potential of our method for higher orders. We were not able to reliably measure the memory overhead for this version of our approach. Still, we empirically remark that the memory required for parallel Sparse H-Tucker shares the same scalability properties as the sequential version. The results are presented in Figure 5. It is remarkable that the HTucker factorization could not run for none but the 4-order tensor case. For the 6-order case and beyond, the memory it required exceeded the available memory of our server. The same behavior is observed in the case of Tucker-ALS. On the other hand, despite having comparable scalability behavior to Sparse H-Tucker, the CP method could not factorize the highest order tensors (16, 18) due to numerical issues (matrix being close to singular, in a sub-problem assuming a full-rank matrix). Our proposed Sparse H-Tucker enjoys near-linear scalability\nproperties with respect to increasing the non-zero elements or tensor orders for both time and space requirements. Cost-Accuracy Trade-off for increasing orders We would finally like to evaluate the time-error trade-off as the tensor order increases. It was intractable for any method to re-construct the full (dense, due to approximation errors) tensor for any order but the 4th; as such, we evaluated a random sample of 50K out of all the non-zero tensor elements, for each one of the methods (element-wise evaluation). Then, we measured the approximation error of those re-constructed entries with the real ones from the original input tensor. Since the 4th order tensor contained less than 50K non-zero values, we measured the error for the whole tensor. In Figure 5c), we present the results of this experiment. We would like to highlight the fact that as the tensor order increases, our method achieves increasingly beneficial cost-error trade-offs over the CP-ALS method.\nIn particular, for the 12-order tensor, Sparse H-Tucker achieves 18x reduction of the re-construction error in 7.5x less time."}, {"heading": "4.3 Disease phenotyping case study", "text": "In this section, we apply Sparse H-Tucker method to disease phenotyping. The qualitative analysis refers to the results of factorizing the full 18-order disease co-occurrence tensor.\nThe factors of the Sparse H-Tucker model are fit according to a certain tree structure. Such a tree can be obtained directly from existing knowledge such as a domain medical ontology or derived from data. In this case study, we build this dimension tree in a completely data-driven fashion using hierarchical clustering. For each one of the m non-zero values of the input tensor, we create a binary vector of size d, the order of the input tensor. This vector contains ones in the non-null positions of each specific entry. The columns of the m\u00d7d matrix formed are considered as data points into a m-dimensional space and are hierarchically clustered according to the Jaccard coefficient. The tree construction for the H-Tucker model is attempted by the recent work in [4]. However, the cost of their algorithm is prohibitive. Interpretation of output factors We propose the following interpretation of the output factors: the non-zero elements that correspond to each one of the column vectors of the matrices (Ut)t\u2208L(TI) form a concept for each individual mode t of the input tensor. Also, the numerical values of those elements are clear indicators of their \u201ccontribution\u201d to each concept, since these are actual fibers containing co-occurrence counts from the input tensor.\nAs concerns the interpretation of transfer tensors Bt with {t1, t2} = s(t), they should be considered as reflecting the interactions between the concepts of the successor nodes t1, t2. Thus, the (Bt)(i,j,v) elements having the largest absolute value within each i-th slice reflect a joint concept formed through the j-th concept of t1 and the v-th concept of t2. Also, due to our tree construction, the most significant concept interactions are expected to emerge in a bottomup fashion, which facilitates the interpretability if one wants to focus on the dominant emerging concepts. Qualitative analysis We now describe the qualitative results of our application, as they were examined by a domain expert who verified their clinical value and meaningfulness. Our target is to extract clinically meaningful connections between diagnoses from different diagnostic families, which could potentially co-occur and form valuable phenotypes. The most significant concepts grouped together as the result of applying our tensor factorization method, are shown in Table 3.\nAt first, the connections within each diagnostic family reflect well-known clinical associations. For example, concerning intra-mode connections of the endocrine-related diseases, inherited hypercholesterolemia is known to predispose a patient to develop hyperlimidemia due to the inability of receptors in cells to bind cholesterol. Also, hypercholesterolism and hyperlipidemia are associated with type II diabetes mellitus.\nThe most important aspect of our results is that the inter-mode relationships reflect meaningful disease co-occurrences as well. The connection between elements of endocrine-related and of circulatory system diseases reflects a wellknown association, since many diabetes patients may also be hyperlipidemic. Also, hypercholesterolemia and hypertension are known to have synergistic effects on coronary function. Furthermore, the grouping of blood-related diseases with the above is clinically meaningful, since the blood disease anemia is known to co-occur with them. In addition, the coupling of the extracted respiratory-related diseases to the aforementioned groups, is also known to have clinical association. For example, hypercholesterolemia is a potential risk factor for asthma and pre-existing heart failure may impact pneumonia development. The infectious diseases emerging could also form a phenotype with the above, since staphylococcus directly affects heart valves\u2019 functionality. Finally, cardiac murmurs are associated with abnormalities contained in circulatory and blood-related diseases."}, {"heading": "5 Conclusion", "text": "In this work, we propose a scalable high-order tensor factorization method specifically designed for data analytics. Our experiments on real healthcare data established the accuracy and scalability of our approach. Also, its application to the problem of disease phenotyping confirmed its usefulness for healthcare analytics and verified the correctness of our way of interpreting the resulting factors. This work is the first to use the tensor networks\u2019 formalism in practice for unsupervised learning in data mining applications. We would like to stress the fact that Sparse H-Tucker is not limited to healthcare applications; healthcare is just the focus of the current work and the application on more datasets and domains is left as a future work. Besides this, despite being designed to tackle high-order tensors, our proposed method is not limited to them and obvious benefits can be seen even in the case of low-order tensors, as we experimentally verified. Future work will focus on further examining the\nunderlying tree\u2019s construction and the method\u2019s theoretical properties.\nAPPENDIX\nProof of Equation (2)\nProof. The main target is to prove that if we directly form the Bt tensors through Relation (2), then Relation (1) holds for interior nodes as well. Formally, we want to prove that, if Relation (2) holds and {t1, t2} = s(t) then:\n((Ut)i)t\u2208I(TI) = A (t)(:, qi), qi \u2208 Qt (6)\nWe will prove the above proposition for the nodes of the penultimate level of the tree. By induction, this will hold for all interior nodes.\nDue to the restriction on each node\u2019s available column indices w.r.t. its parent node (Relation (5)), the nestedness property of Relation (3) holds, so that we have (element-wise):\n(Ut)it,i = \u2211 j\u2208Qt1 \u2211 l\u2208Qt2 (Bt)i,j,l (Ut1)it1 ,qj (Ut2)it2 ,ql (7)\nwhere it \u2208 It with it = (it1 , it2) and qi \u2208 Qt. At this point, let the assumption that Relation (2) holds. Then, Relation (7) gives:\n(Ut)it,i = \u2211 j\u2208Qt1 \u2211 l\u2208Qt2 \u2211 p\u2208Pt1 \u2211 q\u2208Pt2 (Mt1)qj ,p A (t) (p,q),qi (Mt2)ql,q\n(Ut1)it1 ,qj (Ut2)it2 ,ql\n(8)\nRelation (1) (direct column fiber sampling for leaf nodes) holds by construction. Thus, Relation (8) gives:\n(Ut)it,i = \u2211 q\u2208Pt2 \u2211 l\u2208Qt2 \u2211 p\u2208Pt1 \u2211 j\u2208Qt1 A (t1) it1 ,qj (Mt1)qj ,p A (t) (p,q),qi\n(Mt2)ql,q A (t2) it2 ,ql\n(9)\nBy definition of the CUR decomposition, under the assumption that it is exact, we have:\nA (t) (it1 ,q),qi = \u2211 p\u2208Pt1 \u2211 j\u2208Qt1 A (t1) it1 ,qj (Mt1)qj ,p A (t) (p,q),qi\nThus, Relation (9) gives: (Ut)it,i = \u2211 q\u2208Pt2 \u2211 l\u2208Qt2 A (t2) it2 ,ql (Mt2)ql,q A (t) (it1 ,q),qi = A (t) (it1 ,it2),qi (10)\nwhere the last equation follows again from the CUR decomposition. Since we ended up to Relation (6), then using Relation (2) is correct."}], "references": [{"title": "Matlab tensor toolbox version 2.6", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "Available online,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Fast evaluation of near-field boundary integrals using tensor approximations", "author": ["J. Ballani"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Tree adaptive approximation in the hierarchical tensor format", "author": ["J. Ballani", "L. Grasedyck"], "venue": "SIAM Journal on Scientific Computing, 36(4):A1415\u2013 A1431", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Black box approximation of tensors in hierarchical tucker format", "author": ["J. Ballani", "L. Grasedyck", "M. Kluge"], "venue": "Linear Algebra and its Applications, 438(2):639 \u2013 657", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition", "author": ["J.D. Carroll", "J.-J. Chang"], "venue": "Psychometrika, 35(3):283\u2013319", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1970}, {"title": "W", "author": ["S.R. Chinnamsetty", "M. Espig", "B.N. Khoromskij"], "venue": "Hackbusch, and H.- J. Flad. Tensor product approximation with optimal rank in quantum chemistry. The Journal of chemical physics, 127(8):084110", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Tensor decompositions: a new concept in brain data analysis", "author": ["A. Cichocki"], "venue": "arXiv preprint arXiv:1305.0395,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Era of big data processing: A new approach via tensor networks and tensor decompositions", "author": ["A. Cichocki"], "venue": "arXiv preprint arXiv:1403.2048", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor networks for big data analytics and large-scale optimization problems", "author": ["A. Cichocki"], "venue": "CoRR, abs/1407.3124", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A multilinear singular value decomposition", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "SIAM journal on Matrix Analysis and Applications, 21(4):1253\u20131278", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "On the best rank-1 and rank-(r 1", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "r 2,..., rn) approximation of higher-order tensors. SIAM Journal on Matrix Analysis and Applications, 21(4):1324\u20131342", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing, 36(1):184\u2013206", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Relative-error cur matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications, 30(2):844\u2013881", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast dtt: a near linear algorithm for decomposing a tensor into factor tensors", "author": ["X. Fang", "R. Pan"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 967\u2013976. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "volume 3. JHU Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical singular value decomposition of tensors", "author": ["L. Grasedyck"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A literature survey of low-rank tensor approximation techniques", "author": ["L. Grasedyck", "D. Kressner", "C. Tobler"], "venue": "GAMM-Mitteilungen, 36(1):53\u201378", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations of the parafac procedure: Models and conditions for an", "author": ["R.A. Harshman"], "venue": "explanatory\u201d multi-modal factor analysis", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1970}, {"title": "Limestone: High-throughput candidate phenotype generation via tensor factorization", "author": ["J.C. Ho", "J. Ghosh", "S.R. Steinhubl", "W.F. Stewart", "J.C. Denny", "B.A. Malin", "J. Sun"], "venue": "Journal of biomedical informatics, 52:199\u2013 211", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization", "author": ["J.C. Ho", "J. Ghosh", "J. Sun"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 115\u2013124. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Fema: flexible evolutionary multi-faceted analysis for dynamic behavioral pattern discovery", "author": ["M. Jiang", "P. Cui", "F. Wang", "X. Xu", "W. Zhu", "S. Yang"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1186\u20131195. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gigatensor: Scaling tensor analysis up by 100 times - algorithms and discoveries", "author": ["U. Kang", "E. Papalexakis", "A. Harpale", "C. Faloutsos"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, pages 316\u2013324, New York, NY, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Direct solution of the chemical master equation using quantized tensor trains", "author": ["V. Kazeev", "M. Khammash", "M. Nip", "C. Schwab"], "venue": "PLoS computational biology, 10(3):e1003359", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured data-sparse approximation to high order tensors arising from the deterministic boltzmann equation", "author": ["B. Khoromskij"], "venue": "Mathematics of computation, 76(259):1291\u20131315", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Tensor decompositions and applications", "author": ["T. Kolda", "B. Bader"], "venue": "SIAM Review, 51(3):455\u2013500", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithm 941: htucker\u2014a matlab toolbox for tensors in hierarchical tucker format", "author": ["D. Kressner", "C. Tobler"], "venue": "ACM Transactions on Mathematical Software (TOMS), 40(3):22", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Challenges in multimodal data fusion", "author": ["D. Lahat", "T. Adaly", "C. Jutten"], "venue": "Signal Processing Conference (EUSIPCO), 2013 Proceedings of the 22nd European, pages 101\u2013105. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiway array decomposition analysis of eegs in alzheimer\u2019s disease", "author": ["C.-F.V. Latchoumane", "F.-B. Vialatte", "J. Sol\u00e9-Casals", "M. Maurice", "S.R. Wimalaratna", "N. Hudson", "J. Jeong", "A. Cichocki"], "venue": "Journal of neuroscience methods, 207(1):41\u201350", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences, 106(3):697\u2013 702", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor-cur decompositions for tensor-based data", "author": ["M.W. Mahoney", "M. Maggioni", "P. Drineas"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 327\u2013336, New York, NY, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "W", "author": ["Y. Matsubara", "Y. Sakurai"], "venue": "G. van Panhuis, and C. Faloutsos. Funnel: Automatic mining of spatially coevolving epidemics. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, pages 105\u2013114, New York, NY, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Advances on tensor network theory: symmetries", "author": ["R. Orus"], "venue": "fermions, entanglement, and holography. The European Physical Journal B, 87(11)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor-train decomposition", "author": ["I. Oseledets"], "venue": "SIAM Journal on Scientific Computing, 33(5):2295\u20132317", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Parcube: Sparse parallelizable tensor decompositions", "author": ["E.E. Papalexakis", "C. Faloutsos", "N.D. Sidiropoulos"], "venue": "P. A. Flach, T. D. Bie, and N. Cristianini, editors, ECML/PKDD (1), volume 7523 of Lecture Notes in Computer Science, pages 521\u2013536. Springer", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse hierarchical tucker factorization and its application to healthcare", "author": ["I. Perros", "R. Chen", "R. Vuduc", "J. Sun"], "venue": "Data Mining (ICDM), 2015 IEEE International Conference on, pages 943\u2013948. IEEE", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-Thieme"], "venue": "Proceedings of the third ACM international conference on Web search and data mining, pages 81\u201390. ACM", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiparameter intelligent monitoring in intensive care II (MIMIC-II): a public-access intensive care unit database", "author": ["M. Saeed"], "venue": "Critical care medicine,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Beyond streams and graphs: Dynamic tensor analysis", "author": ["J. Sun", "D. Tao", "C. Faloutsos"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 374\u2013383, New York, NY, USA", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Mach: Fast randomized tensor decompositions", "author": ["C.E. Tsourakakis"], "venue": "SDM, pages 689\u2013700. SIAM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, 31(3):279\u2013311", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1966}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Y. Wang", "R. Chen", "J. Ghosh", "J.C. Denny", "A. Kho", "Y. Chen", "B.A. Malin", "J. Sun"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1265\u20131274, New York, NY, USA", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Analyzing multi-modal data arises in data mining due to the abundance of information available that describes the same data objects [28].", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 238, "endOffset": 245}, {"referenceID": 6, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 238, "endOffset": 245}, {"referenceID": 30, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 257, "endOffset": 261}, {"referenceID": 20, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 287, "endOffset": 291}, {"referenceID": 21, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 41, "endOffset": 45}, {"referenceID": 37, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 106, "endOffset": 118}, {"referenceID": 18, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 106, "endOffset": 118}, {"referenceID": 40, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 106, "endOffset": 118}, {"referenceID": 24, "context": "In data analysis, each dimension is referred to as a mode, order, or way [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 4, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 39, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 9, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 16, "context": "Then, the dense core has size r, which in this case is nearly 9 Petabytes, assuming 8 bytes per (double-precision floating-point) value [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 35, "context": ", 3 orders [37, 15]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 13, "context": ", 3 orders [37, 15]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 13, "context": "models the interactions of each one of the three modes with the two others through a tensor, the horizontal slices of which are further decomposed into two lowrank matrices [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25].", "startOffset": 185, "endOffset": 196}, {"referenceID": 22, "context": "In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25].", "startOffset": 185, "endOffset": 196}, {"referenceID": 23, "context": "In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25].", "startOffset": 185, "endOffset": 196}, {"referenceID": 32, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 15, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 31, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 7, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 7, "context": "1The intuition behind and potential applications of tensor networks in data processing appear in recent surveys by Cichocki [9, 10].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "1The intuition behind and potential applications of tensor networks in data processing appear in recent surveys by Cichocki [9, 10].", "startOffset": 124, "endOffset": 131}, {"referenceID": 34, "context": "ICDM 2015 [36].", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "In addition to the ones of [36], our contributions in this extended version can be summarized as follows:", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "The Eckhart-Young Theorem for the Singular Value Decomposition (SVD) [16] for U\u03a3V = svd(A), where A \u2208 Rm\u00d7n defines that if k < r = rank(A) and Ak = \u2211k i=1 \u03c3iuiv T i , then: min rank(B)=k ||A \u2212 B||2 = ||A \u2212 Ak||2 = \u03c3k+1.", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "stead of using the singular vectors in SVD, the CUR decomposition [30] uses representative columns C and rows R to approximate the input matrix.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "The relative-error guarantees of this method [14] depend on the notion of leverage score sampling.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "based on the row/column norms) are known to give much coarser additive error estimates [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "Tensor versions of CUR approximation are given in [31, 40], but cannot handle high-order tensors due to cost limitations faced, similar to the ones of Tucker.", "startOffset": 50, "endOffset": 58}, {"referenceID": 38, "context": "Tensor versions of CUR approximation are given in [31, 40], but cannot handle high-order tensors due to cost limitations faced, similar to the ones of Tucker.", "startOffset": 50, "endOffset": 58}, {"referenceID": 24, "context": "A fiber is a vector extracted from a tensor by fixing all modes but one and a slice is a matrix extracted from a tensor by fixing all modes but two [26].", "startOffset": 148, "endOffset": 152}, {"referenceID": 17, "context": "The factorization of a tensor into a sum of component rank-one tensors is called the CP/PARAFAC [19, 6] factorization.", "startOffset": 96, "endOffset": 103}, {"referenceID": 4, "context": "The factorization of a tensor into a sum of component rank-one tensors is called the CP/PARAFAC [19, 6] factorization.", "startOffset": 96, "endOffset": 103}, {"referenceID": 17, "context": "The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others.", "startOffset": 110, "endOffset": 121}, {"referenceID": 4, "context": "The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others.", "startOffset": 110, "endOffset": 121}, {"referenceID": 24, "context": "The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others.", "startOffset": 110, "endOffset": 121}, {"referenceID": 39, "context": "The Tucker format is given by the following form [41, 11]: A = (U1, .", "startOffset": 49, "endOffset": 57}, {"referenceID": 9, "context": "The Tucker format is given by the following form [41, 11]: A = (U1, .", "startOffset": 49, "endOffset": 57}, {"referenceID": 9, "context": "If the core tensor is computed in the above way and each U\u03bc contains the leading k\u03bc left singular vectors of A(\u03bc), the factorization of tensor A is called the higher-order SVD (HOSVD) [11, 26].", "startOffset": 184, "endOffset": 192}, {"referenceID": 24, "context": "If the core tensor is computed in the above way and each U\u03bc contains the leading k\u03bc left singular vectors of A(\u03bc), the factorization of tensor A is called the higher-order SVD (HOSVD) [11, 26].", "startOffset": 184, "endOffset": 192}, {"referenceID": 10, "context": "HOSVD is considered as a good initialization to the higher-order orthogonal iteration (HOOI) [12], which is also an ALS-type algorithm, being the most popular way to approximate the Tucker format in real world applications.", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "A tensor network diagram, or just tensor network hereafter, provides an intuitive and concise graphical notation for representing tensors and operations on tensors [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "A tensor network diagram, or just tensor network hereafter, provides an intuitive and concise graphical notation for representing tensors and operations on tensors [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 15, "context": "Hierarchical Tucker and its Limitations One popular model of the tensor network family that shares structural similarities with our proposed model is the Hierarchical Tucker (H-Tucker in short) presented in [17].", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "Intuitively, the HTucker factorization algorithm proposed in [17] first decomposes the input tensor into the Tucker format through the HOSVD and then recursively factorizes the output tensor of this process.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "Another factorization scheme that is based on H-Tucker and is similar to ours was proposed in the tensor community by Ballani et al [5, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 1, "context": "Another factorization scheme that is based on H-Tucker and is similar to ours was proposed in the tensor community by Ballani et al [5, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 15, "context": "Our proposed model\u2019s tree structure is like that of the H-Tucker model [17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "The equations that govern the reconstruction of our model also apply in the H-Tucker model [17], where Equation 3 reflects a property called nestedness; we will use the same terminology hereafter.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Our approach is to form the factors approximating A through the CUR decomposition based on leverage score sampling [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 28, "context": "More specifically, we follow the same sampling strategy as in [30], by retrieving O(k log k/ ) rows or columns for each required approximation, where k is the rank of SVD, which is set to a small value (k = 5).", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "6 [2] in order to support tensor operations.", "startOffset": 2, "endOffset": 5}, {"referenceID": 25, "context": "\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]; \u2022 CP-ALS: Tensor Toolbox [2] implementation; and \u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]; \u2022 CP-ALS: Tensor Toolbox [2] implementation; and \u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]; \u2022 CP-ALS: Tensor Toolbox [2] implementation; and \u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI.", "startOffset": 159, "endOffset": 162}, {"referenceID": 36, "context": "The dataset is called MIMIC-II and can be found in [38] .", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 45, "endOffset": 48}, {"referenceID": 25, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "The tree construction for the H-Tucker model is attempted by the recent work in [4].", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We propose a new tensor factorization method, called the Sparse Hierarchical Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor; the result of our approach is a faster, more space-efficient, and more accurate method. We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18\u00d7 more accurate and 7.5\u00d7 faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert.", "creator": "LaTeX with hyperref package"}}}