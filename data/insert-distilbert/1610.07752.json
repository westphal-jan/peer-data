{"id": "1610.07752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Big Models for Big Data using Multi objective averaged one dependence estimators", "abstract": "even when though, many researchers tried cautiously to explore the various possibilities on multi objective feature selection, still it is yet to be explored with best of its capabilities in data mining applications rather really than going for developing new ones. in this paper, multi - graded objective evolutionary algorithm enora is used to select each the features in a multi - class classification problem. alongside the fusion of ande ( averaged n - dependence estimators ) with n = 1, a variant of naive bayes with efficient feature selection by enora is performed in order to obtain a fast hybrid classifier which listeners can effectively learn from arbitrary big data. this method aims at solving the problem of finding optimal feature subset from full data which at present today still remains to be a difficult problem. the efficacy of the obtained classifier is extensively evaluated with a range of most popular 21 real world dataset, ranging from small to big. the results obtained are encouraging in terms of time, root mean square error, zero - one data loss and classification accuracy.", "histories": [["v1", "Tue, 25 Oct 2016 07:11:11 GMT  (142kb)", "http://arxiv.org/abs/1610.07752v1", "21 pages, 2 Figures, 10 tables"]], "COMMENTS": "21 pages, 2 Figures, 10 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["mrutyunjaya panda"], "accepted": false, "id": "1610.07752"}, "pdf": {"name": "1610.07752.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mrutyunjaya Panda"], "emails": ["mrutyunjaya74@gmail.com"], "sections": [{"heading": null, "text": "Even though, many researchers tried to explore the various possibilities on multi objective feature selection, still it is yet to be explored with best of its capabilities in data mining applications rather than going for developing new ones. In this paper, multi-objective evolutionary algorithm ENORA is used to select the features in a multi-class classification problem. The fusion of AnDE (averaged n-dependence estimators) with n=1, a variant of naive Bayes with efficient feature selection by ENORA is performed in order to obtain a fast hybrid classifier which can effectively learn from big data. This method aims at solving the problem of finding optimal feature subset from full data which at present still remains to be a difficult problem. The efficacy of the obtained classifier is extensively evaluated with a range of most popular 21 real world dataset, ranging from small to big. The results obtained are encouraging in terms of time, Root mean square error, zero-one loss and classification accuracy.\nKey-words: MOEA, ENORA, AODE, Classification, 0/1 loss, RMSE\n1. Introduction\nFeature subset selection basically aims at providing the best possible feature subsets out of the total features available in the dataset in order to reduce the computational overhead in learning, leading to build an simple yet an accurate classifier. It is also noticed that, there are a plenty of approaches present in the literature for obtaining quality feature subsets, still it is considered to be a difficult task to obtain the best ones [1]. In order to address such complex computational problems arising from the large size of the input data, researchers are inspired towards using nature inspired algorithm recently to perform better optimization in classification task and evolutionary algorithms are one of them [2]. The parallel architecture of the Evolutionary algorithms (EA) are a potential candidate to process such big data automatically for optimal parameter setting and more importantly obtaining a viable solution for better interpretation of the model with best classification accuracy possible [3,4].While in single-objective optimization criteria, a single solution exists for which all criteria are optimal; Considering feature subset selection process as a multi-objective one, there is no single optimal solution that can outweigh\nthe other, hence a solution of Pareto-optimal set is obtained. The Pareto-optimal set solutions are considered not to be dominated by any other possible solution [5,6]."}, {"heading": "Motivation", "text": "In some cases the amount of features can make construction of an induction model hard, either because the model cannot fit in memory or construction would take too long. Creating a limited subset can help construct a model within these constraints. It is observed that solving multi-objective optimization problems using the exact methods such as: linear programming and gradient search etc. is too complex a process that can easily be over shadowed by using evolutionary algorithms. This is because of the parallelism and finding similarities by recombination process by the multi-objective optimization process, that make it a interesting area of research in many diverse applications. In spite of all, still there is a lack of studies that compare the performance and different aspects of these approaches. This motivated us to understand and carry out research, whether the multi-objective optimization approach suits to different problems at hand and Consequently, able to build a effective and efficient classifier with small as well as big dataset. The rest of the paper is organized as follows: Section 2 discusses some related research followed by concepts of feature selection methods and MOEA are described in Section 3. The AODE classifier is discussed in Section 4 and then, materials and methods used for the experimentation presented in Section 5. Section 6 provides the experimental results and discussion on the proposed approach. Finally, we conclude in Section 7 with future scope of research.\n2. Related work\nThis section discusses the relevance and possible applications of evolutionary algorithms, particularly genetic algorithms, in the domain of knowledge discovery in databases by different researchers. Dehuri and Ghosh [7] proposes to use a multi-objective genetic based feature selection method for knowledge discovery process and discusses its pros and cons for its validity and potentiality. Ducange, Lazzerini and Marcelloni [8] used NSGA-II, a well known multiobjective optimization algorithm as a feature subset selection in highly imbalanced dataset. They perform the classification and presented their suitability with sensitivity, specificity and interpretability in ROC plane. Laura Emmanuella et al. [9] proposed to use ensemble filter based classification with feature subset selection done by Particle swarm (PSO), genetic algorithm (GA) and ant-colony optimization (ACO) techniques. They perform both mono-objective and biobjective versions for feature selection and finally claimed that the PSO based ensemble classifiers with bi-objective version performs well in 11 datasets investigated. Khan and Baig [10] opined that large number of irrelevant attributes are to be removed from the datasets in order to enhance the accuracy of the classifier. They use latest multi-objective genetic algorithm (NSGA-II) for feature subset selection and then applied with reduced feature sets to ID3 decision tree classifier on several datasets collected from UCI machine learning repository. They finally conclude with a feasibility study with NSGA-II method with individual attributes of the datasets used for investigation. Cataniaa, Zanni-merka, Beuvrona and Colleta [11] discussed the various aspects of multi-objective optimization NSGA-II implementation in Patient Transport services with preliminary results. They compared the results with the itineraries proposed by human experts and found it satisfactory. The authors Stoean and Gorunescu [12] presented a study to\ninvestigate the requirement of evolutionary algorithms to relief the user from the burden of curse of dimensionality. They applied in medical diagnosis considering that feature ranking of the attributes are of paramount importance for better decision making. Abd-Alsabour [13] presented a good review on some of the most recent feature selection methods based on evolutionary algorithms. They discussed about the pros and cons of such a method with their theoretical issues. Mukhopadhyay, Maulik, Bandyopadhyay, and Coello Coello [14] presented a comprehensive survey on recent advances in multi-objective evolutionary optimization techniques as a feature selection and classification for automatic processing of large qualities of data that can solve many real world problems with various various conflicting measures of performance. Cannas, Dessi and Pes [15] proposed four popular filter based procedure along with support vector machine in order to reduce the search space in high dimensional micro array data sets, for obtaining potential solutions to predict and diagnose the disease. A generic review on various filter, wrapper based feature selection and their possible combination are presented by Chandrasekharan and Sahin [16]. They also stated that the beauty of such methods lead to fast, accurate and simple classifiers, after investing their applicability on several standard dataset. Chen and Yao [17] demonstrated the effectiveness of the proposed ensemble of evolutionary multi-objective algorithms and Bayesian automatic relevance determination to obtain better accuracy with reduced feature sets. They conclude that their proposed approach applied in several real world scenarios outperforms other available ensemble approaches. The concept of dominance is used as a part of multi-objective genetic algorithms by the authors Macintyre et al. [18] where they pointed out the tradeoffs between computational complexity with classification accuracy. In order to validate their results, they used neural network and neuro-fuzzy hybrid in two small and high dimensional regression problems. The authors Jim\u00b4enez et al. [19] presented a good research by stating the usefulness of multi-objective evolutionary algorithms in dealing with the cases when number of attributes are very high which is already proven to be the best for attribute selection. They applied ENORA to Multi-Skill Contact Center Data Classification as a posteriori process in a multi objective context and finally compared with NSGA-II for validation of their findings. Ariasa, Gameza, Nielsenb and Puertaa [20] proposed a scalable pair wise class interaction framework for multi-dimension classification by taking several base classifiers and inference methods afterwards. They perform their experiments on a wide range of publicly available dataset and conclude that their approach is efficient in comparison with other exiting straw-men methods. A novel multi-objective genetic algorithm based feature selection combined with support vector machine is used [21] for selection of genes in micro array dataset for cancer diagnosis. They conclude that their approach with two or three objectives based on sensitivity and specificity quality measures are highly appropriate in comparison to the other existing algorithm available in the literature. Shi, Suganthan and Deb [22] proposed to use support vector machine as a classifier for structural classification of protein (SCOP) after the relevant feature are selected by using Multi-Objective Feature Analysis and Selection Algorithm (MOFASA). They conclude with their promising results with its applicability to future biological analysis. Datta, Deb, Fonseca, Lobo and Condado [23] proposed a spatial GIS based multi-objective evolutionary algorithm (NSGAII-LUM) for better understanding of the impact from land uses in Mediterranean landscape from Southern Portugal. Multi-objective optimization (MOO) task to SVM design is evaluated in the real-time detection of pedestrians in infrared images for driver assistance systems, as a pattern recognition task. They discuss about how to model the proposed framework by considering the tradeoffs between accuracy and model complexity with reduced number of support vector [24]. Tan, Teoh, Yua and\nGoh [25] proposed genetic- support vector (GA-SVM) hybrid evolutionary algorithm for attribute selection in data mining. They concluded that their approach applied to real life data sets obtained from UCI machine repository is accurate and consistent, as compared to the several well established algorithms. The authors provided an empirical comparison of various MOO algorithms, considering six test functions, which are aimed at giving an impression to understand which technique performs well under what condition in Zitler, Deb and Thiele [26]. A good tutorial on theory and model are provided by Zitzler, Laumanns, and Bleuler [27], where various algorithmic aspects of MOO such as: assignment of fitness function, elitism and diversity etc. Are discussed. They addressed on how to simplify the MOO by method of exchange with many standard applications.\n3. Evolutionary Multi-objective Optimization (MOEA )\nApplication of multi-objective optimization starts with a prior knowledge on search technique that will lead to get Pareto optima solutions vis-a-vis preventing premature convergence. It is also pointed by Zitzler et al. [27] that due to the complex nature of generating Pareto optimal solution sets, many stochastic search namely: evolutionary algorithms, simulated annealing etc. are developed in order to find a good approximation to the near optimal solutions. The beauty of the evolutionary search lies in combining the several solutions in a recombination process to obtain the new best solutions. Further, it can be noticed that MOO can be a maximization or minimization problem with a number of objective functions [28]. The basic steps involved in such a method used in feature selection process can be observed in Figure 1. Now-a-days, multi-objective evolutionary algorithms have attracted many a researchers in the multi faced applications to provide some viable solutions to multi-objective problems [28-29]; that include to name a few: Non-dominated sorting Genetic algorithm (NSGA) by Deb and Srinivas [30]; Strength Pareto Evolutionary Algorithm (SPEA 1 and 2) [31-32].\nAny feature selection strategy starts with feature subset generation as a first step where some kind of search is done to obtain the candidate feature subset followed by candidate subset evaluation. In candidate subset evaluation, each candidate subset is compared and evaluated with previous best ones with some criteria, then the best one replaces the others. This process continues till stopping criteria is met and finally, the best selected candidate subsets are validated for their suitability on dataset under consideration. One step further in this direction, one can think of using evolutionary algorithms to solve MOO problems, for their inherent capability to deal with set of candidate subset solutions or\npopulation. This makes it possible to find the Pareto optimal set in a single go rather than going for several runs as in the case of the traditional optimization techniques. Multi-objective evolutionary algorithm based optimization (MOEA) mainly tries to address the following issues: (1) How best the Pareto-optimal set is obtained through proper assignment and selection of fitness functions and (2) prevention of convergence at an early stage by intelligently decide the diverse population.\nThe two most popular MOEA are ENORA (Elitist Pareto-based multi-objective evolutionary algorithm for diversity reinforcement) and NSGA-II (elitist non-dominated sorting genetic algorithm). They are discussed below."}, {"heading": "3.1. ENORA", "text": "ENORA [33] is an elitist Pareto- based multi-objective evolutionary algorithm that uses a Mu plus Lambda survival with uniform random initialization and binary tournament selection process for exploring the attribute space. Crowding distance is used as a measure to find ranking on local non-domination level along with self adaptive uniform crossover and self adaptive single bit flip mutation. Ad-hoc elitist generational replacement technique is used to maintain diversity among the individuals. In this paper, multi-objective evolutionary search is used as feature subset selection process using ENORA and the process seems to be maximization-minimization task. First, maximization process is set by the evaluator and then subset cardinality minimization is done as second step. Finally, the non-dominated Pareto optimal solution in the last population with best fitness is chosen as output. To illustrate the process, for example: Given a population P of N individuals, N children are generated by random initialization and binary tournament selection, crossing and mutation. The new population is obtained by electing the N best individuals from the union of parents and children. The ranking of individuals are done by the operator best where each individual comes with a ranking in its slot and their corresponding crowding distance. One individual (x) is better than the other (y) provided the rank of one (x) is at-least equal or same to the other (y) and the crowding distance of x is more than y."}, {"heading": "3.2. NSGA-II: elitist non-dominated sorting genetic algorithm", "text": "NSGA-II [34] is also a popular multi-objective genetic algorithm based constrained optimization technique which is aimed at improving the Pareto optimal solutions using evolutionary operators such as: selection, genetic mutation and crossover. The difference between NSGA-II and ENORA is how the calculation of the rank of the individuals in the population is performed. In ENORA, the rank of an individual in a population is the non-domination level of the individual in its slot, whereas in NSGA-II the rank of an individual in a population is the non-domination level of the individual in all the population.\n4. Average one dependence estimator (AODE)\nNaive Bayes is one of the most popular machine learning techniques which is highly efficient and computationally intensive for small dataset. This is mainly due to its attribute independence assumption based on frequency estimates that provides an accurate classifier. One variant of such a popular Naive Bayes technique is Averaged one-dependence estimator (AODE) Naive Bayes [35]. It is simple yet faster for its more simplicity in attribute independence assumption than Naive Bayes and accurate, that makes it a good competitor in the machine learning approaches.\nIt is worth noting here that if the attribute independence criteria are violated for any means, then the classification accuracy will be affected to a great extent. It is also observed from various research that violation is done in many cases, but if this is acceptable until the probability estimate of the most probable class outweighs the others.\nDuring training stage, AODE produces 3-D joint frequency table with one by class values and the rest two are for attribute values. Care should be taken to ensure that missing class value are to be properly addressed by deleting the whole objects associated with that class. For missing attribute values, one can opt for allowing it for training the classifier with various possibilities of replacement. During the testing phase, conditional probability estimate is obtained from the joint frequency table. There is a significant difference in AODE and A0DE(averaged zero dependence estimators)in that while the former finds the average of all models considering direct attribute independencies, uses alternative attribute inter-dependency criteria for model aggregation, hence successfully overcomes the attribute independence problem; the averaging of aggregates models are obtained through feature subset selection in the later, hence may be trapped in attribute Independence problem. The similarity of AODE and A0DE lies with prediction by using probability estimates of several models."}, {"heading": "Computational complexity", "text": "AODE may be efficient ones while applying in large dataset for its less computational complexity in terms of time and memory space, required during training and testing the classifier. Training time complexity is O(nk2) which is linear with respect to training dataset, where n is the number of attributes and k is the number of training objects. At the same time, the time complexity for the testing an object (k) is O(mn2), where m is the number of class labels. AODE takes very less memory space for storing the joint frequency tables is O (mp2), where m and p denotes number of class labels and number of attributes respectively. While testing, no training data are kept in memory, as training of the classifier is performed on a single sequential pass only.\nSensitivity to large numbers of attributes\nIn order to check the effectiveness of AODE classifier in handling the dataset having large number of attributes, as there is always a chance of possible dilution in attribute independence assumption. Hence, we propose to use MOEA to reduce the attribute size to make AODE is a promising classifier in comparison to the others in terms of accuracy and time to build the model.\n5. Materials and Methods\nIn this section, the details about the 23 dataset consisting small, medium and large in volume, variety, velocity and veracity used in this experiment are listed in Table 1. They are collected from UCI Machine Learning repository [36].\nTo evaluate the performance of AODE with ENORA, all the experiments are conducted by applying them to each data set using 10-fold cross validation on a dual-processor 1.7Ghz Pentium 5 CPU using Windows with 1TB HDD, 4GB RAM in Java environment.\nThe proposed framework for the ENORA based multi-objective evolutionary algorithm with AODE Naive Bayes classifier is shown in Figure 2.\nAs can be seen, the input dataset is applied to ENORA for attribute/feature subset selection process. The detail step wise operation of ENORA is presented in the box mentioned. The obtained reduced attribute sets are now applied to AODE classifier for obtaining the meaningful knowledge in taking a decision in the whole classification process.\nThe model is broad when it deals with large number of variables or attributes. At the same time, the model is both broad and deep, when it finds the numerous variables maintain some kind of complex relations among themselves. In this paper, we consider Poker hand, airline are considered to be big datasets; ALL-AML leukemia, DLBCL Lymphoma, Lung Harvard, Ovarian and medical datasets are taken as medium datasets and the rests are small datasets.\nThe performance measures for our proposed approach ENORA-AODE to its suitability in machine learning applications to big , medium and small datasets are: accuracy, 0-1 loss and root mean square error(RMSE) and time taken to build the model.\nThe classification accuracy sometimes is otherwise known as 0-1 loss because at any instant of testing, the prediction may be correct or false. This is calculated either from a completely separate test dataset or else it could be a cross validation data. The 0-1 loss can be interpreted as probability estimate for which some of the randomly cross validated test data are wrongly classified. This loss should be as low as possible for a good classifier. At the same time, we may consider the classifier is a better one in comparison to others, if it achieves more accuracy, low RMSE and less time to build the model."}, {"heading": "6. Research Findings", "text": "In this section, the experimental results obtained are evaluated to understand the efficacy of the proposed ENORA-AODE classifier model. We perform the assessment of the classifier performance with the help of several matrices such as: accuracy, time to build the model, zeroone loss and RMSE. At first, the experimental study is carried with 21 datasets obtained from UCI Machine learning repository using fusion of ENORA and AODE classifier. The results are presented in Table 2. It can be observed from Table 2 that for large numbers of attributes, the model building time of AODE classifier increases significantly from 0 seconds to 6.24 seconds for completing the 10- fold cross validation. For most of the dataset, it is dramatically faster takes almost no time to build the model. Their corresponding classification accuracy is good with low root mean square error (RMSE), makes the proposed approach, a competitive ones among many available machine learning algorithms. The obtained results for bio-medical datasets and two big datasets (Poker hand and Spam base) are compared with available literature in Table 3, Table 4 with accuracy and Table 5 for RMSE respectively. Further, Table 6 highlights the results with RMSE values for\nvarious real world datasets and compared with other existing works available in the literature.\nFurther for obtaining the efficiency and suitability of our proposed ENORA-AODE fusion approach, extensive comparisons are made by considering win-tie-loss criteria for the whole 22 datasets in terms of accuracy and 0-1 loss. The detailed comparison in terms of accuracy is provided in table 7 and Table 8, while for 0-1 loss interpretation, the results are provided in table 9 and Table 10."}, {"heading": "3 DLBCL 0.03 97.87 0.1504", "text": ""}, {"heading": "2 ALL-AML 0.71 97.37 0.1621", "text": "18 Airline 2.63 26.41 0.2175\n19 Spam base 0.08 93.37 0.2381\n20 Supermarket 0.01 64.03 0.4502\n21 Emotion 0.01 84.32 0.3546"}, {"heading": "CHCGA+S", "text": ""}, {"heading": "CHCGA+R", "text": ""}, {"heading": "MOGA-3", "text": "From Table 3, it can be observed that ours approach performs best in ALL-AML, DLBCL and medical datasets, reasonably well in Colon tumor, Pima-diabetes datasets but poor in breast cancer datasets."}, {"heading": "Methodology Accuracy in %", "text": "Nursery\nSSMA [44] 88.07\nOurs (ENORA-AODE) 70.97\nDLBCL\nMulti-objective binary PSO [46] 91.84\nOurs (ENORA-AODE) 97.87\nALL-AML\nMulti-objective binary PSO [46] 79.09\nOurs (ENORA-AODE) 97.37\nTable 4 confirms that our proposed approach wins the race for German credit, Spam base, HeartC, Lymphography, DLBCL and ALL-AML datasets and loses in Pima-diabetes, Nursery and Breast cancer datasets. The majority of wins goes in favor of our proposed approach as an efficient ones among many presented in the table 4.\nWhile comparing big datasets such as poker hand, our approach concedes lowest RMSE, opines it to be the best model in comparison to other variants of AODE classifier available. It is also found that ours ENORA-AODE is the best for a relatively small datasets like: spam base with lowest RMSE."}, {"heading": "NB [39] 0.2601 0.4204 0.3413 0.1770 0.1652 0.3714 --", "text": ""}, {"heading": "RS [42] --- --- ---- ---- ---- ---- 0.370", "text": "As can be seen from Table 6 that our approach outweighs Naive Bayes, AODE, A2DE, PA2DE in Lymphography (Lymph), waveform, Pen-digit and Heart-C datasets and RS in emotions dataset; but fails in German credit and nursery datasets. Here also, majority wins supports our approach.\nLymph 85.81 86.25 81.52 80.76 84.16 81.87\nWaveform5000 84.1 84.87 83.64 86.31 83.85 84.4\nTo check, whether ours ENORA-AODE approach performs better in comparison to the latest developments in variants of AODE classifiers, a comparison is provided in Table 7 and table 8 in terms if accuracy and 0-1 loss respectively.\nFrom Table-8, we can re-confirm that our proposed approach performs well in accuracy comparison to AWAODE-CFS, WAODE and DT-WAODE with more wins for the datasets in Table 7.\nSimilarly, 0-1 loss criteria are used to evaluate our method with win-tie-loss (W-T-L) and variants of AODE for the datasets shown in Table 9 and Table 10. The results are encouraging as the proposed ENORA-AODE approach has won all the cases with more number of wins for all datasets."}, {"heading": "NB --- 1/4/4 1/5/3 1/4/4 1/5/3 2/4/3 1/1/7", "text": ""}, {"heading": "7. Conclusions", "text": "This paper introduces the fusion of AODE classifier with multi-objective evolutionary algorithm ENORA to address the large number of attributes in the dataset that sometimes not advisable for better machine learning applications. AODE for its nature of behaving linearly for the training data is computationally efficient taking less time to build the model even though the numbers of attributes are large. ENORA is a promising algorithm for its best of its capability to reduce the whole set of attributes to the best possible ones. The results obtained by using the fusion of ENORA-AODE have outperformed all the available AODE variants in terms of having better predictive accuracy, low 0-1 loss and low RMSE. In future, more investigation are proposed for deep broad learning for big data with new nature inspired algorithms, in order to obtain more effective and efficient solutions."}, {"heading": "2010. ISBN: 978-0-471-87339-6", "text": "[29] J.C. Fernandez, C. Hervas, F.J. Martinez-Estudillo, P.A. Gutierrez, Memetic Pareto Evolutionary Artificial Neural Networks to determine growth/no-growth in predictive microbiology, Applied soft computing, 11(1), 534-550, 2011, Elsevier.\n[30] K. Deb and N. Srinivas, Multiobjective optimization using nondominated sorting in genetic algorithms, Journal Evol. Comput., 2, 221\u2013248, 1994.\n[31] L. Thiele and E. Zitzler, \u201cMultiobjective evolutionary algorithms: The strength Pareto approach,\u201d IEEE Trans. Evol. Comput., 3, 257\u2013271, Nov. 1999.\n[32] E. Zitzler, L. Thiele and M. Laumanns, SPEA2: Improving the strength Pareto evolutionary algorithm, in: Proc. EUROGEN Int. Conf. on Evol. methods for Design, Opti. And control with appls. To industrial problems, 95\u2013100, 2001.\n[33] Jimenez F., Sanchez G. And Juarez JM. , Multi-objective evolutionary algorithms for fuzzy classification in survival prediction, Artificial intelligence in medicine, 60(3), 197-219., 2014. oi: 10.1016/j.artmed.2013.12.006\n[34] K. Deb and A. Pratap and S. Agarwal and T. Meyarivan, A Fast and Elitist Multiobjective Genetic Algorithm: NSGA\u2013II, IEEE Transactions on Evolutionary Computation, Volume:6 (2) , 182-197, 2002.\n[35] G. I. Webb, Not so Naive Bayes: aggregating one-dependence estimators, Machine learning,"}, {"heading": "58, 5-24, 2005.", "text": "[36] A. Frank and A. Asunction, UCI Machine Learning repository, 2010. URL http://archieve.ics.uci.edu/ml\n[37]J. Liu and H. Iba, Selective informative genes using a multiobjective evolutionary algorithm, in:proceedings of the IEEE congress on Evolutionary computation, CEC 2002, 1, 297-302.\n[38] J. Hernandez, B. Duval and J.-K. Hao, A genetic embedded approach for gene selection and classification of microarray data, in: E. Marchiori et al. (eds.), EvoBIO in: LNCS, 90-101, 2007.\n[39] G.I. Webb, J. R. Boughton, F. Zheng, K. M. Ting and H. Salem, Decreasingly naive Bayes: aggregating n-dependence estomators, KNOWSYS Lab technical report 2010-01, Monash university, 2010.\n[40] N.A.zaidi, G.I.Webb, M.J.carman and F. Petitjean, Deep broad Learning-Big models for Big data, Journal of Machine Learning research 1, 1-24, 2015.\n[41] M. Vimaladev and B. Kalavathi, A microarray gene expression data classification using hybrid back propagation neural network, genetika, 4693, 1013-1026, 2014.\n[42] T. -H. Chirag, H. -Y. Lo and s. -de Lin, A ranking based KNN approach for multi label classification, Asian conference on machine learning, journal of machine learning researchworkshop and conference proceedings, 25, 81-96, 2012.\n[43] M. R. Gholamian, S.M. Sadatrasoul and Z. Hajimohamadi, Journal of advances in computer research, 3(3), 53-64, 2012.\n[44] S. Garcia, J.R. Cano and F. Herrera, A memetic algorithm for evolutionary prototype selection: a scaling up approach, Pattern Recognition, 41(8), 2693-2709, August, 2008\n[45] F. Rudzinsky, A multiobjective genetic ptimization of interpretability oriented fuzzy rule based classifiers, Applied Soft Computing, 38(C), 118-133, January 2016\n[46] M. Mandal and A. Mukhopadhyay, A graph theoritic approach for identifying nonredundant and relevant gene markers from microarray data using multiobjective binary PSO, PLOS One, 9(3), 2014.\n[47] J. Wu and Z. Cai, Learning average one dependence estimator by attribute weighting, journal of Information and computational science, 8(7), 1063-1073., 2011."}], "references": [{"title": "Feature Subset Selection Based on Bio-Inspired Algorithm", "author": ["C. Yun", "B. O", "J. Yang", "J. Nang"], "venue": "Journal of Information Science and Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A survey of multi-objective evolutionary algorithms for data mining: Part-II", "author": ["A. Mukhopadhyay", "U. Maulik", "S. Bandyopadhyay", "C.A. Coello Coello"], "venue": "IEEE Transactions on Evolutionary Computation, 18(1), 20-35", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiobjective Genetic Algorithms for Clustering", "author": ["U. Maulik", "S. Bandyopadhyay", "A. Mukhopadhyay"], "venue": "Applications in Data Mining and Bioinformatics. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on multi-objective evolutionary algorithms for many-objective problems", "author": ["Carlos Brizuela"], "venue": "Comput Optim Appl,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A Multiobjective Genetic Algorithm Based on a Discrete Selection Procedure", "author": ["Qiang Long", "Changzhi Wu", "XiangyuWang", "Lin Jiang", "Jueyou Li"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Multi-objective genetic fuzzy classifiers for imbalanced and cost-sensitive datasets, WIREs Data Mining", "author": ["Satchidananda Dehuri", "Ashish Ghosh"], "venue": "Knowl Discov", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Multi-objective genetic fuzzy classifiers for imbalanced and cost-sensitive datasets", "author": [], "venue": "Soft Comput,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Filter-based optimization techniques for selection of feature subsets in ensemble systems, Expert Systems with Applications", "author": ["Anne M. de Paula Canuto"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Feature Subset Selection using Non-dominated Sorting", "author": ["A. Khan", "A.R. Baig", "Multi-Objective"], "venue": "Genetic Algorithm, JournalUofUAppliedUResearchUandUTechnology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A Multi Objective Evolutionary Algorithm for Solving a Real Health Care Fleet Optimization Problem, In: proc", "author": ["Carlos Cataniaa", "Cecilia Zanni-Merka", "Pierre Colleta"], "venue": "Of 19th International Conference on Knowledge Based and Intelligent Information and Engineering Systems, Procedia Computer Science 60,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A survey on feature ranking by means of evolutionary computation, Annals of the University of Craiova", "author": ["Ruxandra Stoean", "Florin Gorunescu"], "venue": "Mathematics and Computer Science Series,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A Survey of Multi-Objective Evolutionary Algorithms for Data Mining: Part-I", "author": ["Anirban Mukhopadhyay", "Ujjwal Maulik", "Sanghamitra Bandyopadhyay", "Carlos A. Coello Coello"], "venue": "IEEE Transaction on Evolutionary Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A Filter-based Evolutionary Approach for Selecting Features in High-Dimensional Micro-array Data, Intelligent Information Processing V, IFIP Advances in Information and Communication", "author": ["Laura Maria Cannas", "Nicoletta Dess\u00ec", "Barbara Pes"], "venue": "Technology series,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A survey on feature selection methods", "author": ["Girish Chandrashekar", "Ferat Sahin"], "venue": "Computers and Electrical Engineering", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Evolutionary Multiobjective Ensemble Learning Based on Bayesian Feature Selection", "author": ["Huanhuan Chen", "Xin Yao"], "venue": "Proc. of IEEE Congress on Evolutionary Computation ,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A Multi- Objective Genetic Algorithm Approach to Feature Selection in Neural and Fuzzy Modeling, Evolutionary Optimization, An International", "author": ["Christos Emmanouilidis", "Andrew Hunter", "John Macintyre", "Chris Cox"], "venue": "Journal on the Internet,3", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Puertaa, A Scalable Pairwise Class Interaction Framework for Multidimensional Classification, International", "author": ["Jacinto Ariasa", "Jose A. Gameza", "Thomas D. Nielsenb", "Jose M"], "venue": "Journal of Approximate Reasoning,68(C),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Sensitivity and specificity based multiobjective approach for feature selection: Application to cancer diagnosis", "author": ["J. Garc\u00eda-Nieto", "E. Albaa", "L. Jourdanb", "E. Talbi"], "venue": "Information Processing Letters 109, 887\u2013896", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-Class Protein Fold Recognition Using Multi-Objective Evolutionary Algorithms", "author": ["Stanley Y.M. Shi", "P.N. Suganthan", "Kalyanmoy Deb"], "venue": "in: Proc. Of CIBCB,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Condado, Multi- Objective Evolutionary Algorithm for Land-Use Management Problem", "author": ["Dilip Datta", "Kalyanmoy Deb", "Carlos M. Fonseca", "Fernando Lobo", "Paulo"], "venue": "International Journal of Computational Intelligence Research; 3(4),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Multi-objective optimization of support vector Machines", "author": ["Thorsten Suttorp", "Christian Igel"], "venue": "Multi-objective Machine Learning Studies in Computational Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Q", "author": ["K.C. Tan", "E.J. Teoh"], "venue": "Yua, K.C. Goh , A hybrid evolutionary algorithm for attribute selection in data mining, Expert Systems with Applications 36, 8616\u20138630", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparison of Multiobjective Evolutionary Algorithms: Empirical Results", "author": ["Eckart Zitzler", "Kalyanmoy Deb", "Lothar Thiele"], "venue": "Evolutionary Computation", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "A Tutorial on Evolutionary Multiobjective Optimization, Metaheuristics for Multiobjective Optimisation", "author": ["Eckart Zitzler", "Marco Laumanns", "Stefan Bleuler"], "venue": "Lecture Notes in Economics and Mathematical Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Multi-Objective Optimization using Evolutionary Algorithms.Wiley", "author": ["K. Deb"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Memetic Pareto Evolutionary Artificial Neural Networks to determine growth/no-growth in predictive microbiology", "author": ["J.C. Fernandez", "C. Hervas", "F.J. Martinez-Estudillo", "P.A. Gutierrez"], "venue": "Applied soft computing, 11(1), 534-550", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiobjective optimization using nondominated sorting in genetic algorithms", "author": ["K. Deb", "N. Srinivas"], "venue": "Journal Evol. Comput., 2, 221\u2013248", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "Multiobjective evolutionary algorithms: The strength Pareto approach,", "author": ["L. Thiele", "E. Zitzler"], "venue": "IEEE Trans. Evol. Comput.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "SPEA2: Improving the strength Pareto evolutionary algorithm", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns"], "venue": "in: Proc. EUROGEN Int. Conf. on Evol. methods for Design, Opti. And control with appls. To industrial problems, 95\u2013100", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-objective evolutionary algorithms for fuzzy classification in survival prediction", "author": ["F. Jimenez", "G. Sanchez", "JM. Juarez"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA\u2013II", "author": ["K. Deb", "A. Pratap", "S. Agarwal", "T. Meyarivan"], "venue": "IEEE Transactions on Evolutionary Computation, Volume:6 (2) , 182-197", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Not so Naive Bayes: aggregating one-dependence estimators", "author": ["G.I. Webb"], "venue": "Machine learning, 58, 5-24", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Selective informative genes using a multiobjective evolutionary algorithm, in:proceedings of the IEEE congress on Evolutionary computation", "author": ["J. Liu", "H. Iba"], "venue": "CEC 2002,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "A genetic embedded approach for gene selection and classification of microarray data", "author": ["J. Hernandez", "B. Duval", "J.-K. Hao"], "venue": "in: E. Marchiori et al. (eds.), EvoBIO in: LNCS, 90-101", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Decreasingly naive Bayes: aggregating n-dependence estomators", "author": ["G.I. Webb", "J.R. Boughton", "F. Zheng", "K.M. Ting", "H. Salem"], "venue": "KNOWSYS Lab technical report 2010-01, Monash university", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep broad Learning-Big models for Big data, Journal of Machine Learning research", "author": ["N.A.zaidi", "G.I.Webb", "M.J.carman", "F. Petitjean"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "A microarray gene expression data classification using hybrid back propagation neural network", "author": ["M. Vimaladev", "B. Kalavathi"], "venue": "genetika, 4693, 1013-1026", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "A ranking based KNN approach for multi label classification, Asian conference on machine learning, journal of machine learning", "author": ["T. -H. Chirag", "H. -Y. Lo", "s. -de Lin"], "venue": "researchworkshop and conference proceedings,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Journal of advances in computer research", "author": ["M.R. Gholamian", "S.M. Sadatrasoul", "Z. Hajimohamadi"], "venue": "3(3), 53-64", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A memetic algorithm for evolutionary prototype selection: a scaling up approach", "author": ["S. Garcia", "J.R. Cano", "F. Herrera"], "venue": "Pattern Recognition,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "A multiobjective genetic ptimization of interpretability oriented fuzzy rule based classifiers", "author": ["F. Rudzinsky"], "venue": "Applied Soft Computing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "A graph theoritic approach for identifying nonredundant and relevant gene markers from microarray data using multiobjective binary PSO", "author": ["M. Mandal", "A. Mukhopadhyay"], "venue": "PLOS One, 9(3)", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning average one dependence estimator by attribute weighting", "author": ["J. Wu", "Z. Cai"], "venue": "journal of Information and computational science, 8(7), 1063-1073.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "It is also noticed that, there are a plenty of approaches present in the literature for obtaining quality feature subsets, still it is considered to be a difficult task to obtain the best ones [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 1, "context": "The parallel architecture of the Evolutionary algorithms (EA) are a potential candidate to process such big data automatically for optimal parameter setting and more importantly obtaining a viable solution for better interpretation of the model with best classification accuracy possible [3,4].", "startOffset": 288, "endOffset": 293}, {"referenceID": 2, "context": "The parallel architecture of the Evolutionary algorithms (EA) are a potential candidate to process such big data automatically for optimal parameter setting and more importantly obtaining a viable solution for better interpretation of the model with best classification accuracy possible [3,4].", "startOffset": 288, "endOffset": 293}, {"referenceID": 3, "context": "The Pareto-optimal set solutions are considered not to be dominated by any other possible solution [5,6].", "startOffset": 99, "endOffset": 104}, {"referenceID": 4, "context": "The Pareto-optimal set solutions are considered not to be dominated by any other possible solution [5,6].", "startOffset": 99, "endOffset": 104}, {"referenceID": 5, "context": "Dehuri and Ghosh [7] proposes to use a multi-objective genetic based feature selection method for knowledge discovery process and discusses its pros and cons for its validity and potentiality.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "Ducange, Lazzerini and Marcelloni [8] used NSGA-II, a well known multiobjective optimization algorithm as a feature subset selection in highly imbalanced dataset.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "[9] proposed to use ensemble filter based classification with feature subset selection done by Particle swarm (PSO), genetic algorithm (GA) and ant-colony optimization (ACO) techniques.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Khan and Baig [10] opined that large number of irrelevant attributes are to be removed from the datasets in order to enhance the accuracy of the classifier.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "Cataniaa, Zanni-merka, Beuvrona and Colleta [11] discussed the various aspects of multi-objective optimization NSGA-II implementation in Patient Transport services with preliminary results.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "The authors Stoean and Gorunescu [12] presented a study to", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "Mukhopadhyay, Maulik, Bandyopadhyay, and Coello Coello [14] presented a comprehensive survey on recent advances in multi-objective evolutionary optimization techniques as a feature selection and classification for automatic processing of large qualities of data that can solve many real world problems with various various conflicting measures of performance.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Cannas, Dessi and Pes [15] proposed four popular filter based procedure along with support vector machine in order to reduce the search space in high dimensional micro array data sets, for obtaining potential solutions to predict and diagnose the disease.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "A generic review on various filter, wrapper based feature selection and their possible combination are presented by Chandrasekharan and Sahin [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "Chen and Yao [17] demonstrated the effectiveness of the proposed ensemble of evolutionary multi-objective algorithms and Bayesian automatic relevance determination to obtain better accuracy with reduced feature sets.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "[18] where they pointed out the tradeoffs between computational complexity with classification accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Ariasa, Gameza, Nielsenb and Puertaa [20] proposed a scalable pair wise class interaction framework for multi-dimension classification by taking several base classifiers and inference methods afterwards.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "A novel multi-objective genetic algorithm based feature selection combined with support vector machine is used [21] for selection of genes in micro array dataset for cancer diagnosis.", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "Shi, Suganthan and Deb [22] proposed to use support vector machine as a classifier for structural classification of protein (SCOP) after the relevant feature are selected by using Multi-Objective Feature Analysis and Selection Algorithm (MOFASA).", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "Datta, Deb, Fonseca, Lobo and Condado [23] proposed a spatial GIS based multi-objective evolutionary algorithm (NSGAII-LUM) for better understanding of the impact from land uses in Mediterranean landscape from Southern Portugal.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "They discuss about how to model the proposed framework by considering the tradeoffs between accuracy and model complexity with reduced number of support vector [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "Goh [25] proposed genetic- support vector (GA-SVM) hybrid evolutionary algorithm for attribute selection in data mining.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "The authors provided an empirical comparison of various MOO algorithms, considering six test functions, which are aimed at giving an impression to understand which technique performs well under what condition in Zitler, Deb and Thiele [26].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "A good tutorial on theory and model are provided by Zitzler, Laumanns, and Bleuler [27], where various algorithmic aspects of MOO such as: assignment of fitness function, elitism and diversity etc.", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "[27] that due to the complex nature of generating Pareto optimal solution sets, many stochastic search namely: evolutionary algorithms, simulated annealing etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Further, it can be noticed that MOO can be a maximization or minimization problem with a number of objective functions [28].", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "Now-a-days, multi-objective evolutionary algorithms have attracted many a researchers in the multi faced applications to provide some viable solutions to multi-objective problems [28-29]; that include to name a few: Non-dominated sorting Genetic algorithm (NSGA) by Deb and Srinivas [30]; Strength Pareto Evolutionary Algorithm (SPEA 1 and 2) [31-32].", "startOffset": 179, "endOffset": 186}, {"referenceID": 25, "context": "Now-a-days, multi-objective evolutionary algorithms have attracted many a researchers in the multi faced applications to provide some viable solutions to multi-objective problems [28-29]; that include to name a few: Non-dominated sorting Genetic algorithm (NSGA) by Deb and Srinivas [30]; Strength Pareto Evolutionary Algorithm (SPEA 1 and 2) [31-32].", "startOffset": 179, "endOffset": 186}, {"referenceID": 26, "context": "Now-a-days, multi-objective evolutionary algorithms have attracted many a researchers in the multi faced applications to provide some viable solutions to multi-objective problems [28-29]; that include to name a few: Non-dominated sorting Genetic algorithm (NSGA) by Deb and Srinivas [30]; Strength Pareto Evolutionary Algorithm (SPEA 1 and 2) [31-32].", "startOffset": 283, "endOffset": 287}, {"referenceID": 27, "context": "Now-a-days, multi-objective evolutionary algorithms have attracted many a researchers in the multi faced applications to provide some viable solutions to multi-objective problems [28-29]; that include to name a few: Non-dominated sorting Genetic algorithm (NSGA) by Deb and Srinivas [30]; Strength Pareto Evolutionary Algorithm (SPEA 1 and 2) [31-32].", "startOffset": 343, "endOffset": 350}, {"referenceID": 28, "context": "Now-a-days, multi-objective evolutionary algorithms have attracted many a researchers in the multi faced applications to provide some viable solutions to multi-objective problems [28-29]; that include to name a few: Non-dominated sorting Genetic algorithm (NSGA) by Deb and Srinivas [30]; Strength Pareto Evolutionary Algorithm (SPEA 1 and 2) [31-32].", "startOffset": 343, "endOffset": 350}, {"referenceID": 29, "context": "ENORA [33] is an elitist Pareto- based multi-objective evolutionary algorithm that uses a Mu plus Lambda survival with uniform random initialization and binary tournament selection process for exploring the attribute space.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "NSGA-II [34] is also a popular multi-objective genetic algorithm based constrained optimization technique which is aimed at improving the Pareto optimal solutions using evolutionary operators such as: selection, genetic mutation and crossover.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "One variant of such a popular Naive Bayes technique is Averaged one-dependence estimator (AODE) Naive Bayes [35].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Algorithm/ Dataset Colon Tumor ALL-AML DLBCL Medical Breast Cancer PimaDiabetes CHCGA+S VM[16 ] --- ------73.", "startOffset": 90, "endOffset": 95}, {"referenceID": 13, "context": "CHCGA+R BF [16 ] --- ------70 96.", "startOffset": 11, "endOffset": 16}, {"referenceID": 17, "context": "K-means [21] 78.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "MOGA-3 Obj [21] 89.", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "Liu and Eba [37] 80 -90 ---------", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "Hermandev [38] 84.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "Hybrid GABPN [41 ] --- 89.", "startOffset": 13, "endOffset": 18}, {"referenceID": 38, "context": "ANFIS [43] 70 MOPSO [43] 70 NSGA-II+SFP [45] 72.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": "ANFIS [43] 70 MOPSO [43] 70 NSGA-II+SFP [45] 72.", "startOffset": 20, "endOffset": 24}, {"referenceID": 40, "context": "ANFIS [43] 70 MOPSO [43] 70 NSGA-II+SFP [45] 72.", "startOffset": 40, "endOffset": 44}, {"referenceID": 39, "context": "SSMA [44] 91.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "23 NSGA-II+SFP [45] 86.", "startOffset": 15, "endOffset": 19}, {"referenceID": 39, "context": "SSMA [44] 63.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "SSMA [44] 55.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "SSMA [44] 82.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "15 NSGA-II+SFP [45] 76.", "startOffset": 15, "endOffset": 19}, {"referenceID": 39, "context": "SSMA [44] 97.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "SSMA [44] 88.", "startOffset": 5, "endOffset": 9}, {"referenceID": 41, "context": "Multi-objective binary PSO [46] 91.", "startOffset": 27, "endOffset": 31}, {"referenceID": 41, "context": "Multi-objective binary PSO [46] 79.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "A1DE[40] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 35, "context": "A2DE [40] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "DBL1 [40] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "A1JE [40] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "NB [39] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "3714 -AODE [39] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "A2DE [39] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "PA2DE [39] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 37, "context": "RS [42] -------- ---- ---- ---- 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "Dataset Ours (ENORAAODE) AODE [47] AWAODECFS [47] AWAODE-GW [47] WAODE [47] DTWAODE [47] Breast cancer 72.", "startOffset": 30, "endOffset": 34}, {"referenceID": 42, "context": "Dataset Ours (ENORAAODE) AODE [47] AWAODECFS [47] AWAODE-GW [47] WAODE [47] DTWAODE [47] Breast cancer 72.", "startOffset": 45, "endOffset": 49}, {"referenceID": 42, "context": "Dataset Ours (ENORAAODE) AODE [47] AWAODECFS [47] AWAODE-GW [47] WAODE [47] DTWAODE [47] Breast cancer 72.", "startOffset": 60, "endOffset": 64}, {"referenceID": 42, "context": "Dataset Ours (ENORAAODE) AODE [47] AWAODECFS [47] AWAODE-GW [47] WAODE [47] DTWAODE [47] Breast cancer 72.", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": "Dataset Ours (ENORAAODE) AODE [47] AWAODECFS [47] AWAODE-GW [47] WAODE [47] DTWAODE [47] Breast cancer 72.", "startOffset": 84, "endOffset": 88}, {"referenceID": 34, "context": "Dataset NB [39] AODE [39] A2DE [39] PA2DE [39] FA2DE [39] TAN [39] Ours (ENORA -AODE) Lymph 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "Dataset NB [39] AODE [39] A2DE [39] PA2DE [39] FA2DE [39] TAN [39] Ours (ENORA -AODE) Lymph 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 34, "context": "Dataset NB [39] AODE [39] A2DE [39] PA2DE [39] FA2DE [39] TAN [39] Ours (ENORA -AODE) Lymph 0.", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "Dataset NB [39] AODE [39] A2DE [39] PA2DE [39] FA2DE [39] TAN [39] Ours (ENORA -AODE) Lymph 0.", "startOffset": 42, "endOffset": 46}, {"referenceID": 34, "context": "Dataset NB [39] AODE [39] A2DE [39] PA2DE [39] FA2DE [39] TAN [39] Ours (ENORA -AODE) Lymph 0.", "startOffset": 53, "endOffset": 57}, {"referenceID": 34, "context": "Dataset NB [39] AODE [39] A2DE [39] PA2DE [39] FA2DE [39] TAN [39] Ours (ENORA -AODE) Lymph 0.", "startOffset": 62, "endOffset": 66}], "year": 2016, "abstractText": "Even though, many researchers tried to explore the various possibilities on multi objective feature selection, still it is yet to be explored with best of its capabilities in data mining applications rather than going for developing new ones. In this paper, multi-objective evolutionary algorithm ENORA is used to select the features in a multi-class classification problem. The fusion of AnDE (averaged n-dependence estimators) with n=1, a variant of naive Bayes with efficient feature selection by ENORA is performed in order to obtain a fast hybrid classifier which can effectively learn from big data. This method aims at solving the problem of finding optimal feature subset from full data which at present still remains to be a difficult problem. The efficacy of the obtained classifier is extensively evaluated with a range of most popular 21 real world dataset, ranging from small to big. The results obtained are encouraging in terms of time, Root mean square error, zero-one loss and classification accuracy. Key-words: MOEA, ENORA, AODE, Classification, 0/1 loss, RMSE", "creator": "Bullzip PDF Printer (10.10.0.2307)"}}}