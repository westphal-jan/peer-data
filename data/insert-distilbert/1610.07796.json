{"id": "1610.07796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks", "abstract": "we analyze the performance of encoder - decoder neural models and compare them typically with well - known established methods. the latter represent different classes of traditional approaches that are applied to the monotone sequence - to - sequence tasks ocr post - correction, electronic spelling correction, grapheme - to - phoneme conversion, and cellular lemmatization. such tasks are of practical relevance for various higher - level research fields including digital genetic humanities, automatic text casualty correction, and speech recognition. additionally we investigate how well generic deep - learning technology approaches gradually adapt to these tasks,, and how algorithms they perform normally in comparison with established and more specialized methods, together including our early own adaptation of pruned crfs.", "histories": [["v1", "Tue, 25 Oct 2016 09:14:05 GMT  (830kb)", "https://arxiv.org/abs/1610.07796v1", "Accepted for publication at COLING 2016. See also:this https URL&amp;tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450"], ["v2", "Wed, 26 Oct 2016 13:05:39 GMT  (734kb)", "http://arxiv.org/abs/1610.07796v2", "Accepted for publication at COLING 2016. See also:this https URL&amp;tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450 Version 2: corrected spelling of third author"]], "COMMENTS": "Accepted for publication at COLING 2016. See also:this https URL&amp;tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["carsten schnober", "steffen eger", "erik-l\\^an do dinh", "iryna gurevych"], "accepted": false, "id": "1610.07796"}, "pdf": {"name": "1610.07796.pdf", "metadata": {"source": "CRF", "title": "Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks", "authors": ["Carsten Schnober", "Steffen Eger"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n07 79\n6v 2\n[ cs\n.C L\n] 2\n6 O\nct 2\n01 6"}, {"heading": "1 Introduction", "text": "Encoder-decoder neural models (Sutskever et al., 2014) are a generic deep-learning approach to sequence-to-sequence translation (Seq2Seq) tasks. They encode an input sequence into a vector representation from which the decoder generates an output. These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016).\nWe have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on \u201cpar or better\u201d performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches.\nHere, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field. Their simplicity vis-a\u0300-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models.\nWe compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al. (2016) \u2014 to three very\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\n1We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no \u2018crossing edges\u2019 in corresponding alignments.\nwell-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al., 2012). We also offer our own contribution2 , which may be considered a variation of the principles underlying DirecTL+. For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (Mu\u0308ller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks.\nWe find that traditional models appear to still be on par with or better than encoder-decoder models in most cases, depending on factors such as training data size and the complexity of the task at hand. We show that neural models unfold their strengths as soon as more complex phenomena need to be learned. This becomes clearly visible in the comparison between lemmatization and the other tasks we have investigated. Lemmatization is the only task at hand in which neural models outperform all established systems \u2014 as it is the only one which systematically exhibits long-range dependencies, particularly through Finnish vowel harmony (see Section 5). We are thus able to contrast the different challenges imposed by different tasks and show how these differences have significant impact on the performance of encoder-decoder models in comparison to established Seq2Seq models.\nTo our best knowledge, no systematic comparison with regard to the suitability of these encoderdecoder neural models for a wider and more generic selection of tasks has been conducted."}, {"heading": "2 Task Description", "text": "Throughout, we denote individual tokens in a sequence by ordinary letters x, and a sequence of symbols by ~x. Hence a string of length s is denoted as ~x = x1 . . . xs. Real-valued vectors are denoted by bold-faced letters, x.\nSpelling correction is the problem of converting an \u2018erroneous\u2019 input sequence ~x into a corrected version ~y. In terms of errors committed by humans (typos), spelling correction often deals with errors that are due to keyboard adjacency of characters and grapho-phonemic mismatch (e.g. emergancy \u2192 emergency, wuld \u2192 would).\nOCR post-correction can be seen as a special case of spelling correction. OCR (optical character recognition) is the process of digitizing printed texts automatically, often applied to make text data from the pre-electronic age digitally available (Springmann et al., 2014). Depending on various factors including paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and \u201cunusual\u201d typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn \u2192 m or li \u2192 h.\nPrevious works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupa\u0142a, 2014) were also investigated in previous works.\nG2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which \u2014 as a fundamental building block for speech recognition, speech synthesis, and related tasks \u2014 has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets.\nLemmatization is the task of deriving the lemma from an inflected word form such as atmest\u2192atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.\n2Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq"}, {"heading": "3 Data", "text": "Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate.\nThe Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (\u201cSwiss Alpine Club\u201d) from the years 1864\u20131899 in Swiss German and French. The data has been digitized and OCR errors have been corrected manually. The corpus contains 19,024 pages, 17,186 of which are in Swiss German, and 1,838 are in French. We have extracted 88,302 unique misrecognized words along with their manually corrected counterparts.\nFor our experiments, we have used randomly selected 72K entries for training and test our models on another 9K entries. Furthermore, we report results for each model trained on a reduced training set (10K entries).\nTwitter Typo Corpus3: We use a corpus of 39,172 spelling mistakes extracted from English Tweets with their respective corrections. The manually corrected mistakes come with a context word on both sides. Again, we have split the data randomly, using a training set of 31K entries and 4K for testing. We also report results on the same test set when using a reduced training set with 10K entries.\nThe Combilex data set (Richmond et al., 2009) provides mappings from English graphemes to phonetic representations (Table 1). We use different subsets for training, with 2K, 5K, 10K, and 20K entries respectively. Furthermore, we employ a test set with 26,609 entries.\nFor lemmatization, we use the Wiktionary Morphology Dataset (Durrett and DeNero, 2013). The data set contains inflected forms for different languages and parts of speech, corresponding lemmas, and detailed inflection information, including mood, case, and tense. We conduct experiments on the German and Finnish verb datasets and further reduce the size of the latter by considering present tense indicative verb forms in active voice only. Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al. (2016) because we focus on lemmatization, not inflection generation, as mentioned. We do so because this produces less overhead \u2014 e.g., Faruqui et al. (2016) train 27 different systems for German verbs, one for each inflection type \u2014 and there is a priori not much difference in whether we transform an inflected form to a lemma or vice versa. Hence, the relative ordering of the systems we survey should not be affected by this change of direction in the morphological analysis. In total, we have used training sets of size 43,929 entries for German verbs and 41,094 entries for Finnish verbs, and dev set and test set sizes of 5,400 (German) and 1,200 (Finnish) entries each."}, {"heading": "4 Model Description", "text": "In this section, we briefly describe encoder-decoder neural models, pruned CRFs, and our three baselines."}, {"heading": "4.1 Encoder-Decoder Neural Models", "text": "We compare three variants of encoder-decoder models: the \u2018classic\u2019 variant and two modifications:\n\u2022 enc-dec: Encoder-decoder models using recurrent neural networks (RNNs) for Seq2Seq tasks were introduced by Cho et al. (2014) and Sutskever et al. (2014). The encoder reads an input ~x and\n3Twitter typo corpus: http://luululu.com/tweet/\ngenerates a vector representation e from it. The decoder predicts the output ~y one time step t at a time, based on e. The probability for each output symbol yt hence depends on e and all previously generated output symbols: p(~y|e) = \u220f T \u2032\nt=1 p(yt|e, y1 \u00b7 \u00b7 \u00b7 yt\u22121) where T \u2032 is the length of the output\nsequence. In NLP, most implementations of encoder-decoder models employ LSTM (long shortterm memory) layers as hidden units, which extend generic RNN hidden layers with a memory cell that is able to \u201cmemorize\u201d and \u201cforget\u201d features. This addresses the \u2018vanishing gradients\u2019 problem and allows to catch long-range dependencies.\n\u2022 attn-enc-dec: We explore the attention-based encoder-decoder model proposed by Bahdanau et al. (2014) (Figure 1). It extends the encoder-decoder model by learning to align and translate jointly. The essential idea is that the current output unit yt does not depend on all input units in the same way, as captured by a \u2018global\u2019 vector e encoding the input. Instead, yt may be conditioned upon local context in the input (to which it pays attention).\n\u2022 morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder. This approach is motivated by the observation that input and output are usually very similar in problems such as morphological inflection. Similar ideas have been proposed in Gu et al. (2016) in their so-called \u201cCopyNet\u201d encoder-decoder model (which they apply to text summarization) that allows for portions of the input sequence to be simply copied to the output sequence, without modifications. A priori, this observation seems to apply to our tasks too: at least in spelling correction, the output usually differs only marginally from the input.\nFor the tested neural models, we follow the same overall approach as Faruqui et al. (2016): we perform decoding and evaluation of the test data using an ensemble of k = 5 independently trained models in order to deal with the non-convex nature of the optimization problem of neural networks and the risk of\nrunning into a local optimum (Collobert et al., 2011). The total probability pens for generating an output token yt is estimated from the individual model output probabilities: pens(yt|\u00b7) = 1\nZ\n\u220f k\ni=1 pi(yt|\u00b7)\n1 k with\na normalization factor Z ."}, {"heading": "4.2 Pruned Conditional Random Fields", "text": "Conditional random fields (CRFs) were introduced by Lafferty et al. (2001) and have been a major workhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s. Unfortunately, training and decoding time depend polynomially on the tag set size and exponentially on the order of the CRF. Here, order refers to the dependencies on the label side. This makes higher-order CRFs impractical for large training data sizes, which is the reason why virtually only first-order (linear chain) CRFs were used until recently.\nMu\u0308ller et al. (2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005). PCRFs require much shorter runtime and are thus able to make use of higher orders. Higher orders, in turn, have been shown to be highly beneficial for coarse and fine-grained part-of-speech tagging, outperforming first-order models.\nFor our tasks, we have adapted the implementation from Mu\u0308ller et al. (2013) \u2014 originally designed for sequence labeling \u2014 to general monotone Seq2Seq tasks. Sequence labeling assumes that an input sequence of length N is mapped to an output sequence of identical length N , while in Seq2Seq tasks, input string lengths may be shorter, longer, or equal to output string lengths.\nWe address this by first aligning input and output sequences as exemplified in\nS l u t l e r f i m S t u d \u2205 e r f i rn\nThis alignment matches up character subsequences from both strings. It may include 1-to-zero matches (e.g. l \u2192 \u2205) and 1-to-many matches (e.g. m \u2192 rn). We disallow many-to-1 or many-to-many matches, as they cause a problem during decoding: at test time, it is unclear how to segment a new input string into parts with size \u2265 1. A na\u0131\u0308ve \u2018pipeline\u2019 approach (first segment, then translate the segmented string) leads to error propagation. More sophisticated \u2018joint\u2019 approaches (Jiampojamarn et al., 2010) are considerably more computationally expensive.\nOnce the data is aligned as above, input and (modified) output sequences are of equal lengths and we can directly apply higher-order PCRFs. Below, we show that orders up to 5 (and possibly beyond) are beneficial for the Seq2Seq tasks we consider.4 We refer to this model as PCRF-Seq2Seq in the remainder.\nFeatures Conditional random fields are feature-based, so we need to decide which features we use. In view of the end-to-end nature of neural techniques, requiring little linguistic knowledge, we also minimize feature-engineering effort for the traditional approaches and thus only include very simple features. For each position p to tag, we include all consecutive character m-grams (m ranges from 1 to a maximum order of N ) within a window of size w around p; i.e., in total our window covers 2w + 1 positions. In our experiments below, we report results for windows of size w = 4 and w = 6. For simplicity, we set N = w in each case."}, {"heading": "4.3 Further Baseline Systems", "text": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.\nSequitur (Bisani and Ney, 2008) is a \u2018joint\u2019 model for Seq2Seq in the sense of the classic distinction between joint and discriminative models. Its core architecture is a model over \u2018joint n-grams\u2019, also termed \u2018graphones\u2019 in the original publication (that is, pairs of substrings of the ~x and ~y sequence).\nDirecTL+ (Jiampojamarn et al., 2010) is a discriminative model for monotone Seq2Seq that integrates joint n-gram features. It jointly learns input segmentation, output prediction, and sequence modeling.\n4In our experiments, higher order CRFs (> 3) substantially outperformed first-order models. Typical performance differences were from about 4 to 7% between first-order and fifth-order models. For brevity, we omit results for orders \u2264 3.\nSince it is based on ordinary CRFs, it is virtually impossible to use this system with higher orders for all practically relevant datasets due to very long training times. Moreover, the system is generally very slow because it jointly learns to segment and translate, as mentioned. For this reason, we have only tested it on the Combilex dataset (Table 3), run with comparable parametrizations (context size, etc.) as PCRF-Seq2Seq.\nPhonetisaurus (Novak et al., 2012) implements a weighted finite state transducer (WFST) to align input and output tokens. The EM-driven algorithm is capable of learning multiple-to-multiple alignments where we restrict both sides to a maximum of 2. The alignments learned from the training data are subsequently used to train a character-based n-gram language model. For brevity, we only report results for models with n = 8, which outperformed lower-order models in our experiments."}, {"heading": "5 Results and Analysis", "text": ""}, {"heading": "5.1 Model Performances", "text": "We report the results of all our experiments in terms of word accuracy (WAC), i.e., the fraction of completely correctly predicted output sequences. Table 2 lists WACs for all our systems on the OCR post-correction task (Text+Berg, full and reduced training set) and on the spelling correction task (Twitter, full and reduced training set). Table 3 reports WAC of all tested models on the Combilex dataset with models trained on training sets of different sizes. Table 4 reports WAC for the lemmatization task on the morphology dataset.\nFor the encoder-decoder models, we report the results with one and two layers of sizes 100 and 200 each. We have additionally conducted sample experiments with larger networks which have shown that neither increasing the number of layers nor the size of the layers leads to further improvements. For the PCRF-Seq2Seq models, we report results for windows of sizes w = 4 and w = 6. We note that, a priori, more training data tends to favor larger context size w, whereas a large w may lead to overfitting when training data is small. The same holds for model order.\nWhile more training data obviously increases WAC for every model, the specific impact varies. In general, (attention-based) encoder-decoder models deal relatively well with limited test data in our experiments, achieving WACs comparable to PCRF-Seq2Seq. In contrast, they appear to benefit less from increasing data sizes than CRFs do. On the Twitter dataset, for instance, the best-performing encoderdecoder model increases WAC by 7.7 percentage points when tripling the training data size. At the same time, the 5th-order PCRF-Seq2Seq WAC increases by 13.8. When a large amount of training data is available, CRFs therefore consistently outperform neural models, and so do the specialized baseline systems on the G2P conversion tasks (Table 3).\nSummarizing, we find that PCRF-Seq2Seq performs best among the tested systems for the two spelling correction tasks when large training data is available. The best performance of PCRF-Seq2Seq is roughly 6-7 percentage points better than the best performance of an encoder-decoder model for both Twitter 31K and Text+Berg 72K. For small training set sizes, PCRF-Seq2Seq and the encoder-decoder models are on a similar level. For the G2P task, an analogous pattern emerges. Moreover, here, all classical systems appear to perform similarly, with DirecTL+ and PCRF-Seq2Seq marginally outperforming the others. For lemmatization, the overall picture looks different. For Finnish verbs, we observe the only case in which attention-based encoder-decoder systems clearly outperform all other approaches. For German, neural models also achieve the best results, albeit only marginally above PCRF-Seq2Seq.\nPrevious works that employed encoder-decoder models successfully focused on tasks like machine translation and grammar correction in which more challenging linguistic phenomena such as long-range dependencies and \u2018crossing edges\u2018 (re-ordering) occur frequently. In our experiments, too, neural models only outperform traditional ones when long-range dependencies become relevant, namely in lemmatization. In all other tasks at hand, in contrast, neural models perform worse or equal.\nThe afore-mentioned, more complex linguistic phenomena intuitively require a more global view on long input sequences which is hard to impossible to model for approaches that cannot look beyond a statically defined context. Spelling mistakes, OCR errors, and G2P, however, largely depend on a very local context. For instance, OCR systems typically do not consider more than a small context when\nestimating the probability of a character. Regarding the G2P task, phonetics is generally independent of characters that occur more than two or three positions before of after, at least in most cases and in English. The same is presumably true for human typos, where a mistaken key stroke may be the result of a previous key\u2019s position, but does not correlate to any key that was hit several time steps before. Hence, neural networks are unable to benefit from their often advantageous capability of modeling long-range dependencies here.\nEspecially the Finnish lemmatization experiments confirm that the capability of dealing with longrange dependencies plays an important role. Finnish vowel harmony makes a vowel control other vowels in the word, potentially across multiple syllables; see Faruqui et al. (2016) for more detailed explanation.\nAs a side note, our results are in line with the common notion that the specific impact of a neural network\u2019s size (number and sizes of layers) is almost unpredictable. Our results can only confirm the general rule-of-thumb that larger networks are better for larger training sets, while models with fewer parameters outperform larger ones when training data is smaller."}, {"heading": "5.2 Training Time", "text": "Another potentially limiting factor for the applicability of a model in real-world scenarios, especially for large datasets, is training time. Under all circumstances, weighted finite state transducers (Phonetisaurus) are trained by magnitudes faster than all other approaches. Training times range between as little as 6 seconds for the smallest training set and 247 seconds for the full Text+Berg training set (72K entries).\nIn comparison, training times for the encoder-decoder models range from 2 to 80 hours (without using GPUs) for 30 epochs, depending on the sizes of the networks and the training data. Furthermore, there is no noticeable difference between either of the three encoder-decoder variations. Training time increases approximately linearly with the number of layers, the size of the layers, and the training data size. All these factors add up, meaning that doubling both the number of layers and the size of the layers approximately quadruples training time.\nContrasting DirecTL+ with PCRF-Seq2Seq, both of which rest on similar principles and also perform similarly in our experiments on the G2P task, we find that training PCRF-Seq2Seq was a factor of 30 or 50 times faster than DirecTL+ on Combilex (2K) and Combilex (5K), respectively. In general, training\nfor PCRF-Seq2Seq across our datasets was in the order of minutes to (few) hours."}, {"heading": "5.3 Error Analysis", "text": "We divided three of our test sets (Text+Berg, Twitter, and Combilex) by input string lengths and evaluated PCRF-Seq2Seq and encoder-decoder neural models on these subsets of the test data. As illustrated in Figures 2 and 3, we observe a consistent tendency: PCRF-Seq2Seq performs relatively robustly over input strings of different lengths, while the performance of the encoder-decoder models plummets more drastically with sequences becoming longer, in particular those without attention-mechanism.\nFor shorter sequences, we observe that standard encoder-decoder models even slightly outperform their attention-based counterparts as well as PCRF-Seq2Seq on both the Twitter spelling correction task (Figure 3) and on G2P conversion, in contrast to their rather low performance on the full datasets. On the Text+Berg data, all systems achieve approximately equal WAC for short sequences (Figure 2).\nFor longer sequences, the performance of the encoder-decoder models drops dramatically on all data sets. This effect is also visible, albeit less strong, for the attention-based variant. This can be seen particularly well on the OCR post-correction task (Figure 2), where the test set contains numerous long sequences: the accuracy rate for the standard encoder-decoder model drops from 73.32% on very short sequences to below 10% for very long ones (\u2265 20), whereas the attention-based model drops less drastically to 38.93% (from 76.92%). At the same time, PCRF-Seq2Seq behaves more stably, particularly on the Twitter data (Figure 3). For the Combilex data, the picture looks very similar \u2014 we omit these results for brevity."}, {"heading": "6 Conclusions", "text": "The generality of neural networks makes them appealing for a wide range of possible tasks. In the scope of this work, we have applied encoder-decoder neural models to monotone Seq2Seq tasks. We have shown that they can perform comparably to more specialized models in some cases, but cannot (yet) consistently outperform established approaches, and are sometimes still substantially below them. Furthermore, the advantage of having rendered feature engineering and hyper-parameter optimization in the traditional sense unnecessary is notoriously substituted by the search for optimal neural network topologies.\nAt first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs.\nThe task-specific extensions to the encoder-decoder proposed by Faruqui et al. (2016) have been shown to produce mostly bad results in our settings. This is particularly surprising for the OCR data, for which input and output sequences are usually very similar, so that we had expected that re-feeding the input to the decoder should be equally beneficial in that domain. As discussed, one explanation might be that OCR, or spelling correction generally, putatively exhibits few long-range dependencies. This might explain why the morph-trans approach works quite well and competitive in morphological analysis tasks, as re-confirmed in our experiments. Thus, long-range dependencies might actually be a more crucial aspect for the performance of the model presented by Faruqui et al. (2016) than the similarity between input and output sequence.\nWe conclude that neural networks are far from completely replacing established methods at this point, as the latter can be both faster and more accurate, depending on the properties of the task at hand. A systematic analysis of the complexities and challenges a particular task imposes, remains unavoidable. At the same time, one can argue that encoder-decoder neural models are a relatively recent development and might continue to improve much over the next years. Being very generic and largely task-agnostic, they are already able to outperform traditional and specialized approaches under certain circumstances."}, {"heading": "Acknowledgements", "text": "This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant \u2116 I/82806, and by the German Institute for Educational Research (DIPF), as part of the graduate program \u201dKnowledge Discovery in Scientific Literature\u201c (KDSL)."}], "references": [{"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden 26\u201330 April 2014, pages 569\u2013578.", "citeRegEx": "Ahlberg et al\\.,? 2014", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473 [cs, stat], September.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Joint-Sequence Models for Grapheme-to-Phoneme Conversion", "author": ["Maximilian Bisani", "Hermann Ney."], "venue": "Speech Communication, 50(5):434\u2013451, May.", "citeRegEx": "Bisani and Ney.,? 2008", "shortCiteRegEx": "Bisani and Ney.", "year": 2008}, {"title": "An Improved Error Model for Noisy Channel Spelling Correction", "author": ["Eric Brill", "Robert C. Moore."], "venue": "Proceedings of ACL \u201900, pages 286\u2013293, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Brill and Moore.,? 2000", "shortCiteRegEx": "Brill and Moore.", "year": 2000}, {"title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of ACL \u201905, pages 173\u2013180, Ann Arbor, MI, USA. Association for Computational Linguistics.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078 [cs, stat], June.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Normalizing Tweets with Edit Scripts and Recurrent Neural Embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Proceedings of ACL \u201914, pages 680\u2013686, Baltimore, MD, USA. Association for Computational Linguistics.", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537, February.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Spelling Correction as an Iterative Process that Exploits the Collective Knowledge of Web Users", "author": ["Silviu Cucerzan", "Eric Brill."], "venue": "Proceedings of EMNLP \u201904, pages 293\u2013300, Barcelona, Spain. Association for Computational Linguistics.", "citeRegEx": "Cucerzan and Brill.,? 2004", "shortCiteRegEx": "Cucerzan and Brill.", "year": 2004}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "Proceedings of NAACL-HLT \u201913, pages 1185\u20131195, Atlanta, GA, USA. Association for Computational Linguistics.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "A Comparison of Four Character-Level String-toString Translation Models for (OCR) Spelling Error Correction", "author": ["Steffen Eger", "Tim vor der Br\u00fcck", "Alexander Mehler."], "venue": "The Prague Bulletin of Mathematical Linguistics, 105(1):77\u201399, April.", "citeRegEx": "Eger et al\\.,? 2016", "shortCiteRegEx": "Eger et al\\.", "year": 2016}, {"title": "Designing and comparing g2p-type lemmatizers for a morphology-rich language", "author": ["Steffen Eger."], "venue": "Systems and Frameworks for Computational Morphology - Fourth International Workshop, SFCM 2015, Stuttgart, Germany, September 17-18, 2015, Proceedings, pages 27\u201340.", "citeRegEx": "Eger.,? 2015", "shortCiteRegEx": "Eger.", "year": 2015}, {"title": "Generalized Character-Level Spelling Error Correction", "author": ["Noura Farra", "Nadi Tomeh", "Alla Rozovskaya", "Nizar Habash."], "venue": "Proceedings of ACL \u201914, pages 161\u2013167, Baltimore, MD, USA. Association for Computational Linguistics.", "citeRegEx": "Farra et al\\.,? 2014", "shortCiteRegEx": "Farra et al\\.", "year": 2014}, {"title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT \u201916, pages 634\u2013643, San Diego, CA, USA. Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-tosequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Improved Iterative Correction for Distant Spelling Errors", "author": ["Sergey Gubanov", "Irina Galinskaya", "Alexey Baytin."], "venue": "Proceedings of ACL \u201914, pages 168\u2013173, Baltimore, MD, USA. Association for Computational Linguistics.", "citeRegEx": "Gubanov et al\\.,? 2014", "shortCiteRegEx": "Gubanov et al\\.", "year": 2014}, {"title": "Integrating Joint n-gram Features into a Discriminative Training Framework", "author": ["Sittichai Jiampojamarn", "Colin Cherry", "Grzegorz Kondrak."], "venue": "Proceedings of NAACL-HLT \u201910, pages 697\u2013700, Los Angeles, CA, USA. Association for Computational Linguistics.", "citeRegEx": "Jiampojamarn et al\\.,? 2010", "shortCiteRegEx": "Jiampojamarn et al\\.", "year": 2010}, {"title": "The CMU Arctic speech databases", "author": ["John Kominek", "Alan W Black."], "venue": "Fifth ISCA Workshop on Speech Synthesis, pages 223\u2013224, Pittsburgh, PA, USA.", "citeRegEx": "Kominek and Black.,? 2004", "shortCiteRegEx": "Kominek and Black.", "year": 2004}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of ICML \u201901, pages 282\u2013289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural Network Recognition of Spelling Errors", "author": ["Mark Lewellen."], "venue": "Proceedings of ACL-COLING \u201998, pages 1490\u20131492, Montr\u00e9al, Qu\u00e9bec, Canada. Association for Computational Linguistics.", "citeRegEx": "Lewellen.,? 1998", "shortCiteRegEx": "Lewellen.", "year": 1998}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP \u201915, pages 1412\u20131421, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient Higher-Order CRFs for Morphological Tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EMNLP \u201913, pages 322\u2013332, Seattle, WA, USA. Association for Computational Linguistics.", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak."], "venue": "Proceedings of NAACL-HLT \u201915, pages 922\u2013931, Denver, CO, USA. Association for Computational Linguistics.", "citeRegEx": "Nicolai et al\\.,? 2015", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "WFST-Based Grapheme-to-Phoneme Conversion: Open Source tools for Alignment, Model-Building and Decoding", "author": ["Josef R. Novak", "Nobuaki Minematsu", "Keikichi Hirose."], "venue": "Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 45\u201349, Donostia, Spain. Association for Computational Linguistics.", "citeRegEx": "Novak et al\\.,? 2012", "shortCiteRegEx": "Novak et al\\.", "year": 2012}, {"title": "A Discriminative Candidate Generator for String Transformations", "author": ["Naoaki Okazaki", "Yoshimasa Tsuruoka", "Sophia Ananiadou", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Okazaki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Okazaki et al\\.", "year": 2008}, {"title": "A Deep Graphical Model for Spelling Correction", "author": ["Stephan Raaijmakers."], "venue": "Proceedings of the 25th Benelux Conference on Artificial Intelligence, pages 160\u2013167, Delft, Netherlands. Delft University of Technology.", "citeRegEx": "Raaijmakers.,? 2013", "shortCiteRegEx": "Raaijmakers.", "year": 2013}, {"title": "Grapheme-to-Phoneme Conversion Using Long Short-Term Memory Recurrent Neural Networks", "author": ["Kanishka Rao", "Fuchun Peng", "Hasim Sak", "Fran\u00e7oise Beaufays."], "venue": "Proceedings of ICASSP \u201915, pages 4225\u2013 4229, South Brisbane, QLD, Australia. Institute of Electrical and Electronics Engineers.", "citeRegEx": "Rao et al\\.,? 2015", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Unsupervised Profiling of OCRed Historical Documents", "author": ["Ulrich Reffle", "Christoph Ringlstetter."], "venue": "Pattern Recognition, 46(5):1346\u20131357, May.", "citeRegEx": "Reffle and Ringlstetter.,? 2013", "shortCiteRegEx": "Reffle and Ringlstetter.", "year": 2013}, {"title": "On OCR Ground Truths and OCR Post-correction Gold Standards, Tools and Formats", "author": ["Martin Reynaert."], "venue": "Proceedings of DATeCH \u201914, pages 159\u2013166, New York, NY, USA. ACM.", "citeRegEx": "Reynaert.,? 2014", "shortCiteRegEx": "Reynaert.", "year": 2014}, {"title": "Robust LTS rules with the Combilex speech technology lexicon", "author": ["Korin Richmond", "Robert A.J. Clark", "Susan Fitt."], "venue": "Proceedings of INTERSPEECH \u201909, pages 1295\u20131298, Brighton, UK. ISCA.", "citeRegEx": "Richmond et al\\.,? 2009", "shortCiteRegEx": "Richmond et al\\.", "year": 2009}, {"title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M Rush", "Stuart M Shieber."], "venue": "arXiv preprint arXiv:1604.04677.", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Substring-Based Transliteration", "author": ["Tarek Sherif", "Grzegorz Kondrak."], "venue": "Proceedings of ACL \u201907, pages 944\u2013951, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "Sherif and Kondrak.,? 2007", "shortCiteRegEx": "Sherif and Kondrak.", "year": 2007}, {"title": "OCR of Historical Printings of Latin Texts: Problems, Prospects, Progress", "author": ["Uwe Springmann", "Dietmar Najock", "Hermann Morgenroth", "Helmut Schmid", "Annette Gotscharek", "Florian Fink."], "venue": "Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, pages 71\u201375, New York, NY, USA. ACM.", "citeRegEx": "Springmann et al\\.,? 2014", "shortCiteRegEx": "Springmann et al\\.", "year": 2014}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Proceedings of Neural Information Processing Systems 2014, pages 3104\u20133112, Montr\u00e9al, Qu\u00e9bec, Canada. Curran Associates, Inc.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Pronunciation modeling for improved spelling correction", "author": ["Kristina Toutanova", "Robert C. Moore."], "venue": "Proceedings of ACL \u201902, pages 144\u2013151, Philadelphia, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Toutanova and Moore.,? 2002", "shortCiteRegEx": "Toutanova and Moore.", "year": 2002}, {"title": "A Neural Conversational Model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of the 31st International Conference on Machine Learning, JMLR: W&CP, volume 37, Lille, France.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Is it time to switch to Word Embedding and Recurrent Neural Networks for Spoken Language Understanding", "author": ["Vedran Vukotic", "Christian Raymond", "Guillaume Gravier"], "venue": null, "citeRegEx": "Vukotic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vukotic et al\\.", "year": 2015}, {"title": "Neural Language Correction with Character-Based Attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y Ng."], "venue": "arXiv preprint arXiv:1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "A Probabilistic Approach to String Transformation", "author": ["Gu Xu", "Hang Li", "Ming Zhang", "Ziqi Wang."], "venue": "IEEE Transactions on Knowledge and Data Engineering, 26(5):1063\u20131075, May.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion", "author": ["Kaisheng Yao", "Geoffrey Zweig."], "venue": "Proceedings of INTERSPEECH \u201915, pages 3330\u20133334, Dresden, Germany. ISCA.", "citeRegEx": "Yao and Zweig.,? 2015", "shortCiteRegEx": "Yao and Zweig.", "year": 2015}, {"title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "author": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Workshop on Human-Computer Question Answering, pages 15\u201321, San Diego, CA, USA. Association for Computational Linguistics.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "Encoder-decoder neural models (Sutskever et al., 2014) are a generic deep-learning approach to sequence-to-sequence translation (Seq2Seq) tasks.", "startOffset": 30, "endOffset": 54}, {"referenceID": 5, "context": "These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al.", "startOffset": 142, "endOffset": 160}, {"referenceID": 35, "context": ", 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 40, "context": ", 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al.", "startOffset": 74, "endOffset": 92}, {"referenceID": 30, "context": ", 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016).", "startOffset": 50, "endOffset": 91}, {"referenceID": 37, "context": ", 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016).", "startOffset": 50, "endOffset": 91}, {"referenceID": 17, "context": "(2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer.", "startOffset": 67, "endOffset": 92}, {"referenceID": 39, "context": "Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 106, "endOffset": 145}, {"referenceID": 26, "context": "Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 106, "endOffset": 145}, {"referenceID": 31, "context": ", 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 25, "endOffset": 51}, {"referenceID": 3, "context": ", 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 77, "endOffset": 100}, {"referenceID": 1, "context": "We compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al.", "startOffset": 87, "endOffset": 130}, {"referenceID": 20, "context": "We compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al.", "startOffset": 87, "endOffset": 130}, {"referenceID": 3, "context": "These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer.", "startOffset": 143, "endOffset": 625}, {"referenceID": 3, "context": "These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on \u201cpar or better\u201d performance of their inflection generation neural architecture.", "startOffset": 143, "endOffset": 899}, {"referenceID": 1, "context": "We compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al. (2016) \u2014 to three very", "startOffset": 88, "endOffset": 179}, {"referenceID": 2, "context": "well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 16, "context": "well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al.", "startOffset": 98, "endOffset": 125}, {"referenceID": 23, "context": ", 2010), and Phonetisaurus (Novak et al., 2012).", "startOffset": 27, "endOffset": 47}, {"referenceID": 21, "context": "For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M\u00fcller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks.", "startOffset": 88, "endOffset": 132}, {"referenceID": 18, "context": "For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M\u00fcller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks.", "startOffset": 88, "endOffset": 132}, {"referenceID": 32, "context": "OCR (optical character recognition) is the process of digitizing printed texts automatically, often applied to make text data from the pre-electronic age digitally available (Springmann et al., 2014).", "startOffset": 174, "endOffset": 199}, {"referenceID": 28, "context": "Depending on various factors including paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014).", "startOffset": 132, "endOffset": 148}, {"referenceID": 3, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 34, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al.", "startOffset": 112, "endOffset": 187}, {"referenceID": 8, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al.", "startOffset": 112, "endOffset": 187}, {"referenceID": 15, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al.", "startOffset": 112, "endOffset": 187}, {"referenceID": 38, "context": ", 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al.", "startOffset": 54, "endOffset": 71}, {"referenceID": 24, "context": ", 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013).", "startOffset": 31, "endOffset": 73}, {"referenceID": 12, "context": ", 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013).", "startOffset": 31, "endOffset": 73}, {"referenceID": 27, "context": ", 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013).", "startOffset": 41, "endOffset": 72}, {"referenceID": 25, "context": "Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupa\u0142a, 2014) were also investigated in previous works.", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupa\u0142a, 2014) were also investigated in previous works.", "startOffset": 97, "endOffset": 113}, {"referenceID": 9, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 0, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 22, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 13, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 29, "context": "The Combilex data set (Richmond et al., 2009) provides mappings from English graphemes to phonetic representations (Table 1).", "startOffset": 22, "endOffset": 45}, {"referenceID": 9, "context": "For lemmatization, we use the Wiktionary Morphology Dataset (Durrett and DeNero, 2013).", "startOffset": 60, "endOffset": 86}, {"referenceID": 8, "context": "For lemmatization, we use the Wiktionary Morphology Dataset (Durrett and DeNero, 2013). The data set contains inflected forms for different languages and parts of speech, corresponding lemmas, and detailed inflection information, including mood, case, and tense. We conduct experiments on the German and Finnish verb datasets and further reduce the size of the latter by considering present tense indicative verb forms in active voice only. Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al.", "startOffset": 61, "endOffset": 533}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al.", "startOffset": 93, "endOffset": 138}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al. (2016) because we focus on lemmatization, not inflection generation, as mentioned.", "startOffset": 93, "endOffset": 165}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al. (2016) because we focus on lemmatization, not inflection generation, as mentioned. We do so because this produces less overhead \u2014 e.g., Faruqui et al. (2016) train 27 different systems for German verbs, one for each inflection type \u2014 and there is a priori not much difference in whether we transform an inflected form to a lemma or vice versa.", "startOffset": 93, "endOffset": 316}, {"referenceID": 5, "context": "\u2022 enc-dec: Encoder-decoder models using recurrent neural networks (RNNs) for Seq2Seq tasks were introduced by Cho et al. (2014) and Sutskever et al.", "startOffset": 110, "endOffset": 128}, {"referenceID": 5, "context": "\u2022 enc-dec: Encoder-decoder models using recurrent neural networks (RNNs) for Seq2Seq tasks were introduced by Cho et al. (2014) and Sutskever et al. (2014). The encoder reads an input ~x and", "startOffset": 110, "endOffset": 156}, {"referenceID": 1, "context": "\u2022 attn-enc-dec: We explore the attention-based encoder-decoder model proposed by Bahdanau et al. (2014) (Figure 1).", "startOffset": 81, "endOffset": 104}, {"referenceID": 1, "context": "Illustration from Bahdanau et al. (2014).", "startOffset": 18, "endOffset": 41}, {"referenceID": 13, "context": "\u2022 morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder.", "startOffset": 15, "endOffset": 37}, {"referenceID": 13, "context": "\u2022 morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder. This approach is motivated by the observation that input and output are usually very similar in problems such as morphological inflection. Similar ideas have been proposed in Gu et al. (2016) in their so-called \u201cCopyNet\u201d encoder-decoder model (which they apply to text summarization) that allows for portions of the input sequence to be simply copied to the output sequence, without modifications.", "startOffset": 15, "endOffset": 368}, {"referenceID": 13, "context": "For the tested neural models, we follow the same overall approach as Faruqui et al. (2016): we perform decoding and evaluation of the test data using an ensemble of k = 5 independently trained models in order to deal with the non-convex nature of the optimization problem of neural networks and the risk of", "startOffset": 69, "endOffset": 91}, {"referenceID": 7, "context": "running into a local optimum (Collobert et al., 2011).", "startOffset": 29, "endOffset": 53}, {"referenceID": 4, "context": "(2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005).", "startOffset": 112, "endOffset": 140}, {"referenceID": 17, "context": "Conditional random fields (CRFs) were introduced by Lafferty et al. (2001) and have been a major workhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s.", "startOffset": 52, "endOffset": 75}, {"referenceID": 17, "context": "Conditional random fields (CRFs) were introduced by Lafferty et al. (2001) and have been a major workhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s. Unfortunately, training and decoding time depend polynomially on the tag set size and exponentially on the order of the CRF. Here, order refers to the dependencies on the label side. This makes higher-order CRFs impractical for large training data sizes, which is the reason why virtually only first-order (linear chain) CRFs were used until recently. M\u00fcller et al. (2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005).", "startOffset": 52, "endOffset": 591}, {"referenceID": 4, "context": "(2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005). PCRFs require much shorter runtime and are thus able to make use of higher orders. Higher orders, in turn, have been shown to be highly beneficial for coarse and fine-grained part-of-speech tagging, outperforming first-order models. For our tasks, we have adapted the implementation from M\u00fcller et al. (2013) \u2014 originally designed for sequence labeling \u2014 to general monotone Seq2Seq tasks.", "startOffset": 113, "endOffset": 451}, {"referenceID": 16, "context": "More sophisticated \u2018joint\u2019 approaches (Jiampojamarn et al., 2010) are considerably more computationally expensive.", "startOffset": 38, "endOffset": 65}, {"referenceID": 10, "context": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.", "startOffset": 126, "endOffset": 179}, {"referenceID": 22, "context": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.", "startOffset": 126, "endOffset": 179}, {"referenceID": 11, "context": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.", "startOffset": 126, "endOffset": 179}, {"referenceID": 2, "context": "Sequitur (Bisani and Ney, 2008) is a \u2018joint\u2019 model for Seq2Seq in the sense of the classic distinction between joint and discriminative models.", "startOffset": 9, "endOffset": 31}, {"referenceID": 16, "context": "DirecTL+ (Jiampojamarn et al., 2010) is a discriminative model for monotone Seq2Seq that integrates joint n-gram features.", "startOffset": 9, "endOffset": 36}, {"referenceID": 23, "context": "Phonetisaurus (Novak et al., 2012) implements a weighted finite state transducer (WFST) to align input and output tokens.", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "Finnish vowel harmony makes a vowel control other vowels in the word, potentially across multiple syllables; see Faruqui et al. (2016) for more detailed explanation.", "startOffset": 113, "endOffset": 135}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences.", "startOffset": 88, "endOffset": 111}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs.", "startOffset": 88, "endOffset": 857}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs. The task-specific extensions to the encoder-decoder proposed by Faruqui et al. (2016) have been shown to produce mostly bad results in our settings.", "startOffset": 88, "endOffset": 1101}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs. The task-specific extensions to the encoder-decoder proposed by Faruqui et al. (2016) have been shown to produce mostly bad results in our settings. This is particularly surprising for the OCR data, for which input and output sequences are usually very similar, so that we had expected that re-feeding the input to the decoder should be equally beneficial in that domain. As discussed, one explanation might be that OCR, or spelling correction generally, putatively exhibits few long-range dependencies. This might explain why the morph-trans approach works quite well and competitive in morphological analysis tasks, as re-confirmed in our experiments. Thus, long-range dependencies might actually be a more crucial aspect for the performance of the model presented by Faruqui et al. (2016) than the similarity between input and output sequence.", "startOffset": 88, "endOffset": 1807}], "year": 2016, "abstractText": "We analyze the performance of encoder-decoder neural models and compare them with wellknown established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}