{"id": "1610.08500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees", "abstract": "we formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. more specifically,... we introduce a modeling formalism in which both a human personality and an autonomy protocol can issue commands to a robot towards performing a certain task. these commands are blended into a joint input to the robot. the autonomy protocol is synthesized using an semantic abstraction of possible appropriate human commands accounting rules for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. the synthesis is designed to firmly ensure that explaining the resulting robot behavior satisfies given safety and performance specifications, e. g., in temporal logic. our solution is generally based on nonlinear automatic programming and we address the inherent scalability issue by presenting alternative methods. we might assess the feasibility and the scalability of the approach by an experimental evaluation.", "histories": [["v1", "Wed, 26 Oct 2016 19:49:09 GMT  (1170kb,D)", "http://arxiv.org/abs/1610.08500v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["nils jansen", "murat cubuktepe", "ufuk topcu"], "accepted": false, "id": "1610.08500"}, "pdf": {"name": "1610.08500.pdf", "metadata": {"source": "CRF", "title": "Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees", "authors": ["Nils Jansen", "Murat Cubuktepe", "Ufuk Topcu"], "emails": ["njansen@utexas.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nWe study the problem of shared control, where a robot shall accomplish a task according to a human operator\u2019s goals and given specifications addressing safety or performance. Such scenarios are for instance found in remotely operated semi-autonomous wheelchairs [11]. In a nutshell, the human has a certain action in mind and issues a command. Simultaneously, an autonomy protocol provides\u2014based on the available information\u2014another command. These commands are blended\u2014also referred to as arbitrated\u2014and deployed to the robot.\nEarlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered. In particular, having the human as an integral factor in this scenario, correctness needs to be treated in an appropriate way as a human might not be able to comprehend factors of a system and\u2014in the extremal case\u2014can drive a system into inevitable failure.\nThere are several things to discuss. First, a human might not be sure about which command to take, depending on the scenario or factors like fatigue or incomprehensibility of the problem. We account for uncertainties in human decisions by introducing randomness to choices. Moreover, a means of actually interpreting a command is needed in form of a user interface, e. g., a brain-computer interface; the usually imperfect interpretation adds to the randomness. We call a formal interpretation of the human\u2019s commands the human strategy (this concept will be explained later).\nAs many formal system models are inherently stochastic, our natural formal model for robot actions inside an environment is a Markov decision process (MDP) where\nAll authors are with the University of Texas at Austin, Austin, TX 78751, USA, njansen@utexas.edu\ndeterministic action choices induce probability distributions over system states. Randomness in the choice of actions, like in the human strategy, is directly carried over to these probabilities when resolving nondeterminism. For MDPs, quantitative properties like \u201cthe probability to reach a bad state is lower than 0.01\u201d or \u201cthe cost of reaching a goal is below a given threshold\u201d can be formally verified. If a set of such specifications is satisfied for the human strategy and the MDP, the task can be carried out safely and with good performance.\nGiven that the human strategy induces certain critical actions with a high probability, one or more specifications might be refuted. In this case, the autonomy should provide an alternative strategy that\u2014when blended with the human strategy\u2014satisfies the specifications without discarding too much of the human\u2019s choices. As in [8], the blending puts weight on either the human\u2019s or the autonomy protocol\u2019s choices depending on factors such as the confidence of the human or the level of information the autonomy protocol has at its disposal.\nThe question is now how such a human strategy can be obtained. It seems unrealistic that a human can comprehend an MDP modeling a realistic scenario in the first place; primarily due the possibly very large size of the state space. Moreover, a human might not be good at making sense of probabilities or cost of visiting certain states at all. We employ learning techniques to collect data about typical human behavior. This can, for instance, be performed within a simulation environment. In our case study, we model a typical shared control scenario based on a wheelchair [11] where a human user and an autonomy protocol share the control responsibility. Having a human user solving a task, we compute strategies from the obtained data using inverse reinforcement learning [16], [1]. Thereby, we can give guarantees on how good the obtained strategy approximates the actual intends of the user.\nThe design of the autonomy protocol is the main concern of this paper. We define the underlying problem as a nonlinear optimization problem and propose a technique to address the consequent scalability issues by reducing the problem to a linear optimization problem. After an autonomy protocol is synthesized, guarantees on safety and performance can be given assuming that the user behaves according to the human strategy obtained beforehand. The main contribution is a formal framework for the problem of shared autonomy together with thorough discussions on formal verification, experiments, and current pitfalls. A summary of the approaches and an outline are given in Section II.\nar X\niv :1\n61 0.\n08 50\n0v 1\n[ cs\n.R O\n] 2\n6 O\nct 2\n01 6\nShared control has attracted considerable attention recently. We only overview some recent approaches into context with our results. First, Dragan and Srinivasa discussed strategy blending for shared control in [8], [7]. There, the focus was on the prediction of human goals. Combining these approaches, e. g., by inferring formal safety or performance specifications by prediction of human goals, is an interesting direction for future work. Iturrate et al. presented shared control using feedback based on electroencephalography (a method to record electrical activity of the brain) [13], where a robot is partly controlled via error signals from a brain-computer interface. In [19], Trautman proposes to treat shared control broadly as a random process where different components are modeled by their joint probability distributions. As in our approach, randomness naturally prevents strange effects of blending: Consider actions \u201cup\u201d and \u201cdown\u201d to be blended with equally distributed weight without having means to actually evaluating these weights. Finally, in [10] a synthesis method switches authority between a human operator and the autonomy such that satisfaction of linear temporal logic constraints can be ensured."}, {"heading": "II. SHARED CONTROL", "text": "Consider first Fig. 1 which recalls the general framework for shared autonomy with blending of commands; additionally we have a set of specifications, a formal model for robot behavior, and a blending function. In detail, a robot is to take care of a certain task. For instance, it shall move to a certain landmark. This task is subject to certain performance and safety considerations, e. g., it is not safe to take the shortest route because there are too many obstacles. These considerations are expressed by a set of specifications \u03d51, . . . ,\u03d5n. The possible behaviors of the robot inside an environment are given by a Markov decision process (MDP) Mr. Having MDPs gives rise to choices of certain actions to perform and to randomness in the environment: A chosen path might induce a high probability to achieve the goal while with a low probability, the robot might slip and therefore fail to complete the task.\nNow, in particular, a human user issues a set of commands for the robot to perform. We assume that the commands issued by the human are consistent with an underlying random-\nized strategy \u03c3h for the MDP Mr. Put differently, at design time we compute an abstract strategy \u03c3h of which the set of human commands is one realization. This modeling way allows to account for a variety of imperfections. Although it is not directly issued by a human, we call this strategy the human strategy. Due to possible human incomprehensibility or lack of detailed information, this leads to the fact that the strategy might not satisfy the requirements.\nNow, an autonomy protocol is to be designed such that it provides an alternative strategy \u03c3a, the autonomous strategy. The two strategies are then blended\u2014according to the given blending function b into a new strategy \u03c3ha which satisfies the specifications. The blending function reflects preference over either the decisions of the human or the autonomy protocol. We also ensure that the blended strategy deviates only minimally from the human strategy. At runtime we can then blend decisions of the human user with decisions based on the autonomous strategy. The resulting \u201cblended\u201d decisions are according to the blended strategy \u03c3ha, thereby ensuring satisfaction of the specifications. This procedure, while involving expensive computations at design time, is very efficient at runtime.\nSummarized, the problem we are addressing in this paper is then\u2014in addition to the proposed modeling of the scenario\u2014to synthesize the autonomy protocol in a way such that the resulting blended strategy meets all of the specifications while it only deviates from the human strategy as little as possible. We introduce all formal foundations that we need in Section III. The shared control synthesis problem with all needed formalisms is presented in Section IV as being a nonlinear optimization problem. Addressing scalability, we reduce the problem to a linear optimization problem in Section V. We indicate the feasibility and scalability of our techniques using data-based experiments in Section V and draw a short conclusion in Section VII."}, {"heading": "III. PRELIMINARIES", "text": "1) Models: A probability distribution over a finite or countably infinite set X is a function \u00b5 : X\u2192 [0,1]\u2286R with \u2211x\u2208X \u00b5(x) = \u00b5(X) = 1. The set of all distributions on X is denoted by Distr(X).\nDefinition 1 (MDP): A Markov decision process (MDP) M = (S,sI ,A,P) is a tuple with a set of states S, a unique initial state sI \u2208 S, a finite set A of actions, and a (partial) probabilistic transition function P : S\u00d7A\u2192 Distr(S). MDPs operate by means of nondeterministic choices of actions at each state, whose successors are then determined probabilistically with respect to the associated probability distribution. The enabled actions at state s \u2208 S are denoted by A(s) = {\u03b1 \u2208 A | \u2203\u00b5 \u2208 Distr(S).\u00b5 = P(s,\u03b1)}. To avoid deadlock states, we assume that |A(s)| \u2265 1 for all s \u2208 S. A cost function \u03c1 : S\u00d7A\u2192 R\u22650 for an MDP M adds cost to a transition (s,\u03b1) \u2208 S\u00d7A with \u03b1 \u2208 A(s). A path in an M is a finite (or infinite) sequence \u03c0 = s0\u03b10s1\u03b11 . . . with P(si,\u03b1,si+1) > 0 for all i \u2265 0. If |A(s)| = 1 for all s \u2208 S, all actions can be disregarded and the MDP M reduces to a discrete-time Markov chain (MC).\nThe unique probability measure PrD (\u03a0) for a set \u03a0 of paths of MC D can be defined by the usual cylinder set construction, the expected cost of a set \u03a0 of paths is denoted by ECD (\u03a0), see [2] for details. In order to define a probability measure and expected cost on MDPs, the nondeterministic choices of actions are resolved by socalled strategies. For practical reasons, we restrict ourselves to memoryless strategies, again refer to [2] for details.\nDefinition 2 (Strategy): A randomized strategy for an MDP M is a function \u03c3 : S\u2192Distr(A) such that \u03c3(s)(\u03b1)> 0 implies \u03b1 \u2208 A(s). A strategy with \u03c3(s)(\u03b1) = 1 for \u03b1 \u2208 A and \u03c3(\u03b2 ) = 0 for all \u03b2 \u2208 A\\{\u03b1} is called deterministic. The set of all strategies over M is denoted by SchedM . Resolving all nondeterminism for an MDP M with a strategy \u03c3 \u2208 SchedM yields an induced Markov chain M \u03c3 . Intuitively, the random choices of actions from \u03c3 are transferred to the transition probabilities in M \u03c3 .\nDefinition 3 (Induced MC): Let MDP M = (S,sI ,A,P) and strategy \u03c3 \u2208 SchedM . The MC induced by M and \u03c3 is M \u03c3 = (S,sI ,A,P\u03c3 ) where\nP\u03c3 (s,s\u2032) = \u2211 \u03b1\u2208A(s) \u03c3(s)(\u03b1) \u00b7P(s,\u03b1)(s\u2032) for all s,s\u2032 \u2208 S .\n2) Specifications: A quantitative reachability property P\u2264\u03bb (\u2666T ) with upper probability threshold \u03bb \u2208 [0,1]\u2286Q and target set T \u2286 S constrains the probability to reach T from sI in M to be at most \u03bb . Expected cost properties E\u2264\u03ba(\u2666G) impose an upper bound \u03ba \u2208Q on the expected cost to reach goal states G \u2286 S. Intuitively, bad states T shall only be reached with probability \u03bb (safety specification) while the expected cost for reaching goal states G has to be below \u03ba (performance specification). Probability and expected cost to reach T from sI are denoted by Pr(\u2666T ) and EC(\u2666T ), respectively. Hence, PrD (\u2666T ) \u2264 \u03bb and ECD (\u2666G) \u2264 \u03ba express that the properties P\u2264\u03bb (\u2666T ) and E\u2264\u03ba(\u2666G) are satisfied by MC D . These concepts are analogous for lower bounds on the probability. We also use until properties of the form Pr\u2265\u03bb (\u00acT U G) expressing that the probability of reaching G while not reaching T beforehand is at least \u03bb .\nAn MDP M satisfies both safety specification \u03c6 and performance specification \u03c8 , iff for all strategies \u03c3 \u2208 SchedM it holds that the induced MC M \u03c3 satisfies \u03c6 and \u03c8 , i.e., M \u03c3 |= \u03c6 and M \u03c3 |= \u03c8 . If several performance or safety specifications \u03d51, . . . ,\u03d5n are given MDP M , the simultaneous satisfaction for all strategies, denoted by M |= \u03d51, . . . ,\u03d5n, can be formally verified for an MDP using multi-objective model checking [9].\nHere, we are interested in the synthesis problem, where the aim is to find one particular strategy \u03c3 for which the specifications are satisfied. If for \u03d51, . . . ,\u03d5n and strategy \u03c3 it holds that M \u03c3 |= \u03d51, . . . ,\u03d5n, then \u03c3 is said to admit the specifications, also denoted by \u03c3 |= \u03d51, . . . ,\u03d5n.\nExample 1: Consider Fig. 2(a) depcting MDP M with initial state s0, where states s0 and s1 have choices between actions a or b and c or d, respectively. For instance, action a induces a probabilistic choice between s1 and s3 with probabilities 0.6 and 0.4. The self loops at s2,s3 and s4 indicate looping back with probability one for each action.\nAssume now, a safety specification is given by \u03c6 = P\u22640.21(\u2666s2). The specification is violated for M , as the deterministic strategy \u03c31 \u2208 SchedM with \u03c31(s1)(\u03b1) = 1 and \u03c31(s1)(c) = 1 induces a probability of reaching s2 of 0.36, see the induced MC in Fig. 2(b). For the randomized strategy \u03c3unif \u2208 SchedM with \u03c3unif(s0)(\u03b1) = \u03c3unif(s0)(b) = 0.5 and \u03c3unif(s1)(c) = \u03c3unif(s1)(d) = 0.5, which chooses between all actions uniformly, the specification is also violated: The probability of reaching s2 is 0.25, hence \u03c32 6|= \u03c6 . However, for the deterministic strategy \u03c3safe \u2208 SchedM with \u03c3safe(s0)(b) = 1 and schedsafe(s1)(d) = 1 the probability is 0.16, thus \u03c3safe |= \u03c6 . Note that \u03c3safe minimizes the probability of reaching s2 while \u03c31 maximizes this probability."}, {"heading": "IV. SYNTHESIZING SHARED CONTROL PROTOCOLS", "text": "In this section we describe our formal approach to synthesize a shared control protocol in presence of randomization. We start by formalizing the concepts of blending and strategy perturbation. Afterwards we formulate the general problem and show that the solution to the synthesis problem is correct.\nExample 2: Consider Fig. 3, where a room to navigate in is abstracted into a grid. We will use this as our ongoing example. A wheelchair as in [11] is to be steered from the lower left corner of the grid to the exit on the upper right corner of the grid. There is also an autonomous robotic vacuum cleaner moving around the room; the goal is for the wheelchair to reach the exit without crashing into the vacuum cleaner. We now assume that the vacuum cleaner moves according to probabilities that are fixed according to evidence gathered beforehand; these probabilities are unknown or incomprehensible to the human user. To improve the safety of the wheelchair, it is equipped with an autonomy protocol that is to improve decisions of the human or even overwrite them\nin case of safety hazards. For the design of the autonomy protocol, the evidence data about the cleaner is present.\nNow an obvious strategy to move for the wheelchair, not taking into account the vacuum cleaner, is depicted by the red solid line in Fig. 3(a). As indicated in Fig. 3(b), the strategy proposed by the human is unsafe because there is a high probability to collide with the obstacle. The autonomy protocol computes a safe strategy, indicated by the solid line in Fig. 3(b). As this strategy deviates highly from the human strategy, the dashed line indicates a still safe enough alternative which is a compromise or\u2014in our terminology\u2014 a blending between the two strategies. We assume in the following that possible behaviors of the robot inside the environment are modeled by MDP Mr = (S,sI ,A,P). The human strategy is given as randomized strategy \u03c3h for Mr. We explain how to obtain this strategy in Section VI. Specifications are \u03d51, . . . ,\u03d5n being either safety properties P\u2264\u03bb (\u2666T ) or performance properties E\u2264\u03ba(\u2666T )."}, {"heading": "A. Strategy blending", "text": "Given two strategies, they are to be blended into a new strategy favoring decisions of one or the other in each state of the MDP. In our setting, the human strategy \u03c3h \u2208 SchedMr is blended with the autonomous strategy \u03c3a \u2208 SchedMr by means of an arbitrary blending function. In [8] it is argued that blending intuitively reflects the confidence in how good the autonomy protocol is able to assist with respect to the human user\u2019s goals. In addition, factors probably unknown or incomprehensible for the human such as safety or performance optimization also should be reflected by such a function.\nPut differently, possible actions of the user should be assigned low confidence by the blending function, if he cannot be trusted to make the right decisions. For instance, recall Example 2. At cells of the grid where with a very high probability the wheelchair might collide with the vacuum cleaner, it makes sense to assign a high confidence in the autonomy protocol\u2019s decisions because not all safety-relevant information is present for the human.\nIn order to enable formal reasoning together with such a function we instantiate the blending with a state-dependent function which at each state of an MDP weighs the confidence in both the human\u2019s and the autonomy\u2019s decisions. A more fine-grained instantiation might incorporate not only the current state of the MDP but also the strategies of both human and autonomy or history of a current run of the system. Such a formalism is called linear blending and is used in what follows. In [19], additional notions of blending are discussed.\nDefinition 4 (Linear blending): Given an MDP Mr = (S,sI ,A,P), two strategies \u03c3h,\u03c3a \u2208 SchedMr , and a blending function b : S\u2192 [0,1], the blended strategy \u03c3ha \u2208 SchedMr for all states s \u2208 S, and actions \u03b1 \u2208 A is\n\u03c3ha(s)(\u03b1) = b(s) \u00b7\u03c3h(s)(\u03b1)+(1\u2212b(s)) \u00b7\u03c3a(s)(\u03b1) . Note that the blended strategy \u03c3ha is a well-defined randomized strategy. For each s \u2208 S, the value b(s) represents the\nconfidence in the human\u2019s decisions at this state, i. e., the \u201cweight\u201d of \u03c3h at s.\nComing back to Example 2, the critical cells of the grid correspond to certain states of the MDP Mr; at these states a very low confidence in the human\u2019s decisions should be assigned. For instance at such a state s \u2208 S we might have b(s) = 0.1 leading to the fact that all randomized choices of the human strategy are scaled down by this factor. Choices of the autonomous strategy are only scaled down by factor 0.9. The addition of these scaled choices then gives a new strategy highly favoring the autonomy\u2019s decisions."}, {"heading": "B. Perturbation of strategies", "text": "As mentioned before, we want to ensure that the blended strategy deviates minimally from the human strategy. To now measure such a deviation, we introduce the concept of perturbation which was\u2014on a complexity theoretic level\u2014for instance investigated in [5]. Here, we introduce an additive perturbation for a (randomized) strategy, incrementing or decrementing probabilities of action choices such that a welldefined distribution over actions is maintained.\nDefinition 5 (Strategy perturbation): Given MDP M and strategy \u03c3 \u2208 SchedM , an (additive) perturbation \u03b4 is a function \u03b4 : S\u00d7A\u2192 [\u22121,1] with\n\u2211 \u03b1\u2208A \u03b4 (s,\u03b1) = 0 for all s \u2208 S .\nThe value \u03b4 (s,\u03b1) is called the perturbation value at state s for action \u03b1 . Overloading the notation, the perturbed strategy \u03b4 (\u03c3) is given by\n\u03b4 (\u03c3)(s,\u03b1) = \u03c3(s)(\u03b1)+\u03b4 (s,\u03b1) for all s \u2208 S and \u03b1 \u2208 A ."}, {"heading": "C. Design of the autonomy protocol", "text": "For the formal problem, we are given blending function b, specifications \u03d51, . . . ,\u03d5n, MDP Mr, and human strategy \u03c3h \u2208Mr. We assume that \u03c3h does not satisfy all of the specifications, i. e., \u03c3h 6|= \u03d51, . . . ,\u03d5n. The autonomy protocol provides the autonomous strategy \u03c3a \u2208 SchedMr . According to b, the strategies \u03c3a and \u03c3h are blended into strategy \u03c3ha, see Definition 4, i. e., \u03c3ha(s,\u03b1) = b(s) \u00b7\u03c3a(s,\u03b1)+(1\u2212b(s)) \u00b7 \u03c3h(s,\u03b1). The shared control synthesis problem is to design the autonomy protocol such that for the blended strategy \u03c3ha it holds \u03c3ha |=\u03d51, . . . ,\u03d5n, while minimally deviating from \u03c3h. The deviation from \u03c3h is captured by finding a perturbation \u03b4 as in Definition 5, where, e. g., the infinity norm of all perturbation values is minimal. Our problem involves the explicit computation of a randomized strategy and the induced probabilities, which is inherently nonlinear because the corresponding variables need to be multiplied. Therefore, the canonical formulation is given by a nonlinear optimization program (NLP). We first assume that the only specification is a quantitative reachability property \u03d5 = P\u2264\u03bb (\u2666T ), then we describe how more properties can be included. The program has to encompass defining the autonomous strategy \u03c3a, the perturbation \u03b4 of the human strategy, the blended strategy \u03c3ha, and the probability of reaching the set of target states T \u2286 S.\nWe introduce the following specific set Var of variables: \u2022 \u03c3 s,\u03b1a ,\u03c3 s,\u03b1ha \u2208 [0,1] for each s \u2208 S and \u03b1 \u2208 A define the autonomous strategy \u03c3a and the blended strategy \u03c3ha. \u2022 \u03b4 s,\u03b1 \u2208 [\u22121,1] for each s \u2208 S and \u03b1 \u2208 A are the perturbation variables for \u03c3h and \u03c3ha. \u2022 ps \u2208 [0,1] for each s \u2208 S are assigned the probability of reaching T \u2286 S from state s under strategy \u03c3ha.\nUsing these variables, the NLP reads as follows:\nminimize max{|\u03b4 s\u03b1 | | s \u2208 S,\u03b1 \u2208 A} (1) subject to psI \u2264 \u03bb (2) \u2200s \u2208 T. ps = 1 (3) \u2200s \u2208 S. \u2211\n\u03b1\u2208A \u03c3 s,\u03b1a = \u2211 \u03b1\u2208A \u03c3 s,\u03b1ha = 1 (4)\n\u2200s \u2208 S.\u2200\u03b1 \u2208 A. \u03c3 s,\u03b1ha = \u03c3h(s)(\u03b1)+\u03b4 s,\u03b1 (5) \u2200s \u2208 S. \u2211\n\u03b1\u2208A \u03b4 s,\u03b1 = 0 (6)\n\u2200s \u2208 S.\u2200\u03b1 \u2208 A. \u03c3 s,\u03b1ha = b(s) \u00b7\u03c3h(s)(\u03b1)+(1\u2212b(s)) \u00b7\u03c3 s,\u03b1a (7)\n\u2200s \u2208 S. ps = \u2211 \u03b1\u2208A \u03c3 s,\u03b1ha \u00b7 \u2211 s\u2032\u2208S P(s,\u03b1)(s\u2032) \u00b7 ps\u2032 (8)\nThe NLP works as follows. First, the infinity norm of all perturbation variables is minimized (by minimizing the maximum of all perturbation variables) (1). The probability assigned to the initial state sI \u2208 S has to be smaller than or equal to \u03bb to satisfy \u03d5 = P\u2264\u03bb (\u2666T ) (2). For all target states T \u2286 S, the probability of the corresponding probability variables is assigned one (3). Now, to have well-defined strategies \u03c3a and \u03c3ha, we ensure that the assigned values of the corresponding strategy variables at each state sum up to one (4). The perturbation \u03b4 of the human strategy \u03c3h resulting in the strategy \u03c3ha as in Definition 5 is computed using the perturbation variables (5); in order for the perturbation to be well-defined, the variables have to sum up to zero at each state (6). The blending of \u03c3a and \u03c3ha with respect to b as in Definition 4 is defined in (7). Finally, the probability to reach T \u2286 S from each s \u2208 S is computed in (8), defining a non-linear equation system, where action probabilities, given by the induced strategy \u03c3ha, are multiplied by probability variables for all possible successors.\nNote that this nonlinear program is in fact bilinear due to multiplying the strategy variables \u03c3 s,\u03b1ha with the probability variables ps\u2032 (8). The number of constraints is governed by the number of state and action pairs, i. e., the size of the problem is in O(|Sr| \u00b7 |A|).\nAn assignment of real-valued variables is a function \u03bd : Var \u2192 R; it is satisfying for a set of (in)equations, if each one evaluates to true. A satisfying assignment \u03bd\u2217 is minimizing with respect to objective o if for \u03bd\u2217(o) \u2208R there is no other assignment \u03bd \u2032 with \u03bd \u2032(o) < \u03bd\u2217(o). Using these notions, we state the correctness of the NLP in (1) \u2013 (8).\nTheorem 1 (Soundness and completeness): The NLP is sound in the sense that each minimizing assignment induces a solution to the shared control synthesis problem. It is complete in the sense that for each solution to the shared\ncontrol synthesis there is a minimizing assignment of the NLP.\nSoundness tells that each satisfying assignment of the variables corresponds to strategies \u03c3a and \u03c3ha as well as the perturbation \u03b4 as defined above. Moreover, any optimal solution induces a perturbation minimally deviating from the human strategy \u03c3h. Completeness means that all possible solutions of the shared control synthesis problem can be encoded by this NLP. Unsatisfiability means that no such solution exists; the problem is infeasible."}, {"heading": "D. Additional specifications", "text": "We now explain how the NLP can be extended for further specifications. Assume in addition to \u03d5 = P\u2264\u03bb (\u2666T ), another reachability property \u03d5 \u2032 = P\u2264\u03bb \u2032(\u2666T \u2032) with T \u2032 6= T is given. We add another set of probability variables p\u2032s for each state s\u2208 S; (2) is copied for p\u2032sI and \u03bb \u2032, (3) is defined for all states s \u2208 T \u222aT \u2032 and (8) is copied for all p\u2032s, thereby computing the probability of reaching T \u2032 under \u03c3ha for all states.\nTo handle an expected cost property E\u2264\u03ba(\u2666G) for G \u2286 S, we use variables rs being assigned the expected cost for reaching G for all s \u2208 S. We add the following equations:\nrsI \u2264 \u03ba (9) \u2200s \u2208 G. rs = 0 (10) \u2200s \u2208 S. rs = \u2211\n\u03b1\u2208A\n( \u03c3 s,\u03b1ha \u00b7 r(s,\u03b1)+ \u2211\ns\u2032\u2208S P(s,\u03b1)(s\u2032) \u00b7 rs\u2032 ) (11)\nFirst, the expected cost of reaching G is smaller than or equal to \u03ba at sI (9). Goal state are assigned cost zero (10), otherwise infinite cost is collected at absorbing states. Finally, the expected cost for all other states is computed by (11) where according to the blended strategy \u03c3ha the cost of each action is added to the expected cost of the successors. An important insight is that if all specifications are expected reward properties, the program is no longer nonlinear but a linear program (LP), as there is no multiplication of variables."}, {"heading": "E. Generalized blending", "text": "If the problem is not feasible for the given blending function, optionally the autonomy protocol can try to compute a new function b : S\u2192 [0,1] for which the altered problem is feasible. We call this procedure generalized blending. The idea is that computing this function gives the designer of the protocol insight on where more confidence needs to be placed into the autonomy or, vice versa, where the human cannot be trusted to satisfy the given specifications.\nComputing this new function is achieved by nearly the same NLP as for a fixed blending function while adding variables bs for each state s \u2208 S, defining the new blending function by b(s) = bs. We substitute Equation 7 by\n\u2200s \u2208 S.\u2200\u03b1 \u2208 A. \u03c3 s,\u03b1ha = bs \u00b7\u03c3h(s)(\u03b1)+(1\u2212bs) \u00b7\u03c3 s,\u03b1a . (12)\nA satisfying assignment for the resulting nonlinear program induces a suitable blending function b : S\u2192 [0,1] in addition to the strategies. If this problem is also infeasible, there is no strategy that satisfies the given specifications for MDP Mr.\nCorollary 1: If there is no solution for the NLP given by Equations 1 \u2013 12, there is no strategy \u03c3 \u2208 SchedMr such that \u03c3 |= \u03d51, . . . ,\u03d5n. As there are no restrictions on the blending function, this corollary trivially holds: Consider for instance b with b(s) = 0 for each s\u2208 S. This function disregards the human strategy which may be perturbed to each other strategy \u03c3a = \u03c3ha.\nExample 3: Reconsider the MDP M from Example 1 with specification \u03d5 = P\u22640.21(\u2666{s2}) and the randomized strategy \u03c3unif which takes each action uniformly distributed. As we saw, \u03c3unif 6|= \u03d5 . We choose this strategy as the human strategy \u03c3h = \u03c3unif and Mr = M as the robot MDP. For a blending function bh putting high confidence in the human, e. g., if bh(s)\u2265 0.6 for all s \u2208 S, the problem is infeasible.\nIn Table I we display results putting medium (b1), low (b2), or no confidence (b3) in the human at s0 and s1. We list the assignments for the resulting strategies \u03c3a and \u03c3ha as well as the probability Prha = PrM \u03c3ah r\ns0 (\u2666T ) to reach s2 under the blended strategy \u03c3ha. The results were obtained using the NLP solver IPOPT [4].\nWe observe that for decreasing confidence in the human decisions, the autonomous strategy has higher probabilities for actions a and c which are the \u201cbad\u201d actions here. That means that\u2014if there is a higher confidence in the autonomy\u2014solutions farer away from the optimum are good enough. The maximal deviation from the human strategy is 0.21. Generalized blending with maximizing over the confidence in the human\u2019s decisions at all states s \u2208 S yields bh(s) = 0.582, i. e., we compute the highest possible confidence in the human\u2019s decisions where the problem is still feasible under the given human strategy."}, {"heading": "V. COMPUTATIONALLY TRACTABLE APPROACH", "text": "The nonlinear programming approach presented in the previous section gives a rigorous method to solve the shared control synthesis problem and serves as mathematically concise definition of the problem. However, NLPs are known to have severe restrictions in terms of scalability and suffer from numerical instabilities. The crucial point to an efficient solution is circumventing the expensive computation of optimal randomized strategies and reducing the number of variables. We propose a heuristic solution which enables to use linear programming (LP) while ensuring soundness.\nWe utilize a technique referred to as model repair. Intuitively, an erroneous model is changed such that it satisfies certain specifications. In particular, given a Markov chain D\nand a specification \u03d5 that is violated by D , a repair of D is an automated method that transforms it to new MC D \u2032 such that \u03d5 is satisfied for D \u2032. Transforming refers to changing probabilities or cost while regarding certain side constraints such as keeping the original graph structure.\nIn [3], the first approach to automatically repair an MC model was presented as an NLP. Simulation-based algorithms were investigated in [6]. A heuristic but very scalable technique called local repair was proposed in [17]. This approach greedily changes the probabilities or cost of the original MC until a property is satisfied. An upper bound \u03b4r on changes of probabilities or cost can be specified; correctness and completeness can be given in the sense that if a repair with respect to \u03b4r exists, it will be obtained.\nTake now the MC D\u03c3hr which is induced by the robot MDP Mr and the human strategy \u03c3h. We perform model repair such that the repaired MC D \u2032 = (S,sI ,P\u2032) satisfies the specifications \u03d51, . . . ,\u03d5n. The question is now, how from the repaired MC D \u2032, the strategy \u03c3 \u2032 \u2208 SchedMr can be extracted. More precisely, we need \u03c3 \u2032 inducing exactly D \u2032, i. e., D\u03c3 \u2032r = D \u2032, when applied to MDP Mr.\nFirst, we need to make sure that the repaired MC is consistent with the original MDP such that a strategy \u03c3 \u2032 with D\u03c3 \u2032 r = D\n\u2032 actually exists. Therefore, we define the maximal and minimal possible transition probabilities Pmax and Pmin that can occur in any induced MC of MDP Mr:\nPmax(s,s\u2032) = max{Pr(s,\u03b1)(s\u2032) | \u03b1 \u2208 A} (13) for all s \u2208 S; Pmin is defined analogously. Now, the repair is performed such that in the resulting MC D \u2032 = (S,sI ,P\u2032) for all s,s\u2032 \u2208 S it holds that\nPmin(s,s\u2032)\u2264 P(s,s\u2032)\u2264 Pmax(s,s\u2032) . (14) While obtaining D \u2032, model checking needs to be performed intermediately to check if the specifications are satisfied; once they are, the algorithm terminates. In fact, for each state s \u2208 S, the probability of satisfaction is computed. We assign variables mcs for all s \u2208 S with exactly this probability:\nmcs = Pr(s |= \u03d51, . . . ,\u03d5n) . (15) Now recall the NLP from the previous section, in particular Equation 8 which is the only nonlinear equation of the program. We replace each variable ps by the concrete model checking result mcs for each s \u2208 S:\nmcs = \u2211 \u03b1\u2208A \u03c3 s,\u03b1ha \u00b7 \u2211 s\u2032\u2208S P(s,\u03b1)(s\u2032) \u00b7mcs\u2032 . (16)\nAs (16) is affine in the variables \u03c3ah, the program resulting from replacing (8) by (16) is a linear program (LP). Moreover, (2) and (3) can be removed, reducing the number of constraints and variables. The LP gives a feasible solution to the shared control synthesis problem.\nLemma 1 (Correctness): The LP is sound in the sense that each minimizing assignment induces a solution to the shared control problem. The correctness is given by construction, as the specifications are satisfied for the blended strategy which is derived from\nthe repaired MC. However, the minimal deviation from the human strategy as in Equation 1 is dependent on the previous computation of probabilities for the blended strategy. Therefore, we actually compute an upper bound on the optimal solution. Let \u03b4 \u2217 be the minimal deviation possible for any given problem and \u03b4 be the minimal deviation obtained by the LP resulting from replacing (8) by (16). Let \u2016\u03b4\u2016\u221e and \u2016\u03b4 \u2217\u2016\u221e denote the infinity norms of both perturbations.\nCorollary 2: For the perturbations \u03b4 and \u03b4 \u2217 of \u03c3h it holds that \u2016\u03b4 \u2217\u2016\u221e \u2264 \u2016\u03b4\u2016\u221e. As we mentioned before, the local repair method can employ a bound \u03b4r on the maximal change of probabilities or cost in the model. If a repair exists for a given \u03b4r, the resulting deviation \u03b4 is then bounded by this \u03b4r."}, {"heading": "VI. CASE STUDY AND EXPERIMENTS", "text": "Defining a formal synthesis approach to the shared control scenario requires a precomputed estimation of a human user\u2019s intentions. As explained in the previous chapter, we account for inherent uncertainties by using a randomized strategy over possible actions to take. We discuss how such strategies may be obtained and report on benchmark results."}, {"heading": "A. Experimental setting", "text": "Our setting is the wheelchair scenario from Example 2 inside an interactive Python environment. The size of the grid is variable and an arbitrary number of stationary and randomly moving obstacles (the vacuum cleaner) can be defined. An agent (the wheelchair) is moved according to predefined (randomized) strategies or interactively by a human user.\nFrom this scenario, an MDP with states corresponding to the position of the agent and the obstacles is generated. Actions induce position changes of the agent. The safety specification ensures that the agent reaches a target cell without crashing into an obstacle with a certain high probability \u03bb \u2208 [0,1], formally P\u2265\u03bb (\u00accrash U target). We use the probabilistic model checker PRISM [15] for verification, in form of either a worst\u2013case analysis for each possible strategy or concretely for a specific strategy. The whole toolchain integrates the simulation environment with the approaches described in the previous sections. We use the NLP solver IPOPT [4] and the LP solver Gurobi [12]. To perform model repair for strategies, see Section V, we implemented the greedy method from [17] into our framework augmented by side constraints ensuring well-defined strategies."}, {"heading": "B. Data collection", "text": "We ask five participants to perform tests in the environment with the goal to move the agent to a target cell while never being in same cell as the moving obstacle. From the data obtained from each participant, an individual randomized human strategy \u03c3h for this participant can be obtained via Maximum Entropy Inverse Reinforcement Learning (MEIRL) [22]. Inverse reinforcement learning has\u2014for instance\u2014also been used in [14] to collect data about human behavior in a shared control scenario (though without any formal guarantees) or in [18] to distinguish human intents\nwith respect to different tasks. In our setting, each sample is one particular command of the participant, while we have to assume that command is actually made with the intent to satisfy the specification of safely reaching a target cell. For the resulting strategy, the probability of a possible deviation from the actual intend can be bounded with respect to the number of samples using Hoeffding\u2019s inequality, see [21] for details. On the other hand, we can determine the number of samples needed to get a reasonable approximation of typical behavior.\nThe concrete probabilities of possible deviation depend on O(exp(\u2212n\u03b5)), where n is the number of samples and \u03b5 is the desired upper bound on the deviation between the true probability of satisfying the specification and the average obtained by the sampled data. Here, in order to ensure an upper bound \u03b5 = 0.1 with probability 0.99, the required amount of samples is 265."}, {"heading": "C. Experiments", "text": "The work flow of the experiments is depicted in Figure 4. First off, we discuss sample data for one particular participant using a 8\u00d7 8 grid with one moving obstacle inducing an MDP of 2304 states. In the synthesis, we employ the model repair procedure as explained in Section V because the approach based on NLP is only feasible for very small examples. We design the blending function as follows: At states where the human strategy induces a high probability of crashing, we put low confidence in the human and vice versa. Using this function, the autonomous strategy \u03c3a is created and passed (together with the function) back to the environment. Note that the blended strategy \u03c3ah is ensured to satisfy the specification, see Lemma 1. Now, we let the same participant as before do test runs, but this time we blend the human commands with the (randomized) commands of the autonomous strategy \u03c3a. Then the actual action of the agent is determined stochastically. We obtain the following results. Our safety specification is instantiated with \u03bb = 0.7, ensuring that the target is safely reached with at least probability 0.7. The human strategy \u03c3h has probability 0.546, violating the specification. With the aforementioned blending function we compute \u03c3a which induces probability 0.906. Blending these two strategies into \u03c3ah yields a probability of 0.747. When testing the synthesized autonomy protocol for the individual participant, we observe that his choices are mostly corrected if intentionally bad decisions are made. Also, simulating the blended strategy leeds to the expected result that the agent\ndoes not crash in roughly 70% of the cases. To make the behavior of the strategies more accessible, consider Figure 5. For each \u03c3a, \u03c3ah, and \u03c3h we indicate for each cell of the grid the worst-case probability to safely reach the target. This probability depends on the current position of the obstacle, which is again probabilistic. The darker the color, the higher the probability; thereby black indicates a probability of 1 to reach the target. We observe that the human\u2019s decisions are rather risky even near the target, while for the blended strategy\u2014once the agent is near the target\u2014 there is a very high probability of reaching it safely. This representation also shows that with our approach the blended strategy improves the human strategy while not changing it too much. Specifically, the maximal deviation from the human strategy is 0.27, which is the result of the infinity norm as in Equation 1.\nTo finally assess the scalability of our approach, consider Table II. We generated MDPs for several grid sizes, number of obstacles, and human strategies. We list the number of reachable MDP states (states) and the number of transitions (trans.). We report on the time the synthesis process took (synth.), which is basically the time of solving the LP, and the total time including model checking times using PRISM (total) measured in seconds. To give an indication on the quality of the synthesis, we list the deviation from the human strategy (\u03b4\u221e). A memory out is indicated by \u201c\u2013MO\u2013\u201d. All experiments were conducted on a 2.3GHz machine with 8GB of RAM. Note that MDPs resulting from grid structures are very strongly connected, resulting in a large number of transitions. Thus, the encoding in the PRISM-language [15] is very large, rendering it a very hard problem. We observe that while the procedure is very efficient for models having a few thousand states and hundreds of thousands of transitions, its scalability is ultimately limited due to memory issues. In the future, we will utilize efficient symbolic data structures internal to PRISM. Moreover, we observe that for larger benchmarks the computation time is governed by the solving time of Gurobi."}, {"heading": "VII. CONCLUSION", "text": "We introduced a formal approach to synthesize autonomy protocols in a shared control setting with guarantees on quantitative safety and performance specifications. The practical usability of our approach was shown by means of databased experiments. Future work will concern experiments in\nrobotic scenarios and further improvement of the scalability."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Principles of Model Checking", "author": ["Christel Baier", "Joost-Pieter Katoen"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Model repair for probabilistic systems", "author": ["Ezio Bartocci", "Radu Grosu", "Panagiotis Katsaros", "CR Ramakrishnan", "Scott A Smolka"], "venue": "In TACAS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Large-scale nonlinear programming using IPOPT: An integrating framework for enterprisewide dynamic optimization", "author": ["Lorenz T. Biegler", "Victor M. Zavala"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Perturbation analysis in verification of discrete-time Markov chains", "author": ["Taolue Chen", "Yuan Feng", "David S. Rosenblum", "Guoxin Su"], "venue": "In CONCUR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Model repair for Markov decision processes. In TASE, pages 85\u201392", "author": ["Taolue Chen", "Ernst Moritz Hahn", "Tingting Han", "Marta Kwiatkowska", "Hongyang Qu", "Lijun Zhang"], "venue": "IEEE CS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Formalizing assistive teleoperation", "author": ["Anca D. Dragan", "Siddhartha S. Srinivasa"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A policy-blending formalism for shared control", "author": ["Anca D. Dragan", "Siddhartha S. Srinivasa"], "venue": "I. J. Robotic Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Multi-objective model checking of Markov decision processes", "author": ["Kousha Etessami", "Marta Z. Kwiatkowska", "Moshe Y. Vardi", "Mihalis Yannakakis"], "venue": "Logical Methods in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Synthesis of shared autonomy policies with temporal logic specifications", "author": ["Jie Fu", "Ufuk Topcu"], "venue": "IEEE Trans. Automation Science and Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "A brain-actuated wheelchair: Asynchronous and non-invasive brain-computer interfaces for continuous control of robots", "author": ["F. Gal\u00e1n", "M. Nuttin", "E. Lew", "P.W. Ferrez", "G. Vanacker", "J. Philips", "J. del R. Mill\u00e1n"], "venue": "Clinical Neurophysiology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Shared control of a robot using eeg-based feedback signals. In Proceedings of the 2Nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication, MLIS", "author": ["I\u00f1aki Iturrate", "Jason Omedes", "Luis Montesano"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Shared autonomy via hindsight optimization", "author": ["Shervin Javdani", "J Andrew Bagnell", "Siddhartha Srinivasa"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "PRISM 4.0: Verification of probabilistic real-time systems", "author": ["Marta Kwiatkowska", "Gethin Norman", "David Parker"], "venue": "In CAV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "In Icml,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "A greedy approach for the efficient repair of stochastic models", "author": ["Shashank Pathak", "Erika \u00c1brah\u00e1m", "Nils Jansen", "Armando Tacchella", "Joost-Pieter Katoen"], "venue": "In NFM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Modular inverse reinforcement learning for visuomotor behavior", "author": ["Constantin A Rothkopf", "Dana H Ballard"], "venue": "Biological cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Assistive planning in complex, dynamic environments: a probabilistic approach", "author": ["Pete Trautman"], "venue": "CoRR, abs/1506.06784,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A unified approach to 3 basic challenges in shared autonomy", "author": ["Pete Trautman"], "venue": "CoRR, abs/1508.01545,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["Brian D Ziebart"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "Such scenarios are for instance found in remotely operated semi-autonomous wheelchairs [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "As in [8], the blending puts weight on either the human\u2019s or the autonomy protocol\u2019s choices depending on factors such as the confidence of the human or the level of information the autonomy protocol has at its disposal.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "typical shared control scenario based on a wheelchair [11] where a human user and an autonomy protocol share the control responsibility.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "Having a human user solving a task, we compute strategies from the obtained data using inverse reinforcement learning [16], [1].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "Having a human user solving a task, we compute strategies from the obtained data using inverse reinforcement learning [16], [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "First, Dragan and Srinivasa discussed strategy blending for shared control in [8], [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "First, Dragan and Srinivasa discussed strategy blending for shared control in [8], [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "presented shared control using feedback based on electroencephalography (a method to record electrical activity of the brain) [13], where a robot is partly controlled via error signals from a brain-computer interface.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "In [19], Trautman proposes to treat shared control broadly as a random process where different components are modeled by their joint probability distributions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Finally, in [10] a synthesis method switches authority between a human operator and the autonomy such that satisfaction of linear temporal logic constraints can be ensured.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "1) Models: A probability distribution over a finite or countably infinite set X is a function \u03bc : X\u2192 [0,1]\u2286R with \u2211x\u2208X \u03bc(x) = \u03bc(X) = 1.", "startOffset": 101, "endOffset": 106}, {"referenceID": 1, "context": "The unique probability measure Pr (\u03a0) for a set \u03a0 of paths of MC D can be defined by the usual cylinder set construction, the expected cost of a set \u03a0 of paths is denoted by EC (\u03a0), see [2] for details.", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "For practical reasons, we restrict ourselves to memoryless strategies, again refer to [2] for details.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "2) Specifications: A quantitative reachability property P\u2264\u03bb (\u2666T ) with upper probability threshold \u03bb \u2208 [0,1]\u2286Q and target set T \u2286 S constrains the probability to reach T from sI in M to be at most \u03bb .", "startOffset": 103, "endOffset": 108}, {"referenceID": 8, "context": ",\u03c6n, can be formally verified for an MDP using multi-objective model checking [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "A wheelchair as in [11] is to be steered from the lower left corner of the grid to the exit on the upper right corner of the grid.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "In [8] it is argued that blending intuitively reflects the confidence in how good the autonomy protocol is able to assist with respect to the human user\u2019s goals.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "In [19], additional notions of blending are discussed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Definition 4 (Linear blending): Given an MDP Mr = (S,sI ,A,P), two strategies \u03c3h,\u03c3a \u2208 Schedr , and a blending function b : S\u2192 [0,1], the blended strategy \u03c3ha \u2208 Schedr for all states s \u2208 S, and actions \u03b1 \u2208 A is", "startOffset": 126, "endOffset": 131}, {"referenceID": 4, "context": "To now measure such a deviation, we introduce the concept of perturbation which was\u2014on a complexity theoretic level\u2014for instance investigated in [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "We introduce the following specific set Var of variables: \u2022 \u03c3 s,\u03b1 a ,\u03c3 s,\u03b1 ha \u2208 [0,1] for each s \u2208 S and \u03b1 \u2208 A define the autonomous strategy \u03c3a and the blended strategy \u03c3ha.", "startOffset": 80, "endOffset": 85}, {"referenceID": 0, "context": "\u2022 ps \u2208 [0,1] for each s \u2208 S are assigned the probability of reaching T \u2286 S from state s under strategy \u03c3ha.", "startOffset": 7, "endOffset": 12}, {"referenceID": 0, "context": "If the problem is not feasible for the given blending function, optionally the autonomy protocol can try to compute a new function b : S\u2192 [0,1] for which the altered problem is feasible.", "startOffset": 138, "endOffset": 143}, {"referenceID": 0, "context": "A satisfying assignment for the resulting nonlinear program induces a suitable blending function b : S\u2192 [0,1] in addition to the strategies.", "startOffset": 104, "endOffset": 109}, {"referenceID": 3, "context": "The results were obtained using the NLP solver IPOPT [4].", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "In [3], the first approach to automatically repair an MC model was presented as an NLP.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Simulation-based algorithms were investigated in [6].", "startOffset": 49, "endOffset": 52}, {"referenceID": 15, "context": "A heuristic but very scalable technique called local repair was proposed in [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "The safety specification ensures that the agent reaches a target cell without crashing into an obstacle with a certain high probability \u03bb \u2208 [0,1], formally P\u2265\u03bb (\u00accrash U target).", "startOffset": 140, "endOffset": 145}, {"referenceID": 13, "context": "We use the probabilistic model checker PRISM [15] for verification,", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "We use the NLP solver IPOPT [4] and the LP solver Gurobi [12].", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "the greedy method from [17] into our framework augmented by side constraints ensuring well-defined strategies.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "domized human strategy \u03c3h for this participant can be obtained via Maximum Entropy Inverse Reinforcement Learning (MEIRL) [22].", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "Inverse reinforcement learning has\u2014for instance\u2014also been used in [14] to collect data about human behavior in a shared control scenario (though without any", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "formal guarantees) or in [18] to distinguish human intents Process data via MEIRL Shared control synthesis", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "For the resulting strategy, the probability of a possible deviation from the actual intend can be bounded with respect to the number of samples using Hoeffding\u2019s inequality, see [21] for details.", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "Thus, the encoding in the PRISM-language [15]", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation.", "creator": "LaTeX with hyperref package"}}}