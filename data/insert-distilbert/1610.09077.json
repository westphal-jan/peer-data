{"id": "1610.09077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Integrating Topic Models and Latent Factors for Recommendation", "abstract": "the research of personalized comparison recommendation techniques today has mostly run parted largely into two mainstream directions, related i. e., the factorization - based approaches and topic models. practically, they aim to potentially benefit consumers from the numerical ratings alone and textual reviews, correspondingly, which compose two major information sources in various real - world systems. however, nowadays although the two approaches are supposed to be correlated for their same objective goal of accurate recommendation, there still lacks a clear theoretical understanding of how their objective functions can be mathematically bridged to thus leverage the numerical ratings and textual reviews collectively, and uncertain why such a bridge is intuitively reasonable to consciously match up their learning procedures for the rating prediction stage and top - n recommendation tasks, respectively.", "histories": [["v1", "Fri, 28 Oct 2016 04:20:54 GMT  (1018kb,D)", "https://arxiv.org/abs/1610.09077v1", "10 pages, 3 figures"], ["v2", "Sat, 5 Nov 2016 20:36:43 GMT  (1047kb,D)", "http://arxiv.org/abs/1610.09077v2", "11 pages, 3 figures, version 2"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["danis j wilson", "wei zhang"], "accepted": false, "id": "1610.09077"}, "pdf": {"name": "1610.09077.pdf", "metadata": {"source": "CRF", "title": "Integrating Topic Models and Latent Factors for Recommendation", "authors": ["Danis J. Wilson", "Wei Zhang"], "emails": ["danisjwilson@gmail.com", "wei@andrew.cmu.edu"], "sections": [{"heading": null, "text": "The research of personalized recommendation techniques today has mostly parted into two mainstream directions, i.e., the factorization-based approaches and topic models. Practically, they aim to benefit from the numerical ratings and textual reviews, correspondingly, which compose two major information sources in various real-world systems. However, although the two approaches are supposed to be correlated for their same goal of accurate recommendation, there still lacks a clear theoretical understanding of how their objective functions can be mathematically bridged to leverage the numerical ratings and textual reviews collectively, and why such a bridge is intuitively reasonable to match up their learning procedures for the rating prediction and top-N recommendation tasks, respectively.\nIn this work, we exposit with mathematical analysis that, the vector-level randomization functions to coordinate the optimization objectives of factorizational and topic models unfortunately do not exist at all, although they are usually pre-assumed and intuitively designed in the literature. Fortunately, we also point out that one can avoid the seeking of such a randomization function by optimizing a Joint Factorizational Topic (JFT) model directly. We apply our JFT model to restaurant recommendation, and study its performance in both normal and cross-city recommendation scenarios, where the latter is an extremely difficult task for its inherent cold-start nature. Experimental results on real-world datasets verified the appealing performance of our approach against previous methods, on both rating prediction and top-N recommendation tasks."}, {"heading": "1 Introduction", "text": "The vast amount of items in various web-based applications has made it an essential task to construct reliable Personalized Recommender Systems (PRS) [1, 2]. With the ability to leverage the wisdom of crowds, Collaborative Filtering (CF)-based [3, 4] approaches have achieved significant success and wide application, especially for those Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7, 8, 9, 10, 11, 12,\n\u2217Arcadia University. danisjwilson@gmail.com \u2020Carnegie Mellon University. wei@andrew.cmu.edu\n13, 14] techniques, which model the preferences of users and items collectively through multivariate hidden factors learned from star ratings.\nRecently, researchers have been putting attention on another important information source in many online systems, namely, the textual user reviews. Usually, the ratings and reviews come in pairs in many typical applications, e.g., Amazon and Yelp. While the ratings act as integrated indicators of user attitudes towards products, the reviews serve as more detailed explanations of what aspects users care about and why the corresponding rating is made [15, 16].\nAs such, the application of Topic Models [17] has gained attention to leverage the textual reviews for personalized recommendation, especially the frequently used Latent Dirichlet Allocation (LDA) [18] technique and its variants, for their ability to extract latent topics from reviews that represent the actual factors users care about when making numerical ratings [19, 20, 21]. This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].\nHowever, without a clear mathematical understanding of how the objective functions of LFM and LDA interact with each other when bridged for unified model learning, current approaches have to base themselves on pre-assumed designations to bridge their heterogenous objective functions. e.g., to transform the latent factors in LFM to topic distributions in LDA through manually designed randomization functions such as logistic normalization [19], or to assume the factors and topics be the same vector sampled from mixture Gaussian distributions [20], etc.\nIn this work, we study the mathematical relations between the probability of recommending an item and the estimated user-item correlations from LFM or LDA. Based on this, we prove that a multiplicatively monotonic randomization function that transforms LFM latent factors to LDA topic distributions actually does not exist at all. As a result, although some normalizationbased transformations seem intuitional, they actually make the objective functions of LFM and LDA conflict with each other during optimization, where a higher value of log-likelihood in LDA component may force a\nar X\niv :1\n61 0.\n09 07\n7v 2\n[ cs\n.I R\n] 5\nN ov\n2 01\n6\nlower rating prediction in LFM component, which is not favoured when bridging the two models.\nFortunately, we further find that instead of transforming a latent factor to a topic distribution separately, we can transform the product of latent factors in LFM to the corresponding product of topic distributions in LDA as a whole, thus to avoid the seeking of a theoretically nonexistent randomization function. Based on these findings, we propose the Joint Factorizational Topic (JFT) model to bridge LFM and LDA, so as to adopt the numerical ratings and textual reviews collectively, and at the same time guarantee the inner-model consistency between the LFM and LDA components.\nTo validate the performance, we conduct extensive experiments on (both normal and cross-city) restaurant recommendation, which is a representative task in many Location-Based Services (LBS), yet it is also extremely difficult for its inherent nature of cold-start in cross-city settings. As a result, this problem setting validates the performance of our approach and highlights the effect of leveraging the power of ratings and reviews jointly. What\u2019s more, the concept of location in this work can be further generalized to item categories, which makes it possible for our JFT framework to bridge factorizational and topical models for cross-domain recommendation."}, {"heading": "2 Preliminaries and Definitions", "text": "2.1 Latent Factor Models (LFM) Latent Factor Models (LFM) [4] attempt to encode user and item preferences in a latent factor space so as to estimate the user-item relations for rating prediction, which account for many of the frequently used Matrix Factorization (MF) [6] techniques. Among those, a \u2018standard\u2019 and representative formalization [26] predicts the user-item ratings ru,i with user/item biases and latent factors by,\n(2.1) rate(u, i) = \u03b1+ \u03b2u + \u03b2i + \u03b3u \u00b7 \u03b3i\nwhere \u03b1 is the global offset, \u03b2u and \u03b2i are user and item biases, \u03b3u and \u03b3i are the K-dimensional latent factors of user u and item i, respectively, and \u201c\u00b7\u201d denotes vector multiplication. Intuitively, \u03b3u can be interpreted as the preference of user u to some latent factors, while \u03b3i is the property embedding of item i on those latent factors. Based on a set of observed training records R, the model is typically targeted with the goal of providing accurate rating predictions, where we determine the parameter set \u0398 = {\u03b1, \u03b2u, \u03b2i, \u03b3u, \u03b3i} with the following minimization problem,\n(2.2) \u0398 = argmin \u0398 \u2211 ru,i\u2208R ( rate(u, i)\u2212 ru,i )2 + \u03bb\u2126(\u0398)\nand \u2126(\u0398) is a regularization term. A variety of methods exist to minimize Eq.(2.2), for example, Stochastic\nGradient Descent (SGD) or Alternating Least Squares (ALS) [26]. However, this model merely takes into account the numerical ratings and leaves out the textual reviews, which is information-rich and may well help to provide better recommendations."}, {"heading": "2.2 Latent Dirichlet Allocation (LDA) Different", "text": "from LFM, the LDA model attempts to learn a number of K latent topics from documents (textual reviews in this work), where each word w is assigned to a topic zw, and each topic z is associated with a word distribution \u03c6z. Based on this, each document d \u2208 D is represented with a K-dimensional topic distribution \u03b8d, where the j-th word wd,j in document d discusses its corresponding topic zd,j with probability \u03b8d,zd,j . It is usually convenient to also define the word distribution \u03c6z,w, which is the probability that word w is used for topic z in the whole corpus D. The final model conducts parameter learning by maximizing the likelihood of observing the whole D: (2.3) P (D|\u03b8, \u03c6, z) = \u220f d\u2208D Ld\u220f j=1 \u03b8d,zd,j\u03c6zd,j ,wd,j\nwhere Ld is the length (number of words) of document d. Intuitionally, we are multiplying the probability of seeing a particular topic in \u03b8d with the likelihood of seeing a particular word given the topic to estimate the likelihood of seeing the whole corpus.\n2.3 Randomization Function Let \u03b3 \u2208 RK be an arbitrary vector and \u03b8 \u2208 [0, 1]K be a stochastic vector, where their dimensions are the same K as the latent factors \u03b3u, \u03b3i and latent topics \u03b8d in the previous subsections. According to the definition, we have 0 \u2264 \u03b8k \u2264 1 and \u2016\u03b8\u20161 = \u2211K k=1 \u03b8k = 1. The target of a randomization function f : RK \u2192 RK is to convert\nan arbitrary vector \u03b3 to a probabilistic distribution \u03b8 = f(\u03b3). The inherent nature of a randomization function is the key component to bridge the gap between LFM and LDA models, which links the latent factors \u03b3 in LFM to the topic distributions \u03b8 in LDA, and thus makes it possible to model the numerical ratings and textual reviews in a joint manner.\nIn the background of personalized recommendation, a desired randomization function is expected to be monotonic in the sense that it preserves the orderings, so that the largest value of \u03b3 should also correspond to the largest value in \u03b8, thus the dimensions of the LFM model and the LDA model are inherently aligned during the model learning process to express the useritem relations in a shared feature space. As a result, the basic properties of a randomization function f(\u00b7) can be summarized as follows: (2.4){\n0 \u2264 f(\u03b3)i \u2264 1, \u2016f(\u03b3)\u20161 = 1 \u03b3i < \u03b3j \u2192 f(\u03b3)i < f(\u03b3)j ,\u2200\u03b3 \u2208 RK , 1 \u2264 i, j \u2264 K\nFor example, in [19] the authors designed a randomization function as: (2.5) \u03b8k = f(\u03b3)k = exp (\u03ba\u03b3k)\u2211 k\u2032 exp (\u03ba\u03b3k\u2032)\nwhich conducts logistic normalization on a latent factor. In the following, we investigate the relationship between the objective functions of LFM and LDA, and further point out the properties required on a randomization function to coordinate the models when bridging the two different functions."}, {"heading": "3 Bridging Factors and Topics", "text": "Though various personalization strategies based on different optimization objectives (e.g., RMSE, likelihood, ranking loss, etc) may be designed for more accurate recommendation, the inherently underlying personalization indicator that we are actually considering is P (i|u), which is the probability or tendency for us to recommendation an item i given a user u. Various different strategies are actually estimating this probability either explicitly or implicitly to rank the items for each user so as to construct the personalized recommendation list. In this section, we bridge the LFM and LDA models by expositing how their objective functions rank a list of items based on P (i|u).\n3.1 Probability of Item Recommendation In the Latent Factor Model (LFM), a recommendation list is constructed in descending order of the predicted ratings rate(u, i) for a given user, which means that an item i with a higher rating prediction on user u also gains a\nhigher probability of being recommended P (i|u). As a result,\n(3.6) PLFM(i|u) \u221d rate(u, i) \u221d \u03b3u \u00b7 \u03b3i\nwhere \u221d denotes a positive correlation, and we leave out the parameters \u03b1, \u03b2u and \u03b2i because they are constants given a user and an item in model learning [26].\nIn Latent Dirichlet Allocation (LDA) for personalized recommendation, each user or item is represented by its corresponding set of textual reviews du or di, and the underlying intuition models the topical correlation between them by estimating the potential review du,i that a user may write on an item, based on the topical distributions \u03b8du and \u03b8di . To simplify the notations, we use u, i, and k to denote the user or item document representations du, di and the k-th latent topic zk interchangeably, and we thus have:\n(3.7) P (k|u) = \u03b8u,k and P (k|i) = \u03b8i,k\nThe LDA model conducts likelihood maximization on each observed review du,i given the corresponding user u and item i, and the embedded topical distribution represents the probability of observing each topic k, which is:\n(3.8) P (k|u, i) = \u03b8d,k\nLDA applies an indirect causal effect from users to items via latent topics [18], which means that user u and item i are conditionally independent given topic k, i.e., P (u|i, k) = P (u|k), and this further gives us:\n(3.9) P (u, i, k) = P (u, k)P (i, k)\nP (k)\nBy applying Eq.(3.9) to Eq.(3.8), we decompose the topical distribution of a review into the topical representations of the corresponding user and item:\n(3.10) \u03b8d,k = P (k|u, i) = P (k|u)P (k|i)\nP (u)P (i)\nP (k)P (u, i)\n\u221d P (k|u)P (k|i) = \u03b8u,k\u03b8i,k\nwhere P (u), P (i) and P (u, i) are constants in the LDA procedure, and the latent topics zk are identically independent from each other, giving us constant and equal valued P (k)\u2019s over the K topics. As a result, we have the following conditional recommendation probability for LDA models: (3.11)\nPLDA(i|u) = K\u2211 k=1 P (k|u)P (i|k) = K\u2211 k=1 P (k|u)P (k|i) P (i) P (k)\n\u221d K\u2211 k=1 P (k|u)P (k|i) = K\u2211 k=1 \u03b8u,k\u03b8i,k = \u03b8u \u00b7 \u03b8i\nand this result conforms with Eq.(3.10) in that, the probability of recommending an item given a user is positively correlated to the sum of topic probabilities that a user may textually review on an item.\n3.2 Bridging the Objective Functions According to the conditional item recommendation probabilities given a target user specified in Eq.(3.6) and Eq.(3.11) for LFM and LDA models, respectively, a favoured approach to bridge the two models to leverage the power of both ratings and reviews should coordinate their objective functions, so that a higher value of \u03b3u \u00b7 \u03b3i also corresponds to a higher value in \u03b8u \u00b7 \u03b8i. More precisely, except for the monotonic property defined in Eq.(2.4) on a vector itself, the randomization function f(\u00b7) from \u03b3 to \u03b8 is also required to be monotonic for vector multiplications: (3.12) \u03b31\u00b7\u03b32 < \u03b33\u00b7\u03b34 \u2192 f(\u03b31)\u00b7f(\u03b32) < f(\u03b33)\u00b7f(\u03b34), \u2200\u03b31, \u03b32, \u03b33, \u03b34\nIn this way, the LFM and LDA components in a bridged objective function would not conflict with each other during the model learning process, because both of them increase/decease the recommendation probability P (i|u) at the same time for each single iteration.\nPrevious work intuitionally assumes that a randomization function satisfying the vector-level monotonic property in Eq.(2.4) will also be monotonic on product-level as Eq.(3.12). Frequently used examples are the normalization-based randomization functions, which normalize the elements of \u03b3 to construct \u03b8 so that they sum to one [19, 20, 22, 23]. In [19] for example, a logistic normalization randomization function as in Eq.(2.5) is applied so as to minimize the following joint objective function to bridge the LFM and LDA models:\n(3.13) O = \u2211\nru,i\u2208R\n( rate(u, i)\u2212 ru,i )2\ufe38 \ufe37\ufe37 \ufe38 LFM component \u2212\u03bb L ( D|\u03b8, \u03c6, z )\ufe38 \ufe37\ufe37 \ufe38 LDA component\nwhere the LFM component still minimizes the error in predicted ratings, while the LDA component is the loglikelihood of the probability for the review corpus in Eq.(2.3).\nPrevious designations do seem intuitional and reasonable, and they indeed improve the performance of personalized recommendation in many cases. However, we would like to point out in this work that the vectorlevel monotonic property does not necessarily guarantee the monotonic property on a product-level. Actually, we prove that such a randomization function that satisfies both vector- and product-level monotonic properties does not exist at all, and the proof is exposited in the Appendix of this paper.\nFor this reason, forcing a vector-level randomization function on the latent factors to bridge the LFM and LDA models will result in a conflict between the two components during the procedure of objective optimization, i.e., while the LFM component gains a higher probability of item recommendation with a larger value of \u03b3u\u00b7\u03b3i, the LDA component may reversely force a lower recommendation probability with \u03b8u \u00b7 \u03b8i just because of the mathematical property of the randomization function, which is not favoured in model learning process. This further explains the observation that the prediction accuracy of Eq.(3.13) tends to fluctuate drastically during optimization, although the overall performance generally tends to increase along with the iterations.\n3.3 Direct Product-Level Randomization Despite that a randomization function with product-level monotonic property does not exist, we shall notice a simple fact that the de facto components that we need to consider so as to preserve the orderings of PLFM(i|u) and PLDA(i|u), are the final product of the latent factors or latent topics as a whole, i.e., \u03b3u \u00b7\u03b3i and \u03b8u \u00b7 \u03b8i, rather than each latent factor \u03b3 to a latent topic distribution \u03b8 separately.\nMore precisely, what we really need in the LDA model of Eq.(2.3) is the topic distribution of each document \u03b8d, where we have \u03b8d,k \u221d \u03b8u,k\u03b8i,k by Eq.(3.10). As a result, we can apply a randomization function f(\u00b7) to the product of latent factors \u03b3u,k\u03b3i,k directly, so as to obtain the product of latent topic distributions \u03b8u,k\u03b8i,k as a whole, which is further positively correlated to \u03b8d,k that will finally be adopted by the LDA component for model learning.\nA lot of normalization-based randomization functions guarantee the product-level monotonic property when applied to the product of latent factors directly. In this work, we adopt the logistic-normalization function to enforce \u03b8u,k\u03b8i,k (and thus \u03b8d,k) to be positive and sum to one: (3.14)\n\u03b8d,k \u221d \u03b8u,k\u03b8i,k = f(\u03b3u,k\u03b3i,k) . = exp(\u03b3u,k\u03b3i,k)\u2211 k\u2032 exp(\u03b3u,k\u2032\u03b3i,k\u2032)\nwhich preserves the orderings of the dimensions from \u03b3u \u00b7 \u03b3i to \u03b8u \u00b7 \u03b8i, and thus guarantees the positive correlation between PLFM(i|u) and PLDA(i|u) according to Eq.(3.6) and Eq.(3.11). Based on this direct productlevel randomization, we are fortunately able to bridge the LFM and LDA models to leverage the power of ratings and reviews collectively, and meanwhile make the two components coordinate with each other for model learning, which improves both the performance and stability of personalized recommendation.\nIn the following, we describe our Joint Factoriza-\ntional Topic (JFT) model, as well as its application in the practical scenario of (cross-city) restaurant recommendation."}, {"heading": "4 Joint Factorizational Topic (JFT) Model", "text": "4.1 Model Design The JFT model bridges the LFM component as Eq.(2.1) and the LDA component in Eq.(2.3) according to the product-level randomization. Specifically, let \u03b8d,k = exp(\u03b3u,k\u03b3i,k)\u2211\nk\u2032 exp(\u03b3u,k\u2032\u03b3i,k\u2032 ) in Eq.(2.3), the\nJFT model attempts to optimize the following objective function: (4.15) F (\u0398, \u03c6, z) =\u2211 ru,i\u2208R ( rate(u, i)\u2212 ru,i\n)2 \u2212 \u03bblL(D|\u03b8, \u03c6, z) + \u03bbp\u2126(\u0398) where \u0398 = (\u03b1, \u03b2u, \u03b2i, \u03b3u, \u03b3i) is the parameter set of the LFM component, L(D|\u03b8, \u03c6, z) is the log-likelihood of the whole corpus whose document distributions \u03b8 come from the latent factors \u03b3, and \u2126(\u0398) = \u2211 u,i(\u03b2 2 u + \u03b2 2 i + \u2016\u03b3u\u201622 + \u2016\u03b3i\u201622) is the `2-norm regularizer for the latent parameters.\nIntuitionally, the LDA component L(\u00b7) serves as another regularization term besides the traditional `2- norm regularizer \u2126(\u00b7) for numerical rating prediction, and we trade off between them two with \u03bbl and \u03bbp, respectively. In this way, the JFT model attempts to minimize the error in rating prediction, and meanwhile maximizes the likelihood of observing the corresponding textual reviews.\nMost importantly, the LFM and LDA components are designed to be consistent with each other by product-level randomization in model learning, in that a smaller prediction error in the LFM component functionally invokes a larger likelihood of observing the corresponding textual review, which makes the two components collaborate rather than violate with each other when optimizing the objective function.\n4.2 Fitting the Model We introduce the algorithm for model fitting in this subsection. Typically, the LFM component (with `2-regularizer) can be easily fit with gradient descent, while the log-likelihood LDA component is optimized with Gibbs sampling. As our model jointly includes the two heterogenous components, we construct a learning procedure that optimizes the two components alternatively:\nS1 : {\u0398t, \u03c6t} \u2190 argmin \u0398,\u03c6 F (\u0398, \u03c6, zt\u22121) by gradient descent S2 : Logistic normalization on each topic vector \u03c6tk S3 : Sample ztd,j with probability P (z t d,j = k) = \u03b8 t d,k\u03c6 t k,wd,j\nIn the first step, we fix the topic z of each word in each document, and further compute the gradient of each parameter in {\u0398, \u03c6} while fixing the others. Based on these gradients, the parameters in {\u0398, \u03c6} are updated one by one, where the step size for each parameter is determined by linear search.\nHowever, the gradient descent procedure would not guarantee the word distribution \u03c6 of latent topics to be stochastic vectors. As a result, we conduct logistic normalization for each topic \u03c6k (1 \u2264 k \u2264 K) in the second step, where each dimension of \u03c6k is normalized as \u03c6k,w = exp(\u03c6k,w)\u2211 w\u2032 exp(\u03c6k,w\u2032 ) .\nIn the last step, we preserve the results from the previous steps, and update the topic assignment for each word in each document. Similar to LDA, which assigns each word to the k-th topic according to the likelihood of the word discussing topic k, we set zd,j = k with probability proportional to \u03b8d,k\u03c6k,wd,j , where the indices pair {d, j} denotes the j-th word of document d, \u03b8d is the the topic distribution of document d, and \u03c6k is the word distribution of topic k.\nThe major difference between LDA and the last step of our JFT model is that, the topic distributions \u03b8d are determined base on the product-level randomization from latent factors \u03b3u, \u03b3i and \u03b3c in our model, instead of sampling from a Dirichlet distribution in LDA. As a result, we only need to sample the topic assignments z in each iteration of our JFT model. The probabilistic interpretation of our approach and its relationship with the LFM component are exposited in previous sections.\nFinally, these steps are repeated iteratively until convergence, i.e., the `2-difference in \u0398 is sufficiently small between two consecutive iterations, or that an overfitting is observed in the validation set.\n4.3 Top-N Recommendation We further adapt the JFT model to provide practical top-N recommendation lists beyond numerical rating predictions.\nIt is known that a good performance on rating prediction does not necessarily guarantee a satisfactory performance of top-N recommendation by ranking the items in descending order of the predicted ratings [27]. This is partly because of the contradiction between the goal of recommending items that users would potentially visit and the data (ratings) that we use for model training, i.e., users indeed visited the items in the dataset, no matter what numerical ratings they eventually made on them. Intuitionally, a relatively low predicted rating does not necessarily mean that the user would not be attracted by the item at all, because of the many items with low ratings yet visited by the users.\nAs a result, we train our JFT model (and the baseline approaches) for top-N recommendation in a differ-\nent way from the rating prediction task. Specifically, we feed the learning procedure with binary inputs, where the observed records in R are treated as positive cases (rating=1), and the negative cases (rating=0) are sampled from the unobserved user-item pairs in a balanced negative sampling manner. For clarity, we exposit the sampling, learning, and recommendation produce in Algorithm 1."}, {"heading": "5 Experiments", "text": "In this section, we conduct extensive experiments to validate the performance of our JFT model based on realworld datasets. We first conduct case studies to support the underlying intuition of our approach in crosscity recommendation, and further investigate the performance on rating prediction and top-N recommendation\n5.1 Experimental Setup We collected user reviews from a major restaurant review website Dianping.com in China, including 253,749 reviews from 32,529 users towards 8,026 restaurants located in 194 cities, where each user made 20 or more reviews, including introand cross-city cases, and the home city of a user is registered in his/her profile. Of the whole 253,749 reviews, 233,802 (92%) records fall into intro-city reviews, and the remaining 19,947 (8%) records are cross-city reviews. Each review in the corpus consists of an integer rating ranging from 1 to 5 stars and a piece of textual comment, where the user expresses his/her opinions on the corresponding restaurant. The average length of textual comments is 41.5 words. To feed the LDA component with high quality textual inputs, we conduct part-of-speech tagging and stop word removing for each review with the widely used Stanford NLP toolkit.\nAfter careful tuning with grid search, we set the hyper-parameters \u03bbl = 0.01 and \u03bbp = 0.001, and five-fold cross-validation was conducted in performance evaluation for all methods.\n5.2 Case Study of Regional Features Before introducing the experimental results, we execute an intuitional case study of the distribution of regional features in support of the underlying intuition of cross-city recommendation based on textual reviews.\nTo do so, we select four representative feature words from restaurant reviews (Mutton, Seafood, Spicy, and Price) [28, 29], and calculate the percentage that reviews in each city discussing a feature against the total number of reviews discussing the feature in the whole corpus, which represent the regional distributions of a feature among the major cities across mainland China. We further intuitionally plot the regional distributions for each feature on maps, where each dot represents\nAlgorithm 1 Top-N Recommendation\n1: Input: R,D, N 2: Output: Recommendation list of length N 3: R+ \u2190 R with all ratings reset to be 1 4: Initialize model parameters \u03b1, \u03b2, \u03b3, \u03c6, z randomly 5: while Convergence or t > T do 6: t\u2190 t+ 1, R\u2212 \u2190 \u2205 7: for (u, i) \u2208 R+ do 8: Sample item j from the same city of item i randomly, where (u, j) /\u2208 R+ 9: R\u2212 \u2190 R\u2212 \u222a (u, j) with rating 0 10: end for 11: Update model by Step1 \u223c 3 with {R+ \u222aR\u2212,D} 12: end while 13: Rank the items in descending order of predictions 14: return Top-N Recommended items for each user\na city, and the grey-scale color is proportional to its percentage of discussing a feature [30], as shown in Figure 1.\nIt is observed that the feature Mutton is mainly discussed in cities located in the north and west part of China, which is consistent with the distribution of the grasslands in China. Seafood is mostly discussed along the seacoast, while Spicy is extensively mentioned in provinces that are famous for spicy food, including Sichuan, Hunan, Guangxi, Shaanxi, Xinjiang, etc. Finally, the feature Price is concerned by customers in nearly all the cities, which is also reasonable for its general importance to most users, though they have different flavour preferences.\nWe further display the top 10 words of each topic discovered by the LDA component with K = 10, as shown in Table 3, and find that they exhibit clear meanings in different Cuisine styles (Cantonese, Northwest, Northeast, Western, etc.) or food types (Seafood, Meat, Drinks, Desserts, etc.).\n5.3 Performance on Rating Prediction In this section, we study the performance on rating prediction tasks, and adopt the following state-of-the-art rating prediction baselines.\nLFM: The LFM approach denoted in Eq.(2.2), which takes no advantage of the textual reviews.\nEFM: The Explicit Factor Model presented in [15], which is the state-of-the-art recommendation approach based on textual reviews by sentiment analysis.\nHFT: The Hidden Factors and Topics model in [19], which is the state-of-the-art method that takes advantages of both LFM and LDA by applying vectorlevel randomizations (Eq.(2.5)).\nWe adopt Root Mean Square Error (RMSE) and\nMean Absolute Error (MAE) for evaluation, and the results with the number of topics/factors K = 10 are shown in Table 2. The standard deviations in five-fold cross-validation for each method and metric are\u2264 0.002.\nWe find that all the other approaches gain better performance against LFM, which means that taking advantage of the textual reviews helps to make better rating predictions. Beyond this, our JFT model achieves the best performance against both EFM and HFT. On considering that a major difference between JFT and HFT is the product- and vector-level randomization, this result is consistent with the theoretical analysis to bridge the LFM and LDA components in Section 3.\nTo exhibit a clearer view of the performance on cross-city scenarios, we further take out the cross-city rating records from the test set in each of the 5 folds, and conduct performance evaluation under different choices of the number of latent factors and topics K from 10 through 100. Results for RMSE and MAE are shown in Figure 2. We see that our JFT approach outperforms all other baselines on all choices of topic numbers, which validates the superior performance when we consider the local features of a city from reviews and match them to latent factors in consistent manners.\n5.4 Top-N Recommendation In this subsection, we explore the performance of our approach in more practical top-N recommendation tasks, and make comparison with the following baseline methods:\nWRMF: Weighted Regularized Matrix Factorization [31], which is similar to LFM but applies weighted negative sampling to benefit top-N recommendations.\nBPRMF: Bayesian Personalized Ranking (BPR)\nfor MF [32], which is the state-of-the-art algorithm for top-N recommendation based only on numerical ratings.\nHFT: The original HFT method does not achieve satisfactory top-N performance in our settings. As a result, we improve it with the same binary inputs and negative sampling approach (Alg.1) for fair comparison.\nTo evaluate, we randomly hold out 5 records for each user, and provide top-5 recommendation list for each user, as with most practical applications. We adopt the measures of Precision@5 and NDCG@5, where the latter takes the positions of recommended items into consideration, and the results are shown in Table 4.\nWe see that both our JFT approach and the HFT method (which make use of textual reviews) gain better performance than WRMF and BPRMF (which only make use of ratings). Further more, our JFT method gains a 22% improvement against HFT in terms of precision, and 8.3% on NDCG, which is a superior achievement for practical applications.\nSimilar to the task of rating prediction, we also\nevaluate the top-N performance in cross-city settings. To do so, we select those user-city pairs that a user has at least 5 records in a city beyond his/her home city, which results into 4,021 pairs corresponding to 1,739 users. We thus randomly hold out 5 records for a user in a corresponding city, and construct the recommendation list using the restaurants from that city, which gives us 4,021 lists for evaluation. We also conduct 5-fold cross-validation, and the standard deviations for both Precision and NDCG are \u2264 0.005. Figure 3 shows the results against the number of latent factors/topics K.\nWe see that the performance of our JFT model is better than the baselines on nearly all choices of K. Interestingly, we find that the overall cross-city performance is a magnitude better than that on the whole dataset. This means that user behaviours can be more predictable in cross-city settings, where users do visit local attractions beyond their historical preferences."}, {"heading": "6 Related Work", "text": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications\nin the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.\nEarly systems of personalized recommendations rely on content-based approaches [37], which construct the user/item content profiles and make recommendation by paring users with the contently similar items. Content-based approaches usually gain good accuracy but functionally lack the ability of providing recommendations with novelty, serendipity, and flexibility. Besides, they usually require a large amount of expensive human annotations [1].\nThis further leads to the prospering of Collaborative Filtering (CF)-based recommendation algorithms [4, 3] that leverage the wisdom of the crowds. Typically, they construct the partially observed user-item rating matrix and conduct missing rating prediction based on the historical records of a user, as well as those of the others.\nWith widely recognized performance in rating prediction, scalability, and computational efficiency, the Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7] techniques for CF have been extensively investigated by the research community, and widely applied in practical systems. Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.\nDespite the important success in rating prediction, the CF approaches based solely on the numerical ratings suffer from the problems of explainability [15], coldstart [41, 42], and the difficulty to provide more specific recommendations that meet targeted item aspects [43].\nBesides, related research results show that the performance on numerical rating prediction does not necessarily relate to the performance on practical top-N recommendations [27], and that the numerical star ratings may not always be a reliable indicator of users\u2019 attitudes towards items [44].\nTo alleviate these problems, researchers have been investigating the incorporation of textual reviews for recommendation [45, 46, 47, 48], which is another important information source beyond the star ratings in many systems. Early approaches rely on manually extracted item aspects from reviews for more informed recommendation [49, 43] and rating prediction [50, 51], which improved the performance but also required extensive human participations. As a results, researchers recently have begun to investigate the possibility of integrating the automatic topic modeling techniques on textual reviews and the latent factor modeling approach on numerical ratings for boosted recommendation, and have achieved appealing results [19, 20, 22, 23, 52].\nHowever, without a clear mathematical exposition of the relationships between latent factor models and topic modeling, current approaches have to base themselves on manually designed randomization functions or probabilistic distributions. In this work, however, we attempt to make an exposition on the relationships between the two types of objective functions, and further bridge the inherently heterogenous models in a harmonious way for recommendation with the power of both numerical ratings and textual reviews."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we proposed the Joint Factorizational Topic (JFT) model that leverages the ratings and reviews in a collective manner for recommendation. For the first time, we examine the mathematical mutualdependent relations between LFM and LDA for personalized recommendation, and we prove that vector-level randomization functions that are multiplicatively monotonic actually do not exist, although they are previously frequently used to bridge the heterogeneous LFM and LDA components.\nFortunately, we also find that a direct productlevel randomization approach can be used to bridge the two components and coordinate their behaviors for model learning. Extensive experimental results on case studies, rating prediction, and top-N recommendation tasks verified both the intuition, theoretical basis, and the quantitative performance of our approach.\nThis is the first step towards bridging the latent factor models and topical models for cross-city recommendation, and there is much room for future research and improvements. One important thing to notice is that\nour model is not restricted to cross-\u2018city\u2019 scenarios, but generally applicable to cross-domain recommendations, and the appealing performance of our bridging framework sheds light on possibly new approaches for the traditional task of cross-domain recommendation. Besides the factorizational and topic models, we can even investigate other graphical or deep-representational models to integrate the numerical ratings and topics from textual reviews, so as to bridge the two important personalized information sources broadly seen in many Webbased systems.\nAppendix\nLet \u03b3 denote arbitrary vectors with length K, and F is the set of all randomization functions f : RK \u2192 RK satisfying: (7.16){\n0 \u2264 f(\u03b3)i \u2264 1, \u2016f(\u03b3)\u20161 = 1 \u03b3i < \u03b3j \u2192 f(\u03b3)i < f(\u03b3)j ,\u2200\u03b3 \u2208 RK , 1 \u2264 i, j \u2264 K\nthen there exists no randomization function f \u2208 F with the product-level monotonic property of: (7.17) \u03b31\u00b7\u03b32 < \u03b33\u00b7\u03b34 \u2192 f(\u03b31)\u00b7f(\u03b32) < f(\u03b33)\u00b7f(\u03b34), \u2200\u03b31, \u03b32, \u03b33, \u03b34\nProof: Suppose there exists a randomization function f \u2208 F that meets Eq.(7.17). Let t > 1, and let \u03b1 and \u03b2 be vectors with \u03b1 \u00b7 \u03b2 > 0, then we have t\u03b1 \u00b7 \u03b2 > \u03b1 \u00b7 \u03b2. By applying the property of product-level monotonic in Eq.(7.17) we have:\n(7.18) f(t\u03b1) \u00b7 f(\u03b2) > f(\u03b1) \u00b7 f(\u03b2)\nand this can be equivalently written as: (7.19) ( f(t\u03b1)\u2212 f(\u03b1) ) \u00b7 f(\u03b2) > 0\nLet \u2206 . = f(t\u03b1) \u2212 f(\u03b1), and according to the definition of randomization function in Eq.(7.16), we know that \u2211 k f(t\u03b1)k = \u2211 k f(\u03b1)k = 1, thus we have:\n(7.20) \u2211\nk \u2206k = \u2211 k ( f(t\u03b1)\u2212 f(\u03b1) ) k = 0\nAccording to Eq.(7.19) we know that \u2206 6= 0. Let P denote the indices of all positive elements in vector \u2206, and N be the indices of negative elements. We have:\n(7.21) \u2211\nk\u2208P \u2206k + \u2211 k\u2208N \u2206k = 0\nAs Eq.(7.19) holds for any \u03b2 with \u03b1 \u00b7\u03b2 > 0, without loss of generally, let \u03b2 be a vector where \u03b2k\u2208P = 0 and \u03b2k\u2208N = 1. According to the vector-level monotonic property in Eq.(7.16) and the fact that 0 < 1, we have f(\u03b2)k\u2208P < f(\u03b2)k\u2208N and 0 \u2264 f(\u03b2)k\u2208P\u222aN \u2264\n1. Combined with Eq.(7.21), we further obtain the following:\n(7.22) \u2206 \u00b7 f(\u03b2) = \u2211 k\u2208P \u2206kf(\u03b2)k + \u2211 k\u2208N \u2206kf(\u03b2)k < 0\nwhich is a direct contradiction with Eq.(7.19). As a result, there exists no randomization function f \u2208 F that satisfies the product-level monotonic property in Eq.(7.17)."}], "references": [{"title": "Introduction to Recommender Systems Handbook", "author": ["F. Ricci", "L. Rokach", "B. Shapira"], "venue": "Springer, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Recommender Systems: from Algorithms to User Experience", "author": ["J.A. Konstan", "J. Riedl"], "venue": "User Modeling and User-Adapted Interaction, vol. 22, no. 1-2, pp. 101\u2013 123, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey of Collaborative Filtering Techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in AI, vol. 4, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Advances in Collaborative Filtering", "author": ["Y. Koren", "R. Bell"], "venue": "Recommender Systems Handbook, pp. 145\u2013 186, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent Variable Models and Factor Analysis", "author": ["M. Knott", "D. Bartholomew"], "venue": "Kendall\u2019s Library of Statistics, vol. 2, 1999.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Investigation of Various Matrix Factorization Methods for Large Recommender Systems", "author": ["G. Takacs", "I. Pilaszy", "B. Nemeth", "D. Tikk"], "venue": "Proc. ICDM, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "A Unified View of Matrix Factorization Models", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 358\u2013373, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithms for Nonnegative Matrix Factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Proc. NIPS, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic Matrix Factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Proc. NIPS, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Localized Matrix Factorization for Recommendation based on Matrix Block Diagonal Forms", "author": ["Y. Zhang", "M. Zhang", "Y. Liu", "S. Ma", "S. Feng"], "venue": "WWW, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Improve Collaborative Filtering Through Bordered Block Diagonal Form Matrices", "author": ["Y. Zhang", "M. Zhang", "Y. Liu", "S. Ma"], "venue": "SIGIR, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the Sparsity: Augmented Matrix Factorization with Sampled Constraints on Unobservables", "author": ["Y. Zhang", "M. Zhang", "Y. Zhang", "Y. Liu", "S. Ma"], "venue": "CIKM, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-based recommendation on a webscale", "author": ["Y. Zhang", "M. Zhang", "Y. Liu", "C. Tat-Seng", "Y. Zhang", "S. Ma"], "venue": "Big Data (Big Data), 2015 IEEE International Conference on. IEEE, 2015, pp. 827\u2013836.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Browser-oriented universal cross-site recommendation and explanation based on user browsing logs", "author": ["Y. Zhang"], "venue": "Proceedings of the 8th ACM Conference on Recommender systems. ACM, 2014, pp. 433\u2013436.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Explicit Factor Models for Explainable Recommendation based on Phrase-level Sentiment Analysis", "author": ["Y. Zhang", "G. Lai", "M. Zhang", "Y. Zhang", "Y. Liu", "S. Ma"], "venue": "SIGIR, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Using Adjective Features from User Reviews to Generate Higher Quality and Explainable Recommendations", "author": ["X. Xu", "A. Datta", "K. Dutta"], "venue": "IFIP Advances in Info. and Com. Tech., vol. 389, pp. 18\u201334, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic Topic Models", "author": ["D.M. Blei"], "venue": "Communications of the ACM, vol. 55, no. 4, pp. 77\u201384, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR, vol. 2003, no. 3, pp. 993\u2013 1022, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text", "author": ["J. McAuley", "J. Leskovec"], "venue": "RecSys, pp. 165\u2013172, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Ratings meet Reviews: a Combined Approach to Recommend", "author": ["G. Ling", "M.R. Lyu", "I. King"], "venue": "RecSys, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Hlbpr: A hybrid local bayesian personal ranking method", "author": ["X. Chen", "P. Wang", "Z. Qin", "Y. Zhang"], "venue": "Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee, 2016, pp. 21\u201322.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "fLDA: Matrix Factorization through Latent Dirichlet Allocation", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "WSDM, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative Topic Modeling for Recommending Scientific Articles", "author": ["C. Wang", "D.M. Blei"], "venue": "KDD, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Collaborative Topic Regression with Social Matrix Factorization for Recommendation Systems", "author": ["S. Purushotham", "Y. Liu", "C.-C.J. Kuo"], "venue": "ICML, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to rank features for recommendation over multiple categories", "author": ["X. Chen", "Z. Qin", "Y. Zhang", "T. Xu"], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2016, pp. 305\u2013314.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix Factorization Techniques for Recommender Systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance of Recommender Algorithms on Top-N Recommendation Tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "RecSys, pp. 39\u201346, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A Unified Framework for Emotional Elements Extraction based on Finite State Matching Machine", "author": ["Y. Tan", "Y. Zhang", "M. Zhang", "Y. Liu", "S. Ma"], "venue": "NLPCC, vol. 400, pp. 60\u201371, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Boost phraselevel polarity labelling with review-level sentiment classification", "author": ["Y. Zhang", "M. Zhang", "Y. Liu", "S. Ma"], "venue": "arXiv preprint arXiv:1502.03322, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Sof: Soft-cluster matrix factorization for probabilistic clustering.", "author": ["H. Zhao", "P. Poupart", "Y. Zhang", "M. Lysy"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Collaborative Filtering for Implicit Feedback Datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "ICDM, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L.S. Thieme"], "venue": "UAI, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Amazon.com Recommendations: Item-to-item Collaborative Filtering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "Internet Computing, vol. 7, no. 1, pp. 76\u201380, 2003.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Group Recommendation System for Facebook", "author": ["E.A. Baatarjav", "S. Phithakkitnukoon", "R. Dantu"], "venue": "On the Move to Meaningful Internet Systems: OTM 2008 Workshops, pp. 211\u2013219, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Google News Personalization: Scalable Online Collaborative Filtering", "author": ["A. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "WWW, pp. 271\u2013280, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "The YouTube Video Recommendation System", "author": ["J. Davidson", "B. Liebald", "J. Liu"], "venue": "RecSys, pp. 293\u2013296, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Content-Based Recommendation Systems", "author": ["M. Pazzani", "D. Billsus"], "venue": "The Adaptive Web LNCS, pp. 325\u2013341, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Proc. ICML, 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Maximum-Margin Matrix Factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "NIPS, 2005.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast Maximum Margin Matrix Factorization for Collaborative Prediction", "author": ["J. Rennie", "N. Srebro"], "venue": "ICML, 2005.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Facing the Cold Start Problem in Recommender Systems", "author": ["B. Lika", "K. Kolomvatsos", "S. Hadjiefthymiades"], "venue": "Expert Systems with Applications, vol. 41, no. 4, pp. 2065\u20132073, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding a Needle in a Haystack of Reviews: Cold Start Context- Based Recommender System", "author": ["A. Levi", "O. Mokryn", "C. Diot", "N. Taft"], "venue": "RecSys, pp. 115\u2013122, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "MovieCommenter: Aspect-Based Collaborative Filtering by Utilizing User Comments", "author": ["M. Ko", "H.W. Kim", "M.Y. Yi", "J. Song", "Y. Liu"], "venue": "CollaborateCom, 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Do Users Rate or Review? Boost Phrase-level Sentiment  Labeling with Review-level Sentiment Classification", "author": ["Y. Zhang", "H. Zhang", "M. Zhang", "Y. Liu"], "venue": "SIGIR, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating phrase-level sentiment analysis on textual reviews for personalized recommendation", "author": ["Y. Zhang"], "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining. ACM, 2015, pp. 435\u2013440.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Free text in user reviews: Their role in recommender systems", "author": ["M. Terzi", "M.A. Ferrario", "J. Whittle"], "venue": "RecSys, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Do users rate or review?: boost phrase-level sentiment labeling with review-level sentiment classification", "author": ["Y. Zhang", "H. Zhang", "M. Zhang", "Y. Liu", "S. Ma"], "venue": "Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 2014, pp. 1027\u20131030.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Daily-aware personalized recommendation based on feature-level time series analysis", "author": ["Y. Zhang", "M. Zhang", "Y. Zhang", "G. Lai", "Y. Liu", "H. Zhang", "S. Ma"], "venue": "Proceedings of the 24th International Conference on World Wide Web. ACM, 2015, pp. 1373\u20131383.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Informed Recommender: Basing Recommendations on Consumer Product Reviews", "author": ["S. Aciar", "D. Zhang", "S. Simoff", "J. Debenham"], "venue": "Intelligent Systems, vol. 22, no. 3, pp. 39\u201347, 2007.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Beyond the Stars: Exploiting Free-Text User Reviews to Improve the Accuracy of Movie Recommendations", "author": ["N. Jakob", "S.H. Weber", "M.C. Mller"], "venue": "TSA, 2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond the Stars: Improving Rating Predictions using Review Text Content", "author": ["G. Ganu", "N. Elhadad", "A. Marian"], "venue": "WebDB, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Catch the black sheep: unified framework for shilling attack detection based on fraudulent action propagation", "author": ["Y. Zhang", "Y. Tan", "M. Zhang", "Y. Liu", "C. Tat-Seng", "S. Ma"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence. AAAI Press, 2015, pp. 2408\u20132414.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The vast amount of items in various web-based applications has made it an essential task to construct reliable Personalized Recommender Systems (PRS) [1, 2].", "startOffset": 150, "endOffset": 156}, {"referenceID": 1, "context": "The vast amount of items in various web-based applications has made it an essential task to construct reliable Personalized Recommender Systems (PRS) [1, 2].", "startOffset": 150, "endOffset": 156}, {"referenceID": 2, "context": "With the ability to leverage the wisdom of crowds, Collaborative Filtering (CF)-based [3, 4] approaches have achieved significant success and wide application, especially for those Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7, 8, 9, 10, 11, 12,", "startOffset": 86, "endOffset": 92}, {"referenceID": 3, "context": "With the ability to leverage the wisdom of crowds, Collaborative Filtering (CF)-based [3, 4] approaches have achieved significant success and wide application, especially for those Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7, 8, 9, 10, 11, 12,", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "With the ability to leverage the wisdom of crowds, Collaborative Filtering (CF)-based [3, 4] approaches have achieved significant success and wide application, especially for those Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7, 8, 9, 10, 11, 12,", "startOffset": 208, "endOffset": 211}, {"referenceID": 14, "context": "While the ratings act as integrated indicators of user attitudes towards products, the reviews serve as more detailed explanations of what aspects users care about and why the corresponding rating is made [15, 16].", "startOffset": 205, "endOffset": 213}, {"referenceID": 15, "context": "While the ratings act as integrated indicators of user attitudes towards products, the reviews serve as more detailed explanations of what aspects users care about and why the corresponding rating is made [15, 16].", "startOffset": 205, "endOffset": 213}, {"referenceID": 16, "context": "As such, the application of Topic Models [17] has gained attention to leverage the textual reviews for personalized recommendation, especially the frequently used Latent Dirichlet Allocation (LDA) [18] technique and its variants, for their ability to extract latent topics from reviews that represent the actual factors users care about when making numerical ratings [19, 20, 21].", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "As such, the application of Topic Models [17] has gained attention to leverage the textual reviews for personalized recommendation, especially the frequently used Latent Dirichlet Allocation (LDA) [18] technique and its variants, for their ability to extract latent topics from reviews that represent the actual factors users care about when making numerical ratings [19, 20, 21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 18, "context": "As such, the application of Topic Models [17] has gained attention to leverage the textual reviews for personalized recommendation, especially the frequently used Latent Dirichlet Allocation (LDA) [18] technique and its variants, for their ability to extract latent topics from reviews that represent the actual factors users care about when making numerical ratings [19, 20, 21].", "startOffset": 367, "endOffset": 379}, {"referenceID": 19, "context": "As such, the application of Topic Models [17] has gained attention to leverage the textual reviews for personalized recommendation, especially the frequently used Latent Dirichlet Allocation (LDA) [18] technique and its variants, for their ability to extract latent topics from reviews that represent the actual factors users care about when making numerical ratings [19, 20, 21].", "startOffset": 367, "endOffset": 379}, {"referenceID": 20, "context": "As such, the application of Topic Models [17] has gained attention to leverage the textual reviews for personalized recommendation, especially the frequently used Latent Dirichlet Allocation (LDA) [18] technique and its variants, for their ability to extract latent topics from reviews that represent the actual factors users care about when making numerical ratings [19, 20, 21].", "startOffset": 367, "endOffset": 379}, {"referenceID": 18, "context": "This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].", "startOffset": 174, "endOffset": 198}, {"referenceID": 19, "context": "This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].", "startOffset": 174, "endOffset": 198}, {"referenceID": 21, "context": "This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].", "startOffset": 174, "endOffset": 198}, {"referenceID": 22, "context": "This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].", "startOffset": 174, "endOffset": 198}, {"referenceID": 23, "context": "This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].", "startOffset": 174, "endOffset": 198}, {"referenceID": 24, "context": "This further leads to the recent research direction to bridge the LFM and LDA models, which makes use of the ratings and reviews collectively for personalized recommendation [19, 20, 22, 23, 24, 25].", "startOffset": 174, "endOffset": 198}, {"referenceID": 18, "context": ", to transform the latent factors in LFM to topic distributions in LDA through manually designed randomization functions such as logistic normalization [19], or to assume the factors and topics be the same vector sampled from mixture Gaussian distributions [20], etc.", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": ", to transform the latent factors in LFM to topic distributions in LDA through manually designed randomization functions such as logistic normalization [19], or to assume the factors and topics be the same vector sampled from mixture Gaussian distributions [20], etc.", "startOffset": 257, "endOffset": 261}, {"referenceID": 3, "context": "1 Latent Factor Models (LFM) Latent Factor Models (LFM) [4] attempt to encode user and item preferences in a latent factor space so as to estimate the user-item relations for rating prediction, which account for many of the frequently used Matrix Factorization (MF) [6] techniques.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "1 Latent Factor Models (LFM) Latent Factor Models (LFM) [4] attempt to encode user and item preferences in a latent factor space so as to estimate the user-item relations for rating prediction, which account for many of the frequently used Matrix Factorization (MF) [6] techniques.", "startOffset": 266, "endOffset": 269}, {"referenceID": 25, "context": "Among those, a \u2018standard\u2019 and representative formalization [26] predicts the user-item ratings ru,i with user/item biases and latent factors by,", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "Gradient Descent (SGD) or Alternating Least Squares (ALS) [26].", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "3 Randomization Function Let \u03b3 \u2208 R be an arbitrary vector and \u03b8 \u2208 [0, 1] be a stochastic vector, where their dimensions are the same K as the latent factors \u03b3u, \u03b3i and latent topics \u03b8d in the previous subsections.", "startOffset": 66, "endOffset": 72}, {"referenceID": 18, "context": "For example, in [19] the authors designed a randomization function as:", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "where \u221d denotes a positive correlation, and we leave out the parameters \u03b1, \u03b2u and \u03b2i because they are constants given a user and an item in model learning [26].", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "LDA applies an indirect causal effect from users to items via latent topics [18], which means that user u and item i are conditionally independent given topic k, i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Frequently used examples are the normalization-based randomization functions, which normalize the elements of \u03b3 to construct \u03b8 so that they sum to one [19, 20, 22, 23].", "startOffset": 151, "endOffset": 167}, {"referenceID": 19, "context": "Frequently used examples are the normalization-based randomization functions, which normalize the elements of \u03b3 to construct \u03b8 so that they sum to one [19, 20, 22, 23].", "startOffset": 151, "endOffset": 167}, {"referenceID": 21, "context": "Frequently used examples are the normalization-based randomization functions, which normalize the elements of \u03b3 to construct \u03b8 so that they sum to one [19, 20, 22, 23].", "startOffset": 151, "endOffset": 167}, {"referenceID": 22, "context": "Frequently used examples are the normalization-based randomization functions, which normalize the elements of \u03b3 to construct \u03b8 so that they sum to one [19, 20, 22, 23].", "startOffset": 151, "endOffset": 167}, {"referenceID": 18, "context": "In [19] for example, a logistic normalization randomization function as in Eq.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "It is known that a good performance on rating prediction does not necessarily guarantee a satisfactory performance of top-N recommendation by ranking the items in descending order of the predicted ratings [27].", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "To do so, we select four representative feature words from restaurant reviews (Mutton, Seafood, Spicy, and Price) [28, 29], and calculate the percentage that reviews in each city discussing a feature against the total number of reviews discussing the feature in the whole corpus, which represent the regional distributions of a feature among the major cities across mainland China.", "startOffset": 114, "endOffset": 122}, {"referenceID": 28, "context": "To do so, we select four representative feature words from restaurant reviews (Mutton, Seafood, Spicy, and Price) [28, 29], and calculate the percentage that reviews in each city discussing a feature against the total number of reviews discussing the feature in the whole corpus, which represent the regional distributions of a feature among the major cities across mainland China.", "startOffset": 114, "endOffset": 122}, {"referenceID": 29, "context": "a city, and the grey-scale color is proportional to its percentage of discussing a feature [30], as shown in Figure 1.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "EFM: The Explicit Factor Model presented in [15], which is the state-of-the-art recommendation approach based on textual reviews by sentiment analysis.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "HFT: The Hidden Factors and Topics model in [19], which is the state-of-the-art method that takes advantages of both LFM and LDA by applying vectorlevel randomizations (Eq.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "4 Top-N Recommendation In this subsection, we explore the performance of our approach in more practical top-N recommendation tasks, and make comparison with the following baseline methods: WRMF: Weighted Regularized Matrix Factorization [31], which is similar to LFM but applies weighted negative sampling to benefit top-N recommendations.", "startOffset": 237, "endOffset": 241}, {"referenceID": 31, "context": "for MF [32], which is the state-of-the-art algorithm for top-N recommendation based only on numerical ratings.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications in the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications in the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.", "startOffset": 190, "endOffset": 193}, {"referenceID": 32, "context": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications in the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.", "startOffset": 332, "endOffset": 336}, {"referenceID": 33, "context": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications in the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.", "startOffset": 379, "endOffset": 383}, {"referenceID": 34, "context": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications in the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.", "startOffset": 428, "endOffset": 432}, {"referenceID": 35, "context": "With the continuous growth of various online items across a vast range of the Web, Personalized Recommender Systems (PRS) [1] have set their missions to save users from information overload [2], and they have been widely integrated into various online applications in the forms of, for example, product recommendation in e-commerce [33], friend recommendation in social networks [34], news article recommendation in web portals [35], and video recommendation in video sharing websites [36], etc.", "startOffset": 485, "endOffset": 489}, {"referenceID": 36, "context": "Early systems of personalized recommendations rely on content-based approaches [37], which construct the user/item content profiles and make recommendation by paring users with the contently similar items.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Besides, they usually require a large amount of expensive human annotations [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "This further leads to the prospering of Collaborative Filtering (CF)-based recommendation algorithms [4, 3] that leverage the wisdom of the crowds.", "startOffset": 101, "endOffset": 107}, {"referenceID": 2, "context": "This further leads to the prospering of Collaborative Filtering (CF)-based recommendation algorithms [4, 3] that leverage the wisdom of the crowds.", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "With widely recognized performance in rating prediction, scalability, and computational efficiency, the Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7] techniques for CF have been extensively investigated by the research community, and widely applied in practical systems.", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "With widely recognized performance in rating prediction, scalability, and computational efficiency, the Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7] techniques for CF have been extensively investigated by the research community, and widely applied in practical systems.", "startOffset": 170, "endOffset": 176}, {"referenceID": 6, "context": "With widely recognized performance in rating prediction, scalability, and computational efficiency, the Latent Factor Models (LFM) [5] based on Matrix Factorization (MF) [6, 7] techniques for CF have been extensively investigated by the research community, and widely applied in practical systems.", "startOffset": 170, "endOffset": 176}, {"referenceID": 25, "context": "Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.", "startOffset": 247, "endOffset": 254}, {"referenceID": 37, "context": "Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.", "startOffset": 247, "endOffset": 254}, {"referenceID": 38, "context": "Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.", "startOffset": 303, "endOffset": 311}, {"referenceID": 39, "context": "Perhaps the most early and representative formalization of LFM for recommendation dates back to Koren et al [26], and other variants for personalization include Non-negative Matrix Factorization (NMF) [8], Probabilistic Matrix Factorization (PMF) [9, 38], and Maximum Margin Matrix Factorization (MMMF) [39, 40], etc.", "startOffset": 303, "endOffset": 311}, {"referenceID": 14, "context": "Despite the important success in rating prediction, the CF approaches based solely on the numerical ratings suffer from the problems of explainability [15], coldstart [41, 42], and the difficulty to provide more specific recommendations that meet targeted item aspects [43].", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "Despite the important success in rating prediction, the CF approaches based solely on the numerical ratings suffer from the problems of explainability [15], coldstart [41, 42], and the difficulty to provide more specific recommendations that meet targeted item aspects [43].", "startOffset": 167, "endOffset": 175}, {"referenceID": 41, "context": "Despite the important success in rating prediction, the CF approaches based solely on the numerical ratings suffer from the problems of explainability [15], coldstart [41, 42], and the difficulty to provide more specific recommendations that meet targeted item aspects [43].", "startOffset": 167, "endOffset": 175}, {"referenceID": 42, "context": "Despite the important success in rating prediction, the CF approaches based solely on the numerical ratings suffer from the problems of explainability [15], coldstart [41, 42], and the difficulty to provide more specific recommendations that meet targeted item aspects [43].", "startOffset": 269, "endOffset": 273}, {"referenceID": 26, "context": "Besides, related research results show that the performance on numerical rating prediction does not necessarily relate to the performance on practical top-N recommendations [27], and that the numerical star ratings may not always be a reliable indicator of users\u2019 attitudes towards items [44].", "startOffset": 173, "endOffset": 177}, {"referenceID": 43, "context": "Besides, related research results show that the performance on numerical rating prediction does not necessarily relate to the performance on practical top-N recommendations [27], and that the numerical star ratings may not always be a reliable indicator of users\u2019 attitudes towards items [44].", "startOffset": 288, "endOffset": 292}, {"referenceID": 44, "context": "To alleviate these problems, researchers have been investigating the incorporation of textual reviews for recommendation [45, 46, 47, 48], which is another important information source beyond the star ratings in many systems.", "startOffset": 121, "endOffset": 137}, {"referenceID": 45, "context": "To alleviate these problems, researchers have been investigating the incorporation of textual reviews for recommendation [45, 46, 47, 48], which is another important information source beyond the star ratings in many systems.", "startOffset": 121, "endOffset": 137}, {"referenceID": 46, "context": "To alleviate these problems, researchers have been investigating the incorporation of textual reviews for recommendation [45, 46, 47, 48], which is another important information source beyond the star ratings in many systems.", "startOffset": 121, "endOffset": 137}, {"referenceID": 47, "context": "To alleviate these problems, researchers have been investigating the incorporation of textual reviews for recommendation [45, 46, 47, 48], which is another important information source beyond the star ratings in many systems.", "startOffset": 121, "endOffset": 137}, {"referenceID": 48, "context": "Early approaches rely on manually extracted item aspects from reviews for more informed recommendation [49, 43] and rating prediction [50, 51], which improved the performance but also required extensive human participations.", "startOffset": 103, "endOffset": 111}, {"referenceID": 42, "context": "Early approaches rely on manually extracted item aspects from reviews for more informed recommendation [49, 43] and rating prediction [50, 51], which improved the performance but also required extensive human participations.", "startOffset": 103, "endOffset": 111}, {"referenceID": 49, "context": "Early approaches rely on manually extracted item aspects from reviews for more informed recommendation [49, 43] and rating prediction [50, 51], which improved the performance but also required extensive human participations.", "startOffset": 134, "endOffset": 142}, {"referenceID": 50, "context": "Early approaches rely on manually extracted item aspects from reviews for more informed recommendation [49, 43] and rating prediction [50, 51], which improved the performance but also required extensive human participations.", "startOffset": 134, "endOffset": 142}, {"referenceID": 18, "context": "As a results, researchers recently have begun to investigate the possibility of integrating the automatic topic modeling techniques on textual reviews and the latent factor modeling approach on numerical ratings for boosted recommendation, and have achieved appealing results [19, 20, 22, 23, 52].", "startOffset": 276, "endOffset": 296}, {"referenceID": 19, "context": "As a results, researchers recently have begun to investigate the possibility of integrating the automatic topic modeling techniques on textual reviews and the latent factor modeling approach on numerical ratings for boosted recommendation, and have achieved appealing results [19, 20, 22, 23, 52].", "startOffset": 276, "endOffset": 296}, {"referenceID": 21, "context": "As a results, researchers recently have begun to investigate the possibility of integrating the automatic topic modeling techniques on textual reviews and the latent factor modeling approach on numerical ratings for boosted recommendation, and have achieved appealing results [19, 20, 22, 23, 52].", "startOffset": 276, "endOffset": 296}, {"referenceID": 22, "context": "As a results, researchers recently have begun to investigate the possibility of integrating the automatic topic modeling techniques on textual reviews and the latent factor modeling approach on numerical ratings for boosted recommendation, and have achieved appealing results [19, 20, 22, 23, 52].", "startOffset": 276, "endOffset": 296}, {"referenceID": 51, "context": "As a results, researchers recently have begun to investigate the possibility of integrating the automatic topic modeling techniques on textual reviews and the latent factor modeling approach on numerical ratings for boosted recommendation, and have achieved appealing results [19, 20, 22, 23, 52].", "startOffset": 276, "endOffset": 296}], "year": 2016, "abstractText": "The research of personalized recommendation techniques today has mostly parted into two mainstream directions, i.e., the factorization-based approaches and topic models. Practically, they aim to benefit from the numerical ratings and textual reviews, correspondingly, which compose two major information sources in various real-world systems. However, although the two approaches are supposed to be correlated for their same goal of accurate recommendation, there still lacks a clear theoretical understanding of how their objective functions can be mathematically bridged to leverage the numerical ratings and textual reviews collectively, and why such a bridge is intuitively reasonable to match up their learning procedures for the rating prediction and top-N recommendation tasks, respectively. In this work, we exposit with mathematical analysis that, the vector-level randomization functions to coordinate the optimization objectives of factorizational and topic models unfortunately do not exist at all, although they are usually pre-assumed and intuitively designed in the literature. Fortunately, we also point out that one can avoid the seeking of such a randomization function by optimizing a Joint Factorizational Topic (JFT) model directly. We apply our JFT model to restaurant recommendation, and study its performance in both normal and cross-city recommendation scenarios, where the latter is an extremely difficult task for its inherent cold-start nature. Experimental results on real-world datasets verified the appealing performance of our approach against previous methods, on both rating prediction and top-N recommendation tasks.", "creator": "LaTeX with hyperref package"}}}