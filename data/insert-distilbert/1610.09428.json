{"id": "1610.09428", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Beyond Exchangeability: The Chinese Voting Process", "abstract": "many more online communities present user - contributed responses such as reviews polls of products and answers to questions. user - provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity strengths of responses and the polarity of existing participant votes. we propose the chinese voting process ( cvp ) which models the evolution of helpfulness votes as a human self - reinforcing process dependent explicitly on position similarity and presentation biases. we evaluate this model on ten amazon product reviews 365 and more research than 80 stackexchange forums, roughly measuring the intrinsic quality of individual responses and behavioral coefficients preferences of different communities.", "histories": [["v1", "Fri, 28 Oct 2016 23:38:22 GMT  (111kb,D)", "http://arxiv.org/abs/1610.09428v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.SI", "authors": ["moontae lee", "seok hyun jin", "david m mimno"], "accepted": true, "id": "1610.09428"}, "pdf": {"name": "1610.09428.pdf", "metadata": {"source": "CRF", "title": "Beyond Exchangeability: The Chinese Voting Process", "authors": ["Moontae Lee", "Seok Hyun Jin"], "emails": ["moontae@cs.cornell.edu", "sj372@cornell.edu", "mimno@cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "With the expansion of online social platforms, user-generated content has become increasingly influential. Customer reviews in e-commerce like Amazon are often more helpful than editorial reviews [14], and question answers in Q&A forums such as StackOverflow and MathOverflow are highly useful for coders and researchers [9, 18]. Due to the diversity and abundance of user content, promoting better access to more useful information is critical for both users and service providers. Helpfulness voting is a powerful means to evaluate the quality of user responses (i.e., reviews/answers) by the wisdom of crowds. While these votes are generally valuable in aggregate, estimating the true quality of the responses is difficult because users are heavily influenced by previous votes. We propose a new model that is capable of learning the intrinsic quality of responses by considering their social contexts and momentum.\nPrevious work in self-reinforcing social behaviors shows that although inherent quality is an important factor in overall ranking, users are susceptible to position bias [12, 13]. Displaying items in an order affects users: top-ranked items get more popularity, while low-ranked items remain in obscurity. We find that sensitivity to orders also differs across communities: some value a range of opinions, while others prefer a single authoritative answer. Summary information displayed together can lead to presentation bias [19]. As the current voting scores are visibly presented with responses, users inevitably perceive the score before reading the contents of responses. Such exposure could immediately nudge user evaluations toward the majority opinion, making high-scored responses more attractive. We also find that the relative length of each response affects the polarity of future votes.\nStandard discrete models for self-reinforcing process include the Chinese Restaurant Process and the P\u00f3lya urn model. Since these models are exchangeable, the order of events does not affect the probability of a sequence. However, Table 1 suggests how different contexts\nof votes cause different impacts. While the four sequences have equal numbers of positive and\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 42\n8v 1\n[ cs\n.L G\n] 2\n8 O\nct 2\nnegative votes in aggregate, the fourth votes in the first and last responses are given against a clear majority opinion. Our model treats objection as a more challenging decision, thereby deserving higher weight. In contrast, the middle two sequences receive alternating votes. As each vote is a relatively weaker disagreement, the underlying quality is moderate compared to the other two responses. Furthermore, if these are responses to one item, the order between them also matters. If the initial three votes on the fourth response pushed its display position to the next page, for example, it might not have a chance to get future votes, which recover its reputation.\nThe Chinese Voting Process (CVP) models generation of responses and votes, formalizing the evolution of helpfulness under positional and presentational reinforcement. Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots. The resulting model shows significant improvements in predictive probability for helpfulness votes, especially in the critical early stages of a trajectory. We find that the CVP estimated intrinsic quality ranks responses better than existing system rank, correlating orderly with the sentiment of comments associated with each response. Finally, we qualitatively compare different characteristics of self-reinforcing behavior between communities using two learned coefficients: Trendiness and Conformity. The two-dimensional embedding in Figure 1 characterizes different opinion dynamics from Judaism to Javascript (in StackOverflow).\nRelated work. There is strong evidence that helpfulness voting is socially influenced. Helpfulness ratings on Amazon product reviews differ significantly from independent human annotators [8]. Votes are generally more positive, and the number of votes decreases exponentially based on displayed page position. Review polarity is biased towards matching the consensus opinion [4]: when two reviews contain essentially the same text but differ in star rating, the review closer to the consensus star rating is considered more helpful. There is also evidence that users vote strategically to correct perceived mismatches in review rank [16]. Many studies have attempted to predict helpfulness given review-content features [7, 5, 11, 10, 15]. Each of these examples predicts helpfulness based on text, star-ratings, sales, and badges, but only at a single snapshot. Our work differs in two ways. First, we combine data on Amazon helpfulness votes from [16] with a much larger collection of helpfulness votes from 82 StackExchange forums. Second, instead of considering text-based features (which we hold out for evaluation) within a single snapshot, we attempt to predict the next vote at each stage based on the previous voting trajectory over multiple snapshots without considering textual contents."}, {"heading": "2 The Chinese Voting Process", "text": "Our goal is to model helpfulness voting as a two-phase self-reinforcing stochastic process. In the selection phase, each user either selects an existing response based on their positions or writes a new\nresponse. The positional reinforcement is inspired by the Chinese Restaurant Process (CRP) and Distance Dependent Chinese Restaurant Process (ddCRP). In the voting phase, when one response is selected, the user chooses one of the two feedback options: a positive or negative vote based on the intrinsic quality and the presentational factors. The presentational reinforcement is modeled by a log-linear model with time-varying features based on the P\u00f3lya urn model. The CVP implements the-rich-get-richer dynamics as an interplay of these two preferential reinforcements, learning latent qualities of individual responses as inspired in Table 1. Specifically, each user at time t interested in the item i follows the generative story in Table 2."}, {"heading": "2.1 Selection phase", "text": "The CRP [1, 2] is a self-reinforcing decision process over an infinite discrete set. For each item (product/question) i, the first user writes a new response (review/answer). The t-th subsequent user can choose an existing response j out of J(t\u22121)i possible responses with probability proportional to the number of votes n(t\u22121)j given to the response j by time t\u2212 1, whereas the probability of writing a new response J(t\u22121)i + 1 is proportional to a constant \u03b1. While the CRP models self-reinforcement \u2014 each vote for a response makes that response more likely to be selected later \u2014 there is evidence that the actual selection rate in an ordered list decays with display rank [6]. Since such rankings are mechanism-specific and not always clearly known in advance, we need a more flexible model that can specify various degrees of positional preference. The ddCRP [3] introduces a function f that decays with respect to some distance measure. In our formulation, the distance function varies over time and is further configurable with respect to the specific interface of service providers.\nSpecifically, the function f(t)i (j) in the CVP evaluates the popularity of the j-th response in the item i at time t. Since we assume that popularity of responses is decided by their positional accessibility, we can parametrize f to be inversely proportional to their display ranks. The exponent \u03c4 determines sensitivity to popularity in the selection phase by controlling the degree of harmonic penalization over ranks. Larger \u03c4 > 0 indicates that users are more sensitive to trendy responses displayed near the top. If \u03c4 < 0, users often select low-ranked responses over high-ranked ones for some reasons.1 Note that even if the user at time t does not vote on the j-th response, f(t)i (j) could be different from f (t\u22121) i (j) in the CVP, 2 whereas n(t)ij = n (t\u22121) ij in the CRP. Thus one can view the selection phase of the CVP as a non-exchangeable extension of the CRP via a time-varying function f ."}, {"heading": "2.2 Voting phase", "text": "We next construct a self-reinforcing process for the inner voting phase. The P\u00f3lya urn model is a self-reinforcing decision process over a finite discrete set, but because it is exchangeable, it is unable to capture contextual information encoded in each a sequence of votes. We instead use a log-linear formulation with the urn-based features, allowing other presentational features to be flexibly incorporated based on the modeler\u2019s observations.\nEach response initially has x = x(0) positive and y = y(0) negative votes, which could be fractional pseudo-votes. For each draw of a vote, we return w + 1 votes with the same polarity, thus selfreinforcing when w > 0. The following Table 3 shows time-evolving positive/negative ratios r (t) j = x (t) j /(x (t) j + y (t) j ) and s (t) j = y (t) j /(x (t) j + y (t) j ) of the first two responses: j \u2208 {1, 2} in Table 1 with the corresponding ratio gain \u2206(t)j = r (t) j \u2212 r (t\u22121) j (if v (t) j = 1 or +) or s (t) j \u2212 s (t\u22121) j (if v (t) j = 0 or \u2212).\n1This sometimes happens especially in the early stage when only a few responses exist. 2Say the rank of another response j\u2032 was lower than j\u2019s at time t\u2212 1. If t-th vote given to the response j\u2032\nraises its rank higher than the rank of the response j, then f(t)i (j) < f (t\u22121) i (j) assuming \u03c4 > 0.\nIn this toy setting, the polarity of a vote to a response is an outcome of its intrinsic quality as well as presentational factors: positive and negative votes. Thus we model each sequence of votes by `2-regularized logistic regression with the latent intrinsic quality and the P\u00f3lya urn ratios.3\nmax \u03b8 log T\u220f t=2 logit\u22121 ( qTj + \u03bbr (t\u22121) j + \u00b5s (t\u22121) j ) \u2212 1 2 \u2016\u03b8\u201622 where \u03b8 = ( qTj , \u03bb, \u00b5 ) (1)\nThe {qTj } in the Table 3 shows the result from solving (1) up to T -th votes for each j \u2208 {1, 2}. The initial vote given at t = 1 is disregarded in the training due to its arbitrariness from the uniform prior (x0 = y0). Since it is quite possible to have only positive or only negative votes, Gaussian regularization is necessary. Note that using the urn-based ratio features is essential to encode contextual information. If we instead use raw count features (only the numerators of rj and sj), for example in the first response, the estimated quality qT1 keeps increasing even after getting negative votes from time 4 to 6. Log raw count features are unable to infer the negative quality.\nIn the first response, \u2206(t)1 shows the decreasing gain in positive ratios from t = 1 to 3 and in negative ratios from t = 4 to 6, whereas it gains a relatively large momentum at the first negative vote when t = 4. \u2206(t)2 converges to 0 in the 2nd response, implying that future votes have less effect than earlier votes for alternating +/\u2212 votes. qT2 also converges to 0 as we expect neutral quality in the limit. Overall the model is capable of learning intrinsic quality as desired in Table 1 where relative gains can be further controlled by tuning the initial pseudo-votes (x, y).\nIn the real setting, the polarity score function g(t)i (j) in the CVP evaluates presentational factors of the j-th response in the item i at time t. Because we adopt a log-linear formulation, one can easily add additional information about responses. In addition to the positive ratio r(t)ij and the negative ratio s(t)ij , g also contains a length feature u (t) ij (as given in Table 2), which is the relative length of the response j against the average length of responses in the item i at particular time t. Users in some items may prefer shorter responses than longer ones for brevity, whereas users in other items may blindly believe that longer responses are more credible before reading their contents. The parameter \u03bdi explains length-wise preferential idiosyncrasy as a per-item bias: \u03bdi < 0 means a preference toward the shorter responses. Note that g(t)i (j) could be different from g (t\u22121) i (j) even if the user at time t does not choose to vote.4 All together, the voting phase of the CVP generates non-exchangeable votes."}, {"heading": "3 Inference", "text": "Each phase of the CVP depends on the result of all previous stages, so decoupling these related problems is crucial for efficient inference. We need to estimate community-level parameters, itemlevel length preferences, and response-level intrinsic qualities. The graphical model of the CVP and corresponding parameters to estimate are illustrated in Table 4. We further compute two communitylevel behavioral coefficients: Trendiness and Conformity, which are useful summary statistics for exploring different voting patterns and explaining macro characteristics across different communities.\n3One might think (1) can be equivalently achievable with only two parameters because of r(t)j + s (t) j = 1 for all t. However, such reparametrization adds inconsistent translations to qTj and makes it difficult to interpret different inclinations between positive and negative votes for various communities.\n4If a new response is written at time t, u(t)ij 6= u (t\u22121) ij as the new response changes the average length.\nParameter inference. The goal is to infer parameters \u03b8 = {{qij}, \u03bb, \u00b5, {\u03bdi}}. We sometimes use f and g instead to compactly indicate parameters associated to each function. The likelihood of one CVP step in the item i at time t is L(t)i (\u03c4,\u03b8;\u03b1, \u03c3) ={\n\u03b1\n\u03b1+ \u2211J(t\u22121)i\nj=1 f (t\u22121) i (j)\nN (q i,z\n(t) i\n; 0, \u03c32) } 1[z (t) i =J (t\u22121) i +1] { f (t\u22121) i (z (t) i )\n\u03b1+ \u2211J(t\u22121)i\nj=1 f (t\u22121) i (j)\np(v (t) i |qi,z(t)i , g (t\u22121) i (j))\n} 1[z\n(t) i \u2264J (t\u22121) i ]\nwhere the two terms correspond to writing a new response and selecting an existing response to vote. The fractions in each term respectively indicate the probability of writing a new response and choosing existing responses in the selection phase. The other two probability expression in each term describe quality sampling from a normal distribution and the logistic regression in the voting phase.\nIt is important to note that our setting differs from many CRP-based models. The CRP is typically used to represent a non-parametric prior over the choice of latent cluster assignments that must themselves be inferred from noisy observations. In our case, the result of each choice is directly observable because we have the complete trajectory of helpfulness votes. As a result, we only need to infer the continuous parameters of the process, and not combinatorial configurations of discrete variables. Since we know the complete trajectory where the rank inside the function f is a part of the true observations, we can view each vote as an independent sample. Denoting the last timestamp of the item i by Ti, the log-likelihood becomes `(\u03c4,\u03b8;\u03b1, \u03c3) = \u2211m i=1 \u2211Ti t=1 logL (t) i and is further separated into two pieces:\n`v(\u03b8;\u03c3) = m\u2211 i=1 Ti\u2211 t=1 { 1[write] \u00b7 logN (q i,z (t) i ; 0, \u03c32) + 1[choose] \u00b7 log p(v(t)i |qi,z(t)i , g (t\u22121) i (j)) } , (2)\n`s(\u03c4 ;\u03b1) = m\u2211 i=1 Ti\u2211 t=1 { 1[write] \u00b7 log \u03b1 \u03b1+ \u2211J(t\u22121)i j=1 f (t\u22121) i (j) + 1[choose] \u00b7 log f (t\u22121) i (z (t) i ) \u03b1+ \u2211J(t\u22121)i j=1 f (t\u22121) i (j) } .\nInferring a whole trajectory based only on the final snapshots would likely be intractable for a non-exchangeable model. Due to the continuous interaction between f and g for every time step, small mis-predictions in the earlier stages will cause entirely different configurations. Moreover the rank function inside f is in many cases site-specific.5 It is therefore vital to observe all trajectories of random variables {z(t)i , v (t) i }: decoupling f and g reduces the inference problem into estimating parameters separately for the selection phase and the voting phase. Maximizing `v can be efficiently solved by `2-regularized logistic regression as demonstrated for (1). If the hyper-parameter \u03b1 is fixed, maximizing `s becomes a convex optimization because \u03c4 appears in both the numerator and the denominator. Since the gradient for each parameter in \u03b8 is obvious, we only include the gradient of ` (t) s,i for the particular item i at time t with respect to \u03c4 . Then \u2202`s \u2202\u03c4 = \u2211m i=1 \u2211Ti t=1 \u2202` (t) s,i/\u2202\u03c4 .\n\u2202` (t) s,i\n\u2202\u03c4 =\n1\n\u03c4\n{ 1[z\n(t) i \u2264 J (t\u22121) i ] \u00b7\nf (t\u22121) i (z (t) i ) log f (t\u22121) i (z (t) i )\nf (t\u22121) i (z (t) i )\n\u2212 \u2211J(t\u22121)i j=1 f (t\u22121) i (j) log f (t\u22121) if (j)\n\u03b1+ \u2211J(t\u22121)i j=1 f (t\u22121) i (j)\n} (3)\n5We generally know that Amazon decides the display order by the portion of positive votes and the total number of votes on each response, but the relative weights between them are not known. We do not know how StackExchange forums break ties, which affects highly in the early stages of voting.\nBehavioral coefficients. To succinctly measure overall voting behaviors across different communities, we propose two community-level coefficients. Trendiness indicates the sensitivity to positional popularity in the selection phase. While the community-level \u03c4 parameter renders Trendiness simply to avoid overly-complicated models, one can easily extend the CVP to have per-item \u03c4i to better fit the data. In that case, Trendiness would be a summary statistics for {\u03c4i}. Conformity captures users\u2019 receptiveness to prevailing polarity in the voting phase. To count every single vote, we define Conformity to be a geometric mean of odds ratios between majority-following votes and majoritydisagreeing votes. Let Vi be the set of time steps when users vote rather than writing responses in the item i. Say n is the total number of votes across all items in the target community. Then Conformity is defined as\n\u03ba = { m\u220f i=1 \u220f t\u2208Vi (P (v(t+1)i = 1|qti,z(t+1)i , \u03bbt, \u00b5t, \u03bdti ) P (v (t+1) i = 0|qt\ni,z (t+1) i\n, \u03bbt, \u00b5t, \u03bdti )\n)h(t)i }1/n where h(t)i = { 1 (n +(t) ij \u2265 n \u2212(t) ij )\n\u22121 (n+(t)ij < n \u2212(t) ij )\n.\nTo compute Conformity \u03ba, we need to learn \u03b8t = {qtij , \u03bbt, \u00b5t, \u03bdti} for each t, which is a set of parameters learned on the data only up to the time t. This is because the user at time t cannot see any future which will be given later than the time t. Note that \u03b8t+1 can be efficiently learned by warm-starting at \u03b8t. In addition, while positive votes are mostly dominant in the end, the dominant mood up to time t could be negative, exactly when the user at time t+ 1 tries to vote. In this case, h (t) i becomes \u22121, inverting the fraction to be the ratio of following the majority against the minority. By summarizing learned parameters in terms of two coefficients (\u03c4, \u03ba), we can compare different selection/voting behaviors for various communities."}, {"heading": "4 Experiments", "text": "We evaluate the CVP on product reviews from Amazon and 82 issue-specific forums from the StackExchange network. The Amazon dataset [16] originally consisted of 595 products with daily snapshots of writing/voting trajectories from Oct 2012 to Mar 2013. After eliminating duplicate products6 and products with fewer than five reviews or fragmented trajectories,7 363 products are left. For the StackExchange dataset8, we filter out questions from each community with fewer than five answers besides the answer chosen by the question owner.9 We drop communities with fewer than 100 questions after pre-processing. Many of these are \u201cMeta\u201d forums where users discuss policies and logistics for their original forums.\n6Different seasons of the same TV shows have different ASIN codes but share the same reviews. 7If the number of total votes between the last snapshot of the early fragment and the first snapshot of the later fragment is less than 3, we fill in the missing information simply with the last snapshot of the earlier fragment. 8Dataset and statistics are available at https://archive.org/details/stackexchange. 9The answer selected by the question owner is displayed first regardless of voting scores.\nPredictive analysis. In each community, our prediction task is to learn the model up to time t and predict the action at t + 1. We align all items at their initial time steps and compute the average negative log-likelihood of the next actions based on the current model. Since the complete trajectory enables us to separate the selection and voting phases in inference, we also measure the predictive power of these two tasks separately against their own baselines. For the selection phase, the baseline is the CRP, which selects responses proportional to the number of accumulated votes or writes a new response with the probability proportional to \u03b1.10 When t < 50, as shown in the first column of Table 5, the CVP significantly outperforms the CRP based on paired t-tests (two-tailed). Using the function f based on display rank and Trendiness parameter \u03c4 is indeed a more precise representation of positional accessibility. Especially in the early stages, users often select responses displayed at lower ranks with fewer votes. While the CRP has no ability to give high scores in these cases, the CVP properly models it by decreasing \u03c4 . The comparative advantage of the CVP declines as more votes become available and the correlation between display rank and the number of votes increases. For items with t \u2265 50, there is no significant difference between the two models as exemplified in the third column of Figure 2. These results are coherent across other communities (p > 0.07).\nImproving predictive power on the voting phase is difficult because positive votes dominate in every community. We compare the fully parametrized model to simpler partial models in which certain parameters are set to zero. For example, a model with all parameters but \u03bb knocked out is comparable to a plain P\u00f3lya Urn. As illustrated in the second column of Table 5, we verify that every sub-model is significantly different from the full model in all major communities based on one-way ANOVA test, implying that each feature adds distinctive and meaningful information. Having the item-specific length bias \u03bdi provides significant improvements as well as having intrinsic quality qij and current opinion counts \u03bb. While we omit the log-likelihood results with t \u2265 50, all model better predicts true polarity when t \u2265 50, because the log-linear model obtains a more robust estimate of community-level parameters as the model acquires more training samples.\nQuality analysis. The primary advantage of the CVP is its ability to learn \u201cintrinsic quality\u201d for each response that filters out noise from self-reinforcing voting processes. We validate these scores by comparing them to another source of user feedback: both StackExchange and Amazon allow users to attach comments to responses along with votes. For each response, we record the number of comments and the average sentiment of those comments as estimated by [17]. As a baseline, we\n10We fix \u03b1 to 0.5 after searching over a wide range of values.\nalso calculate the final display rank of each response, which we convert to a z-score to make it more comparable to the quality scores qij . After sorting responses based on display rank and quality rank, we measure the association between the two rankings and comment sentiment with linear regression. Results are shown for StackOverflow in Figure 2. As expected, highly-ranked responses have more comments, but we also find that there are more comments for both high and low values of intrinsic quality. Both better display rank and higher quality score qij are clearly associated with more positive comments (slope \u2208 [0.47, 0.64]), but the residuals of quality rank 0.012 are on average less than the half the residuals of display rank 0.028. In addition, we also calculate the \u201cbumpiness\u201d of these plots by computing the mean variation of two consecutive slopes between each adjacent pair of data points. Quality rank reduces bumpiness of display rank from 0.391 to 0.226 in average, implying the estimated intrinsic quality yields locally consistent ranking as well as globally consistent.11\nCommunity analysis. The 2D embedding in Figure 1 shows that we can compare and contrast the different evaluation cultures of communities using two inferred behavioral coefficients: Trendiness \u03c4 and Conformity \u03ba. Communities are sized according to the number of items and colored based on a manual clustering. Related communities collocate in the same neighborhood. Religion, scholarship, and meta-discussions cluster towards the bottom left, where users are interested in many different opinions, and are happy to disagree with each other. Going from left to right, communities become more trendy: users in trendier communities tend to select and vote mostly on already highly-ranked responses. Going from bottom to top, users become increasingly likely to conform to the\nmajority opinion on any given response. By comparing related communities we can observe that characteristics of user communities determine voting behavior more than technical similarity. Highly theoretical and abstract communities (cstheory) have low Trendiness but high Conformity. More applied, but still graduate-level, communities in similar fields (cs, mathoverflow, stats) show less Conformity but greater Trendiness. Finally, more practical homework-oriented forums (physics, math) are even more trendy. In contrast, users in english are trendy and debatable. Users in Amazon are most sensitive to trendy reviews and least afraid of voicing minority opinion.\nStackOverflow is by far the largest community, and it is reasonable to wonder whether the Trendiness parameter is simply a proxy for size. When we subdivide StackOverflow by programming languages however (see Figure 3), individual community averages can be distinguished, but they all remain in the same region. Javascript programmers are more satisfied with trendy responses than those using c/c++. Mobile developers tend to be more conformist, while Perl hackers are more likely to argue."}, {"heading": "5 Conclusions", "text": "Helpfulness voting is a powerful tool to evaluate user-generated responses such as product reviews and question answers. However such votes can be socially reinforced by positional accessibility and existing evaluations by other users. In contrast to many exchangeable random processes, the CVP takes into account sequences of votes, assigning different weights based on the context that each vote was cast. Instead of trying to model the response ordering function f , which is mechanism-specific and often changes based on service providers\u2019 strategies, we leverage the fully observed trajectories of votes, estimating the hidden intrinsic quality of each response and inferring two behavioral coefficients for community-level exploration. The proposed log-linear urn model is capable of generating nonexchangeable votes with great scalability to incorporate other factors such as length bias or other textual features. As we are more able to observe social interactions as they are occurring and not just summarized after the fact, we will increasingly be able to use models beyond exchangeability.\n11All numbers and p-values in paragraphs are weighted averages on all 83 communities, whereas Table 5 only includes results for the major communities and their own weighted averages due to space limits."}], "references": [{"title": "Exchangeability and related topics", "author": ["D.J. Aldous"], "venue": "In E\u0301cole d\u2019E\u0301te\u0301 St Flour", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D. Blei", "T. Griffiths", "M. Jordan", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Distance dependent chinese restaurant processes", "author": ["D.M. Blei", "P.I. Frazier"], "venue": "Journal of Machine Learning Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "How opinions are received by online communities: A case study on Amazon.Com helpfulness votes", "author": ["C. Danescu-Niculescu-Mizil", "G. Kossinets", "J. Kleinberg", "L. Lee"], "venue": "In Proceedings of World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Designing novel review ranking systems: Predicting the usefulness and impact of reviews", "author": ["A. Ghose", "P.G. Ipeirotis"], "venue": "In Proceedings of the Ninth International Conference on Electronic Commerce,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "F. Radlinski", "G. Gay"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Automatically assessing review helpfulness", "author": ["S.-M. Kim", "P. Pantel", "T. Chklovski", "M. Pennacchiotti"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Low-quality product review detection in opinion summarization", "author": ["J. Liu", "Y. Cao", "C.-Y. Lin", "Y. Huang", "M. Zhou"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Design lessons from the fastest q&a site in the west", "author": ["L. Mamykina", "B. Manoim", "M. Mittal", "G. Hripcsak", "B. Hartmann"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI \u201911,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Prediction of helpful reviews using emotion extraction", "author": ["L. Martin", "P. Pu"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "helpfulness\u2019 in online communities: A measure of message quality", "author": ["J. Otterbacher"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Experimental study of inequality and unpredictability in an artificial cultural", "author": ["M.J. Salganik", "P.S. Dodds", "D.J. Watts"], "venue": "market. Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Leading the herd astray: An experimental study of self-fulfilling prophecies in an artificial cultural mmrket", "author": ["M.J. Salganik", "D.J. Watts"], "venue": "Social Psychology Quarterly,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Buy it, try it, rate it: Study of consumer electronics purchase deicisions in the engagement era", "author": ["W. Shandwick"], "venue": "KRC Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Analyzing and mining comments and comment ratings on the social web", "author": ["S. Siersdorfer", "S. Chelaru", "J.S. Pedro", "I.S. Altingovde", "W. Nejdl"], "venue": "ACM Trans. Web,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Was this review helpful to you?: It depends! context and voting patterns in online content", "author": ["R. Sipos", "A. Ghosh", "T. Joachims"], "venue": "In International Conference on World Wide Web,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Collaborative problem solving: A study of mathoverflow", "author": ["Y.R. Tausczik", "A. Kittur", "R.E. Kraut"], "venue": "In Computer-Supported Cooperative Work and Social Computing, CSCW\u2019", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Beyond position bias: Examining result attractiveness as a source of presentation bias in clickthrough data", "author": ["Y. Yue", "R. Patel", "H. Roehrig"], "venue": "In Proceedings of the 19th International Conference on World Wide Web, WWW", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "Customer reviews in e-commerce like Amazon are often more helpful than editorial reviews [14], and question answers in Q&A forums such as StackOverflow and MathOverflow are highly useful for coders and researchers [9, 18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Customer reviews in e-commerce like Amazon are often more helpful than editorial reviews [14], and question answers in Q&A forums such as StackOverflow and MathOverflow are highly useful for coders and researchers [9, 18].", "startOffset": 214, "endOffset": 221}, {"referenceID": 17, "context": "Customer reviews in e-commerce like Amazon are often more helpful than editorial reviews [14], and question answers in Q&A forums such as StackOverflow and MathOverflow are highly useful for coders and researchers [9, 18].", "startOffset": 214, "endOffset": 221}, {"referenceID": 11, "context": "Previous work in self-reinforcing social behaviors shows that although inherent quality is an important factor in overall ranking, users are susceptible to position bias [12, 13].", "startOffset": 170, "endOffset": 178}, {"referenceID": 12, "context": "Previous work in self-reinforcing social behaviors shows that although inherent quality is an important factor in overall ranking, users are susceptible to position bias [12, 13].", "startOffset": 170, "endOffset": 178}, {"referenceID": 18, "context": "Summary information displayed together can lead to presentation bias [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 7, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 3, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 10, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 9, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 14, "context": "Whereas most previous work on helpfulness prediction [7, 5, 8, 4, 11, 10, 15] has involved a single snapshot, the CVP estimates intrinsic quality of responses solely from selection and voting trajectories over multiple snapshots.", "startOffset": 53, "endOffset": 77}, {"referenceID": 7, "context": "Helpfulness ratings on Amazon product reviews differ significantly from independent human annotators [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "Review polarity is biased towards matching the consensus opinion [4]: when two reviews contain essentially the same text but differ in star rating, the review closer to the consensus star rating is considered more helpful.", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "There is also evidence that users vote strategically to correct perceived mismatches in review rank [16].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Many studies have attempted to predict helpfulness given review-content features [7, 5, 11, 10, 15].", "startOffset": 81, "endOffset": 99}, {"referenceID": 4, "context": "Many studies have attempted to predict helpfulness given review-content features [7, 5, 11, 10, 15].", "startOffset": 81, "endOffset": 99}, {"referenceID": 10, "context": "Many studies have attempted to predict helpfulness given review-content features [7, 5, 11, 10, 15].", "startOffset": 81, "endOffset": 99}, {"referenceID": 9, "context": "Many studies have attempted to predict helpfulness given review-content features [7, 5, 11, 10, 15].", "startOffset": 81, "endOffset": 99}, {"referenceID": 14, "context": "Many studies have attempted to predict helpfulness given review-content features [7, 5, 11, 10, 15].", "startOffset": 81, "endOffset": 99}, {"referenceID": 15, "context": "First, we combine data on Amazon helpfulness votes from [16] with a much larger collection of helpfulness votes from 82 StackExchange forums.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The CRP [1, 2] is a self-reinforcing decision process over an infinite discrete set.", "startOffset": 8, "endOffset": 14}, {"referenceID": 1, "context": "The CRP [1, 2] is a self-reinforcing decision process over an infinite discrete set.", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "While the CRP models self-reinforcement \u2014 each vote for a response makes that response more likely to be selected later \u2014 there is evidence that the actual selection rate in an ordered list decays with display rank [6].", "startOffset": 215, "endOffset": 218}, {"referenceID": 2, "context": "The ddCRP [3] introduces a function f that decays with respect to some distance measure.", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "The Amazon dataset [16] originally consisted of 595 products with daily snapshots of writing/voting trajectories from Oct 2012 to Mar 2013.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "For each response, we record the number of comments and the average sentiment of those comments as estimated by [17].", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "Many online communities present user-contributed responses such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes. We propose the Chinese Voting Process (CVP) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases. We evaluate this model on Amazon product reviews and more than 80 StackExchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities.", "creator": "LaTeX with hyperref package"}}}