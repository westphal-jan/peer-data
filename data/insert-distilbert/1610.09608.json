{"id": "1610.09608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "A Theoretical Study of The Relationship Between Whole An ELM Network and Its Subnetworks", "abstract": "a biological neural access network is constituted by numerous subnetworks and modules with many different functionalities. for an artificial neural network, the relationship between a network and its random subnetworks is also important and useful globally for both effective theoretical validation and algorithmic research, i. e. it can be equally exploited to develop incremental network training algorithm or parallel network training algorithm. in this paper we explore the relationship between an elm computer neural network and its subnetworks. to seek the best of our knowledge, we are the first to rigorous prove a theorem matrix that shows an optimal elm neural network can be scattered into subnetworks and its optimal orthogonal solution polynomial can be constructed recursively by the independently optimal solutions of these subnetworks. based on by the theorem we also formally present two algorithms to train a particularly large elm neural network efficiently : one is a parallel network training algorithm and the other is an incremental a network training algorithm. the experimental results demonstrate the usefulness of the theorem and the validity of the developed algorithms.", "histories": [["v1", "Sun, 30 Oct 2016 06:34:19 GMT  (235kb,D)", "http://arxiv.org/abs/1610.09608v1", "3 figures"]], "COMMENTS": "3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["enmei tu", "guanghao zhang", "lily rachmawati", "eshan rajabally", "guang-bin huang"], "accepted": false, "id": "1610.09608"}, "pdf": {"name": "1610.09608.pdf", "metadata": {"source": "CRF", "title": "A Theoretical Study of The Relationship Between Whole An ELM Network and Its Subnetworks", "authors": ["Enmei Tu", "Guanghao Zhang", "Lily Rachmawati", "Eshan Rajabally", "Guang-Bin Huang"], "emails": ["Rolls-Royce@NTU"], "sections": [{"heading": null, "text": "Index Terms\u2014Extreme Learning Machine, Subnetwork Relationship, Parallel Network Training, Incremental Network Training\nI. INTRODUCTION\nNowadays, huge volumes of data have been collected continuously in various fields, from engineering to scientific research. These data contain valuable information which usually appears in forms of complex patterns residing in the data and highly challenges most of current machine learning methods (such as back propagation network [1]) on effectiveness and/or efficiency. Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.\nThe Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function. The pivotal features of an ELM are that weights and bias of input-to-hidden layer (or input weight for short) are randomly generated and no further tuning is required during the whole learning and prediction process. As a result,\nEnmei Tu is with the Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore\nGuanghao Zhang and Guang-Bin Huang are with School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore\nLily Rachmawati is with Computational Engineering Team, Advanced Technology Centre, Rolls-Royce Singapore Pte Ltd\nEshan Rajabally is with Future Technologies Group, Rolls-Royce Plc, UK\ntraining a neural network is reduced to training the hiddento-output layer weight (or output weight for short), which can be done by simply calculating the Moore-Penrose inverse of a hidden layer matrix. Therefore, an ELM can achieve extremely fast training speed and meanwhile is able to attain a better generalization ability than other conventional methods [12]. Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.\nAlthough the ELM has shown strong capability for various research areas, scalability of big data learning is still a bottleneck of ELM method. Usually large amount of hidden neural nodes are required for complex pattern learning problems and consequently calculating Moore-Penrose inverse of large matrix directly is difficult and potentially impossible due to memory limitation. Training efficiency for a large volume of training data may be another weakness even with a specific high performance toolbox such as [16].\nFor a large scale ELM network learning problem, many researchers are devoted to developing ELM training methods to learn complex patterns from a large amount of data: Heeswijk et al [17] proposed a GPU-accelerated and parallelized method for big data learning. The main focus of this research is efficient learning with implementation on multiple GPU and CPU cores. An OS-ELM based ensemble classification method in super-peer P2P network [18] is proposed for online-sequential ELM training by similar intuition of parallelization training. A high performance toolbox of ELM [19] focused on boosting training by CPU, GPU and HDF5 file format to achieve large scale training, fast file storage and easy installation. He et al [20] proposed a parallel ELM algorithm based on MapReduce, in which the matrix calculation for Moore-Penrose was decomposed and accelerated. A general framework based on MapReduce [21] is proposed by dividing the hidden layer into several groups, running a basic ELM training method for each group and then combining the output of all groups with same weight as the final output. An important concern of this method is that, in fact, simple combination output of each group is not theoretically equal to the basic ELM model with same number of hidden layer nodes . However, none of these researches study the relationship between an ELM network and its subnetworks. Here we show that this relationship is actually of great practical importance and can be exploited to develop algorithms for better training of an ELM network.\nIn this paper, we first prove a main theorem to reveal the relationship between an ELM network and its subnetworks.\nar X\niv :1\n61 0.\n09 60\n8v 1\n[ cs\n.L G\n] 3\n0 O\nct 2\n01 6\n2 To the best of our knowledge, this is the first study to show that the optimal output weight of an ELM network is equal to a linear transformation of its subnetworks\u2019 optimal output weights. Based on this theorem, we also present two ELM training algorithms to train a large ELM network: a parallel network training algorithm and an incremental network training algorithm. We demonstrate the validity of the algorithms with experiments on four popular digits classification datasets.\nThe remainder of the paper is organized as follows: Section 2 briefly reviews key techniques of the ELM. Section 3 proves the main theorem of an ELM network and Section 4 describes two typical applications of the main theorem to solve a large ELM training problem. In Section 5, experimental results of the algorithms are presented, followed by discussions and conclusions in Section 6."}, {"heading": "II. A BRIEF REVIEW OF EXTREME LEARNING MACHINE", "text": "The Extreme Learning Machine (ELM) method was proposed as a generalized multilayer feed-forward neural network with capability of classification and regression. In an ELM, the input weight and bias are randomly generated and then fixed through entire training and predicting process without any tuning.\nAssume the training data set is {x1, x2, ..., xn} and each sample is a d dimensional vector. The corresponding target values of the training samples are {y1, y2, ..., yn} and each target value is c dimensional vector. For real value function regress yk is a real number. For multiclass classification, c is the number of classes and yk is a class indicator vector whose entries are all 0 except for that the ith entry is 1 if sample xk belongs to class i. Let us denote X = (x1, x2, ..., xn) \u2208 Rd\u00d7n and Y = (y1, y2, ..., yn) \u2208 Rn\u00d7c. For a multilayer feedforward neural network with m hidden neurons, the network output corresponding to sample xi is\nF (xi) = m\u2211 j=1 wjhj(xi) = h(xi)W (1)\nwhere W = [w1, w2, ....wm]T \u2208 Rm\u00d7c is the output weight matrix1. h(xi) = [h1(xi), h2(xi), ....hm(xi)] is a row vector representing hidden layer output of sample xi, where h(\u00b7) is a continuous nonlinear function which maps samples from ddimensional input data space to m-dimensional feature space. The mapping function h(\u00b7) is uniquely characterized by a random vector a, a random bias b and an activation function g, i.e. for sample xi and hidden neuron j the mapping function is hj(xi) = g(xi; aj , bj). Note that F (xi) is a row vector of length c. ELM theory has proven that if g is a nonlinear continuous function [9] and a and b are randomly generated according to any continues probability distribution, then universal approximation property would be satisfied, which means that as the hidden layer neuron number m increases, the network can theoretically approximate any complex function with sufficient accuracy.\n1For simplicity, in the following parts we will mention W as output weight or optimal solution of the network, depending on description context.\nTwo most popular activation functions are sigmoid function and Gaussian function and respectively their expressions are\ng(xi; aj ,bj) = 1\n1 + exp(xTi aj + bj)\ng(xi; aj ,bj) = exp(\u2212bj\u2016xi \u2212 aj\u2016)\nThe input weight a and bias b are usually generated from uniform distribution [-1, 1].\nEquation (1) is the output for one sample xi only. The outputs of ELM with m hidden neurons and n input training samples are F (x1) = m\u2211 j=1 wjhj(x1) = h(x1)W ... F (xn) = m\u2211 j=1 wjhj(xn) = h(xn)W (2)\nIn matrix form, equation (2) can be written concisely as\nF = HW\nwhere\nH = g(x1; a1,b1) ... g(x1; am,bm)... ... ... g(xn; a1,b1) ... g(xn; am,bm)  is the hidden layer output matrix (or hidden layer matrix for short). F is a matrix, in which row i is F (xi).\nSince the input weight and bias are randomly generated and fixed as constants, the output weight W is the only parameter that needs to be tuned in network training process and can be obtained by ridge regression with global optimality [11]\nmin W\u2208Rm\u00d7c\n\u2016F \u2212 Y \u20162 + \u03b1\u2016W\u20162 (3)\nwhere parameter \u03b1 is the regularization parameter, representing the tradeoff between minimizing training error and model generalization.\nThe analytic solution for the optimization (3) can be obtained by setting derivative of objective function to zero, which yields\nW = (HTH + I\n\u03b1 )\u22121HTY (4)\nwhere I is the identity matrix. For applications where there are more hidden neurons than training samples (m > n), though rare for big data learning, solutions for W could be ill-conditioned. To handle this problem, Huang et al [14] restrict W to a linear combination of rows of H , i.e. W = HT\u03b2. In this case, HHT is invertible and by multiplying (HHT )\u22121H on both side of the derivative of equation (3), the solution becomes\nW = HT (HHT + I\n\u03b1 )\u22121Y (5)\n3"}, {"heading": "III. RELATIONSHIP BETWEEN ELM NETWORK AND ITS SUBNETWORKS", "text": "It has been demonstrated that biological neural networks contains numerous subnetworks, which have different functionalities and work in coordination to make the whole neural system functions optimally. Therefore the relationships between the subnetworks and the whole neural system are of great importance in neural science and have become the most popular research topic in neural science. For artificial neural networks, similar relationships between a network and its subnetworks are also important and useful for both theoretical and algorithmic research, because they can be used to study the properties of the network and to develop various training algorithms. As an illustration, Fig. 1 displays a network and its subnetworks. However, as far as we know, this relationship has not been well studied.\nIn this section we prove a theorem to show that an ELM network has a tight relationship with its subnetworks. In an ELM network, the input layer weight and bias are randomly generated. Once they are fixed, the network structure is determined and its output layer weight is the only variable that needs to be learned during training process. Therefore, the relationship between a network and its subnetworks is primarily the relationship between their output weights.\nFor simplicity, let us consider a binary division case. If the network is partitioned into two smaller networks, say network I and II, and their output weights are W1 and W2, respectively, our aim is to find out the relationship between the whole network output weight W and the two smaller network output weights W1 and W2.\nTo be more specific, we will prove the following theorem for an ELM network:\nTheorem 1: If W1 and W2 are the optimal output weights of two ELM networks and W is the optimal output weight of\nan ELM network which is constructed by concatenating the two ELM networks together, then there exists a matrix Z or \u2206W so that\nW = Z [ W1 W2 ] (6)\nor equivalently\nW = [ W1 W2 ] \u2212\u2206W (7)\nNow we prove it and find out the exact analytic form of the matrix Z and \u2206W . Without loss of generality, let us assume that the hidden layer of a large network contains 2m neurons2, where m is a positive integer. For manipulation simplicity, the network is partitioned into two equal subnetworks, and each subnetwork has m hidden neurons3. Accordingly, the hidden layer matrix H can be written as a partitioned matrix H =[ H1 H2 ] . Then for the case that number of training samples is greater than hidden layer neurons, i.e. n > 2m, we know from equation (4) that the output weights of network I and II are  W1 = ( Im \u03b1 +HT1 H1 )\u22121 HT1 Y\nW2 = ( Im \u03b1 +HT2 H2 )\u22121 HT2 Y\n(8)\nwhere Im is a m\u00d7m identity matrix. Note that\nHTH = [ HT1 HT2 ] [ H1 H2 ] = [ HT1 H1 H T 1 H2 HT2 H1 H T 2 H2 ] So the optimal output weight of the whole ELM network can be written as\nW = ( I2m \u03b1 +HTH )\u22121 HTY\n= ( I2m \u03b1 + [ HT1 H1 H T 1 H2 HT2 H1 H T 2 H2 ])\u22121 [ HT1 HT2 ] Y\n= ([ A B BT C ])\u22121 [ HT1 Y HT2 Y ] (9)\nwhere we let A = Im\u03b1 +H T 1 H1, B = H T 1 H2 and C = Im \u03b1 + HT2 H2. According to partitioned matrix inverse theory [22], a partitioned matrix inverse can be written as[\nA B BT C\n]\u22121 = [ SC \u22121 \u2212SC\u22121BC\u22121 \u2212C\u22121BTSC\u22121 DC\u22121 ]\nif and only if submatrix C and its Schur complement SC = ( A\u2212BC\u22121BT ) are both invertible, where D = I +C\u22121BTSC \u22121B. Noting that ( I2m \u03b1 +H TH )\nis a positive definite matrix, these two conditions are naturally met according to the following lemma [23]:\n2In the rest parts of the paper, a network of size m means there are m neurons in its hidden layer, since the neuron number in input layer and output layer is fixed for a given problem, i.e. they have to be equal to data dimension and classification classes (or function value dimension for regression), respectively.\n3It should be mentioned that these assumptions are just to simplify expression. Our theorem and algorithms are independent of the partitioning way of the whole network and the neuron number in each subnetwork. It neither requires the network to have even number of neurons nor requires the network to be partitioned in equal size subnetworks.\n4 Lemma 1: The following three statements are equivalent:\n(1) matrix [ A B BT C ] is positive definite; (2) A and its Schur\ncomplement SA = ( C \u2212BTA\u22121B ) are both positive definite;\n(3) C and its Schur complement SC = ( A\u2212BC\u22121BT ) are both positive definite.\nSo equation (9) now becomes\nW =\n[ S\u22121C \u2212S \u22121 C BC \u22121 \u2212C\u22121BTS\u22121C DC\u22121 ] [ HT1 Y HT2 Y ] = [ S\u22121C H T 1 Y \u2212 S\u22121C BC\u22121HT2 Y\n\u2212C\u22121BTS\u22121C HT1 Y +DC\u22121HT2 Y ] (10) Note that in equation (8), W1 and W2 are actually A\u22121HT1 Y and C\u22121HT2 Y , respectively. Furthermore, since (AB) \u22121 =\nB\u22121A\u22121, we have SC\u22121 = ( A\u2212BC\u22121BT )\u22121 =(\nI \u2212A\u22121BC\u22121BT )\u22121\nA\u22121. So equation (10) can be written as\nW =\n[ EW1 \u2212 S\u22121C BW2\n\u2212C\u22121BTEW1 +DW2\n] (11)\nMore obviously,\nW =\n[ E \u2212S\u22121C B\n\u2212C\u22121BTE D ] [ W1 W2 ] (12)\nwhere E = ( I \u2212A\u22121BC\u22121BT )\u22121 = SC\n\u22121A. From equation (12) we can see that the matrix Z in the theorem has the form\nZ =\n[ E \u2212S\u22121C B\n\u2212C\u22121BTE D\n] (13)\nAlternatively, from lemma 1 we know that the matrices A, C, SA and SC are all invertible, so the inverse of the partitioned matrix can also be written as [23][\nA B BT C\n]\u22121 = [ S\u22121C \u2212S \u22121 C BC \u22121\n\u2212S\u22121A BTA\u22121 S \u22121 A\n] (14)\nIn this case equation (9) now becomes\nW =\n[ S\u22121C \u2212S \u22121 C BC \u22121\n\u2212S\u22121A BTA\u22121 S \u22121 A\n] [ HT1 Y HT2 Y ] = [ S\u22121C H T 1 Y \u2212 S\u22121C BC\u22121HT2 Y\n\u2212S\u22121A BTA\u22121HT1 Y + S \u22121 A H T 2 Y ] (15) Similarly, substituting W1 = A\u22121HT1 Y and W2 = C\n\u22121HT2 Y , we have\nW =\n[ S\u22121C AW1 \u2212 S \u22121 C BW2\n\u2212S\u22121A BTW1 + S \u22121 A CW2 ] = [ S\u22121C A \u2212S \u22121 C B\n\u2212S\u22121A BT S \u22121 A C ] [ W1 W2 ] (16) or\nZ =\n[ S\u22121C A \u2212S \u22121 C B\n\u2212S\u22121A BT S \u22121 A C\n] (17)\nFrom equation (12) and (16) we can see the relationship between the whole network output weight and its subnetwork output weight: W can be obtained by concatenating the subnetworks\u2019 output weight together and then multiplying a matrix to adjust it to be optimal. Here we actually also prove\nthat a direct combination of subnetworks\u2019 output weight/output is not optimal, as in [21], since matrix Z is not equal to identity matrix. Note that the adjustment matrix in equation (12) requires to compute one matrix inverse S\u22121C (C\n\u22121 is already computed when solving W2), but the adjustment matrix in equation (16) requires to compute two matrices inverse (SA\u22121 and SC\u22121) and this difference makes equation (12) more practical for design of efficient learning algorithms. Note that equation (17) can be further decomposed as\nZ = [ S\u22121C O O S\u22121A ] [ A \u2212B \u2212BT C ] (18)\nwhere O represents a zero matrix with proper size. From this equation we can see more obviously that instead of computing the inverse of the big matrix[\nA B BT C ] in equation (9), we just need to compute two smaller matrix inverse S\u22121C and S \u22121 A to obtain the optimal solution of the whole ELM network. On the other hand, for the case n < 2m, partition matrix H in the same way but now we have\nHHT = [ H1 H2 ] [HT1 HT2 ] = H1H T 1 +H2H T 2\nAccording to equation (5), the output weight now can be written as\nW = [ HT1 HT2 ]( I2m \u03b1 +H1H T 1 +H2H T 2 )\u22121 Y\n=\n[ HT1 ( A+H2H T 2 )\u22121 Y\nHT2 ( C +H1H T 1 )\u22121 Y ] (19) where, with some abuse of notation, A = ( I2m \u03b1 +H1H T 1\n) and B = ( I2m \u03b1 +H2H T 2 ) . Recall that the Woodbury inverse formula is( P +QQT )\u22121 = P\u22121 \u2212 P\u22121Q ( I +QTP\u22121Q )\u22121 QTP\u22121\nproviding that P\u22121 exists. Since both A and C are invertible according to lemma 1, applying Woodbury formula to each submatrix in equation (19) we have\nW =\n[ HT1 ( A\u22121 \u2212A\u22121H2M\u221211 H2 TA\u22121 ) Y\nHT2 ( C\u22121 \u2212 C\u22121H1M\u221212 H1 TC\u22121 ) Y ] where for concise display purpose we denote M1 =( I +H2 TA\u22121H2 ) and M2 = ( I +H1 TC\u22121H1 ) . From equation (5) we know W1 = HT1 A \u22121Y and W2 = HT2 A\n\u22121Y are the output weights of subnetworks I and II, respectively. After substituting we have\nW =\n[ W1 \u2212H1A\u22121H2M\u221211 H2 TA\u22121Y\nW2 \u2212H2C\u22121H1M\u221212 H1 TC\u22121Y ] Or more concisely,\nW = [ W1 W2 ] \u2212 [ \u2206W1 \u2206W2 ] = [ W1 W2 ] \u2212\u2206W (20)\nwhere \u2206W1 = H1A\u22121H2M\u221211 H2 TA\u22121Y and \u2206W2 = H2C \u22121H1M \u22121 2 H1 TC\u22121Y . Equation (20) means that while\n5 n < 2m, the whole network output weight can be obtained by concatenating its subnetworks\u2019 output weight and then subtracting an adjustment matrix. Note the the solution in equation (4) is equivalent to that one in equation (5), so the two equations in Theorem 1 are also equivalent.\nIt should be mentioned that the final output weight in equation (12), (16) and (20) obtained by concatenating all output weights of its subnetworks is exactly same as the original ELM network. This guarantees the optimality of the solution, hence the performance of the network, as will be shown in the experiments in Section 5."}, {"heading": "IV. APPLICATIONS OF THE THEOREM", "text": "In this section we demonstrate the usefulness of the theorem. will develop two methods for training large scale ELM network using the relationship derived in previous section. The first method is a hierarchical algorithm and the second method is a block-wise incremental algorithm."}, {"heading": "A. Hierarchical Network Training Algorithm", "text": "Equation (16) and (20) tell us that in order to train a large network, we can first partition the network into smaller networks and train each subnetwork individually. Thereafter the output weights of all subnetworks can be used to construct the output weight of the whole network. This divide-andtrain strategy can be easily implemented in parallel to make use of hardware computational ability, i.e. on a multi-core computer or a cluster of computers. Meanwhile memory space requirement is also reduced, because the training process only concerns matrices with half size of the original problem and the space requirements of matrix inverse is cubic in terms of matrix size. Furthermore, the divide-and-train strategy can be further applied to each subnetwork in a hierarchical way, i.e. to continue dividing each subnetwork into two further smaller networks and so on. As an illustration, Fig 2 shows a two-level hierarchical division network.\nFrom equation (16) we know that the relationship between the output weights are:\nW = Z [ WI WII ] (21)\nand  WI = ZI [ W (1) I W (2) I ] WII = ZII [ W (1) II\nW (2) II ] (22) Therefore, to obtain the optimal solution W of the whole (potentially large) network, it is sufficient to train the four much smaller networks to get W (1)I , W (2) I , W (1) II and W (2) II and then use equations (22) and (21) to compute W easily. Training a smaller network has at least three obvious advantages: (1) A smaller matrix manipulation is time and space saving; (2) A smaller matrix inverse tends to be more robust to disturbance and noise; (3) To make full use of parallel architecture of multi-core computer or cluster of computers, the subnetworks can be implemented to run in parallel to further speedup training process.\nB. Incremental Network Training Algorithm\nIf subnetwork II in equation (6) or (7) is treated as a new added part, then the relationship can be utilized to train an incrementally growing network by just solving the new added subnetwork output weight and updating efficiently the whole network output weight. Suppose the network has L neurons in hidden layer and it has already been trained to obtain its output weight W (L). Now l neurons are added to the network and the whole network optimal output weight becomes W (L+l), as shown in Fig. 3.\nAccording to equation (6) we know that\nW (L+l) = Z\n[ W (L)\nW (l)\n] (23)\nwhere W (l) is obtained by using the same training data set to train the new added subnetwork of size l. More specifically, according to equation (12) we have\nW (L+l) =\n[ EW (L) \u2212 S\u22121C BW (l) \u2212C\u22121BTEW (L) +DW (l) ]\n6 If we write\nP =\n[ E\n\u2212C\u22121BTE\n] , Q = [ S\u22121C B \u2212D ] we have a more concise updating formula\nW (L+l) = PW (L) \u2212QW (l) (24)\nThe update equation (24) can be computed much more easily than using equation (5) to solve the whole network problem, because the new added neuron number l is usually much smaller than the whole network hidden neuron number L + l. Matrices C\u22121 and S\u22121C can be calculated easily using previous solutions and thus both P and Q can be obtained efficiently4. A special case of equation (24) is that the new added subnetwork contains only one neuron in hidden layer (l = 1). In this case the updating process can be implemented without explicitly computing any matrix inverse [24]."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In this section we conduct experiments on four popular hand-writing digits datasets to demonstrate the validity of the theorem and the proposed hierarchical and incremental algorithms. The information of the experimental datasets are listed in Table I.\nThe usps5 dataset contains normalized 16 \u00d7 16 grey scale image of US Postal Service handwritten digits. It has 7291 images for training and 2007 images for testing. The mnist6 dataset contains 70000 grey scale hand writing images, among them the first 60000 are training images and the rest 10000 are testing images. Each image is size-normalized and centered in a fixed-size image of size 28 \u00d7 28. The fontdigits7 dataset contains 10000 grey scale digit images of size 28 \u00d7 28, among them the first 5000 images are training images and the last 5,000 images are testing images. The pendigits8 dataset consists of 10992 hand writing digits images. The first 7494 images are training images and the rest 3497 images are testing images. In each image x and y coordinates of each pixel on digit are normalized between 0...100 to be the features.\n4It should be mentioned that equation (24) can also be derived from equation (7), in which case that matrices P and Q are slightly different:\nP = [ S\u22121C A \u2212S\u22121A B T ] , Q = [ S\u22121C B \u2212S\u22121A C ] . But this update equation needs more computational cost, since at each updating time it requires to compute S\u22121A which is of size L.\n5https://www-i6.informatik.rwth-aachen.de/\u223ckeysers/Pubs/SPR2002/ node10.html#tab:usps\n6http://yann.lecun.com/exdb/mnist/ 7http://www.mathworks.com/help/nnet/examples/training-a-deep-neural-\nnetwork-for-digit-classification.html 8https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+ Handwritten+Digits\nIn the hierarchical network training experiment, we only implement one-level hierarchical ELM network in figure 2, with each subnetwork contains 2000 hidden neurons. As a comparison baseline algorithm, the original ELM network contains 4000 hidden neurons and is trained using equation (4) or (5), depending on the relative relationship between the hidden neuron number and sample number. In incremental network training experiment, we start with an ELM with 2000 hidden neurons and then increase the hidden layer size by adding 2000 hidden neurons. The baseline algorithm, the original ELM, is also implemented to have an incremental hidden layer, but the output weight is computed using equation (4) or (5). In order to test the robustness of the hierarchical and incremental network training methods, two most commonly used activation functions are compared, i.e. the sigmoid function and the radial basis function. In each experiment, the algorithms run 5 times and the average of the results are used to compare their performance. The experimental results are in Table II and Table III, in which WH (WI ) is the optimal solution obtained by hierarchical (incremental) training and WO is the optimal solution obtained by original ELM. A similar notation is also utilized for the running time and error rate (i.e. TimeO and TimeH, ErrorO and ErrorH). AcFun stands for activation function.\nFrom these results we can see that for both two activation functions, the optimal solutions and performance of original ELM and hierarchical/incremental ELM are almost identical9. This demonstrates the correctness of the theorem and the validity of proposed training methods. It is worth mentioning that even if we only implemented one level hierarchical/incremental ELM here, the proposed network training methods is much efficient than original ELM, in terms of computational time and memory occupation, and show good potential to train a large ELM network. The larger the network\n9The third row is scientific number, i.e. 4.2e-27 is 4.2\u00d7 10\u221227.\n7\nand dataset are, the more time and memory they can reduce, because the time and space complexity of the original ELM is cubic in terms of either network size in equation (4) or dataset size in equation (5)."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper we study theoretically the relationship between an ELM network and its subnetworks. We prove a theorem which shows that the optimal solution of an ELM network is a linear transformation of its subnetworks\u2019 optimal solutions. This theorem has the potential to be utilized to develop various efficient ELM training algorithms. As an example, we developed two algorithms to train a large ELM network: one is a hierarchical training algorithm and the other is an incremental training algorithm. The validity of both algorithms is demonstrated by experiments. For future work, we will focus on developing more efficient algorithms for training large ELM networks based on the theorem and studying theoretically the criteria for recursively training subnetworks to construct a large ELM network."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was conducted within the Rolls-Royce@NTU Corporate Lab with support from the National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme."}], "references": [{"title": "Brief introduction of back propagation (bp) neural network algorithm and its improvement", "author": ["J. Li", "J.-h. Cheng", "J.-y. Shi", "F. Huang"], "venue": "Advances in Computer Science and Information Engineering. Springer, 2012, pp. 553\u2013558.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from  labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of machine learning research, vol. 7, no. Nov, pp. 2399\u20132434, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A novel graphbased k-means for nonlinear manifold clustering and representative selection", "author": ["E. Tu", "L. Cao", "J. Yang", "N. Kasabov"], "venue": "Neurocomputing, vol. 143, pp. 109\u2013122, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A graph-based semi-supervised k nearest-neighbor method for nonlinear manifold distributed data classification", "author": ["E. Tu", "Y. Zhang", "L. Zhu", "J. Yang", "N. Kasabov"], "venue": "Information Sciences, vol. 367368, pp. 673 \u2013 688, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning", "author": ["M.F.A. Hady", "F. Schwenker"], "venue": "Handbook on Neural Information Processing. Springer, 2013, pp. 215\u2013239.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deformed graph laplacian for semisupervised learning", "author": ["C. Gong", "T. Liu", "D. Tao", "K. Fu", "E. Tu", "J. Yang"], "venue": "IEEE transactions on neural networks and learning systems, vol. 26, no. 10, pp. 2261\u20132274, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "An experimental comparison of semi-supervised learning algorithms for multispectral image classification", "author": ["E. Tu", "J. Yang", "J. Fang", "Z. Jia", "N. Kasabov"], "venue": "Photogrammetric Engineering & Remote Sensing, vol. 79, no. 4, pp. 347\u2013357, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Posterior distribution learning (pdl): A novel supervised learning framework using unlabeled samples to improve classification performance", "author": ["E. Tu", "J. Yang", "N. Kasabov", "Y. Zhang"], "venue": "Neurocomputing, vol. 157, pp. 173\u2013 186, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.-B. Huang", "L. Chen", "C.-K. Siew"], "venue": "Neural Networks, IEEE Transactions on, vol. 17, no. 4, pp. 879\u2013892, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70, no. 1, pp. 489\u2013501, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 42, no. 2, pp. 513\u2013529, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "What are extreme learning machines? filling the gap between frank rosenblatts dream and john von neumanns puzzle", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, vol. 7, no. 3, pp. 263\u2013278, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental learning in human action recognition based on snippets", "author": ["R. Minhas", "A.A. Mohammed", "Q. Wu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, vol. 22, no. 11, pp. 1529\u20131541, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Semisupervised and unsupervised extreme learning machines", "author": ["G. Huang", "S. Song", "J.N. Gupta", "C. Wu"], "venue": "Cybernetics, IEEE Transactions on, vol. 44, no. 12, pp. 2405\u20132417, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Image super-resolution by extreme learning machine", "author": ["L. An", "B. Bhanu"], "venue": "Image processing (ICIP), 2012 19th IEEE International Conference on. IEEE, 2012, pp. 2209\u20132212.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "High-performance extreme learning machines: a complete toolbox for big data applications", "author": ["A. Akusok", "K.-M. Bjork", "Y. Miche", "A. Lendasse"], "venue": "Access, IEEE,  8 vol. 3, pp. 1011\u20131025, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Gpu-accelerated and parallelized elm ensembles for large-scale regression", "author": ["M. Van Heeswijk", "Y. Miche", "E. Oja", "A. Lendasse"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2430\u20132437, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "An os-elm based distributed ensemble classification framework in p2p networks", "author": ["Y. Sun", "Y. Yuan", "G. Wang"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2438\u2013 2443, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting intrinsic variability of filamentary resistive memory for extreme learning machine architectures", "author": ["M. Suri", "V. Parmar"], "venue": "Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 963\u2013968, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel extreme learning machine for regression based on mapreduce", "author": ["Q. He", "T. Shang", "F. Zhuang", "Z. Shi"], "venue": "Neurocomputing, vol. 102, pp. 52\u201358, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Mr-elm: a mapreduce-based framework for large-scale elm training in big data era", "author": ["J. Chen", "H. Chen", "X. Wan", "G. Zheng"], "venue": "Neural Computing and Applications, vol. 27, no. 1, pp. 101\u2013110, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix mathematics: theory, facts, and formulas", "author": ["D.S. Bernstein"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Inversefree extreme learning machine with optimal information updating.", "author": ["S. Li", "Z. You", "H. Guo", "X. Luo", "Z. Zhao"], "venue": "IEEE transactions on cybernetics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "These data contain valuable information which usually appears in forms of complex patterns residing in the data and highly challenges most of current machine learning methods (such as back propagation network [1]) on effectiveness and/or efficiency.", "startOffset": 209, "endOffset": 212}, {"referenceID": 1, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 27, "endOffset": 32}, {"referenceID": 2, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 27, "endOffset": 32}, {"referenceID": 4, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 5, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 6, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 7, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 8, "context": "The Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function.", "startOffset": 35, "endOffset": 41}, {"referenceID": 9, "context": "The Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function.", "startOffset": 35, "endOffset": 41}, {"referenceID": 10, "context": "The Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function.", "startOffset": 35, "endOffset": 41}, {"referenceID": 11, "context": "Therefore, an ELM can achieve extremely fast training speed and meanwhile is able to attain a better generalization ability than other conventional methods [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.", "startOffset": 238, "endOffset": 242}, {"referenceID": 15, "context": "Training efficiency for a large volume of training data may be another weakness even with a specific high performance toolbox such as [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "For a large scale ELM network learning problem, many researchers are devoted to developing ELM training methods to learn complex patterns from a large amount of data: Heeswijk et al [17] proposed a GPU-accelerated and parallelized method for big data learning.", "startOffset": 182, "endOffset": 186}, {"referenceID": 17, "context": "An OS-ELM based ensemble classification method in super-peer P2P network [18] is proposed for online-sequential ELM training by similar intuition of parallelization training.", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "A high performance toolbox of ELM [19] focused on boosting training by CPU, GPU and HDF5 file format to achieve large scale training, fast file storage and easy installation.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "He et al [20] proposed a parallel ELM algorithm based on MapReduce, in which the matrix calculation for Moore-Penrose was decomposed and accelerated.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "A general framework based on MapReduce [21] is proposed by dividing the hidden layer into several groups, running a basic ELM training method for each group and then combining the output of all groups with same weight as the final output.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "ELM theory has proven that if g is a nonlinear continuous function [9] and a and b are randomly generated according to any continues probability distribution, then universal approximation property would be satisfied, which means that as the hidden layer neuron number m increases, the network can theoretically approximate any complex function with sufficient accuracy.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "The input weight a and bias b are usually generated from uniform distribution [-1, 1].", "startOffset": 78, "endOffset": 85}, {"referenceID": 10, "context": "Since the input weight and bias are randomly generated and fixed as constants, the output weight W is the only parameter that needs to be tuned in network training process and can be obtained by ridge regression with global optimality [11]", "startOffset": 235, "endOffset": 239}, {"referenceID": 13, "context": "To handle this problem, Huang et al [14] restrict W to a linear combination of rows of H , i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "Noting that ( I2m \u03b1 +H H ) is a positive definite matrix, these two conditions are naturally met according to the following lemma [23]:", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "Alternatively, from lemma 1 we know that the matrices A, C, SA and SC are all invertible, so the inverse of the partitioned matrix can also be written as [23] [ A B B C ]\u22121", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "Here we actually also prove that a direct combination of subnetworks\u2019 output weight/output is not optimal, as in [21], since matrix Z is not equal to identity matrix.", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "In this case the updating process can be implemented without explicitly computing any matrix inverse [24].", "startOffset": 101, "endOffset": 105}], "year": 2016, "abstractText": "A biological neural network is constituted by numerous subnetworks and modules with different functionalities. For an artificial neural network, the relationship between a network and its subnetworks is also important and useful for both theoretical and algorithmic research, i.e. it can be exploited to develop incremental network training algorithm or parallel network training algorithm. In this paper we explore the relationship between an ELM neural network and its subnetworks. To the best of our knowledge, we are the first to prove a theorem that shows an ELM neural network can be scattered into subnetworks and its optimal solution can be constructed recursively by the optimal solutions of these subnetworks. Based on the theorem we also present two algorithms to train a large ELM neural network efficiently: one is a parallel network training algorithm and the other is an incremental network training algorithm. The experimental results demonstrate the usefulness of the theorem and the validity of the developed algorithms.", "creator": "TeX"}}}