{"id": "1610.09769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks", "abstract": "most real - world data can be modeled as heterogeneous information semantic networks ( hins ) themselves consisting of vertices of multiple types and their relationships. search for similar vertex vertices of the same type in large hins, such them as bibliographic networks and distributed business - review networks, is a fundamental problem with broad applications. although similarity search in hins has been studied previously, most existing database approaches neither explore rich semantic information embedded in the network structures nor take user's decision preference model as given a guidance.", "histories": [["v1", "Mon, 31 Oct 2016 03:15:02 GMT  (1964kb,D)", "http://arxiv.org/abs/1610.09769v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["jingbo shang", "meng qu", "jialu liu", "lance m kaplan", "jiawei han", "jian peng"], "accepted": false, "id": "1610.09769"}, "pdf": {"name": "1610.09769.pdf", "metadata": {"source": "CRF", "title": "Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks", "authors": ["Jingbo Shang", "Meng Qu", "Jialu Liu", "Lance M. Kaplan", "Jiawei Han", "Jian Peng"], "emails": ["jianpeng}@illinois.edu", "2jialu@google.com", "3lance.m.kaplan.civ@mail.mil"], "sections": [{"heading": null, "text": "propose a novel embedding-based framework. It models vertices as low-dimensional vectors to explore network structureembedded similarity. To accommodate user preferences at defining similarity semantics, our proposed framework, ESim, accepts user-defined meta-paths as guidance to learn vertex vectors in a user-preferred embedding space. Moreover, an efficient and parallel sampling-based optimization algorithm has been developed to learn embeddings in large-scale HINs. Extensive experiments on real-world large-scale HINs demonstrate a significant improvement on the effectiveness of ESim over several state-of-the-art algorithms as well as its scalability."}, {"heading": "1. INTRODUCTION", "text": "A heterogeneous information network (HIN ) is a network that consists of multi-typed vertices connected via multityped edges. Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25]. Similarity search in HINs is a fundamental problem for mining large HINs and much digital ink has been spilled over it in the community (e.g., [26, 25, 29, 28]). In this paper, we are particularly interested in utilizing HIN to conduct similarity search among the objects of the same type. For example, given a social media network in Yelp with connections between reviews, users and businesses, we can find similar restaurants (i.e., similarity search among\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nbusinesses), and recommend potential friends with similar preferences (i.e., similarity search among users). Naturally, people tend to employ different semantics in their network-based search even towards the same application. Take a bibliographic network as an example that consists of papers, authors, venues, and terms. To find similar authors to a given author, some users may weigh more on shared technical terms in papers, while others more on shared publication venues. PathSim [26] proposes to use meta-paths to define and guide similarity search in HIN, with good success. A meta-path is represented by a path (i.e., a connected sequence of vertices) at the schema level (e.g., \u3008author\u2212paper\u2212author\u3009). However, PathSim has not explored similarities embedded deeply in the HIN structure. When hunting for similar authors, if the given meta-path is \u3008author\u2212paper\u2212venue\u2212paper\u2212author\u3009, PathSim can only build bridges for authors publishing papers in the same venues (e.g., \u201cWSDM\u201d), but cannot efficiently explore the semantic similarity between venues (e.g., \u201cWSDM\u201d and \u201cWWW\u201d) to improve the search. However, such kind of semantic similarity can be easily implied by embedded semantic structure of the HIN. For an extreme example, if one author only publish in \u201cWSDM\u201d while the other only has publications in \u201cWWW\u201d, their PathSim similarity will be 0. Although a bridge can be built between the two similar venues by traversing some long paths, it becomes much more costly and does not seem to be an elegant way compared with the embedding-based approach studied in this paper.\nAlong another line of study, network-embedding techniques have been recently explored for homogeneous information networks, which treat all vertices and edges as of the same type, represented by LINE [29]. An alternative and better way is to first project the HIN to several bipartite (assuming user-given meta-paths are symmetric) or homogeneous networks and then apply the edge-wise HIN embedding technique, such as PTE [28]. However, the network projection itself is count-based which does not preserve the underlying semantics, and the cost for computing such projected networks is high. For example, taking a user-selected meta-path \u3008author\u2212paper\u2212venue\u2212paper\u2212author\u3009 (i.e., shared-venues) to find similar authors, the projection will generate a homogeneous network consisting of only authors, but it loses important network structure information (e.g., venues and papers) [25] and leads to a rather densely connected network (since many authors may publish in many venues). Clearly, direct modeling of the original heterogeneous information network will capture richer semantics than exploiting the projected networks. ar X iv :1\n61 0.\n09 76\n9v 1\n[ cs\n.S I]\n3 1\nO ct\n2 01\n6\nAcknowledging the deficiency of the above two types of approaches, we propose ESim, a novel embedding-based similarity search framework, with the following contributions. \u2022 A general embedding-based similarity search framework is proposed for HINs, where an HIN may contain undirected, directed, weighted, and un-weighted edges as well as various types of vertices; \u2022 Our framework incorporates a set of meta-paths as an\ninput from a user to better capture the semantic meaning of user-preferred similarity; and \u2022 It handles large-scale HINs efficiently due to a novel sampling method and a parallel optimization framework. To the best of our knowledge, this is the first work that\nproposes a general meta-path guided embedding framework for similarity search in heterogeneous information networks."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1 Meta-Path Guided Similarity Search", "text": "The concept of meta-path, which represents a connected sequence of vertices at the schema level, plays a crucial role in typed and structured search and mining in HINs. PathSim [26] defines the similarity between two vertices of the same type by the normalized count of path instances following a user-specified meta-path between any pair of vertices. [26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11]. Moreover, [27] shows that user guidance can be transformed to a weighted combination of meta-paths. However, PathSim does not explore the similarity embedded in the structure of a HIN. Moreover, PathSim doesn\u2019t have the embedding vectors of vertices, which can make the further analysis more efficient, such as clustering."}, {"heading": "2.2 Embedding-based Similarity Search", "text": "Recently, embedding technique, which aims at learning low-dimensional vector representations for entities while preserving proximities, has received an increasing attention due to its great performance in many different types of tasks. As a special and concrete scenario, embedding of homogeneous networks containing vertices of the same type has been studied recently. LINE [29] and DeepWalk [20] utilize the network link information to construct latent vectors for vertex classification and link prediction. DCA [4] starts from the personalized PageRank but does further decomposition to get better protein-protein interaction predictions in biology networks. However, these homogeneous models cannot capture the information about entity types nor about relations across different typed entities in HINs. There are also embedding algorithms developed for HINs. For example, Chang et al. propose to incorporate deep neural networks to train embedding vectors for both text and images at the same time [3]. Under a supervised setting, PTE [28] utilizes labels of words and constructs bipartite HINs to learn predictive embedding vectors for words. Embedding techniques have been also applied to knowledge graphs to resolve question-answering tasks [6] and retain knowledge relations between entities [32]. However, these are all specially designed for specific types of networks and tasks and thus difficult to be extended to incorporate user guidance. The vector spaces constructed by different methods have different semantic meanings due to the statistics they emphasize. In many real-world scenarios, it is often difficult to find an\nappropriate model, and the models have to be revised to fit into the desired usage."}, {"heading": "2.3 Similarity Search in Vector Spaces", "text": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5]. In our work, we directly utilize cosine similarity based on embedding vectors of vertices. Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2]. We adopt these existing efficient similarity search techniques to support online queries."}, {"heading": "3. PRELIMINARIES", "text": "In this section, a series of definitions and notations are presented. Definition 1. Heterogeneous Information Network is an information network where both vertices and edges have been associated with different types. In a heterogeneous information network G = (V,E,R), V is the set of typed vertices (i.e., each vertex has its own type), R is the set of edge types in the network, and E \u2282 V \u00d7 V \u00d7 R is the edge set. An edge in a heterogeneous information network is an ordered triplet e = \u3008u, v, r\u3009, where u and v are two typed vertices associated with this edge and r is the edge type.\nIn a general heterogeneous information network, there might be multiple typed edges between the same two vertices and the edge could be either directed or undirected. The definition above naturally supports all these cases. To better explain it, we use a bibliographic network as an example.\nExample 1. In the bibliographic network as shown in Fig. 1, the vertex set V consists of three types, {\u201cauthor\u201d, \u201cpaper\u201d, \u201cvenue\u201d}, the edge type set R contains \u201cthe author of the paper\u201d and \u201cthe paper was published in the venue\u201d. The edge set E consists of concrete edges like \u3008u = \u201cJon Kleinberg\u201d, v = \u201cHITS\u201d, r = \u201cthe author of\u201d \u3009, where Jon Kleinberg is one of the authors of the paper HITS. For ease of representation, edge type r is denoted as A\u2212P once there is only one edge type between author and paper. Another edge is \u3008u = \u201cHITS\u201d, v = \u201cJACM\u201d, r = \u201cbe published in\u201d \u3009, which means the paper was published in the venue \u201cJACM\u201d. In this case, r is denoted as P\u2212V . Note that both edge types are undirected here. That is, A\u2212P and P\u2212A are actually referring to the same edge type. So as P\u2212V and V\u2212P .\nWe define the concepts of meta-path and sub-meta-path by concatenating edge types in a sequence, as follows.\nDefinition 2. In a HIN G = (V,E,R), a meta-path is a sequence of compatible edge types M = \u3008r1, r2, . . . , rL\u3009 with length L, \u2200ri \u2208 R, and the outgoing vertex type of ri should match the incoming vertex type of ri+1. For any 1 \u2264 s \u2264 t \u2264 L, we can induce a sub-meta-path Ms,t = \u3008rs, rs+1, . . . , rt\u3009.\nParticularly, an edge type r can be viewed as a length-1 meta-pathM = \u3008r\u3009. A sequence of edges followingM is called a path instance. Because there might be multiple edges between the same pair of vertices, instead of the vertex sequence, the edge sequence is used to describe a path instance. Formally speaking,\nDefinition 3. Given a HIN G = (V,E,R) and a metapathM = \u3008r1, r2, . . . , rL\u3009, any path Pe1 eL = \u3008e1, e2, . . . , eL\u3009 connecting vertices u1 and uL+1 (i.e., vL), is a path instance followingM, if and only if \u22001 \u2264 i \u2264 L, the i-th edge is type-ri, and \u22001 \u2264 i \u2264 L, vi = ui+1.\nWe continue the example of the bibliographic network and list some meta-paths and path instances.\nExample 2. In the bibliographic network as shown in Fig. 1, a length-2 meta-path \u3008A\u2212P, P\u2212A\u3009 (abbrev. as A\u2212P\u2212A) expresses the co-authorship relation. The collaboration between \u201cJure Leskovec\u201d and \u201cJon Kleinberg\u201d on the paper \u201cGraphs over time\u201d is a path instance of A\u2212P\u2212A. Similarly, a length-4 meta-path A\u2212P\u2212V\u2212P\u2212A captures the shared venues and any two authors published a certain paper in a same venue could be its path instance. Besides, P\u2212V\u2212P is a sub-meta-path of A\u2212P\u2212V\u2212P\u2212A.\nDefinition 4. Meta-path Guided Similarity Search is a similarity search task on HINs, where the semantic meanings of the similarity are determined by n meta-paths {M1,M2, . . . ,Mn} specified by the user.\nExample 3. In the bibliographic network, to find similar authors, a user may choose two meta-paths A\u2212P\u2212V\u2212P\u2212A and A\u2212P\u2212A as guidance.\nIn the subsequent discussion, we focus on the case of a single meta-path M. This is because (1) the principal ideas for exploring multiple weight-assigned meta-paths are essentially the same; (2) in our experiments, the performance gain of the optimal weighted combination of meta-paths is not significant. Moreover, we leave the study on derivation of a weighted combination of multiple meta-paths based on user\u2019s high-level guidance (i.e., user providing examples instead of path weights explicitly) to future work. Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15]."}, {"heading": "4. METHODOLOGY", "text": "In this section, to incorporate meta-paths, we formulate a probabilistic embedding model inspired from many previous studies. We propose an efficient and scalable optimization algorithm relying on the sampling of path instances following the given meta-path. Our proposed efficient sampling methods are the most crucial steps and thus are discussed separately. In addition, we also provide thorough time complexity analysis."}, {"heading": "4.1 Probabilistic Embedding Model Incorporating Meta-Paths", "text": "Model Formulation. The basic idea of our approach is to preserve the HIN structure information into the learned embeddings, such that vertices which co-occur in many path instances turn to have similar embeddings. To preserve the structure of a HIN G = (V,E,R), we first define the conditional probability of vertex v connected to\nvertex u by some path instances following the meta-pathM as:\nPr(v|u,M) = exp(f(u, v,M))\u2211 v\u2032\u2208V exp(f(u, v\u2032,M))\n(1)\nwhere function f is a scoring function modeling the relevance between u and v conditioned on the meta-pathM. In particular, we encode the meta-path through the following formulation inspired from [19, 23]:\nf(u, v,M) = \u00b5M + pMTxu + qMTxv + xuTxv\nHere, \u00b5M \u2208 R is the global bias of the meta-pathM, pM and qM \u2208 Rd are local bias vectors which are d dimensional. xu and xv \u2208 Rd are d dimensional embedding vectors for vertices u and v respectively. Under such definition, if the embeddings of two vertices have a larger dot product, the two vertices are likely having a larger relevance score, and thus co-occuring in many path instances. Note that if users want a symmetric score function, \u2200u, v, f(u, v,M) = f(v, u,M), we can restrict pM = qM. For a better understanding of the scoring function f , we can rewrite it as follows\nf(u, v,M) = (\u00b5M \u2212 pMTqM) + (xu + qM)T (xv + pM)\nwhere we can see that pM and qM shift xu and xv according to the semantic of the meta-pathM while \u00b5M adjusts the score to an appropriate range. For a path instance Pe1 eL = \u3008e1 = \u3008u1, v1, r1\u3009, e2 = \u3008u2, v2, r2\u3009, . . . , eL = \u3008uL, vL, rL\u3009\u3009 following the meta-path M = \u3008r1, r2, . . . , rL\u3009, we adopt the following approximation, by approximating the probability of the first vertex.\nPr(Pe1 eL |M) = Pr(u1|M)\u00d7 Pr(Pe1 eL |u1,M) \u221d C(u1, 1|M)\u03b3 \u00d7 Pr(Pe1 eL |u1,M) (2)\nwhere C(u, i|M) represents the number of path instances followingM with the ith vertex being u. \u03b3 is a widely used parameter to control the effect of overly-popular vertices, which is usually 3/4 inspired from [16]. In Sec. 4.2.1, we will show an efficient dynamic programming algorithm to compute C(u, i|M). The conditional probability, Pr(Pe1 eL |u1,M), is now the last undefined term. The simplest definition is Pr(vL|u1,M), which assumes that the probability only depends on the two ends of the path instance and directly applies Eq. (1). However, it omits the intermediate information and is equivalent to projection-based models. In this paper, we propose two possible solutions as follows, and later show \u201cpairwise\u201d is more effective than \u201csequential\u201d, since it exploits the meta-path guidance in a more thorough way. \u2022 Sequential (seq): In this setting, we assume that a vertex is highly relevant to its left/right neighbors in the sequence: Pr(Pe1 eL |u1,M) = \u220fL k=1 Pr(vk|uk,Mk,k).\n\u2022 Pairwise (pair): In this setting, we assume all vertices in a path instance are highly relevant to each other, and thus the probability of the path instance is defined as Pr(Pe1 eL |u1,M) = \u220fL s=1 \u220fL t=s Pr(vt|us,Ms,t). As a\nresult, vertices co-occur in many path instances turn to have large relevance scores. Noise-Contrastive Estimation (NCE). Given the conditional distribution defined in Eqs. (1) and (2), the maximum likelihood training is tractable but expensive because com-\nputing the gradient of log-likelihood takes time linear in the number of vertices. Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities [7, 17]. The basic idea is to sample some observed path instances associated with some noisy ones, and it tries to maximize the probability of each observed instance while minimize the probability of each noisy one. Specifically, we reduce the problem of density estimation to a binary classification, discriminating between samples from path instances following the user selected meta-path and samples from a known noise distribution. In particular, we assume these samples come from the mixture.\n1 K + 1Pr +(Pe1 eL |M) + K K + 1Pr \u2212(Pe1 eL |M)\nwhere Pr+(Pe1 eL |M) denotes the distribution of path instances in the HIN following the meta-pathM. Pr\u2212(Pe1 eL |M) is a noise distribution, and for simplicity we set\nPr\u2212(Pe1 eL |M) \u221d L+1\u220f i=1 C(ui, i |M)\u03b3\nWe further assume the noise samples are K times more frequent than positive path instance samples. The posterior probability that a given sample came from the path instance samples following the given meta-path is\nPr+(Pe1 eL |M) Pr+(Pe1 eL |M) +K \u00b7 Pr\u2212(Pe1 eL |M)\nSince we would like to fit Pr(Pe1 eL |M) to Pr+(Pe1 eL |M), we simply maximize the following expectation.\nLM =EP r+ [ log Pr(Pe1 eL |M)\nPr(Pe1 eL |M) +KPr\u2212(Pe1 eL |M) ] +K EP r\u2212 [ log\nKPr\u2212(Pe1 eL |M) Pr(Pe1 eL |M) +KPr\u2212(Pe1 eL |M) ] Suppose we are using the sequential definition as Eq. (3).\nThe loss function derived from NCE becomes LM,seq =EP r+ [ log\u03c3(\u2206e1 eL|M) ]\n+K EP r\u2212 [ log ( 1\u2212 \u03c3(\u2206e1 eL|M)) )]\nwhere \u2206e1 eL|M = \u2211L i=1 f(ui, vi,M)\u2212log ( K\u00b7Pr\u2212(Pe1 eL |M) ) and \u03c3(\u00b7) is the sigmoid function. Note that when deriving the above equation, we used exp(f(u, v,M)) in place of Pr(v|u,M), ignoring the normalization term in Eq. (1). We can do this because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution [7, 17]. The above expectation is also studied in [16], which replaces \u2206e1 eL|M with \u2211L i=1 f(ui, vi,M) for ease of computation and names the method negative sampling. We follow this idea and have the approximation as follows.\nLM,seq \u2248 \u2211\nPe1 eL following M\nlog\u03c3( L\u2211\ni=1\nf(ui, vi,Mi,i)) +\n\u2211K k=1\nEPke1 eL\u223cP r \u2212|u1,M\n[ log ( 1\u2212 \u03c3( \u2211L\ni=1 f(uki , v k i ,Mi,i)) )] (3)\nAlgorithm 1: ESim Training Framework Require: HIN G = (V,E,R), a user-specified meta-path M, sampling times t, and negative sampling ratio K Return: Vertex Embedding Vectors xu, \u2200u initialize parameters \u00b5,p\u00b7,q\u00b7,x\u00b7 while not converge do\nfor i = 1 to t do p+ \u2190 a sampled positive path instance following the meta-pathM Optimize for a path instance p+ with label 1. s\u2190 the first vertex on p+ for k = 1 to K do\np\u2212 \u2190 a sampled negative path instance following the meta-pathM starting from s Optimize for a path instance p\u2212 with label 0.\nreturn x\u00b7\nThe following loss function under the pairwise setting can be derived from NCE utilizing the same approximation. LM,pair \u2248 \u2211\nPe1 eL following M\nlog\u03c3( L\u2211\ni=1 L\u2211 j=i f(ui, vj ,Mi,j)) +\n\u2211K k=1\nEPke1 eL\u223cP r \u2212|u1,M\n[ log ( 1\u2212 \u03c3( \u2211L\ni=1 \u2211L j=i f(uki , v k j ,Mi,j)) )] (4)\nOnline Similarity Search. For any interested pairs of vertices u and v, their similarity is defined by the cosine similarity between xu and xv, i.e., sim(u, v) = xu\nT xv \u2016xu\u2016\u00b7\u2016xv\u2016 .\nWe choose the cosine similarity metric instead of the function f in Eq. (1) because the norm of vectors xu and xv do not help the similarity search task [22]. Moreover, cosine similarity is equivalent to Euclidean distance when ||xu|| = ||xv|| = 1, which makes the top-k similar vertices of the given vertex u able to be efficiently solved using approximate nearest neighbors search [18] after normalizations."}, {"heading": "4.2 Optimization Algorithm", "text": "The number of vertex pairs \u3008u, v\u3009 that are connected by some path instances following at least one of user-specified meta-paths can be O(|V |2) in the worst case, which is too large for storage or processing when |V | is at the level of millions and even billions, and thus makes it impossible to directly handle the projected networks over the meta-paths. Therefore, sampling a subset of path instances according to their distribution becomes the best and most feasible choice when optimizing, instead of going through every path instance per iteration. Thus even if the network itself contains a large number of edges, our method is still very efficient. The details of our training framework is shown in Algorithm 1. Once a path instance following the meta-path M has been sampled, the gradient decent method is used to update the parameters xu,xv,pM,qM, and \u00b5M one by one. As a result, our sampling-based training framework (Algorithm 1) becomes a stochastic gradient decent framework. The derivations of these gradients are easy and thus are omitted. Moreover, many prior studies have shown that the stochastic gradient descent can be parallelized without any locks. For example, Hogwild [21] provides a general and lock-free strategy for fully parallelizing any stochastic gradient descent algorithms in a shared memory. We utilize this technique to speed up our optimization via multi-threads.\nAlgorithm 2: Pre-computation of C(u, i|M) Require: HIN G = (V,E,R) and meta-path M = \u3008r1, r2, . . . , rL\u3009 Return: C(u, i|M) /* initialization */ for each vertex u \u2208 V do\nif u is possibly as the second vertex in rL then C(u, L+ 1|M)\u2190 1 else C(u, L+ 1|M)\u2190 0\n/* dynamic programming */ for i\u2190 L to 1 do\nfor each vertex u \u2208 V do C(u, i|M)\u2190 0 for each type ri edge \u3008u, v, ri\u3009 do C(u, i|M)\u2190 C(u, i|M) + C(v, i+ 1|R)\nReturn: C(u, i|M)\n4.2.1 Efficient Sampling Given a length-L meta-path M = \u3008r1, r2, . . . , rL\u3009, there\nmight be O(|V |L) different path instances in total. It becomes an obstacle for storing all the path instances while directly sampling over them takes a constant time. We propose to run a dynamic programming algorithm computing auxiliary numbers so that the online sampling part can be done in a constant time. Pre-computation. As mentioned in Sec. 3, the probability of sampling a path instance following the meta-path M is only related to C(u, i|M), which represents the count of path instances following the meta-pathM with the ith vertex being u. First, we need to figure out the boundary case. When i = L + 1, for any vertex u, if it is possible to be the next vertex in an edge of rL (i.e., it could be vL), we have C(u, L+ 1|M) = 1. Otherwise, it should be 0. Then, we derive the following recursion formula when 1 \u2264 i \u2264 L for any vertex u.\nC(u, i|M) = \u2211\nv|\u3008u,v,ri\u3009\u2208E\nC(v, i+ 1|M) (5)\nAn efficient way to do the summation in this formula is to traverse all type-ri edges starting from vertex u as shown in Algorithm 2. Its detailed time complex analysis will be presented later. Online Sampling. Based on the pre-computed C(u, i|M), one can easily figure out an efficient online sampling method for path instances following the user-given meta-path M. The key idea is to sample the vertices on the path instance one by one. That is, the i-th vertex is conditioned on the previous i\u22121 vertices. As shown in Algorithm 3, the sampling pool for the i-th vertex is restricted to the adjacent vertices (via type-ri edges) of the previous (i\u2212 1)-th vertex.\nHowever, things are a little different when dealing with the negative path instances. First, the negative path instances are associated with a positive path instance and thus the first vertex is fixed. Second, the remaining vertices on the negative path instances are independent. Therefore, they are all sampled from V based on \u221d C(u, i|M)\u03b3 , \u2200i > 1.\n4.2.2 Weighted Combination Sometimes, due to the subtle semantic meanings of the\nAlgorithm 3: Sample a positive path instance p+\nRequire: HIN G = (V,E,R), meta-path M = \u3008r1, r2, . . . , rL\u3009, C(u, i|M), and weighting factor \u03b3 Return: a positive path instance followingM u1 \u2190 a random vertex \u221d C(u1, 1|M)\u03b3 from V for i = 1 to L do\nVi \u2190 {v|\u3008ui, v\u3009 \u2208 Eri} /* \u03b3 is only applied at the first vertex when\nsampling positive path instances. */ vi \u2190 a random vertex \u221d C(ui, i|M) from Vi if i < L then\nui+1 \u2190 vi return Pe1 eL = \u3008e1 = \u3008u1, v1, r1\u3009, e2 = \u3008u2, v2, r2\u3009, . . . , eL = \u3008uL, vL, rL\u3009\u3009\nsimilarity, instead of a single meta-path, the weighted combination of n meta-paths could enhance the performance of similarity search. Suppose {\u03bb1, \u03bb2, . . . , \u03bbn} are the weights (\u2200i, \u03bbi > 0 and \u2211n i=1 \u03bbi = 1), the unified loss function becomes the weighted sum over the loss functions of individual meta-paths based on the weights. That is, Lseq =\u2211n\ni=1 \u03bbiLMi,seq and Lpair = \u2211n\ni=1 \u03bbiLMi,pair. The Algorithm 1 can be modified accordingly by first sampling a meta-pathM from \u2200j, Pr(Mj) = \u03bbj in the beginning of the \u201cwhile\u201d loop.\nThe weighted combination of meta-paths can be either explicitly specified by users or learned from a set of similar/dissimilar examples provided by users. Such user-guided meta-path generation has been studied in [27] without considering embedding. Because weight learning is beyond the scope of this paper, we leave such an extension to embeddingbased similarity search to a future work, and adopt grid searches to obtain the best weighted combination of metapaths in Sec. 5.3 assuming we have the groundtruth.\n4.2.3 Complexity Analysis Pre-computation. For a given length-L meta-pathM, we have u \u2208 V and 1 \u2264 i \u2264 L + 1, which means the memory complexity is O(|V |L). For each given i, we only have to consider all type-ri edges starting from different vertices, which implies the time complexity is O((|V |+ |E|)L). Online sampling. We have adopted the alias method [31] to make the online sampling from any discrete distribution O(1). Therefore, we have to precompute all discrete distributions and restore them in the memory. Given a length-L mete-pathM, for the negative sampling, we have O(L) different distributions and each of them is over |V | discrete values; for the positive sampling there are O(|V |L) different distributions but the number of variables depends on the number of edges of the certain type. The total discrete values they have is O(|E|L). In summary, both the time and memory complexities of the preparation of the alias method are O((|V |+ |E|)L), while the time complexity of sampling any path instance becomes O(L). Optimization for a path instance. For a specific path instance Pe1 eL , the time complexity is O(dL), and O(dL2) for different loss functions Lseq and Lpair respectively. Overall. In our framework, for Algorithm 1, there are O(tK) times of path optimization per iteration. Suppose there are T iterations before convergence, considering the choices of different loss functions, Lseq and Lpair, the over-\nall time complexity is O(TtKLd) and O(TtKL2d) respectively. In addition, considering the complicated case of n user-specified meta-paths, O(n(|V |+|E|)L) has to be paid for pre-computations before sampling, where L is the maximum length of given meta-paths. Parallelization. It has been proved that stochastic gradient descent can be fully parallelized without locks [21]. If we use k threads, although the pre-computations remain the same, the time complexity of training can be k times faster."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we evaluate our proposed model ESim, comparing with several state-of-the-art methods on two realworld large-scale HINs both quantitatively and qualitatively. Also, we evaluate the sensitivity of parameters and the efficiency of ESim."}, {"heading": "5.1 Datasets", "text": "Table 1 shows the statistics of two real-world large-scale HINs: DBLP and Yelp. The first dataset, DBLP, is a bibliographic network in computer science, which includes papers (P), authors (A), venues (V), and terms (T) as four types of vertices and takes P as the center in a star network schema. There are 3 types of undirected edges: P\u2212A, P\u2212V , and P\u2212T . The interesting meta-paths that may be specified by users are the co-authorship meta-path A\u2212P\u2212A, the shared venue meta-path A\u2212P\u2212V\u2212P\u2212A, and the shared term meta-path A\u2212P\u2212T\u2212P\u2212A. We have the following two groupings labeled by human experts. \u2022 Research Area. We use the 4-area grouping in DBLP labeled by human experts, which was used when evaluating PathSim [26]. There are 3,750 authors from 4 diverse research domains of computer science: \u201cdata mining\u201d, \u201cdatabase\u201d, \u201cmachine learning\u201d and \u201cinformation retrieval\u201d. \u2022 Research Group. This grouping in DBLP is also la-\nbeled by human experts and is more fine-grained comparing to the Research Area grouping. There are 103 authors from 4 research groups: \u201cChristos Faloutsos\u201d, \u201cJiawei Han\u201d, \u201cMichael I. Jordan\u201d, and \u201cDan Roth\u201d. The second dataset, Yelp, is a social media network of Yelp, released in Yelp Dataset Challenge1. This network includes businesses (B), words in business names (N), reviews of businesses (R), and words in reviews (W) as vertices. There are 3 different types of undirected edges: B\u2212N , B\u2212R, and R\u2212W . The interesting meta-paths that may be specified by users are the shared review word meta-path B\u2212R\u2212W\u2212R\u2212B and the shared name word meta-path B\u2212N\u2212B. We have the following two groupings extracted from the meta-data provided in Yelp dataset. 1https://www.yelp.com/academic_dataset\n\u2022 Business Type. There are various business types in the Yelp dataset, such as \u201crestaurants\u201d, \u201chotels\u201d, \u201cshopping\u201d, and \u201chealth and medical\u201d. Businesses with multiple categories have been discarded to avoid ambiguity. To keep the results from being biased by some dominating types, we randomly sample 881 businesses from each of these four types as labeled data, because the 4-th popular type contains that many businesses. \u2022 Restaurant Type. Since the majority of businesses in the Yelp dataset are restaurants, we look deep into them by dividing them into different types of cuisines. More specifically, we have sampled 270 restaurants from 5 cuisines respectively: \u201cChinese\u201d, \u201cJapanese\u201d, \u201cIndian\u201d, \u201cMexican\u201d, and \u201cGreek\u201d. As a result, there are in total 1350 labeled restaurants in our labeled dataset."}, {"heading": "5.2 Experimental Setting", "text": "Meta-path. We select different meta-paths for different datasets to see how the meta-paths will reflect the userpreferred similarity and affect the performance. In addition, we run grid searches against different groupings to obtain the best weights of different meta-paths for ESim models. Compared Algorithms and Notations. We select the previous state-of-the-art algorithm in the meta-path guided similarity search problem, PathSim, which has been reported to beat many other simiarity search methods, for example, SimRank [11], P-PageRank [10], random walk, and pairwise random walk. In addition, we also consider (heterogeneous) network embedding methods, such as LINE and PTE, which beat other embedding methods like graph factorization [1] and DeepWalk [20]. More details about these methods are as follows. \u2022 PathSim [26] is a meta-path guided similarity search algorithm which utilizes the normalized count of path instances following the user selected meta-path between any pair of vertices. When the meta-path involves text (e.g., A\u2212P\u2212T\u2212P\u2212A), PathSim becomes a text-based similarity \u2014 the cosine similarity using bag-of-words. \u2022 LINE [29] is an embedding algorithm specifically designed\nfor homogeneous networks, which considers both first and second order information in a network (i.e., the neighbors and the neighbors of the neighbors). By treating all vertices and edges in the HIN as homogeneous ones, we can directly apply LINE and denote the model with first order only as LINE-1st and the model using the second order information as LINE-2nd respectively. One can also project the HIN to a weighted homogeneous network based on the user selected meta-path and apply LINE. However, based on our experiments, the results are always worse than PTE and thus omitted. \u2022 PTE [28] decomposes a HIN to a set of edgewise bipartite networks and then learn embedding vectors. To adapt this method to our settings, the way with the best performance we discovered is to project the HIN to a weighted bipartite HIN based on the user selected meta-path. For example, if the selected meta-path is the shared venue meta-path A\u2212P\u2212V\u2212P\u2212A in the bibliographic network, we construct a bipartite HIN consisting of A and V , where the weight of edges between any pair of a type-A vertex and a type-V vertex equals to the numbers of path instances following A\u2212P\u2212V between them.\nESim refers to our proposed meta-path guided embedding model. Considering the choice of loss functions LM,seq and\nLM,pair, the corresponding model are denoted as ESim-seq and ESim-pair respectively. Default Parameters. The parameter \u03b3 controlling the effect of overly-popular vertices is set to 3/4 inspired from [16]. The dimension of the vertex embedding vectors, d, is set to 50. The negative sampling ratio K is set to 5, whereas the sampling times t is set to 1 million by default. The number of working cores is set to 16. Talking about the initialization of global bias, local bias vectors, and embedding vectors, we assign all parameters as a random real value uniformly in [\u22121, 1]. The learning rate in stochastic gradient descent is initialized as 0.25 and later linearly decreased. Machine. The following experiments on execution time were all conducted on a machine equipped two Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz with 20 physical cores in total. Our framework is fully implemented in C++2."}, {"heading": "5.3 AUC Evaluations", "text": "Although it is hard to obtain the labels of the detailed rankings among all pairs of vertices, it is relatively easy to give an estimation based on the labels of the vertex groupings l(\u00b7). Considering the ranking problem for each individual vertex u, if we rank the other vertices based on the similarity scores, it is very natural that we expect the vertices from the same group (similar ones) are at the top of the ranking list whereas the dissimilar ones are at the bottom of the list. More specifically, we define the AUC score as follows. For a better similarity metric, the AUC score should be larger.\nAUC = 1 |V | \u2211 u\u2208V\n\u2211 v,v\u2032\u2208V\u2227l(u)=l(v)\u2227l(u)6=l(v\u2032)\n1sim(u,v)>sim(u,v\u2032)\u2211 v,v\u2032\u2208V\u2227l(u)=l(v)\u2227l(u) 6=l(v\u2032) 1\nNote that the models that generate similarity measures are learned from the whole dataset, whereas the AUC metric is calculated only in the subset of vertices where we have group labels. The subset is usually small because computing AUC needs pairwise similarities among the subset. We have the following observations from Tables 2 and 3. First, about single meta-path we have: \u2022 User-guidance is crucial. The choice of the user-selected meta-path is really important and affects the performance of user-guided models significantly. For example, A\u2212P\u2212V\u2212P\u2212A works the best in the Research Area grouping, where the shared venue is more telling. However,\n2The source code will be published in the author\u2019s GitHub after acceptance.\nMeta-path Combination. We choose the best performing meta-paths A\u2212P\u2212A and A\u2212P\u2212V\u2212P\u2212A in the DBLP dataset and run grid searches for best weights to achieve highest AUC scores in Research Area and Research Group groupings respectively. Because the best performing meta-path in Yelp dataset is always B\u2212R\u2212W\u2212R\u2212B, any combination is useless in this case. Note that this grid search against grouping labels shows an upper bound of the highest possible AUC score, which can be rarely achieved without knowing labels. As shown in Table 2, the improvement of best weighted meta-paths is marginal. Therefore, weighted combination might be necessary to achieve the best performance but the choice of meta-paths is more important."}, {"heading": "5.4 Visualizations", "text": "With embedding vector for each vertex, we can show meaningful visualizations, which layout the vertices of the same type in a two-dimensional space, and check whether the boundaries between different groups are relatively clear. Taking the DBLP dataset as an example, we visualize the vertices with different colors regarding to their group labels in the Research Area grouping and the Research Group grouping. Their embedding vectors are projected to a 2-D space using the t-SNE package [30], which is a nonlinear dimensionality reduction technique and well suited for projecting high-dimensional data into a low dimensional space. Laying out these vertex embedding vectors is challenging, especially for the vertices in the four closely related research areas: \u201cdata mining\u201d, \u201cdatabase\u201d, \u201cmachine learning\u201d and \u201cinformation retrieval\u201d. For different embedding-based meth-\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n60 65\n70 75\n80 85\n90\nDimension\nA U\nC\n12 25 50 100 200\n\u25cf Research Area A\u2212P\u2212V\u2212P\u2212A Research Group A\u2212P\u2212A Business Type B\u2212R\u2212W\u2212R\u2212B Restaurant Type B\u2212R\u2212W\u2212R\u2212B\n(a) Varying d.\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n70 75\n80 85\n90\nNegative Ratio\nA U\nC\n1 3 5 7 9\n\u25cf Research Area A\u2212P\u2212V\u2212P\u2212A Research Group A\u2212P\u2212A Business Type B\u2212R\u2212W\u2212R\u2212B Restaurant Type B\u2212R\u2212W\u2212R\u2212B\n(b) Varying K.\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf\n2 4 6 8 10\n70 75\n80 85\n90\nSamples (million)\nA U\nC\n\u25cf Research Area A\u2212P\u2212V\u2212P\u2212A Research Group A\u2212P\u2212A Business Type B\u2212R\u2212W\u2212R\u2212B Restaurant Type B\u2212R\u2212W\u2212R\u2212B\n(c) Varying total samples.\nFigure 2: Parameter Sensitivity of ESim-pair.\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n0.2 0.4 0.6 0.8 1.0\n0 10\n00 30\n00 50\n00\nPortion\nR un\nni ng\nT im\ne (s\n)\n\u25cf DBLP A\u2212P\u2212V\u2212P\u2212A DBLP A\u2212P\u2212A Yelp B\u2212R\u2212W\u2212R\u2212B\n(a) Varying network sizes.\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n5 10 15\n0 5\n10 15\n# of cores\nS pe\ned up\nR at\nio\n\u25cf DBLP A\u2212P\u2212V\u2212P\u2212A DBLP A\u2212P\u2212A Yelp B\u2212R\u2212W\u2212R\u2212B\n(b) Varying cores.\nFigure 3: Efficiency of ESim-pair.\n(a) ESim-pair (b) PTE (c) LINE-1st\nFigure 4: Visualization of embedding vectors of 10% random sampled authors in DBLP Research Area grouping when M=A\u2212P\u2212V\u2212P\u2212A. Colors correspond to research areas: Database, Data Mining, Information Retrieval, and Machine Learning.\nods (i.e., ESim, PTE, and LINE), we choose to visualize their variants holding the best AUC performance, i.e., ESimpair using A\u2212P\u2212V\u2212P\u2212A, PTE using A\u2212P\u2212V\u2212P\u2212A, and LINE-1st. As shown in Fig. 4, we visualize 10% random samples of 3, 750 authors from 4 research domains of computer science. Better embedding vectors should lead to a clearer figure where the boundaries between different colored points should be clean and almost not interfering with each other. Based on the visualizations, one can easily observe that ESim-pair using A\u2212P\u2212V\u2212P\u2212A provides the best embedding vectors judged from this criterion. Similarly, the visualizations of the Research Groups grouping based on the models with best AUC performance are shown in Fig. 5. Our proposed model ESim-pair using A\u2212P\u2212A clearly beats PTE using A\u2212P\u2212A and LINE-1st. The significant improvements over PTE and LINE observed via visualization are consistent with the previous evaluations."}, {"heading": "5.5 Parameter Sensitivity", "text": "We select the best performing models ESim-pair to study the parameter sensitivity, such as using A\u2212P\u2212V\u2212P\u2212A in the Research Area grouping, using A\u2212P\u2212A in the Research Group grouping, and using B\u2212R\u2212W\u2212R\u2212B in both Business Type and Restaurant Type groupings. We vary the parameter values and see how the AUC performance changes. Based on the curves in Fig. 2(a), we can observe that setting the dimension (d) of vertex embedding vectors as 50\nis reasonable, because too small d cannot sufficiently capture the semantics, while too large d may lead to some overfitting problem. Fig. 2(b) indicates that the AUC scores are not sensitive to the negative sample ratio K and K = 5 is a good choice. As shown in Fig. 2(c), as more samples are optimized during training, the AUC scores keep an increasing trend and finally converge."}, {"heading": "5.6 Scalability and Parallelization", "text": "We investigate the efficiency of ESim by considering both the scalability and the parallelization as shown in Fig. 3(a). We try different portions of network sizes (i.e., |V |+ |E|) in the two networks and run our best performing models, i.e., ESim-pair using A\u2212P\u2212V\u2212P\u2212A and using A\u2212P\u2212A on the DBLP dataset, as well as ESim-pair using B\u2212R\u2212W\u2212R\u2212B on the Yelp dataset. Based on these curves, the running time is linear to the size of networks while the longer metapath costs a little more time, which are consistent with our previous theoretical time complexity analysis. We vary the number of working cores and run our models on the DBLP and Yelp datasets. The results are plotted in Fig. 3(b). The speedup is quite close to linear, which shows that ESim is quite scalable to the number of working cores."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we propose a general embedding-based similarity search framework for heterogeneous information networks (HINs). Our proposed model, ESim, incorporates given meta-paths and network structures to learn vertex embedding vectors. The similarity defined by the cosine similarity between vertex embeddings of the same type has demonstrated its effectiveness, outperforming the previous state-of-the-art algorithms on two real-world large-scale HINs. The efficiency of ESim has also been evaluated and proved to be scalable. There are several directions to further extend this work. First, instead of similarities between vertices of the same type, one can also explore the relevances between vertices of different types. Second, a mechanism could be developed to automatically learn and extract a set of interesting metapaths or their weighted combinations from user-provided rankings or preferences. Third, similarity is the fundamental operation for mining and exploring HINs. This study on similarity measure, defined in HINs based on meta-path guided embedding, and its efficient computations will impact other searching and mining problems in HINs. For example, it is necessary to re-examine clustering, classification and prediction functions in HINs by reconsidering the similarity measures defined based on meta-path guided embedding. Also, mining outliers in networks can be formulated as finding a small subset of vertices with extremely low similarities to other vertices or clusters."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWW, pages 37\u201348, Republic and Canton of Geneva, Switzerland, 2013. International World Wide Web Conferences Steering Committee.\n[2] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. volume 51, pages 117\u2013122, New York, NY, USA, Jan. 2008. ACM.\n[3] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In SIGKDD, pages 119\u2013128, New York, NY, USA, 2015. ACM.\n[4] H. Cho, B. Berger, and J. Peng. Diffusion component analysis: Unraveling functional topology in biological networks. In Research in Computational Molecular Biology, pages 62\u201364. Springer, 2015.\n[5] P. L. Duren. Theory of Hp spaces, volume 38. IMA, 1970.\n[6] K. Gu, J. Miller, and P. Liang. Traversing knowledge graphs in vector space. In EMNLP 2015, 2015.\n[7] M. U. Gutmann and A. Hyv\u00e4rinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. JMLR, 13(1):307\u2013361, 2012.\n[8] D. Hallac, J. Leskovec, and S. Boyd. Network lasso: Clustering and optimization in large graphs. In SIGKDD, pages 387\u2013396, New York, NY, USA, 2015. ACM.\n[9] P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality. In STOC, pages 604\u2013613, New York, NY, USA, 1998. ACM.\n[10] G. Iv\u00e1n and V. Grolmusz. When the web meets the cell: using personalized pagerank for analyzing protein interaction networks. Bioinformatics, 27(3):405\u2013407, 2011.\n[11] G. Jeh and J. Widom. Simrank: A measure of structural-context similarity. In SIGKDD, pages 538\u2013543, New York, NY, USA, 2002. ACM.\n[12] G. Jeh and J. Widom. Scaling personalized web search. In WWW, pages 271\u2013279, New York, NY, USA, 2003. ACM.\n[13] N. Katayama and S. Satoh. The sr-tree: An index structure for high-dimensional nearest neighbor queries. In SIGMOD, pages 369\u2013380, New York, NY, USA, 1997. ACM.\n[14] M. Levandowsky and D. Winter. Distance between sets. Nature, 234(5323):34\u201335, 1971.\n[15] C. Meng, R. Cheng, S. Maniu, P. Senellart, and W. Zhang. Discovering meta-paths in large heterogeneous information networks. In WWW, pages 754\u2013764. International World Wide Web Conferences Steering Committee, 2015.\n[16] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119, 2013.\n[17] A. Mnih and Y. W. Teh. A fast and simple algorithm\nfor training neural probabilistic language models. In ICML, 2012.\n[18] M. Muja and D. G. Lowe. Fast approximate nearest neighbors with automatic algorithm configuration. VISAPP (1), 2, 2009.\n[19] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.\n[20] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701\u2013710, New York, NY, USA, 2014. ACM.\n[21] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, pages 693\u2013701, 2011.\n[22] A. M. Schakel and B. J. Wilson. Measuring word significance using distributed representations of words. arXiv preprint arXiv:1508.02297, 2015.\n[23] J. Shang, T. Chen, H. Li, Z. Lu, and Y. Yu. A parallel and efficient algorithm for learning to match. In ICDM, pages 971\u2013976. IEEE, 2014.\n[24] A. Singhal. Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 24(4):35\u201343, 2001.\n[25] Y. Sun and J. Han. Mining heterogeneous information networks: A structural analysis approach. SIGKDD Explor. Newsl., 14(2):20\u201328, Apr. 2013.\n[26] Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. VLDB 2011, 2011.\n[27] Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu. Integrating meta-path selection with user-guided object clustering in heterogeneous information networks. In SIGKDD, pages 1348\u20131356, New York, NY, USA, 2012. ACM.\n[28] J. Tang, M. Qu, and Q. Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In SIGKDD, pages 1165\u20131174. ACM, 2015.\n[29] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067\u20131077, Republic and Canton of Geneva, Switzerland, 2015. International World Wide Web Conferences Steering Committee.\n[30] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. JMLR, 9(2579-2605):85, 2008.\n[31] A. J. Walker. An efficient method for generating discrete random variables with general distributions. ACM Trans. Math. Softw., 3(3):253\u2013256, Sept. 1977.\n[32] R. Xie, Z. Liu, J. Jia, H. Luan, and M. Sun. Representation learning of knowledge graphs with entity descriptions. In AAAI, 2016.\n[33] P. N. Yianilos. Data structures and algorithms for nearest neighbor search in general metric spaces. In SODA, pages 311\u2013321, Philadelphia, PA, USA, 1993. Society for Industrial and Applied Mathematics.\n[34] X. Yu, Y. Sun, B. Norick, T. Mao, and J. Han. User guided entity similarity search using meta-path selection in heterogeneous information networks. In CIKM, pages 2025\u20132029, New York, NY, USA, 2012. ACM."}], "references": [{"title": "Distributed large-scale natural graph factorization", "author": ["A. Ahmed", "N. Shervashidze", "S. Narayanamurthy", "V. Josifovski", "A.J. Smola"], "venue": "WWW, pages 37\u201348, Republic and Canton of Geneva, Switzerland", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["S. Chang", "W. Han", "J. Tang", "G.-J. Qi", "C.C. Aggarwal", "T.S. Huang"], "venue": "SIGKDD, pages 119\u2013128, New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Diffusion component analysis: Unraveling functional topology in biological networks", "author": ["H. Cho", "B. Berger", "J. Peng"], "venue": "Research in Computational Molecular Biology, pages 62\u201364. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of Hp spaces", "author": ["P.L. Duren"], "venue": "volume 38. IMA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1970}, {"title": "Traversing knowledge graphs in vector space", "author": ["K. Gu", "J. Miller", "P. Liang"], "venue": "EMNLP 2015", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Noise-contrastive estimation of unnormalized statistical models", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "with applications to natural image statistics. JMLR, 13(1):307\u2013361", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Network lasso: Clustering and optimization in large graphs", "author": ["D. Hallac", "J. Leskovec", "S. Boyd"], "venue": "SIGKDD, pages 387\u2013396, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC, pages 604\u2013613, New York, NY, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "When the web meets the cell: using personalized pagerank for analyzing protein interaction networks", "author": ["G. Iv\u00e1n", "V. Grolmusz"], "venue": "Bioinformatics, 27(3):405\u2013407", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Simrank: A measure of structural-context similarity", "author": ["G. Jeh", "J. Widom"], "venue": "SIGKDD, pages 538\u2013543, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Scaling personalized web search", "author": ["G. Jeh", "J. Widom"], "venue": "WWW, pages 271\u2013279, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "The sr-tree: An index structure for high-dimensional nearest neighbor queries", "author": ["N. Katayama", "S. Satoh"], "venue": "SIGMOD, pages 369\u2013380, New York, NY, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Distance between sets", "author": ["M. Levandowsky", "D. Winter"], "venue": "Nature, 234(5323):34\u201335", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1971}, {"title": "Discovering meta-paths in large heterogeneous information networks", "author": ["C. Meng", "R. Cheng", "S. Maniu", "P. Senellart", "W. Zhang"], "venue": "WWW, pages 754\u2013764. International World Wide Web Conferences Steering Committee", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast and simple algorithm  for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 2", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "SIGKDD, pages 701\u2013710, New York, NY, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Measuring word significance using distributed representations of words", "author": ["A.M. Schakel", "B.J. Wilson"], "venue": "arXiv preprint arXiv:1508.02297", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A parallel and efficient algorithm for learning to match", "author": ["J. Shang", "T. Chen", "H. Li", "Z. Lu", "Y. Yu"], "venue": "ICDM, pages 971\u2013976. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Modern information retrieval: A brief overview", "author": ["A. Singhal"], "venue": "IEEE Data Eng. Bull., 24(4):35\u201343", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Mining heterogeneous information networks: A structural analysis approach", "author": ["Y. Sun", "J. Han"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "VLDB 2011", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating meta-path selection with user-guided object clustering in heterogeneous information networks", "author": ["Y. Sun", "B. Norick", "J. Han", "X. Yan", "P.S. Yu", "X. Yu"], "venue": "SIGKDD, pages 1348\u20131356, New York, NY, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["J. Tang", "M. Qu", "Q. Mei"], "venue": "SIGKDD, pages 1165\u20131174. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "WWW, pages 1067\u20131077, Republic and Canton of Geneva, Switzerland", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "An efficient method for generating discrete random variables with general distributions", "author": ["A.J. Walker"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1977}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": "AAAI", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["P.N. Yianilos"], "venue": "SODA, pages 311\u2013321, Philadelphia, PA, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1993}, {"title": "User guided entity similarity search using meta-path selection in heterogeneous information networks", "author": ["X. Yu", "Y. Sun", "B. Norick", "T. Mao", "J. Han"], "venue": "CIKM, pages 2025\u20132029, New York, NY, USA", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 10, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 7, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 28, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 22, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 24, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 25, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 24, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 28, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 27, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 25, "context": "PathSim [26] proposes to use meta-paths to define and guide similarity search in HIN, with good success.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Along another line of study, network-embedding techniques have been recently explored for homogeneous information networks, which treat all vertices and edges as of the same type, represented by LINE [29].", "startOffset": 200, "endOffset": 204}, {"referenceID": 27, "context": "An alternative and better way is to first project the HIN to several bipartite (assuming user-given meta-paths are symmetric) or homogeneous networks and then apply the edge-wise HIN embedding technique, such as PTE [28].", "startOffset": 216, "endOffset": 220}, {"referenceID": 24, "context": ", venues and papers) [25] and leads to a rather densely connected network (since many authors may publish in many venues).", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "PathSim [26] defines the similarity between two vertices of the same type by the normalized count of path instances following a user-specified meta-path between any pair of vertices.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "[26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11].", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "[26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 26, "context": "Moreover, [27] shows that user guidance can be transformed to a weighted combination of meta-paths.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "LINE [29] and DeepWalk [20] utilize the network link information to construct latent vectors for vertex classification and link prediction.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "LINE [29] and DeepWalk [20] utilize the network link information to construct latent vectors for vertex classification and link prediction.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "DCA [4] starts from the personalized PageRank but does further decomposition to get better protein-protein interaction predictions in biology networks.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "propose to incorporate deep neural networks to train embedding vectors for both text and images at the same time [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 27, "context": "Under a supervised setting, PTE [28] utilizes labels of words and constructs bipartite HINs to learn predictive embedding vectors for words.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "Embedding techniques have been also applied to knowledge graphs to resolve question-answering tasks [6] and retain knowledge relations between entities [32].", "startOffset": 100, "endOffset": 103}, {"referenceID": 31, "context": "Embedding techniques have been also applied to knowledge graphs to resolve question-answering tasks [6] and retain knowledge relations between entities [32].", "startOffset": 152, "endOffset": 156}, {"referenceID": 23, "context": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5].", "startOffset": 136, "endOffset": 140}, {"referenceID": 4, "context": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 32, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 12, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 8, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 17, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 1, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 33, "context": "Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15].", "startOffset": 89, "endOffset": 101}, {"referenceID": 26, "context": "Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15].", "startOffset": 89, "endOffset": 101}, {"referenceID": 14, "context": "Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15].", "startOffset": 89, "endOffset": 101}, {"referenceID": 18, "context": "In particular, we encode the meta-path through the following formulation inspired from [19, 23]:", "startOffset": 87, "endOffset": 95}, {"referenceID": 22, "context": "In particular, we encode the meta-path through the following formulation inspired from [19, 23]:", "startOffset": 87, "endOffset": 95}, {"referenceID": 15, "context": "\u03b3 is a widely used parameter to control the effect of overly-popular vertices, which is usually 3/4 inspired from [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities [7, 17].", "startOffset": 246, "endOffset": 253}, {"referenceID": 16, "context": "Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities [7, 17].", "startOffset": 246, "endOffset": 253}, {"referenceID": 6, "context": "We can do this because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution [7, 17].", "startOffset": 185, "endOffset": 192}, {"referenceID": 16, "context": "We can do this because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution [7, 17].", "startOffset": 185, "endOffset": 192}, {"referenceID": 15, "context": "The above expectation is also studied in [16], which replaces \u2206e1 eL|M with \u2211L i=1 f(ui, vi,M) for ease of computation and names the method negative sampling.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "(1) because the norm of vectors xu and xv do not help the similarity search task [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Moreover, cosine similarity is equivalent to Euclidean distance when ||xu|| = ||xv|| = 1, which makes the top-k similar vertices of the given vertex u able to be efficiently solved using approximate nearest neighbors search [18] after normalizations.", "startOffset": 224, "endOffset": 228}, {"referenceID": 20, "context": "For example, Hogwild [21] provides a general and lock-free strategy for fully parallelizing any stochastic gradient descent algorithms in a shared memory.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "Such user-guided meta-path generation has been studied in [27] without considering embedding.", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "We have adopted the alias method [31] to make the online sampling from any discrete distribution O(1).", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "It has been proved that stochastic gradient descent can be fully parallelized without locks [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "We use the 4-area grouping in DBLP labeled by human experts, which was used when evaluating PathSim [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "We select the previous state-of-the-art algorithm in the meta-path guided similarity search problem, PathSim, which has been reported to beat many other simiarity search methods, for example, SimRank [11], P-PageRank [10], random walk, and pairwise random walk.", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "We select the previous state-of-the-art algorithm in the meta-path guided similarity search problem, PathSim, which has been reported to beat many other simiarity search methods, for example, SimRank [11], P-PageRank [10], random walk, and pairwise random walk.", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "In addition, we also consider (heterogeneous) network embedding methods, such as LINE and PTE, which beat other embedding methods like graph factorization [1] and DeepWalk [20].", "startOffset": 155, "endOffset": 158}, {"referenceID": 19, "context": "In addition, we also consider (heterogeneous) network embedding methods, such as LINE and PTE, which beat other embedding methods like graph factorization [1] and DeepWalk [20].", "startOffset": 172, "endOffset": 176}, {"referenceID": 25, "context": "\u2022 PathSim [26] is a meta-path guided similarity search algorithm which utilizes the normalized count of path instances following the user selected meta-path between any pair of vertices.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "\u2022 LINE [29] is an embedding algorithm specifically designed for homogeneous networks, which considers both first and second order information in a network (i.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "\u2022 PTE [28] decomposes a HIN to a set of edgewise bipartite networks and then learn embedding vectors.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "The parameter \u03b3 controlling the effect of overly-popular vertices is set to 3/4 inspired from [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Their embedding vectors are projected to a 2-D space using the t-SNE package [30], which is a nonlinear dimensionality reduction technique and well suited for projecting high-dimensional data into a low dimensional space.", "startOffset": 77, "endOffset": 81}], "year": 2016, "abstractText": "Most real-world data can be modeled as heterogeneous information networks (HINs) consisting of vertices of multiple types and their relationships. Search for similar vertices of the same type in large HINs, such as bibliographic networks and business-review networks, is a fundamental problem with broad applications. Although similarity search in HINs has been studied previously, most existing approaches neither explore rich semantic information embedded in the network structures nor take user\u2019s preference as a guidance. In this paper, we re-examine similarity search in HINs and propose a novel embedding-based framework. It models vertices as low-dimensional vectors to explore network structureembedded similarity. To accommodate user preferences at defining similarity semantics, our proposed framework, ESim, accepts user-defined meta-paths as guidance to learn vertex vectors in a user-preferred embedding space. Moreover, an efficient and parallel sampling-based optimization algorithm has been developed to learn embeddings in large-scale HINs. Extensive experiments on real-world large-scale HINs demonstrate a significant improvement on the effectiveness of ESim over several state-of-the-art algorithms as well as its scalability.", "creator": "LaTeX with hyperref package"}}}