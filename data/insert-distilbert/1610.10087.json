{"id": "1610.10087", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Tensor Switching Networks", "abstract": "as we just present a novel neural network algorithm, the tensor switching ( ts ) transit network, which generalizes the rectified linear unit ( relu ) nonlinearity to tensor - valued hidden units. the ts network copies its relative entire input vector transformation to different locations in an outward expanded representation, with the location determined by its hidden unit relaxation activity. in this way, even a simple linear readout from the ts representation can implement a highly expressive deep - network - like function. the ts network hence avoids the vanishing gradient problem by construction, generally at the cost of larger representation size. we develop several methods to train the ts network, including equivalent kernels for infinitely wide and deep ts networks, a one - pass linear learning algorithm, and two backpropagation - inspired representation learning algorithms. our experimental processing results already demonstrate that the ts network is indeed more expressive and consistently learns faster than standard relu networks.", "histories": [["v1", "Mon, 31 Oct 2016 19:44:50 GMT  (415kb,D)", "http://arxiv.org/abs/1610.10087v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["chuan-yung tsai", "andrew m saxe", "david d cox"], "accepted": true, "id": "1610.10087"}, "pdf": {"name": "1610.10087.pdf", "metadata": {"source": "CRF", "title": "Tensor Switching Networks", "authors": ["Chuan-Yung Tsai", "Andrew Saxe", "David Cox"], "emails": ["chuanyungtsai@fas.harvard.edu", "asaxe@fas.harvard.edu", "davidcox@fas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity. In this work we develop a novel deep learning algorithm, the Tensor Switching (TS) network, which generalizes the ReLU such that each hidden unit conveys a tensor, instead of scalar, yielding a more expressive model. Like the ReLU network, the TS network is a linear function of its input, conditioned on the activation pattern of its hidden units. By separating the decision to activate from the analysis performed when active, even a linear classifier can reach back across all layers to the input of the TS network, implementing a deep-network-like function while avoiding the vanishing gradient problem [5], which can otherwise significantly slow down learning in deep networks. The trade-off is the representation size.\nWe exploit the properties of TS networks to develop several methods suitable for learning in different scaling regimes, including their equivalent kernels for SVMs on small to medium datasets, a one-pass linear learning algorithm which visits each data point only once for use with very large but simpler datasets, and two backpropagation-inspired representation learning algorithms for more generic use. Our experimental results show that TS networks are indeed more expressive and consistently learn faster than standard ReLU networks.\nRelated work is briefly summarized as follows. With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8]. Reordering activation and analysis is proposed by [9]. On tackling the vanishing gradient problem, tensor methods are used by [10] to train single-hidden-layer networks. Convex learning and inference in various deep architectures can be found in [11, 12, 13] too. Finally, conditional linearity of deep ReLU networks is also used by [14], mainly to analyze their performance. In comparison, the TS network does not simply reorder or sever activation and analysis within each hidden layer. Instead, it is a cross-layer generalization of these concepts, which can be applied with most of the recent deep learning architectures [15, 9], not only to increase their expressiveness, but also to help avoiding the vanishing gradient problem (see Sec. 2.3).\n\u2217Equal contribution.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n10 08\n7v 1\n[ cs\n.N E\n] 3\n1 O\nct 2\n01 6"}, {"heading": "2 Tensor Switching Networks", "text": "In the following we first construct the definition of shallow (single-hidden-layer) TS networks, then generalize the definition to deep TS networks, and finally describe their qualitative properties. For simplicity, we only show fully-connected architectures using the ReLU nonlinearity. However, other popular nonlinearities, e.g. max pooling and maxout [16], in addition to ReLU, are also supported in both fully-connected and convolutional architectures."}, {"heading": "2.1 Shallow TS Networks", "text": "The TS-ReLU network is a generalization of standard ReLU networks that permits each hidden unit to convey an entire tensor of activity (see Fig. 1). To describe it, we build up from the standard ReLU network. Consider a ReLU layer with weight matrix W1 \u2208 Rn1\u00d7n0 responding to an input vector X0 \u2208 Rn0 . The resulting hidden activity X1 \u2208 Rn1 of this layer is X1 = max (0n1 ,W1X0) = H (W1X0) \u25e6 (W1X0) where H is the Heaviside step function, and \u25e6 denotes elementwise product. The rightmost equation splits apart each hidden unit\u2019s decision to activate, represented by the term H (W1X0), from the information (i.e. result of analysis) it conveys when active, denoted by W1X0. We then go one step further to rewrite X1 as\nX1 = H (W1X0)\u2297X0\ufe38 \ufe37\ufe37 \ufe38 Z1 W1 \u00d7 1n0 , (1) where we have made use of the following tensor operations: vector-tensor cross product C = A\u2297 B =\u21d2 ci,j,k,... = aibj,k,..., tensor-matrix Hadamard product C = A B =\u21d2 c...,j,i = a...,j,ibj,i and tensor summative reduction C = A\u00d7 1n =\u21d2 c...,k,j = \u2211n i=1 a...,k,j,i. In (1), the input vector X0 is first expanded into a new matrix representation Z1 \u2208 Rn1\u00d7n0 with one row per hidden unit. If a hidden unit is active, the input vector X0 is copied to the corresponding row. Otherwise, the row is filled with zeros. Finally, this expanded representation Z1 is collapsed back by projection onto W1.\nThe central idea behind the TS-ReLU network is to learn a linear classifier directly from the rich, expanded representation Z1, rather than collapsing it back to the lower dimensional X1. That is, in a standard ReLU network, the hidden layer activity X1 is sent through a linear classifier fX (WXX1) trained to minimize some loss function LX (fX). In the TS-ReLU network, by contrast, the expanded representation Z1 is sent to a linear classifier fZ (WZ vec (Z1)) with loss function LZ (fZ). Each TS-ReLU neuron thus transmits a vector of activities (a row of Z1), compared to a standard ReLU neuron that transmits a single scalar (see Fig. 1). Because of this difference, in the following we call the standard ReLU network a Scalar Switching ReLU (SS-ReLU) network."}, {"heading": "2.2 Deep TS Networks", "text": "The construction given above generalizes readily to deeper networks. Define a nonlinear expansion operation as X\u2295W = H (WX)\u2297X and linear contraction operation as Z W = (Z W)\u00d71n, such that (1) becomes Xl = ((Xl\u22121 \u2295Wl) Wl)\u00d71nl\u22121 = Xl\u22121\u2295Wl Wl for a given layer l\nwith Xl \u2208 Rnl and Wl \u2208 Rnl\u00d7nl\u22121 . A deep SS-ReLU network with L layers may then be expressed as a sequence of alternating expansion and contraction steps,\nXL = X0 \u2295W1 W1 \u00b7 \u00b7 \u00b7 \u2295WL WL. (2)\nTo obtain the deep TS-ReLU network, we further define the ternary expansion operation Z\u2295X W = H (WX)\u2297 Z, such that the decision to activate is based on the SS-ReLU variables X, but the entire tensor Z is transmitted when the associated hidden unit is active. Let Z0 = X0. The l-th layer activity tensor of a TS network can then be written as Zl = H (WlXl\u22121) \u2297 Zl\u22121 = Zl\u22121 \u2295Xl\u22121 Wl \u2208 Rnl\u00d7nl\u22121\u00d7\u00b7\u00b7\u00b7\u00d7n0 . Thus compared to a deep SS-ReLU network, a deep TS-ReLU network simply omits the contraction stages,\nZL = Z0 \u2295X0 W1 \u00b7 \u00b7 \u00b7 \u2295XL\u22121 WL. (3)\nBecause there are no contraction steps, the order of Zl \u2208 Rnl\u00d7nl\u22121\u00d7\u00b7\u00b7\u00b7\u00d7n0 grows with depth, adding an additional dimension for each layer. One interpretation of this scheme is that, if a hidden unit at layer l is active, the entire tensor Zl\u22121 is copied to the appropriate position in Zl.1 Otherwise a tensor of zeros is copied. Another equivalent interpretation is that the input vector X0 is copied to a given position Zl(i, j, . . . , k, :) only if hidden units i, j, . . . , k at layers l, l \u2212 1, . . . , 1 respectively are all active. Otherwise, Zl(i, j, . . . , k, :) = 0n0 . Hence activity propagation in the deep TS-ReLU network preserves the layered structure of a deep SS-ReLU network, in which a chain of hidden units across layers must activate for activity to propagate from input to output."}, {"heading": "2.3 Properties", "text": "The TS network decouples a hidden unit\u2019s decision to activate (as encoded by the activation weights {Wl}) from the analysis performed on the input when the unit is active (as encoded by the analysis weights WZ). This distinguishing feature leads to the following 3 properties.\nCross-layer analysis. Since the TS representation preserves the layered structure of a deep network and offers direct access to the entire input (parcellated by the activated hidden units), a simple linear readout can effectively reach back across layers to the input and thus implicitly learns analysis weights for all layers at one time in WZ. Therefore it avoids the vanishing gradient problem by construction.2\nError-correcting analysis. As activation and analysis are severed, a careful selection of the analysis weights can \u201cclean up\u201d a certain amount of inexactitude in the choice to activate, e.g. from noisy or even random activation weights. While for the SS network, bad activation also implies bad analysis.\nFine-grained analysis. To see this, we consider single-hidden-layer TS and SS networks with just one hidden unit. The TS unit, when active, conveys the entire input vector, and hence any full-rank linear map from input to output may be implemented. The SS unit, when active, conveys just a single scalar, and hence can only implement a rank-1 linear map between input and output. By choosing the right analysis weights, a TS network can always implement an SS network,3 but not vice versa. As such, it clearly has greater modeling capacity for a fixed number of hidden units.\nAlthough the TS representation is highly expressive, it comes at the cost of an exponential increase in the size of its representation with depth, i.e. \u220f l nl. This renders TS networks of substantial width and depth very challenging (except as kernels). But as we will show, the expressiveness permits TS networks to perform fairly well without having to be extremely wide and deep, and often noticeably better than SS networks of the same sizes. Also, TS networks of useful sizes still can be implemented with reasonable computing resources, especially when combined with techniques in Sec. 4.3."}, {"heading": "3 Equivalent Kernels", "text": "In this section we derive equivalent kernels for TS-ReLU networks with arbitrary depth and an infinite number of hidden units at each layer, with the aim of providing theoretical insight into how TS-ReLU is analytically different from SS-ReLU. These kernels represent the extreme of infinite (but unlearned) features, and might be used in SVM on datasets of small to medium sizes.\n1For convolutional networks using max pooling, the convolutional-window-sized input patch winning the max pooling is copied. In other words, different nonlinearities only change the way the input is switched.\n2It is in spirit similar to models with skip connections to the output [17, 18], although not exactly reducible. 3Therefore TS networks are also universal function approximators [19].\nConsider a single-hidden-layer TS-ReLU network with n1 hidden units in which each element of the activation weight matrix W1 \u2208 Rn1\u00d7n0 is i.i.d. zero mean Gaussian with arbitrary standard deviation \u03c3. The infinite-width random TS-ReLU kernel between two vectors x,y \u2208 Rn0 is the dot product between their expanded representations (scaled by \u221a 2/n1 for convenience) in the limit\nof infinite hidden units, kTS1 (x,y) = limn1\u2192\u221e vec (\u221a 2/n1 x\u2295W1 )\u1d40 vec (\u221a 2/n1 y \u2295W1 ) =\n2E [H (w\u1d40x)H (w\u1d40y)]x\u1d40y, where w \u223c N ( 0, \u03c32I ) is a n0-dimensional random Gaussian vector. The expectation is the probability that a randomly chosen vector w lies within 90 degrees of both x and y. Because w is drawn from an isotropic Gaussian, if x and y differ by an angle \u03b8, then only the fraction \u03c0\u2212\u03b82\u03c0 of randomly drawn w will be within 90 degrees of both, yielding the equivalent kernel of a single-hidden-layer infinite-width random TS-ReLU network given in (5).4\nkSS1 (x,y) = k\u0304 SS (\u03b8)x\u1d40y =\n( 1\u2212 tan \u03b8 \u2212 \u03b8\n\u03c0\n) x\u1d40y (4)\nkTS1 (x,y) = k\u0304 TS (\u03b8)x\u1d40y =\n( 1\u2212 \u03b8\n\u03c0\n) x\u1d40y (5)\nFigure 2 compares (5) against the linear kernel and the single-hidden-layer infinite-width random SS-ReLU kernel (4) from [20] (see Linear, TS L = 1 and SS L = 1). It has two important qualitative features. First, it has discontinuous derivative at \u03b8 = 0, and hence a much sharper peak than the other kernels.5 Intuitively this means that a very close match counts for much more than a moderately close match. Second, unlike the SS-ReLU kernel which is non-negative everywhere, the TS-ReLU kernel still has a negative lobe, though it is substantially reduced relative to the linear kernel. Intuitively this means that being dissimilar to a support vector can provide evidence against a particular classification, but this negative evidence is much weaker than in a standard linear kernel.\nTo derive kernels for deeper TS-ReLU networks, we need to consider the deeper SS-ReLU kernels as well, since its activation and analysis are severed, and the activation instead depends on its SS-ReLU counterpart. Based upon the recursive formulation from [20], first we define the zeroth-layer kernel k\u20220 (x,y) = x \u1d40y and the generalized angle \u03b8\u2022l = cos \u22121 (k\u2022l (x,y)/\u221ak\u2022l (x,x) k\u2022l (y,y)), where \u2022\ndenotes SS or TS. Then we can easily get kSSl+1 (x,y) = k\u0304 SS ( \u03b8SSl ) kSSl (x,y),\n6 and kTSl+1 (x,y) = k\u0304TS ( \u03b8SSl ) kTSl (x,y), where k\u0304 \u2022 follows (4) or (5) accordingly.\nFigure 2 also plots the deep TS-ReLU and SS-ReLU kernels as a function of depth. The shape of these kernels reveals sharply divergent behavior between the TS and SS networks. As depth increases, the equivalent kernel of the TS network falls off ever more rapidly as the angle between input vectors increases. This means that vectors must be an ever closer match to retain a high kernel value. As argued earlier, this highlights the ability of the TS network to pick up on and amplify small differences between inputs, resulting in a quasi-nearest-neighbor behavior. In contrast, the equivalent kernel of the SS network limits to one as depth increases. Thus, rather than amplifying small differences, it collapses them with depth such that even very dissimilar vectors receive high kernel values.\n4This proof is succinct using a geometric view, while a longer proof can be found in the Supplementary Material. As the kernel is directly defined as a dot product between feature vectors, it is naturally a valid kernel.\n5Interestingly, a similar kernel is also observed by [21] for models with explicit skip connections. 6We write (4) and kSSl differently from [20] for cleaner comparisons against TS-ReLU kernels. However they\nare numerically unstable expressions and are not used in our experiments to replace the original ones in [20]."}, {"heading": "4 Learning Algorithms", "text": "In the following we present 3 learning algorithms suitable for different scenarios. One-pass ridge regression in Sec. 4.1 learns only the linear readout (i.e. analysis weights WZ), leaving the hiddenlayer representations (i.e. activation weights {Wl}) random, hence it is convex and exactly solvable. Inverted backpropagation in Sec. 4.2 learns both analysis and activation weights. Linear RotationCompression in Sec. 4.3 also learns both weights, but learns activation weights in an indirect way."}, {"heading": "4.1 Linear Readout Learning via One-pass Ridge Regression", "text": "In this scheme, we leverage the intuition that precision in the decision for a hidden unit to activate is less important than carefully tuned analysis weights, which can in part compensate for poorly tuned activation weights. We randomly draw and fix the activation weights {Wl}, and then solve for the analysis weights WZ using ridge regression, which can be done in a single pass through the dataset. First, each data point p = 1, . . . , P is expanded into its tensor representation ZpL and then accumulated into the correlation matrices CZZ = \u2211 p vec (Z p L) vec (Z p L) \u1d40 and CyZ = \u2211 p y p vec (ZpL) \u1d40. After all data points are processed once, the analysis weights are determined as WZ = CyZ (CZZ + \u03bbI) \u22121 where \u03bb is an L2 regularization parameter.\nUnlike a standard SS network, which in this setting would only be able to select a linear readout from the top hidden layer to the final classification decision, the TS network offers direct access to entire input vectors, parcellated by the hidden units they activate. In this way, even a linear readout can effectively reach back across layers to the input, implementing a complex function not representable with an SS network with random filters. However, this scheme requires high memory usage, which is on the order of O (\u220fL l=0 n 2 l ) for storing CZZ, and even higher computation cost7 for solving WZ, which makes deep architectures (i.e. L > 1) impractical. Therefore, this scheme may best suit online learning applications which allow only one-time access to data, but do not require a deep classifier."}, {"heading": "4.2 Representation Learning via Inverted Backpropagation", "text": "The ridge regression learning uses random activation weights and only learns analysis weights. Here we provide a \u201cgradient-based\u201d procedure to learn both weights. Learning the analysis weights (i.e. the final linear layer) WZ simply requires \u2202LZ\u2202WZ , which is generally easy to compute. However, since the activation weights Wl in the TS network only appear inside the Heaviside step function H with zero (or undefined) derivative, the gradient \u2202LZ\u2202Wl is also zero. To bypass this, we introduce a sequence of auxiliary variables Al defined by A0 = ZL and the recursion Al = Al\u22121 Wl \u2208 RnL\u00d7nL\u22121\u00d7\u00b7\u00b7\u00b7\u00d7nl . We then derive the pseudo gradient using the proposed inverted backpropagation as\n\u2202\u0302LZ \u2202Wl = \u2202LZ \u2202A0 ( \u2202A1 \u2202A0 )\u2020 \u00b7 \u00b7 \u00b7 ( \u2202Al \u2202Al\u22121 )\u2020 \u2202Al \u2202Wl , (6)\nwhere \u2020 denotes Moore\u2013Penrose pseudoinverse. Because the Al\u2019s are related via the linear contraction operator, these derivatives are non-zero and easy to compute. We find this works sufficiently well as a non-zero proxy for \u2202LZ\u2202Wl .\n7Nonetheless this is a one-time cost and still can be advantageous over other slowly converging algorithms.\nOur motivation with this scheme is to \u201crecover\u201d the learning behavior in SS networks. To see this, first note that AL = A0 W1 \u00b7 \u00b7 \u00b7 WL = XL (see Fig. 3). This reflects the fact that the TS and SS networks are linear once the active set of hidden units is known, such that the order of expansion and contraction steps has no effect on the final output. Hence the linear contraction steps, which alternate with expansion steps in (3), can instead be gathered at the end after all expansion steps. The gradient in the SS network is then\n\u2202LX \u2202Wl = \u2202LX \u2202AL \u2202AL \u2202AL\u22121 \u00b7 \u00b7 \u00b7 \u2202Al+1 \u2202Al \u2202Al \u2202Wl = \u2202LX \u2202AL \u2202AL \u2202AL\u22121 \u00b7 \u00b7 \u00b7 \u2202A1 \u2202A0\ufe38 \ufe37\ufe37 \ufe38\n\u2202LX \u2202A0\n( \u2202A1 \u2202A0 )\u2020 \u00b7 \u00b7 \u00b7 ( \u2202Al \u2202Al\u22121 )\u2020 \u2202Al \u2202Wl .\n(7)\nReplacing \u2202LX\u2202A0 in (7) with \u2202LZ \u2202A0\n, such that the expanded representation may influence the inverted gradient, we recover (6). Compared to one-pass ridge regression, this scheme controls the memory and time complexities at O ( \u220f l nl), which makes training of a moderately-sized TS network on modern computing resources feasible. The ability to train activation weights also relaxes the assumption that analysis weights can \u201cclean up\u201d inexact activations caused by using even random weights."}, {"heading": "4.3 Indirect Representation Learning via Linear Rotation-Compression", "text": "Although the inverted backpropagation learning controls memory and time complexities better than the one-pass ridge regression, the exponential growth of a TS network\u2019s representation still severely constrains its potential toward being applied in recent deep learning architectures, where network width and depth can easily go beyond, e.g., a thousand. In addition, the success of recent deep learning architectures also heavily depends on the acceleration provided by highly-optimized GPU-enabled libraries, where the operations of the previous learning schemes are mostly unsupported.\nTo address these 2 concerns, we provide a standard backpropagation-compatible learning algorithm, where we no longer keep separate X and Z variables. Instead we define Xl = W\u2217l vec (Xl\u22121 \u2295Wl), which directly flattens the expanded representation and linearly projects it against W\u2217l \u2208 Rn \u2217 l\u00d7nlnl\u22121 . In this scheme, even though Wl still lacks a non-zero gradient, the W\u2217l\u22121 of the previous layer can be learned using backpropagation to properly \u201crotate\u201d Xl\u22121, such that it can be utilized by Wl and the TS nonlinearity. Therefore, the representation learning here becomes indirect. To simultaneously control the representation size, one can easily let n\u2217l < nlnl\u22121 such that W \u2217 l becomes \u201ccompressive.\u201d Interestingly, we find n\u2217l = nl often works surprisingly well, which suggests linearly compressing the expanded TS representation back to the size of an SS representation can still retain its advantage, and thus is used as the default. This scheme can also be combined with inverted backpropagation if learning Wl is still desired.\nTo understand why linear compression does not remove the TS representation power, we note that it is not equivalent to the linear contraction operation , where each tensor-valued unit is down projected independently. Linear compression introduces extra interaction between tensor-valued units. Another way to view the linear compression\u2019s role is through kernel analysis as shown in Sec. 3\u2014adding a linear layer does not change the shape of a given TS kernel."}, {"heading": "5 Experimental Results", "text": "Our experiments focus on comparing TS and SS networks with the goal of determining how the TS nonlinearities differ from their SS counterparts. SVMs using SS-ReLU and TS-ReLU kernels are implemented in Matlab based on libsvm-compact [22]. TS networks and all 3 learning algorithms in Sec. 4 are implemented in Python based on Numpy\u2019s ndarray data structure. Both implementations utilize multicore CPU acceleration. In addition, TS networks with only the linear rotation-compression learning are also implemented in Keras, which enjoys much faster GPU acceleration.\nWe adopt 3 datasets, viz. MNIST, CIFAR10 and SVHN2, where we reserve the last 5,000 training images for validation. We also include SVHN2\u2019s extra training set (except for SVMs8) in the training process, and zero-pad MNIST images such that all datasets have the same spatial resolution\u201432\u00d732.\n8Due to the prohibitive kernel matrix size, as SVMs here can only be solved in the dual form.\nFor SVMs, we grid search for both kernels with depth from 1 to 10, C from 1 to 1, 000, and PCA dimension reduction of the images to 32, 64, 128, 256, or no reduction. For SS and TS networks with fully-connected (i.e. MLP) architectures, we grid search for depth from 1 to 3 and width (including PCA of the input) from 32 to 256 based on our Python implementation. For SS and TS networks with convolutional (i.e. CNN) architectures, we adopt VGG-style [15] convolutional layers with 3 standard SS convolution-max pooling blocks,9 where each block can have up to three 3 \u00d7 3 convolutions, plus 1 to 3 fully-connected SS or TS layers of fixed width 256. CNN experiments are based on our Keras implementation. For all MLPs and CNNs, we universally use SGD with learning rate 10\u22123, momentum 0.9, L2 weight decay 10\u22123 and batch size 128 to reduce the grid search complexity by focusing on architectural hyperparameters. All networks are trained for 100 epochs on MNIST and CIFAR10, and 20 epochs on SVHN2, without data augmentation. The source code and scripts for reproducing our experiments are available at https://github.com/coxlab/tsnet.\nTable 1 summarizes our experimental results, including both one-pass (i.e. first-epoch) and asymptotic (i.e. all-epoch) error rates and the corresponding depths (for CNNs, convolutional and fully-connected layers are listed separately). The TS nonlinearities perform better in almost all categories, confirming our theoretical insights in Sec. 2.3\u2014the cross-layer analysis (as evidenced by their low error rates after only one epoch of training), the error-correcting analysis (on MNIST and CIFAR10, for instance, the one-pass error rates of TS MLP RR using fixed random activation are close to the asymptotic error rates of TS MLP LRC and IBP-LRC with trained activation), and the fine-grained analysis (the TS networks in general achieve better asymptotic error rates than their SS counterparts).\n9This decision mainly is to accelerate the experimental process, since TS convolution runs much slower, but we also observe that TS nonlinearities in lower layers are not always helpful. See later for more discussion.\nTo further demonstrate how using TS nonlinearities affects the distribution of performance across different architectures (here, mainly depth), we plot the performance gains (viz. one-pass and asymptotic error rates) introduced by using the TS nonlinearities on all CNN variants in Fig. 4. The fact that most dots are in the first quadrant (and none in the third quadrant) suggests the TS nonlinearities are predominantly beneficial. Also, to ease the concern that the TS networks\u2019 higher complexity may simply consume their advantage on actual run time, we also provide examples of learning progress (i.e. validation error rate) over run time in Fig. 4. The results suggest that even our unoptimized TS network implementation can still provide sizable gains in learning speed.\nFinally, to verify the effectiveness of inverted backpropagation in learning useful activation filters even without the actual gradient, we train single-hidden-layer SS and TS MLPs with 16 hidden units each (without using PCA dimension reduction of the input) and visualize the learned filters in Fig. 5. The results suggest inverted backpropagation functions equally well."}, {"heading": "6 Discussion", "text": "Why do TS networks learn quickly? In general, the TS network sidesteps the vanishing gradient problem as it skips the long chain of linear contractions against the analysis weights (i.e. the auxiliary pathway in Fig. 3). Its linear readout has direct access to the full input vector, which is switched to different parts of the highly expressive expanded representation. This directly accelerates learning. Also, a well-flowing gradient confers benefits beyond the TS layers\u2014e.g. SS layers placed before TS layers also learn faster since the TS layers \u201cself-organize\u201d rapidly, permitting useful error signals to flow to the lower layers faster.10 Lastly, when using the inverted backpropagation or linear rotationcompression learning, although {Wl} or {W\u2217l } do not learn as fast as WZ, and may still be quite random in the first few epochs, the error-correcting nature of WZ can still compensate for the learning progress.\nChallenges toward deeper TS networks. As shown in Fig. 2, the equivalent kernels of deeper TS networks can be extremely sharp and discriminative, which unavoidably hurts invariant recognition of dissimilar examples. This may explain why we find having TS nonlinearities in only higher (instead of all) layers works better, since the lower SS layers can form invariant representations for the higher TS layers to classify. To remedy this, we may need to consider other types of regularization for WZ (instead of L2) or other smoothing techniques [25, 26].\nFuture work. Our main future direction is to improve the TS network\u2019s scalability, which may require more parallelism (e.g. multi-GPU processing) and more customization (e.g. GPU kernels utilizing the sparsity of TS representations), with preferably more memory storage/bandwidth (e.g. GPUs using 3D-stacked memory). With improved scalability, we also plan to further verify the TS nonlinearity\u2019s efficiency in state-of-the-art architectures [27, 9, 18], which are still computationally prohibitive with our current implementation."}, {"heading": "Acknowledgments", "text": "We would like to thank James Fitzgerald, Mien \u201cBrabeeba\u201d Wang, Scott Linderman, and Yu Hu for fruitful discussions. We also thank the anonymous reviewers for their valuable comments. This work was supported by NSF (IIS 1409097), IARPA (contract D16PC00002), and the Swartz Foundation.\n10This is a crucial aspect of gradient descent dynamics in layered structures, which behave like a chain\u2014the weakest link must change first [23, 24]."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit", "author": ["R. Hahnloser", "R. Sarpeshkar", "M. Mahowald", "R. Douglas", "S. Seung"], "venue": "Nature, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "A Field Guide to Dynamical Recurrent Networks, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "A Spike and Slab Restricted Boltzmann Machine", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "AISTATS, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["K. Konda", "R. Memisevic", "D. Krueger"], "venue": "ICLR, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Training Very Deep Networks", "author": ["R. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "NIPS, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "arXiv, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Convex Net: A Scalable Architecture for Speech Pattern Classification", "author": ["L. Deng", "D. Yu"], "venue": "Interspeech, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Input-Convex Deep Networks", "author": ["B. Amos", "Z. Kolter"], "venue": "ICLR Workshop, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex Deep Learning via Normalized Kernels", "author": ["\u00d6. Aslan", "X. Zhang", "D. Schuurmans"], "venue": "NIPS, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of Deep Neural Networks with the Extended Data Jacobian Matrix", "author": ["S. Wang", "A. Mohamed", "R. Caruana", "J. Bilmes", "M. Plilipose", "M. Richardson", "K. Geras", "G. Urban", "O. Aslan"], "venue": "ICML, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Maxout Networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Densely Connected Convolutional Networks", "author": ["G. Huang", "Z. Liu", "K. Weinberger"], "venue": "arXiv, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural network with unbounded activation functions is universal approximator", "author": ["S. Sonoda", "N. Murata"], "venue": "Applied and Computational Harmonic Analysis, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-Margin Classification in Infinite Neural Networks", "author": ["Y. Cho", "L. Saul"], "venue": "Neural Computation, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Avoiding pathologies in very deep networks", "author": ["D. Duvenaud", "O. Rippel", "R. Adams", "Z. Ghahramani"], "venue": "AISTATS, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Scattering Spectrum", "author": ["J. And\u00e9n", "S. Mallat"], "venue": "IEEE T-SP, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "ICLR, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep learning theory of perceptual learning dynamics", "author": ["A. Saxe"], "venue": "COSYNE, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributional Smoothing with Virtual Adversarial Training", "author": ["T. Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "ICLR, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Differential Geometric Regularization for Supervised Learning of Classifiers", "author": ["Q. Bai", "S. Rosenberg", "Z. Wu", "S. Sclaroff"], "venue": "ICML, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 14, "endOffset": 20}, {"referenceID": 1, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 14, "endOffset": 20}, {"referenceID": 2, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 122, "endOffset": 128}, {"referenceID": 3, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 122, "endOffset": 128}, {"referenceID": 4, "context": "By separating the decision to activate from the analysis performed when active, even a linear classifier can reach back across all layers to the input of the TS network, implementing a deep-network-like function while avoiding the vanishing gradient problem [5], which can otherwise significantly slow down learning in deep networks.", "startOffset": 258, "endOffset": 261}, {"referenceID": 5, "context": "With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8].", "startOffset": 177, "endOffset": 186}, {"referenceID": 6, "context": "With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8].", "startOffset": 177, "endOffset": 186}, {"referenceID": 7, "context": "With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8].", "startOffset": 177, "endOffset": 186}, {"referenceID": 8, "context": "Reordering activation and analysis is proposed by [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "On tackling the vanishing gradient problem, tensor methods are used by [10] to train single-hidden-layer networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Convex learning and inference in various deep architectures can be found in [11, 12, 13] too.", "startOffset": 76, "endOffset": 88}, {"referenceID": 11, "context": "Convex learning and inference in various deep architectures can be found in [11, 12, 13] too.", "startOffset": 76, "endOffset": 88}, {"referenceID": 12, "context": "Convex learning and inference in various deep architectures can be found in [11, 12, 13] too.", "startOffset": 76, "endOffset": 88}, {"referenceID": 13, "context": "Finally, conditional linearity of deep ReLU networks is also used by [14], mainly to analyze their performance.", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Instead, it is a cross-layer generalization of these concepts, which can be applied with most of the recent deep learning architectures [15, 9], not only to increase their expressiveness, but also to help avoiding the vanishing gradient problem (see Sec.", "startOffset": 136, "endOffset": 143}, {"referenceID": 8, "context": "Instead, it is a cross-layer generalization of these concepts, which can be applied with most of the recent deep learning architectures [15, 9], not only to increase their expressiveness, but also to help avoiding the vanishing gradient problem (see Sec.", "startOffset": 136, "endOffset": 143}, {"referenceID": 1, "context": "[2 3]", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[2 3]", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[2 3] Tensor Switching ReLU Z1 W1 1", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[2 3] Tensor Switching ReLU Z1 W1 1", "startOffset": 0, "endOffset": 5}, {"referenceID": 15, "context": "max pooling and maxout [16], in addition to ReLU, are also supported in both fully-connected and convolutional architectures.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "It is in spirit similar to models with skip connections to the output [17, 18], although not exactly reducible.", "startOffset": 70, "endOffset": 78}, {"referenceID": 17, "context": "It is in spirit similar to models with skip connections to the output [17, 18], although not exactly reducible.", "startOffset": 70, "endOffset": 78}, {"referenceID": 18, "context": "Therefore TS networks are also universal function approximators [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "Figure 2 compares (5) against the linear kernel and the single-hidden-layer infinite-width random SS-ReLU kernel (4) from [20] (see Linear, TS L = 1 and SS L = 1).", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "Based upon the recursive formulation from [20], first we define the zeroth-layer kernel k\u2022 0 (x,y) = x Ty and the generalized angle \u03b8\u2022 l = cos \u22121 (k\u2022 l (x,y)/\u221ak\u2022 l (x,x) k\u2022 l (y,y)), where \u2022 denotes SS or TS.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "Interestingly, a similar kernel is also observed by [21] for models with explicit skip connections.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "We write (4) and k l differently from [20] for cleaner comparisons against TS-ReLU kernels.", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "However they are numerically unstable expressions and are not used in our experiments to replace the original ones in [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "SVMs using SS-ReLU and TS-ReLU kernels are implemented in Matlab based on libsvm-compact [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "CNN) architectures, we adopt VGG-style [15] convolutional layers with 3 standard SS convolution-max pooling blocks,9 where each block can have up to three 3 \u00d7 3 convolutions, plus 1 to 3 fully-connected SS or TS layers of fixed width 256.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "To remedy this, we may need to consider other types of regularization for WZ (instead of L2) or other smoothing techniques [25, 26].", "startOffset": 123, "endOffset": 131}, {"referenceID": 25, "context": "To remedy this, we may need to consider other types of regularization for WZ (instead of L2) or other smoothing techniques [25, 26].", "startOffset": 123, "endOffset": 131}, {"referenceID": 8, "context": "With improved scalability, we also plan to further verify the TS nonlinearity\u2019s efficiency in state-of-the-art architectures [27, 9, 18], which are still computationally prohibitive with our current implementation.", "startOffset": 125, "endOffset": 136}, {"referenceID": 17, "context": "With improved scalability, we also plan to further verify the TS nonlinearity\u2019s efficiency in state-of-the-art architectures [27, 9, 18], which are still computationally prohibitive with our current implementation.", "startOffset": 125, "endOffset": 136}, {"referenceID": 22, "context": "This is a crucial aspect of gradient descent dynamics in layered structures, which behave like a chain\u2014the weakest link must change first [23, 24].", "startOffset": 138, "endOffset": 146}, {"referenceID": 23, "context": "This is a crucial aspect of gradient descent dynamics in layered structures, which behave like a chain\u2014the weakest link must change first [23, 24].", "startOffset": 138, "endOffset": 146}], "year": 2016, "abstractText": "We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.", "creator": "LaTeX with hyperref package"}}}