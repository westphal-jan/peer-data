{"id": "1611.01423", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "the question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. recent scientific work on theoretical program induction has proposed strong neural architectures, based on abstractions like stacks, turing machines, and interpreters, that operate on abstract computational machines or on execution traces. but the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by its symbolic representations that have syntactic structure, such as logical expressions and source code. combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. as a step in this direction, we propose naming a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. presently these networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. the challenge is that semantic representations must be computed in a syntax - directed manner, because semantics is compositional, but at closely the same time, small changes in syntax can lead to very large pattern changes in semantics, which can be difficult for continuous neural architectures. we perform an exhaustive evaluation on whether the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms technically existing architectures.", "histories": [["v1", "Fri, 4 Nov 2016 15:30:43 GMT  (1604kb,D)", "https://arxiv.org/abs/1611.01423v1", null], ["v2", "Sat, 10 Jun 2017 19:18:55 GMT  (3178kb,D)", "http://arxiv.org/abs/1611.01423v2", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["miltiadis allamanis", "pankajan chanthirasegaran", "pushmeet kohli", "charles a sutton"], "accepted": true, "id": "1611.01423"}, "pdf": {"name": "1611.01423.pdf", "metadata": {"source": "META", "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "emails": ["<t-mialla@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. This is particularly important while dealing with exponentially large domains such as source code and logical expressions. Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different. Although symbolic reasoning is very powerful, it also tends to be hard. For example, problems such as the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse. This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent semantic relation-\nWork started when M. Allamanis was at Edinburgh. This work was done while P. Kohli was at Microsoft. 1Microsoft Research, Cambridge, UK 2University of Edinburgh, UK 3DeepMind, London, UK 4The Alan Turing Institute, London, UK. Correspondence to: Miltiadis Allamanis <t-mialla@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nships. However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning. In this work, we explore the direction of learning continuous semantic representations of symbolic expressions. The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different. Such representations have the potential to allow a new class of symbolic reasoning methods based on heuristics that depend on the continuous representations, for example, by guiding a search procedure in a symbolic solver based on a distance metric in the continuous space. In this paper, we make a first essential step of addressing the problem of learning continuous semantic representations (SEMVECs) for symbolic expressions. Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors. This is an important but hard problem; learning composable SEMVECs of symbolic expressions requires that we learn about the semantics of symbolic elements and operators and how they map to the continuous representation space, thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature. As we show in our evaluation, relatively simple logical and polynomial expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural network architectures.\nOur work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities. They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions. While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions. In particular, in our experiments we find that TREENNs tend to assign similar representations to syntactically similar expressions, even when they are semantically very different. The underlying conceptual problem is how to develop a continuous representation that follows syntax but not too much,\n1To avoid confusion, we use TREENN for recursive neural networks and RNN for recurrent neural networks.\nar X\niv :1\n61 1.\n01 42\n3v 2\n[ cs\n.L G\n] 1\n0 Ju\nn 20\n17\nthat respects compositionality while also representing the fact that a small syntactic change can be a large semantic one.\nTo tackle this problem, we propose a new architecture, called neural equivalence networks (EQNET). EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree. As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than pairwise decisions. The network architecture is based on composing residual-like multi-layer networks, which allows more flexibility in modeling the semantic mapping up the syntax tree. To encourage representations within an equivalence class to be tightly clustered, we also introduce a training method that we call subexpression autoencoding, which uses an autoencoder to force the representation of each subexpression to be predictable and reversible from its syntactic neighbors. Experimental evaluation on a highly diverse class of symbolic algebraic and boolean expression types shows that EQNETs dramatically outperform existing architectures like TREENNs and RNNs.\nTo summarize, the main contributions of our work are: (a) We formulate the problem of learning continuous semantic representations (SEMVECs) of symbolic expressions and develop benchmarks for this task. (b) We present neural equivalence networks (EQNETs), a neural network architecture that learns to represent expression semantics onto a continuous semantic representation space and how to perform symbolic operations in this space. (c) We provide an extensive evaluation on boolean and polynomial expressions, showing that EQNETs perform dramatically better than state-of-the-art alternatives. Code and data are available at groups.inf.ed.ac.uk/cup/semvec."}, {"heading": "2. Model", "text": "In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations for expressions that are semantically equivalent, i.e. they belong to the same equivalence class. Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.\nProblem Hardness. Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse. For example, if we focus on boolean expressions, reducing an expression to the representation of the false equivalence class amounts to proving its non-satisfiability \u2014 an NPcomplete problem. Of course, we do not expect to circumvent an NP-complete problem with neural networks. A\nnetwork for solving boolean equivalence would require an exponential number of nodes in the size of the expression if P 6= NP . Instead, our goal is to develop architectures that efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions in a given training set. The supplementary material shows a sample of such expressions that illustrate the hardness of this problem.\nNotation and Framework. To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN) (Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula. Given a tree T , TREENNs learn distributed representations for each node in the tree by recursively combining the representations of its subtrees using a neural network. We denote the children of a node n as ch(n) which is a (possibly empty) ordered tuple of nodes. We also use par(n) to refer to the parent node of n. Each node in our tree has a type, e.g. a terminal node could be of type \u201ca\u201d referring to the variable a or of type \u201cand\u201d referring to a node of the logical AND (\u2227) operation. We refer to the type of a node n as \u03c4n. In pseudocode, TREENNs retrieve the representation of a tree T rooted at node \u03c1, by invoking the function TREENN(\u03c1) that returns a vector representation r\u03c1 \u2208 RD, i.e., a SEMVEC. The function is defined as TREENN (current node n)\nif n is not a leaf then rn \u2190 COMBINE(TREENN(c0), . . . , TREENN(ck), \u03c4n), where (c0, . . . , ck) = ch(n) else rn \u2190 LOOKUPLEAFEMBEDDING(\u03c4n)\nreturn rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE. Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network. As discussed next, these will both prove to be serious limitations in our setting. To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2)."}, {"heading": "2.1. Neural Equivalence Networks", "text": "Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent. In this work, we find that standard neural architectures do not handle well this challenge. To represent semantics from syntax, we need to learn to recursively compose and decompose semantic representations and re-\nmove syntactic \u201cnoise\u201d. Any syntactic operation may significantly change semantics (e.g. negation, or appending \u2227FALSE) while we may reach the same semantic state through many possible operations. This necessitates using high-curvature operations over the semantic representation space. Furthermore, some operations are semantically reversible and thus we need to learn reversible semantic representations (e.g. \u00ac\u00acA and A should have an identical SEMVECs). Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a). Our network follows the TREENN architecture, i.e. is implemented using TREENN to model the compositional nature of symbolic expressions but is adapted based on the domain requirements. The extensions we introduce have two aims: first, to improve the network training; and second, and more interestingly, to encourage the learned representations to abstract away surface level information while retaining semantic content.\nThe first extension that we introduce is to the network structure at each layer in the tree. Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node. During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform the input SEMVECs to the output SEMVEC and maintain semantic equivalences, requiring high-curvature operations. Part of the problem stems from the fact that within the Euclidean space of SEMVECs some operations need to be non-linear. For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic representation space. Instead, we turn to multi-layer neural networks. In particular, we define the network as shown\nin the function COMBINE in Figure 1b. This uses a twolayer MLP with a residual-like connection to compute the SEMVEC of each parent node in that syntax tree given that of its children. Each node type \u03c4n, e.g., each logical operator, has a different set of weights. We experimented with deeper networks but this did not yield any improvements.\nHowever, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients. This is essentially because of the highly compositional nature of tree structures, where the same network (i.e. the COMBINE non-linear function) is used recursively, causing it to \u201cecho\u201d its own errors and producing unstable feedback loops. We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree. We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm. That is, we set LOOKUPLEAFEMBEDDING(\u03c4n) = C\u03c4n/ \u2016C\u03c4n\u20162 , and we normalize the output of the final layer of COMBINE in Figure 1b. The normalization step of l\u0304out and C\u03c4n is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016). Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation. As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.\nAlthough these modifications seem to improve the representation capacity of the network and its ability to be trained, we found that they were not on their own sufficient for good performance. In our early experiments, we noticed that the\nnetworks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous space were primarily ones that were syntactically similar. At the same time, we observed that the networks did not learn to unify representations of the same equivalence class, observing multiple syntactically distinct but semantically equivalent expressions to have distant SEMVECs.\nTherefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information. We add a regularization term on the SEMVECs that we call a subexpression autoencoder (SUBEXPAE). We design this regularization to encourage the SEMVECs to have two properties: abstraction and reversibility. Because abstraction arguably means removing irrelevant information, a network with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic information rather than semantic information. To achieve this, we introduce a component that aims to encourage reversibility, which we explain by an example. Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree) it is often possible to completely determine or at least place strong constraints on the semantics of the third. For example, consider a boolean formula F (a, b) = F1(a, b) \u2228 F2(a, b) where F1 and F2 are arbitrary propositional formulae over the variables a, b. Then clearly if we know that F implies that a is true but F1 does not, then F2 must imply that a is true. More generally, if F belongs to some equivalence class e0 and F1 belongs to a different class e1, we want the continuous representation of F2 to reflect that there are strong constraints on the equivalence class of F2.\nSubexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child representations together, to encourage dependence in the representations of parents and children. More specifically, given any node p in the tree with children c0 . . . ck, we can define a parent-children tuple [rc0 , . . . , rck , rp] containing the (computed) SEMVECs of the children and parent nodes. What SUBEXPAE does is to autoencode this representation tuple into a low-dimensional space with a denoising autoencoder. We then seek to minimize the reconstruction error of the child representations (r\u0303c0 , . . . , r\u0303ck ) as well as the reconstructed parent representation r\u0303p that can be computed from the reconstructed children. More formally, we minimize the return value of SUBEXPAE in Figure 1c where n is a binary noise vector with \u03ba percent of its elements set to zero. Note that the encoder is specific to the parent node type \u03c4p. Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs in two major ways. First, SUBEXPAE autoencodes on the\nentire parent-children representation tuple, rather than the child representations alone. Second, the encoding is not used to compute the parent representation, but only serves as a regularizer.\nSubexpression autoencoding has several desirable effects. First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions. Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings. Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization. The goal is to nudge the network to learn consistent, reversible semantics. Additionally, subexpression autoencoding has the potential to gradually unify distant representations that belong to the same equivalence class. To illustrate this point, imagine two semantically equivalent c\u20320 and c \u2032\u2032 0 child nodes of different expressions that\nhave distant SEMVECs, i.e. \u2225\u2225rc\u20320 \u2212 rc\u2032\u20320 \u2225\u22252 although COMBINE(rc\u20320 , . . . ) \u2248 COMBINE(rc\u2032\u20320 , . . . ). In some cases due to the autoencoder noise, the differences between the input tuple x\u2032,x\u2032\u2032 that contain rc\u20320 and rc\u2032\u20320 will be non-existent and the decoder will predict a single location r\u0303c0 (possibly different from rc\u20320 and rc\u2032\u20320 ). Then, when minimizing the reconstruction error, both rc\u20320 and rc\u2032\u20320 will be attracted to r\u0303c0 and eventually should merge."}, {"heading": "2.2. Training", "text": "We train EQNETs from a dataset of expressions whose semantic equivalence is known. Given a training set T = {T1 . . . TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 . . . eJ}. We use a supervised objective similar to classification; the difference between classification and our setting is that whereas standard classification problems consider a fixed set of class labels, in our setting the number of equivalence classes in the training set will vary with N . Given an expression tree T that belongs to the equivalence class ei \u2208 E , we compute the probability\nP (ei|T ) = exp\n( TREENN(T )>qei + bi )\u2211 j exp ( TREENN(T )>qej + bj\n) (1) where qei are model parameters that we can interpret as representations of each equivalence class that appears in the training class, and bi are scalar bias terms. Note that in this work, we only use information about the equivalence class of the whole expression T , ignoring available information about subexpressions. This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to the training set. To train the model, we use a max-margin\nobjective that maximizes classification accuracy, i.e. LACC(T, ei) = max (\n0, arg max ej 6=ei,ej\u2208E log P (ej |T ) P (ei|T ) +m ) (2)\nwhere m > 0 is a scalar margin. And therefore the optimized loss function for a single expression tree T that belongs to equivalence class ei \u2208 E is\nL(T, ei) = LACC(T, ei) + \u00b5 |Q| \u2211 n\u2208Q SUBEXPAE(ch(n), n)\n(3)\nwhere Q = {n \u2208 T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and \u00b5 \u2208 (0, 1] a scalar weight. We found that subexpression autoencoding is counterproductive early in training, before the SEMVECs begin to represent aspects of semantics. So, for each epoch t, we set \u00b5 = 1\u2212 10\u2212\u03bdt with \u03bd \u2265 0. Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions. In practice, we found the optimization to be very unstable, yielding suboptimal performance. We believe that this has to do with the compositional and recursive nature of the task that creates unstable dynamics and the fact that equivalence is a stronger property than similarity."}, {"heading": "3. Evaluation", "text": "Datasets. We generate datasets of expressions grouped into equivalence classes from two domains. The datasets from the BOOL domain contain boolean expressions and the POLY datasets contain polynomial expressions. In both domains, an expression is either a variable, a binary operator that combines two expressions, or a unary operator applied to a single expression. When defining equivalence, we interpret distinct variables as referring to different entities in the domain, so that, e.g., the polynomials c \u00b7 (a \u00b7 a+ b) and f \u00b7(d\u00b7d+e) are not equivalent. For each domain, we generate \u201csimple\u201d datasets which use a smaller set of possible operators and \u201cstandard\u201d datasets which use a larger set of more complex operators. We generate each dataset by exhaustively generating all parse trees up to a maximum tree size. All expressions are symbolically simplified into a canonical from in order to determine their equivalence class and are grouped accordingly. Table 1 shows the datasets we generated. In the supplementary material we present some sample expressions. For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) \u2014 although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials. Learning SEMVECs for boolean expressions is already a hard problem; with n boolean variables, there\nare 22 n\nequivalence classes (i.e. one for each possible truth table). We split the datasets into training, validation and test sets. We create two test sets, one to measure generalization performance on equivalence classes that were seen in the training data (SEENEQCLASS), and one to measure generalization to unseen equivalence classes (UNSEENEQCLASS). It is easiest to describe UNSEENEQCLASS first. To create the UNSEENEQCLASS, we randomly select 20% of all the equivalence classes, and place all of their expressions in the test set. We select equivalence classes only if they contain at least two expressions but less than three times the average number of expressions per equivalence class. We thus avoid selecting very common (and hence trivial to learn) equivalence classes in the testset. Then, to create SEENEQCLASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each class into training, validation, SEENEQCLASS test in the proportions 60%\u201315%\u201325%. We provide the datasets online at groups.inf.ed.ac.uk/cup/semvec.\nBaselines. To compare the performance of our model, we train the following baselines. TF-IDF: learns a representation given the expression tokens (variables, operators and parentheses). This captures topical/declarative knowledge but is unable to capture procedural knowledge. GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation. Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens. We also include two recursive neural networks (TREENN). The 1- layer TREENN which is the original TREENN also used by Zaremba et al. (2014). We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections. This shows the effect of SEMVEC normalization and subexpression autoencoder.\nHyperparameters. We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, . . . , 15) statistics (described next). The selected hyperparameters are detailed in the supplementary material."}, {"heading": "3.1. Quantitative Evaluation", "text": "Metrics. To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of each expression (using cosine similarity) that belong to the same equivalence class. More formally, given a test query expression q in an equivalence class c we find the k nearest neighbors Nk(q) of q across all expressions, and define the\nscore as\nscorek(q) = |Nk(q) \u2229 c| min(k, |c|) . (4)\nTo report results for a given testset, we simply average scorek(q) for all expressions q in the testset. We also report the precision-recall curves for the problem of clustering the SEMVECs into their appropriate equivalence classes.\nEvaluation. Figure 2 presents the average per-model precision-recall curves across the datasets. Table 1 shows score5 of UNSEENEQCLASS. Detailed plots are found in the supplementary material. EQNET performs better for all datasets, by a large margin. The only exception is POLY5, where the 2-L TREENN performs better. However, this may have to do with the small size of the dataset. The reader may observe that the simple datasets (containing fewer operations and variables) are easier to learn. Understandably, introducing more variables increases the size of the represented space reducing performance. The tf-idf method performs better in settings with more variables, because it captures well the variables and operations used. Similar observations can be made for sequence models. The one and two layer TREENNs have mixed performance; we believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional nature of TREENNs. Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture. Our evaluation suggests that EQNETs perform much better within the ONEV-POLY setting.\nEvaluation of Compositionality. We evaluate whether EQNETs successfully learn to compute compositional representations, rather than overfitting to expression trees of\na small size. To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more complex ones; for example, training on the training set of BOOL5 but testing on the testset of BOOL8. We average over 11 different train-test pairs (full list in supplementary material) and show the results in Figure 3a and Figure 3b. These graphs again show that EQNETs are better than any of the other methods, and indeed, performance is only a bit worse than in the non-transfer setting.\nImpact of EQNET Components EQNETs differ from traditional TREENNs in two major ways, which we analyze here. First, SUBEXPAE improves performance. When training the network with and without SUBEXPAE, on average, the area under the curve (AUC) of scorek decreases by 16.8% on the SEENEQCLASS and 19.7% on the UNSEENEQCLASS. This difference is smaller in the transfer setting, where AUC decreases by 8.8% on average. However, even in this setting we observe that SUBEXPAE helps more in large and diverse datasets. The second key difference to traditional TREENNs is the output normalization and the residual connections. Comparing our model to the one-layer and two-layer TREENNs again, we find that output normalization results in important improvements (the two-layer TREENNs have on average 60.9% smaller AUC). We note that only the combination of the residual connections and the output normalization improve the performance, whereas when used separately, there are no significant improvements over the two-layer TREENNs."}, {"heading": "3.2. Qualitative Evaluation", "text": "Table 2 shows expressions whose SEMVEC nearest neighbor is of an expression of another equivalence class. Manually inspecting boolean expressions, we find that EQNET confusions happen more when a XOR or implication operator is\nTable 2. Non semantically equivalent first nearest-neighbors from BOOL8 and POLY8. A checkmark indicates that the method correctly results in the nearest neighbor being from the same equivalence class.\nExpr a \u2227 (a \u2227 (a \u2227 (\u00acc))) a \u2227 (a \u2227 (c\u21d2 (\u00acc))) (a \u2227 a) \u2227 (c\u21d2 (\u00acc)) a+ (c \u00b7 (a+ c)) ((a+ c) \u00b7 c) + a (b \u00b7 b)\u2212 b\ntfidf c \u2227 ((a \u2227 a) \u2227 (\u00aca)) c\u21d2 (\u00ac((c \u2227 a) \u2227 a)) c\u21d2 (\u00ac((c \u2227 a) \u2227 a)) a+ (c+ a) \u00b7 c (c \u00b7 a) + (a+ c) b \u00b7 (b\u2212 b) GRU X a \u2227 (a \u2227 (c \u2227 (\u00acc))) (a \u2227 a) \u2227 (c\u21d2 (\u00acc)) b+ (c \u00b7 (a+ c)) ((b+ c) \u00b7 c) + a (b+ b) \u00b7 b\u2212 b 1L-TREENN a \u2227 (a \u2227 (a \u2227 (\u00acb))) a \u2227 (a \u2227 (c\u21d2 (\u00acb))) (a \u2227 a) \u2227 (c\u21d2 (\u00acb)) a+ (c \u00b7 (b+ c)) ((b+ c) \u00b7 c) + a (a\u2212 c) \u00b7 b\u2212 b EQNET X X (\u00ac(b\u21d2 (b \u2228 c))) \u2227 a X X (b \u00b7 b) \u00b7 b\u2212 b\ninvolved. In fact, we fail to find any confused expressions for EQNET not involving these operations in BOOL5 and in the top 100 expressions in BOOL10. As expected, tf-idf confuses expressions with others that contain the same operators and variables ignoring order. In contrast, GRU and TREENN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply nested variables or operators. In contrast, EQNET tends to confuse fewer expressions (as we previously showed) and the confused expressions tend to be more syntactically diverse and semantically related.\nFigure 4 shows a visualization of score5 for each node in the expression tree. One may see that as EQNET knows how\n\u00ac(c \u2295 (a \u2227 ((a \u2295 c) \u2227 b))) ((c \u2228 (\u00acb))\u21d2 a) \u2227 (a \u21d2 a)\n((b \u2295 (\u00acc)) \u2227 b)\u2295 (a \u2228 b) ((b \u00b7 a)\u2212 a) \u00b7 b\na \u2212 ((a + b) \u00b7 a) ((c \u00b7 b) \u00b7 c) \u00b7 a b + ((b \u00b7 b) \u00b7 b)\nFigure 4. Visualization of score5 for all expression nodes for three BOOL10 and four POLY8 test sample expressions using EQNET. The darker the color, the lower the score, i.e. white implies a score of 1 and dark red a score of 0.\nto compose expressions that achieve good score, even if the subexpressions achieve a worse score. This suggests that for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without merging the equivalence classes or affecting the upstream performance of the network. Larger scale interactive t-SNE visualizations can be found at online.\nFigure 5 presents two PCA visualizations of the SEMVECs of simple expressions and their negations/negatives. It can be discerned that the black dots and their negations (in red) are discriminated in the semantic representation space. Figure 5b shows this property in a clear manner: left-right discriminates between polynomials with 1 and \u2212a, topbottom between polynomials with\u2212b and b and the diagonal parellelt to y = \u2212x between c and\u2212c. We observe a similar behavior in Figure 5a for boolean expressions."}, {"heading": "4. Related Work", "text": "Researchers have proposed compilation schemes that can transform any given program or expression to an equivalent neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-\nmann, 1994). One can consider a serialized version of the resulting neural network as a representation of the expression. However, it is not clear how we could compare the serialized representations corresponding to two expressions and whether this mapping preserves semantic distances.\nRecursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications. Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements. EQNET\u2019s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior. In addition, when encoding each expression our architecture does not use a pooling layer but directly produces a single representation for the expression.\nMou et al. (2016) design tree convolutional networks to classify code into student submission tasks. Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics. Piech et al. (2015) also learn distributed matrix representations of student code submissions. However, to learn the representations, they use input and output program states and do not test for program equivalence. Additionally, these representations do not necessarily represent program equivalence, since they do not learn the representations over all possible input-outputs. Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.\nMore closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions. In contrast, EQNETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned SEMVECs to better\nrepresent equivalence, and do not use search when looking for equivalent expressions. Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence. In the future, SEMVECs may be useful within this area.\nOur work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016). In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network architectures but to learn continuous semantic representations (SEMVECs) of expression semantics irrespectively of how they are syntactically expressed or evaluated."}, {"heading": "5. Discussion & Conclusions", "text": "In this work, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge. SEMVECs have the potential of bridging continuous representations with symbolic representations, useful in multiple applications in artificial intelligence, machine learning and programming languages.\nWe show that EQNETs perform significantly better than state-of-the-art alternatives. But further improvements are needed, especially for more robust training of compositional models. In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic space to be represented. Fixed-sized SEMVECs, like the ones used in EQNET, eventually limit the capacity that is available to represent procedural knowledge. In the future, to represent more complex procedures, variable-sized representations would seem to be required."}, {"heading": "Acknowledgments", "text": "This work was supported by Microsoft Research through its PhD Scholarship Programme and the Engineering and Physical Sciences Research Council [grant number EP/K024043/1]. We thank the University of Edinburgh Data Science EPSRC Centre for Doctoral Training for providing additional computational resources."}, {"heading": "A. Synthetic Expression Datasets", "text": "Table 3 and Table 4 are sample expressions within an equivalence class for the two types of datasets we consider."}, {"heading": "B. Detailed Evaluation", "text": "Figure 6 presents the detailed evaluation for our k-NN metric for each dataset. Figure 7 shows the detailed evaluation when using models trained on simpler datasets but tested\non more complex ones, essentially evaluating the learned compositionality of the models. Figure 9 show how the performance varies across the datasets based on their characteristics. As expected as the number of variables increase, the performance worsens (Figure 9a) and expressions with more complex operators tend to have worse performance (Figure 9b). The results for UNSEENEQCLASS look very similar and are not plotted here."}, {"heading": "C. Model Hyperparameters", "text": "The optimized hyperparameters are detailed in Table 5. All hyperparameters were optimized using the Spearmint (Snoek et al., 2012) Bayesian optimization package. The same range of values was used for all common model hyperparameters.\n5 10\nB O\nO L L5 \u2192\nB O\nO L 8\n5 10\nB O\nO L 5\u2192\nB O\nO L 8\n5 10\nB O\nO L 5\u2192\nB O\nO L 10\n5 10\nP O\nLY 8\u2192\nS IM\nP P\nO LY\n8\n5 10\nS IM\nP P\nO LY\n5\u2192 S\nIM P\nP O\nLY 10\n5 10\nS IM\nP P\nO LY\n8\u2192 S\nIM P\nP O\nLY 10\n5 10\nP O\nLY 5\u2192\nS IM\nP P\nO LY\n10\n5 10\nP O\nLY 8\u2192\nS IM\nP P\nO LY\n10\n5 10\nO N\nE V\n-P O\nLY 10 \u2192\nO N\nE V\n-P O\nLY 13\n5 10\nP O\nLY 8\u2192\nO N\nE V\n-P O\nLY 13\n5 10 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP O\nLY 5\u2192\nP O\nLY 8\n(a) SEENEQCLASS evaluation using model trained on simpler datasets. Caption is \u201cmodel trained on\u201d\u2192\u201cTest dataset\u201d.\n5 10\nB O\nO L L5 \u2192\nB O\nO L 8\n5 10\nB O\nO L 5\u2192\nB O\nO L 8\n5 10\nB O\nO L 5\u2192\nB O\nO L 10\n5 10\nP O\nLY 8\u2192\nS IM\nP P\nO LY\n8\n5 10\nS IM\nP P\nO LY\n5\u2192 S\nIM P\nP O\nLY 10\n5 10\nS IM\nP P\nO LY\n8\u2192 S\nIM P\nP O\nLY 10\n5 10\nP O\nLY 5\u2192\nS IM\nP P\nO LY\n10\n5 10 P\nO LY\n8\u2192 S\nIM P\nP O\nLY 10\n5 10\nO N\nE V\n-P O\nLY 10 \u2192\nO N\nE V\n-P O\nLY 13\n5 10\nP O\nLY 8\u2192\nO N\nE V\n-P O\nLY 13\n5 10 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP O\nLY 5\u2192\nP O\nLY 8\n(b) Evaluation of compositionality. UNSEENEQCLASS evaluation using model trained on simpler datasets. Caption is \u201cmodel trained on\u201d\u2192\u201cTest dataset\u201d.\ntf-idf GRU StackRNN TreeNN-1Layer TreeNN-2Layer EqNet\nFigure 7. Evaluation of compositionality. Evaluation of scorex (y axis) for x = 1, . . . , 15. The markers are shown every five ticks of the x-axis to make the graph more clear. TREENN refers to the model of Socher et al. (2012).\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTr ue\nPo si\ntiv e\nR at\ne\ntf-idf GRU StackRNN\nTreeNN-1Layer TreeNN-2Layer EqNet\n(a) SEENEQCLASS\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTr ue\nPo si\ntiv e\nR at\ne\ntf-idf GRU StackRNN\nTreeNN-1Layer TreeNN-2Layer EqNet\n(b) UNSEENEQCLASS\nFigure 8. Receiver operating characteristic (ROC) curves averaged across datasets."}], "references": [{"title": "DeepMath \u2013 Deep sequence models for premise selection", "author": ["Alemi", "Alex A", "Chollet", "Francois", "Irving", "Geoffrey", "Szegedy", "Christian", "Urban", "Josef"], "venue": "arXiv preprint arXiv:1606.04442,", "citeRegEx": "Alemi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemi et al\\.", "year": 2016}, {"title": "A convolutional attention network for extreme summarization of source code", "author": ["Allamanis", "Miltiadis", "Peng", "Hao", "Sutton", "Charles"], "venue": "In ICML,", "citeRegEx": "Allamanis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer", "Chris", "Ballesteros", "Miguel", "Ling", "Wang", "Matthews", "Austin", "Smith", "Noah A"], "venue": "In ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "A neural compiler", "author": ["Gruau", "Fr\u00e9d\u00e9ric", "Ratajszczak", "Jean-Yves", "Wiber", "Gilles"], "venue": "Theoretical Computer Science,", "citeRegEx": "Gruau et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Gruau et al\\.", "year": 1995}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural GPUs learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "In ICLR,", "citeRegEx": "Kaiser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2016}, {"title": "Deep network guided proof search", "author": ["Loos", "Sarah", "Irving", "Geoffrey", "Szegedy", "Christian", "Kaliszyk", "Cezary"], "venue": "arXiv preprint arXiv:1701.06972,", "citeRegEx": "Loos et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Loos et al\\.", "year": 2017}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou", "Lili", "Li", "Ge", "Zhang", "Lu", "Wang", "Tao", "Jin", "Zhi"], "venue": "In AAAI,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Symbolic processing in neural networks", "author": ["Neto", "Jo\u00e3o Pedro", "Siegelmann", "Hava T", "Costa", "J F\u00e9lix"], "venue": "Journal of the Brazilian Computer Society,", "citeRegEx": "Neto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Neto et al\\.", "year": 2003}, {"title": "Learning program embeddings to propagate feedback on student code", "author": ["Piech", "Chris", "Huang", "Jonathan", "Nguyen", "Andy", "Phulsuksombati", "Mike", "Sahami", "Mehran", "Guibas", "Leonidas J"], "venue": "In ICML,", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Neural programming language", "author": ["Siegelmann", "Hava T"], "venue": "In Proceedings of the 12th National Conference on Artificial Intelligence,", "citeRegEx": "Siegelmann and T.,? \\Q1994\\E", "shortCiteRegEx": "Siegelmann and T.", "year": 1994}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher", "Richard", "Pennington", "Jeffrey", "Huang", "Eric H", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning to discover efficient mathematical identities", "author": ["Zaremba", "Wojciech", "Kurach", "Karol", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.", "startOffset": 44, "endOffset": 105}, {"referenceID": 9, "context": "However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.", "startOffset": 44, "endOffset": 105}, {"referenceID": 20, "context": "However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.", "startOffset": 44, "endOffset": 105}, {"referenceID": 18, "context": "They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.", "startOffset": 45, "endOffset": 66}, {"referenceID": 17, "context": "Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.", "startOffset": 121, "endOffset": 142}, {"referenceID": 18, "context": "To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN) (Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.", "startOffset": 120, "endOffset": 147}, {"referenceID": 19, "context": "Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.", "startOffset": 20, "endOffset": 41}, {"referenceID": 17, "context": "Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs in two major ways.", "startOffset": 72, "endOffset": 93}, {"referenceID": 3, "context": "Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.", "startOffset": 123, "endOffset": 144}, {"referenceID": 20, "context": "For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) \u2014 although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.", "startOffset": 163, "endOffset": 185}, {"referenceID": 2, "context": "GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.", "startOffset": 62, "endOffset": 85}, {"referenceID": 2, "context": "GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation. Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.", "startOffset": 62, "endOffset": 235}, {"referenceID": 2, "context": "GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation. Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens. We also include two recursive neural networks (TREENN). The 1layer TREENN which is the original TREENN also used by Zaremba et al. (2014). We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.", "startOffset": 62, "endOffset": 486}, {"referenceID": 16, "context": "We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, .", "startOffset": 70, "endOffset": 90}, {"referenceID": 20, "context": "Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": "TREENN refers to Socher et al. (2012).", "startOffset": 17, "endOffset": 38}, {"referenceID": 18, "context": "Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.", "startOffset": 35, "endOffset": 62}, {"referenceID": 17, "context": "EQNET\u2019s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.", "startOffset": 53, "endOffset": 74}, {"referenceID": 14, "context": "Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications. Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.", "startOffset": 36, "endOffset": 147}, {"referenceID": 9, "context": "Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks. Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics. Piech et al. (2015) also learn distributed matrix representations of student code submissions.", "startOffset": 0, "endOffset": 270}, {"referenceID": 1, "context": "Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 5, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 4, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 11, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 15, "context": "More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.", "startOffset": 36, "endOffset": 58}, {"referenceID": 0, "context": "Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.", "startOffset": 0, "endOffset": 20}], "year": 2017, "abstractText": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.", "creator": "LaTeX with hyperref package"}}}