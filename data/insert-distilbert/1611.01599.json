{"id": "1611.01599", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "lipreading drawing is the task of decoding text from the movement of seeing a speaker's mouth. traditional approaches separated the problem into two stages : designing or learning visual features, and prediction. still more recent deep lipreading approaches are completely end - to - end trainable ( wand et al., 2016 ; chung & j amp ; zisserman, 2016a ). all existing works, however, perform theoretical only word classification, not sentence - level sequence prediction. previous studies have shown that human lipreading performance increases for longer words ( easton & amp ; basala, 1982 ), indicating the importance of features capturing temporal context in an ambiguous communication channel. motivated by this observation, we present lipnet, a model that maps a variable - length sequence of video window frames to text, making use of spatiotemporal convolutions, an lstm recurrent network, and the connectionist temporal classification loss, trained entirely end - to - end. to the best of our knowledge, lipnet search is the first lipreading model to operate at higher sentence - level, using a single end - to - end speaker - independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. on the grid corpus, lipnet achieves 93. 4 % accuracy, outperforming experienced human lipreaders and the previous 79. 6 % state - of - the - art accuracy.", "histories": [["v1", "Sat, 5 Nov 2016 04:05:18 GMT  (3950kb,D)", "http://arxiv.org/abs/1611.01599v1", null], ["v2", "Fri, 16 Dec 2016 16:09:34 GMT  (1926kb,D)", "http://arxiv.org/abs/1611.01599v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["yannis m assael", "brendan shillingford", "shimon whiteson", "nando de freitas"], "accepted": false, "id": "1611.01599"}, "pdf": {"name": "1611.01599.pdf", "metadata": {"source": "CRF", "title": "LIPNET: SENTENCE-LEVEL LIPREADING", "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson"], "emails": ["shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Lipreading plays a crucial role in human communication and speech understanding, as highlighted by the McGurk effect (McGurk & MacDonald, 1976), where one phoneme\u2019s audio dubbed on top of a video of someone speaking a different phoneme results in a third phoneme being perceived.\nLipreading is a notoriously difficult task for humans.1 Most lipreading actuations, besides the lips and sometimes tongue and teeth, are latent and difficult to disambiguate without context (Fisher, 1968; Woodward & Barber, 1960). For example, Fisher (1968) gives 5 categories of visual phonemes (called visemes), out of a list of 23 initial consonant phonemes, that are commonly confused by people when viewing a speaker\u2019s mouth. Many of these were asymmetrically confused, and observations were similar for final consonant phonemes.\nConsequently, human lipreading performance is poor. Hearing-impaired people achieve an accuracy of only 17\u00b112% even for a limited subset of 30 monosyllabic words and 21\u00b111% for 30 compound words (Easton & Basala, 1982).\nAn important goal, therefore, is to automate lipreading. Machine lipreaders have enormous practical potential, with applications in improved hearing aids, silent dictation in public spaces, covert conversations, speech recognition in noisy environments, biometric identification, and silent-movie processing.\nMachine lipreading is difficult because it requires extracting spatiotemporal features from the video (since both position and motion are important). Recent deep learning approaches attempt to extract those features end-to-end. All existing work, however, performs only word classification, not sentence-level sequence prediction.\n\u2020These authors contributed equally to this work. 1LipNet video: https://youtube.com/playlist?list=PLXkuFIFnXUAPIrXKgtIpctv2NuSo7xw3k\nar X\niv :1\n61 1.\n01 59\n9v 1\n[ cs\n.L G\n] 5\nN ov\n2 01\n6\nIn this paper, we present LipNet, which is to the best of our knowledge, the first sentence-level lipreading model. As with modern deep learning based automatic speech recognition (ASR), LipNet is trained end-to-end to make speaker-independent sentence-level predictions. Our model operates at the character-level, using spatiotemporal convolutional neural networks (STCNNs), LSTMs, and the connectionist temporal classification loss (CTC).\nOur empirical results on the GRID corpus (Cooke et al., 2006), one of the only public sentence-level datasets, show that LipNet attains a 93.4% sentence-level word accuracy. The best accuracy reported on an aligned, speaker-dependent word classification version of this task was 79.6% (Wand et al., 2016).\nWe also compare the performance of LipNet with that of hearing-impaired people who can lipread. On average, they achieve an accuracy of 52.3%, in contrast to LipNet\u2019s 1.78\u00d7 higher accuracy in the same sentences.\nFinally, by applying saliency visualisation techniques (Zeiler & Fergus, 2014; Simonyan et al., 2013), we interpret LipNet\u2019s learned behaviour, showing that the model attends to phonologically important regions in the video. Furthermore, by computing intra-viseme and inter-viseme confusion matrices at the phoneme level, we show that almost all of LipNet\u2019s few erroneous predictions occur within visemes, since context is sometimes insufficient for disambiguation."}, {"heading": "2 RELATED WORK", "text": "In this section, we outline various existing approaches to automated lipreading.\nAutomated lipreading: Most existing work on lipreading does not employ deep learning. Such work requires either heavy preprocessing of frames to extract image features, temporal preprocessing of frames to extract video features (e.g., optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009). Generalisation across speakers and extraction of motion features is considered an open problem, as noted in a recent review article (Zhou et al., 2014). LipNet addresses both of these issues.\nClassification with deep learning: In recent years, there have been several attempts to apply deep learning to lipreading. However, all of these approaches perform only word or phoneme classification, whereas LipNet performs full sentence sequence prediction. Approaches include learning multimodal audio-visual representations (Ngiam et al., 2011), learning visual features as part of a traditional speech-style processing pipeline (e.g. HMMs, GMM-HMMs, etc.) for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al., 2016). Many of these approaches mirror early progress in applying neural networks for acoustic processing in speech recognition (Hinton et al., 2012).\nChung & Zisserman (2016a) propose spatial and spatiotemporal convolutional neural networks, based on VGG, for word classification. The architectures are evaluated on a word-level dataset BBC TV (333 and 500 classes), but, as reported, their spatiotemporal models fall short of the spatial architectures by an average of around 14%. Additionally, models cannot handle variable sequence lengths and they do not attempt sentence-level sequence prediction.\nChung & Zisserman (2016b) train an audio-visual max-margin matching model for learning pretrained mouth features, which they use as inputs to an LSTM for 10-phrase classification on the OuluVS2 dataset, as well as a non-lipreading task.\nWand et al. (2016) introduce LSTM recurrent neural networks for lipreading but address neither sentence-level sequence prediction nor speaker independence. This work holds the previous stateof-the-art in the GRID corpus with a speaker-dependent accuracy of 79.6%.\nGarg et al. (2016) apply a VGG pre-trained on faces to classifying words and phrases from the MIRACL-VC1 dataset, which has only 10 words and 10 phrases. However, their best recurrent model is trained by freezing the VGGNet parameters and then training the RNN, rather than training them jointly. Their best model achieves only 56.0% word classification accuracy, and 44.5% phrase classification accuracy, despite both of these being 10-class classification tasks.\nSequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012). The connectionist temporal classification loss (CTC) of Graves et al. (2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al., 2015; Amodei et al., 2015). As mentioned earlier, much recent lipreading progress has mirrored early progress in ASR, but stopping short of sequence prediction.\nNo lipreading work (based on deep learning or not) has performed sentence-level sequence prediction. LipNet demonstrates the first sentence-level results by using CTC. Furthermore, it does not require alignments to do so.\nLipreading Datasets: Lipreading datasets (AVICar, AVLetters, AVLetters2, BBC TV, CUAVE, OuluVS1, OuluVS2) are plentiful (Zhou et al., 2014; Chung & Zisserman, 2016a), but most only contain single words or are too small. One exception is the GRID corpus (Cooke et al., 2006), which has audio and video recordings of 34 speakers who produced 1000 sentences each, for a total of 28 hours across 34000 sentences. Table 1 summarises state-of-the-art performance in each of the main lipreading datasets.\nWe use the GRID corpus to evaluate LipNet because it is sentence-level and has the most data. The sentences are drawn from the following simple grammar: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), where the number denotes how many word choices there are for each of the 6 word categories. The categories consist of, respectively, {bin, lay, place, set}, {blue, green, red, white}, {at, by, in, with}, {A, . . . , Z}\\{W}, {zero, . . . , nine}, and {again, now, please, soon}, yielding 64000 possible sentences. For example, two sentences in the data are \u201cset blue by A four please\u201d and \u201cplace red at C zero again\u201d."}, {"heading": "3 LIPNET", "text": "LipNet is a neural network architecture for lipreading that maps variable-length sequences of video frames to text sequences, and is trained end-to-end. In this section, we describe LipNet\u2019s building blocks and architecture."}, {"heading": "3.1 SPATIOTEMPORAL CONVOLUTIONS", "text": "Convolutional neural networks (CNNs), containing stacked convolutions operating spatially over an image, have been instrumental in advancing performance in computer visions tasks such as object recognition that receive an image as input (Krizhevsky et al., 2012). A basic 2D convolution layer\nfrom C channels to C \u2032 channels (without a bias and with unit stride) computes\n[conv(x,w)]c\u2032ij = C\u2211 c=1 kw\u2211 i\u2032=1 kh\u2211 j\u2032=1 wc\u2032ci\u2032j\u2032xc,i+i\u2032,j+j\u2032 ,\nfor input x and weights w \u2208 RC\u2032\u00d7C\u00d7kw\u00d7kh where we define xcij = 0 for i, j out of bounds. Spatiotemporal convolutional neural networks (STCNNs) can process video data by convolving across time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013). Hence similarly,\n[stconv(x,w)]c\u2032tij = C\u2211 c=1 kt\u2211 t\u2032=1 kw\u2211 i\u2032=1 kh\u2211 j\u2032=1 wc\u2032ct\u2032i\u2032j\u2032xc,t+t\u2032,i+i\u2032,j+j\u2032 ."}, {"heading": "3.2 LONG SHORT-TERM MEMORY", "text": "Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) is a type of recurrent neural network (RNN) that improves upon earlier RNNs by adding cells and gates for propagating information over more time-steps and learning to control this information flow. We use the standard LSTM formulation with forget gates:\n[\u0303it, f\u0303t, o\u0303t, g\u0303t] T = Wxzt +Whht\u22121 + b\nit = sigm(\u0303it) ft = sigm(f\u0303t)\not = sigm(o\u0303t) gt = tanh(g\u0303t)\nct = ft ct\u22121 + it gt ht = ot tanh(ct),\nwhere z := {z1, . . . , zT } is the input sequence to the LSTM, denotes element-wise multiplication, and sigm(r) = 1/(1 + exp(\u2212r)). We use the bidirectional LSTM (Bi-LSTM) introduced by Graves & Schmidhuber (2005): one LSTM maps {z1, . . . , zT } 7\u2192 { \u2212\u2192 h1, . . . , \u2212\u2192 hT }, and another {zT , . . . , z1} 7\u2192 { \u2190\u2212 h1, . . . , \u2190\u2212 hT }, then ht := [ \u2212\u2192 ht, \u2190\u2212 ht]. The Bi-LSTM ensures that ht depends on zt\u2032 for all t\u2032. To parameterise a distribution over sequences, at time-step t let p(ut|z) = softmax(mlp(ht;Wmlp)), where mlp is a feed-forward network with weights Wmlp. Then we can define the distribution over length-T sequences as p(u1, . . . , uT |z) = \u220f 1\u2264t\u2264T p(ut|z), where T is determined by z, the input to the LSTM. In LipNet, z is the output of the STCNN."}, {"heading": "3.3 CONNECTIONIST TEMPORAL CLASSIFICATION", "text": "The connectionist temporal classification (CTC) loss (Graves et al., 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015). Given a model that outputs a sequence of discrete distributions over the token classes (vocabulary) augmented with a special \u201cblank\u201d token, CTC computes the probability of a sequence by marginalising over all sequences that are defined as equivalent to this sequence. This simultaneously removes the need for alignments and addresses variable-length sequences. Let V denote the set of tokens that the model classifies at a single time-step of its output (vocabulary), and the blank-augmented vocabulary V\u0303 = V \u222a { } where denotes the CTC blank symbol. Define the function B : V\u0303 \u2217 \u2192 V \u2217 that, given a string over V\u0303 , deletes adjacent duplicate characters and removes blank tokens. For a label sequence y \u2208 V \u2217, CTC defines p(y|x) = \u2211 u\u2208B\u22121(y) s.t. |u|=T p(u1, . . . , uT |x), where T is the number of time-steps in the sequence model. For example, if T = 3, CTC defines the probability of a string \u201cam\u201d as p(aam) + p(amm) + p( am) + p(a m) + p(am ). This sum is computed efficiently by dynamic programming, allowing us to perform maximum likelihood."}, {"heading": "3.4 LIPNET ARCHITECTURE", "text": "Figure 1 illustrates the LipNet architecture, which starts with 3\u00d7(spatiotemporal convolutions, channel-wise dropout, spatial max-pooling), followed by up-sampling in the time dimension.\nSince it is well known that people utter about 7 phonemes per second, and since LipNet works at the character-level, we concluded that outputting 25 tokens per second (the average frame rate of the video) is too constrained for CTC. Temporal up-sampling allows for more spacing between character outputs. This problem is exacerbated when many words have identical consecutive characters since a CTC blank is required between them.\nSubsequently, the temporal up-sampling is followed by a Bi-LSTM. The Bi-LSTM is crucial for efficient further aggregation of the STCNN output. Finally, a feed-forward network is applied at each time-step, followed by a softmax over the vocabulary augmented with the CTC blank, and then the CTC loss. All layers use rectified linear unit (ReLU) activation functions. More details including hyperparameters can be found in Table 3 of Appendix A."}, {"heading": "4 LIPREADING EVALUATION", "text": "In this section, we evaluate LipNet on the GRID corpus."}, {"heading": "4.1 DATA AUGMENTATION", "text": "Preprocessing: The GRID corpus consists of 34 subjects, each narrating 1000 sentences. The videos for speaker 21 are missing, and a few others are empty or corrupt, leaving 32839 usable videos. We use the data of two male speakers (1 and 2) and two female speakers (20 and 22) for evaluation (3986 videos), and the rest for training (28853 videos). All videos are 3 seconds long with a frame rate of 25fps. The videos were processed with the DLib face detector and the iBug face shape predictor with 68 landmarks. Using these landmarks, we apply an affine transformation to extract a mouth-centred crop of size 100\u00d750 pixels per frame. We standardise the RGB channels over the whole training set to have zero-mean and unit variance.\nAugmentation: We augment the dataset with simple transformations to reduce overfitting, yielding 15.6 times more training data. First, we train on both the regular and the horizontally mirrored image sequence. Second, since the dataset provides word start and end timings for each sentence video, we augment the sentence-level training data with video clips of individual words as additional training instances."}, {"heading": "4.2 BASELINES", "text": "To evaluate LipNet, we compare its performance to that of three hearing-impaired people who can lipread, as well as two ablation models inspired by recent state-of-the-art work (Chung & Zisserman, 2016a; Wand et al., 2016).\nHearing-Impaired People: This baseline was performed by three members of the Oxford Students\u2019 Disability Community. After being introduced to the grammar of the GRID corpus, they observed 10 minutes of annotated videos from the training dataset, then annotated 300 random videos from the evaluation dataset. When uncertain, they were asked to pick the most probable answer.\nBaseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016). See Appendix A for more implementation details.\nBaseline-2D: Based on the LipNet architecture, we replace the STCNN with spatial-only convolutions similar to those of Chung & Zisserman (2016a). Notably, contrary to the results we observe with LipNet, Chung & Zisserman (2016a) report 14% and 31% poorer performance of their STCNNs compared to the 2D architectures in their two datasets."}, {"heading": "4.3 PERFORMANCE EVALUATION", "text": "To measure the performance of LipNet and the baselines, we compute the word error rate (WER) and the character error rate (CER), standard metrics for the performance of ASR models. We produce approximate maximum-probability predictions from LipNet by performing CTC beam search. WER (or CER) is defined as the minimum number of word (or character) insertions, substitutions, and deletions required to transform the prediction into the ground truth, divided by the number of words (or characters) in the ground truth. Note that WER is usually equal to classification error when the predicted sentence has the same number of words as the ground truth, particularly in our case since almost all errors are substitution errors.\nTable 2 summarises the performance of LipNet compared to the baselines. According to the literature, the accuracy of human lipreaders is around 20% (Easton & Basala, 1982; Hilder et al., 2009). As expected, the fixed sentence structure and the limited subset of words for each position in the GRID corpus facilitate the use of context, increasing performance. The three hearing-impaired people achieve individual scores of 57.3%, 50.4%, and 35.5% WER, yielding an average of 47.7% WER.\nBaseline-LSTM performs slightly better than the hearing-impaired people, with 31.4% WER. Interestingly, although Baseline-LSTM replicates the same architecture as Wand et al. (2016) but trains using CTC, our speaker-independent sentence-level baseline performs 1.16\u00d7 better2 than their speaker-dependent word-level performance. We postulate that this is primarily a result of better use of context, but potentially also a result of better use of data by generalising across speakers.\nThe highest performance is achieved by the two architectures enhanced with convolutional stacks. Baseline-2D and LipNet achieve approximately 4.3\u00d7 and 7.2\u00d7 lower WER, respectively, than the hearing-impaired people. The WER for Baseline-2D is 10.9%, whereas for LipNet it is 1.6\u00d7 lower at 6.6%, demonstrating the importance of combining STCNNs with LSTMs. This performance difference confirms the intuition that extracting spatiotemporal features using a STCNN is better than aggregating spatial-only features. This observation contrasts with the empirical observations of Chung & Zisserman (2016a). Furthermore, LipNet\u2019s use of STCNN, LSTM, and CTC cleanly allow processing both variable-length input and variable-length output sequences, whereas the architectures of Chung & Zisserman (2016a) and Chung & Zisserman (2016b) only handle the former.\n2Improvement factor computed as (reference WER) / (improved WER)."}, {"heading": "4.4 LEARNED REPRESENTATIONS", "text": "In this section, we analyse the learned representations of LipNet from a phonological perspective. First, we create saliency visualisations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to attend. In particular, we feed an input into the model and greedily decode an output sequence, yielding a CTC alignment u\u0302 \u2208 V\u0303 \u2217 (following the notation of Sections 3.2 and 3.3). Then, we compute the gradient of \u2211 t p(u\u0302t|x) with respect to the input video frame sequence, but unlike Simonyan et al. (2013), we use guided backpropagation (Springenberg et al., 2014). Second, we train LipNet to predict ARPAbet phonemes, instead of characters, to analyse visual phoneme similarities using intra-viseme and inter-viseme confusion matrices."}, {"heading": "4.4.1 SALIENCY MAPS", "text": "We apply saliency visualisation techniques to interpret LipNet\u2019s learned behaviour, showing that the model attends to phonologically important regions in the video. In particular, in Figure 2 we analyse two saliency visualisations for the words please and lay for speaker 25, based on Ashby (2013).\nThe production of the word please requires a great deal of articulatory movement at the beginning: the lips are pressed firmly together for the bilabial plosive /p/ (frame 1). At the same time, the blade of the tongue comes in contact with the alveolar ridge in anticipation of the following lateral /l/. The lips then part, allowing the compressed air to escape between the lips (frame 2). The jaw and lips then open further, seen in the distance between the midpoints of the upper and lower lips, and the lips spread (increasing the distance between the corners of the mouth), for the close vowel /iy/ (frame 3\u20134). Since this is a relatively steady-state vowel, lip position remains unchanged for the rest of its duration (frames 4\u20138), where the attention level drops considerably. The jaw and the lips then close slightly, as the blade of the tongue needs to be brought close to the alveolar ridge, for /z/ (frames 9\u201310), where attention resumes.\nLay is interesting since the bulk of frontally visible articulatory movement involves the blade of the tongue coming into contact with the alveolar ridge for /l/ (frames 2\u20136), and then going down for the vowel /ey/ (frames 7\u20139). That is exactly where most of LipNet\u2019s attention is focused, as there is little change in lip position."}, {"heading": "4.4.2 VISEMES", "text": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker. This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme-to-viseme mapping of Neti et al. (2000), clustering the phonemes into the following categories: Lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolar-fricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E), Dental (F), Labio-dental (G), and Velar (H). The full mapping can be found in Table 4 in Appendix A. The GRID corpus contain 31 out of the 39 phonemes in ARPAbet. We compute confusion matrices between phonemes and then group phonemes into viseme clusters, following Neti et al. (2000). Figure 3 shows the confusion matrices of the 3 most confused viseme categories, as well as the confusions between the viseme categories. The full phoneme confusion matrix is in Figure 4 in Appendix B.\nGiven that the speakers are British, the confusion between /aa/ and /ay/ (Figure 3a) is most probably due to the fact that the first element, and the greater part, of the diphthong /ay/ is articulatorily identical with /aa/: an open back unrounded vowel (Ferragne & Pellegrino, 2010). The confusion of /ih/ (a rather close vowel) and /ae/ (a very open vowel) is at first glance surprising, but in fact in the sample /ae/ occurs only in the word at, which is a function word normally pronounced with a reduced, weak vowel /ah/. /ah/ and /ih/ are the most frequent unstressed vowels and there is a good deal of variation within and between them, e.g. private and watches (Cruttenden, 2014).\nThe confusion within the categories of bilabial stops /p b m/ and alveolar stops /t d n/ (Figures 3b-c) is unsurprising: complete closure at the same place of articulation makes them look practically identical. The differences of velum action and vocal fold vibration are unobservable from the front.\nFinally, the quality of the viseme categorisation of Neti et al. (2000) is confirmed by the fact that the matrix in Figure 3d is diagonal, with only minor confusion between alveolar (C) and palato-alvealoar (D) visemes. Articulatorily alveolar /s z/ and palato-alvealoar /sh zh/ fricatives are distinguished by only a small difference in tongue position: against the palate just behind the alveolar ridge, which is not easily observed from the front. The same can be said about dental /th/ and alveolar /t/."}, {"heading": "5 CONCLUSIONS", "text": "We proposed LipNet, the first model to apply deep learning for end-to-end learning of a model that maps sequences of image frames of a speaker\u2019s mouth to entire sentences. The end-to-end model eliminates the need to segment videos into words before predicting a sentence. LipNet requires neither hand-engineered spatiotemporal visual features nor a separately-trained sequence model.\nOur empirical evaluation illustrates the importance of spatiotemporal feature extraction and efficient temporal aggregation, confirming the intuition of Easton & Basala (1982). Furthermore, LipNet greatly outperforms a human lipreading baseline, exhibiting 7.2\u00d7 better performance, and 6.6% WER, 3\u00d7 lower than the word-level state-of-the-art (Wand et al., 2016) in the GRID dataset. While LipNet is already an empirical success, the deep speech recognition literature (Amodei et al., 2015) suggests that performance will only improve with more data. In future work, we hope to demonstrate this by applying LipNet to larger datasets, such as a sentence-level variant of that collected by Chung & Zisserman (2016a).\nSome applications, such as silent dictation, demand the use of video only. However, to extend the range of potential applications of LipNet, we aim to apply this approach to a jointly trained audiovisual speech recognition model, where visual input assists with robustness in noisy environments."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by an Oxford-Google DeepMind Graduate Scholarship, the EPSRC, and CIFAR. We would also like to thank: NVIDIA for their generous donation of DGX-1 and GTX Titan X GPUs, used in our experiments; A\u0301ine Jackson, Brittany Klug and Samantha Pugh for helping us measure the experienced lipreader baseline; Mitko Sabev for his phonetics guidance; Odysseas Votsis for his video production help; and Alex Graves and Oiwi Parker Jones for helpful comments."}, {"heading": "A ARCHITECTURE DETAILS", "text": "In this appendix, we provide additional details about the implementation and architecture.\nA.1 IMPLEMENTATION\nLipNet is implemented using Torch, the warp-ctc CTC library (Amodei et al., 2015), and StanfordCTC\u2019s decoder implementation. The network parameters were initialised using Xavier initialisation (Glorot & Bengio, 2010), and the LSTM forget gate biases were initialised to 3 unless stated otherwise. The models were trained with channel-wise dropout (dropout rate p = 0.2) after each pooling layer and mini-batches of size 50. We used the optimiser Adam (Kingma & Ba, 2014) with a learning rate of 10\u22124, and the default hyperparameters: a first-moment momentum coefficient of 0.9, a second-moment momentum coefficient of 0.999, and the numerical stability parameter = 10\u22128.\nThe CER and WER scores were computed using CTC beam search with the following parameters for Stanford-CTC\u2019s decoder: beam width 100, \u03b1 = 1, and \u03b2 = 1.5. On top of that, we use a character 5-gram binarised language model.\nA.2 LIPNET ARCHITECTURE\nThe videos were processed with DLib face detector (King, 2009) and the iBug face shape predictor with 68 landmarks (Sagonas et al., 2013). The RGB input frames were normalised using the following means and standard deviations: [\u00b5R = 0.7136, \u03c3R = 0.1138, \u00b5G = 0.4906, \u03c3G = 0.1078, \u00b5B = 0.3283, \u03c3B = 0.0917].\nTable 3 summarises the LipNet architecture hyperparameters, where T denotes time, C denotes channels, F denotes feature dimension, H and W denote height and width and V denotes the number of words in the vocabulary including the CTC blank symbol.\nNote that spatiotemporal convolution sizes depend on the number of channels, and the kernel\u2019s three dimensions. Spatiotemporal kernel sizes are specified in the same order as the input size dimensions. The input dimension orderings are given in parentheses in the input size column.\nLayers after the Bi-LSTM are applied per-timestep.\nA.3 BASELINE-LSTM ARCHITECTURE\nBaseline-LSTM replicates the setup of Wand et al. (2016), and is trained the same way as LipNet. The model uses an LSTM with 128 neurons. The input frames were converted to grayscale and were down-sampled to 50 \u00d7 25px, dropout p = 0, and the parameters were initialised uniformly with values between [\u22120.05, 0.05]. Finally, LipNet\u2019s temporal up-sampling was disabled as it hindered performance."}, {"heading": "B PHONEMES AND VISEMES", "text": "Table 4 shows the phoneme to viseme clustering of Neti et al. (2000) and Figure 4 shows LipNet\u2019s full phoneme confusion matrix."}], "references": [{"title": "Improved speaker independent lip reading using speaker adaptive training and deep neural networks", "author": ["I. Almajai", "S. Cox", "R. Harvey", "Y. Lan"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Almajai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Almajai et al\\.", "year": 2016}, {"title": "Deep Speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Understanding phonetics", "author": ["P. Ashby"], "venue": null, "citeRegEx": "Ashby.,? \\Q2013\\E", "shortCiteRegEx": "Ashby.", "year": 2013}, {"title": "Lip reading in the wild", "author": ["J.S. Chung", "A. Zisserman"], "venue": "In Asian Conference on Computer Vision,", "citeRegEx": "Chung and Zisserman.,? \\Q2016\\E", "shortCiteRegEx": "Chung and Zisserman.", "year": 2016}, {"title": "Out of time: automated lip sync in the wild", "author": ["J.S. Chung", "A. Zisserman"], "venue": "In Workshop on Multi-view Lip-reading,", "citeRegEx": "Chung and Zisserman.,? \\Q2016\\E", "shortCiteRegEx": "Chung and Zisserman.", "year": 2016}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Cooke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cooke et al\\.", "year": 2006}, {"title": "Gimson\u2019s pronunciation of English", "author": ["A. Cruttenden"], "venue": null, "citeRegEx": "Cruttenden.,? \\Q2014\\E", "shortCiteRegEx": "Cruttenden.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "The story of lip-reading, its genesis and development", "author": ["F. DeLand"], "venue": null, "citeRegEx": "DeLand.,? \\Q1931\\E", "shortCiteRegEx": "DeLand.", "year": 1931}, {"title": "Perceptual dominance during lipreading", "author": ["R.D. Easton", "M. Basala"], "venue": "Perception & Psychophysics,", "citeRegEx": "Easton and Basala.,? \\Q1982\\E", "shortCiteRegEx": "Easton and Basala.", "year": 1982}, {"title": "Formant frequencies of vowels in 13 accents of the british isles", "author": ["E. Ferragne", "F. Pellegrino"], "venue": "Journal of the International Phonetic Association,", "citeRegEx": "Ferragne and Pellegrino.,? \\Q2010\\E", "shortCiteRegEx": "Ferragne and Pellegrino.", "year": 2010}, {"title": "Confusions among visually perceived consonants", "author": ["C.G. Fisher"], "venue": "Journal of Speech, Language, and Hearing Research,", "citeRegEx": "Fisher.,? \\Q1968\\E", "shortCiteRegEx": "Fisher.", "year": 1968}, {"title": "Classification and feature extraction by simplexization", "author": ["Y. Fu", "S. Yan", "T.S. Huang"], "venue": "IEEE Transactions on Information Forensics and Security,", "citeRegEx": "Fu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2008}, {"title": "Lip reading using CNN and LSTM", "author": ["A. Garg", "J. Noyola", "S. Bagadia"], "venue": "Technical report,", "citeRegEx": "Garg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garg et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Information theoretic feature extraction for audio-visual speech recognition", "author": ["M. Gurban", "J.-P. Thiran"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Gurban and Thiran.,? \\Q2009\\E", "shortCiteRegEx": "Gurban and Thiran.", "year": 2009}, {"title": "Comparison of human and machine-based lip-reading", "author": ["S. Hilder", "R. Harvey", "B.-J. Theobald"], "venue": "In AVSP, pp", "citeRegEx": "Hilder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hilder et al\\.", "year": 2009}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Dlib-ml: A machine learning toolkit", "author": ["D.E. King"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "King.,? \\Q2009\\E", "shortCiteRegEx": "King.", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deep learning of mouth shapes for sign language", "author": ["O. Koller", "H. Ney", "R. Bowden"], "venue": "In ICCV Workshop on Assistive Computer Vision and Robotics,", "citeRegEx": "Koller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Patch-based representation of visual speech", "author": ["P. Lucey", "S. Sridharan"], "venue": "In Proceedings of the HCSNet workshop on Use of vision in human-computer interaction,", "citeRegEx": "Lucey and Sridharan.,? \\Q2006\\E", "shortCiteRegEx": "Lucey and Sridharan.", "year": 2006}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "In NAACL,", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Extraction of visual features for lipreading", "author": ["I. Matthews", "T.F. Cootes", "J.A. Bangham", "S. Cox", "R. Harvey"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Matthews et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Matthews et al\\.", "year": 2002}, {"title": "Hearing lips and seeing", "author": ["H. McGurk", "J. MacDonald"], "venue": "voices. Nature,", "citeRegEx": "McGurk and MacDonald.,? \\Q1976\\E", "shortCiteRegEx": "McGurk and MacDonald.", "year": 1976}, {"title": "Audio visual speech recognition", "author": ["C. Neti", "G. Potamianos", "J. Luettin", "I. Matthews", "H. Glotin", "D. Vergyri", "J. Sison", "A. Mashari"], "venue": "Technical report,", "citeRegEx": "Neti et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Neti et al\\.", "year": 2000}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Lipreading using convolutional neural network", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "In INTERSPEECH,", "citeRegEx": "Noda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Noda et al\\.", "year": 2014}, {"title": "Multimodal fusion and learning with uncertain features applied to audiovisual speech recognition", "author": ["G. Papandreou", "A. Katsamanis", "V. Pitsikalis", "P. Maragos"], "venue": "In Workshop on Multimedia Signal Processing,", "citeRegEx": "Papandreou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2007}, {"title": "Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition", "author": ["G. Papandreou", "A. Katsamanis", "V. Pitsikalis", "P. Maragos"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Papandreou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2009}, {"title": "Adaptive multimodal fusion by uncertainty compensation", "author": ["V. Pitsikalis", "A. Katsamanis", "G. Papandreou", "P. Maragos"], "venue": "In INTERSPEECH,", "citeRegEx": "Pitsikalis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pitsikalis et al\\.", "year": 2006}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["C. Sagonas", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "Sagonas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sagonas et al\\.", "year": 2013}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR Workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Audio-visual speech recognition using bimodal-trained bottleneck features for a person with severe hearing", "author": ["Y. Takashima", "R. Aihara", "T. Takiguchi", "Y. Ariki", "N. Mitani", "K. Omori", "K. Nakazono"], "venue": "loss. INTERSPEECH,", "citeRegEx": "Takashima et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takashima et al\\.", "year": 2016}, {"title": "Lipreading with long short-term memory", "author": ["M. Wand", "J. Koutnik", "J. Schmidhuber"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Wand et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wand et al\\.", "year": 2016}, {"title": "Phoneme perception in lipreading", "author": ["M.F. Woodward", "C.G. Barber"], "venue": "Journal of Speech, Language, and Hearing Research,", "citeRegEx": "Woodward and Barber.,? \\Q1960\\E", "shortCiteRegEx": "Woodward and Barber.", "year": 1960}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In European Conference on Computer Vision, pp", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Lipreading with local spatiotemporal descriptors", "author": ["G. Zhao", "M. Barnard", "M. Pietikainen"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "A review of recent advances in visual speech decoding", "author": ["Z. Zhou", "G. Zhao", "X. Hong", "M. Pietik\u00e4inen"], "venue": "Image and Vision Computing,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "B PHONEMES AND VISEMES Table 4 shows the phoneme to viseme clustering of Neti et al. (2000) and Figure 4 shows LipNet\u2019s full phoneme confusion matrix. Table 4: Phoneme to viseme clustering", "author": ["Neti"], "venue": null, "citeRegEx": "Neti,? \\Q2000\\E", "shortCiteRegEx": "Neti", "year": 2000}], "referenceMentions": [{"referenceID": 42, "context": "More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a).", "startOffset": 64, "endOffset": 109}, {"referenceID": 11, "context": "1 Most lipreading actuations, besides the lips and sometimes tongue and teeth, are latent and difficult to disambiguate without context (Fisher, 1968; Woodward & Barber, 1960).", "startOffset": 136, "endOffset": 175}, {"referenceID": 11, "context": "1 Most lipreading actuations, besides the lips and sometimes tongue and teeth, are latent and difficult to disambiguate without context (Fisher, 1968; Woodward & Barber, 1960). For example, Fisher (1968) gives 5 categories of visual phonemes (called visemes), out of a list of 23 initial consonant phonemes, that are commonly confused by people when viewing a speaker\u2019s mouth.", "startOffset": 137, "endOffset": 204}, {"referenceID": 5, "context": "Our empirical results on the GRID corpus (Cooke et al., 2006), one of the only public sentence-level datasets, show that LipNet attains a 93.", "startOffset": 41, "endOffset": 61}, {"referenceID": 42, "context": "6% (Wand et al., 2016).", "startOffset": 3, "endOffset": 22}, {"referenceID": 39, "context": "Finally, by applying saliency visualisation techniques (Zeiler & Fergus, 2014; Simonyan et al., 2013), we interpret LipNet\u2019s learned behaviour, showing that the model attends to phonologically important regions in the video.", "startOffset": 55, "endOffset": 101}, {"referenceID": 30, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 45, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 35, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 37, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 36, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 46, "context": "Generalisation across speakers and extraction of motion features is considered an open problem, as noted in a recent review article (Zhou et al., 2014).", "startOffset": 132, "endOffset": 151}, {"referenceID": 33, "context": "Approaches include learning multimodal audio-visual representations (Ngiam et al., 2011), learning visual features as part of a traditional speech-style processing pipeline (e.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 41, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 34, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 26, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 41, "context": ", 2015), or combinations thereof (Takashima et al., 2016).", "startOffset": 33, "endOffset": 57}, {"referenceID": 20, "context": "Many of these approaches mirror early progress in applying neural networks for acoustic processing in speech recognition (Hinton et al., 2012).", "startOffset": 121, "endOffset": 142}, {"referenceID": 17, "context": "Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012).", "startOffset": 221, "endOffset": 282}, {"referenceID": 7, "context": "Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012).", "startOffset": 221, "endOffset": 282}, {"referenceID": 20, "context": "Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012).", "startOffset": 221, "endOffset": 282}, {"referenceID": 29, "context": "(2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al., 2015; Amodei et al., 2015).", "startOffset": 107, "endOffset": 170}, {"referenceID": 1, "context": "(2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al., 2015; Amodei et al., 2015).", "startOffset": 107, "endOffset": 170}, {"referenceID": 46, "context": "Lipreading Datasets: Lipreading datasets (AVICar, AVLetters, AVLetters2, BBC TV, CUAVE, OuluVS1, OuluVS2) are plentiful (Zhou et al., 2014; Chung & Zisserman, 2016a), but most only contain single words or are too small.", "startOffset": 120, "endOffset": 165}, {"referenceID": 5, "context": "One exception is the GRID corpus (Cooke et al., 2006), which has audio and video recordings of 34 speakers who produced 1000 sentences each, for a total of 28 hours across 34000 sentences.", "startOffset": 33, "endOffset": 53}, {"referenceID": 5, "context": ", 2006; Dahl et al., 2012; Hinton et al., 2012). The connectionist temporal classification loss (CTC) of Graves et al. (2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al.", "startOffset": 8, "endOffset": 126}, {"referenceID": 42, "context": "Although the GRID corpus contains entire sentences, Wand et al. (2016) consider only the simpler case of predicting isolated words.", "startOffset": 52, "endOffset": 71}, {"referenceID": 27, "context": "Convolutional neural networks (CNNs), containing stacked convolutions operating spatially over an image, have been instrumental in advancing performance in computer visions tasks such as object recognition that receive an image as input (Krizhevsky et al., 2012).", "startOffset": 237, "endOffset": 262}, {"referenceID": 23, "context": "Spatiotemporal convolutional neural networks (STCNNs) can process video data by convolving across time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013).", "startOffset": 138, "endOffset": 178}, {"referenceID": 22, "context": "Spatiotemporal convolutional neural networks (STCNNs) can process video data by convolving across time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013).", "startOffset": 138, "endOffset": 178}, {"referenceID": 17, "context": "The connectionist temporal classification (CTC) loss (Graves et al., 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": ", 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015).", "startOffset": 133, "endOffset": 196}, {"referenceID": 29, "context": ", 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015).", "startOffset": 133, "endOffset": 196}, {"referenceID": 42, "context": "To evaluate LipNet, we compare its performance to that of three hearing-impaired people who can lipread, as well as two ablation models inspired by recent state-of-the-art work (Chung & Zisserman, 2016a; Wand et al., 2016).", "startOffset": 177, "endOffset": 222}, {"referenceID": 42, "context": "Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016).", "startOffset": 147, "endOffset": 166}, {"referenceID": 42, "context": "Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016). See Appendix A for more implementation details. Baseline-2D: Based on the LipNet architecture, we replace the STCNN with spatial-only convolutions similar to those of Chung & Zisserman (2016a). Notably, contrary to the results we observe with LipNet, Chung & Zisserman (2016a) report 14% and 31% poorer performance of their STCNNs compared to the 2D architectures in their two datasets.", "startOffset": 148, "endOffset": 361}, {"referenceID": 42, "context": "Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016). See Appendix A for more implementation details. Baseline-2D: Based on the LipNet architecture, we replace the STCNN with spatial-only convolutions similar to those of Chung & Zisserman (2016a). Notably, contrary to the results we observe with LipNet, Chung & Zisserman (2016a) report 14% and 31% poorer performance of their STCNNs compared to the 2D architectures in their two datasets.", "startOffset": 148, "endOffset": 445}, {"referenceID": 19, "context": "According to the literature, the accuracy of human lipreaders is around 20% (Easton & Basala, 1982; Hilder et al., 2009).", "startOffset": 76, "endOffset": 120}, {"referenceID": 42, "context": "Interestingly, although Baseline-LSTM replicates the same architecture as Wand et al. (2016) but trains using CTC, our speaker-independent sentence-level baseline performs 1.", "startOffset": 74, "endOffset": 93}, {"referenceID": 39, "context": "First, we create saliency visualisations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to attend.", "startOffset": 41, "endOffset": 87}, {"referenceID": 40, "context": "(2013), we use guided backpropagation (Springenberg et al., 2014).", "startOffset": 38, "endOffset": 65}, {"referenceID": 39, "context": "First, we create saliency visualisations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to attend. In particular, we feed an input into the model and greedily decode an output sequence, yielding a CTC alignment \u00fb \u2208 \u1e7c \u2217 (following the notation of Sections 3.2 and 3.3). Then, we compute the gradient of \u2211 t p(\u00fbt|x) with respect to the input video frame sequence, but unlike Simonyan et al. (2013), we use guided backpropagation (Springenberg et al.", "startOffset": 42, "endOffset": 435}, {"referenceID": 2, "context": "In particular, in Figure 2 we analyse two saliency visualisations for the words please and lay for speaker 25, based on Ashby (2013).", "startOffset": 120, "endOffset": 133}, {"referenceID": 11, "context": "This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968).", "startOffset": 98, "endOffset": 137}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker.", "startOffset": 13, "endOffset": 27}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker.", "startOffset": 13, "endOffset": 45}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker. This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme-to-viseme mapping of Neti et al. (2000), clustering the phonemes into the following categories: Lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolar-fricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E), Dental (F), Labio-dental (G), and Velar (H).", "startOffset": 13, "endOffset": 372}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker. This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme-to-viseme mapping of Neti et al. (2000), clustering the phonemes into the following categories: Lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolar-fricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E), Dental (F), Labio-dental (G), and Velar (H). The full mapping can be found in Table 4 in Appendix A. The GRID corpus contain 31 out of the 39 phonemes in ARPAbet. We compute confusion matrices between phonemes and then group phonemes into viseme clusters, following Neti et al. (2000). Figure 3 shows the confusion matrices of the 3 most confused viseme categories, as well as the confusions between the viseme categories.", "startOffset": 13, "endOffset": 843}, {"referenceID": 6, "context": "private and watches (Cruttenden, 2014).", "startOffset": 20, "endOffset": 38}, {"referenceID": 32, "context": "Finally, the quality of the viseme categorisation of Neti et al. (2000) is confirmed by the fact that the matrix in Figure 3d is diagonal, with only minor confusion between alveolar (C) and palato-alvealoar (D) visemes.", "startOffset": 53, "endOffset": 72}, {"referenceID": 42, "context": "6% WER, 3\u00d7 lower than the word-level state-of-the-art (Wand et al., 2016) in the GRID dataset.", "startOffset": 54, "endOffset": 73}, {"referenceID": 1, "context": "While LipNet is already an empirical success, the deep speech recognition literature (Amodei et al., 2015) suggests that performance will only improve with more data.", "startOffset": 85, "endOffset": 106}, {"referenceID": 1, "context": "While LipNet is already an empirical success, the deep speech recognition literature (Amodei et al., 2015) suggests that performance will only improve with more data. In future work, we hope to demonstrate this by applying LipNet to larger datasets, such as a sentence-level variant of that collected by Chung & Zisserman (2016a).", "startOffset": 86, "endOffset": 330}], "year": 2016, "abstractText": "Lipreading is the task of decoding text from the movement of a speaker\u2019s mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy.", "creator": "LaTeX with hyperref package"}}}