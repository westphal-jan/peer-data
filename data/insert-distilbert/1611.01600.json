{"id": "1611.01600", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Loss-aware Binarization of Deep Networks", "abstract": "deep neural network models, though very powerful and highly equally successful, are computationally extensively expensive in terms of space and time. recently, there have been a number of attempts on binarizing the network weights and activations. this largely greatly reduces the network size, grows and replaces the redundant underlying component multiplications to additions or even xnor bit renaming operations. however, existing binarization schemes fundamentally are based initially on simple matrix approximation and still ignore ignore the effect of binarization on the loss. in this paper, we propose a proximal newton fourier algorithm with fast diagonal hessian approximation that directly minimizes the loss w. r. t. the fastest binarized weights. the underlying proximal step system has an remarkably efficient closed - third form solution, and the second - order information can be efficiently obtained from the weighted second moments already computed by the adam optimizer. experiments on both feedforward and recurrent networks show that the proposed loss - aware binarization algorithm outperforms existing binarization schemes, and one is also more widely robust for wide and deep networks.", "histories": [["v1", "Sat, 5 Nov 2016 04:23:42 GMT  (395kb,D)", "https://arxiv.org/abs/1611.01600v1", null], ["v2", "Fri, 3 Mar 2017 02:49:19 GMT  (837kb,D)", "http://arxiv.org/abs/1611.01600v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["lu hou", "quanming yao", "james t kwok"], "accepted": true, "id": "1611.01600"}, "pdf": {"name": "1611.01600.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP NETWORKS", "Lu Hou", "Quanming Yao", "James T. Kwok"], "emails": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recently, deep neural networks have achieved state-of-the-art performance in various tasks such as speech recognition, visual object recognition, and image classification (LeCun et al., 2015). Though powerful, the large number of network weights leads to space and time inefficiencies in both training and storage. For instance, the popular AlexNet, VGG-16 and Resnet-18 all require hundred of megabytes to store, and billions of high-precision operations on classification. This limits its use in embedded systems, smart phones and other portable devices that are now everywhere.\nTo alleviate this problem, a number of approaches have been recently proposed. One attempt first trains a neural network and then compresses it (Han et al., 2016; Kim et al., 2016). Instead of this two-step approach, it is more desirable to train and compress the network simultaneously. Example approaches include tensorizing (Novikov et al., 2015), parameter quantization (Gong et al., 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass.\nCourbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks. Besides binarizing the weights, Hubara et al. (2016) further binarized the activations. Rastegari et al. (2016) also learned to scale the binarized weights, and obtained better results. Besides, they proposed the XNOR-network with both weights and activations binarized as in (Hubara et al., 2016). Instead of binarization, ternary-connect quantizes each weight to {\u22121, 0, 1} (Lin et al., 2016). Similarly, the ternary weight network (Li & Liu, 2016) and DoReFa-net (Zhou et al., 2016) quantize weights to three levels or more. However, though using more bits allows more accurate weight approximations, specialized hardwares are needed for the underlying non-binary operations.\nBesides the huge amount of computation and storage involved, deep networks are difficult to train because of the highly nonconvex objective and inhomogeneous curvature. To alleviate this problem, Hessian-free methods (Martens & Sutskever, 2012) use the second-order information by conjugate gradient. A related method is natural gradient descent (Pascanu & Bengio, 2014), which utilizes ge-\nar X\niv :1\n61 1.\n01 60\n0v 2\n[ cs\n.N E\n] 3\nM ar\n2 01\n7\nometry of the underlying parameter manifold. Another approach uses element-wise adaptive learning rate, as in Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015). This can also be considered as preconditioning that rescales the gradient so that all dimensions have similar curvatures.\nIn this paper, instead of directly approximating the weights, we propose to consider the effect of binarization on the loss during binarization. We formulate this as an optimization problem using the proximal Newton algorithm (Lee et al., 2014) with a diagonal Hessian. The crux of proximal algorithms is the proximal step. We show that this step has a closed-form solution, whose form is similar to the use of element-wise adaptive learning rate. The proposed method also reduces to BinaryConnect (Courbariaux et al., 2015) and the Binary-Weight-Network (Hubara et al., 2016) when curvature information is dropped. Experiments on both feedforward and recurrent neural network models show that it outperforms existing binarization algorithms. In particular, BinaryConnect fails on deep recurrent networks because of the exploding gradient problem, while the proposed method still demonstrates robust performance. Notations: For a vector x, \u221a x denotes the element-wise square root, |x| denotes the element-wise\nabsolute value, \u2016x\u2016p = ( \u2211 i |xi|p) 1 p is the p-norm of x, x 0 denotes that all entries of x are positive, sign(x) is the vector with [sign(x)]i = 1 if xi \u2265 0 and \u22121 otherwise, and Diag(x) returns a diagonal matrix with x on the diagonal. For two vectors x and y, x y denotes the elementwise multiplication and x y denotes the element-wise division. For a matrix X, vec(X) returns the vector obtained by stacking the columns of X, and diag(X) returns a diagonal matrix whose diagonal elements are extracted from diagonal of X."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 WEIGHT BINARIZATION IN DEEP NETWORKS", "text": "In a feedforward neural network with L layers, let the weight matrix (or tensor in the case of a convolutional layer) at layer l be Wl. We combine the (full-precision) weights from all layers as w = [w>1 ,w > 2 , . . . ,w > L ] >, where wl = vec(Wl). Analogously, the binarized weights are denoted as w\u0302 = [w\u0302>1 , w\u0302 > 2 , . . . , w\u0302 > L ] >. As it is essential to use full-precision weights during updates (Courbariaux et al., 2015), typically binarized weights are only used during the forward and backward propagations, but not on parameter update. At the tth iteration, the (full-precision) weight wtl is updated by using the backpropagated gradient\u2207l`(w\u0302t\u22121) (where ` is the loss and\u2207l`(w\u0302t\u22121) is the partial derivative of ` w.r.t. the weights of the lth layer). In the next forward propagation, it is then binarized as w\u0302tl = Binarize(w t l), where Binarize(\u00b7) is some binarization scheme.\nThe two most popular binarization schemes are BinaryConnect (Courbariaux et al., 2015) and Binary-Weight-Network (BWN) (Rastegari et al., 2016). In BinaryConnect, binarization is performed by transforming each element of wtl to \u22121 or +1 using the sign function:1\nBinarize(wtl) = sign(w t l). (1)\nBesides the binarized weight matrix, a scaling parameter is also learned in BWN. In other words, Binarize(wtl) = \u03b1 t lb t l , where \u03b1 t l > 0 and b t l is binary. They are obtained by minimizing the difference between wtl and \u03b1 t lb t l , and have a simple closed-form solution:\n\u03b1tl = \u2016wtl\u20161 nl , btl = sign(w t l), (2)\nwhere nl is the number of weights in layer l. Hubara et al. (2016) further binarized the activations as x\u0302tl = sign(x t l), where x t l is the activation of the lth layer at iteration t."}, {"heading": "2.2 PROXIMAL NEWTON ALGORITHM", "text": "The proximal Newton algorithm (Lee et al., 2014) has been popularly used for solving composite optimization problems of the form\nmin x f(x) + g(x),\n1A stochastic binarization scheme is also proposed in (Courbariaux et al., 2015). However, it is much more computational expensive than (1) and so will not be considered here.\nwhere f is convex and smooth, and g is convex but possibly nonsmooth. At iteration t, it generates the next iterate as\nxt+1 = argmin x \u2207f(xt)>(x\u2212 xt) + (x\u2212 xt)>H(x\u2212 xt) + g(x),\nwhere H is an approximate Hessian matrix of f at xt. With the use of second-order information, the proximal Newton algorithm converges faster than the proximal gradient algorithm (Lee et al., 2014). Recently, by assuming that f and g have difference-of-convex decompositions (Yuille & Rangarajan, 2002), the proximal Newton algorithm is also extended to the case where g is nonconvex (Rakotomamonjy et al., 2016)."}, {"heading": "3 LOSS-AWARE BINARIZATION", "text": "As can be seen, existing weight binarization methods (Courbariaux et al., 2015; Rastegari et al., 2016) simply find the closest binary approximation of w, and ignore its effects to the loss. In this paper, we consider the loss directly during binarization. As in (Rastegari et al., 2016), we also binarize the weight wl in each layer as w\u0302l = \u03b1lbl, where \u03b1l > 0 and bl is binary.\nIn the following, we make the following assumptions on `. (A1) ` is continuously differentiable with Lipschitz-continuous gradient, i.e., there exists \u03b2 > 0 such that \u2016\u2207`(u)\u2212\u2207`(v)\u20162 \u2264 \u03b2 \u2016u\u2212 v\u20162 for any u,v; (A2) ` is bounded from below."}, {"heading": "3.1 BINARIZATION USING PROXIMAL NEWTON ALGORITHM", "text": "We formulate weight binarization as the following optimization problem:\nminw\u0302 `(w\u0302) (3) s.t. w\u0302l = \u03b1lbl, \u03b1l > 0, bl \u2208 {\u00b11}nl , l = 1, . . . , L, (4)\nwhere ` is the loss. Let C be the feasible region in (4), and define its indicator function: IC(w\u0302) = 0 if w\u0302 \u2208 C, and\u221e otherwise. Problem (3) can then be rewritten as\nmin w\u0302 `(w\u0302) + IC(w\u0302). (5)\nWe solve (5) using the proximal Newton method (Section 2.2). At iteration t, the smooth term `(w\u0302t) is replaced by the second-order expansion\n`(w\u0302t\u22121) +\u2207`(w\u0302t\u22121)>(w\u0302t \u2212 w\u0302t\u22121) + 1 2 (w\u0302t \u2212 w\u0302t\u22121)>Ht\u22121(w\u0302t \u2212 w\u0302t\u22121),\nwhere Ht\u22121 is an estimate of the Hessian of ` at w\u0302t\u22121. Note that using the Hessian to capture second-order information is essential for efficient neural network training, as ` is often flat in some directions but highly curved in others. By rescaling the gradient, the loss has similar curvatures along all directions. This is also called preconditioning in the literature (Dauphin et al., 2015a).\nFor neural networks, the exact Hessian is rarely positive semi-definite. This can be problematic as the nonconvex objective leads to indefinite quadratic optimization. Moreover, computing the exact Hessian is both time- and space-inefficient on large networks. To alleviate these problems, a popular approach is to approximate the Hessian by a diagonal positive definite matrix D. One popular choice is the efficient Jacobi preconditioner. Though an efficient approximation of the Hessian under certain conditions, it is not competitive for indefinite matrices (Dauphin et al., 2015a). More recently, it is shown that equilibration provides a more robust preconditioner in the presence of saddle points (Dauphin et al., 2015a). This is also adopted by popular stochastic optimization algorithms such as RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015). Specifically, the second moment v in these algorithms is an estimator of diag(H2) (Dauphin et al., 2015b). Here, we use the square root of this v, which is readily available in Adam, to construct D = Diag([diag(D1)>, . . . , diag(DL)>]>), where Dl is the approximate diagonal Hessian at layer l. In general, other estimators of diag(H) can also be used.\nAt the tth iteration of the proximal Newton algorithm, the following subproblem is solved:\nminw\u0302t \u2207`(w\u0302t\u22121)>(w\u0302t \u2212 w\u0302t\u22121) + 1\n2 (w\u0302t \u2212 w\u0302t\u22121)>Dt\u22121(w\u0302t \u2212 w\u0302t\u22121) (6)\ns.t. w\u0302tl = \u03b1 t lb t l , \u03b1 t l > 0, b t l \u2208 {\u00b11}nl , l = 1, . . . , L.\nProposition 3.1 Let dt\u22121l \u2261 diag(D t\u22121 l ), and\nwtl \u2261 w\u0302t\u22121l \u2212\u2207l`(w\u0302 t\u22121) dt\u22121l . (7)\nThe optimal solution of (6) can be obtained in closed-form as\n\u03b1tl = \u2016dt\u22121l wtl\u20161 \u2016dt\u22121l \u20161 , btl = sign(w t l). (8)\nTheorem 3.1 Assume that [dtl ]k > \u03b2 \u2200l, k, t, the objective of (5) produced by the proximal Newton algorithm (with closed-form update of w\u0302t in Proposition 3.1) converges.\nNote that both the loss ` and indicator function IC(\u00b7) in (5) are not convex. Hence, convergence analysis of the proximal Newton algorithm in (Lee et al., 2014), which is only for convex problems, cannot be applied. Recently, Rakotomamonjy et al. (2016) proposed a nonconvex proximal Newton extension. However, it assumes a difference-of-convex decomposition which does not hold here.\nRemark 3.1 When Dt\u22121l = \u03bbI, i.e., the curvature is the same for all dimensions in the lth layer, (8) then reduces to the BWN solution in (2) In other words, BWN corresponds to using the proximal gradient algorithm, while the proposed method corresponds to the proximal Newton algorithm with diagonal Hessian. In composite optimization, it is known that the proximal Newton method is more efficient than the proximal gradient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016).\nRemark 3.2 When \u03b1tl = 1, (8) reduces to sign(wtl), which is the BinaryConnect solution in (1).\nFrom (7) and (8), each iteration first performs gradient descent along \u2207l`(w\u0302t\u22121) with an adaptive learning rate 1 dt\u22121l , and then projects it to a binary solution. As discussed in (Courbariaux et al., 2015), it is important to keep a full-precision weight during training. Hence, we replace (7) by wtl \u2190 w t\u22121 l \u2212 \u2207l`(w\u0302t\u22121) d t\u22121 l . The whole procedure, which will be called Loss-Aware Binarization (LAB), is shown in Algorithm 1. In steps 5 and 6, following (Li & Liu, 2016), we first rescale input xt\u22121l to the lth layer with \u03b1l, so that multiplications in dot products and convolutions become additions.\nWhile binarizing weights changes most multiplications to additions, binarizing both weights and activations saves even more computations as additions are further changed to XNOR bit operations (Hubara et al., 2016). Our Algorithm 1 can also be easily extended by binarizing the activations with the simple sign function."}, {"heading": "3.2 EXTENSION TO RECURRENT NEURAL NETWORKS", "text": "The proposed method can be easily extended to recurrent neural networks. Let xl and hl be the input and hidden states, respectively, at time step (or depth) l. A typical recurrent neural network has a recurrence of the form hl = Wxxl +Wh\u03c3(hl\u22121) + b (equivalent to the more widely known hl = \u03c3(Wxxl+Whhl\u22121+b) (Pascanu et al., 2013) ). We binarize both the input-to-hidden weight Wx and hidden-to-hidden weight Wh. Since weights are shared across time in a recurrent network, we only need to binarize Wx and Wh once in each forward propagation. Besides weights, one can also binarize the activations (of the inputs and hidden states) as in the previous section.\nIn deep networks, the backpropagated gradient takes the form of a product of Jacobian matrices (Pascanu et al., 2013). In a vanilla recurrent neural network,2 for activations hp and hq at depths p and q, respectively (where p > q), \u2202hp\u2202hq = \u220f q<l\u2264p \u2202hl \u2202hl\u22121 = \u220f q<l\u2264pW > h diag(\u03c3\n\u2032(hl\u22121)). The necessary condition for exploding gradients is that the largest singular value \u03bb1(Wh) of Wh is larger than some given constant (Pascanu et al., 2013). The following Proposition shows that for any binary Wh, its largest singular value is lower-bounded by the square root of its dimension.\nProposition 3.2 For any W \u2208 {\u22121,+1}m\u00d7n (m \u2264 n), \u03bb1(W) \u2265 \u221a n.\n2Here, we consider the vanilla recurrent neural network for simplicity. It can be shown that a similar behavior holds for the more commonly used LSTM.\nAlgorithm 1 Loss-Aware Binarization (LAB) for training a feedforward neural network. Input: Minibatch {(xt0,yt)}, current full-precision weights {wtl}, first moment {m t\u22121 l }, second moment {vt\u22121l }, and learning rate \u03b7t. 1: Forward Propagation 2: for l = 1 to L do 3: \u03b1tl = \u2016dt\u22121l w t l\u20161\n\u2016dt\u22121l \u20161 ;\n4: btl = sign(w t l); 5: rescale the layer-l input: x\u0303tl\u22121 = \u03b1 t lx t l\u22121; 6: compute ztl with input x\u0303 t l\u22121 and binary weight b t l ; 7: apply batch-normalization and nonlinear activation to ztl to obtain x t l ; 8: end for 9: compute the loss ` using xtL and y\nt; 10: Backward Propagation 11: initialize output layer\u2019s activation\u2019s gradient \u2202`\n\u2202xtL ;\n12: for l = L to 2 do 13: compute \u2202`\n\u2202xtl\u22121 using \u2202` \u2202xtl , \u03b1tl and b t l ;\n14: end for 15: Update parameters using Adam 16: for l = 1 to L do 17: compute gradients\u2207l`(w\u0302t) using \u2202`\u2202xtl and x t l\u22121; 18: update first moment mtl = \u03b21m t\u22121 l + (1\u2212 \u03b21)\u2207l`(w\u0302t); 19: update second moment vtl = \u03b22v t\u22121 l + (1\u2212 \u03b22)(\u2207l`(w\u0302t) \u2207l`(w\u0302t)); 20: compute unbiased first moment m\u0302tl = m t l/(1\u2212 \u03b2t1); 21: compute unbiased second moment v\u0302tl = v t l/(1\u2212 \u03b2t2); 22: compute current curvature matrix dtl = 1 \u03b7t ( 1+ \u221a v\u0302tl ) ; 23: update full-precision weights wt+1l = w t l \u2212 m\u0302tl dtl ; 24: update learning rate \u03b7t+1 = UpdateRule(\u03b7t, t+ 1); 25: end for\nThus, with weight binarization as in BinaryConnect, the exploding gradient problem becomes more severe as the weight matrices are often large. On the other hand, recall that \u03bb1(cW\u0302h) = c\u03bb1(W\u0302h) for any non-negative c. The proposed method alleviates this exploding gradient problem by adaptively learning the scaling parameter \u03b1h."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we perform experiments on the proposed binarization scheme with both feedforward networks (Sections 4.1 and 4.2) and recurrent neural networks (Sections 4.3 and 4.4)."}, {"heading": "4.1 FEEDFORWARD NEURAL NETWORKS", "text": "We compare the original full-precision network (without binarization) with the following weightbinarized networks: (i) BinaryConnect; (ii) Binary-Weight-Network (BWN); and (iii) the proposed Loss-Aware Binarized network (LAB). We also compare with networks having both weights and activations binarized:3 (i) BinaryNeuralNetwork (BNN) (Hubara et al., 2016), the weight-andactivation binarized counterpart of BinaryConnect; (ii) XNOR-Network (XNOR) (Rastegari et al., 2016), the counterpart of BWN; (iii) LAB2, the counterpart of the proposed method, which binarizes weights using proximal Newton method and binarizes activations using a simple sign function.\nThe setup is similar to that in Courbariaux et al. (2015). We do not perform data augmentation or unsupervised pretraining. Experiments are performed on three commonly used data sets:\n3We use the straight-through-estimator (Hubara et al., 2016) to compute the gradient involving the sign function.\n1. MNIST: This contains 28 \u00d7 28 gray images from ten digit classes. We use 50000 images for training, another 10000 for validation, and the remaining 10000 for testing. We use the 4-layer model:\n784FC \u2212 2048FC \u2212 2048FC \u2212 2048FC \u2212 10SVM, where FC is a fully-connected layer, and SVM is a L2-SVM output layer using the square hinge loss. Batch normalization, with a minibatch size 100, is used to accelerate learning. The maximum number of epochs is 50. The learning rate for the weight-binarized (resp. weight-and-activation-binarized) network starts at 0.01 (resp. 0.005), and decays by a factor of 0.1 at epochs 15 and 25.\n2. CIFAR-10: This contains 32 \u00d7 32 color images from ten object classes. We use 45000 images for training, another 5000 for validation, and the remaining 10000 for testing. The images are preprocessed with global contrast normalization and ZCA whitening. We use the VGG-like architecture:\n(2\u00d7128C3)\u2212MP2\u2212(2\u00d7256C3)\u2212MP2\u2212(2\u00d7512C3)\u2212MP2\u2212(2\u00d71024FC)\u221210SVM, where C3 is a 3\u00d73 ReLU convolution layer, andMP2 is a 2\u00d72 max-pooling layer. Batch normalization, with a minibatch size of 50, is used. The maximum number of epochs is 200. The learning rate for the weight-binarized (resp. weight-and-activation-binarized) network starts at 0.03 (resp. 0.02), and decays by a factor of 0.5 after every 15 epochs.\n3. SVHN: This contains 32 \u00d7 32 color images from ten digit classes. We use 598388 images for training, another 6000 for validation, and the remaining 26032 for testing. The images are preprocessed with global and local contrast normalization. The model used is:\n(2\u00d764C3)\u2212MP2\u2212(2\u00d7128C3)\u2212MP2\u2212(2\u00d7256C3)\u2212MP2\u2212(2\u00d71024FC)\u221210SVM. Batch normalization, with a minibatch size of 50, is used. The maximum number of epochs is 50. The learning rate for the weight-binarized (resp. weight-and-activation-binarized) network starts at 0.001 (resp. 0.0005), and decays by a factor of 0.1 at epochs 15 and 25.\nSince binarization is a form of regularization (Courbariaux et al., 2015), we do not use other regularization methods (like Dropout). All the weights are initialized as in (Glorot & Bengio, 2010). Adam (Kingma & Ba, 2015) is used as the optimization solver.\nTable 1 shows the test classification error rates, and Figure 1 shows the convergence of LAB. As can be seen, the proposed LAB achieves the lowest error on MNIST and SVHN. It even outperforms the full-precision network on MNIST, as weight binarization serves as a regularizer. With the use of curvature information, LAB outperforms BinaryConnect and BWN. On CIFAR-10, LAB is slightly outperformed by BinaryConnect, but is still better than the full-precision network. Among the schemes that binarize both weights and activations, LAB2 also outperforms BNN and the XNOR-Network."}, {"heading": "4.2 VARYING THE NUMBER OF FILTERS IN CNN", "text": "As in Zhou et al. (2016), we study sensitivity to network width by varying the number of filters K on the SVHN data set. As in Section 4.1, we use the model\n(2\u00d7 KC3)\u2212MP2\u2212 (2\u00d7 2KC3)\u2212MP2\u2212 (2\u00d7 4KC3)\u2212MP2\u2212 (2\u00d7 1024FC)\u2212 10SVM. Results are shown in Table 2. Again, the proposed LAB has the best performance. Moreover, as the number of filters increases, degradation due to binarization becomes less severe. This suggests\nthat more powerful models (e.g., CNN with more filters, standard feedforward networks with more hidden units) are less susceptible to performance degradation due to binarization. We speculate that this is because large networks often have larger-than-needed capacities, and so are less affected by the limited expressiveness of binary weights. Another related reason is that binarization acts as regularization, and so contributes positively to the performance."}, {"heading": "4.3 RECURRENT NEURAL NETWORKS", "text": "In this section, we perform experiments on the popular long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997). Performance is evaluated in the context of character-level language modeling. The LSTM takes as input a sequence of characters, and predicts the next character at each time step. The training objective is the cross-entropy loss over all target sequences. Following Karpathy et al. (2016), we use two data sets (with the same training/validation/test set splitting): (i) Leo Tolstoy\u2019s War and Peace, which consists of 3258246 characters of almost entirely English text with minimal markup and has a vocabulary size of 87; and (ii) the source code of the Linux Kernel, which consists of 6206996 characters and has a vocabulary size of 101.\nWe use a one-layer LSTM with 512 cells. The maximum number of epochs is 200, and the number of time steps is 100. The initial learning rate is 0.002. After 10 epochs, it is decayed by a factor of 0.98 after each epoch. The weights are initialized uniformly in [0.08, 0.08]. After each iteration, the gradients are clipped to the range [\u22125, 5], and all the updated weights are clipped to [\u22121, 1]. For the weight-and-activation-binarized networks, we do not binarize the inputs, as they are one-hot vectors in this language modeling task.\nTable 3 shows the testing cross-entropy values. As in Section 4.1, the proposed LAB outperforms other weight binarization schemes, and is even better than the full-precision network on the Linux Kernel data set. BinaryConnect does not work well here because of the problem of exploding gradients (see Section 3.2 and more results in Section 4.4). On the other hand, BWN and the proposed LAB scale the binary weight matrix and perform better. LAB also performs better than BWN as curvature information is considered. Similarly, among schemes that binarize both weights and activations, the proposed LAB2 also outperforms BNN and XNOR-Network."}, {"heading": "4.4 VARYING THE NUMBER OF TIME STEPS IN LSTM", "text": "In this experiment, we study the sensitivity of the binarization schemes with varying numbers of unrolled time steps (TS) in LSTM. Results are shown in Table 4. Again, the proposed LAB has the best performance. When TS = 10, the LSTM is relatively shallow, and all binarization schemes have similar performance as the full-precision network. When TS \u2265 50, BinaryConnect fails,\nwhile BWN and the proposed LAB perform better (as discussed in Section 3.2). Figure 2 shows the distributions of the hidden-to-hidden weight gradients for TS = 10 and 100. As can be seen, while all models have similar gradient distributions at TS = 10, the gradient values in BinaryConnect are much higher than those of the other algorithms for the deeper network (TS = 100).\nNote from Table 4 that as the time step increases, all except BinaryConnect show better performance. However, degradation due to binarization also becomes more severe. This is because the weights are shared across time steps. Hence, error due to binarization also propagates across time."}, {"heading": "5 CONCLUSION", "text": "In this paper, we propose a binarization algorithm that directly considers its effect on the loss during binarization. The binarized weights are obtained using proximal Newton algorithm with diagonal Hessian approximation. The proximal step has an efficient closed-form solution, and the secondorder information in the Hessian can be readily obtained from the Adam optimizer. Experiments show that the proposed algorithm outperforms existing binarization schemes, has comparable performance as the original full-precision network, and is also robust for wide and deep networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614513). We thank Yongqi Zhang for helping with the experiments, and developers of Theano (Theano Development Team, 2016), Pylearn2 (Goodfellow et al., 2013) and Lasagne. We also thank NVIDIA for the support of Titan X GPU."}, {"heading": "A PROOF OF PROPOSITION 3.1", "text": "\u2207`(w\u0302t\u22121)>(w\u0302t \u2212 w\u0302t\u22121) + 1 2 (w\u0302t \u2212 w\u0302t\u22121)>Dt\u22121(w\u0302t \u2212 w\u0302t\u22121)\n= 1\n2 L\u2211 l=1 (\u221a (dt\u22121l ) > ( w\u0302tl \u2212 (w\u0302t\u22121l \u2212\u2207l`(w\u0302 t\u22121) dt\u22121l ) ))2 + c1\n= 1\n2 L\u2211 l=1 (\u221a (dt\u22121l ) >(w\u0302tl \u2212wtl) )2 + c1\n= 1\n2 L\u2211 l=1 (\u221a (dt\u22121l ) >(\u03b1tlb t l \u2212wtl) )2 + c1,\nwhere c1 = \u2212 12 (\u221a (dt\u22121l ) >(\u2207l`(w\u0302t\u22121) dt\u22121l ) )2 . Since \u03b1tl > 0,d t l 0,\u2200l = 1, 2, . . . , L, we\nhave btl = sign(w t l). Moreover,\n1\n2 L\u2211 l=1 ( \u221a (dt\u22121l ) >(\u03b1tlb t l \u2212wtl))2 + c1 = 1 2 L\u2211 l=1 (\u221a (dt\u22121l ) >( \u2223\u2223\u03b1tl1\u2212 |wtl |\u2223\u2223))2 + c1\n= L\u2211 l=1 1 2 \u2016dt\u22121l \u20161(\u03b1 t l) 2 \u2212 \u2016dt\u22121l w t l\u20161\u03b1tl + c2,\nwhere c2 = c1 \u2212 12 \u2016dt\u22121l w t l\u2016 2 1\n\u2016dt\u22121l \u20161 . Thus, the optimal \u03b1tl is\n\u2016dt\u22121l w t l\u20161\n\u2016dt\u22121l \u20161 ."}, {"heading": "B PROOF OF THEOREM 3.1", "text": "Let \u03b1 = [\u03b1t1 . . . , \u03b1 t L] >, and denote the objective in (3) by F (w\u0302,\u03b1). As w\u0302t is the minimizer in (6), we have\n`(w\u0302t\u22121) +\u2207`(w\u0302t\u22121)>(w\u0302t \u2212 w\u0302t\u22121) + 1 2 (w\u0302t \u2212 w\u0302t\u22121)>Dt\u22121(w\u0302t \u2212 w\u0302t\u22121) \u2264 `(w\u0302t\u22121). (9)\nFrom Assumption A1, we have\n`(w\u0302t) \u2264 `(w\u0302t\u22121) +\u2207`(w\u0302t\u22121)>(w\u0302t \u2212 w\u0302t\u22121) + \u03b2 2 \u2225\u2225w\u0302t \u2212 w\u0302t\u22121\u2225\u22252 2 . (10)\nUsing (9) and (10), we obtain\n`(w\u0302t) \u2264 `(w\u0302t\u22121)\u2212 1 2 (w\u0302t \u2212 w\u0302t\u22121)>(Dt\u22121 \u2212 \u03b2I)(w\u0302t \u2212 w\u0302t\u22121)\n\u2264 `(w\u0302t\u22121)\u2212 mink,l([d t\u22121 l ]k \u2212 \u03b2) 2 \u2225\u2225w\u0302t \u2212 w\u0302t\u22121\u2225\u22252 2 .\nLet c3 = mink,l,t([dt\u22121l ]k \u2212 \u03b2) > 0. Then,\n`(w\u0302t) \u2264 `(w\u0302t\u22121)\u2212 c3 2 \u2225\u2225w\u0302t \u2212 w\u0302t\u22121\u2225\u22252 2 . (11)\nFrom Assumption A2, ` is bounded from below. Together with the fact that {`(w\u0302t)} is monotonically decreasing from (11), the sequence {`(w\u0302t)} converges, thus the sequence {F (w\u0302t,\u03b1t)} also converges."}, {"heading": "C PROOF OF PROPOSITION 3.2", "text": "Let the singulars values of W be \u03bb1(W) \u2265 \u03bb2(W) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbm(W).\n\u03bb21(W) \u2265 1\nm m\u2211 i=1 \u03bb2i (W) = 1 m \u2016W\u20162F = 1 m mn = n.\nThus, \u03bb1(W) \u2265 \u221a n."}], "references": [{"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.P. David"], "venue": "In NIPS,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Y. Dauphin", "H. de Vries", "Y. Bengio"], "venue": "In NIPS, pp", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "RMSprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "Technical Report arXiv:1502.04390,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In AISTAT, pp", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "Technical Report arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and Huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "In ICLR,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, pp", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "In ICLR,", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "In ICLR,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Proximal Newton-type methods for minimizing composite functions", "author": ["J.D. Lee", "Y. Sun", "M.A. Saunders"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Ternary weight networks", "author": ["F. Li", "B. Liu"], "venue": "Technical Report", "citeRegEx": "Li and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Li and Liu.", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Training deep and recurrent networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Neural Networks: Tricks of the trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "In NIPS, pp", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Pascanu and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu and Bengio.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "DC proximal Newton for nonconvex optimization problems", "author": ["A. Rakotomamonjy", "R. Flamary", "G. Gasso"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Rakotomamonjy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rakotomamonjy et al\\.", "year": 2016}, {"title": "XNOR-Net: ImageNet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "In ECCV,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "The concave-convex procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "NIPS, 2:1033\u20131040,", "citeRegEx": "Yuille and Rangarajan.,? \\Q2002\\E", "shortCiteRegEx": "Yuille and Rangarajan.", "year": 2002}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "Technical Report arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["S. Zhou", "Z. Ni", "X. Zhou", "H. Wen", "Y. Wu", "Y. Zou"], "venue": "Technical Report", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "One attempt first trains a neural network and then compresses it (Han et al., 2016; Kim et al., 2016).", "startOffset": 65, "endOffset": 101}, {"referenceID": 10, "context": "One attempt first trains a neural network and then compresses it (Han et al., 2016; Kim et al., 2016).", "startOffset": 65, "endOffset": 101}, {"referenceID": 16, "context": "Example approaches include tensorizing (Novikov et al., 2015), parameter quantization (Gong et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 5, "context": ", 2015), parameter quantization (Gong et al., 2014), and binarization (Courbariaux et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016).", "startOffset": 26, "endOffset": 97}, {"referenceID": 20, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016).", "startOffset": 26, "endOffset": 97}, {"referenceID": 14, "context": "Instead of binarization, ternary-connect quantizes each weight to {\u22121, 0, 1} (Lin et al., 2016).", "startOffset": 77, "endOffset": 95}, {"referenceID": 24, "context": "Similarly, the ternary weight network (Li & Liu, 2016) and DoReFa-net (Zhou et al., 2016) quantize weights to three levels or more.", "startOffset": 70, "endOffset": 89}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass. Courbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks.", "startOffset": 27, "endOffset": 303}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass. Courbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks. Besides binarizing the weights, Hubara et al. (2016) further binarized the activations.", "startOffset": 27, "endOffset": 498}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass. Courbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks. Besides binarizing the weights, Hubara et al. (2016) further binarized the activations. Rastegari et al. (2016) also learned to scale the binarized weights, and obtained better results.", "startOffset": 27, "endOffset": 557}, {"referenceID": 3, "context": "Another approach uses element-wise adaptive learning rate, as in Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 23, "context": ", 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015).", "startOffset": 18, "endOffset": 32}, {"referenceID": 12, "context": "We formulate this as an optimization problem using the proximal Newton algorithm (Lee et al., 2014) with a diagonal Hessian.", "startOffset": 81, "endOffset": 99}, {"referenceID": 0, "context": "The proposed method also reduces to BinaryConnect (Courbariaux et al., 2015) and the Binary-Weight-Network (Hubara et al.", "startOffset": 50, "endOffset": 76}, {"referenceID": 2, "context": "Another approach uses element-wise adaptive learning rate, as in Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015). This can also be considered as preconditioning that rescales the gradient so that all dimensions have similar curvatures.", "startOffset": 74, "endOffset": 183}, {"referenceID": 0, "context": "As it is essential to use full-precision weights during updates (Courbariaux et al., 2015), typically binarized weights are only used during the forward and backward propagations, but not on parameter update.", "startOffset": 64, "endOffset": 90}, {"referenceID": 0, "context": "The two most popular binarization schemes are BinaryConnect (Courbariaux et al., 2015) and Binary-Weight-Network (BWN) (Rastegari et al.", "startOffset": 60, "endOffset": 86}, {"referenceID": 20, "context": ", 2015) and Binary-Weight-Network (BWN) (Rastegari et al., 2016).", "startOffset": 40, "endOffset": 64}, {"referenceID": 12, "context": "The proximal Newton algorithm (Lee et al., 2014) has been popularly used for solving composite optimization problems of the form min x f(x) + g(x),", "startOffset": 30, "endOffset": 48}, {"referenceID": 0, "context": "A stochastic binarization scheme is also proposed in (Courbariaux et al., 2015).", "startOffset": 53, "endOffset": 79}, {"referenceID": 12, "context": "With the use of second-order information, the proximal Newton algorithm converges faster than the proximal gradient algorithm (Lee et al., 2014).", "startOffset": 126, "endOffset": 144}, {"referenceID": 19, "context": "Recently, by assuming that f and g have difference-of-convex decompositions (Yuille & Rangarajan, 2002), the proximal Newton algorithm is also extended to the case where g is nonconvex (Rakotomamonjy et al., 2016).", "startOffset": 185, "endOffset": 213}, {"referenceID": 0, "context": "As can be seen, existing weight binarization methods (Courbariaux et al., 2015; Rastegari et al., 2016) simply find the closest binary approximation of w, and ignore its effects to the loss.", "startOffset": 53, "endOffset": 103}, {"referenceID": 20, "context": "As can be seen, existing weight binarization methods (Courbariaux et al., 2015; Rastegari et al., 2016) simply find the closest binary approximation of w, and ignore its effects to the loss.", "startOffset": 53, "endOffset": 103}, {"referenceID": 20, "context": "As in (Rastegari et al., 2016), we also binarize the weight wl in each layer as \u0175l = \u03b1lbl, where \u03b1l > 0 and bl is binary.", "startOffset": 6, "endOffset": 30}, {"referenceID": 12, "context": "Hence, convergence analysis of the proximal Newton algorithm in (Lee et al., 2014), which is only for convex problems, cannot be applied.", "startOffset": 64, "endOffset": 82}, {"referenceID": 12, "context": "Hence, convergence analysis of the proximal Newton algorithm in (Lee et al., 2014), which is only for convex problems, cannot be applied. Recently, Rakotomamonjy et al. (2016) proposed a nonconvex proximal Newton extension.", "startOffset": 65, "endOffset": 176}, {"referenceID": 12, "context": "In composite optimization, it is known that the proximal Newton method is more efficient than the proximal gradient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016).", "startOffset": 126, "endOffset": 172}, {"referenceID": 19, "context": "In composite optimization, it is known that the proximal Newton method is more efficient than the proximal gradient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016).", "startOffset": 126, "endOffset": 172}, {"referenceID": 0, "context": "As discussed in (Courbariaux et al., 2015), it is important to keep a full-precision weight during training.", "startOffset": 16, "endOffset": 42}, {"referenceID": 18, "context": "A typical recurrent neural network has a recurrence of the form hl = Wxxl +Wh\u03c3(hl\u22121) + b (equivalent to the more widely known hl = \u03c3(Wxxl+Whhl\u22121+b) (Pascanu et al., 2013) ).", "startOffset": 148, "endOffset": 170}, {"referenceID": 18, "context": "In deep networks, the backpropagated gradient takes the form of a product of Jacobian matrices (Pascanu et al., 2013).", "startOffset": 95, "endOffset": 117}, {"referenceID": 18, "context": "The necessary condition for exploding gradients is that the largest singular value \u03bb1(Wh) of Wh is larger than some given constant (Pascanu et al., 2013).", "startOffset": 131, "endOffset": 153}, {"referenceID": 20, "context": ", 2016), the weight-andactivation binarized counterpart of BinaryConnect; (ii) XNOR-Network (XNOR) (Rastegari et al., 2016), the counterpart of BWN; (iii) LAB2, the counterpart of the proposed method, which binarizes weights using proximal Newton method and binarizes activations using a simple sign function.", "startOffset": 99, "endOffset": 123}, {"referenceID": 0, "context": "The setup is similar to that in Courbariaux et al. (2015). We do not perform data augmentation or unsupervised pretraining.", "startOffset": 32, "endOffset": 58}, {"referenceID": 0, "context": "Since binarization is a form of regularization (Courbariaux et al., 2015), we do not use other regularization methods (like Dropout).", "startOffset": 47, "endOffset": 73}, {"referenceID": 24, "context": "As in Zhou et al. (2016), we study sensitivity to network width by varying the number of filters K on the SVHN data set.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "Following Karpathy et al. (2016), we use two data sets (with the same training/validation/test set splitting): (i) Leo Tolstoy\u2019s War and Peace, which consists of 3258246 characters of almost entirely English text with minimal markup and has a vocabulary size of 87; and (ii) the source code of the Linux Kernel, which consists of 6206996 characters and has a vocabulary size of 101.", "startOffset": 10, "endOffset": 33}, {"referenceID": 6, "context": "We thank Yongqi Zhang for helping with the experiments, and developers of Theano (Theano Development Team, 2016), Pylearn2 (Goodfellow et al., 2013) and Lasagne.", "startOffset": 123, "endOffset": 148}], "year": 2017, "abstractText": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "creator": "LaTeX with hyperref package"}}}