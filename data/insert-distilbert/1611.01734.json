{"id": "1611.01734", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Deep Biaffine Attention for Neural Dependency Parsing", "abstract": "while deep learning parsing approaches have proven very successful at finding the structure of sentences, most neural dependency structure parsers ultimately use neural networks only for feature extraction, and then use those input features in traditional parsing algorithms. in contrast, this paper builds off recent work using general - purpose neural network extraction components, training an attention mechanism over processing an lstm to attend objects to the head domain of the phrase. we get significant state - of - the - art results for standard dependency parsing benchmarks, achieving achieving 95. 44 % uas and 93. 76 % las on the ptb dataset, scoring 0. 8 % and 1. 0 % improvement, respectively, over andor et al. ( 2016 ). in addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model's performance, such as reducing the value of beta2 in the adam optimization algorithm.", "histories": [["v1", "Sun, 6 Nov 2016 07:26:38 GMT  (22kb)", "http://arxiv.org/abs/1611.01734v1", null], ["v2", "Tue, 22 Nov 2016 02:01:39 GMT  (22kb)", "http://arxiv.org/abs/1611.01734v2", "Under review for ICLR 2017; fixed typos and clarified prediction process"], ["v3", "Fri, 10 Mar 2017 04:37:03 GMT  (19kb)", "http://arxiv.org/abs/1611.01734v3", "Accepted to ICLR 2017; updated with new results and comparison to more recent models, including current state-of-the-art"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["timothy dozat", "christopher d manning"], "accepted": true, "id": "1611.01734"}, "pdf": {"name": "1611.01734.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tdozat@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n01 73\n4v 1\n[ cs\n.C L\n] 6\nN ov\n2 01\n6 Under review as a conference paper at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way, such as semantic parsing (Monroe & Wang, 2014) and retrieving images based on a textual description (Socher et al., 2014). However, frequent incorrect parses can severely inhibit final performance, even completely road-blocking lines of research; for this reason, improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks.\nIn recent years, using deep learning in dependency parsers has gained a lot of attention due to the uncanny ability of neural networks to find real statistical patterns from data. The usual approach involves essentially training a neural network feature extractor and using the neural features in place of handcrafted ones to take discrete actions in a traditional parsing algorithm. Until recently, little to no research had successfully built dependency parsers that only used components that have been used successfully in a wide variety of neural models (although Vinyals et al. (2015) build such a sequence-to-sequence constituency tree parser). There are a few reasons why one might want to build this kind of parser: neural tools have shown a great deal of promise across domains, so it would be wise to see to what extent dependency parsing can benefit from them; a parser that only uses general tools can benefit from innovations in those tools that come from other rapidly moving domains; and advances in the realm of dependency parsing have a higher chance of impacting other fields when they all share the same tools.\nIn this paper, we build on a recently proposed neural dependency parser that uses only \u201cneural\u201d components\u2014BiLSTMs and attention\u2014drawing on ideas proposed in the neural machine translation literature. Our model substitutes the concatenation-based attention mechanism they use with a variant of the bilinear attention proposed in the neural machine translation literature by Luong et al. (2015), augmenting it with additional MLP layers and making it parallel to traditional classification over a fixed number of classes. In addition to using bilinear transformations to predict dependency arc structures, we also show how to extend bilinear transformations to predict the dependency relations as well.\nFurthermore, we explore how different hyperparameter choices\u2014some specific to dependency parsing, others more generally applicable\u2014impacted performance, finding that deviating from estab-\nlished conventions can have a significant positive effect on the quality of the final model. By using this variation on traditional attention and exploring alternative hyperparameter configurations, we achieve state-of-the-art performance on standard dependency parsing tasks by a considerable margin."}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": ""}, {"heading": "2.1 TRANSITION-BASED NEURAL PARSING", "text": "Transition-based parsers\u2014such as shift-reduce parsers\u2014parse sentences from left to right, maintaining a \u201cbuffer\u201d of words that have not yet been parsed and a \u201cstack\u201d of words whose head has not been seen or whose dependents have not all been fully parsed. At each step, transition-based parsers can manipulate the stack and buffer and assign arcs from one word to another. One can then train any multi-class machine learning classifier on features extracted from the stack, buffer, and previous arc actions in order to predict the next action. Here we summarize the contributions made by models that use neural networks to make these transition actions.\nChen & Manning (2014) make the first successful attempt at incorporating deep learning into a dependency parser (henceforth the CM parser). Their approach involves using a feedforward network classifier to make parsing actions; at each step, the network takes as input the concatenation of the word, tag, and label (when applicable) embeddings for words in critical positions (e.g. the top three words on the stack and the leftmost and rightmost dependents of the top two) and puts them through a multilayer perceptron (MLP) that assigns a probability to each action the parser can take. At each step, the parser takes the most probable action, updating the stack and buffer accordingly.\nThe principal limitation of the CM parser is that it doesn\u2019t have access to the entire sentence when making each parsing action, and a number of other approaches have modified or augmented the CM parser to address this. Dyer et al. (2015) and Ballesteros et al. (2016) replace the input to the feedforward network\u2014which in the CM parser is a concatenation of embeddings\u2014with the output of LSTMs over the stack, buffer, and previous actions. Weiss et al. (2015) and Andor et al. (2016) achieve state of the art performance by instead augmenting it with a beam search and a CRF loss so that the model can avoid committing to partial parses that later evidence might reveal to be incorrect."}, {"heading": "2.2 GRAPH-BASED NEURAL PARSING", "text": "Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Because these general-purpose MST parsing algorithms are deterministic, they don\u2019t have access to any information about the sentence other than the weight of each arc, so interactions between words and phrases need to be captured in the process that produces the weights.\nKiperwasser & Goldberg (2016) take a graph-based approach to neural dependency parsing that is in many ways reminsicent of attention in neural machine translation as described by Bahdanau et al. (2014). In the attention model of Bahdanau et al., the recurrent output vector r(target)i of the current word i being generated in the target translation is concatenated with the recurrent output vector\nr (source) j of each word in the source sentence 1, and the result is fed into an MLP with a single linear output node representing the score of target word i aligning to source word j:\nhij = MLP ( r (target) i \u2295 r (source) j )\n(1)\nsij = u \u22a4 hij (2)\nKiperwasser & Goldberg (2016) effectively apply this mechanism to dependency parsing, where the word that a given token most strongly attends to is interpreted as its head. They use a bidirectional LSTM to generate a recurrent output vector ri for each word i, and for each pair of words i, j, they use the same kind of MLP to compute the score of word i being a dependent on word j:\nh (arc) ij = MLP (arc)(ri \u2295 rj) (3)\ns (arc) ij = u \u22a4 h (arc) ij (4)\nThe predicted tree structure is then the tree where each word i depends on the word j with the highest score sij . They train this model using a hinge loss to maximize the margin between the gold tree and the highest scoring incorrect tree.\nDependency relations are predicted in a similar fashion; the model concatenates ri with its gold (or at test time, predicted) head word\u2019s recurrent output vector ryi , and feeds that concatenation into another MLP (5) with an output layer that generates a score for each possible dependency relation (6), which they also train according to a hinge loss. They find that this setup gets excellent results on English and Chinese dependency parsing tasks.\nh (rel) i,yi = MLP(rel)(ri \u2295 ryi) (5)\ns (rel) i,yi = Uh (rel) i,yi + b(rel) (6)\nCheng et al. (2016) similarly propose a graph-based neural dependency parser, but in a way that attempts to circumvent the limitation of graph-based parsers being unable to condition the scores of each possible arc on previous parsing decisions. In addition to having one bidirectional recurrent network that computes a recurrent hidden vector ri for each word, they have additional, unidirectional recurrent networks (left-to-right and right-to-left) that keep track of the probability of previous parsing decisions, and use these together to predict the scores for each arc."}, {"heading": "3 PROPOSED MODEL", "text": ""}, {"heading": "3.1 DEEP BIAFFINE PARSING", "text": "The traditional attention mechanism of Bahdanau et al. (2014) is not the only one that has been proposed in the literature; Luong et al. (2015) argue for substituting the MLP in the attention mechanism with a single bilinear transformation, mapping the target recurrent output vector r(target)i and the source recurrent output vector r(source)j to a score for the alignment:\nsij = r \u22a4(target) i Ur (source) j (7)\nThe straightforward application of this to dependency parsing would be to define the score of a potential dependency arc as a bilinear map between the dependent and the potential head.\ns (arc) ij = r \u22a4 i Urj (8)\nHowever, there are at least two disadvantages of using the recurrent vectors directly. The first is that they contain much more information than is necessary for calculating the value of sij\u2014because they\u2019re recurrent, they also contain information needed for calculating scores elsewhere in the sequence. Training on the entire vector then means training on superfluous information, which is likely\n1In this paper we follow the convention of using lowercase italic letters for scalars and indices, lowercase bold letters for vectors, uppercase italic letters for matrices, uppercase bold letters for higher order tensors. We also maintain this notation when indexing; so row i of matrix A would be represented as ai.\nH(arc-dep) \u2295 1 U (arc) H(arc-dep) S(arc)\nto lead to overfitting. The second disadvantage is that the recurrent vector ri consists of the concatenation of the left recurrent state \u2190\u2212r i and the right recurrent state \u2212\u2192 r i, meaning using ri by itself in the bilinear transformation keeps the features learned by the two LSTMs distinct; ideally we\u2019d like the model to learn features composed from both. We can address both of these issues simultaneously by first applying (distinct) MLP functions with a smaller hidden size to the two recurrent states ri and rj before the bilinear operation. This allows the model to combine the two recurrent states together while also reducing the dimensionality. We call this a deep bilinear attention mechanism, as opposed to shallow bilinear attention, which uses the recurrent states directly.\nh (arc-dep) i = MLP (arc-dep)(ri) (9)\nh (arc-head) j = MLP (arc-head)(rj) (10)\nWe also make a smaller change to the bilinear attention mechanism. In a traditional classification task, the distribution of classes is often uneven, so the output layer of the model normally includes a bias term designed to capture the prior probability P (yi = c) of each class, with the rest of the model focusing on learning the likelihood of each class given the data P (yi = c|xi). In dependency parsing, the distribution of dependents is similarly uneven\u2014many words have a global tendency to attract dependents (e.g. verbs, which frequently take many dependents) and others have a global tendency to deter them (e.g. function words, which generally have no dependents). In order to capture the prior probabilityP (yi = j|rj) of a word taking any dependent (rather than the likelihood P (yi = j|ri, rj) of a word taking a dependent given what the potential dependent is), we include a bias term linear in h(arc-head)j , making it a biaffine transformation rather than a bilinear one.\ns (arc) ij = h \u22a4(arc-dep) i U (arc) h (arc-head) j (11)\n+w\u22a4(arc)h (arc-head) j (12)\nAs with Kiperwasser & Goldberg (2016), the predicted tree is the one where each word is a dependent of its highest scoring head. This model can be trained with a hinge-loss or a cross-entropy objective; here, we use cross-entropy. Figure 2 shows one configuration of the proposed model."}, {"heading": "3.2 DEEP BIAFFINE CLASSIFICATION", "text": "In order to predict the labels, we use a parallel mechanism to deep biaffine attention. We want the label that the model predicts for a given word to be conditioned on that word\u2019s head (e.g. we want a word like \u201cfast\u201d be classified as an adverbial modifier when it depends on a verb, but not when it depends on a noun). So again, we use MLPs to transform the recurrent state of the current word ri and its gold or predicted head yi\u2019s recurrent state ryi , but this time we let the model predict an\narray of scores\u2014one for each possible label\u2014by allowing the biaffine transformation to map the two vectors to a third vector rather than a scalar. This can then also be trained under a hinge loss or cross-entropy loss objective, and we use the latter.\nh (rel-dep) i = MLP (rel-dep)(ri) (13)\nh (rel-head)\ny (arc) i\n= MLP(rel-head)(r y (arc) i ) (14)\ns (rel) i = h (rel-dep) i U (rel) h (rel-head)\ny (arc) i\n(15)\n+W (rel) (\nh (rel-dep) i \u2295 h (rel-head)\ny (arc) i\n)\n(16)\n+ b(rel) (17)\nAgain, in our model, the MLPs serve to reduce dimensionality and build features from the two halves of the last BiLSTM output state. In this part of the model, the term in line (16) captures global preferences for the kinds of labels word i can take, as well as global preferences for the kinds of dependents word i\u2019s head can take (e.g. articles will have a strong preference to take a determiner label and a noun will have a strongly prefer dependents to take determiner or adjectival labels)."}, {"heading": "3.3 PRACTICAL CONSIDERATIONS", "text": "A noteworthy advantage of bilinear or biaffine attention over traditional attention is that it requires less memory to compute. Traditional attention requires explicitly computing a size d hidden state for each ordered pair of words in the length n sentence; the resulting tensorH is (n\u00d7n\u00d7d)-dimensional. Bilinear attention, however, can be computed more memory-efficiently; because it uses matrix multiplications, it never explicitly computes a full (n \u00d7 n \u00d7 d) hidden state. As a result, computing traditional attention requires O(dn2) memory, whereas computing bilinear attention requires only O(dn + n2) memory. Similarly, the memory complexity of our approach to label classification is O(dnc+n2c)\u2014with c being the number of classes to predict\u2014whereas the complexity of the traditional attention approach is O(dn2 + cn). As long as the number of labels c is significantly smaller than the length of the longest sequence in the dataset, this bilinear classification mechanism will also be more memory-efficient than any variant that concatenates each pair of recurrent output vectors.\nSince BiLSTMs use O(dn) memory, this choice can noticeably affect the memory requirements of the network. While our TensorFlow (Abadi et al., 2015) implementation2 of a deep biaffine parser with multiple hidden MLP layers can train with less than 2.5GB of GPU memory, our implementation of an otherwise identical parser that uses a traditional attention mechanism requires more than 4GB, even with only one hidden MLP layer. Since the attention mechanism has far fewer parameters than the BiLSTMs, the concatenation-based approach winds up using upwards of 33% of its consumed memory on training a part of the network that comprises less than 1% of the parameters, which is clearly not ideal."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "4.1 HYPERPARAMETER SELECTION", "text": "In this section we go in depth into how the different hyperparameters affect parsing performance. Here we use the Penn Treebank train and validation splits, converted to the Stanford Dependencies using version 3.5.0 of the Stanford dependency converter, reporting unlabeled attachment score (UAS) and labeled attachment score (LAS). When not otherwise specified, our model uses: 100- dimensional word and tag embeddings with word vectors initialized to GloVe (Pennington et al., 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4- layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al., 2015), also with a 67% keep probability; the Adam optimizer (Kingma & Ba,\n2Using a different framework or implementation could yield different results\n2014) with \u03b21 = \u03b22 = .9 and learning rate 2e\u22123, annealed continuously at a rate of .75 every 2,500 iterations; 120 epochs of training, with batches of approximately 5,000 tokens. Words that only occur once in the training set are replaced with a special <UNK> token. These hyperparameters were selected based on a random search followed by further refining using both grid search and trial-and-error. The performance of the different hyperparameter configurations is shown in Table 1 and Table 2."}, {"heading": "4.1.1 DEPTH", "text": "First we examine how making the network deeper affects performance, since all other models discussed hereuse two-layer neural networks (except Cheng et al. (2016), who use one-layer networks). We test using two or four BiLSTM layers, and zero, one, or two MLP layers after the last BiLSTM. What we find is that making the network deeper improves performance to an extent\u2014when the BiLSTM is shallow, adding an MLP doesn\u2019t significantly improve performance, presumably because the dimensionality reduction limits its representative power too much\u2014but when the BiLSTM is deeper and can learn more abstract features, dimensionality reduction successfully helps the model avoid overfitting. Using a deeper MLP, however, actually hinders performance. The training accuracies of the two- and four-LSTM models with two-layer MLPs are comparable, suggesting that the deeper network isn\u2019t overfitting. Instead, it seems more likely that the biggest gain from the MLP layer is dimensionality reduction rather than adding significant further nonlinear abstraction, so the second layer serves only to needlessly distort the information learned by the LSTM.\nIt should be noted, however, that the increase in accuracy does come with a cost\u2013while the twolayer LSTM can parse about 1000 sentences per second on an nVidia Titan X GPU machine, the four-layer one can only parse about 500 sentences per second. Including dimensionality reduction, however, speeds up parsing by about 50 sentences per second."}, {"heading": "4.1.2 ATTENTION MECHANISM", "text": "Next, we compare three attention-based scoring mechanisms\u2014since our model uses a very different hyperparameter configuration from Kiperwasser & Goldberg\u2019s, we implemented the concatenationbased attention mechanism in addition to our deep biaffine one. However, the biaffine layer of our parser has O(d2) parameters and the last layer of the concatenation-based one has only O(d). In order to ensure that the extra parameters in our model don\u2019t influence the outcome, we also train a special case of our model with a O(d)-parameter bilinear layer where U (arc) and each slice U (rel)\n\u00b7,i,\u00b7\nis diagonal; for efficiency we also abstain from including the linear bias terms. What we find is that both the full biaffine parser and the smaller diagonal bilinear parser significantly outperform the concatenation-based one."}, {"heading": "4.1.3 RECURRENT CELL", "text": "We also tested to see how the choice of LSTM or GRU affects performance. However, we found that GRUs were unable to train with recurrent dropout, with the loss exploding after a few iterations of training even under a lower learning rate. Greff et al. (2015) suggest modifying the formulation of LSTMs to make them more GRU-like by using a coupled input-forget gate (CifLSTM), but retaining the output gate of the vanilla LSTM. We thus modified the formulation slightly to remove one of the tanh nonlinearities\u2014which is not needed when using an update gate zt (18 - 19)\u2014and trained\nparsers using them.\nct = it \u2299 tanh(at) + (1\u2212 ft)\u2299 ct\u22121 Vanilla LSTM cell (18)\nct = zt \u2299 at + (1\u2212 zt)\u2299 ct\u22121 Cif-LSTM cell (19)\nCritically, CifLSTM cells were able to train with dropout in spite of sharing one simplification to LSTMs that GRUs have; the reason why GRUs failed to train almost certainly has to do with scaling up the hidden state at training time. When using the dropped hidden state to compute the activations for the gates and next hidden state, the previous hidden state needs to be scaled up by the inverse of the keep probability in order to ensure that the expected activations at training time are the same as the actual activations at test time (Srivastava et al., 2014). However, because the GRU cell always reveals its hidden state and uses the update gate to retain the value of the hidden state across steps, scaling up the hidden state at every step in the sequence increases the magnitude of the the activations exponentially. Because of LSTMs\u2019 distinction between the cell state and the hidden state, they don\u2019t suffer from this problem to nearly the same extent.\nOne recently proposed alternative to dropout, zoneout (Krueger et al., 2016), would address this issue with using dropout in GRUs\u2013however, we leave experimenting with this for future work."}, {"heading": "4.1.4 OPTIMIZATION ALGORITHM", "text": "We choose to optimize with Adam (Kingma & Ba, 2014), which keeps a moving average of the L2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average, ensuring that the magnitude of the gradients will on average be close to one. However, we find that the value for \u03b22 recommended by Kingma & Ba\u2014which controls the decay rate for this moving average\u2014is too high. When this value is very large, the magnitude of the current update is heavily influenced by the larger magnitude of gradients very far in the past, with the effect that the optimizer can\u2019t adapt quickly to recent changes in the model. Thus we find that setting \u03b22 to .9 instead of .999 makes a large, positive impact on final performance."}, {"heading": "4.1.5 TAG DROPOUT", "text": "All models under consideration here\u2014including our own\u2014use POS tags as input. While we find training on POS tags to be very helpful for final performance, we want to ensure that our model doesn\u2019t overfit to specific sequences of POS tags and we want to ensure that it remains robust to upstream tagging errors. So to keep our model from depending too heavily on POS tags, we randomly drop them 15% of the time, finding this to noticeably improve performance."}, {"heading": "4.2 MODEL COMPARISON", "text": ""}, {"heading": "4.2.1 DATASETS", "text": "In this work we show test results for the proposed model on three datasets, coming from two sources: the English Penn Treebank, automatically converted from constituency trees into Stanford Dependencies using both version 3.3.0 and version 3.5.0 of the Stanford Dependency converter (PTB-SD 3.3.0 and PTB-SD 3.5.0); and the Chinese Penn Treebank version 5.1 (CTB 5.1), automatically converted from constituency trees into the CoNLL 2007 dependency format with Penn2Malt. PTB-SD\n3.3.0 and CTB 5.1 are datasets standardly used in other dependency parsing work over the past four years, and PTB-SD 3.5.0 is an updated version of PTB-SD 3.3.0. As is standard, we omit puncuation from evaluation and use predicted POS tags for the English PTB dataset\u2014generated from the Stanford POS tagger (Toutanova et al., 2003)\u2014and gold POS tags for the Chinese PTB dataset. Note that in the previous section we reported validation scores on PTB-SD 3.5.0, but in this section we report our test scores on PTB-SD 3.3.0 and compare those to those of other approaches in the literature, in order to keep the hyperparameter choices fairly independent of the test set. Word embeddings for Chinese were generated using Word2Vec (Mikolov et al., 2013) on Chinese Wikipedia."}, {"heading": "4.2.2 RESULTS", "text": "Here, we compare our model to a number of others in the literature: the models of Dyer et al. (2015) and Ballesteros et al. (2016), which use LSTMs to generate features for a transition-based parser; the models of Weiss et al. (2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al. (2016), which like this work uses BiLSTMs to generate feature embeddings used for an attention-based parser.\nWhat we see is that, with the exception of LAS on CTB 5.1, our implementation achieves stateof-the-art results by a fairly substantial margin. It\u2019s also worth pointing out that our parser also performs better on PTB-SD 3.5.0 and 3.3.0. While it\u2019s possible that this is in part because we tuned on 3.5.0, we think it probably represents improvements in the dependency representation and converter quality between version 3.3.0 and 3.5.0.\nOn the CTB labeled attachment score, our model underperforms state-of-the-art in spite of getting state-of-the-art unlabeled attachment results. We have two possible explanations for this: the first is that our pretrained embeddings were inferior to those used by the other researchers, and don\u2019t capture label information as well; the second, which we feel is more likely, is that the POS tag dropout prevented the model from heavily relying on tags to make label predictions. Since ours and all other models considered train and evaluate on gold tags, this regularization likely hurts the model\u2019s ability to learn the high correlation between gold tags and labels. Since in practice models won\u2019t have access to gold tags, we don\u2019t see this as a point of major concern, but future research will need to substantiate this hypothesis."}, {"heading": "5 CONCLUSION", "text": "In this paper we proposed using a modified version of bilinear attention in a neural network dependency parser, and showed that our approach outperformed state-of-the-art by a fairly large margin. We also discussed in detail some of the hyperparameter choices that we found to make a critical difference in end performance: we find that deeper networks of four LSTM layers outperform shallower networks of two LSTM layers when using our deep biaffine attention mechanism; we find GRU cells to have significant difficulty training with dropout, but LSTMs (vanilla or with a coupled input-forget gate) have no trouble; we argue that the default settings for the Adam optimizer should be tweaked; and we demonstrated tag dropout to be effective. Future work will explore the perfor-\nmance of this parser on a wider variety of languages and, especially for morphology-rich languages, augment it with a smarter way of handling out of vocabulary tokens."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Globally normalized transitionbased neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Gal and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dependency parsing features for semantic parsing", "author": ["Will Monroe", "Yushi Wang"], "venue": null, "citeRegEx": "Monroe and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Monroe and Wang.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model\u2019s performance, such as reducing the value of \u03b22 in the Adam optimization algorithm.", "startOffset": 35, "endOffset": 55}, {"referenceID": 17, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way, such as semantic parsing (Monroe & Wang, 2014) and retrieving images based on a textual description (Socher et al., 2014).", "startOffset": 363, "endOffset": 384}, {"referenceID": 16, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way, such as semantic parsing (Monroe & Wang, 2014) and retrieving images based on a textual description (Socher et al., 2014). However, frequent incorrect parses can severely inhibit final performance, even completely road-blocking lines of research; for this reason, improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks. In recent years, using deep learning in dependency parsers has gained a lot of attention due to the uncanny ability of neural networks to find real statistical patterns from data. The usual approach involves essentially training a neural network feature extractor and using the neural features in place of handcrafted ones to take discrete actions in a traditional parsing algorithm. Until recently, little to no research had successfully built dependency parsers that only used components that have been used successfully in a wide variety of neural models (although Vinyals et al. (2015) build such a sequence-to-sequence constituency tree parser).", "startOffset": 364, "endOffset": 1230}, {"referenceID": 13, "context": "Our model substitutes the concatenation-based attention mechanism they use with a variant of the bilinear attention proposed in the neural machine translation literature by Luong et al. (2015), augmenting it with additional MLP layers and making it parallel to traditional classification over a fixed number of classes.", "startOffset": 173, "endOffset": 193}, {"referenceID": 5, "context": "Dyer et al. (2015) and Ballesteros et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016) replace the input to the feedforward network\u2014which in the CM parser is a concatenation of embeddings\u2014with the output of LSTMs over the stack, buffer, and previous actions.", "startOffset": 11, "endOffset": 37}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016) replace the input to the feedforward network\u2014which in the CM parser is a concatenation of embeddings\u2014with the output of LSTMs over the stack, buffer, and previous actions. Weiss et al. (2015) and Andor et al.", "startOffset": 11, "endOffset": 229}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016) achieve state of the art performance by instead augmenting it with a beam search and a CRF loss so that the model can avoid committing to partial parses that later evidence might reveal to be incorrect.", "startOffset": 11, "endOffset": 31}, {"referenceID": 2, "context": "Kiperwasser & Goldberg (2016) take a graph-based approach to neural dependency parsing that is in many ways reminsicent of attention in neural machine translation as described by Bahdanau et al. (2014). In the attention model of Bahdanau et al.", "startOffset": 179, "endOffset": 202}, {"referenceID": 2, "context": "The traditional attention mechanism of Bahdanau et al. (2014) is not the only one that has been proposed in the literature; Luong et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 2, "context": "The traditional attention mechanism of Bahdanau et al. (2014) is not the only one that has been proposed in the literature; Luong et al. (2015) argue for substituting the MLP in the attention mechanism with a single bilinear transformation, mapping the target recurrent output vector r i and the source recurrent output vector r j to a score for the alignment: sij = r \u22a4(target) i Ur (source) j (7) The straightforward application of this to dependency parsing would be to define the score of a potential dependency arc as a bilinear map between the dependent and the potential head.", "startOffset": 39, "endOffset": 144}, {"referenceID": 16, "context": "When not otherwise specified, our model uses: 100dimensional word and tag embeddings with word vectors initialized to GloVe (Pennington et al., 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al.", "startOffset": 124, "endOffset": 149}, {"referenceID": 6, "context": ", 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al., 2015), also with a 67% keep probability; the Adam optimizer (Kingma & Ba, Using a different framework or implementation could yield different results", "startOffset": 366, "endOffset": 388}, {"referenceID": 15, "context": "When not otherwise specified, our model uses: 100dimensional word and tag embeddings with word vectors initialized to GloVe (Pennington et al., 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al.", "startOffset": 125, "endOffset": 361}, {"referenceID": 5, "context": "First we examine how making the network deeper affects performance, since all other models discussed hereuse two-layer neural networks (except Cheng et al. (2016), who use one-layer networks).", "startOffset": 143, "endOffset": 163}, {"referenceID": 9, "context": "Greff et al. (2015) suggest modifying the formulation of LSTMs to make them more GRU-like by using a coupled input-forget gate (CifLSTM), but retaining the output gate of the vanilla LSTM.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "One recently proposed alternative to dropout, zoneout (Krueger et al., 2016), would address this issue with using dropout in GRUs\u2013however, we leave experimenting with this for future work.", "startOffset": 54, "endOffset": 76}, {"referenceID": 4, "context": "1 Model UAS LAS UAS LAS Dyer et al. (2015) 93.", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.", "startOffset": 2, "endOffset": 28}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.56 91.42 87.65 86.21 Kiperwasser & Goldberg (2016) 93.", "startOffset": 2, "endOffset": 82}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.56 91.42 87.65 86.21 Kiperwasser & Goldberg (2016) 93.9 91.9 87.6 86.1 Cheng et al. (2016) 94.", "startOffset": 2, "endOffset": 122}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.56 91.42 87.65 86.21 Kiperwasser & Goldberg (2016) 93.9 91.9 87.6 86.1 Cheng et al. (2016) 94.10 91.49 88.1 85.7 Weiss et al. (2015) 94.", "startOffset": 2, "endOffset": 164}, {"referenceID": 1, "context": "41 - Andor et al. (2016) 94.", "startOffset": 5, "endOffset": 25}, {"referenceID": 19, "context": "As is standard, we omit puncuation from evaluation and use predicted POS tags for the English PTB dataset\u2014generated from the Stanford POS tagger (Toutanova et al., 2003)\u2014and gold POS tags for the Chinese PTB dataset.", "startOffset": 145, "endOffset": 169}, {"referenceID": 14, "context": "Word embeddings for Chinese were generated using Word2Vec (Mikolov et al., 2013) on Chinese Wikipedia.", "startOffset": 58, "endOffset": 80}, {"referenceID": 4, "context": "Here, we compare our model to a number of others in the literature: the models of Dyer et al. (2015) and Ballesteros et al.", "startOffset": 82, "endOffset": 101}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016), which use LSTMs to generate features for a transition-based parser; the models of Weiss et al.", "startOffset": 11, "endOffset": 37}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016), which use LSTMs to generate features for a transition-based parser; the models of Weiss et al. (2015) and Andor et al.", "startOffset": 11, "endOffset": 140}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al.", "startOffset": 11, "endOffset": 163}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al. (2016), which like this work uses BiLSTMs to generate feature embeddings used for an attention-based parser.", "startOffset": 11, "endOffset": 187}], "year": 2016, "abstractText": "While deep learning parsing approaches have proven very successful at finding the structure of sentences, most neural dependency parsers use neural networks only for feature extraction, and then use those features in traditional parsing algorithms. In contrast, this paper builds off recent work using general-purpose neural network components, training an attention mechanism over an LSTM to attend to the head of the phrase. We get state-of-the-art results for standard dependency parsing benchmarks, achieving 95.44% UAS and 93.76% LAS on the PTB dataset, 0.8% and 1.0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model\u2019s performance, such as reducing the value of \u03b22 in the Adam optimization algorithm.", "creator": "LaTeX with hyperref package"}}}