{"id": "1611.01875", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Challenges of Feature Selection for Big Data Analytics", "abstract": "we are surrounded by huge amounts of large - scale high dimensional data. it is desirable to reduce the dimensionality need of data for many learning tasks due itself to the curse of dimensionality. feature selection has shown its effectiveness in mapping many applications by building simpler and more comprehensive model, improving learning performance, and preparing clean, understandable data. recently, some unique characteristics of big data such as data velocity and data variety present challenges to the feature selection problem. in choosing this paper, we fully envision these challenges of feature selection for big data analytics. in particular, we ideally first should give a brief introduction together about feature selection and then detail the challenges of feature selection for structured, heterogeneous and streaming data as well as its scalability and stability issues. at last, simply to facilitate and promote the feature selection research, alternatively we present an open - source feature selection repository ( scikit - feature ), which consists of most of current popular feature selection algorithms.", "histories": [["v1", "Mon, 7 Nov 2016 02:17:43 GMT  (1127kb)", "http://arxiv.org/abs/1611.01875v1", "Special Issue on Big Data, IEEE Intelligent Systems, 2016. arXiv admin note: text overlap witharXiv:1601.07996"]], "COMMENTS": "Special Issue on Big Data, IEEE Intelligent Systems, 2016. arXiv admin note: text overlap witharXiv:1601.07996", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jundong li", "huan liu"], "accepted": false, "id": "1611.01875"}, "pdf": {"name": "1611.01875.pdf", "metadata": {"source": "CRF", "title": "Challenges of Feature Selection for Big Data Analytics", "authors": ["Jundong Li"], "emails": ["jundongl@asu.edu", "huan.liu@asu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n01 87\n5v 1\n[ cs\n.L G\nKeywords. Feature Selection; Big Data; Repository"}, {"heading": "1 A Brief Introduction of Feature Selection", "text": "Massive amounts of high dimensional data are pervasive in multiple different domains, ranging from social media, e-commerce, bioinformatics, health care, transportation to online education. As an example, we show the growth trend of instance numbers and feature numbers in the UCI machine learning repository [1] in Figure 1. As can be observed, both the data sample size and feature numbers are continuously growing over time. When applying data mining and machine learning algorithms on high dimensional data, a critical issue is known as curse of dimensionality. It refers to the phenomenon that data becomes\n\u2217It is a preprint version. The final version is to appear in Special Issue on Big Data, IEEE Intelligent Systems.\nsparser in high dimensional space, adversely affecting algorithms designed for low dimensional space. In addition, the existence of high dimensional features will significantly demand more on the computational and memory storage requirements.\nFeature selection, as a type of dimension reduction technique, has been proven to be effective and efficient in handling high dimensional data [11, 7]. It directly selects a subset of relevant features for the model construction. Since feature selection keeps a subset of original features, one of its major merit is that it well maintains the physical meanings of the original feature sets, and gives better model readability and interpretability. Due to this particular reason, it is more widely applied in many real world applications such as gene analysis and text mining. Feature selection obtains relevant features by removing irrelevant and redundant features. The removal of these irrelevant and redundant features reduces the computational and storage costs without significant loss of\ninformation or negative degradation of the learning performance. Taking Figure 2 as an example, feature f1 is a relevant feature which can separate two classes (clusters) in Figure 2(a); while in Figure 2(b), feature f2 is considered as a redundant feature w.r.t feature f1 since feature f1 already can discriminate two classes (clusters) well; in Figure 2(c), feature f3 is an irrelevant feature as it does not contain useful information to separate two classes (clusters).\nAccording to the availability of class labels, we can categorize feature selection algorithms into supervised and unsupervised methods. Supervised feature selection is usually taken as a preprocessing step for the classification/regression task. It chooses features that can discriminate data instances from different classes or regression targets. Since the label information is known a priori, relevance of a feature is normally assessed by its correlation with class labels. On the other hand, unsupervised feature selection is generally applied for the clustering task. Without class labels to guide feature selection, it evaluates feature importance by some alternative criteria such as data similarity, local discriminative information and data reconstruction error.\nWith regard to search strategies, feature selection algorithms can be divided into wrapper methods, filter methods and embedded methods. Wrapper methods typically use the learning performance of a predefined model to evaluate the feature relevance. Specifically, it repeatedly chooses a subset of features and then evaluates the learning performance with these selected features until\nthe highest learning performance is obtained. Since it scans through the whole search space, it is slow and seldom used in practice. Filter methods, on the other hand, do not rely on any learning algorithms and are therefore more efficient. They exploit the characteristics of data to measure the feature relevance. Usually, they measure the scores of features based on some ranking criteria and then return the top ranked features. Since these methods do not explicitly consider the bias of learning algorithms, the selected features may not be optimal for a particular learning task. Embedded methods provide a trade-off solution between filter and wrapper methods which embed the feature selection with the model learning, thus they inherit the merits of wrapper and filter methods: first, they include the interactions with the learning algorithm; and second, they are far more efficient than the wrapper methods since they do not need to evaluate feature sets iteratively."}, {"heading": "2 Challenges of Feature Selection", "text": "Recently, the popularity of big data presents some challenges for the traditional feature selection task. Meanwhile, some unique characteristics of big data also bring about new opportunities for the feature selection research. In the next few subsections, we will present these challenges of feature selection for big data analytics from the following six aspects. In particular, the challenges of structured features, linked data, multi-source data and multi-view data, streaming data and features are from the perspective of data; while the last two challenges of scalability and stability, are from the performance perspective."}, {"heading": "2.1 Structured Features", "text": "Most of existing feature selection algorithms are designed for generic data and they are based on a strong assumption that features do not have explicit correlations. In other words, they completely ignore the intrinsic structures among features. For example, these feature selection methods may select the same subset of features even though the features are reshuffled [18]. In many real world applications, features exhibit various kinds of structures, e.g., spatial or temporal smoothness, disjoint groups, overlap groups, trees and graphs. When applying feature selection algorithms on the datasets with structured features, it is beneficial to explicitly incorporate this prior knowledge, which may improve post learning tasks such as classification and clustering. Next, we will focus on the most three common feature structures, i.e., group structure, tree structure and graph structure.\nThe first structure features may exhibit is group structure. Examples of group structured features include different frequency bands represented as groups in signal processing [13] and genes with similar functionalities acting as groups in bioinformatics [12]. Therefore, when performing feature selection, it is more appealing to explicitly take into consideration the group structure among features. Figure 3 shows an illustrative example of features with group structures\n(4 groups). In addition to the group structures, features can also form other kinds of structures such as tree structure. For example, in image processing such as face images, different pixels (features) can be represented as a tree, where the root node indicates the whole face, its child nodes can be the different organs, and each specific pixel is considered as a leaf node. In other words, these pixels enjoy a spatial locality structure. Figure 4 shows an example of 8 features with four layers of tree structure. Another motivating example is that genes/proteins may form certain hierarchical tree structures [6].\nFeatures may also form graph structures. For example, in natural language processing, if we take each word as a feature, we have synonyms and antonyms relationships between different words [3]. Moreover, many biological studies show that genes tend to work in groups according to their biological functions, and there are strong dependencies between some genes. Since features show some dependencies, we can model the features by an undirected graph, where nodes represent features and edges among nodes show the pairwise dependencies between features. An illustrative example of 7 features with graph structure is shown in Figure 5."}, {"heading": "2.2 Linked Data", "text": "Linked data becomes ubiquitous in many platforms such as Twitter1 (tweets linked through hyperlinks), social networks in Facebook2 (users connected by friendships) and biological networks (protein interactions). Since linked data are correlated with each other by different types of links, they are distinct from traditional attribute-value data. Figure 6 presents an illustrative example of linked data and its two representations. Figure 6(a) shows 8 linked instances (u1 to u8) while Figure 6(b) is a conventional representation of attribute-value data such that each row corresponds to one instance and each column corresponds to one feature. As mentioned above, linked data provides an extra source of information in the form of links, which can be represented by an adjacency matrix, illustrated in Figure 6(c). The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information."}, {"heading": "2.3 Multi-Source Data and Multi-View Data", "text": "In many data mining and machine learning tasks, we have multiple data sources for the same set of data instances. For example, recent advancement in bioinformatics reveal the existence of some non-coding RNA species in addition to the widely used messenger RNA, these non-coding RNA species functions across a variety of biological processes. The availability of multiple data sources makes it possible to address some problems otherwise unsolvable using a single source, since the multi-faceted representations of data can help depict some intrinsic patterns hidden in a single source of data. For multi-source feature selection,\n1https://twitter.com/ 2https://www.facebook.com/\nwe usually have a target source and other sources complement the selection of features on the target source [19].\nMulti-view sources represent different facets of data instances via different feature spaces. These feature spaces are naturally dependent and also high dimensional, which suggests that feature selection is necessary to prepare these sources for effective data mining tasks such as multi-view clustering. A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations [15, 17]. For example, we would like to select pixel features, tag features, and text features about images in Flickr3 simultaneously. Since multi-view feature selection is designed to select features across multiple views by using their relations, they are naturally different from multi-source feature selection. We illustrate the difference between multi-source feature selection and multi-view feature selection in Figure 7.\n3https://www.flickr.com/"}, {"heading": "2.4 Streaming Data and Features", "text": "In many scenarios, we are faced with a significant amount of data which need to be processed in a real time to gain insights. In the worst cases, the size of data or the features are unknown or even infinite, thus it is not practical to wait until all data instances or features are available to perform feature selection. For streaming data, one motivating example is that in online spam email detection problem, new emails are constantly arriving, it is not easy to employ batch-mode feature selection methods to select relevant feature in a timely manner. Therefore, some feature selection algorithms are proposed to maintain and update a feature subset when new data streams are constantly arriving. The\nprocess of feature selection on data streams is illustrated in Figure 8. In some settings when the streaming data cannot be loaded into the memory, one pass of the data is required. We can only make one pass of the data where the second pass is either unavailable or computational expensive. How to select relevant features timely by one pass of data [5] is still a challenging problem.\nOn an orthogonal setting, feature selection for streaming features also has its practical significance. For example, Twitter produces more than 320 millions of tweets everyday and a large amount of slang words (features) are continuously being generated. These slang words promptly grab users attention and become popular in a short time. Therefore, it is more preferable to perform streaming feature selection to rapidly adapt to the changes [9]. A general framework of streaming feature selection is presented in Figure 9. At each time step, a typical streaming feature selection algorithm first determines whether to accept the most recently arrived feature; if the feature is added to the selected feature set, it then determines whether to discard some existing features from the selected feature set. The process repeats until no new features show up anymore."}, {"heading": "2.5 Scalability", "text": "With the tremendous growth of dataset sizes, the scalability of most current feature selection algorithms may be jeopardized. In many scientific and busi-\nness applications, data are usually measured in terabyte (1TB = 1012 bytes). Normally, datasets in the scale of terabytes cannot be loaded into the memory directly and therefore limits the usability of most feature selection algorithms. Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very largescale datasets [14]. Recently, big data of ultrahigh dimensionality has emerged in many real-world applications such as text mining and information retrieval. Most feature selection algorithms does not scale well on the ultrahigh dimensional data, its efficiency deteriorates quickly or is even computational infeasible. In this case, well-designed feature selection algorithms in linear or sub-linear running time are more preferred."}, {"heading": "2.6 Stability", "text": "The stability of these algorithms is also an important consideration when developing new feature selection algorithms [4]. A motivating example from the field of bioinformatics shows that domain experts would like to see the same set or similar set of genes (features) to be selected each time when they obtain new samples in the small amount of perturbation. Otherwise domain experts would not trust these algorithms when they get quite different sets of features with small data perturbation. It is also found that the underlying characteristics of data may greatly affect the stability of feature selection algorithms and the stability issue may also be data dependent. These factors include the dimensionality of feature, number of data instances, etc. In against with supervised feature selection, stability of unsupervised feature selection algorithms has not be well studied yet. Studying stability for unsupervised feature selection is much more difficult than that of the supervised methods. The reason is that in unsupervised feature selection, we do not have enough prior knowledge about the cluster structure of the data. Thus we are uncertain that if the new data instance, i.e., the perturbation belongs to any existing clusters or will introduce new clusters."}, {"heading": "3 Feature Selection Repository", "text": "To tackle the challenges of feature selection for big data analytics and to promote the feature selection research in this community, we present an open-source feature selection repository - scikit-feature (http://featureselection.asu.edu/). The purpose of this feature selection repository is to collect some widely used feature selection algorithms that have been developed in the feature selection research to serve as a platform for facilitating their application, comparison and joint study. The feature selection repository also effectively assists researchers to achieve more reliable evaluation in the process of developing new feature selection algorithms.\nCurrently, scikit-feature consists of popular feature selection algorithms in the following categories:\n\u2022 Similarity based feature selection\n\u2022 Information theoretical based feature selection\n\u2022 Statistical based feature selection\n\u2022 Sparse learning based feature selection\n\u2022 Wrapper based feature selection\n\u2022 Structural feature selection\n\u2022 Streaming feature selection\nAmong these different categories of feature selection methods, similarity based, information theoretical based, and statistical based methods correspond to the filter methods discussed above. Wrapper based methods and sparse learning based methods correspond to the wrapper methods and embedded methods, respectively. We also include structural features, linked data, multi-view and multi-source data to the category of structural feature selection, and streaming data and features to the streaming feature selection category.\nIn addition, scikit-feature also provides many benchmark feature selection datasets, and evaluation examples on how to evaluate feature selection algorithms via classification or clustering task. The experimental results can be obtained from our repository project website (http://featureselection.asu.edu/datasets.php). For each dataset, we list all applicable feature selection algorithms along with its evaluation on either classification or clustering task. We also provide an interactive tool FeatureMiner [2] to ease the usage of these feature selection algorithms based on the repository."}, {"heading": "Acknowledgements", "text": "This material is, in part, supported by National Science Foundation (NSF) under grant number IIS-1217466."}], "references": [{"title": "Featureminer: A tool for interactive feature selection", "author": ["Kewei Cheng", "Jundong Li", "Huan Liu"], "venue": "In CIKM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Stable feature selection for biomarker discovery", "author": ["Zengyou He", "Weichuan Yu"], "venue": "Computational biology and chemistry,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Unsupervised feature selection on data streams", "author": ["Hao Huang", "Shinjae Yoo", "S Kasiviswanathan"], "venue": "In CIKM, pages 1031\u20131040,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["Rodolphe Jenatton", "Jean-Yves Audibert", "Francis Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Feature selection: A data perspective", "author": ["Jundong Li", "Kewei Cheng", "Suhang Wang", "Fred Morstatter", "Robert P Trevino", "Jiliang Tang", "Huan Liu"], "venue": "arXiv preprint arXiv:1601.07996,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Toward time-evolving feature selection on dynamic networks", "author": ["Jundong Li", "Xia Hu", "Ling Jian", "Huan Liu"], "venue": "In ICDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Unsupervised streaming feature selection in social media", "author": ["Jundong Li", "Xia Hu", "Jiliang Tang", "Huan Liu"], "venue": "In CIKM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Robust unsupervised feature selection on networked data", "author": ["Jundong Li", "Xia Hu", "LiangWu", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Computational methods of feature selection", "author": ["Huan Liu", "Hiroshi Motoda"], "venue": "CRC Press,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Supervised group lasso with applications to microarray data analysis", "author": ["Shuangge Ma", "Xiao Song", "Jian Huang"], "venue": "BMC bioinformatics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Subband correlation and robust speech recognition", "author": ["James McAuley", "Ji Ming", "Darryl Stewart", "Philip Hanna"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Parallel large scale feature selection for logistic regression", "author": ["Sameer Singh", "Jeremy Kubica", "Scott Larsen", "Daria Sorokina"], "venue": "In SDM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised feature selection for multi-view data in social media", "author": ["Jiliang Tang", "Xia Hu", "Huiji Gao", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Unsupervised feature selection for linked social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "In KDD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Multi-view clustering and feature learning via structured sparsity", "author": ["Hua Wang", "Feiping Nie", "Heng Huang"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Sparse methods for biomedical data", "author": ["Jieping Ye", "Jun Liu"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Spectral feature selection for data mining", "author": ["Zheng Alan Zhao", "Huan Liu"], "venue": "CRC Press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "Feature selection, as a type of dimension reduction technique, has been proven to be effective and efficient in handling high dimensional data [11, 7].", "startOffset": 143, "endOffset": 150}, {"referenceID": 4, "context": "Feature selection, as a type of dimension reduction technique, has been proven to be effective and efficient in handling high dimensional data [11, 7].", "startOffset": 143, "endOffset": 150}, {"referenceID": 15, "context": "For example, these feature selection methods may select the same subset of features even though the features are reshuffled [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Examples of group structured features include different frequency bands represented as groups in signal processing [13] and genes with similar functionalities acting as groups in bioinformatics [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Examples of group structured features include different frequency bands represented as groups in signal processing [13] and genes with similar functionalities acting as groups in bioinformatics [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "Another motivating example is that genes/proteins may form certain hierarchical tree structures [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 52, "endOffset": 63}, {"referenceID": 7, "context": "The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 52, "endOffset": 63}, {"referenceID": 13, "context": "The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 52, "endOffset": 63}, {"referenceID": 16, "context": "we usually have a target source and other sources complement the selection of features on the target source [19].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations [15, 17].", "startOffset": 152, "endOffset": 160}, {"referenceID": 14, "context": "A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations [15, 17].", "startOffset": 152, "endOffset": 160}, {"referenceID": 2, "context": "How to select relevant features timely by one pass of data [5] is still a challenging problem.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Therefore, it is more preferable to perform streaming feature selection to rapidly adapt to the changes [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very largescale datasets [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "6 Stability The stability of these algorithms is also an important consideration when developing new feature selection algorithms [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "We also provide an interactive tool FeatureMiner [2] to ease the usage of these feature selection algorithms based on the repository.", "startOffset": 49, "endOffset": 52}], "year": 2016, "abstractText": "We are surrounded by huge amounts of large-scale high dimensional data. It is desirable to reduce the dimensionality of data for many learning tasks due to the curse of dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive model, improving learning performance, and preparing clean, understandable data. Recently, some unique characteristics of big data such as data velocity and data variety present challenges to the feature selection problem. In this paper, we envision these challenges of feature selection for big data analytics. In particular, we first give a brief introduction about feature selection and then detail the challenges of feature selection for structured, heterogeneous and streaming data as well as its scalability and stability issues. At last, to facilitate and promote the feature selection research, we present an open-source feature selection repository (scikit-feature), which consists of most of current popular feature selection algorithms.", "creator": "LaTeX with hyperref package"}}}