{"id": "1611.02683", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "sequence to sequence models are successful tools for supervised computing sequence learning tasks, these such as machine translation. despite their success, these models still require much labeled data and it is unclear how to improve them using unlabeled data, which is much less expensive to obtain. in this paper, we present simple changes that lead to a significant improvement in the accuracy of seq2seq models when the labeled set is small. our method intializes compared the encoder and decoder of the seq2seq model with the trained weights of two language models, and then all weights are jointly fine - tuned with labeled data. an additional language modeling loss can be used to regularize the model during fine - tuning. we apply this method to low - resource tasks in their machine resource translation and abstractive summarization cases and find that it significantly improves the subsequent supervised models. our main finding is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state - of - the - art results on the wmt english $ \\ rightarrow $ german task. our model method obtains an improvement ratio of 1. 3 bleu from the previous best models on both wmt'92 14 and wmt'15 english $ \\ rightarrow $ german. our ablation study shows that pretraining helps seq2seq models in different ways depending on the nature of the task : translation benefits from the improved generalization whereas summarization benefits from integrating the improved optimization.", "histories": [["v1", "Tue, 8 Nov 2016 20:42:26 GMT  (89kb,D)", "http://arxiv.org/abs/1611.02683v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["prajit ramachandran", "peter j liu", "quoc v le"], "accepted": true, "id": "1611.02683"}, "pdf": {"name": "1611.02683.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SEQUENCE LEARNING", "Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "emails": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016). The main weakness of sequence to sequence models, and deep networks in general, lies in the fact that they can easily overfit when the amount of supervised training data is small.\nWhile it has been suggested that unlabeled data can be used to address this weakness, there is little evidence of a simple and general method achieving a significant improvement on important baselines. In this work, we propose a simple and effective technique for using unsupervised pretraining to improve seq2seq models. Our proposal is to initialize both encoder and decoder networks with pretrained language models. More specifically, to train a seq2seq model mapping from a source domain to a target domain, we first train two language models. One language model is trained on an unlabeled corpus of the source domain and the second is trained on an unlabeled corpus of the target domain. The source-side language model is used to initialize the encoder and the target-side language model is used to initialize the decoder. Finally, the entire seq2seq model is fine-tuned with the labeled corpus.\nWe benchmark this method on machine translation for English\u2192German and abstractive summarization in a low-resource setting on CNN and Daily Mail articles. Our ablation study shows that among many other possible choices of using a language model in seq2seq with attention, the above proposal works best. Our study also shows that, for translation, the main gains come from the\n\u2217Work done as an intern on Google Brain.\nar X\niv :1\n61 1.\n02 68\n3v 1\n[ cs\n.C L\n] 8\nN ov\n2 01\n6\nimproved generalization due to the pretrained features, whereas for summarization the gains come from the improved optimization due to pretraining the encoder which has been unrolled for hundreds of timesteps. On both tasks, our proposed method always improves generalization on the test sets. Our main result is that a seq2seq model, with pretraining, exceeds the strongest possible baseline in both neural machine translation and phrase-based machine translation. Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT\u201914 and WMT\u201915 English\u2192German. On abstractive summarization, our method achieves competitive results to the strongest baselines."}, {"heading": "2 UNSUPERVISED PRETRAINING FOR SEQUENCE TO SEQUENCE LEARNING", "text": "In the following section, we will describe our basic unsupervised pretraining procedure for sequence to sequence learning and how to modify sequence to sequence learning to effectively make use of the pretrained weights. We then show several extensions to improve the basic model."}, {"heading": "2.1 BASIC PROCEDURE", "text": "The basic procedure of our approach is to pretrain both the encoder and decoder networks in the sequence to sequence framework with language models, which can be trained on large amounts of unlabeled text data. This can be seen in Figure 1, where the parameters in the shaded boxes are pretrained. In the following we will describe the method in detail by using machine translation as an example application, but the method can be applied to all sequence to sequence learning tasks.\nFirst, two monolingual datasets are collected, one for the source side language Dsrc, and one for the target side language Dtgt. A language model (LM) is trained on each dataset independently, giving an LM trained on the source side corpus Lsrc, and an LM trained on the target side corpus Ltgt. Lsrc and Ltgt can be different sizes. To simplify our explanation we assume that the LMs have one LSTM layer (Hochreiter & Schmidhuber, 1997), though this technique works regardless of the number of layers.\nAfter two language models are trained, a multi-layer seq2seq model M is constructed. The embedding and first LSTM layer of the encoder are initialized with the corresponding trained weights of Lsrc. Likewise, the embedding, first LSTM layer, and softmax of the decoder are initialized with the corresponding trained weights of Ltgt. We call the encoder and decoder parameters that are pretrained \u03b8enc and \u03b8dec respectively. All other LSTM layers are initialized randomly. There is no connection between the first LSTM layer of the encoder and the first LSTM layer of the decoder. In this sense, the first LSTM layer serves as a feature extractor. Finally, during the training of M , all the parameters of the seq2seq model \u03b8M are fine-tuned on the labeled dataset Dlabel.\nAnother approach would be to pretrain the entire LSTM, instead of just the embeddings, first LSTM layer, and softmax. However, this requires pretraining a new model for every architecture change, which can be very costly given that a large language model can take on the order of a week or weeks to train (Jozefowicz et al., 2016). Instead, we find that training just two LMs and using our initialization scheme is sufficient to demonstrate significant gains while being very flexible."}, {"heading": "2.2 IMPROVING THE MODEL", "text": "We also employ three additional methods to further improve the model above. The three methods are: a) Monolingual language modeling losses, b) Residual connections and c) Attention over multiple layers. The three methods are shown in Figure 2 and will be discussed below.\nMonolingual language modeling losses: After the seq2seq model M is initialized with the two LMs, it is fine-tuned with a labeled dataset. However, there is no requirement that the pretrained parameters stay close to their original pretrained values. That is, the seq2seq model can forget the learned information from language modeling and overfit on the labeled set. To combat this problem, we incorporate additional monolingual language modeling losses to optimize M :\nL = Llabel(Dlabel; \u03b8M ) + \u03b1Lsrc(Dsrc; \u03b8src) + \u03b2Ltgt(Dtgt; \u03b8tgt) The additional losses Lsrc and Ltgt regularize the seq2seq model by forcing it to correctly model both the normal seq2seq task and the original monolingual language modeling tasks. Note that Lsrc only involves the pretrained encoder parameters \u03b8src and Ltgt only involves the pretrained decoder parameters \u03b8tgt. This means that the parameters that were pretrained must be useful for both the original language modeling task and the new seq2seq task. In practice, we implemented this joint unsupervised and supervised objective by a round-robin training scheme. First, \u03b8enc and \u03b8dec are optimized with respect to Lsrc and Ltgt and then all parameters \u03b8M are updated with respect to Llabel. In this paper, we used \u03b1 = \u03b2 = 1. We found this regularization gave a large improvement in final test performance, and is an important component of our method.\nThis is also the motivation for why there is no connection between the first LSTM layers of the encoder and decoder. Without a connection, the first LSTM layers just serve as feature extractors. If the additional language modeling costs are added, the first LSTM layers can focus purely on deriving powerful features for both the language modeling and seq2seq task, without needing to spend capacity for communication between the encoder and decoder.\nResidual connections: As described, the input vector to the decoder softmax layer is a random vector because the high level (non-first) layers of the LSTM are randomly initialized. This slows\ndown training and introduces random gradients to the pretrained parameters, reducing the effectiveness of pretraining. One method to avoid this problem is to freeze all pretrained weights at the start of training and only train the randomly initialized weights. Afterwards, all parameters can be finetuned together. We used a simpler solution by introducing a residual connection (He et al., 2016) from the output of the first LSTM layer directly to the input of the softmax. So,\nhsoftmax = h1 + LSTMN (. . . (LSTM2(h1)) . . .)\nwhere LSTMi represents the ith LSTM layer. This residual connection makes the initialized decoder equivalent to Ltgt with random noise added to the input of the softmax. We found this formulation improved performance and decreased the time to convergence. In the experiments we show below, we used a gated residual connection with a bias initialized to a large positive as in Gulcehre et al. (2015), but later experimentation showed that a regular residual connection works just as well.\nAttention over multiple layers: In all our models, we used an attention mechanism (Bahdanau et al., 2015). We experimented with attending over both the top level LSTM states, as is done normally, and the first layer LSTM states of the encoder in order to get low and high level features. Given a query vector qt from the decoder, encoder states from the first layer h11, . . . , h 1 T , and encoder states from the last layer hL1 , . . . , h L T , we compute the attention context vector ct as follows:\n\u03b1i = exp(qt \u00b7 hNi )\u2211T j=1 exp(qt \u00b7 hNj )\nc1t = T\u2211 i=1 \u03b1ih 1 i c N t = T\u2211 i=1 \u03b1ih N i ct = [c 1 t ; c N t ]\nNote that attention weights \u03b1i are only computed once using the top level encoder states. Furthermore, since our pretraining scheme makes very few assumptions about the model structure, we also experimented with passing the attention vector ct as input into the next timestep (Luong et al., 2015b). We do not pass c into the first LSTM layer, because the size of the first LSTM layer is predetermined by Ltgt which only takes in the embedding as input. Instead, c is passed as input to the second LSTM layer by concatenating it with the output of the first LSTM layer.\nWe used all three improvements in our experiments. However, in general we noticed that the benefits of the attention modifications are minor in comparison with the benefits of the additional language modeling objectives and residual connections."}, {"heading": "3 RELATED WORK", "text": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models.\nDai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation. Firat et al. (2016) introduced fine-tuning techniques that gave promising results for zero-resource neural machine translation.\nSennrich et al. (2015b) introduced a data augmentation technique called backtranslation for neural machine translation. In order to increase the size of the labeled dataset, a target\u2192source model is trained with the labeled dataset and used to translate monolingual target data into the source domain. The new synthetic pairs are added to the labeled dataset. Backtranslation yielded state of the art performance. However this technique does not have a natural analogue to other domains that are not\ninvertible, like summarization. We note that their technique is complementary to ours, and may lead to additional gains in machine translation. Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder. Their method demonstrates strong improvements. However, it is sensitive to how the monolingual corpora are picked, has to do an expensive top-k search every step for the autoencoding objective, and shows no improvement when using both target and source monolingual data over using just target monolingual data.\nGulcehre et al. (2015) is the work closest to ours. They combine an LM with an already trained seq2seq model by fine-tuning additional deep output layers. However, their method produces small improvements over the supervised baseline (\u2264 0.5 BLEU). We suspect that their method does not produce significant gains because (i) the models are trained independently of each other and are not fine-tuned (ii) the LM is combined with the seq2seq model after the last layer, wasting the benefit of the low level LM features, and (iii) only using the LM on the decoder side. Venugopalan et al. (2016) addressed (i) but still experienced minor improvements. Using pretrained GloVe (Pennington et al., 2014) vectors for embeddings was more important for their model."}, {"heading": "4 EXPERIMENTS", "text": "In the following section, we apply our approach to two important tasks in seq2seq learning: machine translation and abstractive summarization. On each task, we compare against the previous best systems. We also perform ablation experiments to understand the behavior of each component of our method."}, {"heading": "4.1 MACHINE TRANSLATION", "text": "Dataset and Evaluation: For machine translation, we evaluate our method on the WMT English\u2192German task (Bojar et al., 2015). We used the WMT 14 training dataset, which is slightly smaller than the WMT 15 dataset. Because the dataset has some noisy examples, we used a language detection system to filter the training examples. Sentences pairs where the either source was not English or the target was not German were thrown away. This resulted in around 4 million training examples. Following Sennrich et al. (2015b), we use subword units (Sennrich et al., 2015a) with 89500 merge operations, giving a vocabulary size around 90000. The validation set is the concatenated newstest2012 and newstest2013, and our test sets are newstest2014 and newstest2015. Evaluation on the validation set was with case-sensitive BLEU (Papineni et al., 2002) on tokenized text using multi-bleu.perl. Evaluation on the test sets was with case-sensitive BLEU on detokenized text using mteval-v13a.pl. The monolingual training datasets are the News Crawl English and German corpora, each of which has more than a billion tokens.\nExperimental settings: The language models were trained in the same fashion as (Jozefowicz et al., 2016) We used a 1 layer 4096 dimensional LSTM with the hidden state projected down to 1024 units (Sak et al., 2014) and trained for one week on 32 Tesla K40 GPUs. Our seq2seq model was a 3 layer model, where the second and third layers each have 1000 hidden units. The monolingual objectives, residual connection, and the modified attention were all used. We used the Adam optimizer (Kingma & Ba, 2015) and train with asynchronous SGD on 16 GPUs for speed. We used a learning rate of 5e-5 which is multiplied by 0.8 every 50K steps after an initial 400K steps, gradient clipping with norm 5.0 (Pascanu et al., 2013), and dropout of 0.2 on non-recurrent connections (Zaremba et al., 2014). We used early stopping on validation set perplexity. A beam size of 10 was used for decoding. Our ensemble is constructed with the 5 best performing models on the validation set, which are trained with different hyperparameters.\nResults: Table 1 shows the results of our method in comparison with other baselines. Our method achieves a new state of the art for single model performance on both newstest2014 and newstest2015. In fact, our best single model outperforms the previous state of the art ensemble of 4 models. Our ensemble of 5 models matches or exceeds the previous best ensemble of 12 models.\nAblation study: In order to better understand the effects of pretraining, we conducted an ablation study by modifying the pretraining scheme. Figure 3 shows the drop in validation BLEU of various\nablations compared with the full model. The full model uses LMs trained with monolingual data to initialize the encoder and decoder, in addition to the language modeling objective. In the following, we interpret the findings of the study. Note that some findings are specific to the translation task.\nPretraining the decoder is better than pretraining the encoder: Only pretraining the encoder leads to a 1.6 BLEU point drop while only pretraining the decoder leads to a 1.0 BLEU point drop. This suggests that it is more important to pretrain the decoder than the encoder for translation. One possible explanation is that the decoder has a \u2019tougher job\u2019 than the encoder because it has to both understand the semantic meaning of the source sentence and learn to generate the target sentence. In contrast, the encoder only needs to capture the semantic meaning of the source side. Pretraining the decoder is especially helpful for generation because it is initialized as a language model that can already generate in the target language.\nPretrain as much as possible because the benefits compound: Na\u0131\u0308vely, one might expect the benefits of pretraining to be additive. That is, the gains of pretraining the entire model is sum of the gains of only pretraining the encoder and only pretraining the decoder. Concretely, given the drops of no pretraining at all (\u22122.0) and only pretraining the encoder (\u22121.6), the additive estimate of the drop of only pretraining the decoder side is \u22122.0 \u2212 (\u22121.6) = \u22120.4. However the actual drop is \u22121.0 which is a much larger drop than the additive estimate. This suggests that the benefits of pretraining various components are not independent but actually compound on top of each other.\nPretraining the softmax is important: Pretraining only the embeddings and first LSTM layer gives a large drop of 1.6 BLEU points. This suggests that pretraining the decoder softmax is important. Since the gradients from the non-pretrained softmax will be random at the start of training, the pretrained embeddings and LSTM weights will be overwritten with noise which will neutralize the benefits of pretraining. The performance of this ablation may be improved if the pretrained weights were frozen for some iterations before fine-tuning the entire model jointly. The size of the softmax may also influence the importance of pretraining the softmax, but we did not explore this.\nThe language modeling objective is a strong regularizer: The drop in BLEU points of pretraining the entire model and not using the LM objective is as bad as using the LM objective without pretraining. This suggests that the LM objective acts as a strong regularizer that prevents the weights from drifting too far away from their pretrained state. This ensures that the high quality features that are extracted by the original language model continue to be used. Furthermore, the results imply that the LM objective improves performance regardless of pretraining or not. Future work can use this method to improve their performance even if no pretraining is done.\nPretraining on a lot of unlabeled data is essential for learning to extract powerful features: Surprisingly, if the model is initialized with LMs that are trained on the source part and target part of the parallel corpus, the drop in performance is as large as not pretraining at all. This suggests that the primary benefits of pretraining for the translation task come not from the better parameter initialization scheme, but from having learned to extract powerful features. Because the parallel corpus is an order of magnitude smaller than monolingual corpora, the LMs are not able to learn powerful features. Furthermore, the experiments show that the domain of the unlabeled corpora is not as important as the size. For this, we trained two LMs on the English and German Wikipedia. Wikipedia is a different domain than the parallel corpus, which consists mainly of translations from the European Parliment and online news articles (Bojar et al., 2015). The number of tokens in the Wikipedia corpora is comparable to that of the News Crawl monolingual corpora. Initializing with the Wikipedia LMs leads to only a minor drop in performance. This implies that pretraining is effective as long as enough unlabeled data is available.\nVery low resource experiments: We also benchmarked the pretraining technique on a very low resource machine translation setup. We first took a random subset of 100K examples from the entire English\u2192German parallel corpus, which is 2.5% of the entire corpus. We trained a randomly initialized baseline and a pretrained model on the subset. For the pretrained model, fixing the embeddings and softmax to their pretrained weights gave the best performance. This is an important advantage of pretraining because the embedding and softmax usually make up the majority of parameters in the model. Freezing them can help prevent severe overfitting. Table 2 shows that the pretraining technique gives a significant improvement of 5.3 BLEU points over the baseline."}, {"heading": "System BLEU", "text": "We were also interested in tracking the performance of pretraining as a function of dataset size. For this, we trained a a model with and without pretraining on random subsets of the English\u2192German corpus. Both models use the additional LM objective. The results are summarized in Figure 4. When a 100% of the labeled data is used, the gap between the pretrained and no pretrain model is 2.0 BLEU points. However, that gap grows when less data is available. When trained on 20% of the labeled data, the gap becomes 3.8 BLEU points. This demonstrates that the pretrained models degrade less as the labeled dataset becomes smaller."}, {"heading": "4.2 ABSTRACTIVE SUMMARIZATION", "text": "Dataset and Evaluation: For a low-resource abstractive summarization task, we use the CNN/Daily Mail corpus from (Hermann et al., 2015). Following Nallapati et al. (2016), we modify the data collection scripts to restore the bullet point summaries. The task is to predict the bullet\npoint summaries from a news article. The dataset has fewer than 300K document-summary pairs. To compare against Nallapati et al. (2016), we used the anonymized corpus. However, for our ablation study, we used the non-anonymized corpus.1 We evaluate our system using full length ROUGE (Lin, 2004). For the anonymized corpus in particular, we considered each highlight as a separate sentence following Nallapati et al. (2016). In this setting, we used the English Gigaword corpus (Napoles et al., 2012) as our larger, unlabeled \u201cmonolingual\u201d corpus, although all data used in this task is in English.\nExperimental settings: We use subword units (Sennrich et al., 2015a) with 31500 merges, resulting in a vocabulary size of about 32000. We use up to the first 600 tokens of the document and predict the entire summary. Only one language model is trained and it is used to initialize both the encoder and decoder, since the source and target languages are the same. However, the encoder and decoder are not tied. The LM is a one-layer LSTM of size 1024 trained in a similar fashion to Jozefowicz et al. (2016). For the seq2seq model, we use the same settings as the machine translation experiments. The only differences are that we use a 2 layer model with the second layer having 1024 hidden units, and that the learning rate is multiplied by 0.8 every 30K steps after an initial 100K steps.\nResults: Table 3 summarizes our results on the anonymized version of the corpus. Our pretrained model is only able to match the previous baseline seq2seq of Nallapati et al. (2016). However, our model is a unidirectional LSTM while they use a bidirectional LSTM. They also use a longer context of 800 tokens, whereas we used a context of 600 tokens due to GPU memory issues. Furthermore, they use pretrained word2vec (Mikolov et al., 2013) vectors to initialize their word embeddings. As we show in our ablation study, just pretraining the embeddings itself gives a large improvement.\nAblation study: We performed an ablation study similar to the one performed on the machine translation model. The results are reported in Figure 5. Here we report the drops on ROUGE-1, ROUGE-2, and ROUGE-L on the non-anonymized validation set.\n1We encourage future researchers to use the non-anonymized version because it is a more realistic summarization setting with a larger vocabulary. Our numbers on the non-anonymized test set are 35.56 ROUGE-1, 14.60 ROUGE-2, and 25.08 ROUGE-L. We did not consider highlights as separate sentences.\nPretraining improves optimization: There is a large drop in performance without any pretraining at all. However, in contrast with the machine translation model, it is more beneficial to only pretrain the encoder than only the decoder of the summarization model. One interpretation is that because the encoder is unrolled for hundreds of timesteps, the pretrained encoder weights enable the gradient to flow much further back in time than randomly initialized weights. This also explains why pretraining on the parallel corpus is as good as pretraining on the unlabeled corpus, which was not observed in the translation model. The pretrained weights improve optimization, which enables a better model to be trained. Interestingly, only pretraining the embeddings gives a significant improvement over no pretraining at all. Randomly initialized encoder embeddings may be difficult to learn because the long unrolling of the encoder leads to vanishing gradient (Pascanu et al., 2013). Pretraining the embeddings eases the burden of optimization and gives the model a good foundation to learn with.\nThe language modeling objective is a strong regularizer: A model without the LM objective has a nontrivial drop in ROUGE scores. Since there are only a few hundred thousand examples in the dataset, the model can easily overfit. The LM objective combats this by forcing the encoder to maintain its ability to generate English. However, unlike the translation task, pretraining is significantly more important than the LM objective. This is probably because the LM objective helps primarily with regularization while pretraining helps with optimization, which is more important in the long document summarization setting.\nHuman evaluation: As ROUGE may not be able to capture the quality of summarization, we also performed a small qualitative study to understand the human impression of the summaries produced by different models. We took 200 random documents and compared the performance of a pretrained and non-pretrained system. The document, gold summary, and the two system outputs were presented to a human evaluator who was asked to rate each system output on a scale of 1-5 with 5 being the best score. The system outputs were presented in random order and the evaluator did not know the identity of either output. The evaluator noted if there were repetitive phrases or sentences in either system outputs. Unwanted repetition was also noticed by Nallapati et al. (2016).\nTable 4 and 5 show the results of the study. In both cases, the pretrained system outperforms the system without pretraining in a statistically significant manner. The better optimization enabled by pretraining improves the generated summaries and decreases unwanted repetition in the output."}, {"heading": "5 CONCLUSION", "text": "We showed that pretraining sequence to sequence models is a simple but effective technique that can aid in both generalization and optimization. Our scheme involves pretraining two language models in the source and target domain, and initializing the embeddings, first LSTM layers, and softmax of a sequence to sequence model with the weights of the language models. Using an extra language modeling objective during fine-tuning helps regularize the model. In our experiments, we showed that pretraining can improve results in tasks with hundreds of thousands or millions of examples. A key advantage of this technique is that it is flexible and can be applied to a large variety of tasks, including conversation modeling for chatbots or question answering. We hope that future work uses pretraining as a reliable way to improve performance on sequence to sequence tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank George Dahl, Andrew Dai, Laurent Dinh, Stephan Gouws, Geoffrey Hinton, Rafal Jozefowicz, Pooya Khorrami, Phillip Louis, Ramesh Nallapati, Arvind Neelakantan, Xin Pan, Abi See, Rico Sennrich, Luke Vilnis, Yuan Yu and the Google Brain team for their help with the project."}, {"heading": "Source Document", "text": ""}, {"heading": "SELECTED SUMMARIZATION OUTPUTS", "text": ""}, {"heading": "APPENDIX", "text": ""}, {"heading": "Source Document", "text": ""}, {"heading": "Source Document", "text": ""}, {"heading": "Source Document", "text": ""}, {"heading": "Source", "text": ""}, {"heading": "SELECTED ENGLISH\u2192GERMAN OUTPUTS", "text": ""}, {"heading": "Source", "text": ""}, {"heading": "Source", "text": ""}, {"heading": "Source", "text": ""}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio"], "venue": "In ICASSP,", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Findings of the 2015 workshop on statistical machine translation", "author": ["Ond\u0159ej Bojar", "Rajen Chatterjee", "Christian Federmann", "Barry Haddow", "Matthias Huck", "Chris Hokamp", "Philipp Koehn", "Varvara Logacheva", "Christof Monz", "Matteo Negri", "Matt Post", "Carolina Scarton", "Lucia Specia", "Marco Turchi"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1606.04596,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Phone recognition with the mean-covariance restricted boltzmann machine", "author": ["George Dahl", "Marc\u2019Aurelio Ranzato", "Abdel rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Dahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le"], "venue": "In NIPS", "citeRegEx": "Dai and Le.,? \\Q2015\\E", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman-Vural", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1606.04164,", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1503.03535,", "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Montreal neural machine translation systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "WikiReading: A novel large-scale language understanding task over wikipedia", "author": ["Alexandre Lacoste", "Andrew Fandrianto", "Daniel Hewlett", "David Berthelot", "Illia Polosukhin", "Jay Han", "Llion Jones", "Matthew Kelcey"], "venue": null, "citeRegEx": "Lacoste et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lacoste et al\\.", "year": 2016}, {"title": "ROUGE: a package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of the Workshop on Text Summarization Branches Out (WAS", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In ICLR,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Sequence-to-sequence RNNs for text summarization", "author": ["Ramesh Nallapati", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1602.06023,", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S. Huang"], "venue": "arXiv preprint arXiv:1412.6597,", "citeRegEx": "Paine et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paine et al\\.", "year": 2014}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W. Senior", "Fran\u00e7oise Beaufays"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1508.07909,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improving LSTM-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "venue": "arXiv preprint arXiv:1604.01729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong"], "venue": "In EMNLP,", "citeRegEx": "Zhang and Zong.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "In EMNLP,", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 38, "context": "Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al.", "startOffset": 38, "endOffset": 110}, {"referenceID": 6, "context": "Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al.", "startOffset": 38, "endOffset": 110}, {"referenceID": 38, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 0, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 4, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 1, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 37, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 27, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 43, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 17, "context": "However, this requires pretraining a new model for every architecture change, which can be very costly given that a large language model can take on the order of a week or weeks to train (Jozefowicz et al., 2016).", "startOffset": 187, "endOffset": 212}, {"referenceID": 12, "context": "We used a simpler solution by introducing a residual connection (He et al., 2016) from the output of the first LSTM layer directly to the input of the softmax.", "startOffset": 64, "endOffset": 81}, {"referenceID": 11, "context": "In the experiments we show below, we used a gated residual connection with a bias initialized to a large positive as in Gulcehre et al. (2015), but later experimentation showed that a regular residual connection works just as well.", "startOffset": 120, "endOffset": 143}, {"referenceID": 0, "context": "Attention over multiple layers: In all our models, we used an attention mechanism (Bahdanau et al., 2015).", "startOffset": 82, "endOffset": 105}, {"referenceID": 14, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 2, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 7, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 9, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 20, "context": "However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016).", "startOffset": 132, "endOffset": 221}, {"referenceID": 33, "context": "However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016).", "startOffset": 132, "endOffset": 221}, {"referenceID": 12, "context": "However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016).", "startOffset": 132, "endOffset": 221}, {"referenceID": 29, "context": "Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014).", "startOffset": 107, "endOffset": 127}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting.", "startOffset": 8, "endOffset": 510}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains.", "startOffset": 8, "endOffset": 837}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance.", "startOffset": 8, "endOffset": 1017}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length.", "startOffset": 8, "endOffset": 1138}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation.", "startOffset": 8, "endOffset": 1252}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation. Firat et al. (2016) introduced fine-tuning techniques that gave promising results for zero-resource neural machine translation.", "startOffset": 8, "endOffset": 1401}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation. Firat et al. (2016) introduced fine-tuning techniques that gave promising results for zero-resource neural machine translation. Sennrich et al. (2015b) introduced a data augmentation technique called backtranslation for neural machine translation.", "startOffset": 8, "endOffset": 1533}, {"referenceID": 32, "context": "Using pretrained GloVe (Pennington et al., 2014) vectors for embeddings was more important for their model.", "startOffset": 23, "endOffset": 48}, {"referenceID": 5, "context": "Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder. Their method demonstrates strong improvements. However, it is sensitive to how the monolingual corpora are picked, has to do an expensive top-k search every step for the autoencoding objective, and shows no improvement when using both target and source monolingual data over using just target monolingual data. Gulcehre et al. (2015) is the work closest to ours.", "startOffset": 0, "endOffset": 525}, {"referenceID": 5, "context": "Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder. Their method demonstrates strong improvements. However, it is sensitive to how the monolingual corpora are picked, has to do an expensive top-k search every step for the autoencoding objective, and shows no improvement when using both target and source monolingual data over using just target monolingual data. Gulcehre et al. (2015) is the work closest to ours. They combine an LM with an already trained seq2seq model by fine-tuning additional deep output layers. However, their method produces small improvements over the supervised baseline (\u2264 0.5 BLEU). We suspect that their method does not produce significant gains because (i) the models are trained independently of each other and are not fine-tuned (ii) the LM is combined with the seq2seq model after the last layer, wasting the benefit of the low level LM features, and (iii) only using the LM on the decoder side. Venugopalan et al. (2016) addressed (i) but still experienced minor improvements.", "startOffset": 0, "endOffset": 1094}, {"referenceID": 3, "context": "Dataset and Evaluation: For machine translation, we evaluate our method on the WMT English\u2192German task (Bojar et al., 2015).", "startOffset": 103, "endOffset": 123}, {"referenceID": 30, "context": "Evaluation on the validation set was with case-sensitive BLEU (Papineni et al., 2002) on tokenized text using multi-bleu.", "startOffset": 62, "endOffset": 85}, {"referenceID": 3, "context": "Dataset and Evaluation: For machine translation, we evaluate our method on the WMT English\u2192German task (Bojar et al., 2015). We used the WMT 14 training dataset, which is slightly smaller than the WMT 15 dataset. Because the dataset has some noisy examples, we used a language detection system to filter the training examples. Sentences pairs where the either source was not English or the target was not German were thrown away. This resulted in around 4 million training examples. Following Sennrich et al. (2015b), we use subword units (Sennrich et al.", "startOffset": 104, "endOffset": 517}, {"referenceID": 17, "context": "Experimental settings: The language models were trained in the same fashion as (Jozefowicz et al., 2016) We used a 1 layer 4096 dimensional LSTM with the hidden state projected down to 1024 units (Sak et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 34, "context": ", 2016) We used a 1 layer 4096 dimensional LSTM with the hidden state projected down to 1024 units (Sak et al., 2014) and trained for one week on 32 Tesla K40 GPUs.", "startOffset": 99, "endOffset": 117}, {"referenceID": 31, "context": "0 (Pascanu et al., 2013), and dropout of 0.", "startOffset": 2, "endOffset": 24}, {"referenceID": 44, "context": "2 on non-recurrent connections (Zaremba et al., 2014).", "startOffset": 31, "endOffset": 53}, {"referenceID": 16, "context": "BLEU System ensemble? newstest2014 newstest2015 Supervised Neural MT (Jean et al., 2015) single 22.", "startOffset": 69, "endOffset": 88}, {"referenceID": 3, "context": "Wikipedia is a different domain than the parallel corpus, which consists mainly of translations from the European Parliment and online news articles (Bojar et al., 2015).", "startOffset": 149, "endOffset": 169}, {"referenceID": 13, "context": "Dataset and Evaluation: For a low-resource abstractive summarization task, we use the CNN/Daily Mail corpus from (Hermann et al., 2015).", "startOffset": 113, "endOffset": 135}, {"referenceID": 13, "context": "Dataset and Evaluation: For a low-resource abstractive summarization task, we use the CNN/Daily Mail corpus from (Hermann et al., 2015). Following Nallapati et al. (2016), we modify the data collection scripts to restore the bullet point summaries.", "startOffset": 114, "endOffset": 171}, {"referenceID": 22, "context": "1 We evaluate our system using full length ROUGE (Lin, 2004).", "startOffset": 49, "endOffset": 60}, {"referenceID": 28, "context": "In this setting, we used the English Gigaword corpus (Napoles et al., 2012) as our larger, unlabeled \u201cmonolingual\u201d corpus, although all data used in this task is in English.", "startOffset": 53, "endOffset": 75}, {"referenceID": 26, "context": "To compare against Nallapati et al. (2016), we used the anonymized corpus.", "startOffset": 19, "endOffset": 43}, {"referenceID": 22, "context": "1 We evaluate our system using full length ROUGE (Lin, 2004). For the anonymized corpus in particular, we considered each highlight as a separate sentence following Nallapati et al. (2016). In this setting, we used the English Gigaword corpus (Napoles et al.", "startOffset": 50, "endOffset": 189}, {"referenceID": 17, "context": "The LM is a one-layer LSTM of size 1024 trained in a similar fashion to Jozefowicz et al. (2016). For the seq2seq model, we use the same settings as the machine translation experiments.", "startOffset": 72, "endOffset": 97}, {"referenceID": 25, "context": "Furthermore, they use pretrained word2vec (Mikolov et al., 2013) vectors to initialize their word embeddings.", "startOffset": 42, "endOffset": 64}, {"referenceID": 26, "context": "Our pretrained model is only able to match the previous baseline seq2seq of Nallapati et al. (2016). However, our model is a unidirectional LSTM while they use a bidirectional LSTM.", "startOffset": 76, "endOffset": 100}, {"referenceID": 27, "context": "System ROUGE-1 ROUGE-2 ROUGE-L Seq2seq + pretrained embeddings (Nallapati et al., 2016) 32.", "startOffset": 63, "endOffset": 87}, {"referenceID": 27, "context": "47 + temporal attention (Nallapati et al., 2016) 35.", "startOffset": 24, "endOffset": 48}, {"referenceID": 31, "context": "Randomly initialized encoder embeddings may be difficult to learn because the long unrolling of the encoder leads to vanishing gradient (Pascanu et al., 2013).", "startOffset": 136, "endOffset": 158}, {"referenceID": 27, "context": "Unwanted repetition was also noticed by Nallapati et al. (2016). Table 4 and 5 show the results of the study.", "startOffset": 40, "endOffset": 64}], "year": 2016, "abstractText": "Sequence to sequence models are successful tools for supervised sequence learning tasks, such as machine translation. Despite their success, these models still require much labeled data and it is unclear how to improve them using unlabeled data, which is much less expensive to obtain. In this paper, we present simple changes that lead to a significant improvement in the accuracy of seq2seq models when the labeled set is small. Our method intializes the encoder and decoder of the seq2seq model with the trained weights of two language models, and then all weights are jointly fine-tuned with labeled data. An additional language modeling loss can be used to regularize the model during fine-tuning. We apply this method to low-resource tasks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main finding is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English\u2192German task. Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT\u201914 and WMT\u201915 English\u2192German. Our ablation study shows that pretraining helps seq2seq models in different ways depending on the nature of the task: translation benefits from the improved generalization whereas summarization benefits from the improved optimization.", "creator": "LaTeX with hyperref package"}}}