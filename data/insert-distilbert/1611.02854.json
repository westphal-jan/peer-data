{"id": "1611.02854", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Lie-Access Neural Turing Machines", "abstract": "recent work since has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for relational algorithmic learning ( graves et al. 2014 ; weston et al. 2014 ). these models utilize differentiable versions representative of traditional discrete memory - access structures ( random access, stacks, tapes ) to provide the variable - length storage necessary for computational tasks. in this work, we propose an alternative model, lie - access memory, that is explicitly designed for the neural setting. in this paradigm, memory is accessed using a continuous head in a key - space manifold. the head is moved via lie group actions, such as shifts or rotations, generated by a mechanical controller, and soft memory access is performed by considering the distance to keys associated completely with each memory. we argue that lie groups provide a natural generalization of discrete memory structures, such as turing machines, as they provide inverse and identity operators while maintain differentiability. to experiment with this approach, we implement several simplified lie - access neural turing machine ( ml lantm ) with different lie groups. we ultimately find that this approach is able to perform it well on a range of fundamental algorithmic cognitive tasks.", "histories": [["v1", "Wed, 9 Nov 2016 08:51:54 GMT  (870kb,D)", "http://arxiv.org/abs/1611.02854v1", "Submitted to ICLR. Rewrite and improvement ofthis https URL"], ["v2", "Sun, 5 Mar 2017 21:03:22 GMT  (1046kb,D)", "http://arxiv.org/abs/1611.02854v2", "Published at ICLR. Rewrite and improvement ofarXiv:1602.08671"]], "COMMENTS": "Submitted to ICLR. Rewrite and improvement ofthis https URL", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["greg yang", "alexander m rush"], "accepted": true, "id": "1611.02854"}, "pdf": {"name": "1611.02854.pdf", "metadata": {"source": "CRF", "title": "Lie-Access Neural Turing Machines", "authors": ["Greg Yang", "Alexander M. Rush"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Pioneering work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has demonstrated that explicit external memories can be effectively utilized within deep neural networks, without sacrificing the end-to-end nature of these models. Recently researchers have studied many variations of external neural memories. Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and others. This work has the potential to extend deep networks beyond the limitations of fixed-length encodings such as standard recurrent neural networks (RNNs).\nA shared theme in many of these works (and earlier exploration of neural memory) is to re-frame traditional memory access paradigms to be continuous and possibly differentiable to allow for simple backpropagation. In MemNNs, traditional random-access memory is replaced with a ranking approach that finds the most likely memory. In the work of Grefenstette et al. (2015), classical stack-based memory is replaced by a soft differentiable stack data-structure. In NTMs, sequential local-access memory is simulated by an explicit tape data structure.\nWe propose to instead specify a class of memory access techniques specifically designed for the neural context, that do not attempt to directly model known discrete access methods. In particular, inspired by discrete Turing machines, we argue that a neural memory should maintain the following properties: (A) maintaining differentiability for end-to-end training and (B) provide a robust relative indexing scheme (perhaps in addition to random-access).\nLie groups, mathematical groups with differentiable group operations, provide a natural structure for neural memory access. By definition, their differentiability satisfies the concerns of Criterion\nar X\niv :1\n61 1.\n02 85\n4v 1\n[ cs\n.N E\n] 9\nN ov\n2 01\n6\nA. Additionally the group axioms provide identity, invertibility, and associativity, all of which are desirable properties for a relative indexing scheme, and all of which are satisfied by standard Turing machines. Notably though, simple group properties like invertibility are not satisfied by neural Turing machines, differentiable neural computers, or even by simple soft-tape machines.\nTo experiment with this approach, we implement a neural Turing machine with an LSTM controller and several versions of Lie-access memory, which we call Lie-access neural Turing machines (LANTM). The details of these models are exhibited in Section 4. Our main experimental results are presented in Section 5. The LANTM model is able to learn non-trivial algorithmic tasks such as copying and permutating sequences with a similar accuracy of more traditional memory-based approaches, and significantly better than fixed memory LSTM models. The memory structures and key transformation learned by the model resemble interesting continuous space representations of traditional discrete memory data structures."}, {"heading": "2 Background: Recurrent Neural Networks with Memory", "text": "We focus particularly on the case of an RNN controller of an abstract memory network. Formally, an RNN is a differentiable function RNN : X \u00d7 H \u2192 H, where X is an arbitrary input space and H the space of internal states. On input (x(1), . . . , x(T )) \u2208 X T and with initial state h(0) \u2208 H, the RNN transitions into states h(1), . . . , h(T ),\nh(t) := RNN(x(t), h(t\u22121)).\nThese states can then be used for downstream tasks, e.g. sequence prediction (y(1), . . . , y(T )), based on an additional transformation and prediction layer y(t) = F (h(t)) such as a linear-layer followed by a softmax. RNNs can be trained end-to-end by backpropagation-through-time (BPTT) (Werbos, 1990). In practice, we use a particular variant of RNN called the long short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997). LSTM\u2019s hidden state consists of two variables (c(t), h(t)), where h(t) is also the output to the external world; we however use the above notation for simplicity.\nIn models with external memories, the RNN can serve as the controller (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015). This means that (1) the entire system carries state over time from both the RNN and the external memory, and (2) the RNN controller collects readings from and computes additional instructions to the external memory. The system is now described by the recurrence:\nh(t) := RNN([x(t); \u03c1(t\u22121)], h(t\u22121)),\n\u03a3(t), \u03c1(t) := RW(\u03a3(t\u22121), h(t)),\nwhere \u03a3 is the memory state, and \u03c1(t) is the memory being read. As above, output prediction can be made using a standard final layer y(t) := F (h(t)). Writing occurs in the mutation of \u03a3 at each time step. Throughout this work, \u03a3 will take the form {. . . (ki, vi, si) . . .} where vi \u2208 Rm are the memories, ki is an arbitrary key, and si \u2208 R is a memory strength, described below. In order for the model to be trainable, the memory function RW must also be differentiable. Several forms of differentiable memory have been proposed in the literature. We begin by describing two simple forms: (neural) random-access memory and (neural) tape-based memory. For this section, we focus on the read step and assume \u03a3 is fixed.\nRandom-Access Memory Random-access memory consists of using a now standard attentionmechanism or soft MemNN to read a memory based on a controller. This description is similar to the work of Miller et al. (2016). The controller hidden state is used to output a random-access pointer q\u0303(h(t)) that determines a weighting of memory vectors via dot products with the corresponding keys. This weighting in turn determines the read values via convex combination (attention).\nwi := si exp \u3008q\u0303, ki\u3009\u2211 j sj exp\u3008q\u0303, kj\u3009 for all i, \u03c1 := \u2211 i wivi\nThe final read memory can be seen as determined by how \u201cclose\u201d the read pointer was to each of the keys.\nTape-Based Memory Neural memories can also keep around a state indicating a current read position. Following notation from Turing machines, we call this state the head, q. In the simplest case the recurrence now has the form,\n\u03a3\u2032, q\u2032, \u03c1 = RW(\u03a3, q, h),\nalthough we might incorporate multiple heads.\nIn the simplest case of soft tape-based memory (a naive version of the much more complicated neural Turing machine), the keys ki indicate positions along a tape with ki = i. The head q is a probability distribution over locations of the memory i. It determines the read value by directly specifying the weight, i.e. wi = qi . The controller can only \u201cshift\u201d the head by outputting a kernel K = K(h) = (K\u22121,K0,K+1) in the probability simplex \u2206 2 and applying convolution.\nq\u2032 := q \u2217K, i.e. q\u2032j = qj\u22121K+1 + qjK0 + qj+1K\u22121, wi := q \u2032 i for all i, \u03c1 := \u2211 i wivi.\nWe can view this as the soft version of a single-step discrete Turing machine where the kernel can softly shift the \u201chead\u201d of the machine one to the left, one to the right, or remain in the same location."}, {"heading": "3 Lie Groups for Memory", "text": "Let us revisit now the classical discrete Turing machine and the movement of its head over a tape. In particular, between reads, the head can move any integer number of steps left or right. Moving a + b steps and then c steps eventually puts the head at the same location as moving a steps and then b + c steps \u2014 i.e. the head movement is associative. In addition, the machine should be able to reverse a head shift, for example, in a stack simulation algorithm, going from push to pop \u2014 i.e. each head movement should also have a corresponding inverse. Finally, the head should also be allowed to stay put, for example, to read a single data item and use it for multiple time points.\nThese movements correspond directly to group actions: the possible head movements should be associative, and contain inverse and identity elements. This group acts on the set of possible head locations. In a TM, the set of Z-valued head movement acts on the set of locations on the Z-indexed infinite tape. By our reasoning above, if a Turing machine is to store data contents at points in a general space K (instead of an infinite Z-indexed tape), then its head movements should form a group and act on K via group actions. For a neural memory system, we desire most things to be differentiable (or almost everywhere differentiable). The notion of \u201cdifferentiable groups\u201d is well-studied in mathematics, where they are known as Lie groups, and \u201cdifferentiable group actions\u201d are correspondingly called Lie group actions. In our case, using Lie group actions as generalized head movements on a general space (more accurately, manifolds) would most importantly mean that we can take derivatives of these movements and perform the usual backpropagation algorithm."}, {"heading": "4 Lie-Access Neural Turing Machines", "text": "These properties motivate us to propose, Lie access as an alternative formalism to popular softversions of neural memory systems, such as probabilistic tapes, which surprisingly do not satisfy invertibility and often do not provide an identity.1 Our Lie-access memory will consist of a set of\n1The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and explained in section 2 does not in general have inverses. Indeed, the authors reported problems with the soft head losing \u201csharpness\u201d over time, which they dealt with by \u201csharpening coefficients.\u201d In the followup\npoints in a manifold K. We replace the discrete head, with continuous head, q \u2208 K. The head will move based on a set of Lie group actions a \u2208 A generated by the controller. To read memories, we will rely on a distance measure in this space, d : K\u00d7K \u2192 R\u22650.2 Together these properties describe a general class of possible neural memory architectures.\nFormally a Lie-access neural Turing machine (LANTM) computes the following function,\n\u03a3\u2032, q\u2032, q\u2032(w), \u03c1 := RW(\u03a3, q, q(w), h)\nwhere q, q(w) \u2208 K are resp. read and write heads, and \u03a3 is the memory itself. We implement \u03a3 as above as a weighted dictionary \u03a3 = {. . . , (ki, vi, si), . . .} where ki \u2208 K is a key, vi \u2208 Rm is the value, and si \u2208 [0, 1] is a strength value, such that when si = 0, the entry (ki, vi) can be ignored."}, {"heading": "4.1 Addressing Procedure", "text": "A(h) \u00b7 q = (\u03b1, \u03b2) + (x, y) = (x+ \u03b1, y + \u03b2). \u2022 The rotation group SO(3) acting on the\nsphere S2 = {v \u2208 R3 : \u2016v\u2016 = 1}. Each rotation can be described by its axis \u03be and angle \u03b8. An action (\u03be, \u03b8)\u00b7q is just the appropriate rotation of the point q, and is given by Rodrigues\u2019 rotation formula\nA(h) \u00b7 q = (\u03be, \u03b8) \u00b7 q = q cos \u03b8 + (k \u00d7 q) sin \u03b8 + k\u3008k, q\u3009(1\u2212 cos \u03b8)."}, {"heading": "4.2 Reading Memories", "text": "Recall that memories are stored in \u03a3, each with a key, ki, memory vector, vi, and strength, si. Memories are read using a soft weighting scheme based on their key address. The weighting is defined\nwork, Graves et al. (2016) utilize a temporal memory link matrix for actions. They note, \u201cthe operation Lw smoothly shifts the focus forwards to the locations written ... whereas L>w shifts the focus backwards\u201d but do not enforce this as a true inverse. They also explicitly do not include an identity, noting \u201cSelf-links are excluded (the diagonal of the link matrix is always 0)\u201d.\n2 This metric should satisfy a compatibility relation with the Lie group action When points x, y \u2208 X are simultaneously moved by the same Lie group action v, their distance should stay the same (One possible mathematical formalization is thatX should be Riemannian manifold and the Lie group should be a subgroup of X\u2019s isometry group.): d(vx, vy) = d(x, y). This condition ensures that if the machine writes a sequence of data along a \u201cstraight line\u201d at points x, vx, v2x, . . . , vkx, then it can read the same sequence by emitting a read location y close to x and then follow the \u201cv-trail\u201d y, vy, v2y, . . . , vky.\nby a weight vector w, and the read vector is a linear combination over memories: \u03c1\u2032 := \u2211\ni wivi. While there are many possible weighting schemes, we use one based on the distance of each memory from the head in key-space assuming a metric d on K. One method is to compute weight w of each memory using the polynomial law for all i as,\nwi := sid(q \u2032, ki) \u22122\u2211\nj sjd(q \u2032, kj)\u22122\nwith the convention that it takes the limit value when q \u2192 ki for some i. We call this method of converting a read key to a set of weighting InvNorm for short.\nAn alternative is to use the more common exponential law/annealed softmax weight scheme, which computes the weights wi as\nwi := si exp(\u2212d(q\u2032, ki)2/T )\u2211 j sj exp(\u2212d(q\u2032, kj)2/T ) ,\nwhere T is a temperature emitted by the controller at time t that represents the certainty of its reading, i.e. higher T creates more uniform w."}, {"heading": "4.3 Writing Memories", "text": "The writing procedure is similar to reading. The LANTM maintains a separate write head q(w) that moves analogously to the read head, i.e. with action function a(w) and updated value q \u2032 (w) . At each call to RW, a new memory is automatically appended to \u03a3 with k = q\u2032(w). The corresponding v and s are created by MLP\u2019s v(h) \u2208 Rm and s(h) \u2208 [0, 1] taking h as input. After writing, the new memory set is,\n\u03a3\u2032 := \u03a3 \u222a {(q\u2032(w), v(h), s(h))}.\nNo explicit erase mechanism is provided, but to erase a memory (k, v, s), the controller may just write (k,\u2212v, s)."}, {"heading": "4.4 Interpolations", "text": "In practice we can combine this relative addressing procedure with direct random-access by outputting an interpolation. Write t(h) \u2208 [0, 1] for the interpolation gate and q\u0303(h) \u2208 K for our proposed random-access layer. For key space manifolds K like Rn, there\u2019s a well defined straight-line interpolation between two points, so we can set q\u2032 := A \u00b7 (t\u00d7 q + (1\u2212 t)\u00d7 q\u0303) where we have omitted the implied dependence on h. For other manifolds like the spheres Sn that have well-behaved projection functions \u03c0 : Rn \u2192 Sn, we can just project the straight-line interpolation to the sphere: q\u2032 := A \u00b7 \u03c0(t\u00d7 q + (1\u2212 t)\u00d7 q\u0303). In the case of a sphere Sn, \u03c0 is just L2-normalization. 3\nAdditionally, we also extend the model to interpolate the last action A(t\u22121) with a candidate action A(h) via a gate r(h) \u2208 [0, 1] to produce the final action A(t) (replacing A above). This is so that the controller may move a head in \u201ca straight line\u201d by just saturating both t and r. For example, for the translation group Rn, we have straight-line interpolation as above, A(t) := r\u00d7A+ (1\u2212 r)\u00d7A(t\u22121). For the rotation group SO(n), each rotation is represented by its axis \u03be \u2208 Sn\u22121 and angle \u03b8 \u2208 (\u2212\u03c0, \u03c0], and we just interpolate each separately \u03be(t) := \u03c0(r\u00d7\u03be+(1\u2212r)\u00d7\u03be(t\u22121)) and \u03b8(t) := r\u00d7\u03b8+(1\u2212r)\u00d7\u03b8(t\u22121). where \u03c0 is L2-normalization. 4\n3Technically, in the sphere case, dom\u03c0 = Rd \u2212 {0}. But in practice one almost never gets 0 from a straight-line interpolation, so computationally this makes little difference.\n4 There is, in fact, a canonical way to interpolate the most common Lie groups, including all of the groups mentioned above, based on the exponential map and the Baker-Campbell-Hausdorff formula (Lee,"}, {"heading": "5 Experiments", "text": "We experiment with this new approach to memory on a set of algorithmic learning tasks. We are particularly interested in: (a) how Lie-access memory can trained, (b) whether it can learn be effectively utilized for algorithmic learning, and (c) what internal structures the model learns compared to systems based directly on discrete memory. In particular Lie access is not equipped with an explicit stack or tape, so it needs to learn continuous patterns that capture these properties.\nSetup. Our experiments utilize an LSTM controller in a version of the encoder-decoder setup (Sutskever et al., 2014), i.e. an encoding input pass followed by decoding output pass. The encoder reads and writes memories at each step; the decoder only reads memories. The encoder is given \u3008s\u3009, followed by an the input sequence, and then \u3008/s\u3009 to terminate input. In the decoder, the system is not re-fed its output or the correct symbol, i.e. we do not use teacher forcing. The decoder must correctly emit an end-of-output symbol \u3008/e\u3009 to terminate. Models and Baselines. We implement three main baseline models including: (a) a standard LSTM encoder-decoder, without explicit external memory, (b) a random access memory network, RAM using the key-value formulation as described the background, roughly analogous to an attentionbased encoder-decoder, (c) an interpolation of a RAM/Tape-based memory network as described in the background, i.e. a highly simplified version of a true NTM. Our models include four versions of Lie-access memory. The main model, LANTM, has an LSTM controller, with a shift group A = R2 acting additively on K = R2. We also consider a model SLANTM with spherical memory, utilizing a rotation group A = SO(3) acting on keys in the sphere K = S2. For both of the models, we experiment with indexing using InvNorm (default) and with an annealed softmax.5\nModel Setup. For all tasks, the LSTM baseline has 1 to 4 layers, each with 256 cells. Each of the other models has a single-layer, 50-cell LSTM controller, with memory width (i.e. the size of each memory vector) 20. Other parameters such as learning rate, decay, and intialization are found through grid search. Further hyperparameter details are give in the appendix.\nTasks. Our experiments are on a series of algorithm tasks shown in Table 1a. The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein. Additionally we also include two harder tasks repeat copy and priority sort. In repeat copy each model must repeat a sequence of length 20, N times. In priority sort, each item of the input sequence is given a priority, and the model must output them in priority order. The models are trained with random 32K examples and tested on 3.2K random examples.\nPrediction is performed via argmax at each step; training is done by minimizing negative log likelihood with RMSProp. For each task, we train each model with the same number of samples. In particular it is not the case that we train till convergence as was done in e.g. Grefenstette et al. (2015). To evaluate the performance of the models, we compute the fraction of characters correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine/ coarse following Grefenstette et al. (2015). We score the models on randomly generated in-sample 1x data and out-of-sample 2x data, with sequence lengths 2k (or repeat number 2N in the case of repeat copy) to test the generalization of the system.\nResults. Main results comparing the different memory systems and read computations on a series of tasks are shown in Table 1b. Consistent with previous work the fixed-memory LSTM system is able to do reasonably well on the 1x samples, but fails consistently when required to generalize to the 2x samples, unable to solve any 2x problem correctly. The RAM (attention-based) and the\n2012), but the details are outside the scope of this paper and the computational cost, while acceptable in control theory settings, is too hefty for us. Interested readers are referred to Shingel (2009) and Marthinsen (1999).\n5 Note that the read weight calculation of a SLANTM with softmax is essentially the same as the RAM model: For head q\u2032, exp(\u2212d(q\u2032, ki)2/T ) = exp(\u2212\u2016q\u2032\u2212ki\u20162/T ) = exp(\u2212(2\u22122\u3008q\u2032, ki\u3009)/T ), where the last equality comes from \u2016q\u2032\u2016 = \u2016ki\u2016 = 1. Therefore the weights wi = si exp(\u2212d(q \u2032,ki) 2/T )\u2211\nj sj exp(\u2212d(q\u2032,kj)2/T ) = si exp(\u22122\u3008q \u2032,ki\u3009/T )\u2211 j sj exp(\u22122\u3008q\u2032,kj\u3009/T ) ,\nwhich is the RAM weighting scheme.\nTask Input Output Size k |V| 1 - Copy a1a2a3 \u00b7 \u00b7 \u00b7 ak a1a2a3 \u00b7 \u00b7 \u00b7 ak [2, 64] 128 2 - Reverse a1a2a3 \u00b7 \u00b7 \u00b7 ak akak\u22121ak\u22122 \u00b7 \u00b7 \u00b7 a1 [2, 64] 128 3 - Bigram Flip a1a2a3a4 \u00b7 \u00b7 \u00b7 a2k\u22121a2k a2a1a4a3 \u00b7 \u00b7 \u00b7 a2ka2k\u22121 [1, 16] 128 4 - Double a1a2 \u00b7 \u00b7 \u00b7 ak 2\u00d7 |ak \u00b7 \u00b7 \u00b7 a1| [2, 40] 10 5 - Interleaved Add a1a2a3a4 \u00b7 \u00b7 \u00b7 a2k\u22121a2k |a2ka2k\u22122 \u00b7 \u00b7 \u00b7 a2|+ |a2k\u22121a2k\u22123 \u00b7 \u00b7 \u00b7 a1| [2, 16] 10 6 - Odd First a1a2a3a4 \u00b7 \u00b7 \u00b7 a2k\u22121a2k a1a3 \u00b7 \u00b7 \u00b7 a2k\u22121a2a4 \u00b7 \u00b7 \u00b7 a2k [1, 16] 128 7 - Repeat Copy Na1 \u00b7 \u00b7 \u00b7 a20 a1 \u00b7 \u00b7 \u00b7 a20 \u00b7 \u00b7 \u00b7 a1 \u00b7 \u00b7 \u00b7 a20 (N times) N \u2208 [1, 5] 128 8 - Priority Sort 5a52a29a9 \u00b7 \u00b7 \u00b7 a1a2a3 \u00b7 \u00b7 \u00b7 ak [2, 10] 128\n(a) Task descriptions and parameters. |ak \u00b7 \u00b7 \u00b7 a1| means the decimal number repesented by decimal digits ak \u00b7 \u00b7 \u00b7 a1. Arithmetic tasks have all numbers formatted with the least significant digits on the left and with zero padding. The Double task takes an integer x \u2208 [0, 10k) padded to k digits and outputs 2x in k + 1 digits, zero padded to k + 1 digits. The Interleaved Add task takes two integers x, y \u2208 [0, 10k) padded to k digits and interleaved, forming a length 2k input sequence and outputs x + y zero padded to k + 1 digits. The last two tasks use numbers in unary format: N is the shorthand for a length N sequence of a special symbol @, encoding N in unary, e.g. 3 = @@@.\nBase Memory Lie LSTM RAM RAM/Tape LANTM LANTM-s SLANTM SLANTM-s\n1x 2x 1x 2x 1x 2x 1x 2x 1x 2x 1x 2x 1x 2x\n1 74/50 22/0 ? 60/0 99/97 40/0 ? F ? F ? F ? 99/94 2 94/77 30/0 ? 53/3 ? 53/0 ? 99/99 ? 80/55 ? 93/45 ? 96/55 3 86/80 28/0 ? 71/9 ? 65/5 ? F ? 80/55 ? 82/19 ? 83/24 4 98/85 43/0 ? 72/9 ? 69/1 ? F ? F ? 94/73 ? 99/92 5 99/98 56/0 ? 58/7 ? 77/19 ? 99/99 58/31 25/0 ? 99/99 ? 99/93 6 85/67 29/0 ? 40/4 ? 64/13 ? 66/33 99/85 87/18 ? 45/4 97/60 84/12 7 54/31 29/0 ? 99/10 ? 95/40 84/50 48/0 99/92 98/5 99/84 98/15 ? 99/77 8 ? 43/0 ? 73/11 ? 66/18 ? 97/72 ? 97/74 ? 93/60 ? 97/76\n(b) Main results. Numbers represent the accuracy percentages on the fine/coarse evaluations of the tasks. Symbol ? indicates exact 100% accuracy (Fine scores above 99.5 are not rounded up). Baselines are described in the paper. 1x scores are obtained from test sequences of same lengths k (or N in repeat copy) as used in training. 2x scores are tested on sequence lengths 2k (or 2N in repeat copy) for generalization. For example, copy task 2x testing involves lengths 65 to 128. LANTM and SLANTM use InvNorm while LANTM-s and SLANTM-s use softmax weighting scheme. Best 2x fine and coarse scores are bolded.\nRAM/tape hybrid are much stronger baselines, succeeding completely on 1x samples, but performing less well on the 2x generalization. With regard to 2x generalization, RAM does slightly better than RAM/tape on copy, reverse, bigram Flip, and double tasks, but RAM/tape does much better than RAM on the rest \u2014 interleaved add, odd first, repeat copy, and priority sort \u2014 which can be seen as more complex tasks than the previous cohort.\nThe last four columns illustrate the performance of the LANTM models. We found the InvNorm LANTM model to be the most effective, achieving near perfect accuracy on the first five tasks. InvNorm seems to give improvement here over the more common softmax approach to memory access. We also compare results with the sperical SLANTM model. A spherical memory system is a bit atypical, but these results demonstrate that different Lie groups can easily be substituted in, and trained. Interestingly for SLANTM, the softmax achieves better generalization. SLANTM is particularly effective on task 7 (repeat copy), getting the best generalization accuracies.\nQualitative. We did further qualitative analysis of the different Lie-access techniques to see how the models were learning the underlying tasks. Figure 2 shows two diagrams of the LANTM model of the tasks of priority sort and repeat copy. Figure 3 shows two diagrams of the SLANTM model for the same two tasks. They demonstrate that the models indeed learn interesting algorithms."}, {"heading": "6 Conclusion", "text": "This paper introduces Lie-access memory as an alternative neural memory access paradigm, and explored several different implementations of this approach. LANTMs follow similar axioms as discrete Turing machines while providing differentiability. Experiments show that simple models can learn algorithmic tasks. Internally these models naturally learn equivalence of standard data structures like stack and cyclic lists. In future work we hope to experiment with more groups and to scale these methods to more difficult reasoning tasks."}, {"heading": "A Experimental details", "text": "We obtain our results by performing a grid search over the grid specified in Table A.1 and also over seeds 1 to 3, and take the best scores. We bound the norm of the LANTM head shifts by 1, whereas we try both bounding and not bounding the angle of rotation in our grid for SLANTM. We initialize the Lie access models to favor Lie access over random access through the interpolation mechanism discussed in section 4.4.\nThe RAM model read mechanism is as discussed in section 2, and writing is done by appending new (k, v, s) tuples to the memory \u03a3. The only addition to this model in RAM/tape is that left and right keys are now computed using shifted convolution with the read weights:\nkL := \u2211 i wi+1ki\nkR := \u2211 i wi\u22121kR\nand these keys kL and kR are available (along with the random access key output by the controller) to the controller on the next turn to select from via interpolation."}], "references": [{"title": "Neural Turing Machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "[cs],", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": "memory. Nature,", "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "[cs],", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "[cs],", "citeRegEx": "Joulin and Mikolov.,? \\Q2015\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Neural GPUs Learn Algorithms", "author": ["ukasz Kaiser", "Ilya Sutskever"], "venue": "[cs],", "citeRegEx": "Kaiser and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2015}, {"title": "Grid Long Short-Term Memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "[cs],", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Richard Socher"], "venue": "[cs],", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Neural Random-Access Machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "[cs],", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Introduction to Smooth Manifolds", "author": ["John Lee"], "venue": "Number 218 in Graduate Texts in Mathematics. Springer, 2 edition,", "citeRegEx": "Lee.,? \\Q2012\\E", "shortCiteRegEx": "Lee.", "year": 2012}, {"title": "Interpolation in Lie Groups", "author": ["A. Marthinsen"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Marthinsen.,? \\Q1999\\E", "shortCiteRegEx": "Marthinsen.", "year": 1999}, {"title": "Key-value memory networks for directly reading", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "documents. CoRR,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Interpolation in special orthogonal groups", "author": ["Tatiana Shingel"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Shingel.,? \\Q2009\\E", "shortCiteRegEx": "Shingel.", "year": 2009}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "[cs],", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "[cs],", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Memory Networks. arXiv:1410.3916 [cs, stat", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "URL http://arxiv.org/abs/1410.3916", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Reinforcement Learning Neural Turing Machines - Revised", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "[cs],", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al., 2014; Weston et al., 2014).", "startOffset": 160, "endOffset": 202}, {"referenceID": 16, "context": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al., 2014; Weston et al., 2014).", "startOffset": 160, "endOffset": 202}, {"referenceID": 0, "context": "Pioneering work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al.", "startOffset": 49, "endOffset": 76}, {"referenceID": 16, "context": ", 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has demonstrated that explicit external memories can be effectively utilized within deep neural networks, without sacrificing the end-to-end nature of these models.", "startOffset": 43, "endOffset": 64}, {"referenceID": 16, "context": "Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.", "startOffset": 57, "endOffset": 123}, {"referenceID": 13, "context": "Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.", "startOffset": 57, "endOffset": 123}, {"referenceID": 7, "context": "Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.", "startOffset": 57, "endOffset": 123}, {"referenceID": 0, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 6, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 8, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 2, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 6, "context": ", 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and others.", "startOffset": 53, "endOffset": 80}, {"referenceID": 0, "context": "Pioneering work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has demonstrated that explicit external memories can be effectively utilized within deep neural networks, without sacrificing the end-to-end nature of these models. Recently researchers have studied many variations of external neural memories. Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and others. This work has the potential to extend deep networks beyond the limitations of fixed-length encodings such as standard recurrent neural networks (RNNs). A shared theme in many of these works (and earlier exploration of neural memory) is to re-frame traditional memory access paradigms to be continuous and possibly differentiable to allow for simple backpropagation. In MemNNs, traditional random-access memory is replaced with a ranking approach that finds the most likely memory. In the work of Grefenstette et al. (2015), classical stack-based memory is replaced by a soft differentiable stack data-structure.", "startOffset": 50, "endOffset": 1275}, {"referenceID": 15, "context": "RNNs can be trained end-to-end by backpropagation-through-time (BPTT) (Werbos, 1990).", "startOffset": 70, "endOffset": 84}, {"referenceID": 0, "context": "In models with external memories, the RNN can serve as the controller (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015).", "startOffset": 70, "endOffset": 145}, {"referenceID": 2, "context": "In models with external memories, the RNN can serve as the controller (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015).", "startOffset": 70, "endOffset": 145}, {"referenceID": 11, "context": "This description is similar to the work of Miller et al. (2016). The controller hidden state is used to output a random-access pointer q\u0303(h) that determines a weighting of memory vectors via dot products with the corresponding keys.", "startOffset": 43, "endOffset": 64}, {"referenceID": 0, "context": "Our Lie-access memory will consist of a set of The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and explained in section 2 does not in general have inverses.", "startOffset": 117, "endOffset": 138}, {"referenceID": 0, "context": "work, Graves et al. (2016) utilize a temporal memory link matrix for actions.", "startOffset": 6, "endOffset": 27}, {"referenceID": 14, "context": "Our experiments utilize an LSTM controller in a version of the encoder-decoder setup (Sutskever et al., 2014), i.", "startOffset": 85, "endOffset": 109}, {"referenceID": 2, "context": "The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein.", "startOffset": 60, "endOffset": 87}, {"referenceID": 2, "context": "The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein. Additionally we also include two harder tasks repeat copy and priority sort. In repeat copy each model must repeat a sequence of length 20, N times. In priority sort, each item of the input sequence is given a priority, and the model must output them in priority order. The models are trained with random 32K examples and tested on 3.2K random examples. Prediction is performed via argmax at each step; training is done by minimizing negative log likelihood with RMSProp. For each task, we train each model with the same number of samples. In particular it is not the case that we train till convergence as was done in e.g. Grefenstette et al. (2015). To evaluate the performance of the models, we compute the fraction of characters correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine/ coarse following Grefenstette et al.", "startOffset": 60, "endOffset": 804}, {"referenceID": 2, "context": "The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein. Additionally we also include two harder tasks repeat copy and priority sort. In repeat copy each model must repeat a sequence of length 20, N times. In priority sort, each item of the input sequence is given a priority, and the model must output them in priority order. The models are trained with random 32K examples and tested on 3.2K random examples. Prediction is performed via argmax at each step; training is done by minimizing negative log likelihood with RMSProp. For each task, we train each model with the same number of samples. In particular it is not the case that we train till convergence as was done in e.g. Grefenstette et al. (2015). To evaluate the performance of the models, we compute the fraction of characters correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine/ coarse following Grefenstette et al. (2015). We score the models on randomly generated in-sample 1x data and out-of-sample 2x data, with sequence lengths 2k (or repeat number 2N in the case of repeat copy) to test the generalization of the system.", "startOffset": 60, "endOffset": 1040}, {"referenceID": 11, "context": "Interested readers are referred to Shingel (2009) and Marthinsen (1999).", "startOffset": 35, "endOffset": 50}, {"referenceID": 10, "context": "Interested readers are referred to Shingel (2009) and Marthinsen (1999). 5 Note that the read weight calculation of a SLANTM with softmax is essentially the same as the RAM model: For head q\u2032, exp(\u2212d(q\u2032, ki)/T ) = exp(\u2212\u2016q\u2212ki\u2016/T ) = exp(\u2212(2\u22122\u3008q\u2032, ki\u3009)/T ), where the last equality comes from \u2016q\u2032\u2016 = \u2016ki\u2016 = 1.", "startOffset": 54, "endOffset": 72}], "year": 2017, "abstractText": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al., 2014; Weston et al., 2014). These models utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the variable-length storage necessary for computational tasks. In this work, we propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and soft memory access is performed by considering the distance to keys associated with each memory. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintain differentiability. To experiment with this approach, we implement several simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.", "creator": "LaTeX with hyperref package"}}}