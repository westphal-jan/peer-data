{"id": "1611.02879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks", "abstract": "in this work, we propose a training algorithm for an audio - visual automatic speech recognition ( av - asr ) system models using deep recurrent neural network ( rnn ). using first, we train each a deep rnn acoustic inference model with a connectionist inverse temporal temporal classification ( ctc ) objective function. the frame labels obtained from the traditional acoustic model are then explicitly used to perform a non - linear dimensionality reduction of the visual features matrix using a novel deep bottleneck network. audio modeling and visual features are fused and used to train a fusion rnn. the use of bottleneck features for visual modality helps the model ensembles to converge properly during training. our system is evaluated routinely on grid corpus. our results show generally that presence of visual modality gives significant improvement in character error rate ( cer ) at various levels of noise even when actually the model rat is simply trained without noisy data. we also provide a partial comparison of two fusion methods : feature fusion and decision fusion.", "histories": [["v1", "Wed, 9 Nov 2016 10:24:52 GMT  (311kb,D)", "http://arxiv.org/abs/1611.02879v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["abhinav thanda", "shankar m venkatesan"], "accepted": false, "id": "1611.02879"}, "pdf": {"name": "1611.02879.pdf", "metadata": {"source": "CRF", "title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks", "authors": ["Abhinav Thanda", "Shankar M Venkatesan"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Audio-visual speech recognition, connectionist temporal classification, recurrent neural network"}, {"heading": "1 Introduction", "text": "Audio-visual automatic speech recognition (AV-ASR) is a case of multi-modal analysis in which two modalities (audio and visual) complement each other to recognize speech. Incorporating visual features, such as speaker\u2019s lip movements and facial expressions, into automatic speech recognition (ASR) systems has been shown to improve their performances especially under noisy conditions. To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2]. More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.\nEnd-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM. The RNN trained with CTC directly learns a mapping between audio feature frames and character/phoneme sequences. This method eliminates the need for intermediate\n1 Version (Aug 2016) accepted in 4th International Workshop on Multimodal pattern recognition of social signals in human computer interaction(MPRSS 2016), a satellite event of the International Conference on Pattern Recognition (ICPR 2016)\nar X\niv :1\n61 1.\n02 87\n9v 1\n[ cs\n.C V\n] 9\nN ov\n2 01\n6\nGMM/HMM training thereby simplifying the training procedure. To our knowledge, so far AV-ASR systems based on RNN trained with CTC have not been explored.\nIn this work, we design and evaluate an audio-visual ASR (AV-ASR) system using deep recurrent neural network (RNN) and CTC objective function. The design of an AV-ASR system includes the tasks of visual feature engineering, and audio-visual information fusion. Figure 1 shows the AV-ASR pipeline at test time. This work mainly deals with the visual feature extraction and processing steps and training protocol for the fusion model. Proper visual features are important especially in the case of RNNs as RNNs are difficult to train. Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24]. We employ a similar idea to improve the discriminatory power of video features. We show that this helps the RNN to converge properly when compared with raw features. Finally, we compare the performances of feature fusion and decision fusion methods.\nThe paper is organized as follows: Section 2 presents the prior work on AVASR. Bi-directional RNN and its training using CTC objective function are discussed in Section 3. Section 4 describes the feature extraction steps for audio and visual modalities. In section 5 different fusion models are explained. Section 6 explains the training protocols and experimental results. Finally, we summarize our work in 7."}, {"heading": "2 Related Work", "text": "The differences between various AV-ASR systems lie chiefly in the methods employed for visual feature extraction and audio-visual information fusion. Visual feature extraction methods can be of 3 types[21] : 1. Appearance based features where each pixel in the mouth region of the speaker (ROI) is considered to be informative. Usually a transformation such as DCT or PCA is applied to the ROI to reduce the dimensions. Additional feature processing such as mean normalization, intra-frame and inter-frame LDA may be applied [13][21]. 2. Shape based features utilize the geometric features such as height, width and area of the lip region or build a statistical model of the lip contours whose parameters are used as features. 3. Combination of appearance and shape based features.\nFusion methods can be broadly divided into two types[21][14]: 1. Feature fusion 2. Decision fusion. Feature fusion models perform a low level integration of audio and visual features and this involves a single model which is trained on the fused features. Feature fusion may include a simple concatenation of features or feature weighting and is usually followed by a dimensionality reduction transformation like LDA. Decision fusion is applied in cases where the output classes for the two modalities are same. Various decision fusion methods based on variants of HMMs have been proposed[4][2]. In Multistream HMM the emission probability of a state of audio-visual system is obtained by a linear combination of loglikelihoods of individual streams for that state. The parameters of HMMs for individual streams can be estimated separately or jointly. While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams. For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14]\nApplication of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR. In [12], Deep Belief Networks(DBN) are explored. In [18] the authors train separate networks for audio and visual inputs and fuse the final layers of two networks, and then build a third DNN with the fused features. In addition, [18] presents a new DNN architecture with a bilinear soft-max layer which further improves the performance. In [20] a deep de-noising auto-encoder is used to learn noise robust speech features. The auto-encoder is trained with MFCC features of noisy speech as input and reconstructs clean features. The outputs of final layer of the auto-encoder are used as audio features. A CNN is trained with images from the mouth region as input and phoneme labels as output. The final layers of the two networks are then combined to train a multi-stream HMM."}, {"heading": "3 Sequence Labeling Using RNN", "text": "The following notations are adopted in this paper. For an utterance u of length Tu, O u a = (O u a,1, O u a,2, ..., O u a,Tu) and O u v = (O u v,1, O u v,2, ..., O u v,Tu) denote the observation sequences of audio and visual frames where Oa,t \u2208 Rda and Ov,t \u2208 Rdv . We assume equal frame rates for audio and visual inputs which is ensured in experiments by means of interpolation. Ouav = (O u av,1, O u av,2, ..., O u av,Tu) where O u\nav,t = [O u a,t, O u\nv,t] \u2208 Rdav where dav = da + dv denotes the concatenated features at time t for utterance u. The corresponding label sequence is given by l = (l1, l2, ..., lSu) where Su \u2264 Tu and li \u2208 L where L is the set of English letters and an additional element representing a space. For ease of representation, we drop the utterance index u. All the models described in this paper are character based."}, {"heading": "3.1 Bi-directional RNN", "text": "RNNs are a class of neural networks used to map sequences to sequences. This is possible because of the feedback connections between hidden nodes. In a bi-\ndirectional RNN, the hidden layer has two components each corresponding to forward(past) and backward(future) connections. For a given input sequence O = (O1, O2, ..., OT ), the output of the network is calculated as follows: forward pass through forward component of the hidden layer at a given instant t is given by\nh f t = g(W f hoOt + W f hhh f t\u22121 + b f h) (1)\nwhere Wfho is the input-to-hidden weights for forward component, W f hh corresponds to hidden-to-hidden weights between forward components, and b f\nh is the forward component bias. g is a non-linearity depending on the choice of the hidden layer unit. Similarly, forward pass through the backward component of the hidden layer is given by\nh b\nt = g(W b hoOt + W b hhh\nb t\u22121 + b b h) (2)\nwhere Wbho, W b hh, b\nb h are the corresponding parameters for the backward com-\nponent. The input to next layer is the concatenated vector [hft ,h b t ]. In a deep RNN multiple such bidirectional hidden layers are stacked. RNNs are trained using Back-Propagation Through Time (BPTT) algorithm. The training algorithm suffers from vanishing gradients problem which is overcome by using a special unit in hidden layer called the Long Short Term Memory(LSTM)[11][6]."}, {"heading": "3.2 Connectionist Temporal Classification", "text": "DNNs used in ASR systems are frame-level classifiers i.e., each frame of the input sequence is requires a class label in order for the DNN to be trained. The framelevel labels are usually HMM states, obtained by first training a GMM/HMM model and then by forced alignment of input sequences to the HMM states. CTC objective function[7][8] obviates the need for such alignments as it enables the network to learn over all possible alignments.\nLet the input sequence be O = (O1, O2, ..., OT ) and a corresponding label sequence l = (l1, l2, ..., lS) where S \u2264 T . The RNN employs a soft-max output layer containing one node for each element in L\u2032 where L\u2032 = L\u222a{\u03c6}. The number of output units is |L\u2032| = |L| + 1. The additional symbol \u03c6 represents a blank label meaning that the network has not produced an output for that input frame. The additional blank label at the output allows us to define an alignment \u03c0 of length T containing elements of L\u2032. For example, (A\u03c6\u03c6M\u03c6), (\u03c6A\u03c6\u03c6M) are both alignments of length 5 for the label sequence AM . Accordingly, a many to one map B : L\u2032T 7\u2212\u2192 L\u2264T can be defined which generates the label sequence from an alignment.\nAssuming that the posterior probabilities obtained at soft-max layer, at each instant are independent we get\nP (\u03c0|O) = T\u220f t=1 P (kt|Ot) (3)\nwhere k \u2208 L\u2032 and\nP (kt|Ot) = exp(ykt )\n\u03a3k\u2032 exp(yk \u2032 t )\n(4)\nwhere ykt is the input to node k of the soft-max layer at time t The likelihood of the label sequence given an observation sequence can be calculated by summing (3) over all possible alignments.\nP (l|O) = \u2211\n\u03c0\u2208B\u22121(l)\nP (\u03c0|O) (5)\nThe goal is to maximize the log-likelihood logP (l|O) estimation of a label sequence given an observation sequence. Equation 5 is computationally intractable since the number of alignments increases exponentially with the number of labels. For efficient computation of (5), forward-backward algorithm is used."}, {"heading": "4 Feature Extraction", "text": ""}, {"heading": "4.1 Audio Features", "text": "The sampling rate of audio data is converted to 16kHz. For each frame of speech signal of 25ms duration, filter-bank features of 40 dimensions are extracted. The filter-bank features are mean normalized and \u2206 and \u2206\u2206 features are appended. The final 120 dimensional features are used as audio features."}, {"heading": "4.2 Visual Features", "text": "The video frame rate is increased to match the rate of audio frames through interpolation. For AV-ASR, the ROI for visual features is the region surrounding the speaker\u2019s mouth. Each frame is converted to gray scale and face detection is performed using Viola-Jones algorithm. The 64x64 lip region is extracted by detecting 68 landmark points[15] on the speakers face, and cropping the ROI surrounding speakers mouth and chin. 100 dimensional DCT features are extracted from the ROI.\nAfter several experiments of training with DCT features, we found that RNN training either exploded or converged poorly. In order improve the discriminatory power of the visual features, we perform non-linear dimensionality reduction of the features using a deep bottleneck network. Bottleneck features are obtained by training a neural network in which one of the hidden layers has relatively small dimensions. The DNN is trained using cross-entropy cost function with character labels as output. The frame-level character labels required for DNN training are obtained by first training an acoustic model (RNNa) and then obtaining the outputs from the final soft-max layer of RNNa.\nThe DNN configuration is given by dim\u2212 1024\u2212 1024\u2212 40\u2212 1024\u2212 opdim where dim = 1100 and is obtained by splicing each 100 dimensional video frame\nwith a context of 10 frames - 5 on each side. opdim = |L\u2032|. After training, the last 2 layers are discarded and 40-dimensional outputs are used as visual features. The final dimension of visual feature vector is 120 including the \u2206 and \u2206\u2206 features."}, {"heading": "5 Fusion models", "text": "In this work, the fusion models are character based RNNs trained using CTC objective function i.e. L\u2032 is the set of English alphabet including a blank label. The two fusion models are shown in Figure 2"}, {"heading": "5.1 Feature Fusion", "text": "In feature fusion technique, a single RNNav is trained by concatenating the audio and visual features using the CTC objective function. In the test phase, at each instant the concatenated features are forward propagated through the network. In the CTC decoding step, the posterior probabilities obtained at the soft-max layer are converted to pseudo log-likelihoods[23] as\nlogPav(Oav,t|k) = logPav(k|Oav,t)\u2212 logP (k) (6)\nwhere k \u2208 L\u2032 and P (k) is the prior probability of class k obtained from the training data [17]."}, {"heading": "5.2 Decision Fusion", "text": "In decision fusion technique the audio and visual modalities are modeled by separate networks, RNNa and RNNv respectively. RNNv is a lip-reading system.\nThe networks are trained separately. In the test phase, for a given utterance the frame level, the pseudo log-likelihoods of RNNa and RNNv are combined as\nlogPav(Oa,t, Ov,t|k) = \u03b3 logPa(k|Oa,t) + (1\u2212 \u03b3) logPv(k|Ov,t)\u2212 logP (k) (7)\nwhere 0 \u2264 \u03b3 \u2264 1 is a parameter dependent on the noise level and the reliability of each modality[4]. For example, at higher levels of noise in audio input, a low value of \u03b3 is preferred. In this work, we adapt the parameter \u03b3 for each utterance based on KL-divergence measure between the posterior probability distributions of RNNa and RNNv. The divergence between the posterior probability distributions is expected to vary as the noise in the audio modality increases. The KL-divergence is scaled to a value in [0, 1] using logistic sigmoid. The parameter b was determined empirically from validation dataset.\nDKL(Pv||Pa) = \u2211 i PvlogPa (8)\nwhere we consider the posteriors of RNNv as the true distribution based on the assumption that video input is always free from noise.\n\u03b3 = 1\n1 + exp(\u2212DKL + b) (9)"}, {"heading": "6 Experiments", "text": "The system was trained and tested on GRID audio-visual corpus[3]. GRID corpus is a collection of audio and video recordings of 34 speakers (18 male, 16 female) each uttering a 1000 sentences. Each utterance has a fixed length of approximately 3 seconds. The total number of words in the vocabulary is 51. The syntactic structures of all sentences are similar as shown below.\n< command > < color > < preposition > < letter > < digit > < adverb > Ex. PLACE RED AT M ZERO PLEASE"}, {"heading": "6.1 Training", "text": "In the corpus obtained, the video recordings for speaker 21 were not available. In addition, 308 utterances by various speakers could not be processed due to various errors. The dataset in effect consisted of 32692 utterances 90% of the which (containing 29423 utterances) was used for training and cross validation while the remaining (10%) data was used as test set. Both training and test data contain utterances from all of the speakers. Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].\nRNNa-Acoustic model RNNa contains 2 bi-directional LSTM hidden layers.Input to the network is 120-dimensional vector containing filter-bank coefficients along with \u2206 and \u2206\u2206 features. The model parameters are randomly initialized within the range [-0.1,0.1]. The initial learning rate is set to 0.00004. Learning rate adaption is performed as follows: when the improvement in accuracy on the cross-validation set between two successive epochs falls below 0.5%, the learning rate is halved.The halving continues for each subsequent epoch until the training stops when the increase in frame level accuracy is less than 0.1%.\nDeep Bottleneck Network The training protocol similar to [23] was followed to train the bottleneck network. Input video features are mean normalized and spliced. Cross-entropy loss function is minimized using mini-batch Stochastic Gradient Descent (SGD). The frames are shuffled randomly before each epoch. Batch size is set to 256 and initial learning rate is set to 0.008. Learning rate adaptation similar to acoustic model is employed.\nRNNv-Lip Reader RNNv is trained with bottleneck network features as input. The network architecture and training procedure is same as RNNa. Figure 3 depicts the learning curves when trained with bottleneck features and DCT features. The figure shows that bottleneck features are helpful in proper convergence of the model.\nRNNav The feature fusion model RNNav consists of 3 bi-directional LSTM hidden layers. The input dimension is 240, corresponding to filter-bank coeffi-\ncients of audio modality, bottleneck features of visual modality and their respective \u2206 features. The initialization and learning rate adaption are similar to acoustic model training. However, the learning rate adaptation is employed only after a minimum number of(in this case 20) epochs are completed.\nDuring each utterance in an epoch we first present the fused audio-visual fused input sequence followed by the input sequence with audio input set to very low values. This prevents the RNNav from over-fitting to audio only inputs. Thus the effective number of sequences presented to the network in a given epoch is twice the total number of training utterances (AV and V features). After the training with AV and V features we train the network once again with two epochs of audio only utterances obtained by turning off the visual modality."}, {"heading": "6.2 Results", "text": "The audio-visual model is tested with three levels of babble noise 0dB SNR, 10dB SNR and clean audio. Noise was added to test data artificially by mixing babble noise with clean audio .wav files. In order to show the importance of visual modality under noisy environment, the model is tested with either audio or video inputs turned off. A token WFST[17] is used to map the paths to their corresponding label sequences. The token WFST obtains this mapping by removing all the blanks and repeated labels. Character Error Rate(CER) is obtained from the decoded and expected label sequences by calculating the Edit distance between them. The CER results are shown in Table 1.\nWe observe that with clean audio input, audio only RNNa performs significantly better (CER 2.45%) compared to audio-visual RNNav (CER 5.74%). However as audio becomes noisy, the performance of RNNa deteriorates significantly whereas the performance of RNNav remains relatively stable. Under noisy conditions the feature fusion model behaves as if it is not receiving any input from the audio modality.\nTable 1 also gives a comparison between feature fusion model and decision fusion model. We find that feature fusion model performs better than decision fusion model in all cases except under clean audio conditions. The poor CER of RNNa, RNNv model indicates that the frame level predictions between RNNa and RNNv are not synchronous. However, both the fusion models provide significant gains under noisy audio inputs. While there is large difference between RNNa and other models with clean inputs, we believe this difference is due to the nature of dataset and will reduce with larger datasets."}, {"heading": "7 Conclusions And Future Work", "text": "In this work we presented an audio-visual ASR system using deep RNNs trained with CTC objective function. We described a feature processing step for visual features using deep bottleneck layer and showed that it helps in faster convergence of RNN model during training. We presented a training protocol in which either of the modalities is turned off during training in order to avoid dependency on a single modality. Our results indicate that the trained model is robust to noise. In addition, we compared fusion strategies at the feature level and at the decision level.\nWhile the use of bottleneck features for visual modality helps in training, it requires frame level labels which involves an additional step of training audio RNN. Therefore, our system is not yet end-to-end. Our experiments in visual feature engineering with unsupervised methods like multi-modal auto-encoder[19] did not produce remarkable results. In future work we intend to explore other unsupervised methods for visual feature extraction such as canonical correlation analysis."}], "references": [{"title": "Multimodal speech processing using asynchronous hidden markov models", "author": ["S. Bengio"], "venue": "Information Fusion 5(2), 81\u201389", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Coupled hidden markov models for complex action recognition", "author": ["M. Brand", "N. Oliver", "A. Pentland"], "venue": "Computer vision and pattern recognition, 1997. proceedings., 1997 ieee computer society conference on. pp. 994\u2013999. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America 120(5), 2421\u20132424", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Audio-visual speech modeling for continuous speech recognition", "author": ["S. Dupont", "J. Luettin"], "venue": "IEEE transactions on multimedia 2(3), 141\u2013151", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Extracting deep bottleneck features using stacked auto-encoders", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 3377\u20133381. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural networks", "author": ["A. Graves"], "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, pp. 15\u201335. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. pp. 369\u2013 376. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "ICML. vol. 14, pp. 1764\u20131772", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech: Scaling up end-toend speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A Coates"], "venue": "arXiv preprint arXiv:1412.5567", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Tandem connectionist feature extraction for conventional hmm systems", "author": ["H. Hermansky", "D.P. Ellis", "S. Sharma"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. vol. 3, pp. 1635\u2013 1638. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7596\u20137599. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving audio-visual speech recognition with an infrared headset", "author": ["J. Huang", "G. Potamianos", "C. Neti"], "venue": "AVSP 2003-International Conference on Audio-Visual Speech Processing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Audiovisual fusion: Challenges and new approaches", "author": ["A.K. Katsaggelos", "S. Bahaadini", "R. Molina"], "venue": "Proceedings of the IEEE 103(9), 1635\u20131653", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1867\u20131874", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn", "author": ["Y. Miao"], "venue": "arXiv preprint arXiv:1401.6984", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). pp. 167\u2013174. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Y. Mroueh", "E. Marcheret", "V. Goel"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 2130\u20132134. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11). pp. 689\u2013696", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio-visual speech recognition using deep learning", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Applied Intelligence 42(4), 722\u2013737", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Recent advances in the automatic recognition of audiovisual speech", "author": ["G. Potamianos", "C. Neti", "G. Gravier", "A. Garg", "A.W. Senior"], "venue": "Proceedings of the IEEE 91(9), 1306\u20131326", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding. No. EPFL-CONF-192584, IEEE Signal Processing Society", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "INTERSPEECH. pp. 2345\u20132349", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved bottleneck features using pretrained deep neural networks", "author": ["D. Yu", "M.L. Seltzer"], "venue": "Interspeech. vol. 237, p. 240", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": "More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "End-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "End-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM.", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "End-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Visual feature extraction methods can be of 3 types[21] : 1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Additional feature processing such as mean normalization, intra-frame and inter-frame LDA may be applied [13][21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "Additional feature processing such as mean normalization, intra-frame and inter-frame LDA may be applied [13][21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "Fusion methods can be broadly divided into two types[21][14]: 1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "Fusion methods can be broadly divided into two types[21][14]: 1.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Various decision fusion methods based on variants of HMMs have been proposed[4][2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Various decision fusion methods based on variants of HMMs have been proposed[4][2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams.", "startOffset": 115, "endOffset": 118}, {"referenceID": 20, "context": "For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14] Application of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14] Application of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14] Application of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "In [12], Deep Belief Networks(DBN) are explored.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18] the authors train separate networks for audio and visual inputs and fuse the final layers of two networks, and then build a third DNN with the fused features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In addition, [18] presents a new DNN architecture with a bilinear soft-max layer which further improves the performance.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "In [20] a deep de-noising auto-encoder is used to learn noise robust speech features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The training algorithm suffers from vanishing gradients problem which is overcome by using a special unit in hidden layer called the Long Short Term Memory(LSTM)[11][6].", "startOffset": 161, "endOffset": 165}, {"referenceID": 5, "context": "The training algorithm suffers from vanishing gradients problem which is overcome by using a special unit in hidden layer called the Long Short Term Memory(LSTM)[11][6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "CTC objective function[7][8] obviates the need for such alignments as it enables the network to learn over all possible alignments.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "CTC objective function[7][8] obviates the need for such alignments as it enables the network to learn over all possible alignments.", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "The 64x64 lip region is extracted by detecting 68 landmark points[15] on the speakers face, and cropping the ROI surrounding speakers mouth and chin.", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "In the CTC decoding step, the posterior probabilities obtained at the soft-max layer are converted to pseudo log-likelihoods[23] as", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "where k \u2208 L\u2032 and P (k) is the prior probability of class k obtained from the training data [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "where 0 \u2264 \u03b3 \u2264 1 is a parameter dependent on the noise level and the reliability of each modality[4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "The KL-divergence is scaled to a value in [0, 1] using logistic sigmoid.", "startOffset": 42, "endOffset": 48}, {"referenceID": 2, "context": "The system was trained and tested on GRID audio-visual corpus[3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 21, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "Deep Bottleneck Network The training protocol similar to [23] was followed to train the bottleneck network.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "A token WFST[17] is used to map the paths to their corresponding label sequences.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Our experiments in visual feature engineering with unsupervised methods like multi-modal auto-encoder[19] did not produce remarkable results.", "startOffset": 101, "endOffset": 105}], "year": 2016, "abstractText": "In this work, we propose a training algorithm for an audiovisual automatic speech recognition (AV-ASR) system using deep recurrent neural network (RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal Classification (CTC) objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate (CER) at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion.", "creator": "LaTeX with hyperref package"}}}