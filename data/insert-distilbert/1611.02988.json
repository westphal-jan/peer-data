{"id": "1611.02988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Distant supervision for emotion detection using Facebook reactions", "abstract": "we exploit the facebook reaction feature in a distant manually supervised fashion to train a support vector machine word classifier for emotion button detection, using several feature combinations arranged and combining randomly different parallel facebook pages. we test our models on existing benchmarks for emotion detection and show that employing only information about that is derived completely automatically, thus without relying on any handcrafted lexicon as it's usually done, but we can achieve competitive results. the results also show that there is large room for aesthetic improvement, especially by gearing the collection of possible facebook pages, with a view to the target domain.", "histories": [["v1", "Wed, 9 Nov 2016 15:49:31 GMT  (180kb,D)", "http://arxiv.org/abs/1611.02988v1", "Proceedings of the Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media (PEOPLES 2016), held in conjunction with COLING 2016, Osaka, Japan"]], "COMMENTS": "Proceedings of the Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media (PEOPLES 2016), held in conjunction with COLING 2016, Osaka, Japan", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris pool", "malvina nissim"], "accepted": false, "id": "1611.02988"}, "pdf": {"name": "1611.02988.pdf", "metadata": {"source": "CRF", "title": "Distant supervision for emotion detection using Facebook reactions", "authors": ["Chris Pool", "Malvina Nissim"], "emails": ["c.pool@anchormen.nl", "m.nissim@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "In the spirit of the brevity of social media\u2019s messages and reactions, people have got used to express feelings minimally and symbolically, as with hashtags on Twitter and Instagram. On Facebook, people tend to be more wordy, but posts normally receive more simple \u201clikes\u201d than longer comments. Since February 2016, Facebook users can express specific emotions in response to a post thanks to the newly introduced reaction feature (see Section 2), so that now a post can be wordlessly marked with an expression of say \u201cjoy\u201d or \u201csurprise\u201d rather than a generic \u201clike\u201d.\nIt has been observed that this new feature helps Facebook to know much more about their users and exploit this information for targeted advertising (Stinson, 2016), but interest in people\u2019s opinions and how they feel isn\u2019t limited to commercial reasons, as it invests social monitoring, too, including health care and education (Mohammad, 2016). However, emotions and opinions are not always expressed this explicitly, so that there is high interest in developing systems towards their automatic detection. Creating manually annotated datasets large enough to train supervised models is not only costly, but also\u2014especially in the case of opinions and emotions\u2014difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010). Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.e. using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica. Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon. Emoticons are used as proxies also by Hallsmar and Palm (2016), who use distributed vector representations to find which words are interchangeable with emoticons but also which emoticons are used in a similar context.\nWe take advantage of distant supervision by using Facebook reactions as proxies for emotion labels, which to the best of our knowledge hasn\u2019t been done yet, and we train a set of Support Vector Machine models for emotion recognition. Our models, differently from existing ones, exploit information which\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 1.\n02 98\n8v 1\n[ cs\n.C L\n] 9\nN ov\n2 01\n6\nis acquired entirely automatically, and achieve competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. For explanatory purposes, related work is discussed further and more in detail when we describe the benchmarks for evaluation (Section 3) and when we compare our models to existing ones (Section 5). We also explore and discuss how choosing different sets of Facebook pages as training data provides an intrinsic domain-adaptation method."}, {"heading": "2 Facebook reactions as labels", "text": "For years, on Facebook people could leave comments to posts, and also \u201clike\u201d them, by using a thumbs-up feature to explicitly express a generic, rather underspecified, approval. A \u201clike\u201d could thus mean \u201cI like what you said\u201d, but also \u201cI like that you bring up such topic (though I find the content of the article you linked annoying)\u201d.\nWe collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library1. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section 4) and with an eye to the nature of the datasets available for evaluation (see Section 5). The choice of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section 6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\nFor each page, we downloaded the latest 1000 posts, or the maximum available if there are fewer, from February 2016, retrieving the counts of reactions for each post. The output is a JSON file containing a list of dictionaries with a timestamp, the post and a reaction vector with frequency values, which indicate how many users used that reaction in response to the post (Figure 2). The resulting emotion vectors must then be turned into an emotion label.3\nIn the context of this experiment, we made the simple decision of associating to each post the emotion with the highest count, ignoring like as it is the default and most generic reaction peo-\nple tend to use. Therefore, for example, to the first post in Figure 2, we would associate the label sad, as it has the highest score (284) among the meaningful emotions we consider, though it also has non-zero scores for other emotions. At this stage, we didn\u2019t perform any other entropy-based selection of posts, to be investigated in future work.\n1https://pypi.python.org/pypi/facebook-sdk 1Note that thankfulwas only available during specific time spans related to certain events, as Mother\u2019s Day in May 2016."}, {"heading": "3 Emotion datasets", "text": "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section 3.4. A summary is provided in Table 1, which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section 4), all three have been used as benchmarks for our evaluation."}, {"heading": "3.1 Affective Text dataset", "text": "Task 14 at SemEval 2007 (Strapparava and Mihalcea, 2007) was concerned with the classification of emotions and valence in news headlines. The headlines where collected from several news websites including Google news, The New York Times, BBC News and CNN. The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise, in line with the six basic emotions of Ekman\u2019s standard model (Ekman, 1992). Valence was to be determined as positive or negative. Classification of emotion and valence were treated as separate tasks. Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson\u2019s r to measure the correlation between the system scores and the gold standard; and a coarsegrained method where each emotion score was converted to a binary label, and precision, recall, and f-score were computed to assess performance. As it is done in most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), we also treat this as a classification problem (coarse-grained). This dataset has been extensively used for the evaluation of various unsupervised methods (Strapparava and Mihalcea, 2008), but also for testing different supervised learning techniques and feature portability (Mohammad, 2012)."}, {"heading": "3.2 Fairy Tales dataset", "text": "This is a dataset collected by Alm (2008), where about 1,000 sentences from fairy tales (by B. Potter, H.C. Andersen and Grimm) were annotated with the same six emotions of the Affective Text dataset, though with different names: Angry, Disgusted, Fearful, Happy, Sad, and Surprised. In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences where all annotators agreed are used, and the labels angry and disgusted are merged. We adopt the same choices."}, {"heading": "3.3 ISEAR", "text": "The ISEAR (International Survey on Emotion Antecedents and Reactions (Scherer and Wallbott, 1994; Scherer, 1997)) is a dataset created in the context of a psychology project of the 1990s, by collecting questionnaires answered by people with different cultural backgrounds. The main aim of this project was to gather insights in cross-cultural aspects of emotional reactions. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of seven major emotions (joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions covered the way they had appraised a given situation and how they reacted. The final dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion, making this the largest dataset out of the three we use."}, {"heading": "3.4 Overview of datasets and emotions", "text": "We summarise datasets and emotion distribution from two viewpoints. First, because there are different sets of emotions labels in the datasets and Facebook data, we need to provide a mapping and derive a subset of emotions that we are going to use for the experiments. This is shown in Table 1, where in\nthe \u201cMapped\u201d column we report the final emotions we use in this paper: anger, joy, sadness, surprise. All labels in each dataset are mapped to these final emotions, which are therefore the labels we use for training and testing our models.\nSecond, the distribution of the emotions for each dataset is different, as can be seen in Figure 3.\nIn Figure 4 we also provide the distribution of the emotions anger, joy, sadness, surprise per Facebook page, in terms of number of posts (recall that we assign to a post the label corresponding to the majority emotion associated to it, see Section 2).\nWe can observe that for example pages about news tend to have more sadness and anger posts, while pages about cooking and tv-shows have a high percentage of joy posts. We will use this information to find the best set of pages for a given target domain (see Section 5)."}, {"heading": "4 Model", "text": "There are two main decisions to be taken in developing our model: (i) which Facebook pages to select as training data, and (ii) which features to use to train the model, which we discuss below. Specifically, we first set on a subset of pages and then experiment with features. Further exploration of the interaction between choice of pages and choice of features is left to future work, and partly discussed in Section 6. For development, we use a small portion of the Affective data set described in Section 3.1, that is the portion that had been released as development set for SemEval\u2019s 2007 Task 14 (Strapparava and Mihalcea, 2007), which contains 250 annotated sentences (Affective development, Section 3.1). All results reported in this section are on this dataset. The test set of Task 14 as well as the other two datasets described in Section 3 will be used to evaluate the final models (Section 4)."}, {"heading": "4.1 Selecting Facebook pages", "text": "Although page selection is a crucial ingredient of this approach, which we believe calls for further and deeper, dedicated investigation, for the experiments described here we took a rather simple approach. First, we selected the pages that would provide training data based on intuition and availability, then chose different combinations according to results of a basic model run on development data, and eventually tested feature combinations, still on the development set.\nFor the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation (Pedregosa et al., 2011) on different combinations of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure 3), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class."}, {"heading": "4.2 Features", "text": "In selecting appropriate features, we mainly relied on previous work and intuition. We experimented with different combinations, and all tests were still done on Affective development, using the pages for the best model (B-M) described above as training data. Results are in Table 2. Future work will further explore the simultaneous selection of features and page combinations.\nStandard textual features We use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation.\nAffect Lexicons This feature is used in all unsupervised models as a source of information, and we mainly include it to assess its contribution, but eventually do not use it in our final model.\nWe used the NRC10 Lexicon because it performed best in the experiments by (Mohammad, 2012), which is built around the emotions anger, anticipation, disgust, fear, joy, sadness, and surprise, and the valence values positive and negative. For each word in the lexicon, a boolean value indicating presence or absence is associated to each emotion. For a whole sentence, a global score per emotion can be obtained by summing the vectors for all content words of that sentence included in the lexicon, and used as feature.\nWord Embeddings As additional feature, we also included Word Embeddings, namely distributed representations of words in a vector space, which have been exceptionally successful in boosting performance in a plethora of NLP tasks. We use three different embeddings:\n\u2022 Google embeddings: pre-trained embeddings trained on Google News and obtained with the skipgram architecture described in (Mikolov et al., 2013). This model contains 300-dimensional vectors for 3 million words and phrases.\n\u2022 Facebook embeddings: embeddings that we trained on our scraped Facebook pages for a total of 20,000 sentences. Using the gensim library (R\u030cehu\u030ar\u030cek and Sojka, 2010), we trained the embeddings with the following parameters: window size of 5, learning rate of 0.01 and dimensionality of 100. We filtered out words with frequency lower than 2 occurrences.\n\u2022 Retrofitted embeddings: Retrofitting (Faruqui et al., 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it\u2019s done for example to create senseaware (Iacobacci et al., 2015) or sentiment-aware (Tang et al., 2014) embeddings.4 In this work, we retrofit general embeddings to include information about emotions, so that emotion-similar words can get closer in space. Both the Google as well as our Facebook embeddings were retrofitted with\n4Training emotion-aware embeddings is a strategy that we plan to explore in future work.\nlexical information obtained from the NRC10 Lexicon mentioned above, which provides emotionsimilarity for each token. Note that differently from the previous two types of embeddings, the retrofitted ones do rely on handcrafted information in the form of a lexical resource."}, {"heading": "4.3 Results on development set", "text": "We report precision, recall, and f-score on the development set. The average f-score is reported as microaverage, to better account for the skewed distribution of the classes as well as in accordance to what is usually reported for this task (Mohammad and Kiritchenko, 2015).\nFrom Table 2 we draw three main observations. First, a simple tf-idf bag-of-word mode works already very well, to the point that the other textual and lexicon-based features don\u2019t seem to contribute to the overall f-score (0.368), although there is a rather substantial variation of scores per class. Second, Google embeddings perform a lot better than Facebook embeddings, and this is likely due to the size of the corpus used for training. Retrofitting doesn\u2019t seem to help at all for the Google embeddings, but it does boost the Facebook embeddings, leading to think that with little data, more accurate task-related information is helping, but corpus size matters most. Third, in combination with embeddings, all features work better than just using tf-idf, but removing the Lexicon feature, which is the only one based on hand-crafted resources, yields even better results. Then our best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features."}, {"heading": "5 Results", "text": "In Table 3 we report the results of our model on the three datasets standardly used for the evaluation of emotion classification, which we have described in Section 3.\nOur B-M model relies on subsets of Facebook pages for training, which were chosen according to their performance on the development set as well as on the observation of emotions distribution on different pages and in the different datasets, as described in Section 4. The feature set we use is our best on the development set, namely all the features plus Google-based embeddings, but excluding the lexicon. This makes our approach completely independent of any manual annotation or handcrafted resource. Our model\u2019s performance is compared to the following systems, for which results are reported in the referred literature. Please note that no other existing model was re-implemented, and results are those reported in the respective papers.\nKim et al. (2010) experiment with four different unsupervised techniques that rely on lexicon-derived information. In Table 3 we report the scores for their best average performing approach, namely a\nCNMF-based categorical classification. They made the decision not to deal with surprise because this emotion is not present in the ISEAR dataset.\nStrapparava and Mihalcea (2008) experiment with several models based on a core LSA model and, in their best performing model (LSA-all emotion words) whose results we report in Table 3, also use information from lexical resources both in their general (WordNet (Fellbaum, 1998)) and emotionaware (WordNet Affect (Strapparava et al., 2004)) form.\nDanisman and Alpkocak (2008) adopt a supervised approach, training a model using the ISEAR dataset and testing it on the Affective text dataset. They only report results per category in terms of f-score, without further specification of how precision and recall contribute.\nWe have mentioned that the selection of Facebook pages is relevant and can be also thought of as a tool for domain adaptation in accordance with the characteristics of the target domains/datasets (see also Section 2 and Figures 3\u20134). Although we believe that such an interesting aspect will require deeper investigation (see also Section 6), we preliminary test this assumption by developing and comparing two more models: a model that uses a combination of pages that we expect will perform best on the Fairy Tales dataset (FT-M), and a model that uses a combination of pages that should perform best on the ISEAR dataset (ISE-M). The feature set is kept the same for all three models.\nFT-M The sentences in the Fairy Tales dataset are quite different compared to the news headlines in the development set. Looking at the distribution in this dataset, as can be seen in Figure 3, Joy is the most frequent class. We selected the pages HuffPostWeirdNews, ESPN and CNN for this model especially looking at the performance for the emotions that are most frequent in this dataset.\nISE-M As described in Section 3.3, the sentences in the ISEAR collection are also different compared to the two other datasets. Looking at the distribution in Figure 3 and according to performance on relevant emotions (we took into account the absence of Surprise in this dataset), we selected the pages Time, The Guardian and CookingLight for this model.\nIn Table 3 we report results for all of the models mentioned above. We indicate averages only for our models, since not all approaches deal with the same sets of emotions and we cannot easily compute them. We discuss results both in terms of how our models fair with respect to other systems as reported the literature, as well as how they compare to one another with a view to the selection of Facebook pages.\nCompared to other systems, our models are globally competitive, given that B-M is entirely unsupervised. Overall, the unsupervised but heavily lexicon-based best model of (Kim et al., 2010) performs well on all emotions, excluding surprise, which they do not address (thus also making their classification task slightly easier). Differently from existing systems, our models appear rather balanced in terms of performance on the different emotions as well as in precision and recall, and are able to deal well with the variance of the datasets.\nOn the Affective Text dataset, we have the highest precision for all emotions but joy, though on this emotion our models have very good recall. The highest recall for all emotions for this dataset is reported in (Strapparava and Mihalcea, 2008), together with extremely low precision. Such skewed performance for all emotions can only be explained if different emotion-specific models were trained rather than a single multiclass model, but this is not described as such in the paper. The authors state that their models are completely unsupervised, which is true in terms of training data, but they nevertheless augment them with information derived from hand-crafted resources.\nOn the Fairy Tales dataset, (Kim et al., 2010) Chaffar and Inkpen (2011) also used the Fairy tales dataset to evaluate a supervised model using features like bag-of-words, N-grams and lexical emotion features, but report cross-validated results using accuracy only, and are therefore harder to compare.\nOn the ISEAR dataset, which is the largest, our models perform best for all emotions but anger, for which however we achieve the highest precision with all our models. From the perspective of comparing our models, we do not observe any real correlation between our actual best performances and the models designed to best perform on a given dataset. For example, B-M\nwas expected to perform best on the Affective Text, but it is outperformed by FT-M in the precision of detecting anger and sadness, and overall for the detection of surprise. Generally, by looking at averages, it seems that our best performing model across datasets is ISE-M. However, the extremely large variance among scores for the same emotion on the three datasets, highlights the differences among such datasets and the need to better tailor training data to different domains. The large discrepancy in detecting different emotions in the same dataset also deserves further investigation. We discuss such issues further in the next section, with a view to future work."}, {"heading": "6 Discussion, conclusions and future work", "text": "We have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource. An interesting aspect of our approach is the view to domain adap-\ntation via the selection of Facebook pages to be used as training data. We believe that this approach has a lot of potential, and we see the following directions for improvement. Feature-wise, we want to train emotion-aware embeddings, in the vein of work by Tang et al. (2014), and Iacobacci et al. (2015). Retrofitting FB-embeddings trained on a larger corpus might also be successful, but would rely on an external lexicon.\nThe largest room for yielding not only better results but also interesting insights on extensions of this approach lies in the choice of training instances, both in terms of Facebook pages to get posts from, as well as in which posts to select from the given pages. For the latter, one could for example only select posts that have a certain length, ignore posts that are only quotes or captions to images, or expand posts by including content from linked html pages, which might provide larger and better contexts (Plank et al., 2014). Additionally, and most importantly, one could use an entropy-based measure to select only posts that have a strong emotion rather than just considering the majority emotion as training label. For the former, namely the choice of Facebook pages, which we believe deserves the most investigation, one could explore several avenues, especially in relation to stance-based issues (Mohammad et al., 2016). In our dataset, for example, a post about Chile beating Colombia in a football match during the Copa America had very contradictory reactions, depending on which side readers would cheer for. Similarly, the very same political event, for example, would get very different reactions from readers if it was posted on Fox News or The Late Night Show, as the target audience is likely to feel very differently about the same issue. This also brings up theoretical issues related more generally to the definition of the emotion detection task, as it\u2019s strongly dependent on personal traits of the audience. Also, in this work, pages initially selected on availability and intuition were further grouped into sets to make training data according to performance on development data, and label distribution. Another criterion to be exploited would be vocabulary overlap between the pages and the datasets.\nLastly, we could develop single models for each emotion, treating the problem as a multi-label task. This would even better reflect the ambiguity and subjectivity intrinsic to assigning emotions to text, where content could be at same time joyful or sad, depending on the reader."}, {"heading": "Acknowledgements", "text": "In addition to the anonymous reviewers, we want to thank Lucia Passaro and Barbara Plank for insightful discussions, and for providing comments on draft versions of this paper."}], "references": [{"title": "Affect in text and speech", "author": ["Ebba Cecilia Ovesdotter Alm."], "venue": "ProQuest.", "citeRegEx": "Alm.,? 2008", "shortCiteRegEx": "Alm.", "year": 2008}, {"title": "Emotions in text: dimensional and categorical models", "author": ["Rafael A Calvo", "Sunghwan Mac Kim."], "venue": "Computational Intelligence, 29(3):527\u2013543.", "citeRegEx": "Calvo and Kim.,? 2013", "shortCiteRegEx": "Calvo and Kim.", "year": 2013}, {"title": "Using a heterogeneous dataset for emotion analysis in text", "author": ["Soumaya Chaffar", "Diana Inkpen."], "venue": "Advances in Artificial Intelligence, pages 62\u201367. Springer.", "citeRegEx": "Chaffar and Inkpen.,? 2011", "shortCiteRegEx": "Chaffar and Inkpen.", "year": 2011}, {"title": "Feeler: Emotion classification of text using vector space model", "author": ["Taner Danisman", "Adil Alpkocak."], "venue": "AISB 2008 Convention Communication, Interaction and Social Intelligence, volume 1, page 53.", "citeRegEx": "Danisman and Alpkocak.,? 2008", "shortCiteRegEx": "Danisman and Alpkocak.", "year": 2008}, {"title": "An argument for basic emotions", "author": ["Paul Ekman."], "venue": "Cognition & emotion, 6(3-4):169\u2013200.", "citeRegEx": "Ekman.,? 1992", "shortCiteRegEx": "Ekman.", "year": 1992}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1606\u20131615, Denver, Colorado, May\u2013June. Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "WordNet", "author": ["Christiane Fellbaum."], "venue": "Wiley Online Library.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "CS224N Project Report, Stanford, 1:12.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Multi-class sentiment classification on twitter using an emoji training heuristic", "author": ["Fredrik Hallsmar", "Jonas Palm."], "venue": "Technical report, KTH/Skolan f\u00f6r datavetenskap och kommunikation (CSC). University essay.", "citeRegEx": "Hallsmar and Palm.,? 2016", "shortCiteRegEx": "Hallsmar and Palm.", "year": 2016}, {"title": "Sensembed: learning sense embeddings for word and relational similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "Proceedings of ACL, pages 95\u2013105.", "citeRegEx": "Iacobacci et al\\.,? 2015", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Evaluation of unsupervised emotion models to textual affect recognition", "author": ["Sunghwan Mac Kim", "Alessandro Valitutti", "Rafael A Calvo."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 62\u201370. Association for Computational Linguistics.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL \u201909, pages 1003\u20131011, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Using hashtags to capture fine emotion categories from tweets", "author": ["Saif M Mohammad", "Svetlana Kiritchenko."], "venue": "Computational Intelligence, 31(2):301\u2013326.", "citeRegEx": "Mohammad and Kiritchenko.,? 2015", "shortCiteRegEx": "Mohammad and Kiritchenko.", "year": 2015}, {"title": "A dataset for detecting stance in tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry."], "venue": "Proceedings of 10th edition of the the Language Resources and Evaluation Conference (LREC), Portoro\u017e, Slovenia.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "Portable features for classifying emotional text", "author": ["Saif Mohammad."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 587\u2013591, Montr\u00e9al, Canada, June. Association for Computational Linguistics.", "citeRegEx": "Mohammad.,? 2012", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "Sentiment analysis: Detecting valence, emotions, and other affectual states from text", "author": ["Saif M. Mohammad."], "venue": "Herb Meiselman, editor, Emotion Measurement. Elsevier.", "citeRegEx": "Mohammad.,? 2016", "shortCiteRegEx": "Mohammad.", "year": 2016}, {"title": "Scikitlearn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay."], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Adapting taggers to twitter with not-so-distant supervision", "author": ["Barbara Plank", "Dirk Hovy", "Ryan T McDonald", "Anders S\u00f8gaard."], "venue": "COLING, pages 1783\u20131792.", "citeRegEx": "Plank et al\\.,? 2014", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Experimenting with distant supervision for emotion classification", "author": ["Matthew Purver", "Stuart Battersby."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 482\u2013491. Association for Computational Linguistics.", "citeRegEx": "Purver and Battersby.,? 2012", "shortCiteRegEx": "Purver and Battersby.", "year": 2012}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/publication/884893/en.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Evidence for universality and cultural variation of differential emotion response patterning", "author": ["Klaus R Scherer", "Harald G Wallbott."], "venue": "Journal of personality and social psychology, 66(2):310.", "citeRegEx": "Scherer and Wallbott.,? 1994", "shortCiteRegEx": "Scherer and Wallbott.", "year": 1994}, {"title": "The role of culture in emotion-antecedent appraisal", "author": ["Klaus R Scherer."], "venue": "Journal of personality and social psychology, 73(5):902.", "citeRegEx": "Scherer.,? 1997", "shortCiteRegEx": "Scherer.", "year": 1997}, {"title": "Facebook reactions, the totally redesigned like button, is here", "author": ["Liz Stinson."], "venue": "Wired. http://www. wired.com/2016/02/facebook-reactions-totally-redesigned-like-button/.", "citeRegEx": "Stinson.,? 2016", "shortCiteRegEx": "Stinson.", "year": 2016}, {"title": "Semeval-2007 task 14: Affective text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 70\u201374. Association for Computational Linguistics.", "citeRegEx": "Strapparava and Mihalcea.,? 2007", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2007}, {"title": "Learning to identify emotions in text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 2008 ACM symposium on Applied computing, pages 1556\u20131560. ACM.", "citeRegEx": "Strapparava and Mihalcea.,? 2008", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2008}, {"title": "Wordnet affect: an affective extension of wordnet", "author": ["Carlo Strapparava", "Alessandro Valitutti"], "venue": "In LREC,", "citeRegEx": "Strapparava and Valitutti,? \\Q2004\\E", "shortCiteRegEx": "Strapparava and Valitutti", "year": 2004}, {"title": "Learning sentiment-specific word embedding for Twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1555\u20131565.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "It has been observed that this new feature helps Facebook to know much more about their users and exploit this information for targeted advertising (Stinson, 2016), but interest in people\u2019s opinions and how they feel isn\u2019t limited to commercial reasons, as it invests social monitoring, too, including health care and education (Mohammad, 2016).", "startOffset": 148, "endOffset": 163}, {"referenceID": 16, "context": "It has been observed that this new feature helps Facebook to know much more about their users and exploit this information for targeted advertising (Stinson, 2016), but interest in people\u2019s opinions and how they feel isn\u2019t limited to commercial reasons, as it invests social monitoring, too, including health care and education (Mohammad, 2016).", "startOffset": 328, "endOffset": 344}, {"referenceID": 25, "context": "Creating manually annotated datasets large enough to train supervised models is not only costly, but also\u2014especially in the case of opinions and emotions\u2014difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010).", "startOffset": 211, "endOffset": 261}, {"referenceID": 10, "context": "Creating manually annotated datasets large enough to train supervised models is not only costly, but also\u2014especially in the case of opinions and emotions\u2014difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010).", "startOffset": 211, "endOffset": 261}, {"referenceID": 10, "context": "Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011).", "startOffset": 130, "endOffset": 174}, {"referenceID": 2, "context": "Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011).", "startOffset": 130, "endOffset": 174}, {"referenceID": 12, "context": "using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica.", "startOffset": 88, "endOffset": 108}, {"referenceID": 19, "context": ", 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica.", "startOffset": 112, "endOffset": 140}, {"referenceID": 1, "context": ", 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.", "startOffset": 8, "endOffset": 58}, {"referenceID": 1, "context": ", 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.e. using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica. Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon.", "startOffset": 8, "endOffset": 501}, {"referenceID": 0, "context": "Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon. Emoticons are used as proxies also by Hallsmar and Palm (2016), who use distributed vector representations to find which words are interchangeable with emoticons but also which emoticons are used in a similar context.", "startOffset": 110, "endOffset": 246}, {"referenceID": 24, "context": "Task 14 at SemEval 2007 (Strapparava and Mihalcea, 2007) was concerned with the classification of emotions and valence in news headlines.", "startOffset": 24, "endOffset": 56}, {"referenceID": 4, "context": "The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise, in line with the six basic emotions of Ekman\u2019s standard model (Ekman, 1992).", "startOffset": 137, "endOffset": 150}, {"referenceID": 10, "context": "As it is done in most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), we also treat this as a classification problem (coarse-grained).", "startOffset": 50, "endOffset": 119}, {"referenceID": 2, "context": "As it is done in most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), we also treat this as a classification problem (coarse-grained).", "startOffset": 50, "endOffset": 119}, {"referenceID": 25, "context": "This dataset has been extensively used for the evaluation of various unsupervised methods (Strapparava and Mihalcea, 2008), but also for testing different supervised learning techniques and feature portability (Mohammad, 2012).", "startOffset": 90, "endOffset": 122}, {"referenceID": 15, "context": "This dataset has been extensively used for the evaluation of various unsupervised methods (Strapparava and Mihalcea, 2008), but also for testing different supervised learning techniques and feature portability (Mohammad, 2012).", "startOffset": 210, "endOffset": 226}, {"referenceID": 10, "context": "In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences where all annotators agreed are used, and the labels angry and disgusted are merged.", "startOffset": 36, "endOffset": 105}, {"referenceID": 2, "context": "In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences where all annotators agreed are used, and the labels angry and disgusted are merged.", "startOffset": 36, "endOffset": 105}, {"referenceID": 0, "context": "This is a dataset collected by Alm (2008), where about 1,000 sentences from fairy tales (by B.", "startOffset": 31, "endOffset": 42}, {"referenceID": 21, "context": "3 ISEAR The ISEAR (International Survey on Emotion Antecedents and Reactions (Scherer and Wallbott, 1994; Scherer, 1997)) is a dataset created in the context of a psychology project of the 1990s, by collecting questionnaires answered by people with different cultural backgrounds.", "startOffset": 77, "endOffset": 120}, {"referenceID": 22, "context": "3 ISEAR The ISEAR (International Survey on Emotion Antecedents and Reactions (Scherer and Wallbott, 1994; Scherer, 1997)) is a dataset created in the context of a psychology project of the 1990s, by collecting questionnaires answered by people with different cultural backgrounds.", "startOffset": 77, "endOffset": 120}, {"referenceID": 24, "context": "1, that is the portion that had been released as development set for SemEval\u2019s 2007 Task 14 (Strapparava and Mihalcea, 2007), which contains 250 annotated sentences (Affective development, Section 3.", "startOffset": 92, "endOffset": 124}, {"referenceID": 17, "context": "For the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation (Pedregosa et al., 2011) on different combinations of pages.", "startOffset": 164, "endOffset": 188}, {"referenceID": 15, "context": "We used the NRC10 Lexicon because it performed best in the experiments by (Mohammad, 2012), which is built around the emotions anger, anticipation, disgust, fear, joy, sadness, and surprise, and the valence values positive and negative.", "startOffset": 74, "endOffset": 90}, {"referenceID": 11, "context": "\u2022 Google embeddings: pre-trained embeddings trained on Google News and obtained with the skipgram architecture described in (Mikolov et al., 2013).", "startOffset": 124, "endOffset": 146}, {"referenceID": 20, "context": "Using the gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010), we trained the embeddings with the following parameters: window size of 5, learning rate of 0.", "startOffset": 25, "endOffset": 50}, {"referenceID": 5, "context": "\u2022 Retrofitted embeddings: Retrofitting (Faruqui et al., 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it\u2019s done for example to create senseaware (Iacobacci et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": ", 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it\u2019s done for example to create senseaware (Iacobacci et al., 2015) or sentiment-aware (Tang et al.", "startOffset": 252, "endOffset": 276}, {"referenceID": 27, "context": ", 2015) or sentiment-aware (Tang et al., 2014) embeddings.", "startOffset": 27, "endOffset": 46}, {"referenceID": 13, "context": "The average f-score is reported as microaverage, to better account for the skewed distribution of the classes as well as in accordance to what is usually reported for this task (Mohammad and Kiritchenko, 2015).", "startOffset": 177, "endOffset": 209}, {"referenceID": 6, "context": "Strapparava and Mihalcea (2008) experiment with several models based on a core LSA model and, in their best performing model (LSA-all emotion words) whose results we report in Table 3, also use information from lexical resources both in their general (WordNet (Fellbaum, 1998)) and emotionaware (WordNet Affect (Strapparava et al.", "startOffset": 260, "endOffset": 276}, {"referenceID": 10, "context": "Overall, the unsupervised but heavily lexicon-based best model of (Kim et al., 2010) performs well on all emotions, excluding surprise, which they do not address (thus also making their classification task slightly easier).", "startOffset": 66, "endOffset": 84}, {"referenceID": 25, "context": "The highest recall for all emotions for this dataset is reported in (Strapparava and Mihalcea, 2008), together with extremely low precision.", "startOffset": 68, "endOffset": 100}, {"referenceID": 10, "context": "On the Fairy Tales dataset, (Kim et al., 2010) Chaffar and Inkpen (2011) also used the Fairy tales dataset to evaluate a supervised model using features like bag-of-words, N-grams and lexical emotion features, but report cross-validated results using accuracy only, and are therefore harder to compare.", "startOffset": 28, "endOffset": 46}, {"referenceID": 2, "context": ", 2010) Chaffar and Inkpen (2011) also used the Fairy tales dataset to evaluate a supervised model using features like bag-of-words, N-grams and lexical emotion features, but report cross-validated results using accuracy only, and are therefore harder to compare.", "startOffset": 8, "endOffset": 34}, {"referenceID": 25, "context": "11 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008) 0.", "startOffset": 0, "endOffset": 29}, {"referenceID": 25, "context": "54 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008) 0.", "startOffset": 0, "endOffset": 29}, {"referenceID": 25, "context": "44 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008) 0.", "startOffset": 0, "endOffset": 29}, {"referenceID": 25, "context": "07 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010)", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008)", "startOffset": 0, "endOffset": 29}, {"referenceID": 18, "context": "For the latter, one could for example only select posts that have a certain length, ignore posts that are only quotes or captions to images, or expand posts by including content from linked html pages, which might provide larger and better contexts (Plank et al., 2014).", "startOffset": 249, "endOffset": 269}, {"referenceID": 14, "context": "For the former, namely the choice of Facebook pages, which we believe deserves the most investigation, one could explore several avenues, especially in relation to stance-based issues (Mohammad et al., 2016).", "startOffset": 184, "endOffset": 207}, {"referenceID": 22, "context": "Feature-wise, we want to train emotion-aware embeddings, in the vein of work by Tang et al. (2014), and Iacobacci et al.", "startOffset": 80, "endOffset": 99}, {"referenceID": 9, "context": "(2014), and Iacobacci et al. (2015). Retrofitting FB-embeddings trained on a larger corpus might also be successful, but would rely on an external lexicon.", "startOffset": 12, "endOffset": 36}], "year": 2016, "abstractText": "We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it\u2019s usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain.", "creator": "TeX"}}}