{"id": "1611.03125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "A Modular Theory of Feature Learning", "abstract": "learning representations of data, and in particular learning features for guiding a subsequent prediction task, has been a continuously fruitful public area of research delivering impressive empirical results in recent years. however, relatively little is understood about beyond what it makes a representation ` good '. we propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in selecting the risk of some learner participating using the learned features as compared to the original inputs. we describe a set of sufficient conditions for unsupervised representation learning to likely provide a benefit, as measured by this risk gap. these conditions decompose the problem of when representation learning works into its constituent parts, things which can be separately evaluated using an unlabeled sample, suitable domain - specific assumptions about the proposed joint distribution, and analysis of the feature learner and subsequent supervised learner. we provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low - dimensional manifold and when it forms clusters. we compare our approach to a recently proposed analysis of semi - supervised learning.", "histories": [["v1", "Wed, 9 Nov 2016 22:40:15 GMT  (385kb,D)", "http://arxiv.org/abs/1611.03125v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel mcnamara", "cheng soon ong", "robert c williamson"], "accepted": false, "id": "1611.03125"}, "pdf": {"name": "1611.03125.pdf", "metadata": {"source": "CRF", "title": "A Modular Theory of Feature Learning", "authors": ["Daniel McNamara", "Cheng Soon Ong", "Robert C. Williamson"], "emails": ["bob.williamson}@anu.edu.au"], "sections": [{"heading": null, "text": "Learning representations of data, and in particular learning features for a subsequent prediction task, has been a fruitful area of research delivering impressive empirical results in recent years. However, relatively little is understood about what makes a representation \u2018good\u2019. We propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in the risk of some learner using the learned features as compared to the original inputs. We describe a set of sufficient conditions for unsupervised representation learning to provide a benefit, as measured by this risk gap. These conditions decompose the problem of when representation learning works into its constituent parts, which can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about the joint distribution, and analysis of the feature learner and subsequent supervised learner. We provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low-dimensional manifold and when it forms clusters. We compare our approach to a recently proposed analysis of semi-supervised learning."}, {"heading": "1 Introduction", "text": "The predictive power of machine learning algorithms is highly dependent on the features that they receive as inputs. Traditionally, features have been handcrafted by domain experts. While this works well in some cases, it provides no performance guarantees and requires an expensive custom implementation for each new problem. Representation learning, also known as feature learning, entails automatically transforming input data to enhance the performance of prediction algorithms. In the last decade representation learning techniques using unlabeled data have been used to achieve empirical advances in areas such as computer vision [HS06] and natural language processing [MSC+13], and are expected to be at the forefront of further advances in machine learning [LBH15]. However, there are few theoretical results on when such techniques offer a benefit.\nThe main contribution of this paper is a set of sufficient conditions under which unsupervised representation learning provably improves task performance. These conditions can be evaluated using an unlabeled data sample, analysis of the proposed feature learner and supervised learner, and suitable assumptions about shared structure between the marginal distribution PX and the joint distribution PXY . The novelty of our result is its generality beyond any single representation learning technique and its theoretical rather than empirical approach. Furthermore we demonstrate the importance of considering the subsequent task for which the features will be used, including the supervised learner and loss function, in the definition of what makes \u2018good\u2019 features.\nThe paper is structured as follows. In Section 3 we set out sufficient conditions for unsupervised representation learning to be successful and describe its relationship with semi-supervised learning. In Section 4, we instantiate the conditions using the example of cluster structure in the unlabelled data, with a second example on manifold structure in the Supplement.\nar X\niv :1\n61 1.\n03 12\n5v 1\n[ cs\n.L G\n] 9\nN ov\n2 01"}, {"heading": "1.1 This is unlike standard learning theory papers", "text": "There are two important features of the paper worth calling out: 1) We analyse a processing pipeline, not just a single step. The use of sequential pipelines is common in practice, but rarely addressed theoretically. Our methodology seems novel in this regard. 2) We analyse the problem in terms of risk gaps, rather than sample complexity. This is illustrated in Figure 1, which we explain in detail in Section 3. While ultimately it is desirable to say something about finite sample performance, the current technology of bounds seems inadequate for the task at hand. Fortunately, by using risk bounds we can legitimately compare performance across the complex pipeline of processing inherent in the problem we address avoiding the impropriety of only comparing upper bounds.\nIn Section 2 we provide a comparison with current approaches to representation learning as well as existing theoretical results, which are largely focused on the limitations rather than the benefits of representation learning."}, {"heading": "2 Related work", "text": "Many representation learning techniques have been developed, including those using unlabeled data, and have achieved considerable empirical success [BCV13] but few theoretical guarantees concerning the effect on task performance. The literature to date demonstrates the usefulness of such techniques while also highlighting the need for more analysis of when and why they work.\nThe desire for computational efficiency has motivated techniques to learn low-dimensional manifold embeddings. Principal components analysis (PCA) is the canonical such technique, and has been extended to kernel variants such as Isomap, Laplacian eigenmaps and local linear embedding [MRT12]. It has been shown theoretically that it is possible to compress a finite set of high dimensional points to a low dimensional representation while bounding the distortion in pointwise Euclidean distances [JL84]. However, manifold learning approaches have typically not proved any improvement in the performance of a subsequent learner.\nEmpirical results in the field of deep learning have shown the power of learning multiple levels of representations. While more recent results have focused on supervised representation learning, initial advances used unsupervised techniques such as the autoencoder [HS06]. The effect of unsupervised pre-training has been studied empirically [EBC+10], with observed benefits in terms of both reduced training set error and improved generalization. While attempts have been made to theorize unsupervised representation learning in studies such as [SMG14] \u2014 which concluded that a certain kind of random initialization could achieve the same condition as unsupervised pre-training \u2014 mostly experimental results have outpaced theory. Such techniques often learn overcomplete representations (in a higher dimension than the original inputs), moving away from a paradigm of dimensionality reduction to one of learning features which are well-suited to the final classifier, which in neural networks is typically a linear separator.\nDespite these advances, theoretical results derived from information theory are pessimistic. Using learned features can never decrease the risk of the Bayes optimal classifier [vRW15]. This is because the set of hypotheses involving a feature transformation step followed by a prediction step is a subset of all possible hypotheses. This result is similar to the data processing inequality, which states that if random variables x, y, z form a Markov chain y \u2192 x \u2192 z, then I(z, y) \u2264 I(x, y) where I is mutual information [CT12]. Both results share the idea that information cannot be created by data manipulation. Hence, representation learning cannot be shown to be useful without considering the subsequent hypothesis learner."}, {"heading": "2.1 Relationship to existing representation learning approaches", "text": "Techniques aimed at learning linearly separable features can also be analyzed using our theorems. Our example (see Section 4.1), which involves exploiting cluster structure in the unlabeled data, shares the key objective of deep neural networks: learning a representation which will be linearly separable. Methods which aim to learn the kernel also share this objective, although the kernel function implicitly rather than explicitly defines the feature space.\nManifold learning techniques can be analyzed using our theorems, and is discussed in the Supplement. The conditions in these cases require that the unlabeled data lies near a low-dimensional manifold, that the manifold structure is related to the labels, and that the proposed reparametrization in terms of the manifold structure is compatible with the supervised learner used.\nA distinctive aspect of our approach is that it provides high probability performance guarantees in terms of the performance of a particular subsequent learner, unlike most unsupervised learning approaches where the objective function has no relation to the supervised task of interest [SJG+15]. The high probability risk bounds we present in our examples demonstrate the feasibility of our approach but in future could be tightened. Another original element is the use of property testing to determine which representation learning technique is best suited to the unlabeled data, rather than adopting a \u2018one size fits all\u2019 approach."}, {"heading": "2.2 Relationship to semi-supervised learning", "text": "The schematic presented in Figure 1 can be seen as a special case of a more general dilemma faced by machine learning practitioners. Given some existing system, an additional step is proposed. We would like to know under what circumstances such a step will enhance the system\u2019s performance. We may also characterize semi-supervised learning in this way, where the step is some particular use of unlabeled data. The differences between the two approaches are shown in Figure 2.\nIn semi-supervised learning, if a compatibility function exists which allows the elimination of incompatible hypotheses using only unlabeled data, performance may improve as the optimization will be more straightforward over a smaller hypothesis class and less sensitive to noise where few labeled data points are available. Furthermore, a tighter generalization error bound will be possible [BB10] (see Appendix B). However, if the target function lies outside the original hypothesis class, semi-supervised learning will not help to discover it.\nIn unsupervised representation learning, the hypothesis class changes and hence it is possible to learn hypotheses not included in the original hypothesis class. The cluster example (Section 4.1) illustrates this point. In some cases the size of the hypothesis class is reduced, such as using a lower-dimensional representation, so that a tighter generalization error bound is possible, or equivalently a reduction in sample complexity (see Theorem 1). This paper is more ambitious again in that it seeks to show that the risk upper bound using the learned representation is not only lower than the original risk upper bound, but lower than the original risk lower bound (see Theorem 2)."}, {"heading": "3 When unsupervised representation learning improves task performance", "text": "Our goal is to determine when unsupervised representation learning enhances the performance of a subsequent supervised hypothesis learner. This situation describes a range of common machine learning scenarios. Do the features learned by an autoencoder enhance the performance of a linear classifier compared to using the original inputs? Does unsupervised pre-training improve the performance of a supervised neural network? Does a particular kernel function outperform a linear kernel when used with a hypothesis class of linear separators (recalling that kernel functions implicitly specify a feature space)? Do distributed vector representations of words outperform one-hot unigram representations for natural language processing tasks? Our approach to estimating the effect of unsupervised representation learning is shown in Figure 1. We examine when it can be shown that the path including a representation learning step reduces risk."}, {"heading": "3.1 Problem statement", "text": "LetX , Y andZ be the input, output and learned feature spaces respectively. Let fL be an unsupervised feature learning algorithm, Su \u2208 Xmu be an unlabeled sample, and f : X \u2192 Z be the feature map learned using fL and Su. Let hL be a supervised hypothesis learner, Sl \u2208 {X \u00d7 Y }ml be a labeled sample, and h : X \u2192 Y be the hypothesis learned using hL and Sl. Let hZL be the supervised hypothesis learner accepting inputs in the learned feature space, SZl =\n\u22c3 {x,y}\u2208Sl {f(x), y} be the\nlabeled sample transformed into the learned feature space, and hZ : Z \u2192 Y be the hypothesis learned using hZL and S Z l . The risk of h is R(h) = E{x,y}\u223cPXY [L(h(x), y)], where L is a loss function. Similarly, let the risk of hZ using the feature map f be R(hZ \u25e6 f) = E{x,y}\u223cPXY [L(hZ(f(x)), y)]. The procedure in Figure 1 involves comparing a learner hL, which produces an hypothesis of type X \u2192 Y , with a learner hZL which produces an hypothesis of type Z \u2192 Y . While these learners have different type signatures, to isolate the effect of representation learning it will be convenient to consider situations where the two learners are similar; for example, both are logistic regression but accept inputs of different dimension. If it is possible to construct hZL from hL through a straightforward change of type signature without substantively changing the learner, we will refer to hL as \u2018transparently polymorphic\u2019 (see Supplement A.4 for further discussion)."}, {"heading": "3.2 Set of sufficient conditions", "text": "We provide a set of sufficient conditions for unsupervised representation learning to reduce the risk of a hypothesis learner, as shown in Table 1. We motivate these conditions by noting that each independent aspect of the prediction context (the source nodes in Figure 1) is associated with a condition. Hence we are unlikely to be able to reduce the set of conditions, a topic we discuss further in Supplement A.1. The conditions allow us to measure the effect of representation learning by separately analyzing specific aspects of the problem setting and then combining the results.\nThe conditions use a series of intermediate risk terms, measuring the extent to which particular properties of the prediction context hold. RA(PX) measures the extent to which some member of a class of structural properties holds for PX , Ra(PX) measures the extent to which some specific structural property holds, and R\u0302a(PX) is an empirical estimate of Ra(PX) calculated using the unlabeled sample Su and a property test. Furthermore, RB(PXY ) measures the extent to which the labeled distribution PXY shares structure with the unlabeled distribution, RC(f) measures the extent to which the learned feature map f exploits the structure of the unlabeled distribution, and RE(PXY ) measures the complexity of the joint distribution.\nWe show that, given an unlabeled sample and a specific prediction context, it is possible to obtain a high probability upper bound on the risk of a hypothesis learner using features learned from the unlabeled sample, and a high probability lower bound on the risk gap induced by using these learned features. A standard approach would be to compare the results of experiments on a supervised task with and without the representation learning step. Our alternative approach offers two benefits. First, it provides a guarantee on the benefit of representation learning for a class of tasks rather than requiring validation for each task individually. Second, it disaggregates why representation learning is effective, allowing the development of new techniques that are theoretically well-grounded.\nTheorem 1 shows that if a set of sufficient conditions hold, with high probability it is possible to upper bound the risk of a supervised learner using features learned from unlabeled data. The conditions can each be separately evaluated and collectively guarantee that the bound holds. The conditions require that PX has some testable structure, that PXY shares structure with PX , that the feature learner fL exploits the structure in PX , and that the hypothesis learner hL exploits the learned features. The bound Zmax achieved is specific to the prediction context (see Theorem 13 for an example). Hence the contribution of the theorem is the decomposition of the problem into tractable components, rather than a particular numerical bound. The result motivates a high level algorithm to select the best feature learner fL from a number of options, as shown in Algorithm 1 (see Supplement C). Such risk bounds can also be used to compute labeled sample complexity.\nTheorem 1 Suppose that given a sample Su drawn from PX and a property test, R\u0302a(PX) \u2264 \u0302A. Suppose also that given a feature learner fL and a hypothesis learner hL, it can be shown that with probability at least 1\u2212\u03b4, conditions A,B,C and D in Table 1 all hold. Then if a hypothesis hZ \u25e6f is constructed from Su and a sample Sl drawn from PXY as described in Section 3.1, with probability at least 1\u2212 \u03b4, R(hZ \u25e6 f) \u2264 Zmax.\nTheorem 2 shows that if additional conditions hold, with high probability unsupervised representation learning reduces risk compared to using the original inputs for some supervised learner. These additional conditions require that the joint distribution is \u2018hard\u2019 to learn and that the hypothesis learner cannot learn accurately from the original inputs. The result will be of interest when the risk gap \u2206R > 0. This is the main result of the paper as it decomposes the effect of unsupervised representation learning into a set of conditions that can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about PXY , and analytical properties of the feature\nlearner fL and hypothesis learner hL. Once again, the bound min \u2212 Zmax achieved is specific to the prediction context (see Theorem 3 for an example), rather than a particular numerical bound.\nTheorem 2 Suppose that given a sample Su drawn from PX and a property test, R\u0302a(PX) \u2264 \u0302A. Suppose also that given a feature learner fL and a hypothesis learner hL, it can be shown with probability at least 1\u2212 \u03b4, conditions A,B,C,D,E and F in Table 1 all hold. Then if hypotheses h and hZ \u25e6 f are constructed from Su and a sample Sl drawn from PXY as described in Section 3.1, with probability at least 1\u2212 \u03b4, \u2206R := R(h)\u2212R(hZ \u25e6 f) \u2265 min \u2212 Zmax.\nThe proofs of the above theorems essentially just string together the conditions. We instantiate Theorems 1 and 2 with two examples, in Section 4 and the Supplement."}, {"heading": "4 Application of Theorem 1 and 2: Cluster representation", "text": "We present an illustrative example of the sufficient conditions for unsupervised representation learning. The first learns a representation from cluster structure and demonstrates the application of Theorem 2. The second example in the supplement discusses manifold learning, and applies Theorem 1. In both examples we assume a bounded continuous input space X = [0, 1]n, a binary output space Y = {0, 1}, and a zero/one loss function L(y, y\u2032) = 1(y 6= y\u2032). We sketch the strategy used for our proofs. We prove each condition individually with high probability and then take a union bound to show that with high probability all conditions hold (see Supplement A.3 for a discussion of these high probability statements). We divide the input space into hypercubes and run an algorithm to test finitely many properties within some property class, yielding the property test result R\u0302a(PX). Condition A(PX) is achieved by reducing the measurement of a property of PX to binary classification with a finite hypothesis class, allowing the use of standard finite hypothesis class generalization error bounds. Condition B(PXY ) is assumed, where a domain expert specifies shared structure between PX and PXY if the property test passes. Condition C(fL) is derived by designing the property test such that it checks that the feature learner fL will work on PX . Condition D(hL) is shown using the fact that, with high probability, if a large enough labeled sample is drawn from a finite number of bins, the total probability mass of bins containing no labeled points will not be too large (see Lemma 10). In the second example, condition E(PXY ) on the complexity of the joint distribution is also provided by a domain expert. Condition F (PX , hL) is shown by adding the most favorable possible labels to the unlabeled sample Su, running hL on this training set to determine the minimum empirical risk achievable by a hypothesis learned by hL, and then using a standard VC dimension-based result to lower bound the risk of this hypothesis."}, {"heading": "4.1 Learning cluster representation improves risk", "text": "We present an example where unsupervised representation learning provably reduces risk. The example uses cluster structure in the unlabeled data to learn a representation where each cluster is\nmapped to a one-hot code, as shown in Figure 3. Assuming that the hypothesis learner learns a linear separator and that points within clusters share labels, in the new representation low risk will be achieved. In the original input space, any linear separator will achieve risk greater than some strictly positive threshold if the clusters are not linearly separable. Hence it is possible to prove that representation learning offers a benefit, as formalized in Theorem 3, which instantiates Theorem 2. This result will be meaningful when the risk gap is positive (see Figure 7 for an example).\nTheorem 3 Let R\u0302a(PX) be the result of the cluster property test described in Algorithm 2 run on an unlabeled sample Su. Let s be a side length parameter and k be the number of clusters (see Section 4.2). Let PXY satisfy Assumptions 5 and 8, fL be the feature learner and hL be the hypothesis learner in Section 4.2. Let \u03b2 be a lower bound on the empirical risk of a hypothesis learned by hL on a training set constructed by adding labels to Su according to some distribution PXY such that RB(PXY ) \u2264\nB and RE(PXY ) \u2265 E (see Supplement E.6). Let min := \u03b2 \u2212 \u221a 8(n+1) log 2emun+1 +8 log 12 \u03b4\nmu and\nZmax := 1 mu (s\u2212n log 2 + log 3\u03b4 ) + maxt\u2208[0,1] (k + 1 \u2212 \u03b43 (1 \u2212 t)\u2212ml)t. Suppose R\u0302a(PX) = 0. Then if hypotheses h and hZ \u25e6 f are constructed from Su and a labeled sample Sl, with probability at least 1\u2212 \u03b4, \u2206R := R(h)\u2212R(hZ \u25e6 f) \u2265 min \u2212 Zmax.\n4.2 Conditions A(PX), B(PXY ), C(fL), D(hL)\nWe consider a property which measures the extent to which PX is concentrated on disjoint clusters and describe an algorithm for testing whether this property approximately holds from a finite unlabeled sample. The quantity RA(PX) \u2208 [0, 1], defined below, describes the extent to which the property holds, with RA(PX) = 0 indicating that the property perfectly holds.\nGiven a distribution PX and a hypercube side length parameter s, let XA be the set of all sets Xa composed of disjoint regions and for which every point in a region included in Xa is near some point supported by PX (see Supplement E.3 for a formal definition). For some set Xa, let k = |Xa|, La(x) := 1(x 6\u2208\n\u22c3 Xi\u2208Xa Xi) and R\u0302a(PX) be the result of the property test described in Algorithm\n2, where if R\u0302a(PX) = 0 then La(x) = 0 for all x \u2208 Su. Let Ra(PX) := Ex\u223cPX [La(x)] and RA(PX) := min\nXa\u2208XA Ra(PX).\nLemma 4 Let Su be a sample drawn from PX and let R\u0302a(PX) be calculated using Su and the property test described in Algorithm 2. Let \u0302A := 0 and A := 1mu (s\n\u2212n log 2 + log 3\u03b4 ). With probability at least 1\u2212 \u03b43 , R\u0302a(PX) \u2264 \u0302A =\u21d2 Ra(PX) \u2264 A \u2227RA(PX) \u2264 A.\nWe adopt a variant of the cluster assumption, which has previously been used in the context of semi-supervised learning [Rig07, SNZ09]. We assume that nearby points share labels, given that the property test for clusteredness passes (see Section 4.2). We set B := 0, indicating strict within-cluster label agreement, but envisage relaxing this assumption in future work.\nLet \u03b3 := s\u221a n be a cluster separation parameter. Let d\u03b3 be a distance function such that d\u03b3(x0, xG) = 0\nif there is some path x0, . . . , xG such that G\u22121\u2227 i=0 (||xi \u2212 xi+1||2 \u2264 \u03b3) \u2227 G\u2227 i=0 (p(xi) > 0), otherwise d\u03b3(x0, xG) = 1. Let RB(PXY ) := E{x,y},{x\u2032,y\u2032}\u223cPXY [LB({x, y}, {x\u2032, y\u2032})], where LB({x, y}, {x\u2032, y\u2032}) := 1(d\u03b3(x, x\u2032) = 0)1(y 6= y\u2032).\nAssumption 5 Let B := 0. Assume RA(PX) \u2264 A =\u21d2 RB(PXY ) \u2264 B .\nHaving established that the marginal distribution PX has the property that the data lies in clusters, and making the assumption that points within clusters share labels, we now state the condition that the unsupervised feature learner fL exploits this cluster structure. We design the learner to produce a feature map f which maps all points within a cluster to the same point in the feature space.\nDefine fL as follows, yielding the feature map f . Run the property test described in Algorithm 2, which we assume passes and returns the set Xa. Set k = |Xa|. For Xi \u2208 Xa, for all x \u2208 Xi\nset f(x) to be a k-dimensional vector whose ith position is 1, and whose other positions are 0. For those points x \u2208 X for which x 6\u2208 Xi for all Xi \u2208 Xa, set f(x) to be the zero vector. Let RC(f) := Ex\u223cPX [LC(x, f)], where LC(x, f) := max{x\u2032:f(x)=f(x\u2032)} d\u03b3(x, x \u2032).\nLemma 6 Let C := A. Ra(PX) \u2264 A =\u21d2 RC(f) \u2264 C .\nLet hL be a learner which conducts empirical risk minimization over all linear classifiers of type X \u2192 Y , using the labeled sample Sl. Let hZL be a learner which conducts empirical risk minimization over all linear classifiers of type Z \u2192 Y , using the transformed labeled sample SZl , yielding the hypothesis hZL \u25e6 f . Note that the bound on R(hZ \u25e6 f) shown is independent of PXY given RB(PXY ) \u2264 B and RC(f) \u2264 C .\nLemma 7 Let B := 0 and Zmax := C + max t\u2208[0,1] (k + 1\u2212 \u03b43 (1\u2212 t)\u2212ml)t. With probability at least 1\u2212 \u03b43 , RB(PXY ) \u2264 B \u2227RC(f) \u2264 C =\u21d2 R(hZ \u25e6 f) \u2264 Zmax.\n4.3 Conditions E(PXY ), F (PX , hL)\nWe provide a lower bound on the risk of learning in the original input space rather than the learned representation. For this lower bound to be meaningful, we require that the joint distribution is \u2018difficult\u2019 to learn in some way. In particular, we assume that there is no label which is correct for almost all inputs.\nWithout loss of generality assume PXY has the property p(y = 0) \u2265 p(y = 1). Let RE(PXY ) = E{x,y}\u223cPXY [LE({x, y})], where LE({x, y}) = y.\nAssumption 8 Let E \u2208 (0, 12 ] be specified by a domain expert. Assume that RE(PXY ) \u2265 E .\nRecall that hL is a learner conducting empirical risk minimization on the labeled sample Sl over the set H of linear classifiers of type X \u2192 Y , yielding the hypothesis h. Assume that hL is guaranteed to select the hypothesis in H with the minimum empirical risk. We now provide a high probability lower bound on R(h), which is also dependent on PX , and will be meaningful when it is greater than zero. Note that this bound is independent of the number of labeled examples ml.\nLemma 9 Let B := 0, E \u2208 (0, 12 ] be specified by a domain expert, Su be a sample from PX and \u03b2 be a lower bound on the empirical risk of a hypothesis learned by hL on a training set constructed by adding labels to Su according to some distribution PXY such that RB(PXY ) \u2264 B and RE(PXY ) \u2265 E (see Supplement E.6). Let min := \u03b2 \u2212 \u221a 8(n+1) log 2emun+1 +8 log 12 \u03b4\nmu . With probability at least 1\u2212 \u03b43 , \u222b y PXY dy = PX\u2227RE(PXY ) \u2265 E\u2227RB(PXY ) \u2264 B =\u21d2 R(h) \u2265 min.\nWe have now stated all lemmas used by the proof of Theorem 3, which is the main result for the cluster representation example."}, {"heading": "5 Conclusion", "text": "We have developed a theory that explains when unsupervised pre-training works \u2014 a set of sufficient conditions which with high probability imply that a change of representation will improve task performance. These conditions depend jointly upon structural properties of the distribution, the feature learner, the subsequent supervised learner and the loss function used. We instantiated the conditions on an example which exploits the cluster structure, with a second example on manifold structure in the Supplement. Our approach of analysing a processing pipeline seems novel, where it seems necessary to consider risk gaps instead of sample complexity.\nThe modular nature of our argument is a central feature; it breaks an obscure black-box into understandable components. And furthermore, the instantiation of each component, for a particular problem, reduces to relatively standard application of existing techniques."}, {"heading": "A Motivation for and technical discussion of proposed sufficient conditions", "text": "We sketch the motivation for the sufficient conditions proposed for unsupervised representation learning, as well as technical aspects of these conditions which were omitted from the main text due to space requirements."}, {"heading": "A.1 Motivation for sufficient conditions", "text": "The process for evaluating the effect of unsupervised representation learning is described in Figure 1. By identifying source nodes which represent independent elements of the prediction context, we associate a condition with each such element, as stated in Table 1. Thus, although we do not formally demonstrate that the conditions are necessary for unsupervised representation learning, it appears unlikely that the conditions can be reduced further.\nFurthermore, the modular structure of the conditions means that the task of determining the effect of representation learning is \u2018factorized\u2019 over the different elements of the prediction context, making analysis more tractable. Given an unlabeled data sample, a feature learner, a supervised learner, and suitable assumptions about shared structure between the marginal distribution PX and the joint distribution PXY , it is possible to assess the effect of representation learning with high probability. Note that while PX is not independent of PXY , we treat PX as an independent element because PXY is unknown, and treat the assumption of shared structure in PXY as a separate independent element.\nThe conditions imply that the value of a particular representation will depend upon on the hypothesis learner. A representation which improves the performance of a particular hypothesis learner \u2014 for example, empirical risk minimization over the class of linear separators \u2014 may hinder the performance of another learner which can effectively learn in the original input space. Furthermore, abstracting away from a particular hypothesis learner to consider only the Bayes optimal classifier demonstrates that representation learning never helps in this setting [vRW15]. Consequently, it appears necessary to include dependence on a subsequent hypothesis learner when defining \u2018good\u2019 features.\nIt is clear that the values of R(hZ \u25e6 f) and R(h) also depend on the loss function L, and hence so do the conditions D and F . For brevity we suppress this in the notation used in Table 1 and Figure 1. We also note that there is typically a connection between the loss function and the hypothesis learner. The learner will typically optimize parameters according to the loss function, or some variant of the loss function that makes optimization easier and/or introduces regularization.\nWe propose an upper bound on R(hZ \u25e6 f) that is independent of PXY given that RB(PXY ) \u2264 B and RC(f) \u2264 C (see Table 1). This means that the bound applies to a wide class of distributions and therefore has the advantage of being quite general. It is also somewhat loose, as it does not consider PX itself, and a tighter bound which includes dependence on PX may be possible. On the other hand, to achieve a meaningful lower bound on R(h) we require dependence on PX since it is likely that for some choices of PX , R(h) is zero or small. This is the reason that condition F , which is associated with hL as shown in Figure 1, also depends on PX .\nA.2 Implication structure of sufficient conditions\nThe conditions allow a property test to assess the effect of representation learning through decomposition into a series of implications, as shown in Figure 4. The property test conducted on Su implies (condition A) both that PX has some specific property, which in our examples is concentration on a particular region, and that PX has some property within a property class, which in our examples is concentration on some region of a particular type. The specific property of PX allows an appropriately constructed feature map f to preserve the useful information in PX while moving to an input space that is simpler for a particular subsequent hypothesis learner to learn from (condition C). The property class of PX is a more natural piece of information to provide to a domain expert with a view to eliciting shared structure in PXY (condition B).\nThe upper bound on R(hZ \u25e6 f) depends both on the fact that f simplifies the structure of the inputs to be compatible with hypothesis learner hZL , and that the input structure is shared with the joint distribution PXY (condition D). The lower bound on R(h) involves adding labels to the unlabeled data, subject to some constraints, and using the results of empirical risk minimization on \u2018best case scenario\u2019 labels to provide a lower bound (condition F ). Such constraints can be constructed using the existing assumption about PXY required for the upper bound on R(hZ \u25e6 f), plus an additional assumption about the complexity of PXY (condition E).\nA.3 Interpretation of high probability statements\nThe statements made with probability at least 1 \u2212 \u03b4 should be interpreted as follows. For any distribution PXY , if an unlabeled sample Su and a labeled sample Sl are drawn, the following statements (where applicable) will all be true with probability at least 1\u2212 \u03b4.\n1. For all regions in some set of regions specified independently of Su, the total probability mass lying outside the region, Ra(PX), is not too much greater than the empirical estimate R\u0302a(PX) calculated from Su. (both examples, see Lemmas 14 and 4)\n2. For a set of regions specified independently of Sl, there exists some subset of these regions containing at least some fixed proportion of probability mass, for which Sl includes at least one point within each region in the subset. (both examples, see Lemmas 17 and 7)\n3. For all regions in some set of regions specified independently of Su, the total probability mass lying inside the region, pc, is not too much less than the empirical estimate p\u0302c calculated from Su. (manifold example only, see Lemma 16)\n4. For a fixed hypothesis class H , the risk R(h) of a hypothesis h \u2208 H learned using some labeling of Su is not too much less than some empirical quantity \u03b2 calculated from Su. (cluster example only, see Lemma 9)"}, {"heading": "A.4 Concept of transparent polymorphism", "text": "Our analysis allows the comparison of a pair of (potentially unrelated) supervised learners, but focuses on \u2018transparently polymorphic\u2019 learners to isolate the effect of the representation learning step. These are families of learning algorithms such that given a change of type signature the algorithm can straightforwardly be extended to the new type. The examples given in this text \u2014 empirical risk minimization over the class of linear classifiers, and 1-nearest neighbor using Euclidean distance \u2014 are examples of such learners. A precise definition of this class of learners is not attempted here and does not appear to have been well-studied elsewhere. A quantitative measure of the transparent polymorphism of a particular learner is of interest in its own right given the ubiquity of such learners\nin practical applications. It is easy to construct a learner which is not transparently polymorphic: for example, a learner which conducts logistic regression if it receives inputs with ten dimensions or fewer, and which uses a support vector machine if it receives inputs with more than ten dimensions. The same issue arises in relation to the number of labeled examples available to the learner. A \u2018transparently polymorphic\u2019 learner should behave predictably when the the number of labeled examples available to it is varied."}, {"heading": "B Generalization error bounds in semi-supervised learning", "text": "A formalization of semi-supervised learning is provided in [BB10], including improved generalization error bounds compared to supervised learning. We describe this approach to build on the comparison between unsupervised representation learning and semi-supervised learning presented in Section 2.2. For a hypothesis class H and input space X , define a compatibility function \u03c7 : H \u00d7X \u2192 [0, 1]. The incompatibility of a hypothesis h with respect to an unlabeled distribution PX is defined as RU (h) = 1\u2212Ex\u223cPX [\u03c7(h, x)], while the incompatibility of a hypothesis with respect to an unlabeled sample Su of mu points is R\u0302U (h) = 1\u2212 1mu \u2211 x\u2208Su \u03c7(h, x).\nThe benefit of semi-supervised learning rests on the idea that for the target function h\u2217, RU (h\u2217) = 0 in the simplest case, which we consider here, or in other cases RU (h\u2217) is small. Moreover, for only a small subset of hypotheses HPX ,\u03c7( ) \u2282 H , RU (h) \u2264 for all h \u2208 HPX ,\u03c7( ), so that the size of the hypothesis class is reduced. In this case we assume that the hypothesis class H is finite. For a hypothesis for which R\u0302U (h) = 0 and R\u0302(h) = 0, and a sufficiently large number of unlabeled examples mu, the number of labeled examples ml required to show that that with probability at least 1\u2212 \u03b4, R(h) \u2264 is:\nml = 1 (log |HPX ,\u03c7( )|+ log 2\n\u03b4 ) (1)\nAssuming that the compatibility function \u03c7 and marginal distribution PX allow a significant subset of the hypothesis space H to be pruned, the sample complexity is reduced in comparison to the typical supervised learning requirement (see Lemma 11).\nThe result can also be generalized to infinite hypothesis classes, although in this case it is not possible to directly obtain improved sample complexity using VC-dimension as a measure of hypothesis class size. Instead the authors propose the use of a distribution dependent measure, HPX ,\u03c7( )[m,PX ], which measures the expected number of splits of m points drawn from PX , using hypotheses h \u2208 H such that RU (h) \u2264 ."}, {"heading": "C Algorithm for selecting a feature learner", "text": "Algorithm 1 uses risk upper bounds to select a feature learner. It is presented as a high-level conceptual algorithm rather than as a tool for immediate practical use. The algorithm accepts a set of possible feature learners FL, an unlabeled sample Su, a hypothesis learner hL, the number of labeled samples ml, the level of certainty parameter \u03b4 and a dictionary of bounds. Each bound in the dictionary contains a property test for PX based on an unlabeled sample, a feature learner fL and a hypothesis learner hL to which the bound applies, and appropriate assumptions on PXY based on the results of the property test. Two examples of such bounds are provided in Section 4 of this paper.\nFor each bound, the algorithm runs the associated property test and using this computes the bound result. If the bound is tighter than any previous bound, then the current feature learner is selected as the best to date. The list of bounds should include a bound for the feature learner which simply returns the identity function. This approach is similar to structural risk minimization, which selects the hypothesis space minimizing a probabilistic upper bound on risk [Vap13].\nFunction select_feature_learner(FL,Su,hL,ml,\u03b4,bound_dictionary) best_bound_result:=Inf, best_fL:=null for each fL in FL do\nfor each bound in bound_dictionary do if bound[fL]=fL and bound[hL]=hL then\nproperty_test_result:=property_test(bound[test],Su) bound_result:=compute_bound(bound,property_test_result,ml,\u03b4) if bound_result<best_bound_result then\nbest_bound_result:=bound_result best_fL := fL\nend end\nend end return [best_fL, best_bound_result]\nAlgorithm 1: Selecting a feature learner using risk upper bounds"}, {"heading": "D Main theorem proofs and lemmas used in examples", "text": "We present the proofs of the main theorems. We also introduce a technical lemma and present standard generalization error bounds which are used in the proofs for the theorems associated with the examples."}, {"heading": "D.1 Proof of Theorem 1", "text": "Proof By the definitions in Table 1, R\u0302a(PX) \u2264 \u0302A \u2227A(PX) =\u21d2 Ra(PX) \u2264 A \u2227RA(PX) \u2264 A. RA(PX) \u2264 A\u2227B(PXY ) =\u21d2 RB(PXY ) \u2264 B . Also,Ra(PX) \u2264 A\u2227C(fL) =\u21d2 RC(f) \u2264 C . Furthermore, D(hL) \u2227 RB(PXY ) \u2264 B \u2227 RC(f) \u2264 C =\u21d2 R(hZ \u25e6 f) \u2264 Zmax. Therefore R\u0302a(PX) \u2264 \u0302A \u2227A(PX) \u2227B(PXY ) \u2227C(fL) \u2227D(hL) =\u21d2 R(hZ \u25e6 f) \u2264 Zmax. If R\u0302a(PX) \u2264 \u0302A and with probability at least 1\u2212 \u03b4, A,B,C and D all hold, then with at least the same probability the right hand side of the statement holds."}, {"heading": "D.2 Proof of Theorem 2", "text": "Proof By Theorem 1, R\u0302a(PX) \u2264 \u0302A\u2227A(PX)\u2227B(PXY )\u2227C(fL)\u2227D(hL) =\u21d2 R(hZ \u25e6f) \u2264 Zmax. Similarly, R\u0302a(PX) \u2264 \u0302A \u2227 A(PX) \u2227 B(PXY ) \u2227 E(PXY ) \u2227 F (PX , hL) =\u21d2 R(h) \u2265 min. If R\u0302a(PX) \u2264 \u0302A and with probability at least 1\u2212 \u03b4, A,B,C,D,E and F all hold, then with at least the same probability the right hand sides of both statements hold.\nD.3 Technical lemma on sample coverage of finite number of bins\nWe introduce and analyze a technical lemma which will be of use in establishing condition D(hL) in our examples. In a setting where a labeled sample is drawn from a finite number of bins, Lemma 10 provides a probabilistic upper bound on the total probability mass of the bins containing no labeled points. Figure 5 demonstrates that the bound is tightest when the labeled sample size is large relative to the number of bins.\nLemma 10 Assume that a labeled sample Sl of size ml is drawn from k bins. With probability at least 1 \u2212 \u03b4, the total probability mass of the bins containing no labeled points is no greater than \u03b1 := max\nt\u2208[0,1] g(t) = (k \u2212 \u03b4(1\u2212 t)\u2212ml)t. Furthermore, g(t) is concave.\nProof Within each of bin i lies some probability mass pi. Now consider the set Q containing the q bins with highest probability mass. Let t = min\nQ pi. With probability at least 1 \u2212 q(1 \u2212 t)ml , all\nof the q intervals contain at least one point in Sl. This can be shown by observing that for a single interval it will be empty with probability at most (1\u2212 t)ml and then applying the union bound over\nt\u2208[0,1]\ng(t) (right) is shown for various values of\nml and k, where again \u03b4 = 0.05.\nthe q intervals. Observe that the remaining intervals have mass at most (k \u2212 q)t and eliminate q by setting \u03b4 = q(1 \u2212 t)ml , yielding an upper bound on the mass of the remaining intervals of (k \u2212 \u03b4(1\u2212 t)\u2212ml)t =: g(t) if t is known. If t is unknown, the upper bound is \u03b1 := max\nt\u2208[0,1] g(t). Note\nthat q \u2264 1t so that the equation \u03b4 \u2264 1t (1\u2212 t)ml provides an upper bound on t. g(t) is concave since d2g dt2 = \u2212\u03b4ml(1\u2212 t)\u2212(ml+1)(1 + t(ml+1) 1\u2212t ) is non-positive for t \u2208 [0, 1].\nD.4 Standard generalization error bounds\nWe present standard generalization error bounds which we make use of in subsequent lemmas. Lemma 11 is used in the case of finite hypothesis classes, while Lemma 12 is used in the case of infinite hypothesis classes. The proofs for both lemmas appear in [MRT12]. In the case of Lemma 12, the bound is presented as an upper bound on R(h) with the square root term on the right hand side added to rather than subtracted from R\u0302(h). However, a symmetric lower bound can be straightforwardly derived using the same proof.\nLemma 11 For a hypothesis h drawn from a finite hypothesis class H learned from m examples for which R\u0302(h) = 0, with probability at least 1\u2212 \u03b4, R(h) \u2264 1m (log |H|+ log 1\u03b4 ).\nLemma 12 Let d be the VC dimension of hypothesis class H . For all hypotheses h drawn from H learned from m examples, with probability at least 1\u2212 \u03b4, R(h) \u2265 R\u0302(h)\u2212 \u221a 8d log 2emd +8 log 4 \u03b4\nm ."}, {"heading": "E Supplementary material for the cluster example", "text": "The parameters used for the cluster example in Section 4.1 are summarized in Table 2. Proofs for Theorem 3 and the lemmas used are shown subsequently. For a given set of parameters, we plot the risk gap induced by the learned features in Figure 7 (right), which depends both on the number of unlabeled points mu and the number of labeled points ml. In particular, the learned features are provably useful only when the risk gap is positive. This is not always the case, which is not surprising since we expect that a given representation learning technique will only be useful in certain prediction contexts."}, {"heading": "E.1 Kernel interpretation", "text": "It is possible to obtain a dual form of linear classifiers working with a kernel function k in the original input space, which is equivalent to taking a dot product in the feature space associated with the feature\nmap f . This may be written as k(x, x\u2032) = \u3008f(x), f(x\u2032)\u3009. By the definition of f(x) stated in Section 4.2, and setting cx and cx\u2032 to be the hypercubes in which x and x\u2032 lie respectively, we obtain:\nk(x, x\u2032) = 1, if \u2203x0, . . . , xG \u2208 Su s.t. x0 \u2208 cx \u2227 xG \u2208 cx\u2032 \u2227 G\u22121\u2227 i=0 ||xi \u2212 xi+1||2 \u2264 s \u221a n\n0, otherwise.\nThus the proposed representation can be viewed as a form of unsupervised kernel learning. However, in the general case of hypothesis learner hL, no kernel interpretation is possible because hypotheses learned will not in general have a dual form like linear classifiers."}, {"heading": "E.2 Proof of Theorem 3", "text": "Proof With probability at least 1\u2212 \u03b4, by the union bound, we have A(PX) by Lemma 4, B(PXY ) by Assumption 5, C(fL) by Lemma 6, D(hL) by Lemma 7, E(PXY ) by Assumption 8 and F (PX , hL) by Lemma 9. Applying Theorem 2, we have \u2206R \u2265 min \u2212 Zmax."}, {"heading": "E.3 Proof of Lemma 4", "text": "First, given a distribution PX and a side length parameter s, let \u03b3 := s\u221an and let XA be the set of all sets Xa with the following properties:\n1. Xa = k\u22c3 i=1 {Xi} for some finite k \u2265 2, where Xi \u2282 X for all i \u2264 k\n2. ||x\u2212 x\u2032||2 > \u03b3 for all x \u2208 Xi, x\u2032 \u2208 Xj , i 6= j 3. For all x \u2208 Xi there exists some point x\u2032 supported by PX such that ||x\u2212 x\u2032||2 \u2264 \u03b3.\nThe algorithm for testing for and learning cluster structure from unlabeled data is presented in Algorithm 2. The algorithm returns failure if it cannot find at least two disjoint clusters that all of Su lie within. In future this restriction could be relaxed to find clusters that most of the data lies within, allowing us to consider the case where \u0302A > 0.\nData: unlabeled sample Su, input space X , side length parameter s Result: Clusters (Xa) and proportion of Su not concentrated on clusters (R\u0302a), or failure Divide X into hypercubes of side s Set \u03b3 := s\u221a\nn\nBuild graph from Su, placing an edge between two points x and x\u2032 if ||x\u2212 x\u2032||2 \u2264 \u03b3 Extract components of graph, where Ci is the set of points in the ith component Set Xa := {} for each component Ci do\nSet Xi := {} for each point x \u2208 Ci do\nFind the hypercube cx such that x \u2208 cx Set Xi := Xi \u222a cx\nend Set Xa := Xa \u222a {Xi}\nend if |Xa| \u2265 2 then\nR\u0302a := 0 return {Xa, R\u0302a};\nelse return failure end Algorithm 2: Testing for and learning cluster structure\nWe now state the proof of Lemma 4.\nProof If R\u0302a(PX) = 0, then Algorithm 2 succeeds and also returns some Xa \u2208 XA. For each set Xa \u2208 XA define a hypothesis ha, where ha(x) = 1 if x \u2208 \u22c3 Xi\u2208Xa Xi, and ha(x) = 0 otherwise. Let HA be the set of such hypotheses, where |HA| \u2264 2s \u2212n\n. Let R(ha) := Ex\u223cPX [L(ha(x), 1)] and R\u0302(ha) :=\n1 mu \u2211 x\u2208Su L(ha(x), 1), recalling that L is 0/1 loss.\nBecause R\u0302(ha) = 1mu \u2211 x\u2208Su La(x) by definition, and 1mu \u2211 x\u2208Su La(x) = 0 if R\u0302a(PX) = 0 by the\nconstruction of Algorithm 2, we have R\u0302(ha) = 0. Hence we may apply Lemma 11, which yields the required high probability upper bound on R(ha). Since R(ha) = Ra(PX) by definition, the same bound holds for Ra(PX). The bound also holds for RA(PX), since RA(PX) := min\nXa\u2208XA Ra(PX)."}, {"heading": "E.4 Proof of Lemma 6", "text": "Proof By the construction of fL, for all points x such that f(x) is not the zero vector, LC(x, f) := max\n{x\u2032:f(x)=f(x\u2032)} d\u03b3(x, x\n\u2032) = 0. These points x are precisely those points for which\nLa(x) = 0. Therefore if Ra(PX) \u2264 A, then RC(f) \u2264 A."}, {"heading": "E.5 Proof of Lemma 7", "text": "Proof Applying f to PX induces a distribution supported at k + 1 points. With probability at least 1\u2212 \u03b43 , Ex\u223cPX [ min{f(x\u2032),y\u2032}\u2208SZl 1(f(x) 6= f(x\u2032))] \u2264 max t\u2208[0,1] (k+ 1\u2212 \u03b43 (1\u2212 t)\u2212ml)t =: \u03b1, by Lemma 10.\nRecall that by the definition of RC(f), if we randomly draw a point x from PX then with probability at most C there exists some x\u2032 such that d\u03b3(x, x\u2032) > 0 and f(x) = f(x\u2032). Combining the last two bounds by the union bound, with probability at most \u03b1 + C there is either no point {f(x\u2032), y\u2032} \u2208 SZl such that f(x) = f(x\u2032), or d\u03b3(x, x\u2032) > 0 for at least one point {f(x\u2032), y\u2032} \u2208 SZl such that f(x) = f(x\u2032). If neither of these possibilities occur, hZ \u25e6 f will be able to correctly classify the point. This is because there is at least one training set point such that f(x) = f(x\u2032), for all points in the training set such that f(x) = f(x\u2032) we have d\u03b3(x, x\u2032) = 0, we know that if d\u03b3(x, x\n\u2032) = 0 then the labels of x and x\u2032 must agree because RB(PXY ) = 0, and finally we know that hZ \u25e6 f will achieve zero training set error since a linear classifier in k dimensions can perfectly classify k+1 points. Therefore we have with probability at least 1\u2212 \u03b43 , R(hZ \u25e6f) \u2264 C +\u03b1."}, {"heading": "E.6 Proof of Lemma 9", "text": "Proof Run the property testing algorithm described in Algorithm 2 to obtain k = |Xa|. Run 1 2k(k \u2212 1) tests of the following kind, for each pair Xi, Xj \u2208 Xa, i 6= j. Construct the labeled set Siju which includes only those points in Su that lie in Xi \u222aXj , adding the labels y = 1 for x \u2208 Xi and y = 0 for x \u2208 Xj . Run empirical risk minimization on Siju to produce the hypothesis hij , which we assume has the minimum empirical risk of any hypothesis in H . Let Su be the set of all possible labeled samples constructed by adding labels to Su, subject to the constraints RB(PXY ) = 0 and RE(PXY ) \u2265 E . If these constraints hold, we have for all h \u2208 H: \u03b2\n:= min i,j 1 mu \u2211 {x,y}\u2208Siju L(hij(x), y)\n\u2264 min i,j 1 mu \u2211 {x,y}\u2208Siju L(h(x), y)\n\u2264 min S\u2217u\u2208Su 1 mu \u2211 {x,y}\u2208S\u2217u L(h(x), y).\n= min S\u2217u\u2208Su R\u0302(h).\nThe first inequality holds because hij was constructed through empirical risk minimization guaranteed to find the hypothesis in H with the minimum empirical risk over Siju . The second inequality holds since RB(PXY ) = 0 implies that the labels of points within clusters agree and RE(PXY ) \u2265 E > 0 implies that at least one pair of clusters must have different labels. The third equality holds by the definition of empirical risk R\u0302(h) for some sample S\u2217u.\nUsing this inequality and the high probability lower bound on R(h) in terms of R\u0302(h) provided by Lemma 12, noting that the VC dimension of the class of linear classifiers in n dimensions is n+ 1, yields the required result."}, {"heading": "F Example: Manifold representation", "text": "We present an example which exploits low dimensional manifold structure present in the unlabeled distribution to learn a new representation, as shown in Figure 6. A toy setting for this example is the binary classification problem of determining whether there are crocodiles in a particular section of a river. Crocodiles tend to concentrate in particular regions of the river, since they swim along it and cannot move over land. A representation parametrized by river position rather than latitude and longitude will allow a nearest-neighbor algorithm to better learn where there are crocodiles.\nA probabilistic upper bound on the risk of a subsequent supervised learner using the learned representation is shown in Theorem 13, which instantiates Theorem 1. This result will be meaningful when the bound is less than 1 (see Figure 7 for an example). In this case the learner is the 1-nearest neighbor algorithm using Euclidean distance. Our approach is similar to density-based distances [BRS12], except that we use the density to learn an explicit representation and continue to use Euclidean distance in the learned feature space.\nThe parameters used for the manifold example in Section F are summarized in Table 3. Proofs for Theorem 13 and the lemmas used are shown subsequently. For a given set of parameters, we plot the upper bound on risk using the learned features in Figure 7 (left), which depends both on the number of unlabeled points mu and the number of labeled points ml.\nTheorem 13 Let R\u0302a(PX) be the result of the manifold property test described in Algorithm 3 run on an unlabeled sample Su. Let s be a side length parameter and \u03b3 be a manifold length parameter (see Section F.1). Let j be a proximity parameter and B be a label agreement parameter (see Section F.2). Let Zmax := 1 mu (n\u03b3s log 3 \u2212 n log s + log 3\u03b4 ) + B + maxt\u2208[0,1]( \u03b3 js \u2212 \u03b43 (1 \u2212 t)\u2212ml)t. Suppose\nR\u0302a(PX) = 0, PXY satisfies Assumption 15, fL is the feature learner in Section F.3, and hL is the hypothesis learner in Section F.4. Then if a hypothesis hZ \u25e6 f is constructed from Su and a labeled sample Sl, with probability at least 1\u2212 \u03b4, R(hZ \u25e6 f) \u2264 Zmax.\nProof With probability at least 1\u2212 \u03b4, by the union bound, we have A(PX) by Lemma 14, B(PXY ) by Assumption 15, C(fL) by Lemma 16 and D(hL) by Lemma 17. Applying Theorem 1 we have R(hZ \u25e6 f) \u2264 Zmax.\nIn principle it should also be possible to provide a lower bound on the performance of the subsequent supervised learner using the original inputs. We found that the distribution-independent upper bound presented here, which is tighter with more labeled samples, is not tight enough to be less than such a distribution-specific lower bound, which is tighter with few labeled samples (see Appendix A.1). In future our upper bound may be tightened by creating dependence on the distribution PX .\nF.1 Condition A(PX)\nWe consider a property which measures the extent to which PX is concentrated on particular type of one-dimensional manifold, and describe an algorithm for testing this property from an unlabeled sample. The quantity RA(PX) \u2208 [0, 1], defined below, describes the extent to which the property holds, with RA(PX) = 0 indicating that it perfectly holds. While we analyze a particular class of one-dimensional manifolds, in future we envisage that this restriction can be relaxed.\nGiven a distribution PX , a hypercube side length parameter s and a manifold length parameter \u03b3, let XA be the set of all regions that form a one-dimensional manifold on which PX is approximately concentrated (see the proof for a formal definition). For some region Xa, let La(x) := 1( min\nx\u2032\u2208Xa ||x\u2212\nx\u2032||2 > s \u221a n\n2 ) and R\u0302a(PX) be a quantity calculated by the property test described in Algorithm 3, where if R\u0302a(PX) = 0 then La(x) = 0 for all x \u2208 Su. Let Ra(PX) := Ex\u223cPX [La(x)] and RA(PX) := min\nXa\u2208XA Ra(PX).\nLemma 14 Let Su be a sample drawn from PX and let R\u0302a(PX) be calculated using Su and the property test described in Algorithm 3. Let \u0302A := 0 and A := 1mu ( n\u03b3 s log 3\u2212 n log s+ log 3\u03b4 ). With probability at least 1\u2212 \u03b43 , R\u0302a(PX) \u2264 \u0302A =\u21d2 Ra(PX) \u2264 A \u2227RA(PX) \u2264 A.\nFirst, given a distribution PX , a hypercube side length parameter s and a manifold length parameter \u03b3, we define XA be the set of all regions Xa \u2286 X such that:\n1. Xa is a connected, non-self-intersecting curve of length at most \u03b3\n2. For all x \u2208 Xa, there exists some point x\u2032 supported by PX such that ||x\u2212 x\u2032||2 < s \u221a n\n2 .\nThe algorithm for testing for and learning a one-dimensional manifold is presented in Algorithm 3. The algorithm adopts a depth-first search strategy and returns failure if it cannot find a onedimensional manifold that all of Su lies near. In future this restriction could be relaxed to find a manifold that most of the data lies near, allowing us to consider the case where \u0302A > 0.\nData: unlabeled sample Su, input space X , side length s, manifold length \u03b3 Result: Curve (Xa) and proportion of Su not concentrated on Xa (R\u0302a), or failure Divide X into hypercubes of side s for each hypercube c do\nif c is not empty then path:=explore(c,0) if path is not failure then\nXa := curve connecting the centers of the hypercubes in the order specified by path R\u0302a := 0\nreturn {Xa, R\u0302a} end\nend end return failure\nFunction explore(path,path_distance) if path_distance> \u03b3 then\nreturn failure end for i =1:length(path)-n do\nif path[length(path)]=path[i] then return failure\nend end if all squares not on path are empty then\nreturn path end for each non-empty neighbor of path[length(path)] not currently on path do\nnew_path:=[path,neighbor] distance_added:=distance from path[length(path)] to center of neighbor new_path_distance:=path_distance+distance_added if explore(new_path) is not failure then\nreturn explore(new_path,new_path_distance) end\nend return failure\nAlgorithm 3: Testing for and learning a one-dimensional manifold\nWe now state the proof of Lemma 14.\nProof If R\u0302a(PX) = 0, then Algorithm 3 succeeds and also returns some Xa \u2208 XA. For each region Xa \u2208 XA define a hypothesis ha, where ha(x) = 1 if 1( min\nx\u2032\u2208Xa ||x \u2212 x\u2032||2 \u2264 s\n\u221a n 2 ), and ha(x) = 0\notherwise. Let HA be the set of such hypotheses, where |HA| \u2264 s\u2212n(3n) \u03b3 s , since the path may start at any of the s\u2212n hypercubes and then may take at most \u03b3s steps, each of which must be to one of at most 3n options including neighboring hypercubes or remaining at the same hypercube if the\npath length is less than \u03b3. Let R(ha) := Ex\u223cPX [L(ha(x), 1)] and R\u0302(ha) := 1mu \u2211 x\u2208Su L(ha(x), 1), recalling that L is 0/1 loss.\nBecause R\u0302(ha) = 1mu \u2211 x\u2208Su La(x) by definition, and 1mu \u2211 x\u2208Su La(x) = 0 if R\u0302a(PX) = 0 by the\nconstruction of Algorithm 3, we have R\u0302(ha) = 0. Hence we may apply Lemma 11, which yields the required high probability upper bound on R(ha). Since R(ha) = Ra(PX) by definition, the same bound holds for Ra(PX). The bound also holds for RA(PX), since RA(PX) := min\nXa\u2208XA Ra(PX).\nF.2 Condition B(PXY )\nGiven that the data lies on a low dimensional manifold, we assume that close points as measured by distance along the manifold are likely to share labels. The discovery of manifold structure alone may not be useful for prediction, unless the joint distribution also exhibits shared structure.\nLet C be the set of all hypercubes containing at least one point in Su, pc be the probability mass within hypercube c and p\u0302c be the empirical estimate of this mass obtained from the sample Su. Let r \u2264 min\nc\u2208C (min{pc|P [Bin(mu; pc) \u2265 p\u0302cmu] \u2265 s\u03b43\u03b3 }) be a parameter indicating the minimum\nprobability mass contained in small regions along the manifold. Let dr,\u221ans(x, x\u2032) be the length of the shortest path between x and x\u2032 such that for all points x\u2032\u2032 along the path the following condition holds: there exists a region T such that max\nxT\u2208T ||x\u2032\u2032 \u2212 xT ||2 \u2264\n\u221a ns and \u222b T p(xT )dxT \u2265 r. Let j be a\nparameter controlling how close points must be such that they are likely to share labels.\nLet RB(PXY ) := E{x,y}\u223cPXY [ max{x\u2032,y\u2032} LB(y, y \u2032)], where the maximization is over points {x\u2032, y\u2032} : p(x\u2032, y\u2032) > 0 \u2227 dr,\u221ans(x, x\u2032) \u2264 (j + 1) \u221a ns, and LB(y, y\u2032) := 1(y 6= y\u2032). A small value of RB(PXY ) indicates that for most points x, all points x\u2032 that are close in terms of manifold distance in the sense that dr,\u221ans(x, x\u2032) \u2264 (j + 1) \u221a ns, will have the same label as x.\nAssumption 15 Let B \u2208 [0, 1] be specified by a domain expert. Assume that RA(PX) \u2264 A =\u21d2 RB(PXY ) \u2264 B .\nF.3 Condition C(fL)\nWe specify a feature learner fL which, if one-dimensional manifold structure can be detected, exploits this structure to learn a representation. Define fL as follows, yielding the feature map f . Run the property test described in Algorithm 3, which we assume passes and returns the curve Xa. Let x0 be the center of the first hypercube on the curve and set f(x) = 0 for all points in this hypercube. For points x lying in other hypercubes through which Xa passes, set f(x) to be the distance along Xa from x0 to the center of the point\u2019s hypercube. For points x lying in hypercubes whose centers are not in Xa, set f(x) to be some constant such that f(x) \u2212\u03b3. We would now like to quantify the probability that if f(x) and f(x\u2032) are close, points x and x\u2032 are close in the original space as measured by the shortest distance of a path between them through regions of high probability density. Let RC(f) := Ex\u223cPX [ max\nx\u2032:|f(x)\u2212f(x\u2032)|\u2264js LC(x, x\n\u2032)] and\nLC(x, x \u2032) := 1(dr, \u221a ns(x, x \u2032) > (j + 1) \u221a ns). Note that the definition of RC(f) depends on the parameter r, which is upper bounded by an expression dependent on \u03b4.\nLemma 16 Let C := A. With probability at least 1\u2212 \u03b43 , Ra(PX) \u2264 A =\u21d2 RC(f) \u2264 C .\nProof Recall that pc is the probability mass within hypercube c and p\u0302c is the empirical estimate of this mass obtained from the sample Su. With probability at least 1\u2212 s\u03b43\u03b3 we have the following for a single hypercube c whose center lies on Xa:\npc\n\u2265 min{pc|P [Bin(mu; pc) \u2265 p\u0302cmu] \u2265 s\u03b43\u03b3 } \u2265 r.\nThere are at most \u03b3s hypercubes whose centers lie on Xa. By the union bound, with probability at least 1 \u2212 \u03b43 , pc \u2265 r for all of these hypercubes. In that case, if x is in a hypercube whose center lies on Xa, then for all x\u2032, if |f(x) \u2212 f(x\u2032)| \u2264 js then there must be a path between x and x\u2032 passing through at most j + 1 hypercubes each of which contains probability mass at least r. Such a path must be of length at most (j + 1) \u221a ns. Therefore for such values of x, for all x\u2032, |f(x) \u2212 f(x\u2032)| \u2264 js =\u21d2 dr,\u221ans(x, x\u2032) \u2264 (j + 1) \u221a ns and and hence LC(x, x \u2032) = 0. Since RA(PX) \u2264 A, the chances of drawing some x in a hypercube whose center does not lie onXa is at most A. ThereforeRC(f) \u2264 A, still with probability at least 1\u2212 \u03b43 .\nF.4 Condition D(hL)\nLet hL be the 1-nearest neighbor learner using Euclidean distance trained on the labeled sample Sl, yielding the hypothesis h(x) = y\u2217, where {x\u2217, y\u2217} = argmin\n{x\u2032,y\u2032}\u2208Sl ||x\u2212 x\u2032||2. Similarly, let hZL be the\n1-nearest neighbor learner using Euclidean distance trained on the transformed labeled sample SZl , yielding the hypothesis hZ(f(x)) = y\u2217, where {f(x\u2217), y\u2217} = argmin\n{f(x\u2032),y\u2032}\u2208SZl |f(x) \u2212 f(x\u2032)|. Note\nthat the bound on R(hZ \u25e6 f) shown is independent of PXY given RB(PXY ) \u2264 B and RC(f) \u2264 C .\nLemma 17 Let Zmax := max t\u2208[0,1] ( \u03b3js \u2212 \u03b43 (1 \u2212 t)\u2212ml)t + B + C . With probability at least 1 \u2212 \u03b43 , RB(PXY ) \u2264 B \u2227RC(f) \u2264 C =\u21d2 R(hZ \u25e6 f) \u2264 Zmax.\nProof Partition [0, \u03b3] into \u03b3js intervals of equal width. For a point f(x) in an interval containing at least one point in the sample SZl , min{f(x\u2032),y\u2032}\u2208SZl |f(x)\u2212 f(x\u2032)| \u2264 js. With probability at least 1\u2212 \u03b43 , Ex\u223cPX [1( min {f(x\u2032),y\u2032}\u2208SZl |f(x)\u2212 f(x\u2032)| > js)] \u2264 max t\u2208[0,1] ( \u03b3js \u2212 \u03b43 (1\u2212 t)\u2212ml)t =: \u03b1, by Lemma 10.\nRecall that by the definition of RC(f), if we randomly draw a point x from PX , the probability that there exists some x\u2032 such that |f(x)\u2212 f(x\u2032)| \u2264 js and dr,\u221ans(x, x\u2032) > (j + 1) \u221a ns is at most C . Furthermore recall by the definition of RB(PXY ), if we randomly draw a point {x, y} from PXY , the probability that there exists some supported {x\u2032, y\u2032} such that dr,\u221ans(x, x\u2032) \u2264 (j + 1) \u221a ns and y\u2032 6= y is at most B . Combining the last three bounds by the union bound, with probability at most \u03b1 + C + B , for a point {x, y} randomly drawn from PXY , there is either no point {f(x\u2032), y\u2032} \u2208 SZl such that |f(x) \u2212 f(x\u2032)| \u2264 js, or dr,\u221ans(x, x\u2032) > (j + 1) \u221a ns for at least one point {f(x\u2032), y\u2032} \u2208 SZl such that |f(x) \u2212 f(x\u2032)| \u2264 js, or y\u2032 6= y for at least one point in {f(x\u2032), y\u2032} \u2208 SZl such that dr,\u221ans(x, x\u2032) \u2264 (j + 1) \u221a ns. If none of these possibilities occur, a hypothesis hZ \u25e6 f constructed using the 1-nearest neighbor learner and SZl will be able to correctly classify x. Therefore we have with probability at least 1\u2212 \u03b43 , R(hZ \u25e6f) \u2264 \u03b1+ B+ C as required.\nWe have now stated all lemmas used by the proof of Theorem 13, which is the main result for the manifold representation example."}], "references": [{"title": "A discriminative model for semi-supervised learning", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "Journal of the ACM,", "citeRegEx": "Balcan and Blum.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2010}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Semi-supervised learning with density based distances", "author": ["Avleen S. Bijral", "Nathan Ratliff", "Nathan Srebro"], "venue": "In Uncertainty in Artificial Intelligence 2011 Proceedings,", "citeRegEx": "Bijral et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bijral et al\\.", "year": 2012}, {"title": "Elements of Information Theory", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E. Hinton", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption", "author": ["Philippe Rigollet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rigollet.,? \\Q2007\\E", "shortCiteRegEx": "Rigollet.", "year": 2007}, {"title": "Towards Principled Unsupervised Learning", "author": ["Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals"], "venue": "[cs],", "citeRegEx": "Sutskever et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["Aarti Singh", "Robert Nowak", "Xiaojin Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Singh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "A Theory of Feature Learning", "author": ["Brendan van Rooyen", "Robert C. Williamson"], "venue": "arXiv preprint arXiv:1504.00083,", "citeRegEx": "Rooyen and Williamson.,? \\Q2015\\E", "shortCiteRegEx": "Rooyen and Williamson.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Learning representations of data, and in particular learning features for a subsequent prediction task, has been a fruitful area of research delivering impressive empirical results in recent years. However, relatively little is understood about what makes a representation \u2018good\u2019. We propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in the risk of some learner using the learned features as compared to the original inputs. We describe a set of sufficient conditions for unsupervised representation learning to provide a benefit, as measured by this risk gap. These conditions decompose the problem of when representation learning works into its constituent parts, which can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about the joint distribution, and analysis of the feature learner and subsequent supervised learner. We provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low-dimensional manifold and when it forms clusters. We compare our approach to a recently proposed analysis of semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}