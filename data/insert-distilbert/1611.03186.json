{"id": "1611.03186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "SoK: Applying Machine Learning in Security - A Survey", "abstract": "naturally the idea of applying machine learning ( ml ) to solve problems in security domains is almost 3 decades old. as information and communications grow more sufficiently ubiquitous and more data become available, many key security risks arise as well... as appetite to manage and not mitigate such risks. consequently, research on applying and designing ml algorithms and systems for security has grown fast, ranging from intrusion detection systems ( ids ) and malware classification to security policy management ( spm ) and information engineering leak checking. in this paper, we systematically efficiently study the methods, algorithms, and system designs in academic publications from 2008 - 2015. that actively applied ml algorithm in security domains. 98 different percent of the surveyed 2 papers appeared in the 6 highest - ranked academic security conferences and 1 conference known for pioneering ml applications in security. we examine the generalized system designs, regarding underlying assumptions, measurements, and use cases in active research. our examinations lead to 1 ) a taxonomy on ml engineering paradigms : and security domains for future exploration and exploitation,, and 2 ) an agenda detailing open and theoretical upcoming challenges. first based on our survey, 2015 we also suggest a point leave of view module that treats security as a game theory theory problem instead of a batch - trained ml problem.", "histories": [["v1", "Thu, 10 Nov 2016 05:08:02 GMT  (238kb,D)", "http://arxiv.org/abs/1611.03186v1", "18 pages, 2 figures, 11 tables"]], "COMMENTS": "18 pages, 2 figures, 11 tables", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["heju jiang", "jasvir nagra", "parvez ahammad"], "accepted": false, "id": "1611.03186"}, "pdf": {"name": "1611.03186.pdf", "metadata": {"source": "CRF", "title": "SoK: Applying Machine Learning in Security - A Survey", "authors": ["Heju Jiang", "Jasvir Nagra", "Parvez Ahammad"], "emails": ["}@instartlogic.com"], "sections": [{"heading": "1. INTRODUCTION AND MOTIVATION", "text": "Since Dorothy Denning\u2019s seminal 1987 paper on intrusion detection [1], ML and data mining(DM) have steadily gained attention in security applications. DARPA\u2019s 1998 network intrusion detection evaluation [2], and KDD(Conference on Knowledge Discovery and Data Mining) Cup\u2019s 1999 challenge [3, 4] have raised profile of ML in security contexts. Yet, constrained by hardware and system resources[4], largescale ML applications did not receive much attention for many years.\n\u2217Corresponding authors\nIn 2008, ACM Conference on Computer and Communications Security(CCS) hosted the 1st Artificial Intelligence in Security(AISec) Workshop, which has since been a dedicated venue at a top-level security conference for the intersection of ML and security. From 2008, the pace of research and publicity of ML in security started to accelerate in academic communities (section 2.3), and industry venues (e.g. Black Hat, RSA) also shifted interests. For instance, ML in security was still a topic of minority interest at Black Hat USA 2014 in August [5], but at RSA 2016 in February, the majority of vendors claimed to deploy ML in their products [6]. A part of this shift may be motivated by the sudden increase in blackswan events like the discovery of CRIME, BEAST and Heartbleed vulnerabilities. The discovery of these vulnerabilities suggest that organizations may be attacked via previously unknown classes of attacks. To defend against these types of attacks requires monitoring not just for known vectors attacks, but also for behavior suggestive of a compromised machine. The latter requires the gathering and analysis of much larger sets of data.\nAdvances in hardware and data processing capacities enabled large-scale systems. With increasing amount of data from growing numbers of information channels and devices, the analytic tools and intelligent behaviors provided by ML becomes increasingly important in security. With DARPA\u2019s Cyber Grand Challenge final contest looming [7], research interest in ML and security is becoming even more conspicuous. Now is the crucial time to examine research works done in ML applications and security. To do so, we studied the state-of-art of ML research in security between 2008 and early 2016, and systematize this research area in 3 ways:\n1. We survey cutting-edge research on applied ML in security, and provide a high-level overview taxonomy of ML paradigms and security domains. 2. We point to research challenges that will improve, enhance, and expand our understanding, designs, and efficacy of applying ML in security. 3. We emphasize a position which treats security as a game theory problem."}, {"heading": "2. OVERVIEW: STRUCTURE & SCOPE", "text": "While we realize there are different ways to classify existing security problems based on purpose, mechanism, targeted assets, and point of flow of the attack, our SoK\u2019s section structure is based on the \u201cSecurity and Privacy\u201d category of 2012 ACM Computing Classification System[8], which is a\nar X\niv :1\n61 1.\n03 18\n6v 1\n[ cs\n.C R\n] 1\n0 N\nov 2\n01 6\ncombination of specific use cases(e.g. malware, phishing), technique (e.g. information flow), and targeted assets(e.g. web application, proxies). We present the state-of-art ML applications in security as the following: Section 3 and Table 2 & 3 discusses Network Security1, Section 4 and Table 4 surveys Security Services, Section 5 and Table 5 specifies advances in Software & Applications Security, Section 6 and Table 6 & 7 lays out taxonomy for System Security, and Section 7 and Table 8, 9 & 10 summarizes progress since 2008 in Malware Detection, IDS, and Social Engineering. Throughout the survey, we share our frameworks for ML system designs, assumptions, and algorithm deployments in security.\nWe focus our survey on security applications and securityrelated ML and AI problems on the defense side, hence our scope excludes theories related to security such as differential privacy and privacy-preservation in ML algorithms[9, 10, 11], and excludes ML applications in side channel attacks such as [12, 13, 14]. Partly because there is already a 2013 SoK on evolution of Sybil defense[15] in online social networks(OSN), and partly because we would like to leave it as a small exercise to our readers, we excluded Sybil defense schemes in OSN as well[16, 17, 18, 19, 20]. Still with a broad base, we propose an alternative position to frame security issues, and we also recommend a taxonomy for ML applications in security use cases. Yet, we do not conclude with a terminal list of \u201cright\u201d or \u201ccorrect\u201d approaches or methods. We believe that the range of the applications is too wide to fit into one singular use case or analysis framework. Instead, we intend this paper as a systematic design and method overview of thinking about researching and developing ML algorithms and applications, that will guide researchers in their problem domains on an individual basis. We target our work to security researchers and practitioners, so we assume that our readers have general knowledge for key security domains and awareness of common ML algorithms, and we also define terms when needed.\nWe agree with assessment of top conferences in [21]2. We\n1All papers are listed in chronological order with the first author\u2019s last name followed by venue acronym and year. 2We use the same conference-ranking websites to first decide our list of top-level conferences:(1)Microsoft Academic Search - Top Conferences in Security & Privacyhttp://academic.research.microsoft.com/ RankList?entitytype=3&topDomainID=2&subDomainID=2, (2)Guofei Gu - Computer Security Conference Ranking and Statistichttp://faculty.cs.tamu.edu/guofei/sec_\nsystematically went through all proceedings between 2008 and early 2016 of the top 6 network- and computer-security conferences to collect relevant papers. Because of KDD\u2019s early and consistent publication record on ML applications in Security, and its status as a top-level venue for ML and DM applications, we also include KDD\u2019s 2008-2015 proceedings. To demonstrate the wide-ranging research attention drawn to ML applications in security, we also added chosen selections from the workshop AISec, International Conference on Machine Learning(ICML), Neural Information Processing Systems(NIPS), and Internet Measurement Conference(IMC) papers between 2008-2015, mostly in the \u201cFuture Development\u201d section."}, {"heading": "2.1 ML System Designs in Security", "text": "Figure 1 shows the generalization of ML system designs when applied in security, that emerged from our survey of the papers(the legend is on the figure\u2019s bottom left). In different use cases, the system components may embody different names, but their functionalities and positions are captured in the figure. For example:\n1. Knowledge base is baseline of known normality and/or abnormality, depending on use cases, they include but are not limited to blacklist(BL), whitelist(WL), watchlist; known malware signatures, system traces, and their families; initial set of malicious web pages; existing security policies or rules, etc.. 2. Data sources are where relevant data is collected. They can be either off-line or live online data feed, e.g. malware traces collected after execution(off-line), URL stream(online). 3. Training data are labeled data which are fed to classifiers in training. They can be standard research datasets, new data(mostly from industry) labeled by human, synthetic datasets, or a mix. 4. Pre-processor and feature extractor construct features from data sources, for example: URL aggregators, graph representations, SMTP header extractions, n-gram model builders.\nconf_stat.htm, and (3) Jianying Zhou - Top Crypto and Security Conferences Ranking http://icsd.i2r.a-star. edu.sg/staff/jianying/conference-ranking.html. All 3 rankings have the same top 6, and because Crypto and Eurocrypt do not have papers within our focus, we decided on these 4: ACM CCS, IEEE S&P(hereafter \u201cSP\u201d), NDSS, and USENIX Security(hereafter \u201cSec\u201d or \u201cUSENIX\u201d)\nDynamic analyzer and static analyzer are used most often in malware-related ML tasks, and human feedback loop is added when the system\u2019s design intends to be semi-supervised or human-in-the-loop(HITL)."}, {"heading": "2.2 ML Paradigms in Security Problems", "text": "Table 1 shows a matrix with rows indicating different ways of classifying the security problems, and the columns showing well-understood ML paradigms. Based on the threat models and modeling purposes presented in the papers, we qualitatively group the attacker into three groups. If there are multiple attacker types in one section, the section\u2019s numbering appears multiple times accordingly.\n1. Passive attackers make no attempt to evade detections; their behaviors fit into descriptions of the threat models. 2. Semi-aggressive attackers have knowledge of the detectors, and only attempt to evade detections. 3. Active attackers do not only have knowledge of the detectors and attempt to evade detections, but also actively try to poison, mislead, or thwart detection. 4. Knowledge of attackers, is the information in at least one of the five aspects: the learning algorithms themselves, the algorithms\u2019 feature spaces, the algorithm\u2019s parameters, training and evaluation data - regardless of being labeled or not - used by the algorithms, and decision feedback given by the algorithms [23, 24, 25].\nInfluenced by [22, 23], we extend their definitions, and qualitatively categorize attackers\u2019 primary purpose as to compromise confidentiality, availability or integrity of legitimate systems, services, and users.\n1. Attacks on confidentiality compromise the confidential or secret information of systems, services, or users (e.g. password crackers). 2. Attacks on availability make systems and services unusable with unwanted information, requests, or many errors in defense schemes (e.g. DDoS, spam). 3. Attacks on integrity masquerade maliciously intentions as benign intentions in systems, services, and users (e.g. malware).\nWe also define ML paradigms shown in the matrix:\n1. Supervised learning uses labeled data for training. 2. Semi-supervised learning uses both labeled and un-\nlabeled data for training. 3. Unsupervised learning has no labeled data available\nfor training. 4. Human-in-the-loop(HITL) learning incorporates ac-\ntive human feedback to algorithm\u2019s decisions into the knowledge base and/or algorithms.\n5. Game Theory(GT)-based learning considers learning as a series of strategic interactions between the model learner and actors with conflicting goals. The actors can be data generators, feature generators, chaotic human actors, or a combination[23, 24, 26, 27, 28].\nFor \u201dMeans of Attacks\u201d in Table 1, server, network, and user are straightforward and intuitive, so here we only describe \u201cclient app\u201d and \u201cclient machine\u201d. Client app is any browser-based means of attack on any client device, and client machine is any non-browser-based means of attack on any client device.\nAs shown in Table 1, the majority of surveyed papers in different security domains use supervised learning to deal with passive or semi-aggressive attackers. However, the core requirement of supervised learning - labeled data - is not always viable or easy to obtain, and authors have repeatedly written about the difficulty of obtaining labeled data for training. Based on this observation, we conclude that exploring semi-supervised and unsupervised learning approaches would expand the research foundation of ML applications in security domains, because semi-supervised and unsupervised learning can utilize unlabeled datasets which had not been used by supervised learning approaches before.\nMoreover, during our survey, we realized that many ML applications in security assume that training and testing data come from the same distribution (in statistical terms, this is the assumption of stationarity). However, in the real world, it is highly unlikely that data are stationary, let alone that the data could very well be generated by an adversarial data generator producing training and/or testing data sets, as the case in [26], or simply be generated responding to specific\nmodels as in [29]. Our observation from the comprehensive survey confirmed [23]\u2019s statement, and we propose that GT-based learning approaches and HITL learning system designs should be explored more, in order to design more efficient security defense mechanisms to deal with active and unpredictable adversaries. At the same time, human knowledge and judgment in HTIL should go beyond feature engineering, to providing feedback to decisions made by ML models. Some theory-leaning papers have modeled spam filtering as Bayesian games or Stackelberg games[26, 29]. Use cases in data sampling, model training with poisoned or low-confidence data have also been briefly explored in literature[27, 30, 31]."}, {"heading": "2.3 Timeline of ML in Security", "text": "Based on seminal works and establishments in notable venues, the gradually increasing levels of interest in ML research applied to Security is fairly visible. Here we gathered some milestone events:\n1. 1987: Denning published \u201cAn Intrusion Detection System\u201d [1], first framing security as a learning problem 2. 1998: DARPA IDS design challenge[2] 3. 1999: KDD Cup IDS design challenge[3, 4] 4. 2008: CCS hosted the 1st AISec workshop. Continues\nto operate each year[9] 5. 2007, 2008: Twice, KDD hosted the International Work-\nshop on Privacy, Security, and Trust(PinKDD)[32] 6. 2010, 2012: Twice, KDD hosted Intelligence and Se-\ncurity Informatics Workshop(ISI)[33, 34] 7. 2011: \u201cAdversarial Machine Learning\u201dpublished in 4th\nAISec[23] 8. 2012: \u201cPrivacy and Cybersecurity: The Next 100 Years\u201d\nby Landwehr et al published[35] 9. 2013: Manifesto from Dagstuhl Perspectives Workshop\npublished as \u201cMachine Learning Methods for Computer Security\u201d by Joseph et al [36]. 10. 2014: KDD hosted its 1st \u201cSecurity & Privacy\u201d session in the main conference program[37] 11. 2014: ICML hosted its 1st, and so far the only workshop on Learning, Security, and Privacy(LSP)[38] 12. 2016: AAAI hosted its 1st Artificial Intelligence for Cyber Security workshop(AISC)[39]"}, {"heading": "2.4 Related Work", "text": "Despite the surge of research interests and industry applications in the intersection of ML and security, few surveys or overviews were published after 2008, the watershed year of increasing interest in this particular domain. In 2013 [40]surveyed server-side web application security, [41] surveyed data mining applied to security in the cloud focusing on intrusion detection, [42] discussed an ML perspective in network anomaly detection. While they are helpful and informative, the former two are limited by their scope and perspective, and the latter serves as a textbook, hence absent the quintessential of survey - mapping the progresses and charting the state-of-art. A collection of papers in 2002 and 2012 [43] discussed applications of DM in computer security, but lacks a systematic survey on ML applications in resolving security issues. [44] briefly compared two network anomaly detection techniques, but limited in scope. [45] of 2009 conducted a comprehensive survey in anomaly detection techniques, some involving discussions of security domains. The Dagstuhl Manifesto in 2013 [36] articulated the status quo and looked to the future of ML in security, but the majority of the literature listed were published before 2008. [46] of 2010 highlighted use cases and challenges for ML in network intrusion detection, but did not incorporate a high-level review of ML in security in recent years."}, {"heading": "3. NETWORK SECURITY", "text": ""}, {"heading": "3.1 Botnets and Honeypots", "text": "Research works on botnets among our surveyed literature focuses mainly on designing systems to detect commandand-control(C&C) botnets, where many bot-infected machines are controlled and coordinated by few entities to carry out malicious activities[47, 48, 49]. Those systems need to learn decision boundaries between human and bot activities, therefore ML-based classifiers are at the core of those systems, and are often trained by labeled data in supervised learning environments. The most popular classifier is support vector machines(SVMs) with different kernels, while spatial-temporal time series analysis and probabilistic inferences are also notable techniques employed in ML-based classifiers. Topic clustering, mostly seen in natural language processing(NLP), is used to build a large-scale system to\nidentify bot queries [50]. In botnet detection literature, 3 core assumptions are widely shared:\n1. Botnet protocols are mostly C&C [47, 48, 51] 2. Individual bots within same botnets behave similarly\nand can be correlated to each other [51, 52] 3. Botnet behaviors are different and distinguishable from\nlegitimate human user, e.g. human behaviors are more complex[53, 54, 55]\nOther stronger assumptions include that bots and humans interact with different server groups [56], and content features from messages generated by bots and human are independent [53]. While classification techniques differ, WLs, BLs, hypothesis testing, and a classifier [47, 49, 53] are usual system components. Attempts have been made to abstract state machine models of network to simulate realworld network traffic and create honeypots [57]. Ground truths are often heuristic [52], labeled by human experts, or a combination - even at large scale, human labeled ground truths are used, for example in [55], game masters\u2019 visual inspections serve as ground truth to detect bots in online games. In retrospect, the evolution of botnet detection is clear: from earlier and more straightforward uses of classification techniques such as clustering and NB, the research focus has expanded from the last step of classification, to the important preceding step of constructing suitable metrics, that measures and distinguishes bot-based and humanbased activities[50, 55]."}, {"heading": "3.2 Proxies and DNS", "text": "Classifying DNS domains that distribute or host malware, scams, and malicious content has drawn research interest especially in passive DNS analysis. There are two main approaches: reputation system[58, 59, 60] and classifier[61, 62]. Reputation system scores benign and malicious domains and DNS hosts, and a ML-based classifier learns boundaries between the two. Nonetheless, both reputation system and classifier use various decision trees, random forest(RF), na\u0308\u0131ve Bayes(NB), SVM, and clustering techniques for mostly supervised learning-based scoring and classification. Many features used are from protocols and network infrastructures, e.g. border gateway protocol(BGP) and updates, automated systems(AS), registration, zone, hosts, and public BLs. Similar to botnet detectors, variations of BL, WL, and honeypots[63] are used in similar functions as knowledge bases, while ground truths are often taken from public BLs, limited WLs, and anti-virus(AV) vendors such as McAfee and Norton [58, 59, 61]. But before any ML attempts take place, most studies would assume the following:\n1. Malicious uses of DNS are distinct and distinguishable from legitimate DNS services. 2. The data collection process - regardless of different names such as data flow, traffic recorder, or packet assembler - follows a centralized model. In other words, all the traffic/data/packets flow through certain central node or nodes to be collected.\nStronger assumptions include that AS hijackers cannot manipulate AS path before it reaches them[60], and maliciousness will trigger an accurate IP address classifier to fail[64]. Besides analyzing the status quo, [64, 65, 66] showed efforts\nto preemptively protect network measurement integrity and predict potentially malicious activities from web domains and IP address spaces."}, {"heading": "4. SECURITY SERVICES", "text": "Both offense and defense for access control, authentication, and authorization reside within the domain of Security Services. Defeating audio and visual CAPTCHAs(Completely Automated Public Turing test to tell Computers and Humans Apart)[67, 68, 68, 69, 70], cracking passwords[71, 72, 73], measuring password strengths[72, 74, 75], and uncovering anonymity[76, 77, 78, 79] are 4 major use cases. On the offense, specialized ML domains such as computer vision, signal processing, and NLP automate attacks on user authentication services i.e. textual or visual passwords and CAPTCHAs, and uncover hidden identities and services. On the defense side, entropy-based and ML-based systems calculate password strengths. Other than traditional user authentication schemes, behavioral metrics of users are also introduced. Following the generalized ML pipeline shown in Figure 1, the \u201cclassifier\u201d is replaced by \u201crecognition engine\u201d in the password cracking process, and \u201cuser differentiation engine\u201d in authentic metric engineering [80, 81]. Hence the process becomes: \u201cData source\u2192 Pre-process & feature extraction\u2192 Recognition or user differentiation engine\u2192 Decision\u201d for ML-based security services. A noteworthy trend to observe, is that attacks on CAPTCHAs are getting more generalized - from utilizing SVM in 2008 to attack a specific type of text CAPTCHA[67], in 2015 a generic attach approach to attack text-based CAPTCHA [70] was proposed.\nML-based attacks on textual and visual CAPTCHA typically follow the 4-step process:\n1. Segmentation: e.g. signal to noise ratio(SNR) for audio; hue, color, value(HSV) for visual [67, 69, 70, 82] 2. Signal or image representation: e.g. discrete Fourier transformation(audio)[82], letter binarization(visual) [70] 3. Feature extraction: e.g. spectro-temporal features, character strokes [67, 82] 4. Recognition: K-nearest neighbor(KNN), SVM(RBF kernel), convolutional neural networks(CNN) [68, 71]\nOn the side of password-related topics in security services, there are 2 password models: whole-string Markov models, and template-based models [73]. Concepts in statistical language modeling, such as natural language encoder and ngrams associated with Markov models(presented as directed graphs with nodes labeled by n-grams), and context-free grammars are common probabilistic foundations to build password strength meters and password crackers [71, 72, 82]."}, {"heading": "5. SOFTWARE & APPLICATION SECURITY", "text": "ML research in software and applications security mostly concentrate on web application security in our survey, and have used supervised learning to train popular classifiers such as NB and SVM to detect web-based malware and JavaScript(JS) code[83, 84], filter unwanted resources and requests such as malicious advertisements[85, 86, 87, 88], predict unwanted resources and requests(e.g. future blacklisted websites)[89, 90, 91], and quantify web application vulnerabilities[92]. While [91] explored building web application anomaly detector with scarce training data, most use\ncases follow the supervised paradigm assuming plentiful labeled data: Data source(web applications, static/dynamic analyzers)\u2192 feature extraction(often with specific pre-filter, metrics, and de-obfuscator if needed) \u2192 classifiers trained with labeled data. Apart from this supervised setting, if a human expert\u2019s feedback is added after classifiers\u2019 decisions[85], it forms a semi-supervised system. Regardless of system designs, the usual assumption holds: malicious activities or actors are different from normal and benign ones likely do not change much. The knowledge bases of normality and abnormality can vary, from historical regular expression lists[86] to other publicly available detectors[84]. Graphbased algorithms[87] and image recognition[90] are both used in resource filtering, but in detecting JS malware and evasions and quantifying leaks, having suitable measurements of similarities is a significant focal point. Indeed, from [83, 84, 92], ML-based classifiers do well in finding similarities between mutated malicious code snippets, while the same code pieces could evade static or dynamic analyzer detections."}, {"heading": "6. SYSTEM SECURITY", "text": ""}, {"heading": "6.1 Vulnerability and Policy Management", "text": "As Landwehr noticed[93], ML can be applied in SPM. However, in automatic fingerprinting of operating systems(OS), C4.5 decision tree, SVM, RF, KNN - some most commonly used ML-based classifiers in security - failed to distinguish remote machine instances with coarse- and fine-grained differences, as the algorithms cannot exploit semantic knowledge of protocols or send multi-packet probes [94]. Yet by taking advantage of semantic and syntactic features, plus semi-supervised system design, [95, 96, 97] showed that SVM(optimized by sequential minimal optimization[SMO] algorithm), KNN, and NLP techniques do well in Android SPM. On the other hand, in vulnerability management, [98, 99, 100, 101, 102], clustering techniques have done well in predicting future incidents and infer vulnerability patterns in code, as well as NB, SVM, and RF in ranking risks and identifying proper permission levels. Both vulnerability management and SPM also focus on devising proper metrics for ML applications: from heuristics based on training set [100], Jaro distance [101], to outside reputation system oracles [99], metrics are needed to compare dependency graphs, string similarities, and inferred vulnerability patterns. In most use cases, because of the need for labeled data to train supervised learning systems, many systems follow the generalized training process in Figure 1: \u201cKnowledge base \u2192 offline trainer \u2192 online or offline classifier\u201d. When policy management decisions need feedback, a HITL design is in place where end human users\u2019 feedback is directed to knowledge base. One distinguishing tradition in ML applications research in this domain, is a strong emphasis on measurement - selecting or engineering proper similarity or scoring metrics are often important points of discussion in research literature. From earlier uses of heuristics in clustering algorithms, to more recent semantic connectivity measurement applied in semisupervised systems, both the metrics and the system designs for vulnerability and security policy management have evolved to not only identify, but also to infer and predict future vulnerable instances.\n6.2 Information Flow and DDoS\nCompared to other security domains, ML research in information flow and DDoS focus more on evasion tactics and limits of ML systems in adversarial environments. Hence we grouped together the two sub-domains, and marked studies in Table 7 with \u201c(IF)\u201d and \u201c(DDoS)\u201d accordingly. For DDoS[27, 103, 104], the usual assumption is that patterns of attack and abuse traffic are different from normal traffic [103], but [104] challenged it by proposing an adversary who can generate attributes that look as plausible as actual attributes in benign patterns, and caused failure in ML-based automated signature generation to distinguish benign and malicious byte sequences. Then, [27] introduced GT to evaluate DDoS attack and defense in real-world. For information flow[24, 105, 106, 107], assumptions can take various forms. In PDF classifiers based on document structural features, it is malicious PDF has different document structures than good PDFs [105]; in Android privacy leak detector, it is the majority of an Android application\u2019s semantically similar peers has similar privacy disclosure scenarios[106]. But [24] poses semi-aggressive and active attackers with some information about the data, feature sets, and/or algorithms, and then attackers successfully evade ML-based PDF classifiers. Another example is, PDF malware could be classified [105], and then a generic and automated evasion technique based on genetic programming is successfully experimented[107]. Overall, while using SVM, RF, and decision trees trained with labeled data to detect and predict DDoS and malicious information and data flows, ML applications in information flow and DDoS challenge the usual assumption of stationary adversary behaviors. From collecting local information only, to proposing a general game theory-based framework to evaluate DDoS attacks and defense, and from using static method to detect malicious PDF file to generic automated evasion, the scope of ML applications in both DDoS and IF have expanded and generalized over the years."}, {"heading": "7. MALWARE, SOCIAL ENGINEERING & IDS", "text": ""}, {"heading": "7.1 Malware Detection and Mitigations", "text": "Program-centric or system-centric, there are 3 areas that draw most ML application research attention in malware: malware detection[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], classifying unknown malware into families[31, 120, 121, 122, 123, 124], and auto-extract program or protocol specifications [125, 126, 127]. Realizing the signature and heuristic-based malware detectors can be evaded by obfuscation and polymorphism [118], more behavior-based matching and clustering systems and algorithms have been researched. Figure 1 already shows a generalized ML system design for malware detection and classification, and a more detailed description is below:\n1. Collect malware artifacts and samples, analyze them, execute them in a controlled virtual environment to collect traces, system calls, API calls, etc. [116, 119, 122] 3. Or, directly use information from already completed static and/or dynamic analyses.\n3We direct our readers for more details in [21], which evaluated rigor and prudence of academic research up to 2011 that rely on malware execution, and provided a guide rubric for safety for handling malware datasets\n2. Decide or devise similarity measurements between generalized binaries, system call graphs(SCG), function call graphs(FCG), etc., then extract features [120, 121, 124, 126] 3. Classify malware artifacts into families in-sample, or cluster them with known malware families. The classifiers and clustering engines are usually trained with labeled data[58, 118, 123]. Popular ones are SVM and RF for classification, and hidden Markov model(HMM) and KNN alongside different clustering techniques.\nEven in the use case of auto-extract specifications, supervised learning with labeled data is needed when behavior profiles, state machine inferences, fuzzing, and message clustering are present. Evasion techniques of detectors and poisoning of ML algorithms are also discussed, and typical evasion techniques include obfuscation, polymorphism, mimicry, and reflecting set generation[24, 113]. Malware detection and matching based on structural information and behavior profiles[118, 120, 124] show a tendency to use graph-based clustering and detection algorithms, and similarity measurement used in these algorithms have ranged from Jaccard distance to new graph-based matching metrics. While clustering techniques have been mostly used in malware detection, a nearest neighbor technique is explored to evade malware detection."}, {"heading": "7.2 Social Engineering: Phishing, Malicious Content and Behaviors", "text": "Spams, malicious webpages and URLs that redirect or mislead un-suspecting users to malware, scams, or adult content [66] is perhaps as old as civilian use of the Internet. Research literature mostly focus on 3 major areas: detecting phishing malicious URLs[128, 129, 130, 131, 132, 133], filtering spam or fraudulent content [26, 29, 134, 135, 136, 137, 138, 139, 140, 141], and detecting malicious user account behaviors[142, 143, 144, 145, 146]. Moreover, because phishing 4 is a classic social engineering tactic, it is often the gateway of many studies to detect malicious URLs, spam, and fraudulent content. To identify malicious URLs, MLbased classifiers draw features from webpage content(lexical, visual, etc.), URL lexical features, redirect paths, host-based features, or some combinations of them. Such classifiers usually act in conjunction with knowledge bases which are usually in-browser URL BLs or from web service providers. If the classifier is fed with URL-based features, it is common to set an URL aggregator as a pre-processor before extracting features. Mostly using supervised learning paradigm, NB, SVM with different kernels, and LR are popular ML classifiers for filtering spam and phishing. Meanwhile, GTbased learning to deal with active attackers is also evaluated in spam filtering. [26] evaluates a Bayesian game model where the defense is not fully informed of the attacker\u2019s objectives and the active adversary can exercise control over data generation, [29] proposes a Stackelberg game where spammer reacts to the learner\u2019s moves. Stronger assumptions also exist: for example, [139] assumes spammers\u2019\n4We agree with [131]\u2019s definition of phishing: \u201cwithout permission, alleges to act on behalf of a third party with the intention of confusing viewers into performing an action with which the viewer would only trust a true agent of the third party\u201d\nphone blocks follow a beta distribution as conjugate prior for Bernoulli and binomial distribution. Another social engineering tactic is spoofing identities with fake or compromised user accounts, and detection of such malicious behaviors utilize features from user profiles, spatial-, temporal-, and spatial-temporal patterns, and user profiles are used in particular to construct normality. Graph representation and trust propagation models are also deployed to distinguish genuine and malicious accounts with different behavior and representations[144, 145, 146]. Tracing the chronology of applying ML to defend against social engineering, one trend is clear: while content-, lexical-, and syntactic-based features are still being widely used, constructing graph representations and exploring temporal patterns of redirect paths, events, accounts, and behaviors have been on the rise as feature spaces for ML applications in defend against social engineering efforts. Accordingly, the ML techniques have also changed from different classification schemes to graphic models. It is also noteworthy that in [29, 146], addressing adversarial environments\u2019 challenges to ML systems is elaborated as primary research areas, instead of a short discussion point."}, {"heading": "7.3 IDS", "text": "From feature sets to algorithms and systems, IDS has been extensively studied. However, as [147] cautioned, ML can be easily conflated with anomaly detection. While both are applied to build IDS, important difference is that ML aims to generalize expert-defined distinctions, but anomaly detection focuses on finding unusual patterns, while attacks are not necessarily anomalous. For example, [148] distinguished n-gram model\u2019s different use cases: anomaly detection uses it to construct normality(hence more appropriate when no attack is available for learning), and ML classifiers learn to discriminate between benign and malicious n-grams(hence more appropriate when more labeled data is present). Since 2008, works at top venues have added to the rigor for ML applications in IDS. For example, a common assumption of IDS is: Anomalous or malicious behaviors or traffic flows are fundamentally different from normal ones, but [147] challenges the assumption by studying low-cardinality intrusions where attackers don\u2019t send a large number of probes. To address adversarial learning environment and minimal labels in training data, semisupervised paradigms, especially active learning, are also used[147, 149]. Heterogeneous designs of IDS in different use cases give rise to many ad-hoc evaluations in research works, and a reproducibility and comparison framework was proposed to address the issue[150]. Meanwhile, techniques such as graph-based community detection[151], time series-based methods[152, 153], and generalized support vector data description in cyber-physical system and adversarial environment for auto-feature selection[154], have also emerged. Although they carry different assumptions of normality and feature representations, the supervised ML system design remains largely the same. Besides the fact the more techniques and use cases have been proposed, the focus of research in IDS had evolved from discovering new techniques and use cases, to rigorously evaluating fundamental assumptions and workflows of IDS. For example, while feature selection has stayed as a major component, there are re-examination of assumptions and measurements on what constitutes normality and abnormality[151], alternative to more easily acquire\ndata and use low-confidence data for ML systems[149], and proposal on validating reproducibility of results from different settings[150]."}, {"heading": "8. FUTURE DEVELOPMENT", "text": "One key goal of our SoK survey is to help researchers look into the future. ML applications in security domains are attracting academic research attention as well as industrial interest, and this presents a valuable opportunity for researchers to navigate the landscapes between ML theories and security applications. There are also opportunities to explore if there are some types of ML paradigms that are especially well suited to particular security problems. Apart from highlighting that 1) semi-supervised and unsupervised ML paradigms are more effective in utilizing unlabeled data, hence ease the difficulty of obtaining labeled data, and 2) GT-based ML paradigms and HITL ML system designs will become more influential in dealing with semi-aggressive and aggressive attackers, we also share the following seven speculations of future trends, based on our current SoK.\n1. Metric Learning: Measurement has become more and more conspicuous for ML research in security, mostly in similarity measurement for clustering algorithms[53, 65, 76, 120]. Proper measurements and metrics are also used to construct ground truths to evaluate ML-based classifiers, and also have important roles in feature engineering[55, 140, 155, 156]. Given the ubiquitous presence of metrics and the complex nature of constructing them, ML applications in security will benefit much from metric learning. 2. NLP: Malicious content, spam, and malware analysis and detections have used tools from statistical language modeling(e.g. n-gram-based representation for strings in code and HTTP request)[62, 74, 116, 138, 141, 157], As textual information explodes, NLP will become more widely used beyond source filtering and clustering e.g. [57] use n-gram models to infer state machines of protocols. 3. Upstream movement of ML in security defense designs. In malware detection and classifications, behaviorand signature-based malware classifiers have used inputs from static and dynamic binary analysis as features[110, 111, 123, 126], and [112] already shows RNN can be applied to automatically recognize functions in binary analysis. We also see ML algorithms applied in vulnerability, device, and security policy management, DDoS mitigation, information flow quantifications, and network infrastructure[103, 106, 116, 141]. Hence, it is reasonable to expect that more ML systems and algorithms will move upstream in more security domains. 4. Scalability: With increasing amount of data from growing numbers of information channels and devices, scale of ML-based security defenses will become a more important aspect in researching ML applications in security [12, 50, 109, 131]. As a result, large-scale systems will enable distributed graph algorithms in malware analysis, AS path hijacker tracing, cyberphysical system fault correlation, etc..[49, 56, 72, 96, 106, 118] 5. Specialized probabilistic models will be applied beyond the context of classifiers, e.g. access control[81]. 6. High FP rates have always been a concern for system\narchitects and algorithm researchers [86, 150]. Reducing FP rates will grow from an ad-hoc component in various system designs, to independent formal frameworks, algorithms, and system designs. 7. Privacy enforcement was framed as a learning problem recently in [158], in the light of many publications on privacy-preservation in ML algorithms, and privacy enhancement by probabilistic models[11, 159, 160, 161, 162, 163]. This new trend will become more prominent."}, {"heading": "9. CONCLUSION", "text": "In this paper, we analyzed ML applications in security domains by surveying literature from top venues of our field between 2008 and early 2016. We attempted to bring clarity to a complex field with intersecting expertises by identifying common use cases, generalized system designs, common assumptions, metrics or features, and ML algorithms applied in different security domains. We constructed a matrix showing the intersections of ML paradigms and three different taxonomy structures to classify security domains, and show that while much research has been done, explorations in GT-based ML paradigms and HITL ML system designs are still much desired (and under-utilized) in the context of active attackers. We point out 7 promising areas of research based on our observations, and argue that while ML applications can be powerful in security domains, it is critical to match the ML system designs with the underlying constraints of the security applications appropriately."}, {"heading": "10. ACKNOWLEDGMENT", "text": "We would like to thank Megan Yahya, Krishnaprasad Vikram, and Scott Algatt for their time and valuable feedback.\nAPPENDIX References\n[1] D. E. Denning, \u201cAn intrusion-detection model,\u201d IEEE Trans. Softw. Eng., 1987.\n[2] (1998) Darpa intrusion detection evaluation. [Online]. Available: http://www.ll.mit.edu/ideval/data/\n[3] (1999) Kdd cup 1999 data. [Online]. Available: http:// kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n[4] C. Elkan. (1999) Results of the kdd\u201999 classifier learning contest. [Online]. Available: http://cseweb. ucsd.edu/\u02dcelkan/clresults.html\n[5] Black hat usa 2014. [Online]. Available: https: //www.blackhat.com/us-14/training\n[6] Rsa 2016. [Online]. Available: https://www. rsaconference.com/events/us16/expo-sponsors/ exhibitor-list\n[7] (2016) Darpa cyber grand challenge. [Online]. Available: http://www.cybergrandchallenge.com\n[8] ACM. (2012) Acm 2012 computing classification system. [Online]. Available: https://www.acm.org/ publications/class-2012\n[9] (2016) Aisec 2016 topic classifications. [Online]. Available: http://teamcore.usc.edu/people/arunesh/ AISec2016/call-for-papers.html\n[10] R. Chow, P. Golle, and J. Staddon, \u201cDetecting privacy leaks using corpus-based association rules,\u201d in KDD 2008.\n[34] \u2014\u2014. (2012) 2012 kdd intelligence and security informatics workshop. [Online]. Available: http: //cci.drexel.edu/isi/isi-kdd2012/\n[35] C. Landwehr, D. Boneh, J. C. Mitchell, S. M. Bellovin, S. Landau, and M. E. Lesk, \u201cPrivacy and cybersecurity: The next 100 years,\u201d Proceedings of the IEEE, vol. 100, no. Special Centennial Issue, 2012.\n[36] A. D. Joseph, P. Laskov, F. Roli, J. Tygar, and B. Nelson, \u201cDagstuhl Manifestos, Volume 3, Issue 1, January to December 2013, Complete Issue,\u201d Dagstuhl Manifestos, 2014.\n[37] KDD. (2014) 2014 kdd security and privacy session. [Online]. Available: http://dl.acm.org/citation.cfm? id=2623330\n[38] ICML. (2014) Icml 2014 workshop on learning, security and privacy. [Online]. Available: https: //sites.google.com/site/learnsecprivacy2014/\n[39] AAAI. Aaai 2016 workshop aisc. [Online]. Available: http://www-bcf.usc.edu/\u02dcaruneshs/AICS2016/ index.html\n[40] X. Li and Y. Xue, \u201cA survey on server-side approaches to securing web applications,\u201d ACM Comput. Surv., vol. 46, no. 4, April 2014.\n[41] P. Aggarwal and M. Chaturvedi, \u201cApplication of data mining techniques for information security in a cloud: A survey,\u201d International Journal of Computer Applications, vol. 80, no. 13, 2013.\n[42] D. Bhattacharyya and J. Kalita, Network Anomaly Detection: A Machine Learning Perspective. CRC Press, 2013.\n[43] D. Barbara\u0301 and S. Jajodia, Applications of Data Mining in Computer Security, ser. Advances in Information Security, 2012, 2012.\n[44] T. Ahmed, B. Oreshkin, and M. Coates, \u201cMachine learning approaches to network anomaly detection,\u201d in Proceedings of the 2nd USENIX workshop on Tackling computer systems problems with machine learning techniques, 2007.\n[45] V. Chandola, A. Banerjee, and V. Kumar, \u201cAnomaly detection: A survey,\u201d ACM Comput. Surv., vol. 41, no. 3, 2009.\n[46] R. Sommer and V. Paxson, \u201cOutside the closed world: On using machine learning for network intrusion detection,\u201d in SP 2010.\n[47] G. Gu, J. Zhang, and W. Lee, \u201cBotsniffer: Detecting botnet command and control channels in network traffic,\u201d in NDSS 2008.\n[48] G. Jacob, R. Hund, C. Kruegel, and T. Holz, \u201cJackstraws: Picking command and control connections from bot traffic,\u201d in USENIX Security\u201911.\n[49] S. Nagaraja, P. Mittal, C.-Y. Hong, M. Caesar, and N. Borisov, \u201cBotgrep: Finding p2p bots with structured graph analysis,\u201d in USENIX Security\u201910.\n[50] J. Zhang, Y. Xie, F. Yu, D. Soukal, and W. Lee, \u201cIntention and origination: An inside look at large-scale bot queries,\u201d in NDSS 2013.\n[51] G. Jacob, E. Kirda, C. Kruegel, and G. Vigna, \u201cPubcrawl: Protecting users and businesses from crawlers,\u201d in USENIX Security\u201912.\n[52] G. Gu, R. Perdisci, J. Zhang, W. Lee et al., \u201cBotminer: Clustering analysis of network traffic for protocol-and structure-independent botnet detection,\u201d in USENIX Security 2009.\n[53] S. Gianvecchio, M. Xie, Z. Wu, and H. Wang, \u201cMeasurement and classification of humans and bots in internet chat,\u201d in USENIX security 2008.\n[54] X. Hu, M. Knysz, and K. G. Shin, \u201cRb-seeker: Autodetection of redirection botnets.\u201d in NDSS 2009.\n[55] E. Lee, J. Woo, H. Kim, A. Mohaisen, and H. K. Kim, \u201cYou are a game bot!: uncovering game bots in mmorpgs via self-similarity in the wild,\u201d in NDSS 2016.\n[56] F. Chen, S. Ranjan, and P.-N. Tan, \u201cDetecting bots via incremental ls-svm learning with dynamic feature adaptation,\u201d in KDD 2011.\n[57] T. Krueger, H. Gascon, N. Kra\u0308mer, and K. Rieck, \u201cLearning stateful models for network honeypots,\u201d in AISec 2012.\n[58] M. Antonakakis, R. Perdisci, D. Dagon, W. Lee, and N. Feamster, \u201cBuilding a dynamic reputation system for dns.\u201d in USENIX Security\u201910.\n[59] M. Antonakakis, R. Perdisci, W. Lee, N. Vasiloglou II, and D. Dagon, \u201cDetecting malware domains at the upper dns hierarchy.\u201d in USENIX Security\u201911.\n[60] T. Qiu, L. Ji, D. Pei, J. Wang, J. J. Xu, and H. Ballani, \u201cLocating prefix hijackers using lock,\u201d in USENIX Security 2009.\n[61] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi, \u201cExposure: Finding malicious domains using passive dns analysis,\u201d in NDSS 2011.\n[62] Y. Song, A. D. Keromytis, and S. J. Stolfo, \u201cSpectrogram: A mixture-of-markov-chains model for anomaly detection in web traffic.\u201d in NDSS 2009.\n[63] S. Small, J. Mason, F. Monrose, N. Provos, and A. Stubblefield, \u201cTo catch a predator: A natural language approach for eliciting malicious payloads,\u201d in USENIX Security 2008.\n[64] S. Venkataraman, D. Brumley, S. Sen, and O. Spatscheck, \u201cAutomatically inferring the evolution of malicious activity on the internet.\u201d\n[65] O. Fatemieh, A. Farhadi, R. Chandra, and C. A. Gunter, \u201cUsing classification to protect the integrity of spectrum measurements in white space networks.\u201d in NDSS 2011.\n[66] T. Vissers, W. Joosen, and N. Nikiforakis, \u201cParking sensors: Analyzing and detecting parked domains.\u201d in NDSS 2015.\n[67] P. Golle, \u201cMachine learning attacks against the asirra captcha,\u201d in CCS 2008.\n[68] E. Bursztein, M. Martin, and J. Mitchell, \u201cText-based captcha strengths and weaknesses,\u201d in CCS 2011.\n[69] H. Gao, W. Wang, J. Qi, X. Wang, X. Liu, and J. Yan, \u201cThe robustness of hollow captchas,\u201d in CCS 2013.\n[70] H. Gao, J. Yan, F. Cao, Z. Zhang, L. Lei, M. Tang, P. Zhang, X. Zhou, X. Wang, and J. Li, \u201cA simple generic attack on text captchas,\u201d in NDSS 2016.\n[71] M. Weir, S. Aggarwal, B. De Medeiros, and B. Glodek, \u201cPassword cracking using probabilistic context-free grammars,\u201d in SP 2009.\n[72] R. Chatterjee, J. Bonneau, A. Juels, and T. Ristenpart, \u201cCracking-resistant password vaults using natural language encoders,\u201d in SP 2015.\n[73] J. Ma, W. Yang, M. Luo, and N. Li, \u201cA study of probabilistic password models,\u201d in SP 2014.\n[74] C. Castelluccia, M. Du\u0308rmuth, and D. Perito, \u201cAdaptive password-strength meters from markov models.\u201d in NDSS 2012.\n[75] P. G. Kelley, S. Komanduri, M. L. Mazurek, R. Shay, T. Vidas, L. Bauer, N. Christin, L. F. Cranor, and J. Lopez, \u201cGuess again (and again and again): Measuring password strength by simulating passwordcracking algorithms,\u201d in SP 2012.\n[76] S. Zander and S. J. Murdoch,\u201cAn improved clock-skew measurement technique for revealing hidden services,\u201d in USENIX Security 2008.\n[77] S. Afroz, A. C. Islam, A. Stolerman, R. Greenstadt, and D. McCoy, \u201cDoppelga\u0308nger finder: Taking stylometry to the underground,\u201d in SP 2014.\n[78] A. Narayanan, H. Paskov, N. Z. Gong, J. Bethencourt, E. Stefanov, E. C. R. Shin, and D. Song, \u201cOn the feasibility of internet-scale author identification,\u201d in SP 2012.\n[79] K. P. Dyer, S. E. Coull, T. Ristenpart, and T. Shrimpton, \u201cPeek-a-boo, i still see you: Why efficient traffic analysis countermeasures fail,\u201d in SP 2012.\n[80] N. Zheng, A. Paloski, and H. Wang, \u201cAn efficient user verification system via mouse movements,\u201d in CCS 2011.\n[81] M. Frank, D. Basin, and J. M. Buhmann, \u201cA class of probabilistic models for role engineering,\u201d in CCS 2008.\n[82] E. Bursztein, R. Beauxis, H. Paskov, D. Perito, C. Fabry, and J. Mitchell, \u201cThe failure of noise-based non-continuous audio captchas,\u201d in SP 2011.\n[83] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, \u201cZozzle: Fast and precise in-browser javascript malware detection,\u201d in USENIX Security\u201911.\n[84] A. Kapravelos, Y. Shoshitaishvili, M. Cova, C. Kruegel, and G. Vigna, \u201cRevolver: An automated approach to the detection of evasive web-based malware,\u201d in USENIX Security 2013.\n[85] D. Sculley, M. E. Otey, M. Pohl, B. Spitznagel, J. Hainsworth, and Y. Zhou, \u201cDetecting adversarial advertisements in the wild,\u201d in KDD 2011.\n[86] S. Bhagavatula, C. Dunn, C. Kanich, M. Gupta, and B. Ziebart, \u201cLeveraging machine learning to improve unwanted resource filtering,\u201d in AISec 2014.\n[87] J. Zhang, P. A. Porras, and J. Ullrich, \u201cHighly predictive blacklisting,\u201d in USENIX Security 2008.\n[88] L. Lu, R. Perdisci, and W. Lee, \u201cSurf: detecting and measuring search poisoning,\u201d in CCS 2011.\n[89] K. Soska and N. Christin, \u201cAutomatically detecting vulnerable websites before they turn malicious,\u201d in USENIX Security 2014.\n[90] K. Borgolte, C. Kruegel, and G. Vigna, \u201cMeerkat: Detecting website defacements through image-based object recognition,\u201d in USENIX Security 2015.\n[91] W. K. Robertson, F. Maggi, C. Kruegel, and G. Vigna, \u201cEffective anomaly detection with scarce training data.\u201d in NDSS 2010.\n[92] P. Chapman and D. Evans, \u201cAutomated black-box detection of side-channel vulnerabilities in web applications,\u201d in CCS 2011.\n[93] C. Landwehr, \u201cCyber security and artificial intelligence: From fixing the plumbing to smart water,\u201d in AISec 2008.\n[94] D. W. Richardson, S. D. Gribble, and T. Kohno, \u201cThe limits of automatic os fingerprint generation,\u201d in AISec 2010.\n[95] S. Rasthofer, S. Arzt, and E. Bodden, \u201cA machinelearning approach for classifying and categorizing android sources and sinks.\u201d in NDSS 2014.\n[96] R. Pandita, X. Xiao, W. Yang, W. Enck, and T. Xie, \u201cWhyper: Towards automating risk assessment of mobile applications,\u201d in USENIX Security 2013.\n[97] R. Wang, W. Enck, D. Reeves, X. Zhang, P. Ning, D. Xu, W. Zhou, and A. M. Azab, \u201cEaseandroid: Automatic policy analysis and refinement for security enhanced android via large-scale semi-supervised learning,\u201d in USENIX Security 2015.\n[98] A. A. Makanju, A. N. Zincir-Heywood, and E. E. Milios, \u201cClustering event logs using iterative partitioning,\u201d in KDD 2009.\n[99] M. Bozorgi, L. K. Saul, S. Savage, and G. M. Voelker, \u201cBeyond heuristics: learning to classify vulnerabilities and predict exploits,\u201d in KDD 2010.\n[100] H. Peng, C. Gates, B. Sarma, N. Li, Y. Qi, R. Potharaju, C. Nita-Rotaru, and I. Molloy, \u201cUsing probabilistic generative models for ranking risks of android apps,\u201d in CCS 2012.\n[101] F. Yamaguchi, A. Maier, H. Gascon, and K. Rieck, \u201cAutomatic inference of search patterns for taint-style vulnerabilities,\u201d in SP 2015.\n[102] Y. Liu, A. Sarabi, J. Zhang, P. Naghizadeh, M. Karir, M. Bailey, and M. Liu, \u201cCloudy with a chance of breach: Forecasting cyber security incidents,\u201d in USENIX Security 2015.\n[103] J. L. Berral, N. Poggi, J. Alonso, R. Gavalda, J. Torres, and M. Parashar, \u201cAdaptive distributed mechanism against flooding network attacks based on machine learning,\u201d in AISec 2008.\n[104] S. Venkataraman, A. Blum, and D. Song, \u201cLimits of learning-based signature generation with adversaries,\u201d in NDSS 2008.\n[105] N. S\u030crndic and P. Laskov, \u201cDetection of malicious pdf files based on hierarchical document structure,\u201d in NDSS 2013.\n[106] K. Lu, Z. Li, V. P. Kemerlis, Z. Wu, L. Lu, C. Zheng, Z. Qian, W. Lee, and G. Jiang, \u201cChecking more and alerting less: Detecting privacy leakages via enhanced data-flow analysis and peer voting.\u201d in NDSS 2015.\n[107] W. Xu, Y. Qi, and D. Evans, \u201cAutomatically evading classifiers,\u201d in NDSS 2016.\n[108] A. Tamersoy, K. Roundy, and D. H. Chau, \u201cGuilt by association: large scale malware detection by mining file-relation graphs,\u201d in KDD 2014.\n[109] L. Invernizzi, S. Miskovic, R. Torres, C. Kruegel, S. Saha, G. Vigna, S.-J. Lee, and M. Mellia, \u201cNazca: Detecting malware distribution in large-scale networks.\u201d in NDSS 2014.\n[110] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and K. Rieck, \u201cDrebin: Effective and explainable detection of android malware in your pocket.\u201d in NDSS 2014.\n[111] M. Graziano, D. Canali, L. Bilge, A. Lanzi, and D. Balzarotti, \u201cNeedles in a haystack: Mining information from public dynamic analysis sandboxes for malware intelligence,\u201d in USENIX Security 2015.\n[112] E. C. R. Shin, D. Song, and R. Moazzezi, \u201cRecognizing functions in binaries with neural networks,\u201d in USENIX Security 2015.\n[113] C. Smutz and A. Stavrou, \u201cWhen a tree falls: Using diversity in ensemble classifiers to identify evasion in malware detectors.\u201d\n[114] M. Antonakakis, R. Perdisci, Y. Nadji, N. Vasiloglou, S. Abu-Nimeh, W. Lee, and D. Dagon, \u201cFrom throwaway traffic to bots: Detecting the rise of dga-based malware,\u201d in USENIX Security\u201912.\n[115] M. S. Rahman, T.-K. Huang, H. V. Madhyastha, and M. Faloutsos, \u201cEfficient and scalable socware detection in online social networks,\u201d in USENIX Security\u201913.\n[116] A. Lanzi, D. Balzarotti, C. Kruegel, M. Christodorescu, and E. Kirda, \u201cAccessminer: Using systemcentric models for malware protection,\u201d in CCS 2010.\n[117] Y. Ye, T. Li, S. Zhu, W. Zhuang, E. Tas, U. Gupta, and M. Abdulhayoglu, \u201cCombining file content and file relations for cloud based malware detection,\u201d in KDD 2011.\n[118] C. Kolbitsch, P. M. Comparetti, C. Kruegel, E. Kirda, X. Zhou, and X. Wang, \u201cEffective and efficient malware detection at the end host,\u201d in USENIX Security 2009.\n[119] F. Ahmed, H. Hameed, M. Z. Shafiq, and M. Farooq, \u201cUsing spatio-temporal information in api calls with machine learning algorithms for malware detection,\u201d in AISec 2009.\n[120] D. Kong and G. Yan, \u201cDiscriminant malware distance learning on structural information for automated malware classification,\u201d in KDD 2013.\n[121] K. Borgolte, C. Kruegel, and G. Vigna, \u201cDelta: automatic identification of unknown web-based infection campaigns,\u201d in CCS 2013.\n[122] J. Jang, D. Brumley, and S. Venkataraman, \u201cBitshred: Feature hashing malware for scalable triage and semantic analysis,\u201d in CCS 2011.\n[123] Y. Ye, T. Li, Y. Chen, and Q. Jiang, \u201cAutomatic malware categorization using cluster ensemble,\u201d in KDD 2010.\n[124] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda, \u201cScalable, behavior-based malware clustering.\u201d in NDSS 2009.\n[125] M. Fredrikson, S. Jha, M. Christodorescu, R. Sailer, and X. Yan, in SP\u201910.\n[126] P. M. Comparetti, G. Wondracek, C. Kruegel, and E. Kirda, \u201cProspex: Protocol specification extraction,\u201d in SP 2009.\n[127] D. Kirat and G. Vigna, \u201cMalgene: Automatic extraction of malware analysis evasion signature,\u201d in CCS 2015.\n[128] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, \u201cIdentifying suspicious urls: an application of large-scale online learning,\u201d in ICML 2009.\n[129] \u2014\u2014, \u201cBeyond blacklists: learning to detect malicious web sites from suspicious urls,\u201d in KDD 2009.\n[130] A. Blum, B. Wardman, T. Solorio, and G. Warner, \u201cLexical feature based phishing url detection using online learning,\u201d in AISec 2010.\n[131] C. Whittaker, B. Ryner, and M. Nazif, \u201cLarge-scale automatic classification of phishing pages.\u201d in NDSS 2016.\n[132] S. Lee and J. Kim,\u201cWarningbird: Detecting suspicious urls in twitter stream.\u201d in NDSS 2012.\n[133] P. Zhao and S. C. Hoi, \u201cCost-sensitive online active learning with application to malicious url detection,\u201d in KDD 2013.\n[134] K. Chatterjee, L. de Alfaro, and I. Pye, \u201cRobust content-driven reputation,\u201d in AISec 2008.\n[135] S. Hao, N. A. Syed, N. Feamster, A. G. Gray, and S. Krasser, \u201cDetecting spammers with snare: Spatiotemporal network-level automatic reputation engine.\u201d in USENIX Security 2009.\n[136] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song, \u201cDesign and evaluation of a real-time url spam filtering service,\u201d in SP 2011.\n[137] S. Afroz, M. Brennan, and R. Greenstadt, \u201cDetecting hoaxes, frauds, and deception in writing style online,\u201d in SP 2012.\n[138] L. Invernizzi, P. M. Comparetti, S. Benvenuti, C. Kruegel, M. Cova, and G. Vigna, \u201cEvilseed: A guided approach to finding malicious web pages,\u201d in SP 2012.\n[139] N. Jiang, Y. Jin, A. Skudlark, and Z.-L. Zhang, \u201cGreystar: Fast and accurate detection of sms spam numbers in large cellular networks using gray phone space,\u201d in USENIX Security\u201913.\n[140] Q. Zhang, D. Y. Wang, and G. M. Voelker, \u201cDspin: Detecting automatically spun content on the web,\u201d in NDSS 2014.\n[141] S. Whalen, N. Boggs, and S. J. Stolfo, \u201cModel aggregation for distributed content anomaly detection,\u201d in AISec 2014.\n[142] M. Egele, G. Stringhini, C. Kruegel, and G. Vigna, \u201cCompa: Detecting compromised accounts on social networks,\u201d in NDSS 2013.\n[143] B. Viswanath, M. A. Bashir, M. Crovella, S. Guha, K. P. Gummadi, B. Krishnamurthy, and A. Mislove, \u201cTowards detecting anomalous user behavior in online social networks,\u201d in USENIX Security 2014.\n[144] Y. Boshmaf, D. Logothetis, G. Siganos, J. Ler\u0301\u0131a, J. Lorenzo, M. Ripeanu, and K. Beznosov, \u201cIntegro: Leveraging victim prediction for robust fake account detection in osns,\u201d in NDSS 2015.\n[145] G. Stringhini, P. Mourlanne, G. Jacob, M. Egele, C. Kruegel, and G. Vigna, \u201cEvilcohort: Detecting communit ies of malicious accounts on online services,\u201d in USENIX Security 2015.\n[146] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao, \u201cMan vs. machine: Practical adversarial detection of malicious crowdsourcing workers,\u201d in USENIX Security 2014.\n[147] C. T. Symons and J. M. Beaver, \u201cNonparametric semisupervised learning for network intrusion detection: combining performance improvements with realistic in-situ training,\u201d in AISec 2012.\n[148] C. Wressnegger, G. Schwenk, D. Arp, and K. Rieck, \u201cA close look on n-grams in intrusion detection: anomaly detection vs. classification,\u201d in AISec 2013.\n[149] N. Go\u0308rnitz, M. Kloft, K. Rieck, and U. Brefeld, \u201cActive learning for network intrusion detection,\u201d in AISec 2009.\n[150] B. Juba, C. Musco, F. Long, S. Sidiroglou-Douskos, and M. C. Rinard, \u201cPrincipled sampling for anomaly detection.\u201d in NDSS 2015.\n[151] Q. Ding, N. Katenka, P. Barford, E. Kolaczyk, and M. Crovella, \u201cIntrusion as (anti) social communication: characterization and detection,\u201d in KDD 2012.\n[152] S. Xie, G. Wang, S. Lin, and P. S. Yu, \u201cReview spam detection via temporal pattern discovery,\u201d in KDD 2012.\n[153] M. Momtazpour, J. Zhang, S. Rahman, R. Sharma, and N. Ramakrishnan, \u201cAnalyzing invariants in cyberphysical systems using latent factor regression,\u201d in KDD 2015.\n[154] M. Kloft, U. Brefeld, P. Du\u0308essel, C. Gehl, and P. Laskov, \u201cAutomatic feature selection for anomaly detection,\u201d in AISec 2008.\n[155] T. Holz, C. Gorecki, K. Rieck, and F. C. Freiling, \u201cMeasuring and detecting fast-flux service networks.\u201d in NDSS 2008.\n[156] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. Tygar, \u201cAntidote: understanding and defending against poisoning of anomaly detectors,\u201d in IMC 2009.\n[157] R. Moskovitch, N. Nissim, and Y. Elovici, \u201cAcquisition of malicious code using active learning,\u201d in PinKDD\u201908.\n[158] O. Tripp and J. Rubin, \u201cA bayesian approach to privacy enforcement in smartphones,\u201d in USENIX Security 2014.\n[159] Q. Wang, Z. Lin, N. Borisov, and N. Hopper, \u201crbridge: User reputation based tor bridge distribution with privacy preservation,\u201d in NDSS 2013.\n[160] Y. Cao and J. Yang, \u201cTowards making systems forget with machine unlearning,\u201d in SP 2015.\n[161] Z. Huang, E. Ayday, J. Fellay, J.-P. Hubaux, and A. Juels, \u201cGenoguard: Protecting genomic data against brute-force attacks,\u201d in SP 2015.\n[162] R. Kumar, \u201cMining web logs: applications and challenges,\u201d in KDD 2009.\n[163] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec, and J.-P. Hubaux, \u201cQuantifying location privacy,\u201d in SP 2011.\n[164] B. A. Prakash, N. Valler, D. Andersen, M. Faloutsos, and C. Faloutsos, \u201cBgp-lens: Patterns and anomalies in internet routing updates,\u201d in KDD 2009."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The idea of applying machine learning(ML) to solve prob-<lb>lems in security domains is almost 3 decades old. As infor-<lb>mation and communications grow more ubiquitous and more<lb>data become available, many security risks arise as well as<lb>appetite to manage and mitigate such risks. Consequently,<lb>research on applying and designing ML algorithms and sys-<lb>tems for security has grown fast, ranging from intrusion de-<lb>tection systems(IDS) and malware classification to security<lb>policy management(SPM) and information leak checking. In this paper, we systematically study the methods, algorithms, and system designs in academic publications from 2008-2015 that applied ML in security domains. 98% of the surveyed papers appeared in the 6 highest-ranked academic<lb>security conferences and 1 conference known for pioneering<lb>ML applications in security. We examine the generalized<lb>system designs, underlying assumptions, measurements, and<lb>use cases in active research. Our examinations lead to 1) a<lb>taxonomy on ML paradigms and security domains for future exploration and exploitation, and 2) an agenda detailing open and upcoming challenges. Based on our survey, we also suggest a point of view that treats security as a game theory problem instead of a batch-trained ML prob-<lb>lem.", "creator": "LaTeX with hyperref package"}}}