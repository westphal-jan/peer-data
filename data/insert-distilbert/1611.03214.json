{"id": "1611.03214", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Ultimate tensorization: compressing convolutional and FC layers alike", "abstract": "convolutional neural networks excel in image recognition tasks, but this functionality comes at the cost of high computational and memory complexity. to tackle this problem, [ 1 ] lim developed a tensor factorization framework to compress fully - connected layers. in this paper, we focus on compressing convolutional layers. we show importantly that while the direct application of the tensor framework [ 1 ] to the 4 - dimensional kernel of convolution symmetry does compress the layer, we can do better. we should reshape the convolutional kernel into a tensor tensor of far higher order and factorize it. we combine the proposed approach with use the previous work to compress both fully convolutional and fully - connected layers of a network and achieve 80x network compression intensity rate with 1. 1 % accuracy drop on the cifar - 10 dataset.", "histories": [["v1", "Thu, 10 Nov 2016 08:07:46 GMT  (22kb,D)", "http://arxiv.org/abs/1611.03214v1", "NIPS 2016 workshop: Learning with Tensors: Why Now and How?"]], "COMMENTS": "NIPS 2016 workshop: Learning with Tensors: Why Now and How?", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["timur garipov", "dmitry podoprikhin", "alexander novikov", "dmitry vetrov"], "accepted": false, "id": "1611.03214"}, "pdf": {"name": "1611.03214.pdf", "metadata": {"source": "CRF", "title": "Ultimate tensorization: compressing convolutional and FC layers alike", "authors": ["Timur Garipov", "Dmitry Podoprikhin", "Alexander Novikov", "Dmitry Vetrov"], "emails": ["timgaripov@gmail.com", "podoprikhin.dmitry@gmail.com", "novikov@bayesgroup.ru", "vetrovd@yandex.ru"], "sections": [{"heading": "1 Introduction", "text": "Convolutional Neural Networks (CNNs) show state-of-the-art performance on many problems in computer vision, natural language processing and other fields [2, 3]. At the same time, CNNs require millions of floating point operations to process an image and therefore real-time applications need powerful CPU or GPU devices. Moreover, these networks contain millions of trainable parameters and consume hundreds of megabytes of storage and memory bandwidth [4]. Thus, CNNs are forced to use RAM instead of solely relying on the processor cache \u2013 orders of magnitude more energy efficient memory device [5] \u2013 which increases the energy consumption even more. These reasons restrain the spread of CNNs on mobile devices.\nTo address the storage and memory requirements of neural networks, [1] used tensor decomposition techniques to compress fully-connected layers. They represented the parameters of the layers in the Tensor Train format [6] and learned the network from scratch in this representation. This approach provided enough compression to move the storage bottleneck of VGG-16 [5] from the fully-connected layers to convolutional layers. For a more detailed literature overview, see Sec. 5.\nIn this paper, we propose a tensor factorization based method to compress convolutional layers. Our contributions are:\n\u2022 We experimentally show that applying the Tensor Train decomposition \u2013 the compression technique used in [1] \u2013 directly to the tensor of a convolution yields poor results (see Sec. 6). We explain this behavior and propose a way to reshape the 4-dimensional kernel of a convolution into a multidimensional tensor to fully utilize the compression power of the Tensor Train decomposition (see Sec. 4). \u2022 We experimentally show that the proposed approach allows compressing a network that\nconsists only of convolutions up to 4\u00d7 times with 2% accuracy decrease (Sec. 6).\nar X\niv :1\n61 1.\n03 21\n4v 1\n[ cs\n.L G\n] 1\n0 N\n\u2022 We combine the proposed approach with the fully-connected layers compression of [1]. Compressing both convolutional and fully-connected layers of a network yields 82\u00d7 network compression with 1% accuracy drop, see Sec. 6."}, {"heading": "2 Convolutional Layer", "text": "A convolutional network is a type of feed-forward architecture that transforms an input image to the final class scores using a sequence of layers. The main building block of such networks is a convolutional layer, that transforms the 3-dimensional input tensor X \u2208 RW\u00d7H\u00d7C into the output tensor Y \u2208 RW\u2212l+1\u00d7H\u2212l+1\u00d7S by convolving X with the kernel tensor K \u2208 R`\u00d7`\u00d7C\u00d7S :\nY(x, y, s) = \u2211\u0300 i=1 \u2211\u0300 j=1 C\u2211 c=1 K(i, j, c, s)X (x+ i\u2212 1, y + j \u2212 1, c). (1)\nTo improve the computational performance, many deep learning frameworks reduce the convolution (1) to a matrix-by-matrix multiplication [7, 8] (see Fig. 1). We exploit this matrix formulation to motivate a particular way of applying the Tensor Train format to the convolutional kernel (see Sec. 4). In the rest of this section, we introduce the notation needed to reformulate convolution (1) as a matrix-by-matrix multiplication Y =XK.\nFor convenience, we denote H \u2032 = H \u2212 `+1 and W \u2032 =W \u2212 `+1. Let us reshape the output tensor Y \u2208 RW \u2032\u00d7H\u2032\u00d7S into a matrix Y of size W \u2032H \u2032 \u00d7 S in the following way\nY(x, y, s) = Y (x+W \u2032(y \u2212 1), s).\nLet us introduce a matrixX of size W \u2032H \u2032\u00d7 `2C, the k-th row of which corresponds to the `\u00d7 `\u00d7C patch of the input tensor that is used to compute the k-th row of the matrix Y\nX (x+ i\u2212 1, y + j \u2212 1, c) = X(x+W \u2032(y \u2212 1), i+ `(j \u2212 1) + `2(c\u2212 1)), where y = 1, . . . ,H \u2032, x = 1, . . . ,W \u2032, i, j = 1, . . . , `. Finally, we reshape the kernel tensor K into a matrixK of size `2C \u00d7 S\nK(i, j, c, s) = K(i+ `(j \u2212 1) + `2(c\u2212 1), s). Using the matrices defined above, we can rewrite the convolution definition (1) as Y =XK.\nNote that the compression approach presented in the rest of the paper works with other types of convolutions, such as convolutions with padding, stride larger than 1, or rectangular filters. But for clarity, we illustrate the proposed idea on the basic convolution (1)."}, {"heading": "3 Tensor Train Decomposition", "text": "The TT-decomposition (or TT-representation) of a tensor A \u2208 Rn1\u00d7...\u00d7nd is the set of matrices Gk[jk] \u2208 Rrk\u22121\u00d7rk , where jk = 1, . . . , nk, k = 1, . . . , d, and r0 = rd = 1, such that each of the tensor elements can be represented as\nA(j1, j2, . . . , jd) = G1[j1]G2[j2] . . .Gd[jd]. (2)\nThe elements of the collection {rk}dk=0 are called TT-ranks. The collections of matrices {{Gk[jk]}nkjk=1} d k=1 are called TT-cores [6].\nThe TT-format requires \u2211d\nk=1 nkrk\u22121rk parameters to represent a tensor A \u2208 Rn1\u00d7...\u00d7nd which has \u220fd k=1 nk elements. The TT-ranks rk control the trade-off between the number of parameters versus the accuracy of the representation: the smaller the TT-ranks, the more memory efficient the TT-format is. An attractive property of the TT-format is the ability to efficiently perform basic linear algebra operations on tensors by working on the TT-cores of the TT-format, i.e. without materializing the tensor itself [6].\nFor a matrix \u2013 a 2-dimensional tensor \u2013 the TT-decomposition coincides with the matrix lowrank decomposition. To represent a matrix more compactly than in the low-rank format, the matrix TT-format is defined in a special way. Let us consider a matrix A of size M \u00d7 N, where M = \u220fd k=1mk, N = \u220fd k=1 nk, and reshape it into a tensor A of size n1m1\u00d7n2m2\u00d7 . . .\u00d7ndmd by defining bijective mappings \u00b5(`) = (\u00b51(`), . . . , \u00b5d(`)) and \u03bd(t) = (\u03bd1(t), . . . , \u03bdd(t)). The mapping \u00b5(\u00b7) maps row index ` = 1, . . . ,M into a d-dimensional vector index, where k-th dimension \u00b5k(\u00b7) varies from 1 to mk. The bijection \u03bd(\u00b7) maps column index t = 1, . . . , N into a d-dimensional vector index, where k-th dimension \u03bdk(\u00b7) varies from 1 to nk. Thus, using these mappings, we can form the tensor A, whose k-th dimension is indexed by the compound index (\u00b5k(\u00b7), \u03bdk(\u00b7)), and consider its TT-representation:\nA(`, t) = A((\u00b51(`), \u03bd1(t)), . . . , (\u00b5d(`), \u03bdd(t))) = G1[(\u00b51(`), \u03bd1(t))] . . .Gd[(\u00b5d(`), \u03bdd(t))]."}, {"heading": "4 TT-convolutional Layer", "text": "In this section, we propose two ways to represent a convolutional kernel K in the TT-format. One way is to apply the TT-decomposition to the tensor K directly. To see the drawbacks of this approach, consider a 1\u00d7 1 convolution, which is a small fully-connected layer applied to the channels of the input image in each pixel location. The kernel of such convolution is essentially a 2-dimensional array, and the TT-decomposition of 2-dimensional arrays coincides with the matrix low-rank format. But for fully-connected layers, the matrix TT-format proved to be more efficient than the matrix low-rank format [1]. Thus, we seek for a decomposition that would coincide with the matrix TT-format on 1\u00d7 1 convolutions. Taking into account that a convolutional layer can be formulated as a matrix-by-matrix multiplication (see Sec. 2), we reshape the 4-dimensional kernel tensor into a matrix K of size `2C \u00d7 S, where K(x, y, c, s) = K(`(y \u2212 1) + x+ `2(c\u2212 1), s). Then we apply the matrix TT-format (see Sec. 3) to the matrix K, i.e. reshape it into a tensor K\u0303 and convert it into the TT-format. To reshape the matrix K into a tensor, we assume that its dimensions factorize: C = \u220fd i=1 Ci and S = \u220fd i=1 Si. This assumption us not restrictive since we can always add some dummy channels filled with zeros to increase the values of C and S. Then we can define a (d + 1)-dimensional tensor, where k-th dimension has the length CkSk for k = 1, . . . , d and `2 for k = 0. Thus we obtain the following representation of the matrixK\nK(x+ `(y \u2212 1) + `2(c\u2032 \u2212 1), s\u2032) = K\u0303( (x+ `(y \u2212 1), 1), (c1, s1), . . . , (cd, sd)) = = G\u03030[x+ `(y \u2212 1), 1]G1[c1, s1] . . .Gd[cd, sd], (3)\nwhere c\u2032 = c1 + \u2211d i=2(ci \u2212 1) \u220fi\u22121 j=1 Cj and s \u2032 = s1 + \u2211d i=2(si \u2212 1) \u220fi\u22121 j=1 Sj .\nTo simplify the notation, we index the 0-th core G\u03030 with x and y: G0[x, y] = G\u03030[`(y \u2212 1) + x, 1], where x, y = 1, . . . , `. Finally, substituting G\u03030 into (3), we obtain the following decomposition of the convolution kernel K\nK(x, y, c\u2032, s\u2032) = G0[x, y]G1[c1, s1]...Gd[cd, sd]. (4)\nTo summarize our pipeline starting from an input tensor X (an image): the TT-convolutional layer firstly reshapes the input tensor into a (2+d)-dimensional tensor X\u0303 of size W \u00d7H \u00d7C1\u00d7 . . .\u00d7Cd; then, the layer transforms the input tensor X\u0303 into the output tensor Y\u0303 of size (W \u2212 `+ 1)\u00d7 (H \u2212\n`+ 1)\u00d7 S1 . . .\u00d7 Sd in the following way\nY\u0303(x, y, s1, . . . , sd) = \u2211\u0300 i=1 \u2211\u0300 j=1 \u2211 c1,...,cd X\u0303 (i+ x\u2212 1, j + y \u2212 1, c1, . . . , cd)\nG0[i, j]G1[c1, s1] . . .Gd[cd, sd].\nNote that for a 1 \u00d7 1 convolution (` = 1), the x and y indices vanish from the decomposition (4). The convolutional kernel collapses into a matrix {K(1, 1, c\u2032, s\u2032)}C,Sc\u2032=1,s\u2032=1 and the decomposition (4) for this matrix coincides with the Tensor Train format for the fully-connected layer proposed in [1].\nTo train a network with TT-conv layers, we treat the elements of the TT-cores as the parameters of the layer and apply stochastic gradient descent with momentum to them. To compute the necessary gradients we use automatic differentiation implemented in TensorFlow [9]."}, {"heading": "5 Related Work", "text": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12]. However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1]. This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].\nOne approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17]. Our approach is compatible with the quantization technique: one can quantize the elements of the TT cores of the decomposition. Some works also add Huffman coding on top of other compression techniques [5], which is also compatible with the proposed method.\nAnother approach is to use tensor or matrix decompositions. CP-decomposition [18] and Kronecker product factorization [19] allow to speed up the inference time of convolutions and compress the network as a side effect."}, {"heading": "6 Experiments", "text": "We evaluated the compressing strength of the proposed approach on CIFAR-10 dataset [20], which has 50 000 train images and 10 000 test images. In all the experiments, we used stochastic gradient descent with momentum with coefficient 0.9, trained for 100 epochs starting from the learning rate of 0.1 and decreased it 10\u00d7 after each 30 epochs. To make the experiments reproducible, we released the codebase1.\nWe used two architectures as references: the first one is dominated by the convolutions (they occupy 99.54% parameters of the network), and the second one is dominated by the fully-connected layers (they occupy 95.98% parameters of the network).\nConvolutional network. The first network has the following architecture: conv (64 output channels); BN; ReLU; conv (64 output channels); BN; ReLU; max-pool (3\u00d7 3 with stride 2); conv (128 output channels); BN; ReLU; conv (128 output channels); BN; ReLU; max-pool (3\u00d7 3 with stride 2); conv (128 output channels); BN; ReLU; conv (128 output channels); avg-pool (4\u00d7 4); fc (128\u00d7 10), where \u2019BN\u2019 stands for batch normalization [21] and all convolutional filters are of size 3 \u00d7 3. To compress the network we replace each convolutional layer excluding the first one (it contains less than 1% of the network parameters) with the TT-conv layer (see Sec. 4). For training, we initialize the TT-cores of the TT-conv layers with random noise and train the whole network from scratch.\nWe compare the proposed TT-convolution against the naive approach \u2013 directly applying the TTdecomposition to the 4-dimensional convolutional kernel (see Sec.4). We report that on the 2\u00d7 compression level the proposed approach (0.8% loss of accuracy) outperforms the naive baseline (2.4% loss of accuracy), for details see Tbl. 1a.\n1https://github.com/timgaripov/TensorNet-TF\nTable 1: Compressing convolutional networks on the CIFAR-10 dataset (see Sec. 6). Different rows with the same model name correspond to different choices of the TT-ranks.\n(a) Compressing the first baseline (\u2018conv\u2019), which is dominated by convolutions. \u2018TT-conv\u2019: the proposed compression method; \u2018TT-conv (naive)\u2019: direct application of the TT-decomposition to convolutional kernels.\nModel top-1 acc. compr.\nconv (baseline) 90.7 1 TT-conv 89.9 2.02 TT-conv 89.2 2.53 TT-conv 89.3 3.23 TT-conv 88.7 4.02 TT-conv (naive) 88.3 2.02 TT-conv (naive) 87.6 2.90\n(b) Compressing the second baseline (\u2018conv-fc\u2019), which is dominated by fully-connected layers. \u2018conv-TT-fc\u2019: only the fully-connected part of the network is compressed; \u2018TT-conv-TT-fc\u2019: fullyconnected and convolutional parts are compressed.\nModel top-1 acc. compr.\nconv-fc (baseline) 90.5 1 conv-TT-fc 90.3 10.72 conv-TT-fc 89.8 19.38 conv-TT-fc 89.8 21.01 TT-conv-TT-fc 90.1 9.69 TT-conv-TT-fc 89.7 41.65 TT-conv-TT-fc 89.4 82.87\nNetwork with convolutions and fully-connected layers. The second reference network was obtained from the first one by replacing the average pooling with two fully-connected layers of size 8192\u00d7 1536 and 1536\u00d7 512. To compress the second network, we replace all layers excluding the first and the last one (they occupy less than 1% of parameters) with TT-conv and TT-fc [1] layers. To speed up the convergence, we trained the network in two stages: first we replaced only the convolutional layers with TT-conv layers and trained the network; then we replaced the fully-connected layers with randomly initialized TT-fc layers and fine-tuned the whole model.\nTo compare against [1] we include the results of compressing only fully-connected layers (Tbl. 1b). Initially, the fully-connected part was the memory bottleneck and it was more fruitful to compress it while leaving the convolutions untouched: we obtained 10.72\u00d7 network compression with 0.2% accuracy drop by compressing only fully-connected layers, and 9.61\u00d7 compression with 0.4% accuracy drop by compressing both fully-connected and convolutional layers. But after the first gains, the bottleneck moved to the convolutional part and the fully-connected layers compression capped at about 21\u00d7 network compression. At this point, by additionally factorizing the convolutions we raised the network compression up to 80\u00d7 while losing 1.1% of accuracy (Tbl. 1b)."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed a tensor decomposition approach to compressing convolutional layers of a neural network. By combing this convolutional approach with the work [1] for fully-connected layers, we compressed a convolutional network 80\u00d7 times. These results make a step towards the era of embedding compressed models into smartphones to allow them constantly look and listen to its surroundings.\nIn the future work, we will experiment with the proposed approach on the ILSVRC-2012 dataset [22] on state-of-the-art neural architectures."}], "references": [{"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D. Vetrov"], "venue": "Advances in Neural Information Processing Systems 28 (NIPS), pages 442\u2013450.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1510.00149, 2,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor-Train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM J. Scientific Computing, 33(5):2295\u20132317,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd ACM international conference on Multimedia,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo"], "venue": "on heterogeneous systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "International Conference on Machine Learning (ICML), pages 2285\u20132294,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "Interspeech, pages 2365\u20132369,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "International Conference of Acoustics, Speech, and Signal Processing (ICASSP), pages 6655\u20136659,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "PerforatedCNNs: Acceleration through elimination of redundant convolutions", "author": ["M. Figurnov", "A. Ibraimova", "D. Vetrov", "P. Kohli"], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Speeding-up convolutional neural networks using fine-tuned CP-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Factorized convolutional neural networks", "author": ["M. Wang", "B. Liu", "H. Foroosh"], "venue": "arXiv preprint arXiv:1608.04337,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Convolutional Neural Networks (CNNs) show state-of-the-art performance on many problems in computer vision, natural language processing and other fields [2, 3].", "startOffset": 153, "endOffset": 159}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) show state-of-the-art performance on many problems in computer vision, natural language processing and other fields [2, 3].", "startOffset": 153, "endOffset": 159}, {"referenceID": 3, "context": "Moreover, these networks contain millions of trainable parameters and consume hundreds of megabytes of storage and memory bandwidth [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Thus, CNNs are forced to use RAM instead of solely relying on the processor cache \u2013 orders of magnitude more energy efficient memory device [5] \u2013 which increases the energy consumption even more.", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "To address the storage and memory requirements of neural networks, [1] used tensor decomposition techniques to compress fully-connected layers.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "They represented the parameters of the layers in the Tensor Train format [6] and learned the network from scratch in this representation.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "This approach provided enough compression to move the storage bottleneck of VGG-16 [5] from the fully-connected layers to convolutional layers.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "\u2022 We experimentally show that applying the Tensor Train decomposition \u2013 the compression technique used in [1] \u2013 directly to the tensor of a convolution yields poor results (see Sec.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "\u2022 We combine the proposed approach with the fully-connected layers compression of [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "To improve the computational performance, many deep learning frameworks reduce the convolution (1) to a matrix-by-matrix multiplication [7, 8] (see Fig.", "startOffset": 136, "endOffset": 142}, {"referenceID": 7, "context": "To improve the computational performance, many deep learning frameworks reduce the convolution (1) to a matrix-by-matrix multiplication [7, 8] (see Fig.", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "The collections of matrices {{Gk[jk]} jk=1} d k=1 are called TT-cores [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "without materializing the tensor itself [6].", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "But for fully-connected layers, the matrix TT-format proved to be more efficient than the matrix low-rank format [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "The convolutional kernel collapses into a matrix {K(1, 1, c\u2032, s\u2032)} c\u2032=1,s\u2032=1 and the decomposition (4) for this matrix coincides with the Tensor Train format for the fully-connected layer proposed in [1].", "startOffset": 200, "endOffset": 203}, {"referenceID": 8, "context": "To compute the necessary gradients we use automatic differentiation implemented in TensorFlow [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 10, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 0, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 11, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 12, "context": "However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1].", "startOffset": 97, "endOffset": 105}, {"referenceID": 13, "context": "However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1].", "startOffset": 97, "endOffset": 105}, {"referenceID": 0, "context": "However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1].", "startOffset": 208, "endOffset": 211}, {"referenceID": 4, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 14, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 15, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 16, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 17, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 4, "context": "One approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17].", "startOffset": 204, "endOffset": 215}, {"referenceID": 14, "context": "One approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17].", "startOffset": 204, "endOffset": 215}, {"referenceID": 15, "context": "One approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17].", "startOffset": 204, "endOffset": 215}, {"referenceID": 4, "context": "Some works also add Huffman coding on top of other compression techniques [5], which is also compatible with the proposed method.", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "CP-decomposition [18] and Kronecker product factorization [19] allow to speed up the inference time of convolutions and compress the network as a side effect.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "CP-decomposition [18] and Kronecker product factorization [19] allow to speed up the inference time of convolutions and compress the network as a side effect.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "We evaluated the compressing strength of the proposed approach on CIFAR-10 dataset [20], which has 50 000 train images and 10 000 test images.", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "The first network has the following architecture: conv (64 output channels); BN; ReLU; conv (64 output channels); BN; ReLU; max-pool (3\u00d7 3 with stride 2); conv (128 output channels); BN; ReLU; conv (128 output channels); BN; ReLU; max-pool (3\u00d7 3 with stride 2); conv (128 output channels); BN; ReLU; conv (128 output channels); avg-pool (4\u00d7 4); fc (128\u00d7 10), where \u2019BN\u2019 stands for batch normalization [21] and all convolutional filters are of size 3 \u00d7 3.", "startOffset": 401, "endOffset": 405}, {"referenceID": 0, "context": "To compress the second network, we replace all layers excluding the first and the last one (they occupy less than 1% of parameters) with TT-conv and TT-fc [1] layers.", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "To compare against [1] we include the results of compressing only fully-connected layers (Tbl.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "By combing this convolutional approach with the work [1] for fully-connected layers, we compressed a convolutional network 80\u00d7 times.", "startOffset": 53, "endOffset": 56}, {"referenceID": 20, "context": "In the future work, we will experiment with the proposed approach on the ILSVRC-2012 dataset [22] on state-of-the-art neural architectures.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80\u00d7 network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.", "creator": "LaTeX with hyperref package"}}}