{"id": "1611.03608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Greedy Step Averaging: A parameter-free stochastic optimization method", "abstract": "in this paper we present the direct greedy step averaging ( gsa ) method, a parameter - free stochastic optimization algorithm for a variety of machine learning problems. such as a gradient - based optimization method, gsa makes use of the information from the minimizer of a single sample's loss function, and takes average strategy to calculate either reasonable computed learning rate sequence. while most existing gradient - based algorithms introduce an increasing number of hyper parameters or try to make calculations a trade - off between computational cost and declining convergence efficiency rate, gsa avoids the manual tuning of learning rate and brings users in no more hyper parameters or extra cost. we perform exhaustive numerical experiments for logistic and parameter softmax regression to compare our method with the other state of the art ones on 16 indexed datasets. results show that gsa is robust on various scenarios.", "histories": [["v1", "Fri, 11 Nov 2016 08:23:30 GMT  (414kb)", "http://arxiv.org/abs/1611.03608v1", "23 pages, 24 figures"]], "COMMENTS": "23 pages, 24 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiatian zhang", "fan yao", "yongjun tian"], "accepted": false, "id": "1611.03608"}, "pdf": {"name": "1611.03608.pdf", "metadata": {"source": "CRF", "title": "Greedy Step Averaging: A parameter-free stochastic optimization method", "authors": ["Xiatian Zhang", "Fan Yao", "Yongjun Tian"], "emails": ["yongjun.tian}@tendcloud.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n03 60\n8v 1\n[ cs\n.L G\n] 1\n1 N\nov 2\nKeywords Optimization, algorithm, learning rate, parameter-free, self-adaptive, averaging strategy\n1 Introduction\nA large number of problems in machine learning involve minimizing the sum of a loss function over different training samples. The most prevalent algorithm for these problems is stochastic gradient descent method(SGD) [1, 2]. During a typical iteration step of SGD, the iterator randomly chooses a training instance, takes the negative gradient direction \u2212\u2202li\u2202\u03c9 of the local loss function li as the descent direction, and moves forward with a specified step length, or learning rate \u03bbt. In addition to SGD, there is the standard full gradient(FG) method. Instead of using the gradient of the loss function of a single training sample, it employs evaluation of the total loss function. Although FG method enjoys a much better convergence performance, its formidable computational cost makes it intractable in large scale context: in each step we have to visit all training sample to obtain the full gradient. In industry communities, large datasets are usually under distributed storage and traverse over the whole dataset induce a considerable or even unaffordable input-output expenditure. As a matter of fact, in most industrial cases we only need to obtain a model with tolerable accuracy within limited time and memory. Thus, the SGD method becomes increasingly popular as an expedient alternative to FG method.\nCompared with FG, SGD enjoys greater efficiency in computation and implementation. However, SGD has an evident weakness: the tuning of learning rate \u03b7 is usually a heavy price to pay. Theoretically, to guarantee convergence we need to set learning rate \u03b7t as a decreasing sequence towards zero. However, the decreasing sequence is not well-grounded: why should we regard the training sample being scanned later less important than those being scanned earlier? In fact, since samples with brand new labels could appear at later stage of training process, we should not treat them with a comparatively small learning rate. And not to mention we have to specify the initial value as well as the decreasing rate of \u03b7t. If we choose a constant step-size-strategy, the dilemma arises in another sense especially on large datasets: large \u03b7 leads to faster convergence but may inevitably bring in more fluctuations and a small \u03b7, while it may guarantee a smooth decreasing loss, often entails an unacceptably low convergence rate. In fact, the convergence rate and fluctuation of loss function, equivalent to the estimation bias and variance of parameter \u03c9,\nare intrinsically contradictive [6] and we have to strike a balance between the two. Many attempts to address the dilemma including mini-batch strategy and many other variance reduction tactics have been made. In recent years, there have also emerged many self-adaptive methods like Adagrad, Adadelta, Adam. The ideas behind all these methods are to collect gradient information in previous iterations to calculate a preferable learning rate for the current step. However, all these methods need to specify an initial learning rate which is also a sensible parameter. Moreover, these methods are capable of dynamically adapting the learning rate by introducing new parameters. We have to tune these extra parameters to fit various circumstances. These methods can outperform traditional SGD in some scenarios. For example, Adadelta is claimed to be pretty effective for back propagation(BP) algorithm in neural network training.\nHowever, there is another way to improve SGD: laying aside the learning rate selection issue and trying to alter the scheme to reach an accelerated linear convergence rate. There are already some breakthroughs, such as SAG, SVRG, SCSG algorithms. By exploiting history information of gradient and iteration sequence, these methods achieve a better performance. However, they introduce other sensitive parameters(SVRG, SCSG) and require intermittent full gradient evaluation and enormous memory occupation (SAG). And great efforts are needed to tune relevant parameters to guarantee convergence. Therefore, these attempts to improve SGD have encountered difficulties in most industrial cases.\nFor the reasons we have discussed above, we propose to enhance SGD in a new way which can maintain its greatest advantage of swiftness and at the same time avoid parameter tuning once and for all by introducing a practical step-size selection rule. This new method named as greedy step averaging(GSA) is a self-adaptive parameter free stochastic optimization algorithm for many machine learning problems, and works well for logistic and softmax regression in our experiment. It is equipped with a dynamically selected learning rate and maintains the same order of computational and memory cost.\nThe paper is organized as follows. In the next section, we will present some related works including most state of the art stochastic optimization algorithms which we will later compare with GSA. In the third and fourth section we introduce the heuristic idea and formulation of GSA. In the fifth section, we establish convergence analysis of the method. Finally, in section five and six, we conduct a comparison between GSA and the other methods mentioned above and offer some further discussions.\n2 Related Work\nThere is a large variety of optimization methods derived from SGD utilizing merely the stochastic gradient information. We will not discuss algorithms that are infeasible in computation for highdimensional data sets, e.g. second-order methods such as Newton\u2019s method. Some of these methods claim to accelerate the convergence of SGD like Momentum, SAG, SVRG, at least in some specific circumstances while others claim using former iterative information to compute a optimal update. A full review of this immense literature would be outside the scope of our work. We only comment on the state of the art algorithms and several of the most widely-known ideas.\nFirst we introduce the basic set up of SGD. Suppose our objective loss function is\nminimize\u03c9\u2208Rp g(\u03c9) =\nn \u2211\ni=1\nfi(\u03c9), (1)\nwhere \u03c9 \u2208 Rp denotes the model parameter, fi(\u03c9) = Loss(\u03c9, xi, yi) denotes the loss function with respect to i-th training sample (xi, yi).\nThe update scheme of \u03c9 is thus given by\n\u03c9(k+1) = \u03c9(k) \u2212 \u03b7k\u2207fik(\u03c9 (k)). (2)\nDenote \u03c9\u2217 the unique minimizer of g(\u03c9). It has been proved that the full gradient method achieves a linear convergence rate:\ng(\u03c9k)\u2212 g(\u03c9\u2217) = O(\u03c1k),\nwhere \u03c1 is a constant depending on the condition number of g[5].\nThe convergence performance of SGD is sublinear. Under standard assumptions, for SGD with constant step-size, one can obtain the unique minimizer with a linear convergence rate in a certain tolerance C:\nE[g(\u03c9k)]\u2212 g(\u03c9\u2217) = O(\u03c1k) + C,\nwhere the expectation is taken with respect to all possible selection sequence of training samples, and C is a constant depending on the condition number of g and growing quadratically with n. To achieve a stable convergence behaviour, people often apply a diminishing step-size rule. For SGD under this strategy, we can reach a sublinear convergence rate[3, 4]:\nE[g(\u03c9k)]\u2212 g(\u03c9\u2217) = O(1/k).\nMomentum\nThe Momentum method adds momentum term to SGD, the update scheme has the following form\n\u03c9(k+1) = \u03c9(k) \u2212 \u03b7k\u2207fik(\u03c9 (k)) + \u03b2k(\u03c9 (k) \u2212 \u03c9(k\u22121)).\nMomentum helps accelerate SGD in the relevant direction and dampens oscillations by adding a fraction betak of the update vector of the past time step to the current update vector. The name Momentum demonstrates its property vividly: when move toward the approximated negative gradient direction in the k+1\u2212 th step by \u03b7k, the iterator also move towards the previous direction by \u03b2k for momentum. If we choose \u03b2k = \u03b2 to be a constant, the scheme yields\n\u03c9(k+1) = \u03c9(k) \u2212\nk \u2211\nj=1\n\u03b7j\u03b2 k\u2212j\u2207fij (\u03c9 (j)).\nWe can see that the Momentum method exploits all the previous direction and uses its weighted average as the approximation to the current gradient. Some experiments indicates this method can improve the performance in cases in which the loss function has a narrow valleys. However, the theoretical convergence analysis still remains an open issue. Moreover, we have to allocate extra memory to keep track of previous \u03c9.\nGradient Averaging\nThe Gradient Averaging method [10] is equivalent to the Momentum mentioned above, if we choose the simple arithmetic average to substitute the weighted average in Momentum. The Gradient Averaging method is proved to have a O(1/k) convergence rate, the same as SGD. Its scheme has form\n\u03c9(k+1) = \u03c9(k) \u2212 \u03b7k k\nk \u2211\nj=1\n\u2207fij (\u03c9 (j)).\nNote that one do not have to store all the previous gradient term. Instead, one can only keep track of the average of previous k stochastic gradients Gk = 1 k \u2211k j=1 \u2207fij (\u03c9 (j)), and use formula Gk+1 = k k+1Gk + 1 k+1\u2207fik+1(\u03c9 (k+1)) to evaluate Gk+1 on the run.\nIterate Averaging\nThe idea behind Momentum and Gradient Averaging is to utilize previous gradient information to determine a better descent direction to accelerate convergence. However, rather than averaging the gradients, the previous iterative points \u03c9k can also be taken into account. Some authors use the basic SG iteration but take an average over \u03c9k values to give a new algorithm. With a suitable choice of step-sizes, this gives the same asymptotic efficiency as Newton-like second-order SG methods and also leads to increased robustness of the convergence rate to the exact sequence of step sizes [11]. The update scheme reads\n\u03c9\u0304(k+1) = \u03c9(k) \u2212 \u03b7k\u2207fik(\u03c9 (k)),\n\u03c9(k+1) = 1\nk + 1\nk+1 \u2211\nj=1\n\u03c9\u0304(j).\nThe Iterate Averaging method uses all the previous iterative points and take its average as the next searching point. It has been proved that under certain assumptions of appropriate step-size, this method enjoys a second-order convergence rate[11]. Even for a fixed step-size strategy, it can also show a great robustness to avoid oscillations. But unfortunately, it is extremely sensible to\nthe initial points: a bad starting point can not only hinder the convergence rate but also cause divergence. Besides, it also requires an extra O(m) memory cost as Momentum.\nStochastic Average Gradient\nA typical SAG computes the full gradients at the beginning of iteration, in each following step it chooses a sample\u2019s gradient randomly to refresh the full gradients. The update scheme reads\n\u03c9(k+1) = \u03c9(k) \u2212 \u03b7k n\nn \u2211\ni=1\nd (k) i ,\nwhere the ik denotes the randomly chosen index, and\nd (k) i =\n{\n\u2207fi(\u03c9 (k)), if i = ik, d (k\u22121) i , otherwise.\nLike FG method, SAG incorporates a gradient with respect to each training sample, but like SGD, in each iteration it only computes the gradient with respect to a single training example and the cost of the iteration is independent of n. In [12], the authors show that the SAG iterations have a linear convergence rate. However SAG have at least 2 drawbacks. First it involves the evaluation of full gradient.Even though it calls for full gradient only once, it is hard to implement in some scenarios. Maybe this difficulty can be resolved by arbitrarily choose the initial gradients for each sample(e.g. zero vectors), but the second weakness makes it completely infeasible in certain scenarios: SAG has an extremely large memory cost O(np) because it has to store previous gradient for each sample. There is no free lunch. These are prices we have to pay for linear convergence.\nSV RG\nStochastic variance reduced gradient(SVRG) introduces an explicit variance reduction method for SGD[13]. SVRG method separates the training process into several epochs, in the beginning of each epoch, it requires the computation of full gradient. And during each epoch, one randomly chooses a sample\u2019s gradient to refresh the full gradient. The update scheme reads\n\u03c9(k+1) = \u03c9(k) \u2212 \u03b7(\u2207fi(\u03c9 (k))\u2212\u2207fi(\u03c9\u0303) + \u00b5\u0303)\nwhere \u00b5\u0303 = 1n \u2211n\ni=1(\u2207fi(\u03c9\u0303) is the full gradient, \u03c9\u0303 is updated at the beginning of each epoch. SVRG also has a linear convergence rate, but there are at least 3 parameters to tune: the number of epoch, the iteration number in each epoch and the learning rate. Moreover, SVRG has to evaluate full gradient several times, which will also restricts its application in large scale context. Another variation of SVRG is SAGA [15] which is claimed to support non-strongly convex problems directly and has a better convergence rate. It is essentially at the midpoint between SVRG and SAG.\nSCSG\nStochastically controlled stochastic gradient(SCSG) is a variation of SVRG. As a great improvement of SVRG, the computation cost and the communication cost of SCSG do not necessarily scale linearly with sample size n[14].\nThe main point of SCSG is replacing the full gradient at the beginning of each epoch with batch gradient, and drawing a poisson random number to determine the number of sample to visit in each epoch. However for this algorithm, the batch size is very sensitive. The optimal choice requires some priori knowledge concern to the total loss function.\nAdagrad\nAdagrad [8] is an algorithm for gradient-based optimization, it adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. Adagrad can greatly improve the robustness of SGD because it allocates heterogeneous learning rates on different components of \u03c9 at each iteration step. The update scheme reads\n\u03c9 (k+1) i = \u03c9 (k) i \u2212\n\u03b7 \u221a\nGk,ii + \u03b5 \u00b7 \u2207fi(\u03c9\n(k)),\nGk \u2208 R p\u00d7p here is a diagonal matrix where each diagonal element is the sum of the squares of the gradients with respect component-i up to iteration step-k, and \u03b5 us a smoothing term that avoids division by zero.\nAdagrad\u2019s main weakness is the procedure of accumulating squared gradients in the denominator: Since each added term is positive, the accumulated sum keeps growing during training. This causes the learning rate to shrink and eventually becomes infinitesimally small. As a result, the algorithm is no longer able to acquire additional knowledge. Besides, although Adagrad provides a self-adaptive way to dynamically change the learning rate \u03b7, its initial value still remains to be tuned.\nAdadelta\nAdadelta [9] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w. Instead of inefficiently storing w previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average E[g2]t at time step t then depends (as a fraction \u03b3 similarly to the Momentum term) only on the previous average and the current gradient:\nE[g2]t = \u03b3E[g 2]t\u22121 + (1\u2212 \u03b3)g 2 t (3)\nAnd then they define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:\nE[\u2206\u03b82]t = \u03b3E[\u2206\u03b8 2]t\u22121 + (1\u2212 \u03b3)\u2206\u03b8 2 t (4)\nAnd the update scheme reads\n\u03c9 (k+1) i = \u03c9 (k) i \u2212\n\u221a\nE[\u2206\u03b82]t\u22121 + \u03b5 \u221a\nE[g2]t + \u03b5 \u00b7 \u2207fi(\u03c9\n(k)). (5)\nWith Adadelta, we do not need to set a default learning rate since it has been eliminated from the update rule. The weakness of Adadelta, the same as Adagrad, is that it needs extra O(m) memory cost since each component has a different learning rate.\n3 The Greedy Step Averaging Algorithm\nThe greedy step averaging(GSA) algorithm we are proposing is an enhancement of SGD and therefore can serve as a general optimization framework for a variety class of machine learning algorithms. For example, linear regression, logistic regression, softmax regression, etc. The basic idea of the GSA algorithm is to perform exact line search for each sample\u2019s loss function step by step. Specifically, consider the general loss function\nminimize\u03c9\u2208Rp Loss(\u03c9) =\nn \u2211\ni=1\nli(\u03c9),\nwhere \u03c9 \u2208 Rp denotes the model parameter, li(\u03c9) = Loss(\u03c9, xi, yi) denotes the loss function with respect to i-th training instance (xi, yi). During a typical iteration step of SGD, the iterator takes a training instance, chooses the negative gradient \u2212\u2202li\u2202\u03c9 of the local loss function li as the descent direction, then calculates an optimal step length etat,i ensuring li(\u03c9) to reach the minimum. The update scheme of \u03c9 is thus given by\n\u03c9(t+1) = \u03c9(t) \u2212 \u03b7t,i \u2202li \u2202\u03c9 .\nThe step length \u03b7t,i is calculated through a somehow greedy principle, and that is where the greedy step in the nomenclature of our algorithm comes from.\n3.1 Linear Regression\nFor linear regression, \u03b7t,i has a closed form. Consider its quadratic loss function\nli(\u03c9) = 1\n2 (yi \u2212 \u03c9\nTxi) 2,\nwhere the intercept term is already absorbed in xi. We can write the update scheme as following:\n\u03c9(t+1) = \u03c9(t) \u2212 \u03b7t,ixi(yi \u2212 \u03c9 (t)xi)\nSubstitute (1) into li(\u03c9 (t+1)) = 0 yields\n\u03b7t,i = 1\nxTi xi .\n3.2 Logistic Regression\nFor logistic regression, \u03b7t,i should be obtained by tricks. Consider its cross-entropy loss function\nli(\u03c9) = \u2212yi\u03b2 Txi + log\n( 1 + exp(\u03c9Txi) ) ,\nwhere pi = 1 1+exp(\u2212\u03c9Txi) , \u2202li(\u03c9)\u2202\u03c9 = \u2212xi(yi \u2212 pi). Thus li(\u03c9 (t+1)) = 0 yields an intractable solution \u03b7t,i \u2192 \u221e. However, this can be easily overcame by avoiding choosing the target value at sigmoid function\u2019s asymptotes. Instead, we set a confidence threshold (in fact this idea is also recommended by Yann Lecun for training neural network [19]) at, for example, p\u03021 = 0.95, p\u03020 = 1 \u2212 p\u03021,. Each time we see a positive training instance xi(yi = 1), we update \u03c9 along the local negative gradient while satisfying pi = p\u03021, and vice versa. That is,\n{ pi = 1\n1+exp(\u2212\u03c9(t+1)\u00b7xi) = p\u03021,\n\u03c9(t+1) = \u03c9(t) + \u03b7t,ixi(yi \u2212 pi).\nthe solution gives\n\u03b7t,i = sgn(yi \u2212 0.5) log(p\u03021/p\u03020)\u2212 \u03c9\n(t) \u00b7 xi\nxTi xi(yi \u2212 pi) .\n3.3 Softmax Regression\nSoftmax regression is an extension of logistic regression which serves as an efficient multi-class classification model. However, the calculation of greedy step length \u03b7t,i cannot be directly applied to softmax regression model since the solution of \u03b7t,i does not have a closed form. This problem can be solved by introducing an approximation formula. In a softmax regression model, we assign a weight \u03c9k to class-k respectively, where k \u2208 {1, 2, ..., L}. Let p (k) i = P (Y = k|xi) =\nexp (\u03c9kxi)\u2211 L j=1 exp (\u03c9jxi)\ndenotes the probability that xi belongs to class-k. Thus the loss function of a training instance xi with label k is\nli(\u03c9) = \u2212 log exp (\u03c9kxi)\u2211 L j=1 exp (\u03c9jxi) ,\n= \u2212\u03c9kxi + log (1 + \u2211L j=1 exp (\u03c9jxi)). (6)\nAnd\n\u2202li \u2202\u03c9l =\n{\n\u2212xi(1\u2212 p (l) i ), l = k, xip (l) i , l 6= k.\nThus the greedy step length \u03b7 solves the equation(we omit the subscript-i for simplicity):\np\u0302k = exp [(\u03c9k+\u03b7(1\u2212p (k))xT )x]\u2211 j 6=k exp [(\u03c9k\u2212\u03b7p (j)xT )x]+exp [(\u03c9k+\u03b7(1\u2212p(k))xT )x]\n= exp(\u03c9kx)\u00b7exp((1\u2212p (k))\u03b7)\u2211\nj 6=k exp(\u03c9jx)\u00b7exp(\u2212p (j)\u03b7)+exp(\u03c9kx)\u00b7exp((1\u2212p(k))\u03b7)\n,\n(\u03bb = \u03b7xTx) = e\u03bbek\u00b7b \u2212\u03bb k\u2211\nj 6=k ej \u00b7b \u2212\u03bb j +e \u03bbek\u00b7b \u2212\u03bb k\n\u2248 e\u03bbek\u00b7b \u2212\u03bb k\u2211L\nj=1 ej \u00b7b \u2212\u03bb j\n, (\u03bb << 1).\n(7)\nwhere ej = exp(\u03c9jx), bj = exp(p (j)). Notice that bj \u2208 (1, e), bk e \u2208 (e \u22121, 1). Since we assume \u03bb << 1, thus we can apply linear approximation to the right hand side of (7):\nb\u2212\u03bbj \u2248 1\u2212 \u03bb ln bj \u2248 1\u2212 (bj \u2212 1)\u03bb,\n(bk/e) \u2212\u03bb = (e/bk) \u03bb \u2248 1 + \u03bb ln(e/bk) \u2248 1 + (e/bk \u2212 1)\u03bb,\nSubstitute into (7) to give a linear equation with respect to \u03bb:\np\u0302k\nL \u2211\nj=1\nej(1 + \u03bb\u2212 bj\u03bb) = ek(1\u2212 \u03bb+ e\u03bb/bk). (8)\nfrom (8) we obtain the final approximation formula\n\u03bb = \u2212p\u0302k\n\u2211L j=1 ej + ek\np\u0302k \u2211L j=1 ej(1 \u2212 bj) + ek \u2212 eek/bk . (9)\nAnd finally the step length is obtained by\n\u03b7 = \u2212p\u0302k\n\u2211L j=1 ej + ek\np\u0302k \u2211L j=1 ej(1\u2212 bj) + ek \u2212 eek/bk \u00b7\n1\nxTx (10)\n3.4 Logistic regression derived from approximation formula (10)\nAdmittedly, we can directly utilize softmax regression to classify binary-class datasets in spite of a doubled memory cost compared to classic logistic regression. However, we can conduct a transformation to reduce the extra cost. Here we present the specific form of the approximative greedy step length for logistic regression that is equivalent to the previous introduced softmax regression with binary-class. Denote p1 = 1 1+exp(\u2212\u03c9\u00b7x) , p0 = 1 \u2212 p1, bi = exp(pi), i = 0, 1, and p\u0302 = 0.95 is the confidence threshold. For Softmax regression with binary class, weight \u03c9SM = [\u03c90, \u03c91] T , p0 =\nexp(\u03c90x) exp(\u03c90x)+exp(\u03c91x) , p1 = 1\u2212 p0. For instance (x, y), gradient\ngSM = x(y \u2212 p1) \u00b7\n[\n1 \u22121\n]\n,\nThe update scheme for \u03c9SM is\n\u03c9(t+1) = \u03c9(t) \u2212 \u03b7gSM\n=\n[\n\u03c9 (t) 0 \u2212 \u03b7x(y \u2212 p1) \u03c9 (t) 1 + \u03b7x(y \u2212 p1)\n]\n. (11)\nLet \u03c9LR = \u03c91 \u2212 \u03c90, g LR = x(p1 \u2212 y). From (11) we can derive the update scheme for \u03c9 LR\n\u03c9(t+1) = \u03c9(t) \u2212 2\u03b7x(p1 \u2212 y)\n= \u03c9(t) \u2212 2\u03b7gLR. (12)\nTherefore the greedy step length for logistic regression is \u03b7LR = 2\u03b7. Next we calculate \u03b7. From (10) we have\n\u03b7 = \u2212p\u0302(e0 + e1) + ey\np\u0302[e0(1\u2212 b0) + e1(1 \u2212 b1)] + ey \u2212 eey/by \u00b7\n1\nxTx , (13)\nwhere ej = exp(\u03c9jx), bj = exp(pj), j = 0, 1, y = 1 or 0 represents the label of instance x. For y = 1, use relation e0/e1 = p0/p1, b0b1 = e, we have\n\u03b7 = \u2212p\u0302(e0/e1+1)+1p\u0302[e0/e1\u00b7(1\u2212b0)+1\u2212b1]+1\u2212e/b1 \u00b7 1 xT x\n= \u2212p\u0302(p0/p1+1)+1p\u0302[p0/p1\u00b7(1\u2212b0)+1\u2212b1]+1\u2212b0 \u00b7 1 xT x = p1\u2212p\u0302p\u0302(1\u2212p0b0\u2212p1b1)+p1(1\u2212b0) 1 xT x .\n(14)\nSimilarly, for y = 0 we have\n\u03b7 = p0 \u2212 p\u0302\np\u0302(1\u2212 p0b0 \u2212 p1b1) + p0(1\u2212 b1)\n1\nxTx , (15)\nTherefore we obtain the greedy step length\n\u03b7LR =\n{\np1\u2212p\u0302 p\u0302(1\u2212p0b0\u2212p1b1)+p1(1\u2212b0) 2 xT x , label = 1, p0\u2212p\u0302 p\u0302(1\u2212p0b0\u2212p1b1)+p0(1\u2212b1) 2 xT x , label = 0.\n(16)\n3.5 Averaging Scheme\nThe intuition of our algorithm is straightforward: exploit the information of each learning instance as much as possible. However, foreseeably, this could bring great oscillation of global loss function. To address this issue, we propose a dynamic adapting strategy. Its idea is dynamically adapting over time by using previous information of greedy step lengths. A basic observation is that the closer we come to global minimum, the smaller the greedy step length is since an increasing proportion of training data is better classified. Therefore, if the greedy step lengths remain steadily in a relatively low interval during a period of iteration, we should know our model is closer to convergence and should thus avoid large learning rates in future iterative steps. In consideration of memory costs, we calculate the arithmetic mean of all previous greedy step length E[\u03b7]t as the empirical learning rate:\nE[\u03b7]t = 1\nt\nt \u2211\ni=1\n\u03b7i.\nIn consideration of storage costs, we can compute E[\u03b7]t on the run using\nE[\u03b7]t = t\u2212 1\nt E[\u03b7]t\u22121 +\n1 t \u03b7t. (17)\nFor the complete algorithm details see Algorithm 1. And the general framework of GSA see Algorithm 2.\nAlgorithm 1 GSA algorithm for LR and Softmax\nRequire: Initial parameter \u03c90 1: for t in i \u2208 [0, T ] do 2: Take a Training Sample (xt, yt); 3: Compute Probability pt, Gradient gt; 4: Compute Greedy Step Size \u03b7t = f(xt, yt, pt) by (10); 5: Compute Averaged Greedy Step Size \u03b7\u0304 = mean(\u03b7t); 6: Apply Update \u03c9t+1 = \u03c9t \u2212 \u03b7\u0304gt; 7: end for\nAlgorithm 2 GSA algorithm in general\nRequire: Initial parameter \u03c90, loss function L(\u03c9) = \u2211N\ni=1 li(\u03c9) 1: for t in i \u2208 [0, T ] do 2: Take a Training Sample (xt, yt); 3: Compute Stochastic Gradient gt = \u2202lt \u2202\u03c9 ; 4: Compute Greedy Step Size \u03b7t by exact line search on t(\u03c9t \u2212 \u03b7gt); 5: Compute Averaged Greedy Step Size \u03b7\u0304 = mean(\u03b7t); 6: Apply Update \u03c9t+1 = \u03c9t \u2212 \u03b7\u0304gt; 7: end for\n4 Convergence Analysis\nNow we establish the convergence theory of GSA. Throughout this section, we suppose f(\u03c9) is the objective function, which takes the form of\nf(\u03c9) = 1\nn\nN \u2211\ni=1\nL(\u03c9, xi),\nwhere L(\u03c9, xi) is the standard log-loss function, the update scheme of the k-th iteration reads\n\u03c9(k+1) = \u03c9(k) \u2212 \u03b7(k)\u2207fi(\u03c9 (k)),\nwhere \u03b7(k) > 0 is the learning rate given by (10) and i is randomly extracted from {1, 2, ..., N}. First we deduce some basic observation of \u03b7(k). From (10) we have\n\u03b7k = \u2212p\u0302\n\u2211L j=1 ej+ek\np\u0302k \u2211L j=1 ej(1\u2212bj)+ek\u2212eek/bk \u00b7 1xT x\n= p\u0302\u2212pkp\u0302|1\u2212p\u00b7b|+pk|1\u2212e/bk| \u00b7 1 xT x \u2264 |p\u0302\u2212pk| p\u00b7b\u22121 \u00b7 1 xT x \u2264 |p\u0302\u2212pk| |e1/L\u22121|\u2016x\u20162 \u2264 C \u00b7 |p\u0302\u2212 pk|,\n(18)\nwhere b = (b1, \u00b7 \u00b7 \u00b7 , bL),p = (p1, \u00b7 \u00b7 \u00b7 , pL), pi = ei/ \u2211L j=1 ej. And the second inequality holds because \u2211L\ni=1 pi = 1 and \u220fL i=1 bi = e, C is some constant independent of k. Therefore\n\u03b7(k) = 1k \u2211k\ni=1 \u03b7i \u2264 Ck \u2211k i=1 |p\u0302\u2212 pi| \u2264 C.\n(19)\nis bounded. Not only \u03b7(k) is bounded, we can also assume limk\u2192\u221e \u03b7 (k) = \u03b70 since in each pass |p\u0302\u2212 pi| is almost sampled from an identical distribution. On the other hand, we have the denominator of (21)\n\u2211T k=1 \u03b7 (k) = \u2211T k=1 1 k \u2211k i=1 \u03b7i\n= \u2211T i=1 \u03b7i \u2211T k=i 1 k \u2265 \u03b71 \u2211T k=i 1 k \u2192 \u221e, as T \u2192 \u221e.\n(20)\nNow we present our conclusion. Let \u03c9\u2217 stands for the minimizer of f . Suppose\n\u2203M > 0, s.t.\u2200k,E[||\u2207fi(\u03c9 (k))||2] \u2264 M,\n\u2203G > 0, s.t.E[||\u03c9(0) \u2212 \u03c9\u2217||] \u2264 G,\ndefine fm(T ) = min{f(\u03c9 (0)), f(\u03c9(1)), ..., f(\u03c9(T ))},\nthen we claim that E[fm(T )] \u2192 f(\u03c9 \u2217) +O(\u03b70), as T \u2192 \u221e.\nProof. Taylor expansion gives\n||\u03c9(k+1) \u2212 \u03c9\u2217||2 = ||\u03c9(k) \u2212 \u03b7(k)\u2207fi(\u03c9 (k))\u2212 \u03c9\u2217||2\n= ||\u03c9(k) \u2212 \u03c9\u2217||2 \u2212 2\u03b7(k)\u2207fi(\u03c9 (k))T (\u03c9(k) \u2212 \u03c9\u2217) + [\u03b7(k)]2||\u2207fi(\u03c9 (k))||2.\nTake conditional expectation we obtain\nE[||\u03c9(k+1) \u2212 \u03c9\u2217||2|\u03c9(k)] = E[||\u03c9(k) \u2212 \u03c9\u2217||2|\u03c9(k)]\u2212 2\u03b7(k)E[\u2207fi(\u03c9 (k))T (\u03c9(k) \u2212 \u03c9\u2217)|\u03c9(k)]\n+ [\u03b7(k)]2E[||\u2207fi(\u03c9 (k))||2|\u03c9(k)]\n= ||\u03c9(k) \u2212 \u03c9\u2217||2 \u2212 2\u03b7(k)\u2207f(\u03c9(k))T (\u03c9(k) \u2212 \u03c9\u2217) + [\u03b7(k)]2E[||\u2207fi(\u03c9 (k))||2|\u03c9(k)]\n\u2264 ||\u03c9(k) \u2212 \u03c9\u2217||2 \u2212 2\u03b7(k)(f(\u03c9(k))\u2212 f(\u03c9\u2217)) + [\u03b7(k)]2M.\nTake expedition w.r.t \u03c9(k) and use iteration, the inequality yields\nE[||\u03c9(k+1) \u2212 \u03c9\u2217||2] \u2264 E[||\u03c9(k) \u2212 \u03c9\u2217||2]\u2212 2\u03b7(k)(E[f(\u03c9(k))]\u2212 f(\u03c9\u2217)) + [\u03b7(k)]2M\n\u2264 ...\n\u2264 E[||\u03c9(0) \u2212 \u03c9\u2217||2]\u2212 2\nT \u2211\nk=1\n\u03b7(k)(E[f(\u03c9(k))]\u2212 f(\u03c9\u2217)) +M\nT \u2211\nk=0\n[\u03b7(k)]2\n\u2264 G\u2212 2\nT \u2211\nk=1\n\u03b7(k)(E[f(\u03c9(k))]\u2212 f(\u03c9\u2217)) +M\nT \u2211\nk=0\n[\u03b7(k)]2.\nThus we established an error bound of E[fm(T )] with tolerance O(\u03b70). Similarly, in [3] the author also established an inequality\n\u2016\u2207f(\u03c9)\u2016 \u2264 C\u03b70\nunder some reasonable assumptions. According to their arguments, this result is as far as we are able to attain for SGD with learning rate bounded away from zero.\n5 Numerical Experiment\nIn this section we make comparisons between GSA and some other state of the art algorithms on several open datasets from libsvm. All the 16 datasets we use here are downloaded from [27]. The algorithms that we choose as contrasts are SGD, Adadelta, SCSG for the following reason: SGD is the widely accepted classic benchmark, Adadelta and SCSG are representatives of the family of self-adaptive and variance reduction method, respectively. We perform experiments on 16 datasets, and on 15 out of which the gap between average precision score of GSA and the best performance is less than 1%(except on letter.scale, GSA is beaten by SCSG by 3.5% in precision after 10 epochs). Due to the space limitations, we only offer the experimental illustrations for 4 of them(w1a, mnist, news20, aloi) and left the results on other 12 datasets in appendix. For those datasets which have corresponding test sets(w1a and news20), we take them for validation while for the other datasets we randomly take 80% from the whole dataset as training set and remain the left 20% for testing. Details of datasets are presented in table 2. We implement GSA for logistic and softmax regression, compare its performance with that of SGD, Adadelta and SCSG. In each experiment, we let the iterator run up to several passes(5, 10, or 20, depend on the required number of passes for loss to saturate) of the training set, and evaluate the log of standard cross-entropy loss, the precision score and the roc-auc score over the whole test set. For each algorithm, the hyper parameters are specified in the legend, and no regularization term is attached. The detailed discussion of the comparison is presented in the following subsections. The demonstration of all 3 indicators(loss, precision and roc_auc score) after 1, 2 and the last pass of training sets is shown in appendix, and we highlights the best score in each columns(we did not test SCSG on the largest dataset url for the limitation of computational resource). For each indicator, we calculate the statistical result between GSA and the best score(the mean of Err = scoreGSA \u2212 scorebest) of other 3 methods over 16 datasets. We also count the time that GSA ranks first in terms of different metrics. The corresponding results are shown in table 1. Compared to the best performances among other 3 methods, GSA is not only robust but also fairly competitive.\nFrom Fig.1 we can see that the optimal learning rate for SGD varies on different datasets, and GSA shows a better performance than the best behaviour of SGD on mnist, news20 and aloi. That is because GSA is able to capture the magnitude of the optimal learning rate in the early stage of iteration and dynamically change it to adapt to the situation. In another words, GSA can figure out when to stop. Moreover, GSA can also avoid oscillation by detecting the margin from convergence and adjust learning rate correspondingly.\n5.2 Comparison with Adadelta\nAdadelta has two hyper parameters: the decay rate \u03b3 in (3) and (4), the small constant \u03b5 in (5). Through our experiment we set \u03b3 = (t\u22121)/t to replace the running window average of E[\u2206\u03b82]t and E[g2]t to the whole time average. From Fig.2 we see that the performance of Adadelta significantly depends on \u03b5, which is not like what the author claimed in [9]. In comparison, GSA is more robust on a variety of datasets. We also note that adadelta runs slower than GSA because its per-dimension learning rate introduce an extra O(m) cost in time and space. On the other hand, GSA inherits the advantage of SGD in terms of computational efficiency.\n5.3 Comparison with SCSG\nSCSG is a variation of SVRG. Instead of the evaluation the full gradient, SCSG needs only a batch gradient during the beginning of each epoch. Therefore apart from learning rate r, the batch size B also becomes an important hyper parameter. From Fig.3 we can see that better performance of SCSG is always associated with largerB, which undermines its applicability. On mnist and news20, the acceptable value of B is approximately 1/3 of the training size(20000/60000, 5000/15935). In addition, SCSG is also sensitive to step size r. Due to its sensitivity to the hyper parameters, the linear convergence rate of SCSG is hard to display because the optimal combination of parameter cannot be easily captured. In our experiment on the four datasets, GSA outperforms SCSG in each configuration.\n6 Discussion\nWe propose GSA as a swift and parameter free stochastic optimization algorithm and test it on logistic and softmax regression. We show that GSA is facile in implementation and requires no extra memory. The experimental results on multiple datasets demonstrate that GSA is able to reach the performance of best tuned SGD and can also beat several state of the art algorithms in sense of practicability. Parameter tuning is cumbersome and rely on the priori on datasets to a large extent. If we do not have any domain knowledge of our data or problem, which is always the actual situation, the best way for us to optimize our algorithm is performing grid search. However, when confronted with large scaled data, it turns to be completely intractable. Therefore in industry, people tend to select model parameters based on practical experience. Unfortunately, most current advanced stochastic optimization methods are incapable of beating the best tuned basic SGD; they even crush from time to time. On the other hand, although GSA cannot always outperform the best performance of other prevalent methods, its convergence behaviour is stable and exhibits a comparable performance level of best tuned SGD. Thus we claim that GSA is more practical in application.\nIn addition, we propose an easy implementation for parallelizing GSA. It is not an novel idea, but it works particularly well for GSA since it requires little I/O cost. On spark cluster our applications achieve a great success: For logistic regression model with 109 instances and 107 features, our method converges after only a single pass of dataset within 10 minutes(Source code on Github, see [29]).\nIt should be noted that GSA is essentially basic SGD with an automatically selected learning rate sequence. However, unlike the two popular learning rate selection strategies, the constant step size rule and decreasing step size rule, the typical behaviour of GSA\u2019s step sizes displays a profile between them: it\u2019s basically a diminishing sequence asymptotically decrease towards a non-zero constant a. We believe a is somehow around the best tuned SGD step-size, however the theoretical proof has not been established yet.\nSince the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22]. Suppose that during each iteration, GSA returns a sequence of points \u03c91, ..., \u03c9T . To obtain a final estimation of \u03c9\n\u2217, a simple strategy is to return the last point \u03c9T . It is well-acknowledged that this procedure has a O(1/T ) convergence guarantee for SGD. Another procedure, for which the standard online analysis of SGD applies [20], is to return the average point\n\u03c9\u0304T = 1\nT (\u03c91 + \u00b7 \u00b7 \u00b7+ \u03c9T ).\nHowever, the error bound for this procedure is merely O(log(T )/T ). Recently, in [22] they indicate that O(log(T )/T ) is not the best that one can achieve for strongly convex stochastic problems and have proposed a different algorithm, which is somewhat similar to SGD to achieve the O(1/T ) bound. Besides, in [21] they present a new average tactics called \u03b1-suffix averaging to recover the O(1/T ) rate. Since GSA is originated from the SGD framework, we recommend these averaging schemes as practicable options. However, it will incur an extra storage cost. For most cases without a specific accuracy target, we can just ignore this trick and return the last \u03c9T .\nThere are still several open issues remaining about our work. First, the current version of GSA only supports logistic and softmax regression. Actually, it can easily be generalized to other machine learning algorithm like SVM and linear regression, as long as the unique zero of its objective loss function has a closed form, at least in the sense of approximation. This is however not the case in neural networks. Because the loss function of a neural network is a black box: we only have access to the its value and gradient, which becomes an insurmountable obstacle to apply GSA. Even though a compromised strategy is to conduct linear search for a approximated greedy step size, the cost seems formidable since we cannot afford too many function evaluations within a single iteration step. We plan to explore several variants of GSA in future work to deal with this difficulty. Acknowledgements We wish to thank Yafang Luo and Liran Chen for polishing vocabularies and sentences in this paper, and Yao Lu for several helpful discussions regarding this work.\nReferences\n[1] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22(3):400\u2013407, 1951.\n[2] L. Bottou and Y. LeCun. Large scale online learning. NIPS, 2003.\n[3] Solodov, Mikhail V. \"Incremental gradient algorithms with stepsizes bounded away from zero.\" Computational Optimization and Applications 11.1 (1998): 23-35.\n[4] Nedi\u0107, Angelia, and Dimitri Bertsekas. \"Convergence rate of incremental subgradient algorithms.\" Stochastic optimization: algorithms and applications. Springer US, 2001. 223-264.\n[5] Y. Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.\n[6] Shamir, Ohad, and Tong Zhang. \"Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.\" ICML (1). 2013.\n[7] Qian N. On the momentum term in gradient descent learning algorithms[J]. Neural networks, 1999, 12(1): 145-151.\n[8] Duchi, John, Elad Hazan, and Yoram Singer. \"Adaptive subgradient methods for online learning and stochastic optimization.\" Journal of Machine Learning Research 12.Jul (2011): 2121- 2159.\n[9] Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\" arXiv preprint arXiv:1212.5701 (2012).\n[10] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221\u2013259, 2009.\n[11] Polyak, Boris T., and Anatoli B. Juditsky. \"Acceleration of stochastic approximation by averaging.\" SIAM Journal on Control and Optimization 30.4 (1992): 838-855.\n[12] Roux, Nicolas L., Mark Schmidt, and Francis R. Bach. \"A stochastic gradient method with an exponential convergence rate for finite training sets.\" Advances in Neural Information Processing Systems. 2012.\n[13] Johnson, Rie, and Tong Zhang. \"Accelerating stochastic gradient descent using predictive variance reduction.\" Advances in Neural Information Processing Systems. 2013.\n[14] Lei, Lihua, and Michael I. Jordan. \"Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method.\" arXiv preprint arXiv:1609.03261 (2016).\n[15] Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. \"Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.\" Advances in Neural Information Processing Systems. 2014.\n[16] McMahan, H. Brendan. \"Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization.\" AISTATS. 2011.\n[17] Duchi, John, and Yoram Singer. \"Efficient online and batch learning using forward backward splitting.\" Journal of Machine Learning Research 10.Dec (2009): 2899-2934.\n[18] Xiao, Lin. \"Dual averaging methods for regularized stochastic learning and online optimization.\" Journal of Machine Learning Research 11.Oct (2010): 2543-2596.\n[19] LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.\n[20] Hazan, Elad, Amit Agarwal, and Satyen Kale. \"Logarithmic regret algorithms for online convex optimization.\" Machine Learning 69.2-3 (2007): 169-192.\n[21] Rakhlin, Alexander, Ohad Shamir, and Karthik Sridharan. \"Making gradient descent optimal for strongly convex stochastic optimization.\" arXiv preprint arXiv:1109.5647 (2011).\n[22] Hazan, Elad, and Satyen Kale. \"An optimal algorithm for stochastic strongly-convex optimization.\" arXiv preprint arXiv:1006.2425 (2010).\n[23] Zinkevich, Martin, et al. \"Parallelized stochastic gradient descent.\" Advances in neural information processing systems. 2010.\n[24] Zhang, Yuchen, Martin J. Wainwright, and John C. Duchi. \"Communication-efficient algorithms for statistical optimization.\" Advances in Neural Information Processing Systems. 2012.\n[25] Rosenblatt, Jonathan D., and Boaz Nadler. \"On the optimality of averaging in distributed statistical learning.\" Information and Inference (2016): iaw013.\n[26] Agarwal, Alekh, et al. \"A reliable effective terascale linear learning system.\" Journal of Machine Learning Research 15.1 (2014): 1111-1133.\n[27] http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm\n[28] Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27, 2011\n[29] https://github.com/TalkingData/Fregata\n7 Appendix: More experimental results"}], "references": [{"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics, 22(3):400\u2013407", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1951}, {"title": "Large scale online learning", "author": ["L. Bottou", "Y. LeCun"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Incremental gradient algorithms with stepsizes bounded away from zero.\" Computational Optimization and Applications", "author": ["Solodov", "Mikhail V"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Convergence rate of incremental subgradient algorithms.\" Stochastic optimization: algorithms and applications", "author": ["Nedi\u0107", "Angelia", "Dimitri Bertsekas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": "Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.", "author": ["Shamir", "Ohad", "Tong Zhang"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On the momentum term in gradient descent learning algorithms[J", "author": ["N. Qian"], "venue": "Neural networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization.", "author": ["Duchi", "John", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "ADADELTA: an adaptive learning rate method.", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical programming, 120(1):221\u2013259", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Acceleration of stochastic approximation by averaging.", "author": ["Polyak", "Boris T", "Anatoli B. Juditsky"], "venue": "SIAM Journal on Control and Optimization", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets.", "author": ["Roux", "Nicolas L", "Mark Schmidt", "Francis R. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction.", "author": ["Johnson", "Rie", "Tong Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method.", "author": ["Lei", "Lihua", "Michael I. Jordan"], "venue": "arXiv preprint arXiv:1609.03261", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.", "author": ["Defazio", "Aaron", "Francis Bach", "Simon Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization.\" AISTATS", "author": ["McMahan", "H. Brendan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Efficient online and batch learning using forward backward splitting.", "author": ["Duchi", "John", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization.", "author": ["Xiao", "Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Efficient backprop.\" Neural networks: Tricks of the trade", "author": ["LeCun", "Yann A"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization.", "author": ["Hazan", "Elad", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization.", "author": ["Rakhlin", "Alexander", "Ohad Shamir", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1109.5647", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "An optimal algorithm for stochastic strongly-convex optimization.\" arXiv preprint", "author": ["Hazan", "Elad", "Satyen Kale"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Parallelized stochastic gradient descent.\" Advances in neural information processing systems", "author": ["Zinkevich", "Martin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Communication-efficient algorithms for statistical optimization.", "author": ["Zhang", "Yuchen", "Martin J. Wainwright", "John C. Duchi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "On the optimality of averaging in distributed statistical learning.\" Information and Inference", "author": ["Rosenblatt", "Jonathan D", "Boaz Nadler"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "A reliable effective terascale linear learning system.", "author": ["Agarwal", "Alekh"], "venue": "Journal of Machine Learning Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "LIBSVM : a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The most prevalent algorithm for these problems is stochastic gradient descent method(SGD) [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "The most prevalent algorithm for these problems is stochastic gradient descent method(SGD) [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "are intrinsically contradictive [6] and we have to strike a balance between the two.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "where \u03c1 is a constant depending on the condition number of g[5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "For SGD under this strategy, we can reach a sublinear convergence rate[3, 4]: E[g(\u03c9)]\u2212 g(\u03c9) = O(1/k).", "startOffset": 70, "endOffset": 76}, {"referenceID": 3, "context": "For SGD under this strategy, we can reach a sublinear convergence rate[3, 4]: E[g(\u03c9)]\u2212 g(\u03c9) = O(1/k).", "startOffset": 70, "endOffset": 76}, {"referenceID": 9, "context": "Gradient Averaging The Gradient Averaging method [10] is equivalent to the Momentum mentioned above, if we choose the simple arithmetic average to substitute the weighted average in Momentum.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "With a suitable choice of step-sizes, this gives the same asymptotic efficiency as Newton-like second-order SG methods and also leads to increased robustness of the convergence rate to the exact sequence of step sizes [11].", "startOffset": 218, "endOffset": 222}, {"referenceID": 10, "context": "It has been proved that under certain assumptions of appropriate step-size, this method enjoys a second-order convergence rate[11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "In [12], the authors show that the SAG iterations have a linear convergence rate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "SV RG Stochastic variance reduced gradient(SVRG) introduces an explicit variance reduction method for SGD[13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "Another variation of SVRG is SAGA [15] which is claimed to support non-strongly convex problems directly and has a better convergence rate.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "As a great improvement of SVRG, the computation cost and the communication cost of SCSG do not necessarily scale linearly with sample size n[14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Adagrad Adagrad [8] is an algorithm for gradient-based optimization, it adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "Adadelta Adadelta [9] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.", "startOffset": 18, "endOffset": 21}, {"referenceID": 18, "context": "Instead, we set a confidence threshold (in fact this idea is also recommended by Yann Lecun for training neural network [19]) at, for example, p\u03021 = 0.", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "Similarly, in [3] the author also established an inequality \u2016\u2207f(\u03c9)\u2016 \u2264 C\u03b70 under some reasonable assumptions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "2 we see that the performance of Adadelta significantly depends on \u03b5, which is not like what the author claimed in [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "Since the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 20, "context": "Since the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 21, "context": "Since the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 19, "context": "Another procedure, for which the standard online analysis of SGD applies [20], is to return the average point \u03c9\u0304T = 1 T (\u03c91 + \u00b7 \u00b7 \u00b7+ \u03c9T ).", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Recently, in [22] they indicate that O(log(T )/T ) is not the best that one can achieve for strongly convex stochastic problems and have proposed a different algorithm, which is somewhat similar to SGD to achieve the O(1/T ) bound.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Besides, in [21] they present a new average tactics called \u03b1-suffix averaging to recover the O(1/T ) rate.", "startOffset": 12, "endOffset": 16}], "year": 2016, "abstractText": "In this paper we present the greedy step averaging(GSA) method, a parameter-free stochastic optimization algorithm for a variety of machine learning problems. As a gradient-based optimization method, GSA makes use of the information from the minimizer of a single sample\u2019s loss function, and takes average strategy to calculate reasonable learning rate sequence. While most existing gradient-based algorithms introduce an increasing number of hyper parameters or try to make a trade-off between computational cost and convergence rate, GSA avoids the manual tuning of learning rate and brings in no more hyper parameters or extra cost. We perform exhaustive numerical experiments for logistic and softmax regression to compare our method with the other state of the art ones on 16 datasets. Results show that GSA is robust on various scenarios.", "creator": "LaTeX with hyperref package"}}}