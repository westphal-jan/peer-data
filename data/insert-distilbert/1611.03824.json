{"id": "1611.03824", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Learning to Learn without Gradient Descent by Gradient Descent", "abstract": "we present a learning to learn approach for training recurrent fuzzy neural networks to perform black - box finite global optimization. in the meta - learning phase we use a rather large set of smooth target functions to learn a recurrent neural network ( rnn ) optimizer, which is either a long - short term memory reference network or a differentiable neural computer. after learning, the rnn principle can be applied purely to learn behavior policies in reinforcement learning, as well as other black - box learning tasks, variously including continuous correlated bandits and rapid experimental design. we compare this approach to bayesian optimization, with emphasis on the empirical issues of constraint computation speed, desired horizon length, and problem exploration - exploitation trade - offs.", "histories": [["v1", "Fri, 11 Nov 2016 19:33:01 GMT  (2836kb,D)", "http://arxiv.org/abs/1611.03824v1", null], ["v2", "Fri, 18 Nov 2016 14:13:13 GMT  (2836kb,D)", "http://arxiv.org/abs/1611.03824v2", null], ["v3", "Tue, 29 Nov 2016 21:32:13 GMT  (2836kb,D)", "http://arxiv.org/abs/1611.03824v3", "Accepted by Deep Reinforcement Learning Workshop, NIPS 2016"], ["v4", "Tue, 9 May 2017 07:59:07 GMT  (658kb,D)", "http://arxiv.org/abs/1611.03824v4", "Previous version \"Learning to Learn for Global Optimization of Black Box Functions\" was published in the Deep Reinforcement Learning Workshop, NIPS 2016"], ["v5", "Mon, 5 Jun 2017 14:15:01 GMT  (1138kb,D)", "http://arxiv.org/abs/1611.03824v5", "Accepted by ICML 2017. Previous version \"Learning to Learn for Global Optimization of Black Box Functions\" was published in the Deep Reinforcement Learning Workshop, NIPS 2016"], ["v6", "Mon, 12 Jun 2017 11:19:30 GMT  (569kb,D)", "http://arxiv.org/abs/1611.03824v6", "Accepted by ICML 2017. Previous version \"Learning to Learn for Global Optimization of Black Box Functions\" was published in the Deep Reinforcement Learning Workshop, NIPS 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yutian chen", "matthew w hoffman", "sergio gomez colmenarejo", "misha denil", "timothy p lillicrap", "matthew botvinick", "nando de freitas"], "accepted": true, "id": "1611.03824"}, "pdf": {"name": "1611.03824.pdf", "metadata": {"source": "CRF", "title": "Learning to Learn for Global Optimization of Black Box Functions", "authors": ["Yutian Chen", "Matthew W. Hoffman", "Sergio G\u00f3mez Colmenarejo", "Misha Denil", "Timothy P. Lillicrap", "Nando de Freitas"], "emails": ["yutianc@google.com", "mwhoffman@google.com", "sergomez@google.com", "mdenil@google.com", "countzero@google.com", "nandodefreitas@google.com"], "sections": [{"heading": "1 Introduction", "text": "Findings in developmental psychology have revealed that infants are endowed with a small number of separable systems of core knowledge for reasoning about objects, actions, number, space, and possibly social interactions [Spelke and Kinzler, 2007]. These core systems enable infants to learn a vast set of skills and knowledge rapidly. The most coherent explanation of this phenomena is that the slow learning (or optimization) process of evolution has led to the emergence of components that enable fast and varied forms of learning. In psychology, learning to learn has a long history full of insights on cognition [Ward, 1937, Harlow, 1949, Kehoe, 1988].\nInspired by this, many researchers have tried to build agents capable of learning to learn [Schmidhuber, 1987, Bengio et al., 1992, Hochreiter et al., 2001, Santoro et al., 2016, Andrychowicz et al., 2016, Zoph and Le, 2016]. The scope of research under the umbrella of learning to learn, or metalearning, as it is often called in this literature, is remarkably broad, and can be categorized along several dimensions. The learner can implement and be trained by many different algorithms, including gradient descent, genetic algorithms, simulated annealing, and reinforcement learning algorithms, leading to a large space of possibilities.\nFor instance, one can learn to learn by gradient descent by gradient descent, or learn local Hebbian updates by gradient descent [Andrychowicz et al., 2016, Bengio et al., 1992]. In the former, one uses supervised learning at the meta-level to learn an algorithm for supervised learning, while in the latter, one uses supervised learning at the meta-level to learn an algorithm for unsupervised learning. Other types of learning, including reinforcement learning, can be easily added into this mix.\nThe product of meta-learning is another valuable dimension for categorizing research efforts in this field. In Andrychowicz et al. [2016] the product of meta-learning is a trained RNN, which is subsequently used as an optimization algorithm to fit other models to data or optimize any differentiable objective function. In contrast, in Zoph and Le [2016] the output of meta-learning may also be an RNN, but in their case the RNN is subsequently used as a model that is fit to data using a classical optimizer. In both cases the output of meta-learning is an RNN, but this RNN is interpreted and\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n03 82\n4v 1\n[ st\nat .M\nL ]\n1 1\nN ov\napplied as a model or as an algorithm. In this sense, learning to learn with neural networks blurs the classical distinction between models and algorithms. For a more thorough review of related works on learning to learn, please consult Appendix A.\nIn this work, the product of meta-learning is an algorithm for globally optimizing black-box functions. In this case, the meta learner can be trained using gradients and distillation. Specifically, we address the problem of finding a global minimizer of an unknown (black-box) loss function f :\nx? = argmin x\u2208X f(x) , (1)\nwhere X is some search space of interest, which in this work is assumed to be a continuous space. The black-box function f is not available to the learner in simple closed form at test time, but can be evaluated at a query point x in the domain. This evaluation produces either deterministic or stochastic outputs y \u2208 R such that f(x) = E[y | f(x)]. In other words, we can only observe the function f through unbiased noisy point-wise observations y.\nBlack-box optimization has a long history in machine learning and experimental design. While many tools have been proposed to attack these problems, Bayesian optimization techniques have gained a substantial amount of interest in recent years [Brochu et al., 2009, Shahriari et al., 2016].\nBayesian optimization is a sequential model-based approach to solving problem (1). It has two components. The first component is a probabilistic model, consisting of a prior distribution that captures our beliefs about the behavior of the unknown objective function and an observation model that describes the data generation mechanism. The model can be a Beta-Bernoulli bandit, a random forest with frequentist confidence estimates, a Dirichlet process prior, a Bayesian neural network, or a Gaussian process [Shahriari et al., 2016]. This wide variety of possible models highlights how broad and encompassing this approach can be, despite the fact that it is often associated with Gaussian processes, to the point of receiving the name of Gaussian process bandits in the literature [Srinivas et al., 2010, Kaufmann et al., 2012].\nThe second component is an acquisition function, which is optimized at each step so as to tradeoff exploration and exploitation. Here again we encounter a huge variety of strategies, including Thompson sampling [Thompson, 1933, Scott, 2010, Chapelle and Li, 2011, Agrawal and Goyal, 2013], information gain [Villemonteix et al., 2009, Hennig and Schuler, 2012, Russo and Van Roy, 2014, Shahriari et al., 2014, Herna\u0301ndez-Lobato et al., 2015], probability of improvement [Kushner, 1964, Jones et al., 1998], expected improvement [Moc\u030ckus et al., 1978, Jones et al., 1998, Jones, 2001, Brochu et al., 2009, Snoek et al., 2012, Wang and de Freitas, 2014], upper confidence bounds [Lai and Robbins, 1985, Srinivas et al., 2010], and so on.\nThe requirement for optimizing the acquisition function at each step can be a significant cost, as we will illustrate in the empirical section of this paper. It also raises some theoretical concerns [Wang et al., 2014].\nIn this paper, we will present a learning to learn approach for global optimization of black box functions and contrast it with Bayesian optimization. In the meta-learning phase, we will use a large number of differentiable functions generated with a Gaussian process to train an RNN optimizer. We will make use of two types of RNN: long-short-term memory networks (LSTMs) by Hochreiter and Schmidhuber [1997] and differentiable neural computers (DNCs) by Graves et al. [2016]. In our experiments the RNN uses its memory to store information about previous queries and learns to access its memory to make decisions about which parts of the domain to explore or exploit next. We also consider the use of acquisition functions to guide the process of training the RNN optimisers.\nAfter learning the optimiser, it can be applied to any black-box function. By unrolling the RNN, we generate new candidates for the search process. This process is much faster than applying standard Bayesian optimization, and in particular it does not involve either matrix inversion or optimization of acquisition functions.\nDuring meta-learning, we choose the horizon (number of steps) of the optimization process. We are therefore considering the finite horizon setting that is popular in AB tests Kohavi et al. [2009], Scott [2010] and is often studied under the umbrella of best arm identification in the bandits literature [Bubeck et al., 2009, Gabillon et al., 2011, 2012, Hoffman et al., 2014].\nOur experiments will show that the RNN optimizer is faster and tends to achieve better performance within the horizon for which it is trained. It however underperforms against Bayesian optimization\nfor much longer horizons as it has not learned to explore for longer horizons. These experiments also demonstrate a remarkable degree of transfer: We learn an RNN optimizer using synthetic functions, and then successfully apply it to a broad range of benchmarks from the global optimization literature as well as a simple control problem."}, {"heading": "2 Learning Black-box Optimization", "text": "A black-box optimization algorithm can be summarized by the following loop:\n1. Given the current state of knowledge ht propose a query point xt 2. Observe the response yt 3. Update any internal statistics to produce ht+1\nThis easily maps onto the classical frameworks presented in the previous section where the update step computes statistics and the query step uses these statistics for exploration. In this work we take this framework as a starting point and define a combined update and query rule using a recurrent neural network (RNN) parameterized by \u03b8 such that\nht,xt = RNN\u03b8(ht\u22121,xt\u22121, yt\u22121), (2) yt \u223c p(y | xt) . (3)\nIntuitively this rule can be seen to update its hidden state using data from the previous time step and then propose a new query point. In what follows we will apply this RNN, with shared parameters, to many steps of a black-box optimization process. An example of this computation is shown in Figure 1.\nGiven this rule we now need a way to learn the parameters \u03b8 for any given distribution of functions p(f). Perhaps the simplest loss function one could use is the loss of the final iteration: Lfinal(\u03b8) = Ef [f(xT )] for some time-horizon T . This loss was considered by Andrychowicz et al. [2016] in the context of learning first-order optimizers, but ultimately rejected in favor of\nLobs(\u03b8) = Ef,y1:T\u22121 [ T\u2211 t=1 f(xt) ] . (4)\nA key reason to prefer Lobs is that the amount of information conveyed by Lfinal is temporally very sparse. By instead utilizing a sum of losses to train the optimizer we are able to provide information from every step along this trajectory. Note also that optimizing the summed loss is equivalent to finding a strategy which minimizes the expected cumulative regret. Note that although at test time the optimizer typically only has access to the observation yt, at training time the true loss can be used.\nBy using the above objective function we will be encouraged to trade off exploration and exploitation and hence globally optimize the function f . This is due to the fact that in expectation, any method that is better able to explore and find small values f(x) will be rewarded for these discoveries. However the actual process of optimizing the above loss can be difficult due to the fact that nothing explicitly encourages the optimizer itself to explore.\nWe can encourage exploration by encoding an exploration force directly into the loss function for meta learning. Many such examples exist in the bandit and Bayesian optimization communities, for example\nLEI(\u03b8) = \u2212Ef,y1:T\u22121 [ T\u2211 t=1 EI(xt | y1:t\u22121) ] (5)\nwhere EI(\u00b7) is the expected posterior improvement of querying xt given observations up to time t. This can encourage exploration by giving an explicit bonus to the optimizer rather than just implicitly doing so by means of function evaluations. Alternatively, it is possible to use the observed improvement (OI)\nLOI(\u03b8) = Ef,y1:T\u22121 [ T\u2211 t=1 (f(xt)\u2212min i<t (f(xi))) ] (6)\nThe illustration of Figure 1 shows the optimizer unrolled over many steps, ultimately culminating in the loss function. To train the optimizer we will simply take derivatives of the loss with respect to the RNN parameters \u03b8 and perform stochastic gradient descent (SGD). In order to evaluate these derivatives we assume that derivatives of f can be computed with respect to its inputs. This assumption is made only in order to backpropagate errors from the loss to the parameters, but crucially is not needed at test time. If even at training time the derivatives of f are not available then it would be necessary to approximate these derivatives via an algorithm such as REINFORCE [Williams, 1992].\nTo this point we have made no assumptions about the distribution of training functions p(f). In this work we are interested in learning general-purpose black-box optimizers, and in other words we desire our distribution to be quite broad. As a result we propose the use of Gaussian processes (GPs) as a suitable training distribution. The use of functions sampled from a GP prior also provides functions whose gradients can be easily evaluated and used at training time as noted above. Also the posterior expected improvement used within LEI can be easily computed Moc\u030ckus [1982] and differentiated as well. The major downside of search strategies which are based on GP inference is their cubic complexity at what we refer to as \u201ctest time\u201d. However in some sense the strategies learned in this work can be thought of as a distillation of GP-based techniques, and we will show later that the evaluation of our search strategy is negligible compared to that of standard Bayesian optimization techniques."}, {"heading": "3 Experiments", "text": "We present several experiments that show the breadth of generalization that is achieved by our learned algorithms. We train our algorithm to optimize very simple functions\u2014samples from a GP with a fixed length scale\u2014and show that the learned algorithm is able to generalize from these simple functions to optimize a variety of other test functions that were not seen during training.\nOur results are notable in several ways. First, we show that even though the learned algorithms are not competitive with GP-based Bayesian optimization (BO) when the training and test distributions match, our algorithm is competitive with GP-based methods with regards to generalization across several tasks. In addition to achieving competitive performance, our learned algorithms are significantly faster than GP-based methods, because they avoid several costs: inverting the GP covariance matrix, optimizing the acquisition function, and sampling the GP hyper-parameters.\nWe consider two different recurrent neural network architectures to implement our global optimization algorithms, specifically an LSTM architecture as well as a differentiable neural computer (DNC). We train each algorithm on trajectories of T = 30 steps, and update the algorithm parameters using BPTT to optimize the reward function averaged over the trajectory. We repeat this process for each of the reward functions discussed in Section 2 in order to compare these three objectives.\nHyperparameters for the algorithm (such as number of hidden units, and memory size for the DNC models) are found through grid search.\nWe compare our model with one of the most commonly used BO packages, Spearmint, and a random search baseline. We evaluate the performance at a given search step t \u2264 50, according to the test loss function that is defined as the minimum observed function value up to step t, loss(t) = mini\u2264t f(xi). Throughout this set of experiments, as noted earlier, we train using two forms of RNN: the DNC and LSTM, coupled with the losses introduced in Section 2. E.g. in the following experiments, DNC OI refers to the DNC network trained using the observed improvement loss LOI, LSTM EI refers to the LSTM trained using the expected improvement loss LEI, etc. For the first set of experiments we also compare against training with a loss based on GP-UCB [Srinivas et al., 2010], which we refer to as DNC UCB."}, {"heading": "3.1 Functions from a GP Prior", "text": "In the first set of experiments, we train our model on functions sampled from a GP prior with fixed hyper-parameters. Here for Spearmint we assume a prior distribution equivalent to the training distribution, hence this method knows the ground truth and thus provides a very competitive baseline."}, {"heading": "3.1.1 Generalization to Functions Sampled from the Training Distribution", "text": "We first evaluate the generalization performance on functions sampled from the same training distribution. Notice, however, that those functions are never observed during training. Figure 2 shows the best observed function values as a function of search step t, averaged over 10, 000 sampled functions. As expected Spearmint proves to be the best model under most settings as it makes use of the ground truth of the prior distribution. When the input dimension is 6, however, neural network models outperform Spearmint and we suspect it is because in higher dimensional space, it global optimization is harder within the time frames at which we apply these algorithms. Among NN models those trained with expected/empirical improvement perform better than the model trained with function observations or GP-UCB.\nFigure 3 shows the trajectories of xt for different models on a typical function sample. All of the models do exploration initially, and later settle down around one mode and search more locally. The model trained with EI behaves most similarly to the trajectory of Spearmint. Models trained with observations tend to explore less than the other models and therefore easily miss the global modes, while those trained with the observed improvement keep exploring even in the later stage. We do not observe qualitative difference between the DNC and LSTM models trained with the same reward function."}, {"heading": "3.1.2 Generalization to Functions Sampled with a Different Length-Scale", "text": "We then assess how the learned policies generalize to functions that are not sampled from the training distribution. We generate test functions from a 1-dimensional GP prior with a different length-scale from the training length-scale. We compare the neural network models with Spearmint with a length scale fixed at the same value as the training distribution.\nIn Figure 4 shows the loss of DNC models for functions sampled with length scale ` \u2208 {0.03, 0.1, 0.9} while the training length scale is 0.3. When ` < `train functions vary more rapidly than the training examples and models assuming a more smooth function class may miss the global minimum between detected modes. We observe that the performance of RNN models changes in the same tendency of Spearmint except for the model trained with observations. For functions with very short length scales, 0.03, DNC OI starts to outperform Spearmint with fixed length scales, thanks to its more exploratory behavior."}, {"heading": "3.1.3 Evaluating Transfer of Optimizer on Five Benchmark Functions", "text": "We compare the algorithms on five standard benchmark functions for black-box optimization with dimensions ranging from 1 to 6. To have a more robust evaluation of the performance of each model, we generate multiple instances for each benchmark function by applying a random translation (\u22120.1 \u223c 0.1), scaling (0.9 \u223c 1.1), flipping, and dimension permutation in the input domain. Figure 5 shows the loss compared with a random baseline, Spearmint with length scale fixed to the training length scale, and Spearmint with automatic inference of the GP hyper-parameter values and input warping to deal with non-stationarity [Snoek et al., 2014]. All of the neural network models show competitive performance against Spearmint methods except on the 1D Gramacy & Lee function up to the training horizon, T = 30. The Gramacy & Lee function has a much smaller length scale than 0.3 and none of the models is able to find a good mode except Spearmint, which infers the length scale. This problem can be addressed by training models with several length scales sampled from a distribution, in which case the model should learn to infer the underlying length scale and take a good search strategy accordingly.\nThe neural network optimizers run about 105 times faster than Spearmint with the LSTM architecture and about 104 times faster with the DNC architecture, with detailed numbers in Table 1. This is depicted clearly in Figure 6."}, {"heading": "3.1.4 Evaluating Transfer of the Global Learner to a Simple RL Problem", "text": "We also consider an application to a simple reinforcement learning task described by Hoffman et al. [2009]. In this problem we simulate a physical system consisting of a number of repellers which affect the fall of particles through a 2D-space. The goal is to direct the path of the particles through high reward regions of the state space and maximize the accumulated discounted reward. The fourdimensional state-space in this problem consists of a particles position and velocity. The path of\nthe particles can be controlled by the placement of repellers which push the particles directly away with a force inversely proportional to their distance from the particle. At each time step the particles position and velocity are updated using simple deterministic physical forward simulation. The control policy for this problem consists of 3 learned parameters for each repeller: 2d location and the strength of the repeller.\nIn our experiments we consider a problem with 2 repellers, i.e. 6 parameters. An example trajectory along with the reward structure (contours) and repeller positions (circles) is displayed in Figure 7. We apply the same perturbation as in the previous subsection to study the average performance. The loss functions of all models are also shown in Figure 7."}, {"heading": "4 Discussion and Future Work", "text": "The experiments have shown that up to the training horizon, T = 30, the learned RNN global optimizers either match or outperform both Spearmint with fixed hyper-parameters and Spearmint with inferred hyper-parameters and input warping.\nOur RNN optimizer was trained on functions genereated using a GP with fixed hyper-parameters. The fact that up to T = 30 it can do well on a varied set of tasks illustrates a significant degree of transfer of the RNN learner.\nThe experiments have also shown that the RNNs are massively faster than Bayesian optimization. Hence, for applications involving a known horizon or where speed matters, we recommend the use of the RNN optimizers.\nHowever, the RNN optimizers also appear to have shortcomings. Training for very long horizons is difficult. This issue was also documented recently in Duan et al. [2016]. It is also clear from Figure 5 (Hartmann6 function) and Figure 7 (control problem) that while the RNNs outperform Spearmint, with DNC doing slightly better than the LSTMs, for a horizon less than T = 30, beyond this training horizon Spearmint eventually achieves a lower error than the neural networks. This is because Spearmint has a mechanism for continued exploration that we have not yet incorporated into our neural network models. We are currently exploring extensions to our approach to improve exploration beyond the training horizon."}, {"heading": "A Related work", "text": "The idea of applying machine learning techniques to optimize the process of learning itself has a long history [Schmidhuber, 1987, Naik and Mammone, 1992, Thrun and Pratt, 1998]. The techniques proposed for solving this meta-learning problem are also quite varied, in part because the lack of consensus on how to draw the line between meta learning and ordinary learning.\nThere is somewhat more consensus about the structure of ordinary learning. The target of learning is a model that has several parameters. The model interacts with an algorithm that ingests data and whose purpose is to choose values for the parameters of the model.\nThe meta part of meta learning arises when the target of learning is itself a learning algorithm. In meta learning an algorithm takes the place of the model in an ordinary learning problem, and the algorithm exposes parameters to a meta algorithm, whose role mirrors that of the algorithm in ordinary learning.\nFrequently the meta algorithm is one that could also play the role of a non-meta algorithm in an ordinary learning problem. In fact the purpose of establishing the meta learning framework is to make this possible. Conceptually one could build increasingly high towers of meta-meta algorithms being tuned by meta-meta-meta algorithms, but in practice this is rarely done.\nDifferent approaches to meta learning can be distinguished by how they handle the interface between model and algorithm in the underlying learning problem, and also by whether the product of meta learning is a model or an algorithm.\nSeveral early works approached the problem of meta learning by coupling the model and algorithm into a single system [Sutton, 1992, Schraudolph, 1999]. The insight here is to observe that the process of learning in a static model can be viewed as the evolution of a dynamical system. Meta learning is achieved by applying an ordinary learning algorithm to the parameters of this dynamical system.\nThe coupled setting handles the boundary between model and algorithm by removing it. In many ways this is meta learning through force of interpretation alone, since an observer without prior knowledge of the decoupled state of model and algorithm could not distinguish between an ordinary learning in a dynamic model and a coupled meta learning in a static model.\nHowever, since coupled meta learning reduces so readily to an ordinary learning problem it is often the most straightforward setting to work in. A modern technique in this setting is RL2 [Duan et al., 2016], which lifts an ordinary reinforcement learning agent into the role of an algorithm by reinterpreting the dynamics of the environment. Once the model and algorithm are coupled in this way they are able to apply an ordinary reinforcement learning algorithm to set the parameters of the fused agent-algorithm without further work. The collapse of model and algorithm can be more readily understood by contrasting their work with Mirowski et al. [2016], who attack similar tasks without the meta learning gloss.\nEarly works that take this approach are Schmidhuber [1992] and Schmidhuber [1993]\u2014building on even earlier work of Schmidhuber [1987]\u2014which consider networks that are able to modify their own weights. This leads to end-to-end differentable systems which allow, in principle, for extremely general update strategies to be learned.\nInstead of coupling the model and algorithm the boundary between them can be maintained. Maintaining the boundary allows the model and algorithm to be decoupled post-learning, and maintains a clear separation of concerns between them. Since the model and algorithm decouple in this setting it becomes useful to ask about the product of a meta learning procedure. Throughout the course of meta learning both models and algorithms are trained, but typically only one of them is directly of interest, while the other plays a subordinate role as a tool for realizing the primary target.\nBayesian hyper-parameter optimization is a good example of meta learning where it is the model which is of primary interest. Fitting a response surface used to predict the performance of an underlying model and using that surface to guide selection of different model configurations fits comfortably within the meta learning framework, but the response surface itself is typically not the focus. The product of Bayesian hyper-parameter optimization is a configuration for the model that achieves good performance on the target task.\nZoph and Le [2016] present a \u201cneuralization\u201d of this pattern, carefully engineered for the problem of designing neural network architectures. Their setup trains a controller RNN to produce a string in a custom domain specific language (DSL) for describing neural network architectures. An architecture matching the produced configuration (the \u201cchild\u201d network) is instantiated and trained, and its performance is measured. Although this setup avoids building an explicit model (as is done in Gaussian process based Bayesian optimization), we can view the string produced by the controller RNN as playing the same role as the proposal obtained by maximizing the acquisition function in a response surface framework. Representing the query strategy directly allows them avoid the prickly problem of specifying a geometry for strings in their DSL, but makes it more difficult to re-use the information acquired at the meta-level for future tasks.\nA series of papers from Bengio et al. [1990, 1992, 1995] present methods for local update rules that avoid back-propogation by using simple parametric rules, and Runarsson and Jonsson [2000] extend this to more complex update models. The result of meta learning is in these cases an algorithm (i.e. the local rule), and the models are used simply as a vehicle for learning the rules. The resulting rules can be applied to new models after meta learning.\nAndrychowicz et al. [2016] also learn update rules for optimization, but in a way that is more integrated with the standard optimization framework. Rather than trying to distil a global objective into a local rule, this work focuses on learning how to integrate gradient observations over time in order to achieve fast learning of the model. The resulting algorithm can be applied to new optimization problems, and the component-wise structure of the algorithm allows a single learned algorithm to be applied to new problems of different dimensionality.\nA third angle for characterizing meta learning approaches is to look at where information about the learning process is accumulated. This is different than asking about the product of meta learning, which can be seen by examining the standard setup for Bayesian hyper parameter optimization. The product of meta learning in that case is the model, as discussed above, but the information about the learning process itself is encoded in the response surface.\nUnderstanding where information about learning is accumulated allows us to identify where we might expect to see transfer of information between problems at the meta-level. Decoupled approaches that accumulate meta-information into the algorithm can transfer this knowledge trivially by simply applying the algorithm to a new problem. Methods that pack this information into a query policy do not have any straightforward avenue to transfer in this way.\nMethods that use an auxiliary model to represent the learning process have the most potential for transfer, but may require additional work to extract the appropriate information from this model [Swersky et al., 2013]. This perspective also allows us to draw a connection between meta learning and model based active learning, where a model of the environment is used to guide an active process for collecting data to improve that model [Assael et al., 2015, Agrawal et al., 2016]."}], "references": [{"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["P. Agrawal", "A. Nair", "P. Abbeel", "J. Malik"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Agrawal and Goyal.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2013}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul", "B. Shillingford", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["J.-A.M. Assael", "N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "M.P. Deisenroth"], "venue": "arXiv preprint arXiv:1510.02173,", "citeRegEx": "Assael et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Assael et al\\.", "year": 2015}, {"title": "On the search for new learning rules for ANNs", "author": ["S. Bengio", "Y. Bengio", "J. Cloutier"], "venue": "Neural Processing Letters,", "citeRegEx": "Bengio et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1995}, {"title": "Learning a synaptic learning", "author": ["Y. Bengio", "S. Bengio", "J. Cloutier"], "venue": "rule. Universite\u0301 de Montre\u0301al, De\u0301partement d\u2019informatique et de recherche ope\u0301rationnelle,", "citeRegEx": "Bengio et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1990}, {"title": "On the optimization of a synaptic learning rule", "author": ["Y. Bengio", "S. Bengio", "J. Cloutier", "J. Gecsei"], "venue": "In in Conference on Optimality in Biological and Artificial Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1992}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": "Technical Report UBC TR-2009-23 and arXiv:1012.2599v1, Dept. of Computer Science,", "citeRegEx": "Brochu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2009}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in Neural Information Processing Systems, pages 2249\u20132257,", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Multi-bandit best arm identification", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Hybrid computing using a neural network with dynamic external memory. Nature, 2016", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u00c5ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou", "A.A.P. Badia", "K.M. Hermann", "Y. Zwols", "G. Ostrovski", "A. Cain", "H. King", "C. Summerfield", "P. Blunsom", "K. Kavukcuoglu", "D. Hassabis"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "The formation of learning sets", "author": ["H.F. Harlow"], "venue": "Psychological review,", "citeRegEx": "Harlow.,? \\Q1949\\E", "shortCiteRegEx": "Harlow.", "year": 1949}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C. Schuler"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hennig and Schuler.,? \\Q2012\\E", "shortCiteRegEx": "Hennig and Schuler.", "year": 2012}, {"title": "Predictive entropy search for Bayesian optimization with unknown constraints", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.A. Gelbart", "M.W. Hoffman", "R.P. Adams", "Z. Ghahramani"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["S. Hochreiter", "A.S. Younger", "P.R. Conwell"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning", "author": ["M. Hoffman", "B. Shahriari", "N. de Freitas"], "venue": "In AI and Statistics,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "New inference strategies for solving Markov decision processes using reversible jump MCMC", "author": ["M.W. Hoffman", "H. Kueck", "N. de Freitas", "A. Doucet"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Hoffman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2009}, {"title": "A taxonomy of global optimization methods based on response", "author": ["D. Jones"], "venue": "surfaces. J. of Global Optimization,", "citeRegEx": "Jones.,? \\Q2001\\E", "shortCiteRegEx": "Jones.", "year": 2001}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D. Jones", "M. Schonlau", "W. Welch"], "venue": "J. of Global optimization,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "On bayesian upper confidence bounds for bandit problems", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "A layered network model of associative learning: learning to learn and configuration", "author": ["E.J. Kehoe"], "venue": "Psychological review,", "citeRegEx": "Kehoe.,? \\Q1988\\E", "shortCiteRegEx": "Kehoe.", "year": 1988}, {"title": "Controlled experiments on the web: survey and practical guide", "author": ["R. Kohavi", "R. Longbotham", "D. Sommerfield", "R.M. Henne"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "Kohavi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kohavi et al\\.", "year": 2009}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["H.J. Kushner"], "venue": "Journal of Fluids Engineering,", "citeRegEx": "Kushner.,? \\Q1964\\E", "shortCiteRegEx": "Kushner.", "year": 1964}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Learning to navigate in complex environments", "author": ["P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K. Kavukcuoglu", "D. Kumaran", "R. Hadsell"], "venue": null, "citeRegEx": "Mirowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 2016}, {"title": "The Bayesian approach to global optimization", "author": ["J. Mo\u010dkus"], "venue": "In Systems Modeling and Optimization,", "citeRegEx": "Mo\u010dkus.,? \\Q1982\\E", "shortCiteRegEx": "Mo\u010dkus.", "year": 1982}, {"title": "The application of Bayesian methods for seeking the extremum", "author": ["J. Mo\u010dkus", "V. Tiesis", "A. \u017dilinskas"], "venue": "Toward Global Optimization,", "citeRegEx": "Mo\u010dkus et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Mo\u010dkus et al\\.", "year": 1978}, {"title": "Meta-neural networks that learn by learning", "author": ["D.K. Naik", "R. Mammone"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Naik and Mammone.,? \\Q1992\\E", "shortCiteRegEx": "Naik and Mammone.", "year": 1992}, {"title": "Evolution and design of distributed learning rules", "author": ["T.P. Runarsson", "M.T. Jonsson"], "venue": "In IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks,", "citeRegEx": "Runarsson and Jonsson.,? \\Q2000\\E", "shortCiteRegEx": "Runarsson and Jonsson.", "year": 2000}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["A. Santoro", "S. Bartunov", "M. Botvinick", "D. Wierstra", "T. Lillicrap"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Evolutionary Principles in Self-Referential Learning. On Learning how to Learn: The MetaMeta-Meta...-Hook", "author": ["J. Schmidhuber"], "venue": "PhD thesis, Institut f. Informatik, Tech. Univ. Munich,", "citeRegEx": "Schmidhuber.,? \\Q1987\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1987}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1992}, {"title": "A neural network that embeds its own meta-levels", "author": ["J. Schmidhuber"], "venue": "In International Conference on Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1993\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1993}, {"title": "Local gain adaptation in stochastic gradient descent", "author": ["N.N. Schraudolph"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Schraudolph.,? \\Q1999\\E", "shortCiteRegEx": "Schraudolph.", "year": 1999}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S.L. Scott"], "venue": "Applied Stochastic Models in Business and Industry,", "citeRegEx": "Scott.,? \\Q2010\\E", "shortCiteRegEx": "Scott.", "year": 2010}, {"title": "An entropy search portfolio", "author": ["B. Shahriari", "Z. Wang", "M.W. Hoffman", "A. Bouchard-C\u00f4t\u00e9", "N. de Freitas"], "venue": "In NIPS workshop on Bayesian Optimization,", "citeRegEx": "Shahriari et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2014}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "author": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Shahriari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2016}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Input warping for Bayesian optimization of non-stationary functions", "author": ["J. Snoek", "K. Swersky", "R.S. Zemel", "R.P. Adams"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Snoek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2014}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bar-delta", "author": ["R.S. Sutton"], "venue": "In Association for the Advancement of Artificial Intelligence,", "citeRegEx": "Sutton.,? \\Q1992\\E", "shortCiteRegEx": "Sutton.", "year": 1992}, {"title": "Multi-task Bayesian optimization", "author": ["K. Swersky", "J. Snoek", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Swersky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Learning to learn", "author": ["S. Thrun", "L. Pratt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun and Pratt.,? \\Q1998\\E", "shortCiteRegEx": "Thrun and Pratt.", "year": 1998}, {"title": "An informational approach to the global optimization of expensiveto-evaluate functions", "author": ["J. Villemonteix", "E. Vazquez", "E. Walter"], "venue": "J. of Global Optimization,", "citeRegEx": "Villemonteix et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Villemonteix et al\\.", "year": 2009}, {"title": "Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters", "author": ["Z. Wang", "N. de Freitas"], "venue": "arXiv preprint arXiv:1406.7758,", "citeRegEx": "Wang and Freitas.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Freitas.", "year": 2014}, {"title": "Bayesian multi-scale optimistic optimization", "author": ["Z. Wang", "B. Shakibi", "L. Jin", "N. de Freitas"], "venue": "In AI and Statistics,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Reminiscence and rote learning", "author": ["L.B. Ward"], "venue": "Psychological Monographs,", "citeRegEx": "Ward.,? \\Q1937\\E", "shortCiteRegEx": "Ward.", "year": 1937}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Neural architecture search with reinforcement learning", "author": ["B. Zoph", "Q.V. Le"], "venue": "Technical report, submitted to ICLR", "citeRegEx": "Zoph and Le.,? \\Q2016\\E", "shortCiteRegEx": "Zoph and Le.", "year": 2016}, {"title": "2016] also learn update rules for optimization, but in a way that is more", "author": ["Andrychowicz"], "venue": null, "citeRegEx": "Andrychowicz,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "In Andrychowicz et al. [2016] the product of meta-learning is a trained RNN, which is subsequently used as an optimization algorithm to fit other models to data or optimize any differentiable objective function.", "startOffset": 3, "endOffset": 30}, {"referenceID": 2, "context": "In Andrychowicz et al. [2016] the product of meta-learning is a trained RNN, which is subsequently used as an optimization algorithm to fit other models to data or optimize any differentiable objective function. In contrast, in Zoph and Le [2016] the output of meta-learning may also be an RNN, but in their case the RNN is subsequently used as a model that is fit to data using a classical optimizer.", "startOffset": 3, "endOffset": 247}, {"referenceID": 40, "context": "The model can be a Beta-Bernoulli bandit, a random forest with frequentist confidence estimates, a Dirichlet process prior, a Bayesian neural network, or a Gaussian process [Shahriari et al., 2016].", "startOffset": 173, "endOffset": 197}, {"referenceID": 50, "context": "It also raises some theoretical concerns [Wang et al., 2014].", "startOffset": 41, "endOffset": 60}, {"referenceID": 15, "context": "We will make use of two types of RNN: long-short-term memory networks (LSTMs) by Hochreiter and Schmidhuber [1997] and differentiable neural computers (DNCs) by Graves et al.", "startOffset": 81, "endOffset": 115}, {"referenceID": 12, "context": "We will make use of two types of RNN: long-short-term memory networks (LSTMs) by Hochreiter and Schmidhuber [1997] and differentiable neural computers (DNCs) by Graves et al. [2016]. In our experiments the RNN uses its memory to store information about previous queries and learns to access its memory to make decisions about which parts of the domain to explore or exploit next.", "startOffset": 161, "endOffset": 182}, {"referenceID": 19, "context": "We are therefore considering the finite horizon setting that is popular in AB tests Kohavi et al. [2009], Scott [2010] and is often studied under the umbrella of best arm identification in the bandits literature [Bubeck et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 19, "context": "We are therefore considering the finite horizon setting that is popular in AB tests Kohavi et al. [2009], Scott [2010] and is often studied under the umbrella of best arm identification in the bandits literature [Bubeck et al.", "startOffset": 84, "endOffset": 119}, {"referenceID": 2, "context": "This loss was considered by Andrychowicz et al. [2016] in the context of learning first-order optimizers, but ultimately rejected in favor of", "startOffset": 28, "endOffset": 55}, {"referenceID": 52, "context": "If even at training time the derivatives of f are not available then it would be necessary to approximate these derivatives via an algorithm such as REINFORCE [Williams, 1992].", "startOffset": 159, "endOffset": 175}, {"referenceID": 28, "context": "Also the posterior expected improvement used within LEI can be easily computed Mo\u010dkus [1982] and differentiated as well.", "startOffset": 79, "endOffset": 93}, {"referenceID": 43, "context": "For the first set of experiments we also compare against training with a loss based on GP-UCB [Srinivas et al., 2010], which we refer to as DNC UCB.", "startOffset": 94, "endOffset": 117}, {"referenceID": 42, "context": "Figure 5 shows the loss compared with a random baseline, Spearmint with length scale fixed to the training length scale, and Spearmint with automatic inference of the GP hyper-parameter values and input warping to deal with non-stationarity [Snoek et al., 2014].", "startOffset": 241, "endOffset": 261}, {"referenceID": 18, "context": "We also consider an application to a simple reinforcement learning task described by Hoffman et al. [2009]. In this problem we simulate a physical system consisting of a number of repellers which affect the fall of particles through a 2D-space.", "startOffset": 85, "endOffset": 107}], "year": 2016, "abstractText": "We present a learning to learn approach for training recurrent neural networks to perform black-box global optimization. In the meta-learning phase we use a large set of smooth target functions to learn a recurrent neural network (RNN) optimizer, which is either a long-short term memory network or a differentiable neural computer. After learning, the RNN can be applied to learn policies in reinforcement learning, as well as other black-box learning tasks, including continuous correlated bandits and experimental design. We compare this approach to Bayesian optimization, with emphasis on the issues of computation speed, horizon length, and exploration-exploitation trade-offs.", "creator": "LaTeX with hyperref package"}}}