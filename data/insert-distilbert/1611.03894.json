{"id": "1611.03894", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Unsupervised Learning For Effective User Engagement on Social Media", "abstract": "in this paper, we investigate the effectiveness of automated unsupervised feature learning techniques in predicting wireless user engagement on social media. working specifically, we compare two methods to predict the number of feedbacks ( i. e., short comments ) that a site blog post is likely to receive. one we compare principal image component analysis ( pca ) and sparse autoencoder to a baseline method where the data are only centered and scaled, on each of two models : linear regression and regression tree. we lastly find recommendations that unsupervised learning techniques significantly considerably improve evaluating the prediction accuracy on both models. for the linear regression model, sparse autoencoder achieves the best evaluation result, with an objective improvement in the root mean squared error ( rmse ) on placing the test set of 42 % over the baseline method. for the inverted regression tree model, pca achieves the best conditional result, yet with an improvement in rmse of 15 % increase over the baseline.", "histories": [["v1", "Fri, 11 Nov 2016 22:01:41 GMT  (273kb,D)", "http://arxiv.org/abs/1611.03894v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thai pham", "camelia simoiu"], "accepted": false, "id": "1611.03894"}, "pdf": {"name": "1611.03894.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning For Effective User Engagement on Social Media", "authors": ["Thai T. Pham", "Camelia Simoiu"], "emails": ["thaipham@stanford.edu", "csimoiu@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Social media has become an important tool for public engagement. Businesses are interested in engaging with potential buyers on platforms such as Facebook or their company news pages; bloggers are publishers are interested in increasing their follower and reader base by writing captivating articles that prompt high levels of user feedback (in the form of likes, comments, etc.). Such groups will inevitably need to understand the types of content that is likely to elicit the most user engagement as well as any underlying patterns in the data such as temporal trends. Specifically, we focus on predicting the number of feedbacks (i.e., comments) that a blog post is expected to receive. Given a set of blog documents that appeared in the past, for which the number and time stamp received, the task is to predict how many feedbacks recently published blog-entries will receive in the next H hours.\nA first challenge in answering this question is how to effectively pre-process the data. Despite offering a rich source of information, such data sets are usually noisy, high-dimensional, with many correlated features. In this paper, we focus on two unsupervised learning approaches for pre-processing: the traditional method of Principal Component Analysis (PCA), and a more recently-developed method from deep-learning, the sparse Autoencoder. These pre-processing methods are generally used to reduce dimensionality, eliminate correlations among variables (since there may be a number of irrelevant features in the data set), decrease the computation time, and extract good features for subsequent analyses. PCA linearly transforms the original inputs into new uncorrelated features. The sparse Autoencoder is trained to reconstruct its own inputs through a non-recurrent neural network, and it creates as output new features from non-linear combinations of the old ones. We compare the effects of these two methods on two prediction models of linear regression and regression trees in the feedback prediction task.\nThe rest of the paper proceeds as follows. Section 2 includes a short review of related literature. Section 3 describes the data set used. Section 4 discusses the unsupervised feature learning methods of PCA and sparse Autoencoder. Section 5 gives comparative results from different models for\nar X\niv :1\n61 1.\n03 89\n4v 1\n[ cs\n.L G\n] 1\n1 N\nov 2\nthe unprocessed data, and the pre-processed data using PCA and sparse Autoencoder. Section 6 concludes and opens future research directions. Section 7 acknowledges help."}, {"heading": "2 Literature Review", "text": "Cao et. al. [2] compare PCA, Kernel PCA (KPCA), and Independent Component Analysis (ICA) as applied to Support Vector Machine (SVM) for feature extraction to three data sets (sunspot data, Satan Fe data set A, and five real futures contracts). SVM by feature extraction using PCA, KPCA or ICA can achieve better generalization performance than that without feature extraction, and that KPCA and ICA perform better than PCA on all the studied data sets, with the best performance in KPCA. The reason lies in the fact that KPCA and ICA can explore higher order information of the original inputs than PCA. Based on these results, we expect the (sparse) Autoencoder to perform better than PCA as it is also a non-linear combination of features.\nBuza [1] uses the same data set as we do and compares a variety of models to predict the number of future feedbacks for a blog. They consider two performance metrics: Area Under Curve explained (AUC), and the number of blog pages that were predicted to have the largest number of feedbacks out of the top ten blog pages that had the highest number of feedbacks in reality. Buza [1] examines various models: a multilayer perceptron model, RBF-networks, regression trees (REP-tree, M5Ptree), nearest neighbor models, multivariate linear regression and bagging, however do not preprocess the data. He finds that M5P Trees and REP Trees seem to work very well both in terms of accuracy and runtime, averaging 5\u22126 feedbacks and 84\u221292% for the examined models. The neural network model is competitive to the regression trees. The best models are the linear model (AUC = 92% and 5.2 hits) and neural network (AUC = 87%, 5.8 hits). Bagging does not statistically improve the performance of MLPs and RBF-Network both in terms of comments and AUC (improved the absolute performance, but not taking into account standard deviations). We use the same data set as in Buza [1], but approach the problem differently. Specifically, we use unsupervised feature learning techniques to pre-process the data before running prediction models."}, {"heading": "3 Data Set", "text": "This data set is made available from the UCI Machine Learning Repository, and comprises a total of 37, 279 crawled blog pages. The prediction task is to predict the number of comments for a blog post in the upcoming H = 24 hours. The processed data has a total of 280 features (without the target variable, i.e., number of feedbacks). 1 The blog documents (instances) are transformed into vectors to be inputted to the machine learning algorithm. This collection corresponds approximately 6 GB of plain HTML document (i.e., without images). The following features are extracted from each document:\n\u2022 Basic features: Number of links and feedbacks in the previous 24 hours; number of links and feedbacks in the previous 48 hours; how the number of links and feedbacks increased/decreased since to the publication date of the blog; number of links and feedbacks in the first 24 hours after the publication of the document.\n\u2022 Textual features: The most discriminative bag-of-words features. \u2022 Weekday features: Binary indicator features that describe on which day of the week the\nmain text of the document was published and for which day of the week the prediction has to be calculated.\n\u2022 Parent features: A document dP is treated as a patent of document d, if d is a reply to dP, i.e., there is a trackback link on dP that points to d; parent features are the number of parents, minimum, maximum and average number of feedbacks that the parents received."}, {"heading": "4 Unsupervised Feature Learning Methods", "text": "Prior to implementing any unsupervised feature learning method, we first eliminate four variables which contain only zeros, resulting in 276 predictor variables (i.e., features) and one outcome vari-\n1We use the processed data provided in [1].\nable. Out of these variables, 58 are continuous, and the remainder are binary. We center and scale all non-binary variables to have zero mean and unit standard deviation. This step prevents any one variable from dominating the variance of the data set simply due to taking values in a larger range. We also center and scale the outcome variable for testing later. Histograms of the number of feedbacks received for both train and test sets are included in Figures 1(a) and 1(b).\nWe observe that most values of the centered and scaled response variable are concentrated near zero for both training and test sets. This prevents any one point from potentially skewing the estimation results later on."}, {"heading": "4.1 Principal Component Analysis", "text": "PCA is a powerful technique for extracting structure from high-dimensional data sets. We implement PCA in order to capture the intrinsic variability in the data and reduce the dimensionality of the data set. An important decision in PCA analysis is to determine the optimal number of principal components to use, since the data will be projected onto this new basis of principal components before any subsequent analysis. To determine the optimal number of components, we implement two Gap-style tests as in [3]. In both tests, the Gap formula has the same form\nGap(k) = E[log(Wk)]\u2212 log(Wk),\nwhere log(Wk) is the logarithm of some objective function corresponding to the first k principal components andE[log(Wk)] is calculated in the similar way with the expectation taken over uniform samples from the smallest subspace containing the original data."}, {"heading": "4.1.1 Gap Test Using Reconstruction Error", "text": "In the first approach, we define Wk to be the reconstruction error in using the first k principal components to approximate the data. Specifically, let {xkij}, j = 1, ..., 276 be the projection of the ith data point onto the rank k principal component approximation. Then Wk is defined by\nWk = \u2211 i,j (xij \u2212 x\u0302kij)2\nThe steps for this Gap-style test are as follows.\ni. After calculating the 276 principal components, for each k \u2208 {1, ..., 276}, we determine Wk as above and take their log values to obtain 276 values, log(Wk)\u2019s.\nii. We generate B samples uniformly from the smallest subspace containing the original data and find the principal components for this newly generated matrix. As above, for each k \u2208 {1, ..., 276}, we calculate Wk and take their log values. So for each k, we obtain B values; we take their average to get an estimate forE[log(Wk)] and calculate their standard deviation. We obtain 276 such values, and call them sd[k] for k \u2208 {1, ..., 276}.\niii. Calculate the adjusted standard error according to the formula\nse[k] = \u221a 1 + 1\nB \u00d7 sd[k].\nWe then choose the optimal k to be the smallest value of k such that\nGap[k] \u2265 Gap[k + 1]\u2212 se[k + 1].\nFigure 2 shows a plot of the Gap statistic versus the number of principal components for (a) the first fifty principal components, and (b) the total number of principal components. The optimal number of principal components that minimizes the Gap is k\u2217 = 2."}, {"heading": "4.1.2 Gap Test Using Explained Variation", "text": "In this approach, we define Wk to be the variation explained by the first k principal components. Specifically, Wk is the ratio of the sum of the first k principal components\u2019 variances and the sum of the total variance of all principal components. Mathematically speaking,\nWk = \u2211k i=1 Vi\u2211276 j=1 Vj ,\nwhere Vi is the ith principal component\u2019s variance.\nThe steps in this Gap test are the same as those for the Reconstruction Error approach with the exception of Wk, which we have described above. We then choose the optimal k to be the smallest value of k such that\nGap[k] \u2264 Gap[k + 1]\u2212 se[k + 1].\nWe note the difference in direction of the two conditions in defining the optimal k between the two Gap-style tests. This is as expected because for the Gap test using reconstruction error, we want to minimize Wk while for the Gap test using explained variation we want to maximize the corresponding Wk. However different, this Gap-style test gives the same optimal number of components as the other one which is k\u2217 = 2.\nFigure 3 shows the corresponding plot of the Gap statistic versus the number of principal components for (a) the first fifty principal components, and (b) the total number of principal components."}, {"heading": "4.2 Sparse Autoencoder", "text": "The Autoencoder is based on the concept of sparse coding proposed in a seminal paper by Olshausen et al. [4]. In this paper, we implement a 3-layer Autoencoder (Figure 4) in order to learn a compressed representation (encoding) of the features. Each \u2018neuron\u2019 (circle) represents a computational unit that takes as input x1, x2, ...xn (and a \u201c+1\u201d intercept term, called a bias unit), and outputs\nhW,b(x) = f ( W (2)T f ( W (1)Tx+ b(1) ) + b(2) ) where f : R \u2192 R is called the transfer (activation) function. We choose f(\u00b7) to be the hyperbolic tangent (tanh function). The tanh function was chosen instead of the sigmoid function since its output range, [-1,1], more closely approximates the range of our predictor variable than the sigmoid function (range is [0,1]). The tanh activation function is given below:\nf(z) = exp(z)\u2212 exp(\u2212z) exp(z) + exp(\u2212z) .\nThe leftmost layer of the network is called the input layer, and the rightmost layer the output layer. The middle layer of nodes is called the hidden layer since its values are not observed in the training set.\nThe Autoencoder tries to learn a function hW,b(x) \u2248 x. In other words, it is trying to learn an approximation to the identity function, so as to output x\u0302 that is similar to x. The sparse Autoencoder is the Autoencoder with the sparsity constraint added to the objective function. In other words, the objective function of the sparse Autoencoder is given by the reconstruction error with regularization:\nJ(W, b) = 1\nm m\u2211 i=1 ( \u2016 hW,b(x(i))\u2212 x(i) \u20162 ) + \u03bb nl\u22121\u2211 l=1 sl\u2211 i=1 sl+1\u2211 j=1 ( W (l) ji )2 + \u03c1 nl\u22121\u2211 l=1 sl\u2211 i=1 sl+1\u2211 j=1 \u2223\u2223\u2223W (l)ji \u2223\u2223\u2223 , where m is the number of training samples, nl is the number of layers (3 in our case), and sl is the number of units in layers l. The first term, J(W, b) is an average sum-of-squares error term. The second term is a regularization (L2) term that tends to decrease the magnitude of the weights, and helps prevent overfitting. The weight decay parameter \u03bb controls the relative importance of the terms. The sparsity parameter \u03c1 controls how sparse the Autoencoder is. This neural network is then trained using a back-propagation algorithm, where the objective function is minimized using batch gradient descent.\nAlthough the identity function seems like a trivial function to be trying to learn, by placing constraints on the network, such as the weight decay, the sparsity parameter, and the number of hidden units, it is possible to discover interesting structure about the data. When we use a few hidden units, the network is forced to learn a compressed representation of the input. If there is some structure in the data, for example, if some of the input features are correlated, the algorithm will be able to discover some of those correlations, so that the (sparse) Autoencoder often ends up learning a low-dimensional representation similar to PCA.\nIn order to determine the optimal values of the hyper-parameters and the number of hidden units, we split the data into training, validation and test sets and perform a grid search over the parameter space of the number of units ([2, 5, 10, 15]) and the weight decay \u03bb ([0.0001, 0.01, 0.1]) while using the default value for \u03c1 of 0.01. The reason for not cross-validating over \u03c1 and for using small sets of possible values for \u03bb and the number of hidden units is the high computational cost. For each value of \u03bb, we use 5-fold cross-validation on the training data to select the optimal number of hidden units. We then calculate the root mean squared error (RMSE) on the validation data and choose the value for the weight decay corresponding to the smallest RMSE. This way, we obtain the optimal weight decay and the optimal number of units in the hidden layer.\nThe optimal weight decay was found to be \u03bb = 0.0001. For this \u03bb, we plot the RMSE over the number of units in the hidden layer in Figure 5 and obtain 5 as the optimal number of units.\nWith the determined optimal values of the weight decay, the sparsity parameter, and the number of hidden units, we can also determine the estimated weight matrices and biases. We then fit the training and test data through these weights and biases to obtain the processed data for subsequent analysis."}, {"heading": "5 Results and Discussion", "text": "We use two models to predict the number of feedbacks: linear regression (a linear model) and regression tree (a non-linear model). We compare the Test RMSE achieved using the output from PCA and sparse Autoencoder for each model. As a baseline, we use the centered and scaled data as input to the models. For PCA, we use the projected data onto the k\u2217 = 2 components, and for the sparse Autoencoder, we use the optimal number of 5 units in the hidden layer, as found by cross-validation. Results are summarized in Table 1.\nFor linear regression, PCA achieves an 11% improvement compared to the baseline model while the sparse Autoencoder achieves a 42% improvement. For the regression tree model, PCA achieves a 15% improvement compared to the baseline model. The sparse Autoencoder, however, performs worse than the baseline model (an increase in RMSE from 0.6005 to 1.0531). Unsupervised feature learning in the pre-processing step hence generally improves the prediction accuracy. Taking into account the small range of the outcome variable after scaling and centering, the improvements are certainly significant.\nFor the linear regression model the sparse Autoencoder outperforms PCA. This is likely because the sparse Autoencoder solves many of the drawbacks of PCA: PCA only allows linear combinations of the features, restricting the output to orthogonal vectors in feature space that minimize the reconstruction error; PCA also assumes points are multivariate Gaussian, which is most likely not true in many applications, including ours. The sparse Autoencoder is able to learn much more complex, non-linear representations of the data and thus achieves much better accuracy.\nAn interesting pattern can be observed in the interactions between the linearity and non-linearity of the models and the feature learning methods. The non-linear feature selection method (sparse Autoencoder) achieves significant improvement in RMSE for the linear model (linear regression), while the linear feature selection method (PCA) performs best for the non-linear model (regression tree). Combining the non-linear regression tree model with the non-linear sparse Autoencoder, however, leads to worse results than the baseline. This is likely because regression trees have a top-down construction, splitting at every step on the variable that best divides the data. The sparse Autoencoder returns non-linear combinations of features, making it difficult for regression trees to anticipate. In addition, each tree samples a subset of variables to split on while sparse Autoencoders are known to be sensitive to parameter selection, and hence are not optimized to predict on subsets of the predictor variables, leading to unstable results and poorer performance."}, {"heading": "6 Conclusions and Future Work", "text": "We show empirically that using unsupervised feature learning to pre-process the data can improve the feedback prediction accuracy significantly. These results should be of interest to businesses and publishers in pre-screening or editing their social-media posts prior to publicizing so as to estimate the level of engagement they would be expected to achieve. For instance, an automatic editor may flag posts with low predicted user engagement and draw attention to the writer that revisions may be needed.\nTwo directions of future work are immediately obvious. First, we can extend the work by trying other unsupervised feature learning methods such as ICA and Kernel PCA in order to better understand how these methods (linear versus non linear) interact with the model type and to what extent our observations can be generalized. Second, we would be interested in investigating whether results can be further improved by using different transfer functions and additional hidden layers in the sparse Autoencoder (i.e., stacked sparse Autoencoder) in order to better capture the time-series aspects of the data set. We may also want to compare the feature learning methods on other models\nsuch as SVM, boosting, random forests, etc.\nAcknowledgements: We thank Robert Tibshirani for helpful comments."}], "references": [{"title": "Feedback prediction for blogs.\u201d Data Analysis, Machine Learning and Knowledge Discovery", "author": ["Buza", "Krisztian"], "venue": "Springer International Publishing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine.", "author": ["Cao", "K. Chua", "W. Chong", "H. Lee", "Q. Gu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Estimating the number of clusters in a data set via the Gap statistic.", "author": ["Robert Tibshirani", "Guenther Walther", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society, B,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images.", "author": ["Olshausen", "Bruno A"], "venue": "Nature", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}], "referenceMentions": [{"referenceID": 1, "context": "[2] compare PCA, Kernel PCA (KPCA), and Independent Component Analysis (ICA) as applied to Support Vector Machine (SVM) for feature extraction to three data sets (sunspot data, Satan Fe data set A, and five real futures contracts).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Buza [1] uses the same data set as we do and compares a variety of models to predict the number of future feedbacks for a blog.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Buza [1] examines various models: a multilayer perceptron model, RBF-networks, regression trees (REP-tree, M5Ptree), nearest neighbor models, multivariate linear regression and bagging, however do not preprocess the data.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "We use the same data set as in Buza [1], but approach the problem differently.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": ", features) and one outcome variWe use the processed data provided in [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "To determine the optimal number of components, we implement two Gap-style tests as in [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The tanh function was chosen instead of the sigmoid function since its output range, [-1,1], more closely approximates the range of our predictor variable than the sigmoid function (range is [0,1]).", "startOffset": 85, "endOffset": 91}, {"referenceID": 0, "context": "The tanh function was chosen instead of the sigmoid function since its output range, [-1,1], more closely approximates the range of our predictor variable than the sigmoid function (range is [0,1]).", "startOffset": 191, "endOffset": 196}, {"referenceID": 1, "context": "In order to determine the optimal values of the hyper-parameters and the number of hidden units, we split the data into training, validation and test sets and perform a grid search over the parameter space of the number of units ([2, 5, 10, 15]) and the weight decay \u03bb ([0.", "startOffset": 230, "endOffset": 244}], "year": 2016, "abstractText": "In this paper, we investigate the effectiveness of unsupervised feature learning techniques in predicting user engagement on social media. Specifically, we compare two methods to predict the number of feedbacks (i.e., comments) that a blog post is likely to receive. We compare Principal Component Analysis (PCA) and sparse Autoencoder to a baseline method where the data are only centered and scaled, on each of two models: Linear Regression and Regression Tree. We find that unsupervised learning techniques significantly improve the prediction accuracy on both models. For the Linear Regression model, sparse Autoencoder achieves the best result, with an improvement in the root mean squared error (RMSE) on the test set of 42% over the baseline method. For the Regression Tree model, PCA achieves the best result, with an improvement in RMSE of 15% over the baseline.", "creator": "LaTeX with hyperref package"}}}