{"id": "1611.03954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment", "abstract": "many recent works have demonstrated the benefits integration of knowledge graph embeddings in computers completing monolingual knowledge graphs. inasmuch as related knowledge bases are built in several different languages, achieving cross - lingual knowledge gap alignment will help people in constructing toward a coherent knowledge base, and assist machines in dealing electronically with different expressions of entity relationships across diverse given human languages. unfortunately, presently achieving this highly desirable crosslingual alignment by human computing labor is very more costly and errorprone. thus, we propose mtranse, a translation - based model for multilingual knowledge graph and embeddings, to provide a simple parallel and automated solution. structured by encoding entities and relations of each language in a separated embedding space, mtranse provides transitions for each embedding vector to its cross - lingual counterparts in other spaces, while preserving the functionalities of simpler monolingual embeddings. we first deploy three structurally different techniques to represent cross - lingual transitions, namely axis calibration, translation vectors, linear and linear orientation transformations, and derive five variants for mtranse using different loss transfer functions. our models alone can be trained on partially random aligned sequence graphs, where just a small portion each of triples are aligned with exactly their cross - lingual component counterparts. the experiments on cross - lingual entity matching and triple - wise alignment verification show promising results, with some variants themselves consistently outperforming others on different tasks. soon we also explore discovering how mtranse preserves the key intuitive properties of its monolingual text counterpart transe.", "histories": [["v1", "Sat, 12 Nov 2016 04:28:04 GMT  (719kb,D)", "http://arxiv.org/abs/1611.03954v1", null], ["v2", "Tue, 21 Feb 2017 01:20:30 GMT  (506kb,D)", "http://arxiv.org/abs/1611.03954v2", null], ["v3", "Wed, 17 May 2017 19:33:53 GMT  (506kb,D)", "http://arxiv.org/abs/1611.03954v3", "Extended version of the IJCAI-17 paper"]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["muhao chen", "yingtao tian", "mohan yang", "carlo zaniolo"], "accepted": false, "id": "1611.03954"}, "pdf": {"name": "1611.03954.pdf", "metadata": {"source": "CRF", "title": "Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment", "authors": ["Muhao Chen", "Yingtao Tian", "Mohan Yang", "Carlo Zaniolo"], "emails": ["zaniolo}@cs.ucla.edu;", "yittian@cs.stonybrook.edu"], "sections": [{"heading": "Introduction", "text": "Multilingual knowledge bases such as Wikipedia (Wikipedia 2016) and WordNet (Bond and Foster 2013) are becoming essential sources of knowledge for people and AIrelated applications. These knowledge bases are modeled as knowledge graphs that store two aspects of knowledge: the monolingual knowledge that includes entities and relations recorded in the form of triples, and the cross-lingual knowledge that matches the monolingual knowledge among various human languages.\nThe coverage issue of monolingual knowledge has been widely addressed, and parsing-based techniques for completing monolingual knowledge bases have been well studied in the past (Culotta and Sorensen 2004; Zhou et al. 2005; Jiang and Zhai 2007; Yan et al. 2009; Sun, Grishman, and Sekine 2011; Mousavi et al. 2014). More recently,\nmuch attention has been paid to embedding-based techniques, which provide simple methods to encode entities in low-dimensional embedding spaces and capture relations as means of translations among entity vectors. Given a triple (h, r, t) where r is the relation between entities h and t, then h and t are represented as two k-dimensional vectors h and t, respectively. A function fr(h, t) is used to measure the plausibility of (h, r, t), which also implies the transformation r that characterizes r. Exemplarily, the translationbased model TransE (Bordes et al. 2013) uses the loss function fr(h, t) = \u2016h+ r\u2212 t\u2016 1, where r is characterized as a translation vector learnt from the latent connectivity patterns in the knowledge graph. This model provides a flexible way of predicting a missing item in a triple, or verifying the validity of a generated triple. Other works like TransH (Wang et al. 2014) and TransR (Lin et al. 2015), introduce different loss functions that represent the relational translation in other forms, and have achieved promising results in completing the knowledge graphs.\nWhile embedding-based techniques can help improve the completeness of monolingual knowledge, the problem of applying these techniques on cross-lingual knowledge remains largely unexplored. Such knowledge, including interlingual links (ILLs) that match the same entities, and triplewise alignment (TWA) that represents the same relations, is very helpful in synchronizing different language-specific versions of a knowledge base that evolve independently, as needed to further improve applications built on knowledge bases, such as Q&A systems, semantic Web, and Web search. In spite of its importance, this cross-lingual knowledge remains largely intact. In fact, in the most successful knowledge base Wikipedia, we find that ILLs cover less than 15% entity alignment.\nLeveraging knowledge graph embeddings to cross-lingual knowledge no doubt provides a generic way to help extract and apply such knowledge. However, it is a non-trivial task to find a tractable technique to capture the cross-lingual transitions2. Such transitions are more difficult to capture than relational translations for several reasons: (i) a cross-lingual transition has a far larger domain than any monolingual re-\n1Hereafter, \u2016 \u00b7 \u2016means l1 or l2 norm unless explicitly specified. 2We use the word transition here to differentiate from the rela-\ntional translations among entities in translation-based methods.\nar X\niv :1\n61 1.\n03 95\n4v 1\n[ cs\n.A I]\n1 2\nN ov\n2 01\n6\nlational translation; (ii) it applies on both entities and relations, which have incoherent vocabularies among different languages; (iii) the known alignment for training such transitions usually accounts for a small percentage of a knowledge base. Moreover, the characterization of monolingual knowledge graph structures has to be well-preserved to ensure the correct representation of the knowledge to be aligned.\nTo address the above issues, we propose a multilingual knowledge graph embedding model MTransE, that learns the multilingual knowledge graph structure using a combination of two component models, namely the knowledge model and the alignment model. The knowledge model is responsible for encoding entities and relations in a languagespecific version of knowledge graph. We explore the method that organizes each language-specific version in a separated embedding space, in which MTransE adopts TransE as the knowledge model. On top of that, the alignment model learns cross-lingual transitions for both entities and relations across different embedding spaces, where the following three representations of cross-lingual alignment are considered: distance-based axis calibration, translation vectors, and linear transformations. Thus, we obtain five variants of MTransE based on different loss functions, and identify the best variant by comparing them on cross-lingual alignment tasks using two partially aligned trilingual graphs constructed from Wikipedia triples. We also show that MTransE performs as well as its monolingual counterpart TransE on monolingual tasks.\nThe rest of the paper is organized as follows. We first discuss the related work, and then introduce our approach in the section that follows. After that we present the experimental results, and conclude the paper in the last section."}, {"heading": "Related Work", "text": "While, at the best of our knowledge, there is no previous work on learning multilingual knowledge graph embeddings, we will describe next three lines of work which are closely related to this topic. Knowledge Graph Embeddings. Recently, significant advancement has been made in using the translation-based method to train monolingual knowledge graph embeddings. To characterize a triple (h, r, t), models of this family follow a common assumption hr + r \u2248 tr, where hr and tr are either the original vectors of h and t, or the transformed vectors under a certain transformation w.r.t. relation r. The forerunner TransE (Bordes et al. 2013) sets hr and tr as the original h and t, and with that achieves promising results in handling 1-to-1 relations. Later works improve TransE on complex relations by introducing relation-specific transformations on entities to obtain different hr and tr, including projections on relation-specific hyperplanes in TransH (Wang et al. 2014), linear transformations to heterogeneous relation spaces in TransR (Lin et al. 2015), dynamic matrices in TransD (Ji et al. 2015), and other forms (Jia et al. 2016; Nguyen et al. 2016). All these variants of TransE specialize entity embeddings for different relations, therefore improving knowledge graph completion on complex relations at the cost of increased model complexity. Meanwhile translation-\nbased models cooperate well with other models. For example, variants of TransE are combined with word embeddings to help relation extraction from text (Weston et al. 2013; Zhong et al. 2015).\nIn addition to these, there are non-translation-based methods for learning such embeddings. UM (Bordes et al. 2011) and SE (Bordes et al. 2012) are respectively simplified versions of TransE and TransR. Bilinear model (Jenatton et al. 2012) applies a bilinear transformation between h and t. These models do not explicitly represent relation embeddings. SLM (Collobert and Weston 2008) and NTN (Socher et al. 2013) adopt neural networks to learn structured data, while TADW (Yang et al. 2015) uses DeepWalk on graphs to generate corpora for context-based training. These models are expressive and adaptable for both structured and text corpora, but they are too complex to be incorporated into an architecture supporting multilingual knowledge. Multilingual Word Embeddings. Some approaches learn multilingual word embeddings (Zou et al. 2013; Lauly et al. 2014; Gouws, Bengio, and Corrado 2015) to recognize translation of words in different languages. These models are trained on parallel text corpora, where each sentence is aligned with its translation in the other language, therefore they match translated words based on their frequent co-occurrences. However, such parallelism exists in a very small portion in a knowledge base. Hence the aid of additional signals, such as the ones from the graph structure, is needed to help with the alignment. More importantly, multilingual word embeddings do not explicitly characterize the large variety of relations among entities. Knowledge Bases Alignment. Some projects produce cross-lingual alignment in knowledge bases at the cost of extensive human involvement, including crowdsourcing and designing hand-crafted features dedicated to specific applications. Wikidata (Vrandec\u030cic\u0301 and Kro\u0308tzsch 2014) relies on crowdsourcing to create ILLs for a small portion of entities in Wikipedia. DBpedia (Lehmann et al. 2015) manually aligns about 2,000 frequent relations in its ontology. YAGO3 (Mahdisoltani, Biega, and Suchanek 2015) mines association rules on known matches, which combines diverse kinds of confident scores and requires extensively fine tuning. Many other works use probabilistic methods on schema alignment (de Melo and Weikum 2010; Nguyen et al. 2011; Suchanek et al. 2011; Rinser et al. 2013) that require well-established schemata or ontologies, and involve complicated model dependencies that are neither tractable nor reusable. By contrast, embedding-based methods are simple and general, require little human involvement, and generate task-independent features that further contribute to other NLP tasks (Xing et al. 2015)."}, {"heading": "Multilingual Knowledge Graph Embeddings", "text": "We hereby begin our modeling with the formalization of multilingual knowledge graphs."}, {"heading": "Multilingual Knowledge Graphs", "text": "In a knowledge base KB , we use L to denote the set of languages, and L2 to denote the 2-combination of L (i.e.,\nthe set of unordered language pairs). For a language L \u2208 L, GL denotes the language-specific knowledge graph of L, and EL and RL respectively denote the corresponding vocabularies of entity expression and relation expression. T = (h, r, t) denotes a triple in GL such that h, t \u2208 EL and r \u2208 RL. Boldfaced h, r, t respectively represent the embedding vectors of head h, relation r, and tail t. For a language pair (L1, L2) \u2208 L2, \u03b4(L1, L2) denotes the alignment set which contains the pairs of triples that have already been aligned between L1 and L2. For example, across the languages English and French, we may have ( (State\nof California, capital city, Sacramento), (E\u0301tat de Californie, capitale, Sacramento) ) \u2208 \u03b4(English, French). The alignment set commonly exists in a small portion in a multilingual knowledge base (Vrandec\u030cic\u0301 and Kro\u0308tzsch 2014; Mahdisoltani, Biega, and Suchanek 2015; Lehmann et al. 2015), and is one part of knowledge we want to extend.\nOur model consists of two components that learn on the two facets of KB : the knowledge model that encodes the entities and relations from each language-specific graph structure, and the alignment model that learns the cross-lingual transitions from the existing alignment. We define a model for each language pair from L2 that has a non-empty alignment set. Thus, for a KB with more than two languages, a set of models composes the solution. In the following, we use a language pair (Li, Lj) \u2208 L2 as an example to describe how we define each component of a model."}, {"heading": "Knowledge Model", "text": "For each language L \u2208 L, a dedicated k-dimensional embedding space RkL is assigned for vectors of EL and RL, where R is the field of real numbers. We adopt the basic translation-based method of TransE for each involved language. Therefore its loss function is given as below:\nSK = \u2211\nL\u2208{Li,Lj} \u2211 (h,r,t)\u2208GL \u2016h+ r\u2212 t\u2016\nIt measures the plausibility of all given triples. By minimizing the loss function, the knowledge model preserves monolingual relations among entities, while also acts as a regularizer for the alignment model. Meanwhile, due to the disjointness of the embedding spaces for every language, the knowledge model partitions the knowledge base into disjoint subsets that can be trained in parallel."}, {"heading": "Alignment Model", "text": "The objective of the alignment model is to construct the transitions between the vector spaces of Li and Lj . Its loss function is given as below:\nSA = \u2211\n(T,T \u2032)\u2208\u03b4(Li,Lj)\nSa(T, T \u2032)\nThereof, the alignment score Sa(T, T \u2032) iterates through all pairs of aligned triples. Three different techniques to score the alignment are considered: distance-based axis calibration, translation vectors, and linear transformations. Each of them is based on a different assumption, and constitutes different forms of Sa alongside.\nDistance-based Axis Calibration. This type of alignment models penalize the alignment based on the distances of cross-lingual counterparts. Either of the following two scorings can be adopted to the model.\nSa1 = \u2016h\u2212 h\u2032\u2016+ \u2016t\u2212 t\u2032\u2016 Sa1 regulates that correctly aligned multilingual expressions of the same entity tend to have close embedding vectors. Thus by minimizing the loss function that involves Sa1 on known pairs of aligned triples, the alignment model adjusts axes of embedding spaces towards the goal of coinciding the vectors of the same entity in different languages.\nSa2 = \u2016h\u2212 h\u2032\u2016+ \u2016r\u2212 r\u2032\u2016+ \u2016t\u2212 t\u2032\u2016 Sa2 overlays the penalty of relation alignment to Sa1 to explicitly converge coordinates of the same relation.\nThe alignment models based on axis calibration assume analogous spatial emergence of items in each language. Therefore, it realizes the cross-lingual transition by carrying forward the vector of a given entity or relation from the space of the original language to that of the other language. Translation Vectors. This model encodes cross-lingual transitions into vectors. It consolidates alignment into graph structures and characterizes cross-lingual transitions as regular relational translations. Hence Sa3 as below is derived.\nSa3 = \u2225\u2225h+ veij \u2212 h\u2032\u2225\u2225+ \u2225\u2225r+ vrij \u2212 r\u2032\u2225\u2225+ \u2225\u2225t+ veij \u2212 t\u2032\u2225\u2225\nThereof veij and v r ij are respectively deployed as the entitydedicated and relation-dedicated translation vectors between Li and Lj , such that we have e + veij \u2248 e\u2032 for embedding vectors e, e\u2032 of the same entity e expressed in both languages, and r + vrij \u2248 r\u2032 for those of the same relation. We deploy two translation vectors instead of one, because there are far more distinct entities than relations, and using one vector easily leads to imbalanced signals from relations.\nSuch a model obtains a cross-lingual transition of an embedding vector by adding the corresponding translation vector. Moreover, it is easy to see that veij = \u2212veji and vrij = \u2212vrji hold. Therefore, as we obtain the translation vectors from Li to Lj , we can always use the same vectors to translate in the opposite direction. Linear Transformations. The last category of alignment models deduce linear transformations between embedding spaces. Sa4 as below learns a k \u00d7 k square matrix Meij as a linear transformation on entity vectors from Li to Lj , given k as the dimensionality of the embedding spaces.\nSa4 = \u2225\u2225Meijh\u2212 h\u2032\u2225\u2225+ \u2225\u2225Meijt\u2212 t\u2032\u2225\u2225\nSa5 additionally brings in a second linear transformation Mrij for relation vectors, which is of the same shape as M e ij . The use of a different matrix is again due to different redundancy of entities and relations.\nSa5 = \u2225\u2225Meijh\u2212 h\u2032\u2225\u2225+ \u2225\u2225Mrijr\u2212 r\u2032\u2225\u2225+ \u2225\u2225Meijt\u2212 t\u2032\u2225\u2225\nUnlike axis calibration, linear-transformation-based alignment model treats cross-lingual transitions as the topological transformation of embedding spaces without assuming the similarity of spatial emergence.\nThe cross-lingual transition of a vector is obtained by applying the corresponding linear transformation. It is noteworthy that, regularization of embedding vectors in the training process (which will be introduced soon after) ensures the invertibility of the linear transformations such that Meij \u22121 = Meji and M r ij \u22121 = Mrji. Thus the transition in the revert direction is always enabled even though the model only learns the transformations of one direction."}, {"heading": "Variants of MTransE", "text": "Combining the above two component models, MTransE minimizes the following loss function\nJ = SK + \u03b1SA\nwhere \u03b1 is a hyperparameter that weights SK and SA. As we have given out five variants of the alignment model, each of which correspondingly defines its specific way of computing cross-lingual transitions of embedding vectors. We denote Vark as the variant of MTransE that adopts the k-th alignment model which employs Sak . In practice, the searching of a cross-lingual counterpart for a source is always done by querying the nearest neighbor from the result point of the cross-lingual transition. We denote function \u03c4ij that maps a cross-lingual transition of a vector from Li to Lj , or simply \u03c4 in a bilingual context. As stated, the solution in a multi-lingual scenario consists of a set of models of the same variant defined on every language pair in L2. Table 1 summarizes the model complexity, the definition of cross-lingual transitions, and the complexity of searching a cross-lingual counterpart for each variant."}, {"heading": "Training", "text": "We optimize the loss function using on-line stochastic gradient descent (Wilson and Martinez 2003). At each step, we update the parameter \u03b8 by setting \u03b8 \u2190 \u03b8 \u2212 \u03bb\u2207\u03b8J , where \u03bb is the learning rate. Instead of directly updating J , our implementation optimizes SK and \u03b1SA alternately. In detail, at each epoch we optimize \u03b8 \u2190 \u03b8 \u2212 \u03bb\u2207\u03b8SK and \u03b8 \u2190 \u03b8 \u2212 \u03bb\u2207\u03b8\u03b1SA in separated groups of steps.\nWe enforce the constraint that the l2 norm of any embedding vector is 1, thus regularize embedding vectors to a unit spherical surface. This constraint is employed in the literature (Bordes et al. 2013; Bordes, Weston, and Usunier 2014;\nJenatton et al. 2012) and has two important effects: (i) it helps avoid the case where the training process trivially minimizes the loss function by shrinking the norm of embedding vectors, and (ii) it implies the invertibility of the linear transformations (Xing et al. 2015) for Var4 and Var5.\nWe initialize vectors by drawing from a uniform distribution on the unit spherical surface, and initialize matrices using random orthogonal initialization (Saxe, McClelland, and Ganguli 2014). Negative sampling is not employed in training, which we find does not noticeably affect the results."}, {"heading": "Experiments", "text": "In this section, we evaluate the proposed methods on two cross-lingual tasks: cross-lingual entity matching, and triplewise alignment verification. We also conduct experiments on two monolingual tasks. Besides, a case study with examples of knowledge alignment is included in the attachment."}, {"heading": "Data Sets", "text": "The trilingual data used in our experiments is extracted from DBpedia, which is a knowledge base containing triples extracted from multiple language-specific versions of Wikipedia infoboxes. We started with the English (En), French (Fr), and German (De) versions of DBPedia, and constructed the monolingual knowledge graph for each language using a subset of triples under dbo:Person domain. By verifying ILLs on entities, and multilingual labels from the DBpedia ontology on some involved relations, we obtained English-French and English-German alignment for some triples. Finally, we adjusted the number of involved entities in each language to obtain two data sets shown in Table 2. Thereof, for each of the three languages, WKEFG15k matches the number of nodes (about 15,000) with FB15k\u2014the largest monolingual graph used by many recent works (Zhong et al. 2015; Lin et al. 2015; Ji et al. 2015; Jia et al. 2016; Nguyen et al. 2016), and the number of nodes in each language-specific version of WK-EFG120k is several times of that in FB15k. For both data sets, English and French graphs are of comparable density, and German graphs are much sparser. Meanwhile, we also collect extra ILLs that are not covered by the alignment sets for the evaluation of cross-lingual entity matching, whose quantity is shown in Table 3."}, {"heading": "Cross-lingual Entity Matching", "text": "The objective of this task is to match the same entities from different languages in KB . Due to the large candidate space,\nthis task emphasizes more on ranking a set of candidates rather than acquiring the best answer. We perform this task on both data sets to compare five variants of MTransE. Evaluation Protocol. Each model is trained on a complete data set, while the extra ILLs are used as ground truth for test. We take these unidirectional links between EnglishFrench and English-German, i.e., four directions in total. For each ILL (e, e\u2032), we perform a kNN search from the cross-lingual transition point of e (i.e., \u03c4(e)) and record the rank of e\u2032. Following the convention, we aggregate two metrics over all test cases, i.e., the proportion of ranks no larger than 10 Hits@10 (in percentage), and the mean rank Mean . Thereof, we prefer higher Hits@10 and lower Mean that indicate a better outcome.\nFor training, we select the learning rate \u03bb among {0.001, 0.01, 0.1}, \u03b1 among {1, 2.5, 5, 7.5}, l1 or l2 norm in loss functions, and dimensionality k among {50, 75, 100, 125}. The best configuration on WK-EFG15k is \u03bb = 0.01, \u03b1 = 5, k = 75, l1 norm for Var1 and Var2, l2 norm for other variants, while the best configuration on WK-EFG120k is \u03bb = 0.01, \u03b1 = 5, k = 100, and l2 norm for all variants. The training on both data sets takes 400 epochs for all variants. Results. We report Hits@10 and Mean for WK-EFG15k, and Hits@10 for WK-EFG120k, on the four involved directions of cross-lingual matching in Table 4. Above all, Var4 and Var5 outperform the other three variants under all settings. The fairly close results obtained by these two variants indicate that the interference caused by learning an additional relation-dedicated transformation in Var5 is negligible to the entity-dedicated transformation. Correspondingly, we believe that the reason for Var3 to be outperformed by Var4 and Var5 is that it fails to differentiate well the over-frequent cross-lingual alignment from regular relations. Therefore, the characterization for cross-lingual alignment is negatively affected by the learning process for monolingual relations in a visible degree. Axis calibration appears to be unstable on this task. We hypothesize that this simple technique is affected by two factors: coherence between language-specific versions, and density of the graphs. Var2 is left aside. The fact that it is always outperformed by Var1 shows the neg-\native effect of the calibration based on relations. We believe it is because complex relations are not so well-captured by TransE as explained in (Wang et al. 2014), therefore disturb the calibration of the entire embedding spaces. Although Var1 still visibly precedes Var3 on entity matching between English and French graphs in WK-EFG15k, coherence somewhat drops alongside when scaling up to the larger data set so as to hinder the calibration. The German graphs are sparse, thus should have set a barrier for precisely constructing embedding vectors and hindered calibration on the other side. Therefore Var1 still performs closely to Var3 in the English-German task on WK-EFG15k and EnglishFrench task on WK-EFG120k, but is preceded by Var3 in the last setting. In general, the variants that use linear transformations are the most desired. This conclusion is supported by their promising outcome on this task, and it is also reflected in the precision-recall curves shown in Figure 1."}, {"heading": "Triple-wise Alignment Verification", "text": "This task is to verify whether a given pair of aligned triples are truly cross-lingual counterparts. It produces a classifier that helps with verifying candidates of triple matching (Nguyen et al. 2011; Rinser et al. 2013). Evaluation Protocol. We create positive cases by isolating 20% of the alignment set. Similar to (Socher et al. 2013), we randomly corrupt positive cases to generate negative cases. In detail, given a pair of correctly aligned triples (T, T \u2032), it is corrupted by (i) randomly replacing one of the six elements in the two triples with another element from the same language, or (ii) randomly substituting either T or T \u2032 with another triple from the same language. Cases (i) and (ii) respectively contribute negative cases that are as many as 100% and 50% of positive cases. We use 10-fold crossvalidation on these cases to train and evaluate the classifier.\nWe use a simple threshold-based classifier similar to the widely-used ones for triple classification (Socher et al. 2013; Wang et al. 2014; Lin et al. 2015). For a given pair of aligned triples (T, T \u2032) = ( (h, r, t), (h\u2032, r\u2032, t\u2032) ) , the dissimilarity function is defined as fd(T, T \u2032) = \u2016\u03c4(h)\u2212 h\u2032\u20162 + \u2016\u03c4(r)\u2212 r\u2032\u20162 + \u2016\u03c4(t)\u2212 t\u2032\u20162. The classifier finds a thresh-\nTable 5: Accuracy of triple-wise alignment verification (%).\nData Set WK-EFG15k WK-EFG120k Aligned\nEn&Fr En&De En&Fr En&De Languages\nVar1 93.25 91.24 91.27 91.35 Var2 90.24 86.59 89.36 86.29 Var3 90.38 84.24 87.99 87.04 Var4 94.58 95.03 93.48 93.06 Var5 94.90 94.95 92.63 93.66\nTable 7: Results of relation prediction (Hits@10).\nData Set WK-EFG15k WK-EFG120k Language En Fr En Fr\nTransE 61.79 62.55 60.06 65.29 Var1 60.18 60.73 61.75 65.46 Var2 54.33 62.98 61.11 61.47 Var3 58.32 59.44 60.14 48.06 Var4 63.74 64.77 60.26 67.64 Var5 64.79 63.71 60.77 66.86\nold \u03c3 such that fd < \u03c3 implies positive, otherwise negative. The value of \u03c3 is determined by maximizing the accuracy for each fold on the training set. Such a simple classification rule adequately relies on how precisely each model represents cross-lingual transitions for both entities and relations.\nWe carry forward the corresponding configuration from the last experiment, just to show the performance of each variant under controlled variables. Results. The mean accuracy is shown in Table 5, with a standard deviation below 0.009 in cross-validation for all settings. Thus, the results are statistically sufficient to reflect the performance of classifiers. Note that the results appear to be better than those of the previous task since this is a binary classification problem. Intuitively, the linear-transformationbased methods perform steadily and take the lead on all settings. We also observe that Var5, though learns an additional relation-dedicated transformation, still performs considerably close to Var4 (the difference is at most 0.85%). The simple Var1 is the runner-up to these two and has the gap between 1.65% and 3.79% to the optimal solutions. However the relation-dedicated calibration in Var2 causes a notable setback (4.12% to 8.44% from the optimal solutions). The performance of Var3 falls behind slightly more than Var2 (4.52% to 10.79% from the optimal solutions) due to the failure in distinguishing cross-lingual alignment from regular relations. Meanwhile, we single out the accuracy on the portion of negative cases where only the relation is corrupted for English-French in WK-EFG15k. The five variants receive 97.73%, 93.78%, 82.34%, 98.57%, and 98.54% respectively. The close accuracy of Var4 and Var5 indicates that the only linear transformation learnt from entities in Var4 is enough to substitute the relation-dedicated transformation in Var5 for discriminating relation alignment, while learning the additional transformation in Var5 does not notably interfere the original one. However, it applies differently to axis calibration since Var2 does not improve but actually impairs the cross-lingual transitions for relations. Hence, the linear-transformation-based variants prove to be most successful in verifying triple-wise alignment as well."}, {"heading": "Monolingual Tasks", "text": "The above experiments have shown the strong capability of MTransE in handling cross-lingual tasks. Now we report the results on comparing MTransE with its monolingual counterpart TransE on two monolingual tasks introduced in the literature (Bordes et al. 2013; Bordes et al. 2014), namely tail prediction (predicting t given h and r) and relation prediction (predicting r given h and t), us-\ning the English and French versions of our data sets. Like previous works (Bordes et al. 2013; Wang et al. 2014; Jia et al. 2016), for each language version, 10% triples are selected as the test set, and the remaining becomes the training set. Each MTransE variant is trained upon both language versions of the training set for the knowledge model, while the intersection between the alignment set and the training set is used for the alignment models. TransE is trained on either language version of the training set. Again, we use the configuration from the previous experiment. Results. The results for Hits@10 are reported in Tables 6 and 7. They imply that MTransE preserves well the characterization of monolingual knowledge. For each setting, Var1, Var4, and Var5 perform at least as well as TransE, and some even outperforms TransE under certain settings. This signifies that the alignment model does not interfere much with the knowledge model in characterizing monolingual relations, but might have actually strengthened it since coherent portions of knowledge are unified by the alignment model. Since such coherence is currently not measured, this question is left as a future work. The other question that deserves further attention is, how other knowledge models involving relation-specific entity transformations (Wang et al. 2014; Lin et al. 2015; Ji et al. 2015; Jia et al. 2016; Nguyen et al. 2016) may influence monolingual and cross-lingual tasks."}, {"heading": "Conclusion and Future Work", "text": "At the best of our knowledge, this paper is the first work that generalizes knowledge graph embeddings to the multilingual scenario. Our model MTransE characterizes monolingual relations and compares three different techniques to learn cross-lingual alignment for entities and relations. Extensive experiments on the tasks of cross-lingual entity matching and triple alignment verification show that the technique based on linear transformations is the best among the three. Moreover, we show that MTransE preserves the key properties of monolingual knowledge graph embeddings on monolingual tasks.\nThe results here are very encouraging, but we also point out opportunities for further work and improvements. In particular, we should explore how to substitute the simple loss function of the knowledge model used in MTransE with more advanced ones involving relation-specific entity transformations. More sophisticated tasks of cross-lingual triple completion may also be conducted. Combining MTransE with multilingual word embeddings (Xing et al. 2015) is another meaningful direction since it will provide a useful tool to extract new relations from multilingual text corpora."}, {"heading": "Appendix", "text": ""}, {"heading": "Examples of Knowledge Alignment", "text": "We have already shown the effectiveness of MTransE in aligning cross-lingual knowledge, especially the lineartransformation-based variants Var4 and Var5. Now we discuss several examples to reveal insights on how our methods may be used in cross-lingual knowledge augmentation.\nWe start with the search of cross-lingual counterparts of entities and relations. We choose an entity (or relation) in English and then show the nearest candidates in French and German, respectively. These candidates are listed by decreasing values of the Euclidean distance between their vectors in the target language space and the result point of cross-lingual transition. Several examples are shown in Table 8 and Table 9. In all tables of this subsection, we mark the exact answers as boldfaced, and the conceptually close ones as italic. For example, in Table 8, besides boldfacing the exactly correct answers for Barack Obama and Paris, we consider those who have also been U.S. presidents as conceptually close to Barack Obama, and European cities other than Paris as conceptually close to Paris. Also, in Table 9, those French and German relations that have the meaning of settlements of significance are considered as conceptually close to capital.\nWe then move on to the more complicated cross-lingual triple completion task. We construct queries by replacing one element in an English triple with a question mark, for which we seek for answers in another language. Our methods need to transfer the remaining elements to the space of the target language and pick the best answer for the missing element. Table 10 shows some query answers. It is noteworthy that the basic queries are already useful for aided\ncross-lingual augmentation of knowledge. However, developing a joint model to support complex queries on multilingual knowledge graphs based on MTransE generated features appears to be a promising future work to support Q&A on multilingual knowledge bases.\nFigure 2 shows the PCA projection of the same six English entities in their original English space and in French space after transformation. We can observe that the vectors of English entities show certain structures, where the U.S. cities are grouped together and other countries\u2019 cities are well separated. After transformation into French space, these English entities not only keep their original spatial emergence, but also are close to their corresponding entities in French. This illustrates the transformation preserves monolingual structure and also it is able to capture cross-lingual information. We believe this example illustrates the good performance we have demonstrated in cross-lingual tasks including cross-lingual entity matching and triple-wise alignment verification."}], "references": [{"title": "Linking and extending an open multilingual Wordnet", "author": ["Bond", "F. Foster 2013] Bond", "R. Foster"], "venue": null, "citeRegEx": "Bond et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bond et al\\.", "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2011\\E", "shortCiteRegEx": "Bordes", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2012\\E", "shortCiteRegEx": "Bordes", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In NIPS,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data. Machine Learning 94(2):233\u2013259", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2014\\E", "shortCiteRegEx": "Bordes", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Weston Bordes", "A. Usunier 2014] Bordes", "J. Weston", "N. Usunier"], "venue": "ECML-PKDD,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "R. Weston 2008] Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Dependency tree kernels for relation extraction", "author": ["Culotta", "A. Sorensen 2004] Culotta", "J. Sorensen"], "venue": null, "citeRegEx": "Culotta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2004}, {"title": "Menta: Inducing multilingual taxonomies from Wikipedia", "author": ["de Melo", "G. Weikum 2010] de Melo", "G. Weikum"], "venue": null, "citeRegEx": "Melo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2010}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Bengio Gouws", "S. Corrado 2015] Gouws", "Y. Bengio", "G. Corrado"], "venue": "In ICML,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "A latent factor model for highly multi-relational data", "author": ["Jenatton"], "venue": null, "citeRegEx": "Jenatton,? \\Q2012\\E", "shortCiteRegEx": "Jenatton", "year": 2012}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "author": ["Ji"], "venue": null, "citeRegEx": "Ji,? \\Q2015\\E", "shortCiteRegEx": "Ji", "year": 2015}, {"title": "Locally adaptive translation for knowledge graph embedding", "author": ["Jia"], "venue": null, "citeRegEx": "Jia,? \\Q2016\\E", "shortCiteRegEx": "Jia", "year": 2016}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["Jiang", "J. Zhai 2007] Jiang", "C. Zhai"], "venue": "In NAACL HLT,", "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Lauly"], "venue": "In NIPS,", "citeRegEx": "Lauly,? \\Q2014\\E", "shortCiteRegEx": "Lauly", "year": 2014}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": null, "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Yago3: A knowledge base from multilingual Wikipedias", "author": ["Biega Mahdisoltani", "F. Suchanek 2015] Mahdisoltani", "J. Biega", "F. Suchanek"], "venue": "In CIDR", "citeRegEx": "Mahdisoltani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahdisoltani et al\\.", "year": 2015}, {"title": "Text-mining, structured queries, and knowledge management on web document corpora", "author": ["Mousavi"], "venue": null, "citeRegEx": "Mousavi,? \\Q2014\\E", "shortCiteRegEx": "Mousavi", "year": 2014}, {"title": "Multilingual schema matching for Wikipedia infoboxes", "author": ["Nguyen"], "venue": null, "citeRegEx": "Nguyen,? \\Q2011\\E", "shortCiteRegEx": "Nguyen", "year": 2011}, {"title": "Stranse: a novel embedding model of entities and relationships in knowledge bases", "author": ["Nguyen"], "venue": "In NAACL HLT,", "citeRegEx": "Nguyen,? \\Q2016\\E", "shortCiteRegEx": "Nguyen", "year": 2016}, {"title": "Cross-lingual entity matching and infobox alignment in Wikipedia. Information Systems 38(6):887\u2013907", "author": ["Rinser"], "venue": null, "citeRegEx": "Rinser,? \\Q2013\\E", "shortCiteRegEx": "Rinser", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. ICLR", "author": ["McClelland Saxe", "A.M. Ganguli 2014] Saxe", "J.L. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher"], "venue": null, "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Probabilistic alignment of relations, instances, and schema", "author": ["Suchanek"], "venue": null, "citeRegEx": "Suchanek,? \\Q2011\\E", "shortCiteRegEx": "Suchanek", "year": 2011}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Grishman Sun", "A. Sekine 2011] Sun", "R. Grishman", "S. Sekine"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Wikidata: a free collaborative knowledge base", "author": ["Vrande\u010di\u0107", "D. Kr\u00f6tzsch 2014] Vrande\u010di\u0107", "M. Kr\u00f6tzsch"], "venue": null, "citeRegEx": "Vrande\u010di\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vrande\u010di\u0107 et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Weston"], "venue": null, "citeRegEx": "Weston,? \\Q2013\\E", "shortCiteRegEx": "Weston", "year": 2013}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["Wilson", "D.R. Martinez 2003] Wilson", "T.R. Martinez"], "venue": "Neural Networks", "citeRegEx": "Wilson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2003}, {"title": "Normalized word embedding and orthogonal trans", "author": ["Xing"], "venue": null, "citeRegEx": "Xing,? \\Q2015\\E", "shortCiteRegEx": "Xing", "year": 2015}, {"title": "Unsupervised relation extraction by mining wikipedia texts using information from the web", "author": ["Yan"], "venue": null, "citeRegEx": "Yan,? \\Q2009\\E", "shortCiteRegEx": "Yan", "year": 2009}, {"title": "Network representation learning with rich text information", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["Zhong"], "venue": null, "citeRegEx": "Zhong,? \\Q2015\\E", "shortCiteRegEx": "Zhong", "year": 2015}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2005\\E", "shortCiteRegEx": "Zhou", "year": 2005}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou"], "venue": null, "citeRegEx": "Zou,? \\Q2013\\E", "shortCiteRegEx": "Zou", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Now we report the results on comparing MTransE with its monolingual counterpart TransE on two monolingual tasks introduced in the literature (Bordes et al. 2013; Bordes et al. 2014), namely tail prediction (predicting t given h and r) and relation prediction (predicting r given h and t), using the English and French versions of our data sets.", "startOffset": 141, "endOffset": 181}], "year": 2017, "abstractText": "Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable cross-lingual alignment by human labor is very costly and error-prone. Thus, we propose MTransE, a translationbased model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE.", "creator": "LaTeX with hyperref package"}}}