{"id": "1611.04088", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2016", "title": "Batched Gaussian Process Bandit Optimization via Determinantal Point Processes", "abstract": "gaussian process bandit optimization however has recently emerged as practically a powerful tool for optimizing noisy black box functions. one example arising in machine learning is automatic hyper - parameter optimization where each evaluation of the target function requires training a rough model which may involve days or even weeks of computation. most methods for this so - called \" bayesian optimization \" only allow sequential exploration of the parameter space. however, it obviously is often desirable to propose batches or sets of parameter mapping values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. batch methods require modeling the dynamic interaction between the different evaluations in the batch, which can be expensive in complex scenarios. developing in this paper, we propose a exciting new approach for parallelizing bayesian optimization by modeling accelerating the diversity of a batch via determinantal multiple point processes ( dpps ) whose kernels are learned automatically. this application allows us to generalize a previous result as well well as prove better regret bounds based on dpp sampling. lately our experiments project on a variety genres of 2d synthetic and analytic real - world robotics and simulate hyper - parameter optimization tasks indicate that our older dpp - based methods, especially those based on dpp sampling, outperform state - of - the - art computation methods.", "histories": [["v1", "Sun, 13 Nov 2016 05:52:58 GMT  (1044kb,D)", "http://arxiv.org/abs/1611.04088v1", "To appear at NIPS 2016"]], "COMMENTS": "To appear at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tarun kathuria", "amit deshpande", "pushmeet kohli"], "accepted": true, "id": "1611.04088"}, "pdf": {"name": "1611.04088.pdf", "metadata": {"source": "CRF", "title": "Batched Gaussian Process Bandit Optimization via Determinantal Point Processes", "authors": ["Tarun Kathuria", "Amit Deshpande", "Pushmeet Kohli"], "emails": ["pkohli}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The optimization of an unknown function based on noisy observations is a fundamental problem in various real world domains, e.g., engineering design [34], finance [37] and hyper-parameter optimization [30]. In recent years, an increasingly popular direction has been to model smoothness assumptions about the function via a Gaussian Process (GP), which provides an easy way to compute the posterior distribution of the unknown function, and thereby uncertainty estimates that help to decide where to evaluate the function next, in search of an optima. This Bayesian optimization (BO) framework has received considerable attention in tuning of hyper-parameters for complex models and algorithms in Machine Learning, Robotics and Computer Vision [16, 32, 30, 12].\nApart from a few notable exceptions [9, 8, 11], most methods for Bayesian optimization work by exploring one parameter value at a time. However, in many applications, it may be possible and, moreover, desirable to run multiple function evaluations in parallel. A case in point is when the underlying function corresponds to a laboratory experiment where multiple experimental setups are available or when the underlying function is the result of a costly computer simulation and multiple simulations can be run across different processors in parallel. By parallelizing the experiments, substantially more information can be gathered in the same time-frame; however, future actions must be chosen without the benefit of intermediate results. One might conceptualize these problems as choosing \u201cbatches\u201d of experiments to run simultaneously. The key challenge is to assemble batches (out of a combinatorially large set of batches) of experiments that both explore the function and exploit by focusing on regions with high estimated value."}, {"heading": "1.1 Our Contributions", "text": "Given that functions sampled from GPs usually have some degree of smoothness, in the so-called batch Bayesian optimization (BBO) methods, it is desirable to choose batches which are diverse. Indeed, this is the motivation behind many popular BBO methods like the BUCB [9], UCB-PE [8] and Local Penalization\nar X\niv :1\n61 1.\n04 08\n8v 1\n[ cs\n.L G\n] 1\n3 N\nov 2\n[11]. Motivated by this long line of work in BBO, we propose a new approach that employs Determinantal Point Processes (DPPs) to select diverse batches of evaluations. DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory [29, 22], and have efficient sampling algorithms [18, 19]. The two main ways for fixed cardinality subset selection via DPPs are that of choosing the subset which maximizes the determinant [DPP-MAX, Theorem 3.3] and sampling a subset according to the determinantal probability measure [DPP-SAMPLE, Theorem 3.4]. Following UCB-PE [8], our methods also choose the first point via an acquisition function, and then the rest of the points are selected from a relevance region using a DPP. Since DPPs crucially depend on the choice of the DPP kernel, it is important to choose the right kernel. Our method allows the kernel to change across iterations and automatically compute it based on the observed data. This kernel is intimately linked to the GP kernel used to model the function; it is in fact exactly the posterior kernel function of the GP. The acquisition functions we consider are EST [35], a recently proposed sequential MAP-estimate based Bayesian optimization algorithm with regret bounds independent of the size of the domain, and UCB [31]. In fact, we show that UCB-PE can be cast into our framework as just being DPP-MAX where the maximization is done via a greedy selection rule.\nGiven that DPP-MAX is too greedy, it may be desirable to allow for uncertainty in the observations. Thus, we define DPP-SAMPLE which selects the batches via sampling subsets from DPPs, and show that the expected regret is smaller than that of DPP-MAX. To provide a fair comparison with an existing method, BUCB, we also derive regret bounds for B-EST [Theorem 3.2]. Finally, for all methods with known regret bounds, the key quantity is the information gain. In the appendix, we also provide a simpler proof of the information gain for the widely-used RBF kernel which also improves the bound from O((log T )d+1) [27, 31] to O((log T )d). We conclude with experiments on synthetic and real-world robotics and hyper-parameter optimization for extreme multi-label classification tasks which demonstrate that our DPP-based methods, especially the sampling based ones are superior or competitive with the existing baselines."}, {"heading": "1.2 Related Work", "text": "One of the key tasks involved in black box optimization is of choosing actions that both explore the function and exploit our knowledge about likely high reward regions in the function\u2019s domain. This exploration-exploitation trade-off becomes especially important when the function is expensive to evaluate. This exploration-exploitation trade off naturally leads to modeling this problem in the multi-armed bandit paradigm [26], where the goal is to maximize cumulative reward by optimally balancing this trade-off. Srinivas et al. [31] analyzed the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method [3] to achieve the first sub-linear regret bounds for Gaussian process bandit optimization. These bounds however grow logarithmically in the size of the (finite) search space.\nRecent work by Wang et al. [35] considered an intuitive MAP-estimate based strategy (EST) which involves estimating the maximum value of a function and choosing a point which has maximum probability of achieving this maximum value. They derive regret bounds for this strategy and show that the bounds are actually independent of the size of the search space. The problem setting for both UCB and EST is of optimizing a particular acquisition function. Other popular acquisition functions include expected improvement (EI), probability of improvement over a certain threshold (PI). Along with these, there is also work on Entropy search (ES) [13] and its variant, predictive entropy search (PES) [14] which instead aims at minimizing the uncertainty about the location of the optimum of the function. All the fore-mentioned methods, though, are inherently sequential in nature.\nThe BUCB and UCB-PE both depend on the crucial observation that the variance of the posterior distribution does not depend on the actual values of the function at the selected points. They exploit this fact by \u201challucinating\u201d the function values to be as predicted by the posterior mean. The BUCB algorithm chooses the batch by sequentially selecting the points with the maximum UCB score keeping the mean function the same and only updating the variance. The problem with this naive approach is that it is too \u201coverconfident\u201d of the observations which causes the confidence bounds on the function values to shrink very quickly as we go deeper into the batch. This is fixed by a careful initialization and expanding the confidence bounds which leads to regret bounds which are worse than that of UCB by some multiplicative factor (independent of T and B). The UCB-PE algorithm chooses the first point of the batch via the UCB score and then defines a \u201crelevance region\u201d and selects the remaining points from this region greedily to maximize the information\ngain, in order to focus on pure exploration (PE). This algorithm does not require any initialization like the BUCB and, in fact, achieves better regret bounds than the BUCB.\nBoth BUCB and UCB-PE, however, are too greedy in their selection of batches which may be really far from the optimal due to our \u201cimmediate overconfidence\u201d of the values. Indeed this is the criticism of these two methods by a recently proposed BBO strategy PPES [28], which parallelizes predictive entropy search based methods and shows considerable improvements over the BUCB and UCB-PE methods. Another recently proposed method is the Local Penalization (LP) [11], which assumes that the function is Lipschitz continuous and tries to estimate the Lipschitz constant. Since assumptions of Lipschitz continuity naturally allow one to place bounds on how far the optimum of f is from a certain location, they work to smoothly reduce the value of the acquisition function in a neighborhood of any point reflecting the belief about the distance of this point to the maxima. However, assumptions of Lipschitzness are too coarse-grained and it is unclear how their method to estimate the Lipschitz constant and modelling of local penalization affects the performance from a theoretical standpoint. Our algorithms, in constrast, are general and do not assume anything about the function other than it being drawn from a Gaussian Process."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Gaussian Process Bandit Optimization", "text": "We address the problem of finding, in the lowest possible number of iterations, the maximum (m) of an unknown function f : X \u2192 R where X \u2282 Rd, i.e.,\nm = f(x\u2217) = max x\u2208X f(x).\nWe consider the domain to be discrete as it is well-known how to obtain regret bounds for continous, compact domains via suitable discretizations [31]. At each iteration t, we choose a batch {xt,b}1\u2264b\u2264B of B points and then simultaneously observe the noisy values taken by f at these points, yt,b = f(xt,b) + t,b, where t,k is i.i.d. Gaussian noise N (0, \u03c32). The function is assumed to be drawn from a Gaussian process (GP), i.e., f \u223c GP (0, k), where k : X 2 \u2192 R+ is the kernel function. Given the observations Dt = {(x\u03c4 , y\u03c4 )t\u03c4=1} up to time t, we obtain the posterior mean and covariance functions [25] via the kernel matrix Kt = [k(xi, xj)]xi,xj\u2208Dt and kt(x) = [k(xi, x)]xi\u2208Dt : \u00b5t(x) = kt(x) T (Kt + \u03c3 2I)\u22121yt and kt(x, x \u2032) = k(x, x\u2032)\u2212 kt(x)T (Kt + \u03c32I)\u22121kt(x\u2032). The posterior variance is given by \u03c32t (x) = kt(x, x). Define the Upper Confidence Bound (UCB) f + and Lower Confidence Bound (LCB) f\u2212 as\nf+t (x) = \u00b5t\u22121(x) + \u03b2 1/2 t \u03c3t\u22121(x) f \u2212 t (x) = \u00b5t\u22121(x)\u2212 \u03b2 1/2 t \u03c3t\u22121(x)\nA crucial observation made in BUCB [9] and UCB-PE [8] is that the posterior covariance and variance functions do not depend on the actual function values at the set of points. The EST algorithm in [35] chooses at each timestep t,the point which has the maximum posterior probability of attaining the maximum value m, i.e., the arg maxx\u2208X Pr(Mx|m,Dt) where Mx is the event that point x achieves the maximum value. This turns out to be equal to arg minx\u2208X [ (m\u2212 \u00b5t(x))/\u03c3t(x) ] . Note that this actually depends on the value of m which, in most cases, is unknown. [35] get around this by using an approximation m\u0302 which, under certain conditions specified in their paper, is an upper bound on m. They provide two ways to get the estimate m\u0302, namely ESTa and ESTn. We refer the reader to [35] for details of the two estimates and refer to ESTa as EST.\nAssuming that the horizon T is unknown, a strategy has to be good at any iteration. Let rt,b denote the simple regret, the difference between the value of the maxima and the point queried xt,k, i.e., rt,b = maxx\u2208X f(x)\u2212 f(xt,b). While, UCB-PE aims at minimizing a batched cumulative regret, in this paper we will focus on the standard full cumulative regret defined as RTB = \u2211T t=1 \u2211B b=1 rt,b. This models the case where all the queries in a batch should have low regret. The key quantity controlling the regret bounds of all known BO algorithms is the maximum mutual information that can be gained about f from T measurements : \u03b3T = maxA\u2286X ,|A|\u2264T I(yA, fA) = maxA\u2286X ,|A|\u2264T 1 2 log det(I + \u03c3\n\u22122KA), where KA is the (square) submatrix of K formed by picking the row and column indices corresponding to the set A. The regret for both the UCB and the EST algorithms are presented in the following theorem which is a combination of Theorem 1 in [31] and Theorem 3.1 in [35].\nAlgorithm 1 GP-BUCB/B-EST Algorithm\nInput: Decision set X , GP prior \u00b50, \u03c30, kernel function k(\u00b7, \u00b7), feedback mapping fb[\u00b7] for t = 1 to TB do\nChoose \u03b2 1/2 t =\n{ C \u2032 [ 2 log(|X |\u03c02t2/6)\u03b4 ] for BUCB\nC \u2032 [ minx\u2208X (m\u0302\u2212 \u00b5fb[t])/\u03c3t\u22121(x) ] for B-EST\nChoose xt = arg maxx\u2208X [\u00b5fb[t](x) + \u03b2 1/2 t \u03c3t\u22121(x)] and compute \u03c3t(\u00b7) if fb[t] < fb[t+ 1] then Obtain yt\u2032 = f(xt\u2032) + t\u2032 for t\n\u2032 \u2208 {fb[t] + 1, . . . , fb[t+ 1]} and compute \u00b5fb[t+1](\u00b7) end if\nend for return arg max\nt=1...TB yt\nTheorem 2.1. Let C = 2/ log(1 + \u03c3\u22122) and fix \u03b4 > 0. For UCB, choose \u03b2t = 2 log(|X |t2\u03c02/6\u03b4) and for EST, choose \u03b2t = (minx\u2208X\nm\u0302\u2212\u00b5t\u22121(x) \u03c3t\u22121(x) )2 and \u03b6t = 2 log(\u03c0 2t2/\u03b4). With probability 1\u2212 \u03b4, the cumulative regret\nup to any time step T can be bounded as\nRT = T\u2211 t=1 rt \u2264 {\u221a CT\u03b2T \u03b3T for UCB\u221a CT\u03b3T (\u03b2 1/2 t\u2217 + \u03b6 1/2 T ) for EST where t\u2217 = arg max t \u03b2t."}, {"heading": "2.2 Determinantal Point Processes", "text": "Given a DPP kernel K \u2208 Rm\u00d7m of m elements {1, . . . ,m}, the k-DPP distribution defined on 2Y is defined as picking B, a k-subset of [m] with probability proportional to det(KB). Formally,\nPr(B) = det(KB)\u2211 |S|=k det(KS)\nThe problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18]. The maximization problem in general is NP-hard and furthermore, has a hardness of approximation result of 1/ck for some c > 1. The best known approximation algorithm is by [23] with a factor of 1/ek, which almost matches the lower bound. Their algorithm however is a complicated and expensive convex program. A simple greedy algorithm on the other hand gives a 1/2k log(k)-approximation. For sampling from k-DPPs, an exact sampling algorithm exists due to [10]. This, however, does not scale to large datasets. A recently proposed alternative is an MCMC based method by [1] which is much faster."}, {"heading": "3 Main Results", "text": "In this section, we present our DPP-based algorithms. For a fair comparison of the various methods, we first prove the regret bounds of the EST version of BUCB, i.e., B-EST. We then show the equivalence between UCB-PE and UCB-DPP maximization along with showing regret bounds for the EST version of PE/DPP-MAX. We then present the DPP sampling (DPP-SAMPLE) based methods for UCB and EST and provide regret bounds. In Appendix 4, while borrowing ideas from [27], we provide a simpler proof with improved bounds on the maximum information gain for the RBF kernel."}, {"heading": "3.1 The Batched-EST algorithm", "text": "The BUCB has a feedback mapping fb which indicates that at any given time t (just in this case we will mean a total of TB timesteps), the iteration upto which the actual function values are available. In the batched setting, this is just b(t\u2212 1)/BcB. The BUCB and B-EST, its EST variant algorithms are presented in Algorithm 1. The algorithm mainly comes from the observation made in [35] that the point chosen by EST is the same as a variant of UCB. This is presented in the following lemma.\nLemma 3.1. (Lemma 2.1 in [35]) At any timestep t, the point selected by EST is the same as the point selected by a variant of UCB with \u03b2 1/2 t = minx\u2208X (m\u0302\u2212 \u00b5t\u22121(x))/\u03c3t\u22121(x).\nThis will be sufficient to get to B-EST as well by just running BUCB with the \u03b2t as defined in Lemma 3.1 and is also provided in Algorithm 1. In the algorithm, C \u2032 is chosen to be exp(2C), where C is an upper bound on the maximum conditional mutual information I(f(x); yfb[t]+1:t\u22121|y1:fb[t]) (refer to [9] for details). The problem with naively using this algorithm is that the value of C \u2032, and correspondingly the regret bounds, usually has at least linear growth in B. This is corrected in [9] by two-stage BUCB which first chooses an initial batch of size T init by greedily choosing points based on the (updated) posterior variances. The values are then obtained and the posterior GP is calculated which is used as the prior GP in Algorithm 1. The C \u2032 value can then be chosen independent of B. We refer the reader to the Table 1 in [9] for values of C \u2032 and T init for common kernels. Finally, the regret bounds of B-EST are presented in the next theorem.\nTheorem 3.2. Choose \u03b1t = ( minx\u2208X m\u0302\u2212\u00b5fb[t](x) \u03c3t\u22121(x) )2 and \u03b2t = (C \u2032)2\u03b1t, B \u2265 2, \u03b4 > 0 and the C \u2032 and T init values are chosen according to Table 1 in [9]. At any timestep T , let RT be the cumulative regret of the two-stage initialized B-EST algorithm. Then\nPr{RT \u2264 C \u2032RseqT + 2\u2016f\u2016\u221eT init,\u2200T \u2265 1} \u2265 1\u2212 \u03b4\nProof. The proof is presented in Appendix 1."}, {"heading": "3.2 Equivalence of Pure Exploration (PE) and DPP Maximization", "text": "We now present the equivalence between the Pure Exploration and a procedure which involves DPP maximization based on the Greedy algorithm. For the next two sections, by an iteration, we mean all B points selected in that iteration and thus, \u00b5t\u22121 and kt\u22121 are computed using (t\u2212 1)B observations that are available to us. We first describe a generic framework for BBO inspired by UCB-PE : At any iteration, the first point is chosen by selecting the one which maximizes UCB or EST which can be seen as a variant of UCB as per Lemma 3.1. A relevance region R+t is defined which contains arg maxx\u2208X f+t+1(x) with high probability. Let y\u2022t = f \u2212 t (x \u2022 t ), where x \u2022 t = arg maxx\u2208X f \u2212 t (x). The relevance region is formally defined as\nR+t = {x \u2208 X |\u00b5t\u22121 + 2 \u221a \u03b2t+1\u03c3t\u22121(x) \u2265 y\u2022t }. The intuition for considering this region is that using R+t guarantees that the queries at iteration t will leave an impact on the future choices at iteration t+ 1. The next B \u2212 1 points for the batch are then chosen from R+t , according to some rule. In the special case of UCB-PE, the B \u2212 1 points are selected greedily from R+t by maximizing the (updated) posterior variance, while keeping the mean function the same. Now, at the tth iteration, consider the posterior kernel function after xt,1 has been chosen (say kt,1) and consider the kernel matrix Kt,1 = I + \u03c3\n\u22122[kt,1(pi, pj)]i,j over the points pi \u2208 R+t . We will consider this as our DPP kernel at iteration t. Two possible ways of choosing B \u2212 1 points via this DPP kernel is to either choose the subset of size B \u2212 1 of maximum determinant (DPP-MAX) or sample a set from a (B \u2212 1)-DPP using this kernel (DPP-SAMPLE). In this subsection, we focus on the maximization problem. The proof of the regret bounds of UCB-PE go through a few steps but in one of the intermediate steps (Lemma 5 of [8]), it is shown that the sum of regrets over a batch at an iteration t is upper bounded as\nB\u2211 b=1 rt,b \u2264 B\u2211 b=1 (\u03c3t,b(xt,b)) 2 \u2264 B\u2211 b=1 C2\u03c3 2 log(1 + \u03c3\u22122\u03c3t,b(xt,b)) = C2\u03c3 2 log [ B\u220f b=1 (1 + \u03c3\u22122\u03c3t,b(xt,b) ] where C2 = \u03c3\n\u22122/ log(1 + \u03c3\u22122). From the final log-product term, it can be seen (from Schur\u2019s determinant identity [5] and the definition of \u03c3t,b(xt,b)) that the product of the last B \u2212 1 terms is exactly the B \u2212 1 principal minor of Kt,1 formed by the indices corresponding to S = {xt,b}Bb=2. Thus, it is straightforward to see that the UCB-PE algorithm is really just (B \u2212 1)-DPP maximization via the greedy algorithm. This connection will also be useful in the next subsection for DPP-SAMPLE. Thus, \u2211B b=1 rt,b \u2264 C2\u03c3 2 [ log(1 +\n\u03c3\u22122\u03c3t,1(xt,1)) + log det((Kt,1)S) ] . Finally, for EST-PE, the proof proceeds like in the B-EST case by realising\nthat EST is just UCB with an adaptive \u03b2t. The final algorithm (along with its sampling counterpart; details in the next subsection) is presented in Algorithm 2. The procedure kDPPMaxGreedy(K, k) picks a principal submatrix of K of size k by the greedy algorithm. Finally, we have the theorem for the regret bounds for (UCB/EST)-DPP-MAX.\nAlgorithm 2 GP-(UCB/EST)-DPP-(MAX/SAMPLE) Algorithm\nInput: Decision set X , GP prior \u00b50, \u03c30, kernel function k(\u00b7, \u00b7) for t = 1 to T do\nCompute \u00b5t\u22121 and \u03c3t\u22121 according to Bayesian inference.\nChoose \u03b2 1/2 t =\n{[ 2 log(|X |\u03c02t2/6)\u03b4 ] for UCB[\nminx\u2208X (m\u0302\u2212 \u00b5fb[t])/\u03c3t\u22121(x) ] for EST\nxt,1 \u2190 arg maxx\u2208X \u00b5t\u22121(x) + \u221a \u03b2t\u03c3t\u22121(x) Compute R+t and construct the DPP kernel Kt,1\n{xt,b}Bb=2 \u2190 { kDPPMaxGreedy(Kt,1, B \u2212 1) for DPP-MAX kDPPSample(Kt,1, B \u2212 1) for DPP-SAMPLE\nObtain yt,b = f(xt,b) + t,b for b = 1, . . . , B end for\nTheorem 3.3. At iteration t, let \u03b2t = 2 log(|X |\u03c02t2/6\u03b4) for UCB, \u03b2t = (min m\u0302\u2212\u00b5t\u22121(x)\u03c3t\u22121(x) ) 2 and \u03b6t = 2 log(\u03c02t2/3\u03b4) for EST, C1 = 36/ log(1 + \u03c3 \u22122) and fix \u03b4 > 0, then, with probability \u2265 1 \u2212 \u03b4 the full cumulative regret RTB incurred by UCB-DPP-MAX is RTB \u2264 \u221a C1TB\u03b2T \u03b3TB} and that for EST-DPP-MAX is RTB \u2264 \u221a C1TB\u03b3TB(\u03b2 1/2 t\u2217 + \u03b6 1/2 T ).\nProof. The proof is provided in Appendix 2. It should be noted that the term inside the logarithm in \u03b6t has been multiplied by 2 as compared to the sequential EST, which has a union bound over just one point, xt. This happens because we will need a union bound over not just xt,b but also x \u2022 t ."}, {"heading": "3.3 Batch Bayesian Optimization via DPP Sampling", "text": "In the previous subsection, we looked at the regret bounds achieved by DPP maximization. One natural question to ask is whether the other subset selection method via DPPs, namely DPP sampling, gives us equivalent or better regret bounds. Note that in this case, the regret would have to be defined as expected regret. The reason to believe this is well-founded as indeed sampling from k-DPPs results in better results, in both theory and practice, for low-rank matrix approximation [10] and exemplar-selection for Nystrom methods [20]. Keeping in line with the framework described in the previous subsection, the subset to be selected has to be of size B \u2212 1 and the kernel should be Kt,1 at any iteration t. Instead of maximizing, we can choose to sample from a (B\u2212 1)-DPP. The algorithm is described in Algorithm 2. The kDPPSample(K, k) procedure denotes sampling a set from the k-DPP distribution with kernel K. The question then to ask is what is the expected regret of this procedure. In this subsection, we show that the expected regret bounds of DPP-SAMPLE are less than the regret bounds of DPP-MAX and give a quantitative bound on this regret based on entropy of DPPs. By entropy of a k-DPP with kernel K, H(k \u2212DPP(K)), we simply mean the standard definition of entropy for a discrete distribution. Note that the entropy is always non-negative in this case. Please see Appendix 3 for details. For brevity, since we always choose B \u2212 1 elements from the DPP, we denote H(DPP (K)) to be the entropy of (B \u2212 1)-DPP for kernel K.\nTheorem 3.4. The regret bounds of DPP-SAMPLE are less than that of DPP-MAX. Furthermore, at iteration t, let \u03b2t = 2 log(|X |\u03c02t2/6\u03b4) for UCB, \u03b2t = (min m\u0302\u2212\u00b5t\u22121(x)\u03c3t\u22121(x) ) 2 and \u03b6t = 2 log(\u03c0 2t2/3\u03b4) for EST, C1 = 36/ log(1 + \u03c3 \u22122) and fix \u03b4 > 0, then the expected full cumulative regret of UCB-DPP-SAMPLE satisfies\nR2TB \u2264 2TBC1\u03b2T [ \u03b3TB \u2212 T\u2211 t=1 H(DPP (Kt,1)) +B log(|X |) ]\nand that for EST-DPP-SAMPLE satisfies\nR2TB \u2264 2TBC1(\u03b2 1/2 t + \u03b6 1/2 t ) 2 [ \u03b3TB \u2212 T\u2211 t=1 H(DPP (Kt,1)) +B log(|X |) ]\nProof. The proof is provided in Appendix 3.\nNote that the regret bounds for both DPP-MAX and DPP-SAMPLE are better than BUCB/B-EST due to the latter having both an additional factor of B in the log term and a regret multiplier constant C \u2032. In fact, for the RBF kernel, C \u2032 grows like ed d which is quite large for even moderate values of d."}, {"heading": "4 Experiments", "text": "In this section, we study the performance of the DPP-based algorithms, especially DPP-SAMPLE against some existing baselines. In particular, the methods we consider are BUCB [9], B-EST, UCB-PE/UCBDPP-MAX [8], EST-PE/EST-DPP-MAX, UCB-DPP-SAMPLE, EST-DPP-SAMPLE and UCB with local penalization (LP-UCB) [11]. We used the publicly available code for BUCB and PE1. The code was modified to include the code for the EST counterparts using code for EST 2. For LP-UCB, we use the publicly available GPyOpt codebase 3 and implemented the MCMC algorithm by [1] for k-DPP sampling with = 0.01 as the variation distance error. We were unable to compare against PPES as the code was not publicly available. Furthermore, as shown in the experiments in [28], PPES is very slow and does not scale beyond batch sizes of 4-5. Since UCB-PE almost always performs better than the simulation matching algorithm of [4] in all experiments that we could find in previous papers [28, 8], we forego a comparison against simulation matching as well to avoid clutter in the graphs. The performance is measured after t batch evaluations using immediate regret, rt = |f(x\u0303t)\u2212 f(x\u2217)|, where x\u2217 is a known optimizer of f and x\u0303t is the recommendation of an algorithm after t batch evaluations. We perform 50 experiments for each objective function and report the median of the immediate regret obtained for each algorithm. To maintain consistency, the first point of all methods is chosen to be the same (random). The mean function of the prior GP was the zero function while the kernel function was the squared-exponential kernel of the form k(x, y) = \u03b32 exp[\u22120.5 \u2211 d(xd \u2212 y2d)/l2d]. The hyper-parameter \u03bb was picked from a broad Gaussian hyperprior and the the other hyper-parameters were chosen from uninformative Gamma priors.\n1http://econtal.perso.math.cnrs.fr/software/ 2https://github.com/zi-w/EST 3http://sheffieldml.github.io/GPyOpt/\nOur first set of experiments is on a set of synthetic benchmark objective functions including Branin-Hoo [21], a mixture of cosines [2] and the Hartmann-6 function [21]. We choose batches of size 5 and 10. Due to lack of space, the results for mixture of cosines are provided in Appendix 5 while the results of the other two are shown in Figure 1. The results suggest that the DPP-SAMPLE based methods perform superior to the other methods. They do much better than their DPP-MAX and Batched counterparts. The trends displayed with regards to LP are more interesting. For the Branin-Hoo, LP-UCB starts out worse than the DPP based algorithms but takes over DPP-MAX relatively quickly and approaches the performance of DPP-SAMPLE when the batch size is 5. When the batch size is 10, the performance of LP-UCB does not improve much but both DPP-MAX and DPP-SAMPLE perform better. For Hartmann, LP-UCB outperforms both DPP-MAX algorithms by a considerable margin. The DPP-SAMPLE based methods perform better than LP-UCB. The gap, however, is more for the batch size of 10. Again, the performance of LP-UCB changes much lesser compared to the performance gain of the DPP-based algorithms. This is likely because the batches chosen by the DPP-based methods are more \u201cglobally diverse\u201d for larger batch sizes. The superior performance of the sampling based methods can be attributed to allowing for uncertainty in the observations by sampling as opposed to greedily emphasizing on maximizing information gain.\nWe now consider maximization of real-world objective functions. The first function we consider, robot, returns the walking speed of a bipedal robot [36]. The function\u2019s input parameters, which live in [0, 1]8, are the robot\u2019s controller. We add Gaussian noise with \u03c3 = 0.1 to the noiseless function. The second function, Abalone4 is a test function used in [8]. The challenge of the dataset is to predict the age of a species of sea snails from physical measurements. Similar to [8], we will use it as a maximization problem. Our final experiment is on hyper-parameter tuning for extreme multi-label learning. In extreme classification, one needs to deal with multi-class and multi-label problems involving a very large number of categories. Due to the prohibitively large number of categories, running traditional machine learning algorithms is not feasible. A recent popular approach for extreme classification is the FastXML algorithm [24]. The main advantage of FastXML is that it maintains high accuracy while training in a fraction of the time compared to the\n4The Abalone dataset is provided by the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml/datasets/Abalone\nprevious state-of-the-art. The FastXML algorithm has 5 parameters and the performance depends on these hyper-parameters, to a reasonable amount. Our task is to perform hyper-parameter optimization on these 5 hyper-parameters with the aim to maximize the Precision@k for k = 1, which is the metric used in [24] to evaluate the performance of FastXML compared to other algorithms as well. While the authors of [24] run extensive tests on a variety of datasets, we focus on two small datasets : Bibtex [15] and Delicious[33]. As before, we use batch sizes of 5 and 10. The results for Abalone and the FastXML experiment on Delicious are provided in the appendix. The results for Prec@1 for FastXML on the Bibtex dataset and for the robot experiment are provided in Figure 2. The blue horizontal line for the FastXML results indicates the maximum Prec@k value found using grid search.\nThe results for robot indicate that while DPP-MAX does better than their Batched counterparts, the difference in the performance between DPP-MAX and DPP-SAMPLE is much less pronounced for a small batch size of 5 but is considerable for batch sizes of 10. This is in line with our intuition about sampling being more beneficial for larger batch sizes. The performance of LP-UCB is quite close and slightly better than UCB-DPP-SAMPLE. This might be because the underlying function is well-behaved (Lipschitz continuous) and thus, the estimate for the Lipschitz constant might be better which helps them get better results. This improvement is more pronounced for batch size of 10 as well. For Abalone (see Appendix 5), LP does better than DPP-MAX but there is a reasonable gap between DPP-SAMPLE and LP which is more pronounced for B = 10.\nThe results for Prec@1 for the Bibtex dataset for FastXML are more interesting. Both DPP based methods are much better than their Batched counterparts. For B = 5, DPP-SAMPLE is only slightly better than DPP-MAX. LP-UCB starts out worse than DPP-MAX but starts doing comparable to DPP-MAX after a few iterations. For B = 10, there is not a large improvement in the gap between DPP-MAX and DPPSAMPLE. LP-UCB however, quickly takes over UCB-DPP-MAX and comes quite close to the performance of DPP-SAMPLE after a few iterations. For the Delicious dataset (see Appendix 5), we see a similar trend of the improvement of sampling to be larger for larger batch sizes. LP-UCB displays an interesting trend in this experiment by doing much better than UCB-DPP-MAX for B = 5 and is in fact quite close to the performance of DPP-SAMPLE. However, for B = 10, its performance is much closer to UCB-DPP-MAX. DPP-SAMPLE loses out to LP-UCB only on the robot dataset and does better for all the other datasets. Furthermore, this improvement seems more pronounced for larger batch sizes. We leave experiments with other kernels and a more thorough experimental evaluation with respect to batch sizes for future work."}, {"heading": "5 Conclusion", "text": "We have proposed a new method for batched Gaussian Process bandit (batch Bayesian) optimization based on DPPs which are desirable in this case as they promote diversity in batches. The DPP kernel is automatically figured out on the fly which allows us to show regret bounds for DPP maximization and sampling based methods for this problem. We show that this framework exactly recovers a popular algorithm for BBO, namely the UCB-PE when we consider DPP maximization using the greedy algorithm. We showed that the regret for the sampling based method is always less than the maximization based method. We also derived their EST counterparts and also provided a simpler proof of the information gain for RBF kernels which leads to a slight improvement in the best bound known. Our experiments on a variety of synthetic and real-world tasks validate our theoretical claims that sampling performs better than maximization and other methods."}, {"heading": "6 APPENDIX", "text": ""}, {"heading": "6.1 The Batched-EST Algorithm", "text": "The proofs for B-EST are relatively straightforward which follow from combining the proofs of [9] and [35]. We provide them here for completeness. We first need a series of supporting lemmas which are variants of the lemmas of UCB for EST. These require different bounds than the ones for BUCB.\nLemma 6.1. (Lemma 3.2 in [35]) Pick \u03b4 \u2208 (0, 1) and set \u03b6t = 2 log(\u03c02t2/6\u03b4). Then, for an arbitrary sequence of actions x1, x2, . . . \u2208 X ,\nPr[|f(xt)\u2212 \u00b5t\u22121(xt)| \u2264 \u03b61/2t \u03c3t\u22121(xt)] \u2265 1\u2212 \u03b4, for all t \u2208 [1, T ].\nThe GP-UCB/EST decision rule is,\nxt = arg max x\u2208X\n[ \u00b5t\u22121(x) + \u03b1 1/2 t \u03c3t\u22121(x) ] For EST, \u03b1t = (m\u2212\u00b5t\u22121(x) \u03c3t\u22121(x) )2 . Implicit in this definition of the decision rule is the corresponding confidence interval for each x \u2208 X ,\nCseqt (x) \u2261 [ \u00b5t\u22121(x)\u2212 \u03b11/2t \u03c3t\u22121(x), \u00b5t\u22121(x) + \u03b1 1/2 t \u03c3t\u22121(x) ] ,\nwhere this confidence interval\u2019s upper confidence bound is the value of the argument of the decision rule. Furthermore, the width of any confidence interval is the difference between the uppermost and the lowermost limits, here w = 2\u03b1 1/2 t \u03c3t\u22121(x). In the case of BUCB/B-EST, the batched confidence rules are of the form,\nCbatcht (x) \u2261 [ \u00b5fb[t](x)\u2212 \u03b2 1/2 t \u03c3t\u22121(x), \u00b5fb[t](x) + \u03b2 1/2 t \u03c3t\u22121(x) ] Lemma 6.2. (Similar to Lemma 12 in [9]) If f(xt) \u2208 Cbatcht (xt)\u2200t \u2265 1 and given that actions are selected using EST, it holds that,\nRT \u2264 \u221a TC1\u03b3T (\u03b6 1/2 T + \u03b1 1/2 t\u2217 )\nProof. The proof of Lemma 12 in [9] just uses the fact that the sequential regret bounds of UCB are 2\u03b2 1/2 t \u03c3t(xt). We follow their same proof but use the EST sequential bounds to get the desired result.\nProof. (of Theorem 3.2 in the main paper) The proof is similar to Theorem 5 in [9]. The sum of the regrets over the T timesteps is split over the first T init timesteps and the remaining timesteps. The former term\u2211T init t=1 m \u2212 f(xt) \u2264 2T init\u2016f\u2016\u221e. The latter term is treated as simple BUCB and from Lemma 6.2, we\nget RT init+1:T \u2264 \u221a (T \u2212 T init)C1\u03b3(T\u2212T init)(\u03b6 1/2 T + \u03b1 1/2 t\u2217 ) \u2264 \u221a TC1\u03b3T (\u03b6 1/2 T + \u03b1 1/2 t\u2217 ) as \u03b3T is a non-decreasing function. Combining the two terms gives us the desired result."}, {"heading": "6.2 Batch Bayesian Optimization via DPP-Maximization", "text": "In this section, we present the proof of the regret bounds for BBO via DPP-MAX. Since the GP-UCBDPP-MAX is the same as GP-UCB-PE, we focus on GP-EST-DPP-MAX. We first restate the EST part of Theorem 3.3. Firstly, none of our proofs will depend on the order in which the batch was constructed but for sake of clarity of exposition, whenever needed, we can consider any arbitrary ordering of the B \u2212 1 points chosen by maximizing a (B \u2212 1)-DPP or sampling from it.\nTheorem 6.3. At iteration t, fix \u03b4 > 0 and let \u03b2t = [\nmin x\u2208X m\u2212\u00b5t\u22121(x) \u03c3t\u22121(x)\n] , \u03b6t = 2 log(\u03c0 2t2/3\u03b4) and C1 =\n36/ log(1 + \u03c3\u22122). Then, with probability \u2265 1\u2212 \u03b4, the full cumulative regret incurred by EST-DPP-MAX is Rt \u2264 \u221a C1TB\u03b3TB(\u03b2 1/2 t\u2217 + \u03b6 1/2 TB ).\nNotice that the logarithm term for \u03b6t in the above theorem is twice that of the one in Lemma 6.1. This happens by considering the same proof as that for Lemma 6.1 but taking a union bound over x\u2022t along with xt. We first prove some required lemmas.\nLemma 6.4. The deviation of the first point, selected by either UCB or EST, is bounded by the deviation of any point selected by the DPP-MAX or DPP-SAMPLE in the previous iteration with high probability, i.e.,\n\u2200t < T,\u22002 \u2264 b \u2265 B, \u03c3t,1(xt+1,1) \u2264 \u03c3t\u22121,b(xt,b)\nProof. The proof does not depend on the actual policy (UCB/EST) used for the first point of the batch or whether it was DPP-MAX or DPP-SAMPLE (consider an arbitary ordering of points chosen in either case). By the definition of xt+1,1, we have f + t+1(xt+1,1) \u2265 f + t+1(x \u2022 t ). Also, from Lemma 6.1, we have f+t+1(x \u2022 t ) \u2265 f\u2212t (x\u2022t ). This is different than Lemma 2 in [8] as it now only holds for x\u2022t rather than all x \u2208 X . Thus, with high probability, f+t+1(xt+1,1) \u2265 y\u2022t and thus, xt+1,1 \u2208 R + t . Now, from the definition of xt,b, we have \u03c3t\u22121,b(xt+1,1) \u2264 \u03c3t\u22121,b(xt,b) w.h.p. Using the \u201cInformation never hurts\u201d principle [17], we know that the entropy of f(x) for all locations x can only decrease after observing a point xt,k. For GPs, the entropy is also a non-decreasing function of the variance and thus, we have, \u03c3t,1(xt+1,1) \u2264 \u03c3t\u22121,b(xt,b) and we are done.\nLemma 6.5. (Lemma 3 in [8]) The sum of deviations of the points selected by the UCB/EST policy is bounded by the sum of deviations over all the selected points divided by B. Formally, with high probability,\nT\u2211 t=1 \u03c3t\u22121,1(xt,1) \u2264 1 B T\u2211 t=1 B\u2211 b=1 \u03c3t\u22121,b(xt,b).\nProof. The proof is the same as that of Lemma 3 in [8] but we provide it here for completeness. Using Lemma 6.4 and the definitions of xt,b, we have \u03c3t,1(xt+1,1) \u2264 \u03c3t\u22121,b for all b \u2265 2. Summing over all b, we get for all t \u2265 1, \u03c3t\u22121,1(xt,1) + (B \u2212 1)\u03c3t,1(xt+1,1) \u2264 \u2211B b=1 \u03c3t\u22121,b(xt,b). Now, summing over t, we get the desired result.\nLemma 6.6. (Lemma 4 in [8]) The sum of the variances of the selected points are bounded by a constant factor times \u03b3TB, i.e., \u2203C \u20321 \u2208 R, T\u2211 t=1 B\u2211 b=1 (\u03c3t\u22121,b(xt\u22121,b)) 2 \u2264 C \u20321\u03b3TB. Here C \u20321 = 2log(1+\u03c3\u22122) .\nWe finally prove Theorem 6.3.\nProof. (of Theorem 6.3) Clearly, the proof of Lemma 6.1 holds even for the last B \u2212 1 points selected in a batch. However, t\u2217 only goes over the the T iterations rather than TB evaluations. Thus, the cumulative\nregret is of the form\nRTB = T\u2211 t=1 B\u2211 b=1 rt,b\n\u2264 T\u2211 t=1 B\u2211 b=1 (\u03b2 1/2 t\u2217 + \u03b6 1/2 TB )\u03c3t\u22121,b(xt,b)\n\u2264 (\u03b21/2t\u2217 + \u03b6 1/2 TB ) \u221a\u221a\u221a\u221aTB T\u2211 t=1 B\u2211 b=1 (\u03c3t\u22121,b(xt,b))2 by Cauchy-Schwarz\n\u2264 \u221a TBC \u20321\u03b3TB(\u03b2 1/2 t\u2217 + \u03b6 1/2 TB ) by Lemma 6.6\n\u2264 \u221a TBC1\u03b3TB(\u03b2 1/2 t\u2217 + \u03b6 1/2 TB ) since C \u2032 1 \u2264 C1"}, {"heading": "6.3 Batch Bayesian Optimization via DPP sampling", "text": "In this section, we prove the expected regret bounds obtained by DPP-SAMPLE (Theorem 3.4 of the main paper)\nLemma 6.7. For the points chosen by (UCB/EST)-DPP-SAMPLE, the inequality,\nT\u2211 t=1 (\u03c3t\u22121,1(xt\u22121,1)) 2 \u2264 1 B \u2212 1 T\u2211 t=1 B\u2211 b=2 (\u03c3t\u22121,b(xt\u22121,b)) 2\nholds with high probability.\nProof. Clearly, Lemma 6.5 holds in this case as well. Furthermore, it is easy to see that the inequality obtained by replacing every term in every summation by its square in Lemma 6.5 is also true by a similar proof. Thus, we have\nT\u2211 t=1 (\u03c3t\u22121,1(xt,1)) 2 \u2264 1 B T\u2211 t=1 B\u2211 b=1 (\u03c3t\u22121,b(xt,b)) 2\n=\u21d2 (1\u2212 1 B ) T\u2211 t=1 (\u03c3t\u22121,1(xt,1)) 2 \u2264 1 B T\u2211 t=1 B\u2211 b=2 (\u03c3t\u22121,b(xt,b)) 2\n=\u21d2 T\u2211 t=1 (\u03c3t\u22121,1(xt,1)) 2 \u2264 1 B \u2212 1 T\u2211 t=1 B\u2211 b=2 (\u03c3t\u22121,b(xt,b)) 2\nHence, we are done.\nDefine \u03b7 1/2 t =\n{ 2\u03b2\n1/2 t for UCB\n(\u03b2 1/2 t\u2217 + \u03b6 1/2 t ) for EST\n.\nProof. (of Theorem 3.4 in the main paper) The expectation here is taken over the last B \u2212 1 points in each iteration being drawn from the (B \u2212 1)-DPP with the posterior kernel at the tth iteration. Using linearity of\nexpectation, we get( E [ T\u2211 t=1 B\u2211 b=1 rt,b ])2 = ( T\u2211 t=1 E [ B\u2211 b=1 rt,b ])2\n\u2264 ( T\u2211 t=1 \u03b7 1/2 tB E [ B\u2211 b=1 \u03c3t\u22121,b(xt,b) ])2\n\u2264 \u03b7TBTB T\u2211 t=1 E [ B\u2211 b=1 (\u03c3t\u22121,b(xt,b)) 2 ]\nby Cauchy-Schwarz\n\u2264 \u03b7TB TB2\nB \u2212 1 T\u2211 t=1 E [ B\u2211 b=2 (\u03c3t\u22121,b(xt,b)) 2 ]\nby Lemma 6.7\nIt is easy to see by Schur\u2019s identity and the definition of \u03c3t\u22121,b that the term inside the expectation is just log det((Kt,1)S), where S is the set of B \u2212 1 points chosen in the tth iteration by DPP sampling with kernel Kt,1. Let L = B \u2212 1. Thus,(\nE [ T\u2211 t=1 B\u2211 b=1 rt,b ])2 \u2264 \u03b7TB TB2 B \u2212 1 T\u2211 t=1 ES\u223c(L\u2212DPP (Kt,1)) [ log det((Kt,1)S) ]\nFirstly, since the expectation is less than the maximum and B/(B \u2212 1) \u2264 2, we get that the expected regret has the same bound as the regret bounds for DPP-MAX. This bound is however, loose. We get a bound below which may be worse but that is due to a loose analysis on our part and we can just choose the minimum of below and the DPP-MAX regret bounds. Expanding the expectation, we get\nES\u223c(L\u2212DPP (Kt,1)) [ log det((Kt,1)S) ]\n= \u2211 |S|=L det((Kt,1)S) log(det((Kt,1)S))\u2211 |S|=L det((Kt,1)S) = \u2211 |S|=L det((Kt,1)S) log( det((Kt,1)S))\u2211 |S|=L det((Kt,1)S) )\u2211 |S|=L det((Kt,1)S) + det((Kt,1)S)) log( \u2211 |S|=L det((Kt,1)S))\u2211 |S|=L det((Kt,1)S)\n= \u2212H(L-DPP(Kt,1)) + log( \u2211 |S|=L det((Kt,1)S))\n\u2264 \u2212H(L-DPP(Kt,1)) + log(|X |L max det((Kt,1)S)) \u2264 \u2212H(L-DPP(Kt,1)) + L log(|X |) + log(max det((Kt,1)S))\nPlugging this into the summation and observing that the summation over last term is less than C \u20321\u03b3TB , we get the desired result."}, {"heading": "6.4 Bounds on Information Gain for RBF kernels", "text": "Theorem 6.8. The maximum information gain for the RBF kernel after S timesteps is O ( (log |S|)d )\nProof. Let xS be the vector of points from subset S, that is, xS = (x)x\u2208S , and the noisy evaluations of a function f at these points be denoted by a vector yS = fS + S , where fS = (f(x))x\u2208S and S \u223c N(0, \u03c32I). In Bayesian experimental design, the informativeness or the information gain of S is given by the mutual information between f and these observations I(yS ; f) = H(yS) \u2212 H(yS | f). When f is modeled by a\nGaussian process, it is specified by the mean function \u00b5(x) = Ef(x) and the covariance or kernel function k(x, x\u2032) = E(f(x)\u2212 \u00b5(x)) (f(x\u2032)\u2212 \u00b5(x\u2032)). In this case,\nI(yS ; f) = I(yS ; fS) = 1\n2 log det(I + \u03c3\u22122KS),\nwhere KS = (k(x, x \u2032))x,x\u2032\u2208S . It is easy to see that\nlog det(I + \u03c3\u22122KS) = |S|\u2211 t=1 log ( 1 + \u03c3\u22122\u03bbt(KS) ) Seeger et al. [27] showed that for a Gaussian RBF kernel in d dimensions, \u03bbt(K) \u2264 cBt 1/d , with B < 1. Let\nT = ( log1/B |S| )d |S|. Then for t > T , we have \u03bbt \u2264 c/T , and for t \u2264 T , we have \u03bbt \u2264 c. Therefore,\nlog det(I + \u03c3\u22122KS) = |S|\u2211 t=1 log ( 1 + \u03c3\u22122\u03bbt(KS) ) = \u2211 t\u2264T log ( 1 + \u03c3\u22122\u03bbt(KS) ) + \u2211 t>T log ( 1 + \u03c3\u22122\u03bbt(KS)\n) \u2264 T log ( 1 + \u03c3\u22122c ) + log ( 1 + c\u03c3\u22122\n|S| )T = O(T )\nThus, the maximum information gain for S is upper bounded by O ( (log |S|)d ) ."}, {"heading": "6.5 Experiments", "text": ""}, {"heading": "6.5.1 Synthetic Experiments", "text": ""}, {"heading": "6.5.2 Real-World Experiments", "text": "We first provide the results for the Abalone experiment and then provide the Prec@1 values for the FastXML experiment on the Delicious experiment."}], "references": [{"title": "Monte carlo markov chains algorithms for sampling strongly rayleigh distributions and determinantal point processes", "author": ["N. Anari", "S.O. Gharan", "A. Rezaei"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A nonparametric approach to noisy and costly optimization", "author": ["B.S. Anderson", "A.W. Moore", "D. Cohn"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Using confidence bounds for exploration-exploitation", "author": ["P. Auer"], "venue": "trade-offs. JMLR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Batch bayesian optimization via simulation matching", "author": ["J. Azimi", "A. Fern", "X. Fern"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Determinantal identities: Gauss, schur, cauchy, sylvester, kronecker, jacobi, binet, laplace, muir, and cayley", "author": ["R. Brualdi", "H. Schneider"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1983}, {"title": "On selecting a maximum volume sub-matrix of a matrix and related problems", "author": ["A. \u00c7ivril", "M. Magdon-Ismail"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Exponential inapproximability of selecting a maximum volume sub-matrix", "author": ["A. \u00c7ivril", "M. Magdon-Ismail"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Parallel gaussian process optimization with upper confidence bound and pure exploration", "author": ["E. Contal", "D. Buffoni", "D. Robicquet", "N. Vayatis"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization", "author": ["T. Desautels", "A. Krause", "J.W. Burdick"], "venue": "JMLR, 15:4053\u20134103,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Batch bayesian optimization via local penalization", "author": ["J. Gonzalez", "Z. Dai", "P. Hennig", "N. Lawrence"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "GLASSES: relieving the myopia of bayesian optimisation", "author": ["J. Gonz\u00e1lez", "M.A. Osborne", "N.D. Lawrence"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C. Schuler"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Predicitive entropy search for efficient global optimization of black-box functions", "author": ["J.M. Hernandex-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["I. Katakis", "G. Tsoumakas", "I. Vlahavas"], "venue": "ECML/PKDD Discovery Challenge,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Contextual gaussian process bandit optimization", "author": ["A. Krause", "C.S. Ong"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies", "author": ["Andreas Krause", "Ajit Singh", "Carlos Guestrin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "k-dpps: Fixed-size determinantal point processes", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Determinantal Point Processes for Machine Learning", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Fast dpp sampling for nystrm with application to kernel methods", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Pratical bayesian optimization", "author": ["D. Lizotte"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Determinantal probability measures", "author": ["R. Lyons"], "venue": "Publications Mathe\u0301matiques de l\u2019Institut des Hautes E\u0301tudes Scientifiques,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Randomized rounding for the largest simplex problem", "author": ["A. Nikolov"], "venue": "In STOC, pages 861\u2013870,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Y. Prabhu", "M. Varma"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Gaussian processes for machine learning", "author": ["C. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bul. Am. Math. Soc.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1952}, {"title": "Information consistency of nonparametric gaussian process methods", "author": ["M.W. Seeger", "S.M. Kakade", "D.P. Foster"], "venue": "IEEE Tr. Inf. Theo.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Parallel predictive entropy search for batch global optimization of expensive objective functions", "author": ["A. Shah", "Z. Ghahramani"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Random point fields associated with certain fredholm determinants i: fermion, poisson and boson point processes", "author": ["T. Shirai", "Y. Takahashi"], "venue": "Journal of Functional Analysis,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Practical bayesian optimization of machine learning", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Information-theoretic regret bounds for gaussian process optimization in the bandit setting", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Auto-weka : combined selection and hyper-parameter optimization of classification", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Effective and efficient multilabel classification in domains with large number of labels", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "ECML/PKDD 2008 Workshop on Mining Multidimensional Data,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Review of metamodeling techniques in support of engineering design optimization", "author": ["G. Wang", "S. Shan"], "venue": "Journal of Mechanical Design,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Optimization as estimation with gaussian processes in bandit settings", "author": ["Z. Wang", "B. Zhou", "S. Jegelka"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Feedback control of dynamic bipedal robot locomotion", "author": ["E. Westervelt", "J. Grizzle"], "venue": "Control and Automation Series,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Stochastic optimization models in finance", "author": ["W. Ziemba", "R. Vickson"], "venue": "World Scientific Singapore,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}], "referenceMentions": [{"referenceID": 33, "context": ", engineering design [34], finance [37] and hyper-parameter optimization [30].", "startOffset": 21, "endOffset": 25}, {"referenceID": 36, "context": ", engineering design [34], finance [37] and hyper-parameter optimization [30].", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": ", engineering design [34], finance [37] and hyper-parameter optimization [30].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "This Bayesian optimization (BO) framework has received considerable attention in tuning of hyper-parameters for complex models and algorithms in Machine Learning, Robotics and Computer Vision [16, 32, 30, 12].", "startOffset": 192, "endOffset": 208}, {"referenceID": 31, "context": "This Bayesian optimization (BO) framework has received considerable attention in tuning of hyper-parameters for complex models and algorithms in Machine Learning, Robotics and Computer Vision [16, 32, 30, 12].", "startOffset": 192, "endOffset": 208}, {"referenceID": 29, "context": "This Bayesian optimization (BO) framework has received considerable attention in tuning of hyper-parameters for complex models and algorithms in Machine Learning, Robotics and Computer Vision [16, 32, 30, 12].", "startOffset": 192, "endOffset": 208}, {"referenceID": 11, "context": "This Bayesian optimization (BO) framework has received considerable attention in tuning of hyper-parameters for complex models and algorithms in Machine Learning, Robotics and Computer Vision [16, 32, 30, 12].", "startOffset": 192, "endOffset": 208}, {"referenceID": 8, "context": "Apart from a few notable exceptions [9, 8, 11], most methods for Bayesian optimization work by exploring one parameter value at a time.", "startOffset": 36, "endOffset": 46}, {"referenceID": 7, "context": "Apart from a few notable exceptions [9, 8, 11], most methods for Bayesian optimization work by exploring one parameter value at a time.", "startOffset": 36, "endOffset": 46}, {"referenceID": 10, "context": "Apart from a few notable exceptions [9, 8, 11], most methods for Bayesian optimization work by exploring one parameter value at a time.", "startOffset": 36, "endOffset": 46}, {"referenceID": 8, "context": "Indeed, this is the motivation behind many popular BBO methods like the BUCB [9], UCB-PE [8] and Local Penalization", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "Indeed, this is the motivation behind many popular BBO methods like the BUCB [9], UCB-PE [8] and Local Penalization", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory [29, 22], and have efficient sampling algorithms [18, 19].", "startOffset": 149, "endOffset": 157}, {"referenceID": 21, "context": "DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory [29, 22], and have efficient sampling algorithms [18, 19].", "startOffset": 149, "endOffset": 157}, {"referenceID": 17, "context": "DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory [29, 22], and have efficient sampling algorithms [18, 19].", "startOffset": 198, "endOffset": 206}, {"referenceID": 18, "context": "DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory [29, 22], and have efficient sampling algorithms [18, 19].", "startOffset": 198, "endOffset": 206}, {"referenceID": 7, "context": "Following UCB-PE [8], our methods also choose the first point via an acquisition function, and then the rest of the points are selected from a relevance region using a DPP.", "startOffset": 17, "endOffset": 20}, {"referenceID": 34, "context": "The acquisition functions we consider are EST [35], a recently proposed sequential MAP-estimate based Bayesian optimization algorithm with regret bounds independent of the size of the domain, and UCB [31].", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "The acquisition functions we consider are EST [35], a recently proposed sequential MAP-estimate based Bayesian optimization algorithm with regret bounds independent of the size of the domain, and UCB [31].", "startOffset": 200, "endOffset": 204}, {"referenceID": 26, "context": "In the appendix, we also provide a simpler proof of the information gain for the widely-used RBF kernel which also improves the bound from O((log T )) [27, 31] to O((log T )).", "startOffset": 151, "endOffset": 159}, {"referenceID": 30, "context": "In the appendix, we also provide a simpler proof of the information gain for the widely-used RBF kernel which also improves the bound from O((log T )) [27, 31] to O((log T )).", "startOffset": 151, "endOffset": 159}, {"referenceID": 25, "context": "This exploration-exploitation trade off naturally leads to modeling this problem in the multi-armed bandit paradigm [26], where the goal is to maximize cumulative reward by optimally balancing this trade-off.", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "[31] analyzed the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method [3] to achieve the first sub-linear regret bounds for Gaussian process bandit optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[31] analyzed the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method [3] to achieve the first sub-linear regret bounds for Gaussian process bandit optimization.", "startOffset": 117, "endOffset": 120}, {"referenceID": 34, "context": "[35] considered an intuitive MAP-estimate based strategy (EST) which involves estimating the maximum value of a function and choosing a point which has maximum probability of achieving this maximum value.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Along with these, there is also work on Entropy search (ES) [13] and its variant, predictive entropy search (PES) [14] which instead aims at minimizing the uncertainty about the location of the optimum of the function.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Along with these, there is also work on Entropy search (ES) [13] and its variant, predictive entropy search (PES) [14] which instead aims at minimizing the uncertainty about the location of the optimum of the function.", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "Indeed this is the criticism of these two methods by a recently proposed BBO strategy PPES [28], which parallelizes predictive entropy search based methods and shows considerable improvements over the BUCB and UCB-PE methods.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Another recently proposed method is the Local Penalization (LP) [11], which assumes that the function is Lipschitz continuous and tries to estimate the Lipschitz constant.", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "We consider the domain to be discrete as it is well-known how to obtain regret bounds for continous, compact domains via suitable discretizations [31].", "startOffset": 146, "endOffset": 150}, {"referenceID": 24, "context": "Given the observations Dt = {(x\u03c4 , y\u03c4 )\u03c4=1} up to time t, we obtain the posterior mean and covariance functions [25] via the kernel matrix Kt = [k(xi, xj)]xi,xj\u2208Dt and kt(x) = [k(xi, x)]xi\u2208Dt : \u03bct(x) = kt(x) T (Kt + \u03c3 I)yt and kt(x, x \u2032) = k(x, x\u2032)\u2212 kt(x) (Kt + \u03c3I)kt(x).", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "Define the Upper Confidence Bound (UCB) f + and Lower Confidence Bound (LCB) f\u2212 as f t (x) = \u03bct\u22121(x) + \u03b2 1/2 t \u03c3t\u22121(x) f \u2212 t (x) = \u03bct\u22121(x)\u2212 \u03b2 1/2 t \u03c3t\u22121(x) A crucial observation made in BUCB [9] and UCB-PE [8] is that the posterior covariance and variance functions do not depend on the actual function values at the set of points.", "startOffset": 191, "endOffset": 194}, {"referenceID": 7, "context": "Define the Upper Confidence Bound (UCB) f + and Lower Confidence Bound (LCB) f\u2212 as f t (x) = \u03bct\u22121(x) + \u03b2 1/2 t \u03c3t\u22121(x) f \u2212 t (x) = \u03bct\u22121(x)\u2212 \u03b2 1/2 t \u03c3t\u22121(x) A crucial observation made in BUCB [9] and UCB-PE [8] is that the posterior covariance and variance functions do not depend on the actual function values at the set of points.", "startOffset": 206, "endOffset": 209}, {"referenceID": 34, "context": "The EST algorithm in [35] chooses at each timestep t,the point which has the maximum posterior probability of attaining the maximum value m, i.", "startOffset": 21, "endOffset": 25}, {"referenceID": 34, "context": "[35] get around this by using an approximation m\u0302 which, under certain conditions specified in their paper, is an upper bound on m.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "We refer the reader to [35] for details of the two estimates and refer to ESTa as EST.", "startOffset": 23, "endOffset": 27}, {"referenceID": 30, "context": "The regret for both the UCB and the EST algorithms are presented in the following theorem which is a combination of Theorem 1 in [31] and Theorem 3.", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "1 in [35].", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "Pr(B) = det(KB) \u2211 |S|=k det(KS) The problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18].", "startOffset": 194, "endOffset": 215}, {"referenceID": 6, "context": "Pr(B) = det(KB) \u2211 |S|=k det(KS) The problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18].", "startOffset": 194, "endOffset": 215}, {"referenceID": 5, "context": "Pr(B) = det(KB) \u2211 |S|=k det(KS) The problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18].", "startOffset": 194, "endOffset": 215}, {"referenceID": 9, "context": "Pr(B) = det(KB) \u2211 |S|=k det(KS) The problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18].", "startOffset": 194, "endOffset": 215}, {"referenceID": 0, "context": "Pr(B) = det(KB) \u2211 |S|=k det(KS) The problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18].", "startOffset": 194, "endOffset": 215}, {"referenceID": 17, "context": "Pr(B) = det(KB) \u2211 |S|=k det(KS) The problem of picking a set of size k which maximizes the determinant and sampling a set according to the k-DPP distribution has received considerable attention [23, 7, 6, 10, 1, 18].", "startOffset": 194, "endOffset": 215}, {"referenceID": 22, "context": "The best known approximation algorithm is by [23] with a factor of 1/e, which almost matches the lower bound.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "For sampling from k-DPPs, an exact sampling algorithm exists due to [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "A recently proposed alternative is an MCMC based method by [1] which is much faster.", "startOffset": 59, "endOffset": 62}, {"referenceID": 26, "context": "In Appendix 4, while borrowing ideas from [27], we provide a simpler proof with improved bounds on the maximum information gain for the RBF kernel.", "startOffset": 42, "endOffset": 46}, {"referenceID": 34, "context": "The algorithm mainly comes from the observation made in [35] that the point chosen by EST is the same as a variant of UCB.", "startOffset": 56, "endOffset": 60}, {"referenceID": 34, "context": "1 in [35]) At any timestep t, the point selected by EST is the same as the point selected by a variant of UCB with \u03b2 1/2 t = minx\u2208X (m\u0302\u2212 \u03bct\u22121(x))/\u03c3t\u22121(x).", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "In the algorithm, C \u2032 is chosen to be exp(2C), where C is an upper bound on the maximum conditional mutual information I(f(x); yfb[t]+1:t\u22121|y1:fb[t]) (refer to [9] for details).", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "This is corrected in [9] by two-stage BUCB which first chooses an initial batch of size T init by greedily choosing points based on the (updated) posterior variances.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "We refer the reader to the Table 1 in [9] for values of C \u2032 and T init for common kernels.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "Choose \u03b1t = ( minx\u2208X m\u0302\u2212\u03bcfb[t](x) \u03c3t\u22121(x) )2 and \u03b2t = (C )\u03b1t, B \u2265 2, \u03b4 > 0 and the C \u2032 and T init values are chosen according to Table 1 in [9].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "The proof of the regret bounds of UCB-PE go through a few steps but in one of the intermediate steps (Lemma 5 of [8]), it is shown that the sum of regrets over a batch at an iteration t is upper bounded as", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "From the final log-product term, it can be seen (from Schur\u2019s determinant identity [5] and the definition of \u03c3t,b(xt,b)) that the product of the last B \u2212 1 terms is exactly the B \u2212 1 principal minor of Kt,1 formed by the indices corresponding to S = {xt,b}b=2.", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "The reason to believe this is well-founded as indeed sampling from k-DPPs results in better results, in both theory and practice, for low-rank matrix approximation [10] and exemplar-selection for Nystrom methods [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 19, "context": "The reason to believe this is well-founded as indeed sampling from k-DPPs results in better results, in both theory and practice, for low-rank matrix approximation [10] and exemplar-selection for Nystrom methods [20].", "startOffset": 212, "endOffset": 216}, {"referenceID": 8, "context": "In particular, the methods we consider are BUCB [9], B-EST, UCB-PE/UCBDPP-MAX [8], EST-PE/EST-DPP-MAX, UCB-DPP-SAMPLE, EST-DPP-SAMPLE and UCB with local penalization (LP-UCB) [11].", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "In particular, the methods we consider are BUCB [9], B-EST, UCB-PE/UCBDPP-MAX [8], EST-PE/EST-DPP-MAX, UCB-DPP-SAMPLE, EST-DPP-SAMPLE and UCB with local penalization (LP-UCB) [11].", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "In particular, the methods we consider are BUCB [9], B-EST, UCB-PE/UCBDPP-MAX [8], EST-PE/EST-DPP-MAX, UCB-DPP-SAMPLE, EST-DPP-SAMPLE and UCB with local penalization (LP-UCB) [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "For LP-UCB, we use the publicly available GPyOpt codebase 3 and implemented the MCMC algorithm by [1] for k-DPP sampling with = 0.", "startOffset": 98, "endOffset": 101}, {"referenceID": 27, "context": "Furthermore, as shown in the experiments in [28], PPES is very slow and does not scale beyond batch sizes of 4-5.", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "Since UCB-PE almost always performs better than the simulation matching algorithm of [4] in all experiments that we could find in previous papers [28, 8], we forego a comparison against simulation matching as well to avoid clutter in the graphs.", "startOffset": 85, "endOffset": 88}, {"referenceID": 27, "context": "Since UCB-PE almost always performs better than the simulation matching algorithm of [4] in all experiments that we could find in previous papers [28, 8], we forego a comparison against simulation matching as well to avoid clutter in the graphs.", "startOffset": 146, "endOffset": 153}, {"referenceID": 7, "context": "Since UCB-PE almost always performs better than the simulation matching algorithm of [4] in all experiments that we could find in previous papers [28, 8], we forego a comparison against simulation matching as well to avoid clutter in the graphs.", "startOffset": 146, "endOffset": 153}, {"referenceID": 20, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including Branin-Hoo [21], a mixture of cosines [2] and the Hartmann-6 function [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including Branin-Hoo [21], a mixture of cosines [2] and the Hartmann-6 function [21].", "startOffset": 132, "endOffset": 135}, {"referenceID": 20, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including Branin-Hoo [21], a mixture of cosines [2] and the Hartmann-6 function [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 35, "context": "The first function we consider, robot, returns the walking speed of a bipedal robot [36].", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "The function\u2019s input parameters, which live in [0, 1], are the robot\u2019s controller.", "startOffset": 47, "endOffset": 53}, {"referenceID": 7, "context": "The second function, Abalone is a test function used in [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Similar to [8], we will use it as a maximization problem.", "startOffset": 11, "endOffset": 14}, {"referenceID": 23, "context": "A recent popular approach for extreme classification is the FastXML algorithm [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Our task is to perform hyper-parameter optimization on these 5 hyper-parameters with the aim to maximize the Precision@k for k = 1, which is the metric used in [24] to evaluate the performance of FastXML compared to other algorithms as well.", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "While the authors of [24] run extensive tests on a variety of datasets, we focus on two small datasets : Bibtex [15] and Delicious[33].", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "While the authors of [24] run extensive tests on a variety of datasets, we focus on two small datasets : Bibtex [15] and Delicious[33].", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "While the authors of [24] run extensive tests on a variety of datasets, we focus on two small datasets : Bibtex [15] and Delicious[33].", "startOffset": 130, "endOffset": 134}], "year": 2016, "abstractText": "Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function requires training a model which may involve days or even weeks of computation. Most methods for this so-called \u201cBayesian optimization\u201d only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}