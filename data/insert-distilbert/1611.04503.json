{"id": "1611.04503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot", "abstract": "we propose an approach to build a neural machine translation system with no supervised resources ( i. e., no parallel corpora ) using multimodal embedded representation over texts and images. based on the assumption that text documents are often likely to be described with other multimedia information ( are e. g., images ) somewhat related to the content, we try to indirectly estimate the relevance between two languages. using multimedia as the \" pivot \", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. this modality - agnostic representation is the key to bridging the communication gap between different modalities. putting a decoder on top rank of it, our network can flexibly draw the outputs from observing any local input linguistic modality. notably, in the testing phase, we need only source language texts as the input for translation. in experiments, we tested our method on using two benchmarks to show that such it can achieve reasonable translation performance. we compared and investigated several hundred possible implementations and found that an end - value to - end model that simultaneously optimized both rank loss in multimodal encoders and cross - entropy loss in decoders performed the best.", "histories": [["v1", "Mon, 14 Nov 2016 18:07:54 GMT  (2709kb)", "http://arxiv.org/abs/1611.04503v1", null], ["v2", "Sun, 21 May 2017 17:36:30 GMT  (2655kb)", "http://arxiv.org/abs/1611.04503v2", null], ["v3", "Sun, 23 Jul 2017 15:52:08 GMT  (2656kb)", "http://arxiv.org/abs/1611.04503v3", "Some error corrections in Sect.2.2 and Table 5, Machine Translation, 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.MM", "authors": ["hideki nakayama", "noriki nishida"], "accepted": false, "id": "1611.04503"}, "pdf": {"name": "1611.04503.pdf", "metadata": {"source": "CRF", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot", "authors": ["Hideki Nakayama", "Noriki Nishida"], "emails": ["nakayama@ci.i.u-tokyo.ac.jp", "nishida@nlab.ci.i.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 50\n3v 1\n[ cs\n.C L\n] 1\n4 N\nov 2\nIn experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.\nI. INTRODUCTION\nMachine translation (MT) has been one of the most important challenges in natural language processing. Irrespective of traditional statistical machine translation (SMT) [18] or modern neural machine translation (NMT) [30], methods and data have always been mutually indispensable to each other. Indeed, the success of corpus-based MT is mainly dependent on the quality and scale of available parallel corpora to train MT systems. Recent state-of-the-art NMT systems have shown that translation can be surprisingly improved with sufficiently large-scale data and high computational power [27].\nOn the other hand, how to prepare such corpora has remained a big problem. In some specific domains such as Web news, patents, and Wikipedia, relatively high-quality multilingual translations are made available by content holders or volunteer workers, which have been utilized by researchers for decades [17], [31]. However, in more general cases, it is not always possible to collect a sufficient amount of parallel data because most generic Web documents are monolingual. The human cost for preparing manual translation is quite high, and it is particularly prohibitive for minor language pairs where resources are severely limited.\nTo tackle the situation where no or only a few parallel corpora are available, a branch of MT called pivot-based machine translation has been developed. The idea of the pivotbased approach is to indirectly learn the alignment of the source and target languages with the help of a third modality (e.g., texts in another language). Although previous studies along this line have been mainly based on the third language, in this work, we propose a novel and more general framework to utilize arbitrary multimedia content (e.g., images) as the pivot. Nowadays, we can easily find abundant monolingual text documents with rich multimedia content as the side information, e.g., text with photos or videos posted to social networking sites and blogs. These visual media are expected to be more or less correlated to the counterpart texts following the objective of a document. Considering that we can generally understand the content of images taken in other countries regardless of our own language, visual information can be a universal representation to ground different languages.\nMoreover, in recent years, performance of visual recognition has been dramatically improved owing to the huge success of deep learning, where it is now considered to be on a human level for generic image recognition [19]. We expect that these state-of-the-art visual recognition techniques are now mature enough to accurately extract language-agnostic semantics of images to help improve natural language processing (NLP) tasks. If multimedia pivot-based machine translation is established, we could possibly utilize abundant monolingual multimedia documents naturally provided by Web users to build high-performance and open-domain MT systems.\nOur contributions in this study are as follows: 1) To the best of our knowledge, we are the first to\npropose a zero-resource (i.e., no direct parallel corpus) machine translation method that utilizes multimedia as the pivot. Importantly, pivot images are required only in the training phase. 2) To realize this, we propose a neural network based method combining multimodal (cross-modal) representation learning and encoder-decoder models. We note that our model can align source encoder and target decoder without source-to-target path during training which is often utilized by pseudo corpus based methods. Moreover, our idea is agnostic to implementations of\nencoder and decoder networks. These properties makes our approach applicable not only to MT but also to broad range of tasks. 3) We taxonomized several possible approaches in model topology and learning strategies and extensively investigated their performance."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Resource Problem in Cross-lingual Learning", "text": "Dealing with limitations in the number of good-quality parallel or comparable corpora has been one of the most important issues in cross-lingual learning. One straightforward approach is to automatically mine parallel corpora, typically from noisy Web repositories. Some methods exploited a bootstrap approach starting from base translation systems [33], whereas others utilized external meta information such as links to the same URL to coupling bilingual texts [25]. Among them, images have also been exploited as a key for crosslingual document matching in relatively early works. However, these methods simply rely on OCR reading or near-duplicate (copy) detection of images [22], and thus they cannot identify similarities in semantics, which is a fundamental limitation as compared to our work.\nAnother line of work has been to train MT system from non-parallel data with the help of another modality for indirect knowledge transfer, which is called the pivot-based machine translation. Most recent works have focused on existing popular language to use as the pivot [36], [37], [5]. While creating direct parallel corpora in minor language pairs is practically very difficult, major languages (e.g., English) are relatively often coupled to each language. Source-to-target translation can be realized by first translating the source language into the pivot language and then translating it into the target language. Nonetheless, this method still assumes that source-pivot and pivot-target parallel corpora are available, which would require the effort of human experts if the languages are minor ones. Moreover, it is difficult to use images as the pivot in this approach because explicitly decoding an image from text is not a well-established technique. Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35]."}, {"heading": "B. Computer Vision for Machine Translation", "text": "Grounding a natural language to real-world representations has always been an important topic in NLP, for which computer vision would be the first natural choice [28]. After a huge breakthrough in the use of convolutional neural networks (CNNs) [19], visual recognition has been significantly advanced in terms of both accuracy and flexibility, enabling the development of many brand-new technologies. Amongst them, image captioning, which automatically annotates a description for an input image with natural language, has become one of the hottest topics in recent years [34], [13]. Because image captioning is essentially interpreted as \u201dtranslation\u201d from an\nimage to sentence, it has drawn more and more attention in the NLP community as well.\nRecently, a new research field called multimodal machine translation was proposed [4], [10], which became a subtask in WMT 2016.1 The aim of this task is to use images in addition to source languages as inputs to improve the translation performance, hopefully relaxing ambiguity in alignment that cannot be solved by texts only. The feasibility of this approach has been demonstrated by some methods, such as the neural machine translation model with image input [3] and visualbased reranking of SMT results [10]. However, these methods assume images are available as a part of a query in the testing phase, and thus the objective and setup are entirely different from ours."}, {"heading": "C. Multimodal Embedding", "text": "To use non-language multimedia as the pivot for MT, we need a more flexible mechanism to semantically align different types of data. The key idea here is to derive one common representation shared by all modalities. In other words, whatever the modality is, observed data belonging to the same implicit concept should be mapped into roughly the same point in the embedding space. The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7]. In more recent methods based on deep neural networks, pairwise ranking loss has been shown to significantly improve multimodal embedding [6] owing to its natural capability of learning discriminative nearest-neighbor metrics and stability in gradient-based learning. It was successfully used for image captioning within the framework of the deep encoder-decoder model [16].\nIn this work, we simultaneously optimize source-pivot (image) and pivot-target losses with a shared pivot encoder to implicitly align two languages in the multimodal space, which is the core of our zero-shot learning. We further put a target sequence decoder on top of the multimodal representation to compose an end-to-end encoder-decoder model. From the theoretical viewpoint, our work is in the line of some recently proposed methods in the form of multi-stream encoder-decoder model. We look into these models and ours in detail and describe our contribution in Section III-D."}, {"heading": "III. OUR APPROACH", "text": ""}, {"heading": "A. Overview", "text": "Our goal is to build a translation model from a source language s to a target language t by utilizing the side information (images) as the pivot. Below, we call a pair of a text description d and its counterpart image i a \u201ddocument.\u201d For training the system, suppose that we have Ns monolingual documents in the source language, T s = {dsk, i s k} Ns\nk=1. Similarly, we also have N t documents in the target language, T t = {dtk, i t k} Nt k=1.\n1http://www.statmt.org/wmt16/multimodal-task.html\nE\ns and Et are forced to have high correlations with the image encoder Ev in the multimodal space, on which the decoder of the target language Dt is trained. In the testing phase, translation can be realized by simply feedforwarding through Es and Dt.\nImportantly, T s and T t do not overlap; they do not share the same images at all. While ds and dt obviously appear in different spaces, is and it share a common visual space and can be handled by the same encoder. We let Es(ds), Et(dt), and Ev(i) denote non-linear encoders (i.e., feature extractors) for source language descriptions, target language descriptions, and images, respectively.\nOur model can be divided into roughly two important components. The first component is the multimodal representation learning, in which the parameters of the encoders, Es(ds), Et(dt), and Ev(i), are optimized so that they are mapped into the same semantic space, which we call \u201dthe multimodal space.\u201d If such a good multimodal space is obtained, instances of all modalities should have roughly the same vector representation as long as they are tied together with similar semantic concepts. The second component is to build a target language decoder, Dt, on top of the multimodal space so that the final translation can be realized by Dt (Es(ds)). It should be emphasized that we only need texts for input during the testing phase, similar to standard machine translation.\nFigures 1 illustrates our approach. There are several options in the model topology and training strategies that are thoroughly compared in the experiments. We describe the details in the following sections."}, {"heading": "B. Model Topologies", "text": "We use the pair-wise rank loss proposed in [6] for training encoders to map them to one common multimodal space. For the two-way model, we take the source-image loss as follows (Fig. 1: Top):\nJE 2w(T\ns) = \u2211\nis\n\u2211\nng\nmax{0, \u03b1\u2212 s (Ev(is), Es(ds))\n+ s ( Ev(is), Es(dsng) ) },\n(1)\nwhere \u03b1 is the hyperparameter of margin and the similarity score function, s(), measures the dot product. Note that the outputs of each encoder are unit normalized and thus it is equal to cosine similarity. dsng denotes negative (not coupled) descriptions for is sampled from the same minibatch.\nFor training the decoder, images it in T t are feedforwarded and used as the inputs to measure decoder loss against dt. We take the standard cross-entropy loss.\nJDim(T t) = \u2212\n\u2211\ndt\n1\n|dt|\n|dt| \u2211\nk=1\nlogP ( wk|D t(Ev(it)) ) , (2)\nwhere P (wk) is the probability that the model outputs the ground truth word at step k. Our two-way model is closely related to the image-captioning model proposed by [16] except that we apply different languages to the encoder and decoder parts. This is viewed as an end-to-end fusion of multimodal embedding and image-captioning models.\nIn the three-way model, we further incorporate rank loss on T t in addition to T s (Fig. 1: Bottom) for training the encoders. The encoder loss for the three-way model is defined as follows:\nJE 3w(T\ns, T t) = \u2211\nis\n\u2211\nng\nmax{0, \u03b1\u2212 s (Ev(is), Es(ds))\n+ s ( Ev(is), Es(dsng) ) }+ \u2211\nit\n\u2211\nng\nmax{0,\n\u03b1\u2212 s ( Ev(it), Et(dt) ) + s ( Ev(it), Et(dtng) ) }.\n(3)\nThe advantages of the three-way model over the two-way model are many. First, while image-target alignment is ignored in the two-way model, images implicitly bind source and target languages by jointly enforcing high correlations between them in the three-way model. Thus, multimodal representation itself is expected to be improved for bridging the gap between two languages. Moreover, simultaneously optimizing two constraints would have a positive regularization effect in a manner similar to that of so-called multi-task learning. Second, unlike in the two-way model, the three-way model can utilize both images and descriptions in T t for training decoders of the target language because now they are mapped into a common representation. This is interpreted as a sort of data augmentation and is expected to further improve robustness. The loss for reconstructing target descriptions is as follows:\nJDde(T t) = \u2212\n\u2211\ndt\n1\n|dt|\n|dt| \u2211\nk=1\nlogP ( wk|D t(Et(dt)) ) . (4)\nWe can use either JDim, J D de, or both for training the decoder in the three-way approach. Now, the model can be viewed as a fusion of multimodal embedding, image captioning, and autoencoder of target languages."}, {"heading": "C. Training Strategy", "text": "We investigate two strategies for training the whole model. The first strategy is the two-step approach, in which we first optimize the encoder loss, JE . Then we fix the parameters for all encoders and start optimizing the decoder with respect to JD. The second strategy is the end-to-end approach, in which we jointly optimize encoder and decoder losses. Here we use the combined loss,\nJall = JD + \u03bbJE , (5)\nwhere \u03bb is a weighting parameter."}, {"heading": "D. Difference from Closely Related Methods", "text": "Although our work is, as far as we know, the first attempt of zero-resource machine translation using multimedia pivot, there have been some theoretically close methods that inspired our model. The topology of our network is similar to recently proposed many-to-one sequence-to-sequence model [21]. However, their model is designed for standard multitask learning and does not have a multimodal embedding layer like ours. Therefore, it cannot align a source encoder and a target decoder in zero-shot situations. To deal with\nzero-shot problem, Firat et al.[5] incorporated some synthetic parallel corpora to explicitly include source-to-target path during training. In other words, they approach the zero-shot problem in data-side while we approach in model-side with the help of multimodal embedding technique.\nAs for the pivot-based multimodal representation learning, Funaki et al.[7] and Rajendran et al.[24] used basically the same idea as our multimodal space, implemented with generalized CCA and neural encoders respectively. A major difference is that there models have no cross-modal decoders because their interest was the multimodal representation (embedding) itself. We will show that simultaneously optimizing decoders have positive effects not only for decoding but also for the learned representation itself. Saha et al.[26] proposed an endto-end model of multimodal embedding and target decoder, which is almost identical to our two-way model except that their multimodal fusion is based on correlation loss. As we have described above, our three-way model including the target encoder in multimodal learning have many advantages. In fact, it can significantly improve both the multimodal representation and decoding performance compared to twoway model as we show in the experiments."}, {"heading": "IV. EXPERIMENT", "text": ""}, {"heading": "A. Data Set", "text": "For our study, we used two publicly available multilingual image-description datasets. The IAPR-TC12 dataset [8] has 20,000 images with their English and German descriptions. The original descriptions were provided in German, and their English translations were added by professionals. The recently published Multi30K dataset [4] is specifically designed for research of multimodal machine translation. It has roughly 30,000 images with English and German descriptions for each image. This is an extension of Flickr30K [38], an imagecaption dataset in English, for which German translations are provided by [4]. In this experiment, we used the annotations for the multimodal machine translation task at WMT\u201916. Because the ground-truth German descriptions for the original testing set are reserved for competition and are not publicly available, we used the original validation set for our testing\nset. We then selected 1,000 random samples from the original training set to compose our validation set.\nFor preprocessing, all words were converted into lowercase and tokenized using Natural Language Toolkit, and then those appearing less than 5 times in the training splits were replaced by UNK symbol. Table I summarizes the statistics of the datasets and our experimental setup. We randomly split data into non-overlapping sets for training, validation, and testing. Unnecessary modalities for each split (e.g., German descriptions for Image-English split) were ignored. It is notable that we had no direct English-German parallel data, even in the validation sets.\nUnfortunately, we should say that our datasets are very small compared to standard studies on neural machine translation, although these are the largest multi-lingual image-description datasets as far as we know. We note that our work is in the very beginning stage where our focus is to show the feasibility of zero-shot translation using multimedia pivot, as well as to investigate how each component in our model affects the relative improvements in performance."}, {"heading": "B. Experimental Setup", "text": "Because the choice of encoders and decoders for each modality is not within the scope of this paper, we used the most standard neural models for each domain. For visual encoder Ev , we employed the public VGG-16 network [29], which is one of the most powerful and widely used CNNs pre-trained on the ImageNet dataset [2]. We extracted features from the \u201dfc7\u201d layer of VGG-16 and put another two fully connected (FC) layers with 1024 hidden units each, only which are tuned during the training. For language encoders and decoders Es, Et, and Dt, we used recurrent neural networks (RNNs) with long short-term memory (LSTM) [11]. We used 512- dimensional word embedding and 1024-dimensional hidden units. Note that the dimensions of all encoders should be equal so that they can be coupled in multimodal space. We used the Adam optimizer [15] with minibatch size 32 for training the network, and we stopped optimization when the validation loss no longer improved. We fixed \u03b1 = 0.1 and \u03bb = 100 through our experiments.\nFor evaluation, we used the standard BLEU metrics [23]. However, we found that it was difficult to properly evaluate the\nperformance using the standard BLEU (up to 4-gram) metrics, especially on Multi30K, because it was originally designed for image-captioning tasks and the lengths of the sentences are quite short. Therefore, in this study, we mainly focused on the BLEU+1 metrics [20], which is a modification of BLEU that has smoothing terms for higher-order n-grams, making it possible to evaluate MT performance on short sentences.\nTable II summarizes the results on baseline models. To demonstrate the performance we could obtain with a supervised parallel corpus, we show the scores on sequence-tosequence NMT [30] trained on the same RNN architectures changing the number of randomly sampled parallel data.\nC. In-depth Study of Multimodal Space\nTo separately evaluate the effectiveness of the multimodal space, we first focused on simple nearest-neighbor-based translation. Namely, for a query description in the source language, dsq , we retrieved its nearest-neighbor training sample in T\nt and then simply output its description. This experiment essentially measures the retrieval performance and is appropriate for evaluating the multimodal representation itself.\nAs a baseline, we implemented a naive method based on TFIDF and CNN visual features. For a query, we first retrieved the most similar document in T s in terms of cosine similarity of TFIDF text features. Then, for the coupled image of that document, we retrieved the nearest document in T t in terms of the L2 distance of CNN features (i.e., VGG-16 fc7 layer) whose caption would be output as the translation result.\nIn the multimodal space obtained by our two-way model, we can retrieve the nearest image in the target side, T t, by computing the dot score, which is the criterion we used in the ranking loss (Eq. 1).\nd\u0302t 2w = d t y where y = arg max\nk\ns ( Ev(itk), E s(dsq) ) (6)\nFor the three-way model, in addition to image-based retrieval, we can directly retrieve the nearest description in T t, which is an interesting characteristic of this model.\nd\u0302t 3w = d t y where\ny =\n\n\n\narg max k\ns (\nEv(itk), E s(dsq)\n)\n(image-based)\narg max k\ns ( Et(dtk), E s(dsq) ) (description-based)\n(7)\nTable III shows the results of the nearest-neighbor methods. \u201dwith dec.\u201d represents a multimodal space jointly trained with decoder while others indicate independently trained ones (i.e., the first step in the two-step approach). For reference, we also noted the performance when we randomly sampled a description in T t. As expected, the three-way model generally outperformed the two-way model. Interestingly, we observed that the performance was further improved when we directly retrieved descriptions on the target side. This fact indicates that descriptions projected into the multimodal space still represent some useful information not apparent in the images.\nWe hypothesize that jointly optimizing multimodal embedding loss and decoder loss (end-to-end model) may result in a better multimodal space because decoder learning can be a good constraint in a multi-task learning framework. As shown in the result, \u201dwith dec.\u201d models generally achieve better performance on IAPR-TC12, but not on Multi30K. This is reasonable because independently trained multimodal space is poor on IAPR-TC12 but relatively good on Multi30K as the comparison with \u201dTFIDF + CNN feature\u201d baseline suggests."}, {"heading": "D. Main Results and Discussion", "text": "Table IV shows a detailed comparison of our approach in different configurations. Comparing with the baselines (Table II), our best results are comparable to sequence-to-sequence models using parallel corpora roughly 20% as large as our monolingual ones. We summarize our findings below.\nA comparison of model topologies shows that the three-way models generally outperformed their two-way counterparts. However, when only images were feed-forwarded for training decoder, the differences in performance were subtle, and the two-way model sometimes outperformed the three-way model. The most attractive aspect of the three-way approach is that we can use both image and description for decoder training, which always provided the best results. We can possibly utilize external monolingual corpora to further improve decoders, which we would like to investigate in our future work.\nAs for training strategy, end-to-end training achieved better results than the two-step approach on IAPR-TC12, but there was no significant difference on Multi30K. For IAPR-TC12, as the results of the nearest-neighbor experiment suggest, the multimodal space itself was relatively poor (sometimes outperformed by the TFIDF baseline). In such a case, jointly optimizing the multimodal space (encoders) and the decoder seemed to significantly improve the performance. This result also corresponds to the observation in the previous section.\nWe show the loss curves of English to German translation task on our three-way models. Figure 2 and 3 show the results on IAPR-TC12 and Multi30K, respectively. Note that the training (validation) loss cannot be directly compared to the test loss because they are based on entirely different criteria. Nonetheless, we can see that validation loss and test loss converge in similar timings, making it possible to tune the network properly. Another observation is that decoder training seems to be overfitting earlier on Multi30K. This might be another reason that end-to-end approach showed no significant improvement on this dataset. This point should be further investigated with larger data."}, {"heading": "V. CONCLUSION", "text": "In this work, we tackled a challenging task of training an NMT system from just monolingual data containing multimedia side information. Unlike many previous studies that used\nmultimedia simply in addition to texts as inputs to reinforce machine translation, we used no parallel corpora for training or image inputs in the testing phase. Our system was made possible by training multimodal encoders to share common modality-agnostic semantic representation using images as the pivot. We compared several possible implementations and showed the feasibility of our approach. Notably, we found the three-way model to be particularly promising in terms of both performance and flexibility in handling various modalityspecific data. Although our target in this paper was a fully unsupervised setup, we can naturally include some parallel data in a semi-supervised manner or external monolingual text corpora in the target language to further enhance performance, which is an attractive direction for future research.\nOf course, the experimental results also suggest that we have a long way to go. There is still a significant gap in performance as compared to supervised sequence-to-sequence baselines. We expect this gap to further reduce as we use more expressive visual encoders, powerful attention mechanisms, and multimodal learning methods, all of which have remarkably improved in recent years. Moreover, our current method is intrinsically limited to the domain where texts can be grounded to visual content, which is not always the case\nin generic documents. We would like to extend our approach to handle other side information and investigate how far we can go on automatically crawled noisy Web data, which is an important milestone to realizing true zero-resource MT utilizing abundant multimedia monolingual documents on the Web."}], "references": [{"title": "Learning Bilingual Lexicons Using the Visual Similarity of Labeled Web Images", "author": ["S. Bergsma", "B. Van Durme"], "venue": "Proc. IJCAI, 2011, pp. 1764\u20131769.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A Large-scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. IEEE CVPR, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-language Image Description with Neural Sequence Models", "author": ["D. Elliott", "S. Frank", "E. Hasler"], "venue": "arXiv preprint arXiv:1510.04709, 2015, pp. 1\u201314.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": "arXiv preprint arXiv:1605.00459, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Zeroresource Translation with Multi-lingual Neural Machine Translation", "author": ["O. Firat", "B. Sankaran", "Y. Al-Onaizan", "F.T.Y. Vural", "K. Cho"], "venue": "Proc. EMNLP, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Devise: A Deep Visual-semantic Embedding Model", "author": ["A. Frome", "G. Corrado", "J. Shlens"], "venue": "Proc. NIPS, 2013, pp. 1\u201311.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Image-Mediated Learning for Zero-shot Cross-lingual Document Retrieval", "author": ["R. Funaki", "H. Nakayama"], "venue": "Proc. EMNLP, 2015, pp. 585\u2013 590.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The IAPR TC- 12 Benchmark: A New Evaluation Resource for Visual Information Systems", "author": ["M. Gr\u00fcbinger", "P. Clough", "H. M\u00fcller", "T. Deselaers"], "venue": "Proc. LREC, 2006, pp. 13\u201323.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Canonical Correlation Analysis: An Overview with Application to Learning Methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-taylor"], "venue": "Neural Computation, vol. 16, no. 12, pp. 2639\u20132664, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["J. Hitschler", "S. Riezler"], "venue": "Proc. ACL, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Long Short-term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1\u201332, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Relations Between Two Sets of Variants", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, pp. 321\u2013377, 1936.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1936}, {"title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "Proc. IEEE CVPR, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Bilingual Lexicon Induction with Transferred ConvNet Features", "author": ["D. Kiela", "I. Vulic", "S. Clark"], "venue": "Proc. EMNLP, 2015, pp. 148\u2013 158.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proc. ICLR, 2014, pp. 1\u201313.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Proc. NIPS, 2014, pp. 1\u201313.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Proc. of Machine Translation Summit, vol. 11, 2005, pp. 79\u201386.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. NIPS, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Orange: A Method for Evaluating Automatic Evaluation Metrics for Machine Translation", "author": ["C.-Y. Lin", "F.J. Och"], "venue": "Proc. COLING, 2004, pp. 501\u2013507.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Multitask Sequence to Sequence Learning", "author": ["M.-t. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "Proc. ICLR, 2016, pp. 1\u201310.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Issues in Cross-Language Retrieval from Document Image Collections", "author": ["D. Oard"], "venue": "Proc. of Symposium on Document Image Understanding Technology, 1999, pp. 229 \u2013 234.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "BLEU : A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-j. Zhu"], "venue": "Proc. ACL, 2002, pp. 311\u2013318.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "author": ["J. Rajendran", "M.M. Khapra", "S. Chandar", "B. Ravindran"], "venue": "Proc. NAACL-HLT, 2016, pp. 171\u2013181.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic Parallel Fragment Extraction from Noisy Data", "author": ["J. Riesa", "D. Marcu"], "venue": "Proc. NAACL, 2012, pp. 538\u2013542.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation", "author": ["A. Saha", "M.M. Khapra", "S. Chandar", "J. Rajendran", "K. Cho"], "venue": "Proc. COLING, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "Proc. ACL, 2016, pp. 1\u20139.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["C. Silberer", "M. Lapata"], "venue": "Proc. ACL, 2014, pp. 721\u2013732.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recoginition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proc. NIPS, 2014, pp. 3104\u20133112.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The Sentence-Aligned European Patent Corpus", "author": ["W. Taeger"], "venue": "Proc. EAMT, 2011, pp. 177\u2013184.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving the Multilingual User Experience of Wikipedia Using Cross-Language Name Search", "author": ["R. Udupa", "M.M. Khapra"], "venue": "Proc. NAACL, 2010, pp. 492\u2013500.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Large Scale Parallel Document Mining for Machine Translation", "author": ["J. Uszkoreit", "J. Ponte", "A.C. Popat", "M. Dubiner"], "venue": "Proc. COLING, 2010, pp. 1101\u20131109.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Show and Tell : A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. IEEE CVPR, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-Modal Representations for Improved Bilingual Lexicon Learning", "author": ["I. Vuli", "D. Kiela", "S. Clark", "M.-F. Moens"], "venue": "Proc. ACL, 2016, pp. 188\u2013194.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Pivot Language Approach for Phrase-based Statistical Machine Translation", "author": ["H. Wu", "H. Wang"], "venue": "Machine Translation, vol. 21, no. 3, pp. 165\u2013181, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Revisiting Pivot Language Approach for Machine Translation", "author": ["\u2014\u2014"], "venue": "Proc. IJCNLP-ACL, vol. 1, no. August, 2009, p. 154.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics (TACL), vol. 2, no. April, pp. 67\u201378, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "Irrespective of traditional statistical machine translation (SMT) [18] or modern neural machine translation (NMT) [30], methods and data have always been mutually indispensable to each other.", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "Recent state-of-the-art NMT systems have shown that translation can be surprisingly improved with sufficiently large-scale data and high computational power [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 16, "context": "In some specific domains such as Web news, patents, and Wikipedia, relatively high-quality multilingual translations are made available by content holders or volunteer workers, which have been utilized by researchers for decades [17], [31].", "startOffset": 229, "endOffset": 233}, {"referenceID": 29, "context": "In some specific domains such as Web news, patents, and Wikipedia, relatively high-quality multilingual translations are made available by content holders or volunteer workers, which have been utilized by researchers for decades [17], [31].", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": "Moreover, in recent years, performance of visual recognition has been dramatically improved owing to the huge success of deep learning, where it is now considered to be on a human level for generic image recognition [19].", "startOffset": 216, "endOffset": 220}, {"referenceID": 31, "context": "Some methods exploited a bootstrap approach starting from base translation systems [33], whereas others utilized external meta information such as links to the same URL to coupling bilingual texts [25].", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "Some methods exploited a bootstrap approach starting from base translation systems [33], whereas others utilized external meta information such as links to the same URL to coupling bilingual texts [25].", "startOffset": 197, "endOffset": 201}, {"referenceID": 20, "context": "However, these methods simply rely on OCR reading or near-duplicate (copy) detection of images [22], and thus they cannot identify similarities in semantics, which is a fundamental limitation as compared to our work.", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "Most recent works have focused on existing popular language to use as the pivot [36], [37], [5].", "startOffset": 80, "endOffset": 84}, {"referenceID": 35, "context": "Most recent works have focused on existing popular language to use as the pivot [36], [37], [5].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "Most recent works have focused on existing popular language to use as the pivot [36], [37], [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35].", "startOffset": 221, "endOffset": 224}, {"referenceID": 13, "context": "Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35].", "startOffset": 226, "endOffset": 230}, {"referenceID": 33, "context": "Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35].", "startOffset": 232, "endOffset": 236}, {"referenceID": 26, "context": "Grounding a natural language to real-world representations has always been an important topic in NLP, for which computer vision would be the first natural choice [28].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "After a huge breakthrough in the use of convolutional neural networks (CNNs) [19], visual recognition has been significantly advanced in terms of both accuracy and flexibility, enabling the development of many brand-new technologies.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "Amongst them, image captioning, which automatically annotates a description for an input image with natural language, has become one of the hottest topics in recent years [34], [13].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "Amongst them, image captioning, which automatically annotates a description for an input image with natural language, has become one of the hottest topics in recent years [34], [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "Recently, a new research field called multimodal machine translation was proposed [4], [10], which became a subtask in WMT 2016.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "Recently, a new research field called multimodal machine translation was proposed [4], [10], which became a subtask in WMT 2016.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "The feasibility of this approach has been demonstrated by some methods, such as the neural machine translation model with image input [3] and visualbased reranking of SMT results [10].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "The feasibility of this approach has been demonstrated by some methods, such as the neural machine translation model with image input [3] and visualbased reranking of SMT results [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 239, "endOffset": 242}, {"referenceID": 30, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 290, "endOffset": 294}, {"referenceID": 6, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 294, "endOffset": 297}, {"referenceID": 5, "context": "In more recent methods based on deep neural networks, pairwise ranking loss has been shown to significantly improve multimodal embedding [6] owing to its natural capability of learning discriminative nearest-neighbor metrics and stability in gradient-based learning.", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "It was successfully used for image captioning within the framework of the deep encoder-decoder model [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "We use the pair-wise rank loss proposed in [6] for training encoders to map them to one common multimodal space.", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "Our two-way model is closely related to the image-captioning model proposed by [16] except that we apply different languages to the encoder and decoder parts.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "The topology of our network is similar to recently proposed many-to-one sequence-to-sequence model [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "[5] incorporated some synthetic parallel corpora to explicitly include source-to-target path during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] and Rajendran et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[24] used basically the same idea as our multimodal space, implemented with generalized CCA and neural encoders respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] proposed an endto-end model of multimodal embedding and target decoder, which is almost identical to our two-way model except that their multimodal fusion is based on correlation loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The IAPR-TC12 dataset [8] has 20,000 images with their English and German descriptions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "The recently published Multi30K dataset [4] is specifically designed for research of multimodal machine translation.", "startOffset": 40, "endOffset": 43}, {"referenceID": 36, "context": "This is an extension of Flickr30K [38], an imagecaption dataset in English, for which German translations are provided by [4].", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "This is an extension of Flickr30K [38], an imagecaption dataset in English, for which German translations are provided by [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 27, "context": "For visual encoder E , we employed the public VGG-16 network [29], which is one of the most powerful and widely used CNNs pre-trained on the ImageNet dataset [2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "For visual encoder E , we employed the public VGG-16 network [29], which is one of the most powerful and widely used CNNs pre-trained on the ImageNet dataset [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "For language encoders and decoders E, E, and D, we used recurrent neural networks (RNNs) with long short-term memory (LSTM) [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "We used the Adam optimizer [15] with minibatch size 32 for training the network, and we stopped optimization when the validation loss no longer improved.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "For evaluation, we used the standard BLEU metrics [23].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "Therefore, in this study, we mainly focused on the BLEU+1 metrics [20], which is a modification of BLEU that has smoothing terms for higher-order n-grams, making it possible to evaluate MT performance on short sentences.", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "To demonstrate the performance we could obtain with a supervised parallel corpus, we show the scores on sequence-tosequence NMT [30] trained on the same RNN architectures changing the number of randomly sampled parallel data.", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the \u201dpivot\u201d, we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.", "creator": "LaTeX with hyperref package"}}}