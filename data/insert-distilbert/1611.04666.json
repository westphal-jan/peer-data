{"id": "1611.04666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "A Generic Coordinate Descent Framework for Learning from Implicit Feedback", "abstract": "in recent years, interest in recommender research has shifted from explicit feedback measures towards implicit feedback data. a further diversity of complex models has been proposed for a wide economic variety of applications. despite this, learning from implicit feedback databases is still computationally challenging. so far, most work relies on stochastic displacement gradient descent ( sgd ) solvers which are easy to physically derive, but in practice challenging to apply, especially when for tasks identified with many items. for the simple matrix factorization model, proving an efficient functional coordinate descent ( cd ) graphical solver has been previously proposed. however, efficient graphical cd approaches have not been derived for more complex models.", "histories": [["v1", "Tue, 15 Nov 2016 01:32:33 GMT  (160kb,D)", "http://arxiv.org/abs/1611.04666v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["immanuel bayer", "xiangnan he", "bhargav kanagal", "steffen rendle"], "accepted": false, "id": "1611.04666"}, "pdf": {"name": "1611.04666.pdf", "metadata": {"source": "CRF", "title": "A Generic Coordinate Descent Framework for Learning from Implicit Feedback", "authors": ["Immanuel Bayer", "Xiangnan He", "Bhargav Kanagal", "Steffen Rendle"], "emails": ["immanuel.bayer@uni-", "xiangnan@comp.nus.edu.sg", "bhargav@google.com", "srendle@google.com"], "sections": [{"heading": null, "text": "In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models."}, {"heading": "1. INTRODUCTION", "text": "In recent years, the focus of recommender system research has shifted from explicit feedback problems such as rating prediction to implicit feedback problems. Most of the signal that a user provides about her preferences is implicit. Examples for implicit feedback are: a user watches a video, clicks on a link, etc. Implicit feedback data is much cheaper to obtain than explicit feedback, because it comes with no extra cost for the user and thus is available on a much larger scale. However, learning a recommender system from implicit feedback is computationally expensive because the observed actions of a user need to be contrasted against all the non-observed actions [5, 13].\nStochastic gradient descent (SGD) and coordinate descent (CD) are two widely used algorithms for large scale machine learning. Both algorithms are considered state-of-the-art for learning matrix factorization models from implicit feedback and have been studied extensively. SGD and CD have shown different strengths and weaknesses on various data sets [4,\n\u2217Work done at Google.\n17, 16, 8, 25, 15, 22, 26]. While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10]. In fact, it is even unknown if CD can be used to efficiently optimize complex recommender models. Our work closes this gap and identifies a model property called k-separability, that is a sufficient condition to allow efficient learning from implicit feedback. Based on k-separability, we provide a general framework to derive efficient implicit CD solvers.\nOur paper is organized as follows: First, we introduce the problem of learning from implicit feedback and show that the number of implicit training examples makes the application of standard algorithms challenging. Next, we provide our general framework for efficient implicit learning with CD. We identify k-separability of a model as a sufficient property to make efficient learning feasible and introduce iCD, a generic learning algorithm for k-separable models. In Section 5, we show how to apply iCD to a diverse set of models, including, matrix factorization (MF), factorization machines (FM) and tensor factorization. This section serves both as solutions to popular models as well as a guide for applying the framework to other complex recommender models.\nTo summarize, our contributions are:\n\u2022 We identify a basic property of recommender models that allows efficient CD learning from implicit data.\n\u2022 We provide iCD, a framework to derive efficient implicit CD algorithms.\n\u2022 We apply the framework and derive algorithms for MF, MF with side information, FM, PARAFAC and Tucker Decomposition."}, {"heading": "2. RELATED WORK", "text": "Since several years, matrix factorization (MF) is regarded as the most effective, basic recommender system model. Two optimization strategies dominate the research on MF from implicit feedback data. The first one is Bayesian Personalized Ranking (BPR) [13], a stochastic gradient descent (SGD) framework, that contrasts pairs of consumed to nonconsumed items. The second one is coordinate descent (CD) also known as alternating least squares on an elementwise loss over both the consumed and non-consumed items [5]. In terms of the loss formulation, BPR\u2019s pairwise classification\nar X\niv :1\n61 1.\n04 66\n6v 1\n[ cs\n.I R\n] 1\n5 N\nov 2\nloss is better suited for ranking whereas CD loss is better suited for numerical data. With regard to the optimization task, both techniques face the same challenge of learning over a very large number of training examples. BPR tackles this issue by sampling negative items, but it has been shown that BPR has convergence problems when the number of items is large [7, 12]. It requires more complex, nonuniform, sampling strategies for dealing with this problem [12, 6]. On the other hand, for CD-MF, Hu et al. [5] have derived an efficient algorithm that allows to optimize over the large number of non-consumed items without any cost. This computational trick is exact and does not involve sampling. Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26]. This large body of results indicates that the advantages of CD and BPR are orthogonal and both approaches have their merits.\nOur discussion so far was focused on learning matrix factorization models from implicit data. Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24]. However, work on complex factorization models relies almost exclusively on SGD optimization using the generic BPR framework. Our work, provides the theory as well as a practical framework for deriving CD learners for such complex models. Like CD for MF, our generic algorithm is able to optimize on all nonconsumed items without explicitly iterating over them. To summarize, our paper enables researchers and practitioners to apply CD in their work and gives them a choice between the advantages of BPR and CD."}, {"heading": "3. PROBLEM STATEMENT", "text": "Let I be a set of items and C a set of contexts. Let S be a set of observed feedback where a tuple (c, i, y, \u03b1) \u2208 S indicates that in context c, a score y has been assigned to item i with confidence \u03b1. See Figure 1 for an illustration. We use a general notation of context which can include for instance user, time, location, attributes, history, etc. Section 5 and Section 6 show more examples for context."}, {"heading": "3.1 Recommender Model", "text": "A recommender model y\u0302 : C \u00d7 I \u2192 R is a function that assigns a score to every context-item pair. The model y\u0302 is parameterized by a set of model parameters \u0398. The model y\u0302 is typically used to decide which items to present in a given context.\nThe learning task is to find the values of the model parameters that minimize a loss over the data S, e.g., a squared loss\nL(\u0398|S) = \u2211\n(c,i,y,\u03b1)\u2208S \u03b1 (y\u0302(c, i)\u2212 y)2 + \u2211 \u03b8\u2208\u0398 \u03bb\u03b8\u03b8 2 (1)\nwhere \u03bb\u03b8 is an regularization constant for parameter \u03b8."}, {"heading": "3.2 Coordinate Descent Algorithm", "text": "Objective (1) can be minimized by coordinate descent (CD). CD iterates through the model parameters and updates one parameter at a time. For a selected parameter \u03b8 \u2208 \u0398, CD computes the first L\u2032 and second derivative L\u2032\u2032 of L with respect to the selected coordinate \u03b8:\nL\u2032(\u03b8|S) = 2 \u2211\n(c,i,y,\u03b1)\u2208S\n\u03b1 (y\u0302(c, i)\u2212 y)y\u0302\u2032(c, i) + 2\u03bb\u03b8 \u03b8 (2)\nL\u2032\u2032(\u03b8|S) = 2 \u2211\n(c,i,y,\u03b1)\u2208S\n\u03b1 [(y\u0302(c, i)\u2212 y)y\u0302\u2032\u2032(c, i) + y\u0302\u2032(c, i)2] + 2\u03bb\u03b8\n(3)\nand performs a Newton update step:\n\u03b8 \u2190 \u03b8 \u2212 \u03b7 L \u2032(\u03b8|S)\nL\u2032\u2032(\u03b8|S) (4)\nwhere \u03b7 \u2208 (0, 1] is the step size. For multilinear models, a full step, i.e., \u03b7 = 1, can be chosen without risking divergence [11]. All models in Section 5 fall into this category.\nSuch CD algorithms have been well studied and the runtime complexity is typically linear in the complexity of the training examples and embedding dimension. For MF, [23] shows a complexity of O(|S| k) and for FM, [11] derives a complexity of O(NZ(X) k) where NZ(X) is the number of non-zero entries in the design matrix X. The linear runtime complexity in the number of training examples makes these algorithms well suited for explicit recommendation settings, however, they become infeasible for implicit problems."}, {"heading": "3.3 Learning from Implicit Feedback", "text": "In an implicit recommendation problem, the non-consumed items are meaningful and cannot be ignored. For instance, in Figure 1 (right), the data depicts how often each item was consumed in a context in the past. The non-consumed items, i.e., the ones with a count of zero, are useful to learn user preferences. To formalize, the training data Simpl of an implicit problem consists of a set S+ of observed feedback and all the non-consumed tuples S0\nSimpl = S + \u222a S0, |Simpl| = |C| |I| (5)\nwith\n\u2200(c, i, y, \u03b1) \u2208 S0 : y = 0, \u03b1 = \u03b10. (6)\nS+ contains the observed feedback and is of much smaller scale than Simpl, usually |S+| |C| |I|.\nThe implicit learning problem can be stated as minimizing the objective in eq. (1) over the implicit data Simpl. While possible in theory, in practice, it is infeasible to apply the learning algorithms of Section 3.2 to this problem due to their linear computational runtime in the size of the training data which is |Simpl| = |C||I| for implicit problems. Our paper shows how to derive efficient CD algorithms for optimizing eq. (1) over implicit data."}, {"heading": "4. GENERIC COORDINATE DESCENT ALGORITHM FOR IMPLICIT FEEDBACK", "text": ""}, {"heading": "4.1 Implicit Regularizer", "text": "As discussed in Section 3.3, the reason why training on implicit data is challenging is the large number of implicit examples S0 which is typically |S0| \u2208 O(|C||I|). Note that S0 includes all context-item pairs that are not in S+. We show now that we can rephrase the optimization criterion to sum over all context-item pairs. This reformulation is a prerequisite to later allow the decomposition of the loss in Section 4.2. Moreover it allows to study implicit optimization without having to consider S+.\nLemma 1. Implicit learning can be rephrased as a combination of learning on a small positive set and minimizing the scoring function on any context-item pair.\nargmin \u0398 L(\u0398|Simpl) = argmin \u0398\n( L(\u0398|S) + \u03b10 \u2211 c\u2208C \u2211 i\u2208I\ny\u0302(c, i)2\ufe38 \ufe37\ufe37 \ufe38 =:R(\u0398)\n)\n(7)\nwhere the observed feedback is rescaled\nS := {( c, i, \u03b1\n\u03b1\u2212 \u03b10 y, \u03b1\u2212 \u03b10\n) : (c, i, y, \u03b1) \u2208 S+ } . (8)\nProof. Per definition of the loss (eq. 1) and the implicit training set Simpl (eq. 5)\nL(\u0398|Simpl) = L(\u0398|S+) + \u03b10 \u2211\n(c,i)\u2208S0 y\u0302(c, i)2\n=L(\u0398|S+)\u2212 L(\u0398|{(c, i, 0, \u03b10) : (c, i, y, \u03b1) \u2208 S+}) + \u03b10R(\u0398)\nWe further can collapse each pair of examples into a single one. We show this for the pair (c, i, y, \u03b1) \u2208 S and its counterpart (c, i, 0,\u2212\u03b10).\nL(\u0398|{(c, i, y, \u03b1)}) + L(\u0398|{(c, i, 0,\u2212\u03b10)}) =\u03b1 (y\u0302(c, i)\u2212 y)2 \u2212 \u03b10 y\u0302(c, i)2\n=(\u03b1\u2212 \u03b10) ( y\u0302(c, i)2 \u2212 2 \u03b1\n\u03b1\u2212 \u03b10 y y\u0302(c, i) +\n\u03b1 \u03b1\u2212 \u03b10 y2 )\n=(\u03b1\u2212 \u03b10) ( y\u0302(c, i)\u2212 \u03b1\n\u03b1\u2212 \u03b10 y\n)2 + const\n=L ( \u0398| {( c, i, \u03b1\n\u03b1\u2212 \u03b10 y, \u03b1\u2212 \u03b10\n)}) + const\nThe additional constant does not change the optimum for \u0398, so rescaling of examples as in eq. (8) preserves the optimum.\nThe lemma allows an interesting interpretation of implicit learning tasks. Implicit problems can be seen as explicit or one-class problems with an additional implicit regularizer or bias R(\u0398) for predicting zeros. Compared to a common regularizer such as L2, the implicit regularizer is aware of the model y\u0302. L2 penalizes non-zero model parameters \u0398 whereas the implicit regularizer penalizes non-zero predictions y\u0302. Consequently, the implicit regularizer is less restrictive than L2 because small predictions can be achieved even with large model parameters.\n4.2 iCD Algorithm for k-separable Models As shown in eq. (7), implicit learning can be formulated as explicit learning on a small set S with an expensive implicit regularizer R. Learning models over an explicit loss is already well studied [23, 11], so we focus now on the implicit regularizer\nR(\u0398) = \u2211 c\u2208C \u2211 i\u2208I y\u0302(c, i)2 (9)\nThe general computational complexity is O(|C||I|). In this section, we introduce the concept of a k-separable model. We will provide an efficient implicit CD solver for any k-separable model. In Section 5, we show that many common models are k-separable, including matrix factorization, feature-based approaches such as factorization machines, but also higher-order tensor factorization such as PARAFAC or Tucker decomposition. The iCD framework that we derive in this section is not limited to the models described above but can serve as a blueprint for other k-separable models as well.\nDefinition 1 (k-separable). A model y\u0302(c, i) is called k-separable iff the model can be rewritten as\ny\u0302(c, i) = \u3008\u03c6(c),\u03c8(i)\u3009 = k\u2211 f=1 \u03c6f (c)\u03c8f (i) (10)\nwith functions\n\u03c6 : C \u2192 Rk, \u03c8 : I \u2192 Rk (11)\nwhere \u03c6 is parameterized by \u0398C and \u03c8 is parameterized by \u0398I with \u0398C \u2229\u0398I = \u2205.\nLemma 2. The implicit regularizer of any k-separable model can be decomposed to:\nR(\u0398) = k\u2211 f=1 k\u2211 f \u2032=1 \u2211 c\u2208C\n\u03c6f (c)\u03c6f \u2032(c)\ufe38 \ufe37\ufe37 \ufe38 =:JC(f,f \u2032)\n\u2211 i\u2208I\n\u03c8f (i)\u03c8f \u2032(i)\ufe38 \ufe37\ufe37 \ufe38 =:JI (f,f \u2032)\n(12)\nProof. The lemma follows from inserting the k-separable model (eq. 10) into the implicit regularizer (eq. 9) and rearranging the summations.\nR(\u0398) = \u2211 c\u2208C \u2211 i\u2208I k\u2211 f=1 \u03c6f (c)\u03c8f (i) k\u2211 f \u2032=1 \u03c6f \u2032(c)\u03c8f \u2032(i)\n= k\u2211 f=1 k\u2211 f \u2032=1 (\u2211 c\u2208C \u03c6f (c)\u03c6f \u2032(c) )(\u2211 i\u2208I \u03c6f (i)\u03c6f \u2032(i) )\nThis lemma is key to efficient learning algorithms from implicit data. It shows that the context and item sides can be computed independently, which drops the computational complexity from O(|C| |I|) to O((|C| + |I|) k2). Next, we show how this can be used for gradient computation which is required for the update step in CD (see eq. 4).\nLemma 3. The implicit regularizer gradients of any kseparable model with respect to any model parameter \u03b8 \u2208 \u0398C (or analogously \u03b8 \u2208 \u0398I), can be simplified to\nR\u2032(\u03b8) = 2 k\u2211 f=1 k\u2211 f \u2032=1 JI(f, f \u2032) \u2211 c\u2208C \u03c6f (c)\u03c6 \u2032 f \u2032(c) (13)\nAlgorithm 1 Generic Implicit CD\n1: procedure iCD-Generic(S,C, I) 2: \u0398\u2190 N (0, \u03c3) 3: repeat 4: Compute \u03a6 and \u03a8 if necessary 5: Compute JI 6: for \u03b8 \u2208 \u0398C do 7: Compute L\u2032(\u03b8|S), L\u2032\u2032(\u03b8|S) 8: Compute R\u2032(\u03b8), R\u2032\u2032(\u03b8) 9: \u03b8 \u2190 \u03b8 \u2212 \u03b7 L \u2032(\u03b8|S)+\u03b10 R\u2032(\u03b8)\nL\u2032\u2032(\u03b8|S)+\u03b10 R\u2032\u2032(\u03b8) 10: Update \u03a6 if necessary 11: end for 12: Apply step 5 to 11 to the items. 13: until converged 14: return \u0398 15: end procedure\nR\u2032\u2032(\u03b8) = 2 k\u2211 f=1 k\u2211 f \u2032=1 JI(f, f \u2032) \u2211 c\u2208C [ \u03c6f (c)\u03c6 \u2032\u2032 f \u2032(c) + \u03c6 \u2032 f (c)\u03c6 \u2032 f \u2032(c) ] (14)\nProof. The lemma follows from deriving eq. (12).\nThis lemma shows that computing R\u2032 and R\u2032\u2032 of any context parameter is independent of |I|.\nFrom the analysis follows the recipe to derive an efficient iCD learning algorithm for a model y\u0302. First, rewrite the model as a dot product of \u03c6 and \u03c8. Second, construct the first and second derivative of \u03c6 and \u03c8 with respect to any model parameter \u03b8 \u2208 \u0398. These results allow to compute R\u2032(\u03b8) and R\u2032\u2032(\u03b8) for any model parameter \u03b8 \u2208 \u0398 efficiently. With these gradients for the expensive implicit regularizer, a Newton step can be applied. Algorithm 1 shows a generic iCD algorithm using the ideas of this section.\nMost models allow some further optimizations: (i) When the gradients of \u03c6 or \u03c8 are sparse, some of the summands of eqs. (13, 14) drop. (ii) The model parameters usually have some structure which can be used for traversing the model parameters more systematically. We will show both of these steps in the next section for a variety of models."}, {"heading": "5. APPLICATIONS", "text": "In this section, we apply iCD to two classes of complex factorization models, namely feature-based factorization models and tensor factorization models. We have chosen these two classes because they are very powerful and frequently used. Moreover each of them has some interesting properties with respect to deriving iCD algorithms. The provided algorithms can be directly applied to many common recommender system tasks. This section also serves as a guide for deriving iCD algorithms in general."}, {"heading": "5.1 Matrix Factorization (MF)", "text": "We start by applying our framework to matrix factorization (see Figure 2). For MF, the scoring function is\ny\u0302(c, i) := \u3008wc,hi\u3009 = k\u2211 f=1 wc,f hi,f (15)\nwith model parameters \u0398 = {W,H} where W \u2208 RC\u00d7k and H \u2208 RI\u00d7k.\nAlgorithm 2 Implicit CD for MF\n1: procedure iCD-MF(S,C, I) 2: W,H \u2190 N (0, \u03c3) 3: repeat 4: for f\u2217 \u2208 {1, . . . , k} do 5: for f \u2208 {1, . . . , k} do 6: Compute JI(f\n\u2217, f) 7: end for 8: for c\u2217 \u2208 C do 9: Compute L\u2032(wc\u2217,f\u2217 |S), L\u2032\u2032(wc\u2217,f\u2217 |S)\n10: Compute R\u2032(wc\u2217,f\u2217), R \u2032\u2032(wc\u2217,f\u2217) 11: wc\u2217,f\u2217\u2190wc\u2217,f\u2217\u2212 L\u2032(wc\u2217,f\u2217 |S)+\u03b1R \u2032(wc\u2217,f\u2217 ) L\u2032\u2032(wc\u2217,f\u2217 |S)+\u03b1R\u2032\u2032(wc\u2217,f\u2217 ) 12: end for 13: Apply step 5 to 12 to the items. 14: end for 15: until converged 16: return W,H 17: end procedure\nA MF model is trivially k-separable with\n\u03c6f (c) = wc,f , \u03c8f (i) = hi,f . (16)\nFurthermore, the gradients are sparse\n\u2202\u03c6f (c) \u2202wc\u2217,f\u2217 =\n{ 1, if c = c\u2217 \u2227 f = f\u2217\n0, else (17)\nand all second derivatives are 0. Thus, the regularizer derivatives simplify to\nR\u2032(wc\u2217,f\u2217) = 2 k\u2211 f=1 JI(f, f \u2217)wc\u2217,f (18)\nR\u2032\u2032(wc\u2217,f\u2217) = 2 JI(f \u2217, f\u2217) (19)\nThe derivation is symmetric for the item side. As MF associates each model parameter with an embedding dimension f , we can traverse the parameters one dimension at a time. A full step \u03b7 = 1 can be taken because MF is bilinear. Algorithm 2 shows the full procedure.\nThe computation of JI(f \u2217, \u00b7) is trivially in O(|I| k). Gradient computation of the implicit regularizer is O(k) per parameter and for the explicit part O(|S|) for all parameters. Overall, the algorithm has a complexity of O((|I| + |C|) k2 + |S| k) per iteration."}, {"heading": "5.2 Feature-Based Factorization Models", "text": "One of the most powerful extension of MF is feature based modeling for the context and item. Feature-based factorization models are strictly more powerful than MF and have shown large improvements in many applications (e.g. [2, 11]). For instance, the cold-start problem is commonly solved by replacing or complementing user and item ids with user and item attributes [2]. Another example is context-aware recommendation, where the context is represented by several variables, e.g. location or time in addition to the user id. Also sequential models can be represented by feature based modeling [6].\nLearning general feature-based models on implicit feedback was restricted to BPR so far. This is the first work that provides an implicit CD algorithm for this important model class.\nTo formalize the problem, assume each c \u2208 C is represented by a feature vector xc \u2208 Rp and each i \u2208 I by a feature vector zi \u2208 Rp. See Figure 3 for an illustration.\n5.2.1 MF with Side Information (MFSI) We start with a feature based extension of matrix factor-\nization similar to [2]:\ny\u0302(c, i) = xcW (ziH) t = k\u2211 f=1 ( p\u2211 l=1 xc,l wl,f )( p\u2211 l=1 zi,l hl,f ) (20)\nwith \u0398 = {W,H}. MFSI is k-separable using\n\u03c6f (c) = p\u2211 l=1 xc,l wl,f , \u03c8f (i) = p\u2211 l=1 zi,l hl,f (21)\nand the gradients are sparse\n\u2202\u03c6f (c) \u2202wl\u2217,f\u2217 =\n{ xc,l\u2217 , if f = f \u2217\n0, else (22)\nDue to sparse gradients of \u03c6 and \u03c8, the first and second\nAlgorithm 3 Implicit CD for MF with Side Information\n1: procedure iCD-MFSide(S,C, I) 2: W,H \u2190 N (0, \u03c3) 3: repeat 4: Compute \u03a6 and \u03a8 5: for f\u2217 \u2208 {1, . . . , k} do 6: for f \u2208 {1, . . . , k} do 7: Compute JI(f\n\u2217, f) 8: end for 9: for l\u2217 \u2208 {1, . . . , p} do\n10: Compute L\u2032(wl\u2217,f\u2217 |S), L\u2032\u2032(wl\u2217,f\u2217 |S) 11: Compute R\u2032(wl\u2217,f\u2217), R \u2032\u2032(wl\u2217,f\u2217) 12: wl\u2217,f\u2217\u2190wl\u2217,f\u2217\u2212 L\u2032(wl\u2217,f\u2217 |S)+\u03b1R \u2032(wl\u2217,f\u2217 ) L\u2032\u2032(wl\u2217,f\u2217 |S)+\u03b1R\u2032\u2032(wl\u2217,f\u2217 ) 13: Update \u03a6 14: end for 15: Apply step 6 to 14 to the items. 16: end for 17: until converged 18: end procedure\nregularizer derivatives simplify to:\nR\u2032(wl\u2217,f\u2217) = 2 k\u2211 f=1 JI(f, f \u2217) \u2211 c\u2208C xc,l\u2217 \u03c6f (c) (23)\nR\u2032\u2032(wl\u2217,f\u2217) = 2 JI(f \u2217, f\u2217) \u2211 c\u2208C x2c,l\u2217 (24)\nNote that the sums over the context variable depend only on context where xc,l\u2217 6= 0, so with a sparse iterator, the computation is O(kNZ(X)) for optimizing all of the context variables in a given embedding layer f\u2217.\nThis computation assumes that \u03a6 and \u03a8 are given. Obviously, while optimizing W , \u03a8 does not change and while optimizing H, \u03a6 does not change. However, while optimizing W , \u03a6 changes but can be kept in sync with changes in W by updating:\n\u03c6f\u2217(c)\u2190 \u03c6f\u2217(c) + xc,l\u2217(wnewl\u2217,f\u2217 \u2212 woldl\u2217,f\u2217) (25)\nThe item side can be derived analogously. The total runtime of Algorithm 3 for one epoch over all variables isO(k2 (NZ(X)+ NZ(Z))) for the implicit regularizer.\n5.2.2 Factorization Machines The Factorization Machine (FM) model [11] is a more\ncomplex factorized model that includes biases and interactions between all variables. In general, the FM for a feature vector x \u2208 Rp is defined as\ny\u0302(x) = b+ p\u2211 l=1 xl w\u0303l + \u2211 l=1 \u2211 l\u2032>l \u3008wl,wl\u2032\u3009xlx\u2032l (26)\nwhere b is a global bias parameter, w\u0303 are feature biases and W are the embeddings. In our case, for a context-item pair (c, i), we set the input feature vector x as the concatenation of the context and item feature vectors: x := (xc,zi).\nThe FM model is (k + 2)-separable, with\n\u03c6f (c) = p\u2211 l=1 xc,l wl,f , \u03c8f (i) = p\u2211 l=1 zi,l hl,f , (27)\n\u03c6k+1(c) = b+ p\u2211 l=1 xc,l w\u0303l + \u2211 l=1 \u2211 l\u2032>l \u3008wl,wl\u2032\u3009xc,lxc,l\u2032 , (28)\n\u03c8k+1(i) = 1, (29)\n\u03c6k+2(c) = 1, (30)\n\u03c8k+2(c) = p\u2211 l=1 zi,l h\u0303l + \u2211 l=1 \u2211 l\u2032>l \u3008hl,hl\u2032\u3009zi,lzi,l\u2032 . (31)\nwhere for the context, \u03c6 is parameterized by w\u0303 \u2208 Rp for the linear part and W \u2208 Rp\u00d7k for the factors. And analogously for items, \u03c8 is parameterized by h\u0303 \u2208 Rp for the linear part and H \u2208 Rp\u00d7k for the factors.\nThe gradients are sparse:\n\u2202\u03c6f (c)\n\u2202w\u0303l\u2217 =\n{ xc,l\u2217 , if f = k + 1\n0, else (32)\n\u2202\u03c6f (c) \u2202wl\u2217,f\u2217 =  xc,l\u2217 , if f = f \u2217\nxc,l\u2217(\u03c6f\u2217(c)\u2212 xc,l\u2217wl\u2217,f\u2217), if f = k + 1 0, else\n(33)\nSimilar to MFSI, due to the sparsity in gradients, one of the nested loops drops for the first regularizer derivative R\u2032 and both nested sums drop for the second regularizer derivative R\u2032\u2032. Consequently, the flow and runtime analysis for FM is the same as for MFSI."}, {"heading": "5.3 Tensor Factorization", "text": "Tensor factorization generalizes matrix factorization and deals with problems that involve more than two categorical variables. For instance, in personalized recommendation of tags for bookmarks [20], the context consists of two variables, the user C1 and the bookmark C2, and the item I corresponds to the tag. For personalized web search [19], the context consists of the user C1 and the query C2 and the item I to the web page. The data can be seen as a three mode tensor over C1, C2 and I. Figure 4 shows an example of how observations over context C \u2286 C1 \u00d7 C2 and items I translate to a tensor. A tensor factorization model tries to approximate the tensor with a low rank decomposition (see Figure 5). Although tensor factorization models are multilinear, we show that they fit well into our framework.\nAdditionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the\ntensor data is dense, i.e., the empty parts in the tensor in Figure 4 are filled with zeros. This would imply that context combinations that never have been observed, are used for training as well, i.e., C = C1 \u00d7 C2. In some applications, this might not make sense, for instance if C1 encodes a device type and C2 encodes an operating system version. Our iCD framework works for both sparse and dense context. We will point out the differences when necessary.\n5.3.1 Parallel Factor Analysis (PARAFAC) We first discuss the Parallel Factor Analysis (PARAFAC) [3]\nmodel which is a 3-mode extension of matrix factorization.\ny\u0302(c1, c2, i) := k\u2211 f=1 uc1,f vc2,f wi,f (34)\nwith \u0398 = {U, V,W} where U \u2208 RC1\u00d7k, V \u2208 RC2\u00d7k and W \u2208 RI\u00d7k. PARAFAC is k-separable with\n\u03c6f (c1, c2) = uc1,f vc2,f , \u03c8f (i) = wi,f (35)\nAgain, gradients are sparse:\n\u2202\u03c6f (c1, c2)\n\u2202uc\u22171 ,f\u2217 =\n{ vc2,f , if c1 = c \u2217 1 \u2227 f = f\u2217\n0, else (36)\nand the loss derivatives simplify to\nR\u2032(uc\u22171 ,f\u2217) = 2 k\u2211 f=1 JI(f, f \u2217)uc\u2217,f \u2211 c2:(c \u2217 1 ,c2)\u2208C vc2,f vc2,f\u2217\n(37)\nR\u2032\u2032(uc\u22171 ,f\u2217) = 2 JI(f \u2217, f\u2217) \u2211 c2:(c \u2217 1 ,c2)\u2208C vc2,f\u2217 vc2,f\u2217 (38)\nThe item side is equivalent to matrix factorization. If the context is dense and includes all possible combinations of context variables, i.e., if C = C1 \u00d7 C2, then the computation of JC(f, f \u2032), can be decomposed to:\nJC(f, f \u2032) = \u2211 c1\u2208C\nuc1,f uc1,f \u2032\ufe38 \ufe37\ufe37 \ufe38 =:JC1 (f,f \u2032)\n\u2211 c2\u2208C\nvc2,f vc2,f \u2032\ufe38 \ufe37\ufe37 \ufe38 =:JC2 (f,f \u2032)\n(39)\nThis means, the computation is in O(|C1|+ |C2|) instead of O(|C1| |C2|). On the other hand if C is sparse and contains only the subset of the observed context combinations, i.e., C \u2282 C1 \u00d7 C2, then there is no need for decomposing this sum. The same applies to the loss derivatives of eqs. (37,38): Again, if all possible context is modeled, then {c2 : (c\u22171, c2) \u2208 C} = C2 and thus JC2(f, f \u2032) can replace the sum over C2.\nThe overall runtime for PARAFAC\u2019s implicit regularizer is O((|C|+ |I|) k2) for sparse context and O((|C1|+ |C2|+ |I|) k2) for dense context. The traversal over model parameters can be arranged as in the MF algorithm.\n5.3.2 Tucker Decomposition Tucker Decomposition (TD) [21] is a generalization of\nPARAFAC which computes all interactions between the factor matrices. The strength of each interaction is given by a core tensor B. For our running example with two context variables c1, c2 and one item variable i, TD is defined as\ny\u0302(c1, c2, i) = k1\u2211 f1=1 k2\u2211 f2=1 k3\u2211 f3=1 bf1,f2,f3uc1,f1 vc2,f2 wi,f3 (40)\nwith \u0398 = {B,U, V,W} where B \u2208 Rk1\u00d7k2\u00d7k3 is the core tensor and U \u2208 R|C1|\u00d7k1 , V \u2208 R|C2|\u00d7k2 andW \u2208 R|I|\u00d7k3 . TD is much more computationally expensive than PARAFAC, requiring O(k1 k2 k3) operations just for evaluating the model on one data point.\nEven though Tucker decomposition contains nested sums, it is k3-separable with\n\u03c6f (c1, c2) = k1\u2211 f1=1 k2\u2211 f2=1 bf1,f2,fuc1,f1 vc2,f2 , \u03c8f (i) = wi,f\nThe derivatives of these functions are:\n\u2202\u03c6f (c1, c2)\n\u2202uc1,f\u22171 =\n{\u2211k2 f2=1 bf\u22171 ,f2,f vc2,f2 , if c1 = c \u2217 1\n0, else (41)\n\u2202\u03c6f (c1, c2)\n\u2202vc2,f\u22172 =\n{\u2211k1 f1=1 bf1,f\u22172 ,f uc1,f1 , if c2 = c \u2217 2\n0, else (42)\n\u2202\u03c6f (c1, c2) \u2202bf\u22171 ,f\u22172 ,f\u22173 =\n{ uc1,f\u22171 vc2,f\u22172 , if f = f \u2217 3\n0, else (43)\n\u2202\u03c8f (i) \u2202wi\u2217,f\u22173 =\n{ 1, if f = f\u22173 \u2227 i = i\u2217\n0, else (44)\nUnlike all the other models we have presented so far, the gradients for \u03c6 are non-zero for any factor index f \u2208 {1, . . . , k}. Consequently, the nested loops over factors of the loss gradient (eq. 13) cannot be improved further. However, for \u03c8, which is sparse, the same optimization as in the other models can be applied.\nLike for PARAFAC, if C is dense, i.e., C = C1 \u00d7 C2, we can precompute intermediate matrices for C1 and C2 and the computation of JC(f, f \u2032) simplifies to\nk1\u2211 f1=1 k1\u2211 f \u20321=1 k2\u2211 f2=1 k2\u2211 f \u20322=1 bf1,f2,f bf \u20321,f \u20322,f \u2032JC1(f1, f \u2032 1)JC2(f2, f \u2032 2)\nIf C is sparse, there is no need for this optimization and we can use a straightforward computation of JC . The overall runtime complexities are O(k21 k22 k23 (|C1| + |C2| + |I|)) for dense context and O(k21 k22 k23 (|C|+ |I|)) for sparse context."}, {"heading": "6. EXPERIMENTS", "text": "The main objective of the experiments is to illustrate the generality of the iCD framework. We show how iCD can be applied to a variety of recommender problems that cannot be solved with MF alone. For MF models, efficient coordinate\ndescent algorithms (CD) have been previously proposed [5] and its performance compared against gradient descent algorithms such as BPR [13]. Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8]. The purpose of our experiments is not to compare BPR and CD on yet another dataset, but rather to demonstrate the versatility of the iCD framework and illustrate how it can serve as a building block for future research on complex recommender models. As with MF, it is likely that both iCD and BPR will show strengths in different applications."}, {"heading": "6.1 Experimental Setup", "text": "We evaluate on a dataset of 200, 000 users interacting with YouTube. Our subset contains |I| = 68, 000 videos. The dataset also contains side information about age, country, gender and device info. We apply iCD to three popular recommendation problems \u2013 Cold-Start, Offline Recommendation, and Instant Recommendation (see Section 6.2). We compare the following algorithms:\n\u2022 Popularity: a static recommender that returns the most popular videos.\n\u2022 Coview: returns based on the previously watched video, the most commonly chosen next video.\n\u2022 iCD-MF: user-item matrix factorization using iCD for optimization, similar to [5].\n\u2022 iCD-FM: a factorization machine with varying features for the context (Section 5.2). We report results for different feature choices.\nWe measure the recall and NDCG for the top 100 returned videos. Note that we report relative improvements over the Popularity recommender. All hyperparameters are tuned on a separate tuning holdout set."}, {"heading": "6.2 Results", "text": "6.2.1 Cold-Start Recommendation In the Cold-Start recommendation [2] scenario, we assume\nthat a user interacts with the recommender system for the first time. To simulate this scenario, we select a random subset of users and hold out all their events for evaluation purposes; we train on the remaining users.\nThe common approach for dealing with cold-start is to represent a user by side information [2]. Here, we use the feature-based FM model (iCD-FM) with the user\u2019s age, gender, country and device info as context features. Figure 7 shows that attribute-aware FM achieves a 2x improvement over the baselines. As expected, neither MF nor Coview can do any better than most-popular recommendation.\n6.2.2 Offline Recommendation In the Offline Recommendation scenario, we hold out the\nlast feedback of each user and use all the previous feedback for training. This is the most commonly used protocol to evaluate the performance of a recommender algorithm. We experiment with multiple FM models: (1) iCD-FM A: an FM with user attributes, (2) iCD-FM P: a sequential FM that only uses the previously watched video (similar to FPMC [14] or Coview) and (3) iCD-FM A+P+U: an FM\nthat uses all signals: attributes, previously watched video and user id (similar to FPMC [14] with user attributes). As shown in Figure 6a, the complex FM model with all features achieves the best quality, illustrating the flexibility of feature engineering with iCD.\n6.2.3 Instant Recommendation In large-scale industrial applications, online training is of-\nten not feasible due to complex serving stacks. Commonly, models are periodically trained offline (e.g., every day or week) and applied on a stream of user interactions. When the model is queried to generate recommendations for a user, all feedback until the current time is taken into account for prediction. We simulate this setting by choosing a global cutoff time where all the events before the cutoff are used for training and all the remaining ones for evaluation.\nIn such settings, models relying on user ids, such as MF, cannot capture recent feedback. Instead, describing a user by the sequence of previously watched videos allows for instant personalization. Such a model can be configured using a feature-based FM model (Section 5.2) and we experiment with four configurations (1) iCD-FM A: FM using user attributes, (2) iCD-FM P: a sequential FM based on the previously watched video, (3) iCD-FM H: a FM based on all previously watched videos, (4) iCD-FM A+P+H: an FM combining all signals. As expected, the complex FM model with all features achieves the best quality. Again, we would like to note the generality of the iCD framework, which enables flexible feature engineering."}, {"heading": "6.3 Computational Costs", "text": "As stated in Section 3.3, any conventional CD solver, e.g. [11], could solve the implicit feedback problem. Now, we substantiate that this is infeasible because of the large number of implicit examples. Figure 8 compares the computational cost for learning an FM with a conventional CD to the costs of iCD on our dataset with 70k items. We use three different context features from Figure 6. The plot shows relative costs to iCD-FM P. For all three context choices, conventional CD shows four orders of magnitude higher compuational costs than iCD. The empirical measured runtime for iCD was in the order of minutes; consequently, CD\u2019s four order of magnitude increase in runtime translates to weeks of training for each iteration. Clearly, using a conventional CD solver to optimize the implicit loss directly is infeasible."}, {"heading": "7. CONCLUSION", "text": "In this work, we have presented a general, efficient framework for learning recommender system models from implicit feedback. First, we have shown that learning from implicit feedback can be reformulated as optimizing a cheap explicit loss and an expensive implicit regularizer. Then we have introduced the concept of k-separable models. We have shown that the implicit regularizer of any k-separable model can be computed efficiently without iterating over all context-item pairs. Finally, we have shown that many popular recommender models are k-separable, including matrix factorization, factorization machines and tensor factorization. Moreover, we have provided efficient learning algorithms for these models based on our framework. Our framework is not limited to the models discussed in the paper but designed to serve as a general blueprint for deriving learning algorithms for recommender systems."}, {"heading": "8. REFERENCES", "text": "[1] C. Cheng, H. Yang, M. R. Lyu, and I. King. Where\nYou Like to Go Next: Successive Point-of-Interest Recommendation. In IJCAI, volume 13, pages 2605\u20132611, 2013.\n[2] Z. Gantner, L. Drumond, C. Freudenthaler, S. Rendle, and L. Schmidt-Thieme. Learning attribute-to-feature mappings for cold-start recommendations. In 2010 IEEE International Conference on Data Mining, pages 176\u2013185. IEEE, 2010.\n[3] R. A. Harshman. Foundations of the PARAFAC procedure: Models and conditions for an \u201dexplanatory\u201d multi-modal factor analysis. UCLA Working Papers in Phonetics, 16(1):84, 1970.\n[4] R. He and J. McAuley. VBPR: Visual bayesian personalized ranking from implicit feedback. In D. Schuurmans and M. P. Wellman, editors, AAAI, pages 144\u2013150. AAAI Press, 2016.\n[5] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for implicit feedback datasets. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM \u201908, pages 263\u2013272, 2008.\n[6] B. Kanagal, A. Ahmed, S. Pandey, V. Josifovski, J. Yuan, and L. Garcia-Pueyo. Supercharging recommender systems using taxonomies for learning user purchase behavior. Proc. VLDB Endow., 5(10):956\u2013967, June 2012.\n[7] B. McFee, T. Bertin-Mahieux, D. P. Ellis, and G. R. Lanckriet. The million song dataset challenge. In Proceedings of the 21st International Conference on World Wide Web, WWW \u201912 Companion, pages 909\u2013916, New York, NY, USA, 2012. ACM.\n[8] X. Ning and G. Karypis. Slim: Sparse linear methods for top-n recommender systems. In 2011 IEEE 11th International Conference on Data Mining, pages 497\u2013506. IEEE, 2011.\n[9] W. Pan and L. Chen. GBPR: Group Preference Based Bayesian Personalized Ranking for One-Class Collaborative Filtering. In IJCAI, volume 13, pages 2691\u20132697, 2013.\n[10] I. Pila\u0301szy, D. Zibriczky, and D. Tikk. Fast als-based matrix factorization for explicit and implicit feedback datasets. In Proceedings of the fourth ACM conference on Recommender systems, pages 71\u201378. ACM, 2010.\n[11] S. Rendle. Factorization machines with libfm. ACM Trans. Intell. Syst. Technol., 3(3):57:1\u201357:22, may 2012.\n[12] S. Rendle and C. Freudenthaler. Improving pairwise learning for item recommendation from implicit feedback. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM \u201914, pages 273\u2013282, New York, NY, USA, 2014. ACM.\n[13] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, pages 452\u2013461, Arlington, Virginia, United States, 2009. AUAI Press.\n[14] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th International Conference on World Wide Web,\nWWW \u201910, pages 811\u2013820. ACM, 2010.\n[15] S. Sedhain, A. K. Menon, S. Sanner, and D. Braziunas. On the effectiveness of linear models for one-class collaborative filtering. In Proceedings of the 30th Conference on Artificial Intelligence (AAAI-16), 2016.\n[16] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, A. Hanjalic, and N. Oliver. TFMAP: optimizing MAP for top-n context-aware recommendation. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 155\u2013164. ACM, 2012.\n[17] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, N. Oliver, and A. Hanjalic. CLiMF: learning to maximize reciprocal rank with collaborative less-is-more filtering. In Proceedings of the sixth ACM conference on Recommender systems, pages 139\u2013146. ACM, 2012.\n[18] E. Shmueli, A. Kagian, Y. Koren, and R. Lempel. Care to comment?: recommendations for commenting on news stories. In Proceedings of the 21st international conference on World Wide Web, pages 429\u2013438. ACM, 2012.\n[19] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. Cubesvd: A novel approach to personalized web search. In Proceedings of the 14th International Conference on World Wide Web, WWW \u201905, pages 382\u2013390, New York, NY, USA, 2005. ACM.\n[20] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. A unified framework for providing recommendations in social tagging systems based on ternary semantic analysis. IEEE Trans. on Knowl. and Data Eng., 22(2):179\u2013192, Feb. 2010.\n[21] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:279\u2013311, 1966.\n[22] M. Volkovs and G. W. Yu. Effective latent models for binary feedback in recommender systems. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 313\u2013322. ACM, 2015.\n[23] H.-F. Yu, C.-J. Hsieh, S. Si, and I. Dhillon. Scalable coordinate descent approaches to parallel matrix factorization for recommender systems. In Proceedings of the 12th International Conference on Data Mining, ICDM \u201912, pages 765\u2013774, 2012.\n[24] X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM \u201914, pages 283\u2013292. ACM, 2014.\n[25] T. Zhao, J. McAuley, and I. King. Leveraging social connections to improve personalized ranking for collaborative filtering. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM \u201914, pages 261\u2013270, New York, NY, USA, 2014. ACM.\n[26] T. Zhao, J. McAuley, and I. King. Improving latent factor models via personalized feature projection for one class recommendation. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 821\u2013830. ACM, 2015."}], "references": [{"title": "Where You Like to Go Next: Successive Point-of-Interest Recommendation", "author": ["C. Cheng", "H. Yang", "M.R. Lyu", "I. King"], "venue": "In IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Learning attribute-to-feature mappings for cold-start recommendations", "author": ["Z. Gantner", "L. Drumond", "C. Freudenthaler", "S. Rendle", "L. Schmidt-Thieme"], "venue": "In 2010 IEEE International Conference on Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an \u201dexplanatory\u201d multi-modal factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1970}, {"title": "VBPR: Visual bayesian personalized ranking from implicit feedback", "author": ["R. He", "J. McAuley"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Supercharging recommender systems using taxonomies for learning user purchase behavior", "author": ["B. Kanagal", "A. Ahmed", "S. Pandey", "V. Josifovski", "J. Yuan", "L. Garcia-Pueyo"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "The million song dataset challenge", "author": ["B. McFee", "T. Bertin-Mahieux", "D.P. Ellis", "G.R. Lanckriet"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW \u201912 Companion,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Sparse linear methods for top-n recommender systems", "author": ["X. Ning", "G. Karypis. Slim"], "venue": "In 2011 IEEE 11th International Conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "GBPR: Group Preference Based Bayesian Personalized Ranking for One-Class Collaborative Filtering", "author": ["W. Pan", "L. Chen"], "venue": "In IJCAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Fast als-based matrix factorization for explicit and implicit feedback datasets", "author": ["I. Pil\u00e1szy", "D. Zibriczky", "D. Tikk"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Trans. Intell. Syst. Technol.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Improving pairwise learning for item recommendation from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler"], "venue": "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "In Proceedings of the 19th International Conference on World Wide Web,  WWW", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the effectiveness of linear models for one-class collaborative filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "D. Braziunas"], "venue": "In Proceedings of the 30th Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "TFMAP: optimizing MAP for top-n context-aware recommendation", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic", "N. Oliver"], "venue": "In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "CLiMF: learning to maximize reciprocal rank with collaborative less-is-more filtering", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "N. Oliver", "A. Hanjalic"], "venue": "In Proceedings of the sixth ACM conference on Recommender systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Care to comment?: recommendations for commenting on news stories", "author": ["E. Shmueli", "A. Kagian", "Y. Koren", "R. Lempel"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Cubesvd: A novel approach to personalized web search", "author": ["J.-T. Sun", "H.-J. Zeng", "H. Liu", "Y. Lu", "Z. Chen"], "venue": "In Proceedings of the 14th International Conference on World Wide Web, WWW", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A unified framework for providing recommendations in social tagging systems based on ternary semantic analysis", "author": ["P. Symeonidis", "A. Nanopoulos", "Y. Manolopoulos"], "venue": "IEEE Trans. on Knowl. and Data Eng.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, 31:279\u2013311,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1966}, {"title": "Effective latent models for binary feedback in recommender systems", "author": ["M. Volkovs", "G.W. Yu"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["H.-F. Yu", "C.-J. Hsieh", "S. Si", "I. Dhillon"], "venue": "In Proceedings of the 12th International Conference on Data Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Personalized entity recommendation: A heterogeneous information network approach", "author": ["X. Yu", "X. Ren", "Y. Sun", "Q. Gu", "B. Sturt", "U. Khandelwal", "B. Norick", "J. Han"], "venue": "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Leveraging social connections to improve personalized ranking for collaborative filtering", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Improving latent factor models via personalized feature projection for one class recommendation", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "However, learning a recommender system from implicit feedback is computationally expensive because the observed actions of a user need to be contrasted against all the non-observed actions [5, 13].", "startOffset": 189, "endOffset": 196}, {"referenceID": 12, "context": "However, learning a recommender system from implicit feedback is computationally expensive because the observed actions of a user need to be contrasted against all the non-observed actions [5, 13].", "startOffset": 189, "endOffset": 196}, {"referenceID": 12, "context": "While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10].", "startOffset": 133, "endOffset": 140}, {"referenceID": 9, "context": "While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10].", "startOffset": 133, "endOffset": 140}, {"referenceID": 12, "context": "The first one is Bayesian Personalized Ranking (BPR) [13], a stochastic gradient descent (SGD) framework, that contrasts pairs of consumed to nonconsumed items.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "The second one is coordinate descent (CD) also known as alternating least squares on an elementwise loss over both the consumed and non-consumed items [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "BPR tackles this issue by sampling negative items, but it has been shown that BPR has convergence problems when the number of items is large [7, 12].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "BPR tackles this issue by sampling negative items, but it has been shown that BPR has convergence problems when the number of items is large [7, 12].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "It requires more complex, nonuniform, sampling strategies for dealing with this problem [12, 6].", "startOffset": 88, "endOffset": 95}, {"referenceID": 5, "context": "It requires more complex, nonuniform, sampling strategies for dealing with this problem [12, 6].", "startOffset": 88, "endOffset": 95}, {"referenceID": 4, "context": "[5] have derived an efficient algorithm that allows to optimize over the large number of non-consumed items without any cost.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 16, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 15, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 7, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 7, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 24, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 14, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 21, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 25, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 1, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 3, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 17, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 0, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 8, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 23, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 10, "context": ", \u03b7 = 1, can be chosen without risking divergence [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "For MF, [23] shows a complexity of O(|S| k) and for FM, [11] derives a complexity of O(NZ(X) k) where NZ(X) is the number of non-zero entries in the design matrix X.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "For MF, [23] shows a complexity of O(|S| k) and for FM, [11] derives a complexity of O(NZ(X) k) where NZ(X) is the number of non-zero entries in the design matrix X.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "Learning models over an explicit loss is already well studied [23, 11], so we focus now on the implicit regularizer", "startOffset": 62, "endOffset": 70}, {"referenceID": 10, "context": "Learning models over an explicit loss is already well studied [23, 11], so we focus now on the implicit regularizer", "startOffset": 62, "endOffset": 70}, {"referenceID": 1, "context": "[2, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[2, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "For instance, the cold-start problem is commonly solved by replacing or complementing user and item ids with user and item attributes [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Also sequential models can be represented by feature based modeling [6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "1 MF with Side Information (MFSI) We start with a feature based extension of matrix factorization similar to [2]:", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "2 Factorization Machines The Factorization Machine (FM) model [11] is a more complex factorized model that includes biases and interactions between all variables.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "For instance, in personalized recommendation of tags for bookmarks [20], the context consists of two variables, the user C1 and the bookmark C2, and the item I corresponds to the tag.", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "For personalized web search [19], the context consists of the user C1 and the query C2 and the item I to the web page.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "Additionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the k3", "startOffset": 91, "endOffset": 103}, {"referenceID": 19, "context": "Additionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the k3", "startOffset": 91, "endOffset": 103}, {"referenceID": 9, "context": "Additionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the k3", "startOffset": 91, "endOffset": 103}, {"referenceID": 2, "context": "1 Parallel Factor Analysis (PARAFAC) We first discuss the Parallel Factor Analysis (PARAFAC) [3] model which is a 3-mode extension of matrix factorization.", "startOffset": 93, "endOffset": 96}, {"referenceID": 20, "context": "2 Tucker Decomposition Tucker Decomposition (TD) [21] is a generalization of PARAFAC which computes all interactions between the factor matrices.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "For MF models, efficient coordinate descent algorithms (CD) have been previously proposed [5] and its performance compared against gradient descent algorithms such as BPR [13].", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "For MF models, efficient coordinate descent algorithms (CD) have been previously proposed [5] and its performance compared against gradient descent algorithms such as BPR [13].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 24, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 14, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 21, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 25, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 3, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 16, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 15, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 7, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 4, "context": "\u2022 iCD-MF: user-item matrix factorization using iCD for optimization, similar to [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "1 Cold-Start Recommendation In the Cold-Start recommendation [2] scenario, we assume that a user interacts with the recommender system for the first time.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "The common approach for dealing with cold-start is to represent a user by side information [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 13, "context": "We experiment with multiple FM models: (1) iCD-FM A: an FM with user attributes, (2) iCD-FM P: a sequential FM that only uses the previously watched video (similar to FPMC [14] or Coview) and (3) iCD-FM A+P+U: an FM", "startOffset": 172, "endOffset": 176}, {"referenceID": 13, "context": "that uses all signals: attributes, previously watched video and user id (similar to FPMC [14] with user attributes).", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "[11], could solve the implicit feedback problem.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models.", "creator": "LaTeX with hyperref package"}}}