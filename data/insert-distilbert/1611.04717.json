{"id": "1611.04717", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning", "abstract": "count - based exploration algorithms are known to reportedly perform near - optimally when used in conjunction with tabular reinforcement learning ( rl ) methods for solving small discrete markov decision processes ( mdps ). it is generally thought that count - based methods cannot be applied in high - dimensional state spaces, since most states will only occur once. recent deep rl exploration investment strategies are able to deal with high - dimensional continuous state spaces through increasingly complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. in this work, we describe a surprising finding : a simple generalization of the classic count - based approach can reach near state - of - the - art performance on various high - dimensional and / or continuous deep rl benchmarks. states are mapped to hash codes, which allows to count their occurrences with a hash table. these counts are then used to compute a reward bonus according to the classic simple count - based exploration interval theory. we find that simple hash functions can collectively achieve surprisingly exceptionally good results on many challenging tasks. furthermore, we show that a domain - dependent learned hash code may further correctly improve these results. detailed analysis reveals important aspects of a good hash function : 1 ) having appropriate granularity and 2 ) encoding information often relevant to solving the mdp. this exploration strategy achieves near state - of - the - art performance on both continuous control tasks and atari 2600 games, roughly hence providing a simple yet powerful baseline for solving mdps that require considerable exploration.", "histories": [["v1", "Tue, 15 Nov 2016 06:42:24 GMT  (2076kb,D)", "https://arxiv.org/abs/1611.04717v1", "16 pages. Under review as a conference paper at ICLR 2017"], ["v2", "Wed, 11 Jan 2017 18:29:16 GMT  (2154kb,D)", "http://arxiv.org/abs/1611.04717v2", "18 pages. Under review as a conference paper at ICLR 2017"]], "COMMENTS": "16 pages. Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["haoran tang", "rein houthooft", "davis foote", "adam stooke", "xi chen", "yan duan", "john schulman", "filip de turck", "pieter abbeel"], "accepted": true, "id": "1611.04717"}, "pdf": {"name": "1611.04717.pdf", "metadata": {"source": "META", "title": "#Exploration:A Study of Count-Based Explorationfor Deep Reinforcement Learning", "authors": ["Haoran Tang", "Rein Houthooft", "Davis Foote", "Adam Stooke", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) studies an agent acting in an initially unknown environment, learning through trial and error to maximize rewards. It is impossible for the agent to act near-optimally until it has sufficiently explored the environment and identified all of the opportunities for high reward, in all scenarios. A core challenge in RL is how to balance exploration\u2014actively seeking out novel states and actions that might yield high rewards and lead to long-term gains; and exploitation\u2014maximizing short-term rewards using the agent\u2019s current knowledge. While there are exploration techniques for finite MDPs that enjoy theoretical guarantees, there are no fully satisfying techniques for highdimensional state spaces; therefore, developing more general and robust exploration techniques is an active area of research.\nMost of the recent state-of-the-art RL results have been obtained using simple exploration strategies such as uniform sampling (Mnih et al., 2015) and i.i.d./correlated Gaussian noise (Schulman et al., 2015; Lillicrap et al., 2015). Although these heuristics are sufficient in tasks with well-shaped rewards, the sample complexity can grow exponentially (with state space size) in tasks with sparse rewards (Osband et al., 2016b). Recently developed exploration strategies for deep RL have led to significantly improved performance on environments with sparse rewards. Bootstrapped DQN \u2217These authors contributed equally.\nar X\niv :1\n61 1.\n04 71\n7v 2\n[ cs\n.A I]\n1 1\nJa n\n20 17\n(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains.\nSome of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation\u2013Exploration Bonus (MBIE-EB) of Strehl & Littman (2008) counts state-action pairs with a table n(s, a) and adding a bonus reward of the form \u03b2\u221a\nn(s,a) to encourage exploring less visited pairs.\nKolter & Ng (2009) show that the inverse-square-root dependence is optimal. MBIE and related algorithms assume that the augmented MDP is solved analytically at each timestep, which is only practical for small finite state spaces.\nThis paper presents a simple approach for exploration, which extends classic counting-based methods to high-dimensional, continuous state spaces. We discretize the state space with a hash function and apply a bonus based on the state-visitation count. The hash function can be chosen to appropriately balance generalization across states, and distinguishing between states. We select problems from rllab (Duan et al., 2016) and Atari 2600 (Bellemare et al., 2013) featuring sparse rewards, and demonstrate near state-of-the-art performance on several games known to be hard for na\u00efve exploration strategies. The main strength of the presented approach is that it is fast, flexible and complementary to most existing RL algorithms.\nIn summary, this paper proposes a generalization of classic count-based exploration to high-dimensional spaces through hashing (Section 2); demonstrates its effectiveness on challenging deep RL benchmark problems and analyzes key components of well-designed hash functions (Section 3)."}, {"heading": "2 Methodology", "text": ""}, {"heading": "2.1 Notation", "text": "This paper assumes a finite-horizon discounted Markov decision process (MDP), defined by (S,A,P, r, \u03c10, \u03b3,T ), in which S is the state space, A the action space, P a transition probability distribution, r : S \u00d7 A \u2192 R a reward function, \u03c10 an initial state distribution, \u03b3 \u2208 (0, 1] a discount factor, and T the horizon. The goal of RL is to maximize the total expected discounted reward E\u03c0,P [\u2211T t=0 \u03b3 tr (st, at ) ] over a policy \u03c0, which outputs a distribution over actions given a state."}, {"heading": "2.2 Count-Based Exploration via Static Hashing", "text": "Our approach discretizes the state space with a hash function \u03c6 : S \u2192 Z. An exploration bonus r+ : S \u2192 R is added to the reward function, defined as\nr+(s) = \u03b2\u221a\nn(\u03c6(s)) , (1)\nwhere \u03b2 \u2208 R\u22650 is the bonus coefficient. Initially the counts n(\u00b7) are set to zero for the whole range of \u03c6. For every state st encountered at time step t, n(\u03c6(st )) is increased by one. The agent is trained with rewards (r + r+), while performance is evaluated as the sum of rewards without bonuses.\nNote that our approach is a departure from count-based exploration methods such as MBIE-EB since we use a state-space count n(s) rather than a state-action count n(s, a). State-action counts n(s, a) are investigated in Appendix A.6, but no significant performance gains over state counting could be witnessed.\nAlgorithm 1: Count-based exploration through static hashing 1 Define state preprocessor g : S \u2192 RD 2 (In case of SimHash) Initialize A \u2208 Rk\u00d7D with entries drawn i.i.d. from the standard Gaussian\ndistribution N (0, 1) 3 Initialize a hash table with values n(\u00b7) \u2261 0 4 for each iteration j do 5 Collect a set of state-action samples {(sm, am)}Mm=0 with policy \u03c0 6 Compute hash codes through any LSH method, e.g., for SimHash, \u03c6(sm) = sgn(Ag(sm)) 7 Update the hash table counts \u2200m : 0 \u2264 m \u2264 M as n(\u03c6(sm)) \u2190 n(\u03c6(sm)) + 1 8 Update the policy \u03c0 using rewards { r (sm, am) +\n\u03b2\u221a n(\u03c6(sm ))\n}M\nm=0 with any RL algorithm\nClearly the performance of this method will strongly depend on the choice of hash function \u03c6. One important choice we can make regards the granularity of the discretization: we would like for \u201cdistant\u201d states to be be counted separately while \u201csimilar\u201d states are merged. If desired, we can incorporate prior knowledge into the choice of \u03c6, if there would be a set of salient state features which are known to be relevant.\nAlgorithm 1 summarizes our method. The main idea is to use locality-sensitive hashing (LSH) to convert continuous, high-dimensional data to discrete hash codes. LSH is a popular class of hash functions for querying nearest neighbors based on certain similarity metrics (Andoni & Indyk, 2006). A computationally efficient type of LSH is SimHash (Charikar, 2002), which measures similarity by angular distance. SimHash retrieves a binary code of state s \u2208 S as\n\u03c6(s) = sgn(Ag(s)) \u2208 {\u22121, 1}k, (2) where g : S \u2192 RD is an optional preprocessing function and A is a k \u00d7 D matrix with i.i.d. entries drawn from a standard Gaussian distributionN (0, 1). The value for k controls the granularity: higher values lead to fewer collisions and are thus more likely to distinguish states."}, {"heading": "2.3 Count-Based Exploration via Learned Hashing", "text": "When the MDP states have a complex structure, as is the case with image observations, measuring their similarity directly in pixel space fails to provide the semantic similarity measure one would desire. Previous work in computer vision (Lowe, 1999; Dalal & Triggs, 2005; Tola et al., 2010) introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classification. More recent methods learn complex features directly from data by training convolutional neural networks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015). Considering these results, it may be difficult for SimHash to cluster states appropriately using only raw pixels.\nTherefore, we propose to use an autoencoder (AE) consisting of convolutional, dense, and transposed convolutional layers to learn meaningful hash codes in one of its hidden layers. This AE takes as input states s and contains one special dense layer comprised of D saturating activation functions,\nAlgorithm 2: Count-based exploration using learned hash codes 1 Define state preprocessor g : S \u2192 BD as the binary code resulting from the autoencoder (AE) 2 Initialize A \u2208 Rk\u00d7D with entries drawn i.i.d. from the standard Gaussian distribution N (0, 1) 3 Initialize a hash table with values n(\u00b7) \u2261 0 4 for each iteration j do 5 Collect a set of state-action samples {(sm, am)}Mm=0 with policy \u03c0 6 Add the state samples {sm}Mm=0 to a FIFO replay pool R 7 if j mod jupdate = 0 then 8 Update the AE loss function in Eq. (3) using samples drawn from the replay pool\n{sn}Nn=1 \u223c R, for example using stochastic gradient descent 9 Compute g(sm) = bb(sm)e, the D-dim rounded hash code for sm learned by the AE\n10 Project g(sm) to a lower dimension k via SimHash as \u03c6(sm) = sgn(Ag(sm)) 11 Update the hash table counts \u2200m : 0 \u2264 m \u2264 M as n(\u03c6(sm)) \u2190 n(\u03c6(sm)) + 1 12 Update the policy \u03c0 using rewards { r (sm, am) +\n\u03b2\u221a n(\u03c6(sm ))\n}M\nm=0 with any RL algorithm\nmore specifically sigmoid functions. By rounding the sigmoid output b(s) of this layer to the closest binary number, any state s can be binarized.\nSince gradients cannot be back-propagated through a rounding function, an alternative method must be used to ensure that distinct states are mapped to distinct binary codes. Therefore, uniform noise U (\u2212a, a) is added to the sigmoid output. By choosing uniform noise with a sufficiently high variance, the AE is only capable of reconstructing distinct inputs s if its hidden dense layer outputs values b(s) that are sufficiently far apart from each other (Gregor et al., 2016). Feeding a state s to the AE input, extracting b(s) and rounding it to bb(s)e yields a learned binary code. As such, the loss function L(\u00b7) over a set of collected states {si }Ni=1 is defined as\nL ( {sn}Nn=1 ) = \u2212 1\nN\nN\u2211\nn=1  log p(sn) \u2212 \u03bbK D\u2211 i=1 min { (1 \u2212 bi (sn))2 , bi (sn)2 } . (3)\nThis objective function consists of a cross-entropy term and a term that pressures the binary code layer to take on binary values, scaled by \u03bb \u2208 R\u22650. The reasoning behind this is that uniform noise U (\u2212a, a) alone is insufficient, in case the AE does not use a particular sigmoid unit. This term ensures that an unused binary code output is assigned an arbitrary binary value. When omitting this term, the code is more prone to oscillations, causing unwanted bit flips, and destabilizing the counting process.\nIn order to make the AE train sufficiently fast\u2014which is required since it is updated during the agent\u2019s training\u2014we make use of a pixel-wise softmax output layer (van den Oord et al., 2016) that shares weights between all pixels. The different softmax outputs merge together pixel intensities into discrete bins. The architectural details are described in Appendix A.1 and are depicted in Figure 1. Because the code dimension often needs to be large in order to correctly reconstruct the input, we apply a downsampling procedure to the resulting binary code bb(s)e, which can be done through random projection to a lower-dimensional space via SimHash as in Eq. (2).\nOne the one hand, it is important that the mapping from state to code needs to remain relatively consistent over time, which is nontrivial as the AE is constantly updated according to the latest data (Algorithm 2 line 8). An obvious solution would be to significantly downsample the binary code to a very low dimension, or by slowing down the training process. But on the other hand, the code has to remain relatively unique for states that are both distinct and close together on the image manifold. This is tackled both by the second term in Eq. (3) and by the saturating behavior of the sigmoid units. As such, states that are already well represented in the AE hidden layers tend to saturate the sigmoid units, causing the resulting loss gradients to be close to zero and making the code less prone to change."}, {"heading": "3 Experiments", "text": "Experiments were designed to investigate and answer the following research questions:\n1. Can count-based exploration through hashing improve performance significantly across different domains? How does the proposed method compare to the current state of the art in exploration for deep RL?\n2. What is the impact of learned or static state preprocessing on the overall performance when image observations are used?\n3. What factors contribute to good performance, e.g., what is the appropriate level of granularity of the hash function?\nTo answer question 1, we run the proposed method on deep RL benchmarks (rllab and ALE) that feature sparse rewards, and compare it to other state-of-the-art algorithms. Question 2 is answered by trying out different image preprocessors on Atari 2600 games. Finally, we investigate question 3 in Section 3.3 and 3.4. Trust Region Policy Optimization (TRPO, Schulman et al. (2015)) is chosen as the RL algorithm for all experiments, because it can handle both discrete and continuous action spaces, can conveniently ensure stable improvement in the policy performance, and is relatively insensitive to hyperparameter changes. The hyperparameters settings are reported in Appendix A.1."}, {"heading": "3.1 Continuous Control", "text": "The rllab benchmark (Duan et al., 2016) consists of various control tasks to test deep RL algorithms. We selected several variants of the basic and locomotion tasks that use sparse rewards, as shown in Figure 2, and adopt the experimental setup as defined in (Houthooft et al., 2016)\u2014a description can be found in Appendix A.2. These tasks are all highly difficult to solve with na\u00efve exploration strategies, such as adding Gaussian noise to the actions.\nFigure 3 shows the results of TRPO (baseline), TRPO-SimHash, and VIME (Houthooft et al., 2016) on the classic tasks MountainCar and CartPoleSwingup, the locomotion task HalfCheetah, and the hierarchical task SwimmerGather. Using count-based exploration with hashing is capable of reaching the goal in all environments (which corresponds to a nonzero return), while baseline TRPO with Gaussian control noise fails completely. Although TRPO-SimHash picks up the sparse reward on HalfCheetah, it does not perform as well as VIME. In contrast, the performance of SimHash is comparable with VIME on MountainCar, while it outperforms VIME on SwimmerGather."}, {"heading": "3.2 Arcade Learning Environment", "text": "The Arcade Learning Environment (ALE, Bellemare et al. (2013)), which consists of Atari 2600 video games, is an important benchmark for deep RL due to its high-dimensional state space and wide\nvariety of games. In order to demonstrate the effectiveness of the proposed exploration strategy, six games are selected featuring long horizons while requiring significant exploration: Freeway, Frostbite, Gravitar, Montezuma\u2019s Revenge, Solaris, and Venture. The agent is trained for 500 iterations in all experiments, with each iteration consisting of 0.1 M steps (the TRPO batch size, corresponds to 0.4 M frames). Policies and value functions are neural networks with identical architectures to (Mnih et al., 2016). Although the policy and baseline take into account the previous four frames, the counting algorithm only looks at the latest frame.\nBASS To compare with the autoencoder-based learned hash code, we propose using Basic Abstraction of the ScreenShots (BASS, also called Basic; see Bellemare et al. (2013)) as a static preprocessing function g. BASS is a hand-designed feature transformation for images in Atari 2600 games. BASS builds on the following observations specific to Atari: 1) the game screen has a low resolution, 2) most objects are large and monochrome, and 3) winning depends mostly on knowing object locations and motions. We designed an adapted version of BASS1, that divides the RGB screen into square cells, computes the average intensity of each color channel inside a cell, and assigns the resulting values to bins that uniformly partition the intensity range [0, 255]. Mathematically, let C be the cell size (width and height), B the number of bins, (i, j) cell location, (x, y) pixel location, and z the channel.\nfeature(i, j, z) = \u230a\nB 255C2 \u2211 (x,y)\u2208 cell(i, j) I (x, y, z) \u230b . (4)\nAfterwards, the resulting integer-valued feature tensor is converted to an integer hash code (\u03c6(st ) in Line 6 of Algorithm 1). A BASS feature can be regarded as a miniature that efficiently encodes object locations, but remains invariant to negligible object motions. It is easy to implement and introduces little computation overhead. However, it is designed for generic Atari game images and may not capture the structure of each specific game very well.\nWe compare our results to double DQN (van Hasselt et al., 2016b), dueling network (Wang et al., 2016), A3C+ (Bellemare et al., 2016), double DQN with pseudo-counts (Bellemare et al., 2016), Gorila (Nair et al., 2015), and DQN Pop-Art (van Hasselt et al., 2016a) on the \u201cnull op\u201d metric2. We show training curves in Figure 4 and summarize all results in Table 1. Surprisingly, TRPO-pixelSimHash already outperforms the baseline by a large margin and beats the previous best result on Frostbite. TRPO-BASS-SimHash achieves significant improvement over TRPO-pixel-SimHash on\n1The original BASS exploits the fact that at most 128 colors can appear on the screen. Our adapted version does not make this assumption.\n2The agent takes no action for a random number (within 30) of frames at the beginning of each episode.\nMontezuma\u2019s Revenge and Venture, where it captures object locations better than other methods.3 TRPO-AE-SimHash achieves near state-of-the-art performance on Freeway, Frostbite and Solaris.4\nAs observed in Table 1, preprocessing images with BASS or using a learned hash code through the AE leads to much better performance on Gravitar, Montezuma\u2019s Revenge and Venture. Therefore, an static or adaptive preprocessing step can be important for a good hash function.\nIn conclusion, our count-based exploration method is able to achieve remarkable performance gains even with simple hash functions like SimHash on the raw pixel space. If coupled with domain-dependent state preprocessing techniques, it can sometimes achieve far better results."}, {"heading": "3.3 Granularity", "text": "While our proposed method is able to achieve remarkable results without requiring much tuning, the granularity of the hash function should be chosen wisely. Granularity plays a critical role in count-based exploration, where the hash function should cluster states without under-generalizing or over-generalizing. Table 2 summarizes granularity parameters for our hash functions. In Table 3 we summarize the performance of TRPO-pixel-SimHash under different granularities. We choose Frostbite and Venture on which TRPO-pixel-SimHash outperforms the baseline, and choose as reward bonus coefficient \u03b2 = 0.01 \u00d7 256k to keep average bonus rewards at approximately the same scale. k = 16 only corresponds to 65536 distinct hash codes, which is insufficient to distinguish between semantically distinct states and hence leads to worse performance. We observed that k = 512 tends to capture trivial image details in Frostbite, leading the agent to believe that every state is new and equally worth exploring. Similar results are observed while tuning the granularity parameters for TRPO-BASS-SimHash and TRPO-AE-SimHash.\nThe best granularity depends on both the hash function and the MDP. While adjusting granularity parameter, we observed that it is important to lower the bonus coefficient as granularity is increased. This is because a higher granularity is likely to cause lower state counts, leading to higher bonus rewards that may overwhelm the true rewards.\n3We provide videos of example game play and visualizations of the difference bewteen Pixel-SimHash and BASS-SimHash at https://www.youtube.com/playlist?list=PLAd-UMX6FkBQdLNWtY8nH1-pzYJA_1T55\n4Note that some design choices in other algorithms also impact exploration, such as \u03b5-greedy and entropy regularization. Nevertheless, it is still valuable to position our results within the current literature."}, {"heading": "3.4 A Case Study of Montezuma\u2019s Revenge", "text": "Montezuma\u2019s Revenge is widely known for its extremely sparse rewards and difficult exploration (Bellemare et al., 2016). While our method does not outperform Bellemare et al. (2016) on this game, we investigate the reasons behind this through various experiments. The experiment process below again demonstrates the importance of a hash function having the correct granularity and encoding relevant information for solving the MDP.\nOur first attempt is to use game RAM states instead of image observations as inputs to the policy (details in Appendix A.1), which leads to a game score of 2500 with TRPO-BASS-SimHash. Our second attempt is to manually design a hash function that incorporates domain knowledge, called SmartHash, which uses an integer-valued vector consisting of the agent\u2019s (x, y) location, room number and other useful RAM information as the hash code (details in Appendix A.3). The best SmartHash agent is able to obtain a score of 3500. Still the performance is not optimal. We observe that a slight change in the agent\u2019s coordinates does not always result in a semantically distinct state, and thus the hash code may remain unchanged. Therefore we choose grid size s and replace the x coordinate by b(x \u2212 xmin)/sc (similarly for y). The bonus coefficient is chosen as \u03b2 = 0.01 \u221a s to maintain the scale relative to the true reward5 (see Table 4). Finally, the best agent is able to obtain 6600 total rewards after training for 1000 iterations (1000 M time steps), with a grid size s = 10.\nDuring our pursuit, we had another interesting discovery that the ideal hash function should not simply cluster states by their visual similarity, but instead by their relevance to solving the MDP. We\n5The bonus scaling is chosen by assuming all states are visited uniformly and the average bonus reward should remain the same for any grid size.\nexperimented with including enemy locations in the first two rooms into SmartHash (s = 10), and observed that average score dropped to 1672 (at iteration 1000). Though it is important for the agent to dodge enemies, the agent also erroneously \u201cenjoys\u201d watching enemy motions at distance (since new states are constantly observed) and \u201cforgets\u201d that his main objective is to enter other rooms. An alternative hash function keeps the same entry \u201cenemy locations\u201d, but instead only puts randomly sampled values in it, which surprisingly achieves better performance (3112). However, by ignoring enemy locations altogether, the agent achieves a much higher score (5661) (see Figure 5). In retrospect, we examine the hash codes generated by BASS-SimHash and find that codes clearly distinguish between visually different states (including various enemy locations), but fails to emphasize that the agent needs to explore different rooms. Again this example showcases the importance of encoding relevant information in designing hash functions."}, {"heading": "4 Related Work", "text": "Classic count-based methods such as MBIE (Strehl & Littman, 2005), MBIE-EB and (Kolter & Ng, 2009) solve an approximate Bellman equation as an inner loop before the agent takes an action (Strehl & Littman, 2008). As such, bonus rewards are propagated immediately throughout the state-action space. In contrast, contemporary deep RL algorithms propagate the bonus signal based on rollouts collected from interacting with environments, with value-based (Mnih et al., 2015) or policy gradient-based (Schulman et al., 2015; Mnih et al., 2016) methods, at limited speed. In addition, our proposed method is intended to work with contemporary deep RL algorithms, it differs from classical count-based method in that our method relies on visiting unseen states first, before the bonus reward can be assigned, making uninformed exploration strategies still a necessity at the beginning. Filling the gaps between our method and classic theories is an important direction of future research.\nA related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings.\nBayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b).\nAnother type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards.\nSeveral exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by Osband et al. (2016a), using bootstrapped Q-functions.\nThe most related exploration strategy is proposed by Bellemare et al. (2016), in which an exploration bonus is added inversely proportional to the square root of a pseudo-count quantity. A state pseudocount is derived from its log-probability improvement according to a density model over the state space, which in the limit converges to the empirical count. Our method is similar to pseudo-count approach in the sense that both methods are performing approximate counting to have the necessary generalization over unseen states. The difference is that a density model has to be designed and learned to achieve good generalization for pseudo-count whereas in our case generalization is obtained by a wide range of simple hash functions (not necessarily SimHash). Another interesting connection is that our method also implies a density model \u03c1(s) = n(\u03c6(s))N over all visited states, where N is the total number of states visited. Another method similar to hashing is proposed by Abel et al. (2016), which clusters states and counts cluster centers instead of the true states, but this method has yet to be tested on standard exploration benchmark problems."}, {"heading": "5 Conclusions", "text": "This paper demonstrates that a generalization of classical counting techniques through hashing is able to provide an appropriate signal for exploration, even in continuous and/or high-dimensional MDPs using function approximators, resulting in near state-of-the-art performance across benchmarks. It provides a simple yet powerful baseline for solving MDPs that require informed exploration."}, {"heading": "Acknowledgments", "text": "We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Adam Stooke gratefully acknowledges funding from a Fannie and John Hertz Foundation fellowship. Rein Houthooft is supported by a Ph.D. Fellowship of the Research Foundation - Flanders (FWO)."}, {"heading": "A Appendices", "text": "A.1 Hyperparameter Settings\nFor the rllab experiments, we used batch size 5000 for all tasks except SwimmerGather, for which we used batch size 50000. CartpoleSwingup makes use of a neural network policy with one layer of 32 tanh units. The other tasks make use of a two layer neural network policy of 32 tanh units each for MountainCar and HalfCheetah, and of 64 and 32 tanh units for SwimmerGather. The outputs are modeled by a fully factorized Gaussian distributionN (\u00b5, \u03c32I), in which \u00b5 is modeled as the network output, while \u03c3 is a parameter. CartPoleSwingup makes use of a neural network baseline with one layer of 32 ReLU units, while all other tasks make use of a linear baseline function. For all tasks, we used TRPO step size 0.01 and discount factor \u03b3 = 0.99. We choose SimHash parameter k = 32 and bonus coefficient \u03b2 = 0.01, found through a coarse grid search.\nFor Atari experiments, a batch size of 100000 is used, while the KL divergence step size is set to 0.01. The policy and baseline both have the following architecture: 2 convolutional layers with respectively 16 and 32 filters, sizes 8 \u00d7 8 and 4 \u00d7 4, strides 4 and 2, using no padding, feeding into a single hidden layer of 256 units. The nonlinearities are rectified linear units (ReLUs). The input frames are downsampled to 52 \u00d7 52. The input to policy and baseline consists of the 4 previous frames, corresponding to the frame skip of 4. The discount factor was set to \u03b3 = 0.995. All inputs are rescaled to [\u22121, 1] element-wise. All experiments used 5 different training seeds, except the experiments with the learned hash code, which uses 3 different training seeds. Batch normalization (Ioffe & Szegedy, 2015) is used at each policy and baseline layer. TRPO-pixel-SimHash uses binary codes of size k = 256; BASS (TRPO-BASS-SimHash) extracts features using cell size C = 20 and B = 20 bins. The autoencoder for the learned embedding (TRPO-AE-SimHash) uses a binary hidden layer of 512 bit, which are projected to 64 bit.\nRAM states in Atari 2600 games are integer-valued vectors over length 128 in the range [0, 255]. Experiments on Montezuma\u2019s Revenge with RAM observations use a policy consisting of 2 hidden layers, each of size 32. RAM states are rescaled to a range [\u22121, 1]. Unlike images, only the current RAM is shown to the agent. Experiment results are averaged over 10 random seeds.\nIn addition, we apply counting Bloom filters (Fan et al., 2000) to maintain a small hash table. Details can be found in Appendix A.5.\nThe autoencoder used for the learned hash code has a 512 bit binary code layer, using sigmoid units, to which uniform noise U (\u2212a, a) with a = 0.3 is added. The loss function Eq. (3), using \u03bb = 10, is updated every jupdate = 3 iterations. The architecture looks as follows: an input layer of size 52 \u00d7 52, representing the image luminance is followed by 3 consecutive 6 \u00d7 6 convolutional layers with stride 2 and 96 filters feed into a fully connected layer of size 1024, which connects to the binary code layer. This binary code layer feeds into a fully-connected layer of 1024 units, connecting to a fully-connected layer of 2400 units. This layer feeds into 3 consecutive 6\u00d7 6 transposed convolutional layers of which the final one connects to a pixel-wise softmax layer with 64 bins, representing the pixel intensities. Moreover, label smoothing is applied to the different softmax bins, in which the log-probability of each of the bins is increased by 0.003, before normalizing. The softmax weights are shared among each pixel. All output nonlinearities are ReLUs; Adam (Kingma & Ba, 2015) is used as an optimization scheme; batch normalization (Ioffe & Szegedy, 2015) is applied to each layer. The architecture was shown in Figure 1 of Section 2.3.\nA.2 Description of the Adapted rllab Tasks\nThis section describes the continuous control environments used in the experiments. The tasks are implemented as described in Duan et al. (2016), following the sparse reward adaptation of Houthooft et al. (2016). The tasks have the following state and action dimensions: CartPoleSwingup, S \u2286 R4, A \u2286 R; MountainCar S \u2286 R3, A \u2286 R1; HalfCheetah, S \u2286 R20, A \u2286 R6; SwimmerGather, S \u2286 R33, A \u2286 R2. For the sparse reward experiments, the tasks have been modified as follows. In CartPoleSwingup, the agent receives a reward of +1 when cos(\u03b2) > 0.8, with \u03b2 the pole angle. In MountainCar, the agent receives a reward of +1 when the goal state is reached, namely escaping the valley from the right side. Therefore, the agent has to figure out how to swing up the pole in the absence of any initial external rewards. In HalfCheetah, the agent receives a reward of +1 when\nxbody > 5. As such, it has to figure out how to move forward without any initial external reward. The time horizon is set to T = 500 for all tasks.\nA.3 Examples of Atari 2600 RAM Entries\nTable 5 lists the semantic interpretation of certain RAM entries in Montezuma\u2019s Revenge. SmartHash, as described in Section 3.4, makes use of RAM indices 3, 42, 43, 27, and 67. \u201cBeam walls\u201d are deadly barriers that occur periodically in some rooms.\nA.4 Analysis of Learned Binary Representation\nFigure 6 shows the downsampled codes learned by the autoencoder for several Atari 2600 games (Frostbite, Freeway, and Montezuma\u2019s Revenge). Each row depicts 50 consecutive frames (from 0 to 49, going from left to right, top to bottom). The pictures in the right column depict the binary codes that correspond with each of these frames (one frame per row). Figure 7 shows the reconstructions of several subsequent images according to the autoencoder.\nA.5 Counting Bloom Filter/Count-Min Sketch\nWe experimented with directly building a hashing dictionary with keys \u03c6(s) and values the state counts, but observed an unnecessary increase in computation time. Our implementation converts the integer hash codes into binary numbers and then into the \u201cbytes\u201d type in Python. The hash table is a dictionary using those bytes as keys.\nHowever, an alternative technique called Count-Min Sketch (Cormode & Muthukrishnan, 2005), with a data structure identical to counting Bloom filters (Fan et al., 2000), can count with a fixed integer array and thus reduce computation time. Specifically, let p1, . . . , pl be distinct large prime numbers and define \u03c6 j (s) = \u03c6(s) mod pj . The count of state s is returned as min1\u2264 j\u2264l n j ( \u03c6 j (s) ) . To increase the count of s, we increment n j ( \u03c6 j (s) ) by 1 for all j. Intuitively, the method replaces \u03c6 by weaker\nhash functions, while it reduces the probability of over-counting by reporting counts agreed by all such weaker hash functions. The final hash code is represented as ( \u03c61(s), . . . , \u03c6l (s) ) .\nThroughout all experiments above, the prime numbers for the counting Bloom filter are 999931, 999953, 999959, 999961, 999979, and 999983, which we abbreviate as \u201c6 M\u201d. In addition, we experimented with 6 other prime numbers, each approximately 15 M, which we abbreviate as \u201c90 M\u201d. As we can see in Figure 8, counting states with a dictionary or with Bloom filters lead to similar performance, but the computation time of latter is lower. Moreover, there is little difference between direct counting and using a very larger table for Bloom filters, as the average bonus rewards are almost the same, indicating the same degree of exploration-exploitation trade-off. On the other hand, Bloom filters require a fixed table size, which may not be known beforehand.\nTheory of Bloom Filters Bloom filters (Bloom, 1970) are popular for determining whether a data sample s\u2032 belongs to a dataset D. Suppose we have l functions \u03c6 j that independently assign each data sample to an integer between 1 and p uniformly at random. Initially 1, 2, . . . , p are marked as 0. Then every s \u2208 D is \u201cinserted\u201d through marking \u03c6 j (s) as 1 for all j. A new sample s\u2032 is reported as a member of D only if \u03c6 j (s) are marked as 1 for all j. A bloom filter has zero false negative rate (any s \u2208 D is reported a member), while the false positive rate (probability of reporting a nonmember as a member) decays exponentially in l.\nThough Bloom filters support data insertion, it does not allow data deletion. Counting Bloom filters (Fan et al., 2000) maintain a counter n(\u00b7) for each number between 1 and p. Inserting/deleting s corresponds to incrementing/decrementing n ( \u03c6 j (s) ) by 1 for all j. Similarly, s is considered a member if \u2200 j : n ( \u03c6 j (s) ) = 0.\nCount-Min sketch is designed to support memory-efficient counting without introducing too many over-counts. It maintains a separate count n j for each hash function \u03c6 j defined as \u03c6 j (s) = \u03c6(s) mod pj , where pj is a large prime number. For simplicity, we may assume that pj \u2248 p \u2200 j and \u03c6 j assigns s to any of 1, . . . , p with uniform probability.\nWe now derive the probability of over-counting. Let s be a fixed data sample (not necessarily inserted yet) and suppose a dataset D of N samples are inserted. We assume that pl N . Let n := min1\u2264 j\u2264l n j ( \u03c6 j (s) ) be the count returned by the Bloom filter. We are interested in computing Prob(n > 0|s < D). Due to assumptions about \u03c6 j , we know n j (\u03c6(s)) \u223c Binomial ( N, 1p ) . Therefore,\nProb(n > 0|s < D) = Prob(n > 0, s < D) Prob(s < D)\n= Prob(n > 0) \u2212 Prob(s \u2208 D) Prob(s < D) \u2248 Prob(n > 0)\nProb(s < D)\n=\n\u220fl j=1 Prob(n\nj (\u03c6 j (s)) > 0) (1 \u2212 1/pl)N\n= (1 \u2212 (1 \u2212 1/p)N )l\n(1 \u2212 1/pl)N\n\u2248 (1 \u2212 e \u2212N/p)l\ne\u2212N/pl\n\u2248 (1 \u2212 e\u2212N/p)l .\n(5)\nIn particular, the probability of over-counting decays exponentially in l. We refer the readers to (Cormode & Muthukrishnan, 2005) for other properties of the Count-Min sketch.\nA.6 Robustness Analysis\nApart from the experimental results shown in Table 1 and Table 3, additional experiments have been performed to study several properties of our algorithm.\nHyperparameter sensitivity To study the performance sensitivity to hyperparameter changes, we focus on evaluating TRPO-RAM-SimHash on the Atari 2600 game Frostbite, where the method has a clear advantage over the baseline. Because the final scores can vary between different random seeds, we evaluated each set of hyperparameters with 30 seeds. To reduce computation time and cost, RAM states are used instead of image observations."}, {"heading": "128 \u2013 1475 4248 2801 3239 3621 1543 395", "text": ""}, {"heading": "256 \u2013 2583 4497 4437 7849 3516 2260 374", "text": "The results are summarized in Table 6. Herein, k refers to the length of the binary code for hashing while \u03b2 is the multiplicative coefficient for the reward bonus, as defined in Section 2.2. This table demonstrates that most hyperparameter settings outperform the baseline (\u03b2 = 0) significantly. Moreover, the final scores show a clear pattern in response to changing hyperparameters. Small \u03b2-values lead to insufficient exploration, while large \u03b2-values cause the bonus rewards to overwhelm the true rewards. With a fixed k, the scores are roughly concave in \u03b2, peaking at around 0.2. Higher granularity k leads to better performance. Therefore, it can be concluded that the proposed exploration method is robust to hyperparameter changes in comparison to the baseline, and that the best parameter settings can obtained from a relatively coarse-grained grid search.\nState and state-action counting Continuing the results in Table 6, the performance of state-action counting is studied using the same experimental setup, summarized in Table 7. In particular, a bonus reward r+(s, a) = \u03b2\u221a\nn(s,a) instead of r+(s) = \u03b2\u221a n(s) is assigned. These results show that the\nrelative performance of state counting compared to state-action counting depends highly on the selected hyperparameter settings. However, we notice that the best performance is achieved using state counting with k = 256 and \u03b2 = 0.2."}, {"heading": "128 1475 / 808 4248 / 4302 2801 / 4802 3239 / 7291 3621 / 4243 1543 / 1941 395 / 362", "text": ""}], "references": [{"title": "Exploratory gradient boosting for reinforcement learning in complex domains", "author": ["David Abel", "Alekh Agarwal", "Fernando Diaz", "Akshay Krishnamurthy", "Robert E Schapire"], "venue": "arXiv preprint arXiv:1603.04119,", "citeRegEx": "Abel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abel et al\\.", "year": 2016}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["Alexandr Andoni", "Piotr Indyk"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Andoni and Indyk.,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2006}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["MarcGBellemare", "Yavar Naddaf", "Joel Veness", "andMichael Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "MarcGBellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "MarcGBellemare et al\\.", "year": 2013}, {"title": "Space/time trade-offs in hash coding with allowable errors", "author": ["Burton H. Bloom"], "venue": "Communications of the ACM,", "citeRegEx": "Bloom.,? \\Q1970\\E", "shortCiteRegEx": "Bloom.", "year": 1970}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": [], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Tennenholtz.", "year": 2002}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S Charikar"], "venue": "In Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Charikar.,? \\Q2002\\E", "shortCiteRegEx": "Charikar.", "year": 2002}, {"title": "An improved data stream summary: the count-min sketch and its applications", "author": ["Graham Cormode", "S Muthukrishnan"], "venue": "Journal of Algorithms,", "citeRegEx": "Cormode and Muthukrishnan.,? \\Q2005\\E", "shortCiteRegEx": "Cormode and Muthukrishnan.", "year": 2005}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dalal and Triggs.,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Benchmarking deep reinforcement learning for continous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Summary cache: A scalable wide-area web cache sharing protocol", "author": ["Li Fan", "Pei Cao", "Jussara Almeida", "Andrei Z Broder"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "Fan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2000}, {"title": "Bayesian reinforcement learning: A survey", "author": ["Mohammad Ghavamzadeh", "Shie Mannor", "Joelle Pineau", "Aviv Tamar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Ghavamzadeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2015}, {"title": "Bayes-adaptive simulation-based search with value function approximation", "author": ["Arthur Guez", "Nicolas Heess", "David Silver", "Peter Dayan"], "venue": "In Advances in Neural Information Processing Systems (Advances in Neural Information Processing Systems (NIPS)),", "citeRegEx": "Guez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Near-optimal reinforcement learning in polynomial time.Machine Learning", "author": ["Michael Kearns", "Satinder Singh"], "venue": null, "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J Zico Kolter", "Andrew Y Ng"], "venue": "In Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "Kolter and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng.", "year": 2009}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Object recognition from local scale-invariant features", "author": ["David G Lowe"], "venue": "In Proceedings of the 7th IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Lowe.,? \\Q1999\\E", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Generalization and exploration via randomized value functions", "author": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "What is intrinsic motivation? A typology of computational approaches", "author": ["Pierre-Yves Oudeyer", "Frederic Kaplan"], "venue": "Frontiers in Neurorobotics,", "citeRegEx": "Oudeyer and Kaplan.,? \\Q2007\\E", "shortCiteRegEx": "Oudeyer and Kaplan.", "year": 2007}, {"title": "PAC optimal exploration in continuous space Markov decision processes", "author": ["Jason Pazis", "Ronald Parr"], "venue": "In Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pazis and Parr.,? \\Q2013\\E", "shortCiteRegEx": "Pazis and Parr.", "year": 2013}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Bradly C Stadie", "Sergey Levine", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman.,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2008}, {"title": "Planning to be surprised: Optimal Bayesian exploration in dynamic environments", "author": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 4th International Conference on Artificial General Intelligence (AGI),", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "DAISY: An efficient dense descriptor applied to wide-baseline stereo", "author": ["Engin Tola", "Vincent Lepetit", "Pascal Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tola et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tola et al\\.", "year": 2010}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Learning functions across many orders of magnitudes", "author": ["Hado van Hasselt", "Arthur Guez", "Matteo Hessel", "David Silver"], "venue": "arXiv preprint arXiv:1602.07714,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["ZiyuWang", "Nando de Freitas", "andMarc Lanctot"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "ZiyuWang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ZiyuWang et al\\.", "year": 2016}, {"title": "2016), following the sparse reward adaptation", "author": ["Duan"], "venue": null, "citeRegEx": "Duan,? \\Q2016\\E", "shortCiteRegEx": "Duan", "year": 2016}, {"title": "The tasks have the following state and action dimensions: CartPoleSwingup", "author": ["Houthooft"], "venue": null, "citeRegEx": "Houthooft,? \\Q2016\\E", "shortCiteRegEx": "Houthooft", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Most of the recent state-of-the-art RL results have been obtained using simple exploration strategies such as uniform sampling (Mnih et al., 2015) and i.", "startOffset": 127, "endOffset": 146}, {"referenceID": 28, "context": "/correlated Gaussian noise (Schulman et al., 2015; Lillicrap et al., 2015).", "startOffset": 27, "endOffset": 74}, {"referenceID": 19, "context": "/correlated Gaussian noise (Schulman et al., 2015; Lillicrap et al., 2015).", "startOffset": 27, "endOffset": 74}, {"referenceID": 8, "context": "We select problems from rllab (Duan et al., 2016) and Atari 2600 (Bellemare et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards.", "startOffset": 1, "endOffset": 372}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen.", "startOffset": 1, "endOffset": 865}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation\u2013Exploration Bonus (MBIE-EB) of Strehl & Littman (2008) counts state-action pairs with a table n(s, a) and adding a bonus reward of the form \u03b2 \u221a n(s,a) to encourage exploring less visited pairs.", "startOffset": 1, "endOffset": 1214}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation\u2013Exploration Bonus (MBIE-EB) of Strehl & Littman (2008) counts state-action pairs with a table n(s, a) and adding a bonus reward of the form \u03b2 \u221a n(s,a) to encourage exploring less visited pairs. Kolter & Ng (2009) show that the inverse-square-root dependence is optimal.", "startOffset": 1, "endOffset": 1372}, {"referenceID": 5, "context": "A computationally efficient type of LSH is SimHash (Charikar, 2002), which measures similarity by angular distance.", "startOffset": 51, "endOffset": 67}, {"referenceID": 20, "context": "Previous work in computer vision (Lowe, 1999; Dalal & Triggs, 2005; Tola et al., 2010) introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classification.", "startOffset": 33, "endOffset": 86}, {"referenceID": 34, "context": "Previous work in computer vision (Lowe, 1999; Dalal & Triggs, 2005; Tola et al., 2010) introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classification.", "startOffset": 33, "endOffset": 86}, {"referenceID": 12, "context": "More recent methods learn complex features directly from data by training convolutional neural networks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015).", "startOffset": 104, "endOffset": 174}, {"referenceID": 28, "context": "Trust Region Policy Optimization (TRPO, Schulman et al. (2015)) is chosen as the RL algorithm for all experiments, because it can handle both discrete and continuous action spaces, can conveniently ensure stable improvement in the policy performance, and is relatively insensitive to hyperparameter changes.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "The rllab benchmark (Duan et al., 2016) consists of various control tasks to test deep RL algorithms.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "Figure 2: Illustrations of the rllab tasks used in the continuous control experiments, namely MountainCar, CartPoleSwingup, SimmerGather, and HalfCheetah; taken from (Duan et al., 2016).", "startOffset": 166, "endOffset": 185}, {"referenceID": 22, "context": "Policies and value functions are neural networks with identical architectures to (Mnih et al., 2016).", "startOffset": 81, "endOffset": 100}, {"referenceID": 38, "context": "2 1450 \u2013 3439 \u2013 369 1 While Vezhnevets et al. (2016) reported best score 8108, their evaluation was based on top 5 agents trained with 500M time steps, hence not comparable.", "startOffset": 28, "endOffset": 53}, {"referenceID": 23, "context": ", 2016), Gorila (Nair et al., 2015), and DQN Pop-Art (van Hasselt et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 21, "context": "In contrast, contemporary deep RL algorithms propagate the bonus signal based on rollouts collected from interacting with environments, with value-based (Mnih et al., 2015) or policy gradient-based (Schulman et al.", "startOffset": 153, "endOffset": 172}, {"referenceID": 28, "context": ", 2015) or policy gradient-based (Schulman et al., 2015; Mnih et al., 2016) methods, at limited speed.", "startOffset": 33, "endOffset": 75}, {"referenceID": 22, "context": ", 2015) or policy gradient-based (Schulman et al., 2015; Mnih et al., 2016) methods, at limited speed.", "startOffset": 33, "endOffset": 75}, {"referenceID": 14, "context": ", R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002).", "startOffset": 44, "endOffset": 65}, {"referenceID": 11, "context": "Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods.", "startOffset": 20, "endOffset": 102}, {"referenceID": 33, "context": "Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods.", "startOffset": 20, "endOffset": 102}, {"referenceID": 10, "context": "Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods.", "startOffset": 20, "endOffset": 102}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al.", "startOffset": 118, "endOffset": 669}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration.", "startOffset": 118, "endOffset": 695}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards.", "startOffset": 118, "endOffset": 957}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards.", "startOffset": 118, "endOffset": 985}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus.", "startOffset": 118, "endOffset": 1180}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus.", "startOffset": 118, "endOffset": 1344}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by Osband et al. (2016a), using bootstrapped Q-functions.", "startOffset": 118, "endOffset": 1510}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by Osband et al. (2016a), using bootstrapped Q-functions. The most related exploration strategy is proposed by Bellemare et al. (2016), in which an exploration bonus is added inversely proportional to the square root of a pseudo-count quantity.", "startOffset": 118, "endOffset": 1620}, {"referenceID": 0, "context": "Another method similar to hashing is proposed by Abel et al. (2016), which clusters states and counts cluster centers instead of the true states, but this method has yet to be tested on standard exploration benchmark problems.", "startOffset": 49, "endOffset": 68}], "year": 2017, "abstractText": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various highdimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.", "creator": "LaTeX with hyperref package"}}}