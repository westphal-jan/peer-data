{"id": "1611.04924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Robust Semi-Supervised Graph Classifier Learning with Negative Edge Weights", "abstract": "in a semi - supervised learning scenario, ( possibly noisy ) partially observed labels are used as input to train a classifier, in order to assign labels to unclassified samples. in this paper, we repeatedly study this classifier learning problem from a graph signal processing ( gsp ) perspective. specifically, by viewing a binary decision classifier as a piecewise constant graph - signal in a high - dimensional feature space, we cast classifier learning as a signal restoration problem via a classical maximum a posteriori ( map ) formulation. unlike previous graph - signal restoration works, we consider in addition edges all with negative weights that signify anti - correlation between samples. one unfortunate intermediate consequence is that the graph additive laplacian matrix $ \\ mathbf { do l } $ can be indefinite, and previously proposed graph - signal smoothness prior $ \\ mathbf { x } ^ t \\ mathbf { l } \\ mathbf { x } $ for candidate signal $ \\ mathbf { x } $ can lead to pathological solutions. in response, we derive an optimal perturbation matrix $ \\ real boldsymbol { \\ delta } $ - based on a fast lower - bound computation of the minimum eigenvalue of $ \\ mathbf { l } $ via a novel application of the haynsworth inertia additivity formula - - - so that $ \\ mathbf { l } + \\ boldsymbol { \\ delta } $ is positive semi - definite, resulting in a stable signal prior. further, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph that promotes ambiguity in the continuous classifier signal. finally, we propose an algorithm based on iterative continuous reweighted least squares ( irls ) that solves the posed map problem efficiently. extensive simulation results show that our proposed algorithm outperforms both svm compiler variants and graph - based classifiers using positive - edge graphs noticeably.", "histories": [["v1", "Tue, 15 Nov 2016 16:36:29 GMT  (309kb,D)", "http://arxiv.org/abs/1611.04924v1", "13 pages, submitted to IEEE Journal of Selected Topics in Signal Processing (Special Issue on Graph Signal processing)"], ["v2", "Thu, 20 Jul 2017 13:10:24 GMT  (1036kb,D)", "http://arxiv.org/abs/1611.04924v2", "15 pages, revised for IEEE Transactions on Signal and Information Processing over Network"]], "COMMENTS": "13 pages, submitted to IEEE Journal of Selected Topics in Signal Processing (Special Issue on Graph Signal processing)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gene cheung", "weng-tai su", "yu mao", "chia-wen lin"], "accepted": false, "id": "1611.04924"}, "pdf": {"name": "1611.04924.pdf", "metadata": {"source": "CRF", "title": "Robust Semi-Supervised Graph Classifier Learning with Negative Edge Weights", "authors": ["Gene Cheung", "Yu Mao", "Chia-Wen Lin"], "emails": ["mao}@nii.ac.jp).", "wengtai2008@hotmail.com,", "cwlin@ee.nthu.edu.tw)."], "sections": [{"heading": null, "text": "Index Terms\u2014graph signal processing, signal restoration, classifier learning\nI. INTRODUCTION A fundamental problem in machine learning is semisupervised learning [1]: given partially observed labels (possibly corrupted by noise) as input, train a classifier so that unclassified samples can also be appropriately assigned labels. Among many approaches to the problem is a class of graphbased methods [2]\u2013[5] that model each sample as a node in a graph, connected to other nodes via undirected edges, with weights that reflect pairwise distances in a high-dimensional feature space. See Fig. 1 for an example of a graph with eight nodes (samples) in a two-dimensional feature space. Establishing a graph representation of the data means that intrinsic properties of the graph spectrum (e.g., low graph frequencies that are eigenvectors of the graph Laplacian matrix) can be exploited for label assignment via spectral graph theory [6].\nIn this paper, we extend previous graph-based studies by considering in addition negative edge weights in the graph\nG. Cheung, Y. Mao are with the National Institute of Informatics, Graduate University for Advanced Studies, Tokyo 101-8430, Japan (e-mail: {cheung, mao}@nii.ac.jp).\nW.-T. Su, C.-W. Lin are with National Tsing Hua University, Hsinchu, Taiwan 30013 (e-mail: wengtai2008@hotmail.com, cwlin@ee.nthu.edu.tw).\nand examining their effects on binary graph classifier learning. Conventional formulations in graph signal processing (GSP) [7] use non-negative edge weights that reflect inter-node correlation; an edge weight wi,j = 0 means samples xi and xj are conditionally independent. However, negative edge weights can signify anti-correlation: wi,j = \u22121 means samples xi and xj are expected to take on different values, i.e. |xi\u2212xj | should be large. Incorporating pairwise anti-correlation into the graph should intuitively be beneficial during classifier learning. For example, if edge weight w1,2 is assigned \u22121 in Fig. 1, then from the graph G itself, one already expects x1 and x2 to be assigned opposite labels in a binary classifier.\nTo study the implications of negative edge weights, we view a binary classifier as a piecewise constant (PWC) graph-signal and cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation [8]. We show first that a graph Laplacian matrix L with negative edge weights can be indefinite, and a common graph-signal prior called graph Laplacian regularizer [9]\u2013[20] xTLx for candidate signal x\u2014measuring signal smoothness with respect to the underlying graph\u2014can lead to pathological solutions. In response, we derive an \u201coptimal\u201d perturbation matrix \u2206 so that L + \u2206 is positive semi-definite (PSD), resulting in a stable signal prior. To efficiently compute an approximate \u2206, we propose a fast recursive algorithm that identifies a lower bound for the smallest eigenvalue of L via a novel application of the Haynsworth Inertia Additivity formula [21].\nSecond, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph\u2014an extension of total generalized variation (TGV) [22] to the graph-signal domain\u2014that promotes the right amount of ambiguity in the classifier signal. We show that by interpreting\nar X\niv :1\n61 1.\n04 92\n4v 1\n[ cs\n.L G\n] 1\n5 N\nov 2\n01 6\n2 a graph as an electrical circuit, the generalized smoothness condition is equivalent to the Kirchhoff\u2019s current law [23], leading to an argument why negative edge weights should not be used when considering generalized smoothness.\nFinally, we propose an algorithm based on iterative reweighted least squares (IRLS) [24] that efficiently solves the posed MAP problem for the noisy label scenario. Extensive simulation results show that our proposed algorithm outperforms SVM variants, a well-known robust classifier in the machine learning literature called RobustBoost [25], and graph-based classifiers using positive-edge graphs noticeably for both noiseless and noisy label scenarios.\nThe outline of the paper is as follows. We first overview related works in Section II. We then review basic GSP concepts and define a graph-signal smoothness prior in Section III. In Section IV, we derive the optimal perturbation matrix \u2206 such that L + \u2206 is PSD. In Section V, we describe a fast algorithm to approximate the best \u2206. In Section VI, we present a label noise model, introduce the generalized graph-signal smoothness prior, and present an efficient algorithm to solve the MAP problem for classifier learning with noisy labels. Finally, we present experimental results and conclusions in Section VII and VIII, respectively."}, {"heading": "II. RELATED WORKS", "text": ""}, {"heading": "A. Robust Classifier Learning", "text": "Classifier learning with label noise has garnered much interest, including a workshop1 in NIPS\u201910 [26] and a journal special issue in Neurocomputing [27]. There exists a wide range of approaches, including theoretical (e.g., label propagation in [28]) and application-specific (e.g., emotion detection using inference algorithm based on multiplicative update rule [29]). In this paper, similar to previous works [2]\u2013[5] we choose to build a graph-based classifier, where each acquired sample is represented as a node in a highdimensional feature space and connects to other sample nodes in its neighborhood. Compared to previous works on graphbased classifier learning, our approach is novel in the following aspects. First, we learn a binary classifier via a classical MAP formulation but consider in addition negative edge weights that signify anti-correlation. For the graph-signal smoothness prior to be numerically stable, this requires a perturbation to the graph Laplacian matrix L to make it PSD, which we compute efficiently. Second, we show how generalized graph smoothness notion\u2014extending TGV [22] to the graph-signal domain\u2014can be interpreted intuitively as Kirchoff\u2019s current law and used to promote ambiguity in the classifier solution."}, {"heading": "B. Graph-Signal Restoration", "text": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20]. The common assumption is that the desired graphsignal is smooth or band-limited with respect to an appropriate graph with non-negative edge weights that reflect inter-pixel\n1https://people.cs.umass.edu/ wallach/workshops/nips2010css/\ncorrelation. In contrast, by considering negative edges we incorporate also anti-correlation information into the graph. Further, we define a generalized notion of graph smoothness for signal restoration specifically for classifier learning."}, {"heading": "C. Alternative Graph-Signal Smoothness Prior", "text": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32]. More concretely, instead of the graph Laplacian matrix L, the adjacency matrix W is used as the variation operator to define signal smoothness and graph frequencies. As an example, for graph-signal restoration, a smoothness prior \u2016x\u2212Wx\u2016pp for some positive integer p was proposed in [4]. When edge weights are negative, however, such smoothness prior can become insensible. Consider the three-node graph in Fig. 2. Assuming p = 2, the smoothness prior when w = \u22121 is:\n\u2016x\u2212Wx\u201622 = \u2016(I\u2212W)x\u2016 2 2 = \u2225\u2225\u2225\u2225\u2225\u2225  1 1 01 1 1\n0 1 1  x1x2 x3 \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n= (x1 + x2) 2 + (x1 + x2 + x3) 2 + (x2 + x3) 2\nwhich means the prior is just promoting a small DC term, despite the presence of two negative edges, which is clearly not sensible.\nSuppose a total variation (TV) approach [33] is taken instead, so that a smoothness prior using L but based on l1norm is used instead: i.e., |Lx|. Unfortunately, this prior can also be insensible when edges are negative. Using the same 3-node graph in Fig. 2 but with w = 1, |Lx| is:\n|Lx| = \u2223\u2223\u2223\u2223\u2223\u2223  \u22121 1 01 0 \u22121\n0 \u22121 1  x1x2 x3 \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 x2 \u2212 x1 x1 \u2212 x3 x3 \u2212 x2 \u2223\u2223\u2223\u2223\u2223\u2223 In other words, the prior tries to minimize the difference between every node pair, even though there is a negative edge between node 1 and 2. Thus this prior is also not sensible.\nThese observations motivate our current study to use xTLx as the appropriate smoothness prior; for the same example,\nxTLx = \u22121(x1 \u2212 x2)2 + w(x2 \u2212 x3)2 (1)\nwhich promotes a large difference between node 1 and 2, and promotes a large or small difference between node 2 and 3 depending on the sign of w. This is clearly a more sensible prior. Of course, direct use of xTLx can lead to numerical problems, and thus L must be first perturbed in some optimal manner. This is the focus of this paper.\n3"}, {"heading": "D. Negative Edge Weights in Graphs", "text": "Recent studies in the control community have examined the conditions where one or more negative edge weights would induce a graph Laplacian to be indefinite [34], [35]. The analysis, however, rests on an assumption that there are no cycles in the graph with more than one negative edge, which is too restrictive for binary classifier graphs.\n[36] considered a signed social network where each edge denotes either a cohesive (positive edge weight) or oppositive (negative edge weight) relationship between two vertices. The goal is to identify similar groups within the graph, and thus is akin to a distributed clustering problem, which is unsupervised by definition. In contrast, our goal is to restore a classifier graph-signal from partially observed labels, which is a semisupervised learning problem."}, {"heading": "III. PRELIMINARIES", "text": ""}, {"heading": "A. Graph Definition", "text": "We first introduce definitions in GSP needed for our problem formulation. A graph G(V, E ,W) has a defined set V of N nodes and set E of M edges. Each edge (i, j) \u2208 E connecting nodes i and j is undirected and has an associated scalar weight wi,j . In this paper, we assume that wi,j can be positive or negative; a negative wi,j would mean that samples in nodes i and j are anti-correlated\u2014the samples are expected to have very different values.\nA graph-signal x \u2208 RN on G is a discrete signal of dimension N\u2014one label xi for each node (sample) i in V . If we restrict x to be a binary classifier, then xi can only take on one of two values specifying the class sample i belongs to, i.e., x \u2208 {\u22121, 1}N . However, having the reconstructed graphsignal x\u0302 to take on real values RN allows us to introduce ambiguity in the reconstruction instead of forcing hard binary decisions; this is discussed specifically in Section VI."}, {"heading": "B. Graph Spectrum", "text": "Given edge weight (adjacency) matrix W, we define a diagonal degree matrix D, where di,i = \u2211 j wi,j . A combinatorial graph Laplacian matrix L is then simply L = D\u2212W [7]. Because L is symmetric, one can show via the PerronFrobenious Theorem2 that it can be eigen-decomposed into:\nL = V\u039bVT (2)\nwhere \u039b is a diagonal matrix containing real eigenvalues \u03bbk (not necessarily unique), and V is an eigen-matrix composed of orthogonal eigenvectors vi as columns. If edge weights wi,j are restricted to be non-negative, then one can show that L is positive semi-definite (PSD), meaning that \u03bbk \u2265 0,\u2200k and xTLx \u2265 0, \u2200x. Non-negative eigenvalues \u03bbk can be interpreted as graph frequencies, and eigenvectors vk interpreted as corresponding graph frequency components. Together they define the graph spectrum for graph G.\nIn this paper, we consider also negative edge weights wi,j , and thus eigenvalues \u03bbk can be negative and L can be indefinite. It is then hard to interpret L\u2019s eigenvalues \u03bbk as\n2https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius theorem\nfrequencies, and in general, it is desirable to have a variational operator that is PSD. Thus it is desirable to add a perturbation matrix \u2206 to L such that the resultant L + \u2206 is PSD. We address this problem of finding an optimal \u2206 in Section IV."}, {"heading": "C. Graph-Signal Smoothness Prior", "text": "Traditionally, for graph G with non-negative edge weights, signal x is considered smooth if each label xi on node i is similar to labels xj on neighboring nodes j with large wi,j . In the graph frequency domain, it means that x contains mostly low graph frequency components; i.e., coefficients \u03b1 = VTx are zeros or very small for high frequencies. The smoothest signal is the constant vector 1\u2014the first eigenvector v1 for L corresponding to the smallest eigenvalue \u03bb1 = 0. Note that v1 = 1 has no zero-crossings, and higher frequency components vk have increasingly more zero-crossings according to the nodal domain theorem [37].\nMathematically, we can write that a signal x is smooth if its graph Laplacian regularizer xTLx is small [10], [11]. Graph Laplacian regularizer can be expressed as:\nxTLx = \u2211\n(i,j)\u2208E wi,j (xi \u2212 xj)2 = \u2211 k \u03bbk \u03b1 2 k (3)\nBecause L is PSD, xTLx is lower-bounded by 0. The graph Laplacian regularizer is related to the Rayleigh quotient R(x), which reaches its minimum at the smallest eigenvalue \u03bbmin when x = v1,\n\u03bbmin = R(v1) = vT1 Lv1 vT1 v1\n(4)\nWe can equivalently express the graph Laplacian regularizer as a signal prior in a Bayesian formulation; i.e., the probability Pr(x) of observing a signal x is:\nPr(x) \u221d exp ( \u2212x TLx\n\u03c32\n) (5)\nwhere \u03c3 is a parameter. Note that because vT1 Lv1 = 0, the first eigenvector v1 has the largest probability Pr(v1)."}, {"heading": "D. General Graph-Signal Restoration", "text": "Given defined smoothness prior (5), we can now formulate a general graph-signal restoration problem via a MAP formulation. Suppose we are given a partial observation y \u2208 RK of the desired graph-signal x \u2208 RN , where K < N . The observation y may be corrupted by an additive noise z \u2208 RK , where z is zero-mean Gaussian noise with covariance Q. The noise model for y is thus:\ny = Hx + z (6)\nwhere H \u2208 {0, 1}K\u00d7N is a binary matrix that extracts the K components from x corresponding to y. Given the noise model (6), we can define a likelihood term Pr(y|x):\nPr(y|x) \u221d exp ( \u2212 (y \u2212Hx)\nTQ\u22121(y \u2212Hx) \u03c32\n) (7)\nGiven prior (5) and likelihood (7), we can formulate a MAP problem where instead of posterior Pr(x|y), we maximize the\n4 product Pr(y|x)Pr(x), or equivalently, minimize its negative log, resulting in:\nmin x\n(y \u2212Hx)TQ\u22121(y \u2212Hx) + \u00b5 xTLx (8)\nwhere \u00b5 is a parameter that trades off the importance between the likelihood term and the signal prior. For example, for the special case where x is a classifier signal and the observed labels y are noiseless, then Q is the identity matrix, and \u00b5 can be assigned a small value, so that (8) prioritizes the likelihood (fidelity) term much more than the smoothness prior term.\nBecause both terms in (8) are quadratic, (8) has a closedform optimal solution by taking the derivative with respect to x and setting it to 0:\nx\u2217 = ( HTQ\u22121H + \u00b5L )\u22121 HTQ\u22121y (9)\nBecause of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20]. In this paper, we would also like to retain this classical MAP formulation as much as possible.\nBy considering also negative edge weights in our work, negative eigenvalues \u03bbk are possible, and xTLx can be negative. That means xTLx can no longer be used as a prior for MAP formulation (8), because if v1 computes to vT1 Lv1 < 0, then \u221ev1 would be a pathological optimal solution. This is another motivation to search for a perturbation matrix \u2206 such that L + \u2206 is PSD, so that prior xT (L + \u2206)x can be used to formulate a numerically stable MAP problem in (8)."}, {"heading": "E. Graph Construction", "text": "While our to-be-discussed optimization algorithm is applicable for any undirected graph with positive and negative edge weights, proper assignment of negative edges makes a big difference in actual classification performance. We discuss our edge assignment procedure in a graph G here.\nWe first construct a graph G with nodes V representing N samples. For each sample i, we compute a feature vector hi of some dimension D. Then we can assign non-negative edge weight wi,j using a Gaussian kernel:\nwi,j = exp\n( \u2212 (hi \u2212 hj)\nT\u039e(hi \u2212 hj) \u03c32h\n) (10)\nwhere \u03c3h is a parameter. \u039e is a D\u00d7D diagonal matrix, where \u039ei,i is a feature parameter for the i-th feature. We can assign positive edge weights wi,j to the \u03c9\u2019s nearest neighbors j to node i, while the rest of the nodes have no edges3 to i.\nThis weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight. To improve clustering / classification performance, a feature parameter \u039ei,i is set larger if the i-th feature is more discriminate. For more detailed optimization of feature parameters \u039ei,i\u2014which is not the focus of this paper\u2014see [39].\n3If this relationship is not symmetric, i.e., if i is one of \u03c9 closest neighbors to j but j is not one of \u03c9 closest neighbors to i, then we keep edge (i, j) of weight wi,j anyway. Thus each node has \u2265 \u03c9 neighbors.\nWe next assign negative edges into G as follows. We perform clustering on G to find 2k clusters if k negative edges are targeted for insertion. For each cluster, we identify a centroid, and we connect pairs of centroids that are furthest in weighted feature distance with edges of negative weights (e.g. \u22121). The intuition is that, to maximize classification performance, each negative edge should affect labeling of a maximum number of similar nodes in the neighborhood, hence connecting cluster centroids with large feature distance is reasonable.\nNote also that, in general, more negative edges mean the graph Laplacian L is more indefinite, thus requiring larger perturbation \u2206 to make L+\u2206 PSD. Thus in practice we add only a small number of negative edges to G, that nonetheless leads to noticeable improvement in classification results, as shown in Section VII."}, {"heading": "F. Example of Graph with Negative Edges", "text": "As illustration, we consider a simple example in Fig. 3(a): a 12-node graph where nodes 1 through 6 are similar and are connected by edges of weight 1, and nodes 7 to 12 are similar. Nodes between the two classes are connected by edges of weight 0.1, except pairs (2, 8) and (5, 11)\u2014centroids of four clusters\u2014that are connected by edges of weight \u22121. Graph Lapalacian L for this graph has two negative eigenvalues \u22120.94 and \u22120.76, and the corresponding eigenvectors v1 and v2 are shown in Fig. 3(b).\nWe observe that v1 exhibits the desired behavior of the ground truth classifier x already: similar nodes 1 through 6 (nodes 7 through 12) have the same sign. On the other hand, while v2 have values of opposite sign at connected pairs (2, 8) and (5, 11), which are consistent with the two negative edges, values at connected pairs (3, 4) and (9, 10) have opposite signs, which are not consistent with the corresponding positive edges. Thus for signal restoration, information-rich eigenvectors V should ideally be preserved, and since v1 is more valuable than v2, it should be weighted accordingly in the regularizer. We will return to this point in the next section.\nWe observe also that if we assign two negative edges at node pairs across the two classes different from (2, 8) and (5, 11), then \u03bbmin would have a strictly larger magnitude. For example, assigning \u22121 to edges (1, 7) and (6, 12) (and 0.1 to edges (2, 8) and (5, 11)) would result in \u03bbmin = \u22121.32. It turns out that connecting cluster centroids of the two classes\n5 with negative edges tends to result in the smallest magnitude \u03bbmin, and thus the smallest perturbation \u2206 required to make L + \u2206 PSD. This provides an explanation why our proposed negative edge assignment based on cluster centroids leads to the best classification performance on average."}, {"heading": "IV. FINDING A PERTURBATION MATRIX", "text": "We now address the problem of identifying a perturbation matrix \u2206 such that L + \u2206 is PSD. To impart intuition on the effects of \u2206 on the eigenvalues of L + \u2206, consider first the Weyl\u2019s inequality4. Let symmetric matrix L \u2208 RN\u00d7N have spectral decomposition L = V\u039bVT as described in (2), with eigenvalues \u03bbk along the diagonal of diagonal matrix \u039b. Let \u2206 be a Hermitian matrix with eigenvalues \u03b31 \u2264 . . . \u2264 \u03b3N . Weyl\u2019s inequality states that L + \u2206 has eigenvalues \u03bd1 \u2264 . . . \u2264 \u03bdN , such that:\n\u03bbi + \u03b31 \u2264 \u03bdi \u2264 \u03bbi + \u03b3N (11)\nIn words, (11) states that the i-eigenvalue \u03bdi of L + \u2206 is the i-eigenvalue \u03bbi of L shifted by an amount in the range [\u03b31, \u03b3N ]. Assume that \u03bbmin < 0, hence L is indefinite. The Weyl\u2019s inequality then implies that for L + \u2206 to be PSD: i) a necessary (but not sufficient) condition is \u03b3N \u2265 \u2212\u03bbmin; ii) a sufficient (but not necessary) condition is \u03b31 \u2265 \u2212\u03bbmin.\nObviously, there exists an infinite number of feasible solutions \u2206. Thus a well chosen criteria must be used to differentiate them."}, {"heading": "A. Matrix Perturbation: Minimum-Norm Criteria", "text": "One reasonable choice is the minimum norm criteria, i.e., find \u2206 with the smallest norm such that L + \u2206 is PSD:\nmin \u2206 \u2016\u2206\u2016 s.t. xT (L + \u2206) x \u2265 0, \u2200x (12)\nwhere \u2016.\u2016 is a unitarily invariant norm on RN\u00d7N ; i.e. \u2016U\u2206V\u2016 = \u2016\u2206\u2016 for all orthogonal U and V.\nIt turns out that the solution to (12) is a special case of Theorem 5.1 in [40], which we rephrase as follows. Assume that L has exactly p negative eigenvalues. Theorem 5.1 in [40] states that the optimal perturbation matrix \u2206 with minimum norm \u2016\u2206\u2016, such that L + \u2206 is PSD, is:\n\u2206 = V diag(\u03c4 ) VT (13)\nwhere \u03c4 = [\u03c41, . . . , \u03c4n]:\n\u03c4i = { \u2212\u03bbi if 1 \u2264 i \u2264 p 0 o.w. (14)\nSee [40] for a complete proof. We only make a few important observations. First, it is clear that L + \u2206 is PSD:\nL + \u2206 = Vdiag(\u03bb1 \u2212 \u03bb1, . . . , \u03bbp \u2212 \u03bbp, \u03bbp+1, . . . , \u03bbn)VT\n= Vdiag(0, . . . , 0, \u03bbp+1, . . . , \u03bbn)V T\nSince all the negative eigenvalues of L have been eliminated, L + \u2206 is PSD.\nSecond, due to the definition (13) of \u2206, L + \u2206 can be spectrally decomposed using the same eigenvectors V as\n4https://en.wikipedia.org/wiki/Weyl%27s inequality\noriginal L. As discussed previously, maintaining the same eigen-space in the perturbed matrix L + \u2206 is desirable.\nThird, by eliminating all negative eigenvalues of L to 0, the first p+ 1 eigenvectors v1, . . . ,vp will all evaluate to 0 in the quadratic regularizer:\nvTi (L + \u2206) vi = 0, 1 \u2264 i \u2264 p+ 1 (15)\np + 1 because L contains the DC component 1 that the regularizer also evaluates to 0. Hence the regularizer expresses no preference among the first p+1 eigenvectors. This is a problem during graph-signal restoration. This means that though the graph structure G has a notion of frequencies and the original (numerically unstable) smoothness prior prefers low frequencies, the augmented regularizer does not differentiate and maps the lowest p + 1 frequencies all to zero. This is clearly sub-optimal."}, {"heading": "B. Matrix Perturbation: Minimum-Variance Criteria", "text": "The main problem with the minimum-norm criteria is that the differentiation among different frequency components (eigenvectors) is removed by setting all negative eigenvalues of L to 0. Thus, it is desirable to perturb L in a way that the frequency preferences are preserved (i.e., low frequencies are still preferred over high frequencies), while the overall perturbation is minimized. We mathematically define this notion as follows.\nFirst, considering the regularizer xTLx as a scalar function of eigenvectors vi, we define the gradient differential gi\u2014 the change in gradient\u2014for each eigenvector vi after adding perturbation matrix \u2206 to L:\ngi = \u2223\u2223\u2223\u2223\u2202(xTLx)\u2202vi \u2212 \u2202(x T (L + \u2206)x) \u2202vi \u2223\u2223\u2223\u2223 (16) We then define the average gradient differential g\u0304 for all eigenvectors vi:\ng\u0304 = 1\nN N\u2211 i=1 gi (17)\nFinally, we can define our perturbation criteria as follows:\nmin \u2206\n( g\u0304 + \u03c1\nn\u2211 i=1\n(gi \u2212 g\u0304)2 )\n(18)\nsuch that the resulting perturbed matrix L + \u2206 is PSD. \u03c1 is a parameter that trades off the importance of the average gradient differential with the variance of gradient differentials. We call this criteria in (18) the minimum-variance criteria, because it seeks to minimize the difference in gradient changes among different eigenvectors.\nBecause L is a weighted sum of outer-products vjvTj , gradient of xTLx with respect to vi is:\n\u2202(xTLx) \u2202vi = \u2202 \u2211 j \u03bbjx Tvjv T j x \u2202vi = 2\u03bbi\u03b1ivi (19)\nwhere \u03b1i is the i-th frequency coefficient of x.\n6 Assuming that the perturbed matrix L + \u2206 also retains the same eigenvectors V, then the gradient differential gi can be simplified to:\ngi = |2\u03bbi\u03b1ivi \u2212 2\u03c4i\u03b1ivi| = 2 |(\u03bbi \u2212 \u03c4i)\u03b1i|\nwhere \u03c4i is the i-th eigenvalue of L+\u2206. The last step follows because vi is an unit-norm eigenvector. Instead of considering all possible x, we make a simplifying assumption that |\u03b1i| = 1 for all i and drop the constant 2. Finaly, gi simplifies to:\ngi = |\u03bbi \u2212 \u03c4i| (20)\nIf \u03c1 = 0, then the objective (18) becomes the minimumnorm criteria, and \u2206 defined in (13) is optimal. As discussed, this solution does not differentiate among the first p + 1 frequencies vi, and thus we should set \u03c1 to a larger value to avoid this solution.\nIf \u03c1 is sufficiently large, then the deviation of each gi from g\u0304 becomes the dominant concern. To construct a solution for this case, we define a generalized graph Laplacian matrix Lg [37] as the sum of L and an identity matrix5 I scaled by \u2212\u03bbmin:\nLg = \u2212\u03bbminI + L (21) = V(\u2212\u03bbmin)I VT + V\u039bVT\n= V (\u2212\u03bbminI + \u039b) VT (22)\nwhere \u03bbmin is the smallest eigenvalue in L. We see that Lg has the same eigenvectors V as L, and eigenvalues \u03bbi\u2212\u03bbmin are non-negative, and thus Lg is PSD. Lg is the solution we will focus on in the sequel."}, {"heading": "C. A Simple Lower Bound for \u03bbmin", "text": "we can compute a lower bound for \u03bbmin simply as follows. Denote by L+ and L\u2212 the graph Laplacian matrices corresponding to edges with positive and negative weights in graph G respectively; clearly L = L+ + L\u2212. The Rayleigh quotient for L can be expanded as:\nxTLx\nxTx =\nxT (L+ + L\u2212) x\nxTx (23)\nBecause L+ containing only positive edges is PSD, the first term in the numerator xTL+x is lower-bounded by 0. For the second term, we can first define L\u2212 = \u2212L\u2212, which is PSD, and write:\nxTL\u2212x\nxTx = \u2212\n( xTL\u2212x\nxTx\n) \u2265 \u2212\u03bb\u2212max (24)\nwhere \u03bb\u2212max is the largest eigenvalue of L\u2212. Since \u2212\u03bb\u2212max is also the lower bound of the Rayleigh quotient for L, it is also the lower bound for the smallest eigenvalue \u03bbmin of L:\n\u2212 \u03bb\u2212max \u2264 \u03bbmin (25)\nThus a perturbation matrix \u2206 = \u03bb\u2212maxI would result in L+\u2206 that is PSD. However, the computation of \u03bb\u2212max is O(N\n3) in the worst case [41], and this lower bound is often loose in practice. We next discuss a faster computation of a lower bound for \u03bbmin.\n5We abuse notation slightly here by writing \u03bbI to mean a scalar \u03bb times an \u201cappropriately sized\u201d identity matrix for the matrix addition."}, {"heading": "V. FAST COMPUTATION", "text": "The complexity of eigen-decomposition to compute the smallest eigenvalue \u03bbmin of graph Laplacian L is O(N3) [41]. For a big graph with a large number of nodes, this can be expensive. We thus propose a procedure to compute a fast lower bound \u03bb#min for \u03bbmin, so that when adding perturbation matrix \u2206# = \u2212\u03bb#minI to L, the resulting L + \u2206# is PSD."}, {"heading": "A. Matrix Inertia", "text": "We first define matrix inertia. The inertia In(A) of a matrix A is a set of three numbers counting the positive, negative, and zero eigenvalues in A:\nIn(A) = ( i+(A), i\u2212(A), i0(A) ) (26)\nwhere i+(A), i\u2212(A) and i0(A) denote respectively the number of positive, negative and zero eigenvalues in matrix A. Inertia is an intrinsic property of the matrix; acccording to Sylvester\u2019s Law of Inertia6, the intertia of a matrix is invariant to any congruent transform, i.e.,\nIn(A) = In(PTAP) (27)\nwhere P is an invertible matrix."}, {"heading": "B. Graph Partition", "text": "To reduce complexity, we can divide the node set N into two subsets N1 and N2, so that intensive computation is performed in the node subsets separately. Note that partitioning a graph into two node sets to reduce complexity is also done in Kron reduction [42]. However, [42] considers only PSD L (possibly with self-loops), while we consider indefinite L that requires perturbation \u2206 to make L + \u2206 PSD.\nGiven the two sets N1 and N2, we can write the graph Laplacian L in blocks:\nL = [ L1,1 L1,2 LT1,2 L2,2 ] (28)\nwhere L1,1 and L2,2 are sub-matrices of respective dimension |N1| \u00d7 |N1| and |N2| \u00d7 |N2| corresponding to node sets N1 and N2, and L1,2 is a |N1| \u00d7 |N2| sub-matrix corresponding to cross-connections between N1 and N2.\nWe can now relate the inertia of L with its sub-matrices using the Haynsworth Inertia Additivity formula [21]:\nIn(L) = In(L1,1) + In(L/L1,1) (29)\nwhere L/L1,1 is the Schur Complement7 (SC) of block L1,1 of matrix L in (28), which is defined as\nL/L1,1 = L2,2 \u2212 LT1,2L\u221211,1L1,2 (30)\nThus, if we can ensure that L1,1 and its SC do not contain negative eigenvalues, then L will also have no negative eigenvalues and is PSD. We develop an efficient algorithm based on this idea next.\n7"}, {"heading": "C. Eigenvalue Lower Bound Algorithm", "text": "We propose the following recursive algorithm to find a lower bound \u03bb#min for L. See Fig. 4 for an illustration. We initialize t := 0 and L0 := L. We define a recursive algorithm EvalBound(Lt, t) that returns a lower bound \u03bbtmin for eigenvalues in Lt. It has two steps as described below.\nStep 1: We first partition node set N t in Lt into two subsets N t1 and N t2 , where |N t1 | = r. r is a pre-defined parameter to control computation complexity. N t1 can be chosen by first randomly selecting a node in N t, then perform breadth-first search (BFS) [43] until r nodes are discovered. We eigendecompose Lt1,1 to find its smallest eigenvalue \u03bb t 1. We define the augmented eigenvalue \u03batmin as:\n\u03batmin = { \u03bbt1 \u2212 if \u03bbt1 \u2264 0 0 o.w. (31)\nwhere > 0 is a small parameter. We perturb matrix Lt using computed \u03batmin, i.e. Lt = Lt \u2212 \u03batminI. It is clear that Lt1,1 is positive definite (PD) and thus invertible.\nStep 2: We ensure SC of Lt1,1 of Lt is PSD. By definition, the SC is:\nLt/Lt1,1 = Lt2,2 \u2212 (Lt1,2)T (Lt1,1)\u22121Lt1,2 (32)\nLt/Lt1,1 can be interpreted as a |N t2 | \u00d7 |N t2 | graph Laplacian matrix for nodes N t2 . If |N t2 | \u2264 r, then we eigen-decompose Lt/Lt1,1 and find its smallest eigenvalue \u03bbt2. We compute \u03bbtmin := \u03ba t min + min (\u03bb t 2, 0). We exit the algorithm with \u03bb t min as solution. If |N t2 | > r, we set Lt+1 := Lt/Lt1,1 and recursively call \u03b7tmin := EvalBound(L t+1, t+ 1). Upon return, we compute \u03bbtmin := \u03ba t min + \u03b7 t min and exit the algorithm with \u03bb t min as solution."}, {"heading": "D. Proof of Algorithm Correctness", "text": "We now prove that EvalBound(L, 0) returns a lower bound for the true minimum eigenvalue \u03bbmin of L. Specifically, we prove by induction the following recursion invariant: At each recursive call t, given Lt, the computed \u03bbtmin is a lower bound for eigenvalues of Lt.\n6https://en.wikipedia.org/wiki/Sylvester%27s law of inertia 7https://en.wikipedia.org/wiki/Schur complement\nWe first examine the base case. At a leaf recursive call \u03c4 , in Step 1, L\u03c4 is perturbed using computed \u03ba\u03c4min such that L\u03c41,1 = L\u03c41,1 \u2212 \u03ba\u03c4minI is PD. In Step 2, if computed \u03bb\u03c42 \u2265 0, then SC L\u03c4/L\u03c41,1 is PSD. Since L\u03c41,1 and its SC L\u03c4/L\u03c41,1 are both PSD, by (29) L\u03c4 is also PSD. Hence \u03bb\u03c4min = \u03ba\u03c4min is a lower-bound for matrix L\u03c4 .\nIf \u03bb\u03c42 < 0, then perturbed SC, L\u03c4/L\u03c41,1 \u2212 \u03bb\u03c42I, is PSD. It turns out that if we perturb L\u03c4 again using \u03bb\u03c42 , i.e., L\u2032\u03c4 = L\u03c4 \u2212\u03bb\u03c42I, then L\u2032\u03c4 is PSD. This is because L\u2032\u03c41,1 = L\u03c41,1\u2212\u03bb\u03c42I is PD, and its SC L\u2032\u03c4/L\u2032\u03c41,1 is PSD by the following lemma:\nLemma 1. If L1,1 is PD and L/L1,1 + \u03b4I is PSD for \u03b4 > 0, then SC L\u2032/L\u20321,1, where L \u2032 = L + \u03b4I, is also PSD.\nSee Appendix for a full proof. In this case \u03bb\u03c4min = \u03ba \u03c4 min +\n\u03bb\u03c42 , hence \u03bb \u03c4 min is a lower bound for L \u03c4 . Consider now the inductive case, where at iteration t we assume that, \u03b7tmin := EvalBound(L t+1, t + 1) is a lower bound for Lt+1 = Lt/Lt1,1. From Step 1, we know that Lt is perturbed using \u03batmin so that Lt1,1 = Lt1,1 \u2212 \u03batminI is PD. By assumption, we know that Lt/Lt1,1 can be perturbed using \u03b7tmin such that Lt/Lt1,1 \u2212 \u03b7tminI is PSD. By Lemma 1, we know that Lt\u2212 \u03b7tminI is also PSD. Thus \u03bbtmin := \u03batmin + \u03b7tmin is a lower bound for Lt.\nSince both the base case and the inductive case are proven, the recursion invariant is also proven, and EvalBound(L, 0) returns a lower bound for \u03bbmin of L."}, {"heading": "E. Computation Complexity", "text": "We can estimate the computation cost as follows. The number of recursive calls is N/r. For each recursive call, the dominant cost is eigen-decomposition of a r\u00d7r matrix, which requires O(r3) operations. Thus the overall complexity is O((N/r)r3) = O(Nr2). Compared to the complexity O(N3) of eigen-decomposition of the larger matrix L, this represents a non-trivial computation saving."}, {"heading": "VI. CLASSIFIER LEARNING WITH NOISY LABELS", "text": "Having discussed a fast method to compute \u2206 such that L + \u2206 is PSD, we now discuss specifically the noisy label learning scenario, where observed binary labels y are corrupted not by Gaussian noise (6), but uniform noise unique to binary classifiers. We first motivate our chosen label noise model and describe the negative log likelihood in Section VI-A. Beyond the graph-signal smoothness prior we defined in Section III-C, we can in addition define a generalized smoothness prior in Section VI-B, which promotes ambiguity in the classifier signal. We provide a novel interpretation of generalized smoothness on graph by viewing a graph-signal as voltages on an electrical circuit in Section VI-C. Finally, after formulating the problem with both prior terms in Section VI-D, we propose an algorithm to solve it in Section VI-E."}, {"heading": "A. Label Noise Model", "text": "To model binary label noise, we adopt a uniform noise model [26], where the probability of observing yi = xi,\n8 1 \u2264 i \u2264 K, is 1\u2212 p, and p otherwise; i.e.,\nPr(yi|xi) = {\n1\u2212 p if yi = xi p o.w. (33)\nThis noise model is motivated by the following observation in social media analysis, when labels are often assigned manually by non-experts via crowd-sourcing [26]\u2014i.e. employ many non-experts online to assign labels to a subset of data at a very low cost. Because non-experts can be unreliable (e.g., a nonexpert is not competent in a label assignment task but pretends to be, or he simply assigns label randomly to minimize mental effort), observations y may result in label errors or noise that are uniform and independent.\nThe probability of observing a noise-corrupted y given ground truth x is hence:\nPr(y|x) = pk(1\u2212 p)K\u2212k\nk = \u2016y \u2212Hx\u20160 (34)\n(34) serves as the new likelihood term for this noise model (33). The negative log of this likelihood Pr(y|x) can be rewritten as:\n\u2212 logPr(y|x) = k (log(1\u2212 p)\u2212 log(p))\ufe38 \ufe37\ufe37 \ufe38 \u03b3 \u2212K log(1\u2212 p)\n(35)\nBecause the second term is a constant for fixed K and p, we can ignore it during minimization."}, {"heading": "B. Generalized Smoothness", "text": "Having derived a new likelihood term in (34), we next describe a generalized version of the graph-signal smoothness prior (3) for classifier signal reconstruction.\n1) Positive Edge Weights for Generalized Smoothness: Like TGV for images [22], one can also define a higher-order notion of smoothness for graph-signals using positive edge weights [8]. Specifically, positive edge graph Laplacian L+ is related to the second derivative of continuous functions [7], and so L+x computes the second-order difference on graph-signal x. As an example, the 3-node line graph in Fig. 2 with all edge weights equal to 1 has the following L+:\nL+ =  1 \u22121 0\u22121 2 \u22121 0 \u22121 1  (36) Using the second row L+2,: of L\n+, we can compute the secondorder difference at node x2:\nL+2,:x = \u2212x1 + 2x2 \u2212 x3 (37)\nOn the other hand, the definition of second derivative8 of a function f(x) is:\nf \u2032\u2032(x) = lim h\u21920 f(x+ h)\u2212 2f(x) + f(x\u2212 h) h2\n(38)\nWe see that (37) and (38) are computing the same quantity (with a sign change) in the limit.\n8https://en.wikipedia.org/wiki/Second derivative\nHence if |L+x| is small, then the second-order difference of x is small, or the first-order difference of x is smooth or changing slowly. In other words, the gradient of the signal is smooth with respect to the graph. We express this notion by stating that the square of the l2-norm of L+x is small:\n\u2016L+x\u201622 = xT (L+)TL+x = xT (L+)2x = \u2211 i (\u03bb+i ) 2\u03b12i\n(39) where (39) is true since L+ is symmetric by definition.\n2) Negative Edges for Generalized Smoothness: We illustrate now why negative edge weights should not be used when considering generalized smoothness. Consider again the threenode line graph in Fig. 2, where w = 1. The corresponding second row of the graph Laplacian L is:\nL2,: = [ 1 0 \u22121 ]\n(40)\nThis means that when we compute the generalized smoothness |Lx| at x2, we get |L2,:x| = |x1 \u2212 x3|; i.e., the generalized smoothness at x2 does not actually depend on the value of x2! We provide an intuitive explanation why negative edge weights should not be used next."}, {"heading": "C. Circuit Interpretation of Generalized Smoothness", "text": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44]. We follow a similar approach when attempting to understand generalized smoothness. Suppose we interpret an edge (i, j) as a wire between nodes i and j, and an edge weight wi,j as conductance (equivalently, 1/wi,j as the resistance) between its two endpoints. Let xi and xj represent the voltage at the two endpoints. According to Ohm\u2019s law9, the current ci,j between the two nodes is the voltage difference at the entpoints times the conductance:\nci,j = wi,j(xi \u2212 xj) (41)\nBy Kirchhoff\u2019s current law10 (KCL), the net sum of the currents flowing into a node is zero. Applying KCL to node 2 in the three-node line graph in Fig. 2 connected by weights w1,2 and w2,3, we can write:\nw1,2(x1 \u2212 x2) + w2,3(x3 \u2212 x2) = 0 (42)\nusing Ohm\u2019s law (41). If we desire a signal x to satisfy this condition maximally, we can minimize the absolute value of this current sum:\nmin x \u2223\u2223[ \u2212w1,2 (w1,2 + w2,3) \u2212w2,3 ] x\u2223\u2223 = \u2223\u2223L+2,:x\u2223\u2223 (43) This is in fact the generalized graph-signal smoothness condition we discussed in Section VI-B. Thus we can conclude the following: a graph-signal x on graph G that is perfectly generalized smooth, i.e. |L+x| = 0, is a voltage signal on G that satisfies KCL.\nThis electrical network interpretation also provides an argument why negative edges should not be considered for\n9https://en.wikipedia.org/wiki/Ohm%27s law 10https://en.wikipedia.org/wiki/Kirchhoff%27s circuit laws\n9 generalized smoothness. As done in [42], a generalized graph Laplacian Lg with diagonal element Li,i \u2265 \u2211 j|j 6=i Li,j can be considered a conductance matrix, where an edge (i, j) has branch conductance \u2212Li,j and node i has shunt conductance Li,i\u2212 \u2211 j|j 6=i Li,j \u2265 0. For such a resistive circuit, Ohm\u2019s law is applicable directly and thus KCL is meaningful. However, a negative conductance Li,j < 0 (equivalently, a negative resistance) means the circuit is no longer resistive, and Ohm\u2019s law is not applicable. As a result, KCL\u2014by extension generalized graph-signal smoothness\u2014is no longer meaningful."}, {"heading": "D. Objective Function", "text": "We can now combine the likelihood (34) and the two prior terms together to define an optimization objective as follows:\nmin x \u2016y \u2212Hx\u20160 \u03b3 + \u03c3 \u22122 0 x T Lg x + \u03c3 \u22122 1 x T (L+)2 x (44)\n1) Interpretation of Smoothness Priors for Classifiers: We interpret the two smoothness terms in the context of binary classification. We know that the true signal x is indeed piecewise constant (PWC); each true label xi is binary, and labels of the same class cluster together in the same feature subspace. The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20]. Hence the smoothness prior is appropriate.\nRecall that the purpose of TGV [22] is to avoid oversmoothing a ramp (linear increase / decrease in pixel intensity) in an image, which would happen if only a TV prior is used. A ramp in the reconstructed signal x\u0302 in our classification context would mean an assignment of label other than \u22121 and 1, which can reflect the confidence level in the estimated label; e.g., a computed label x\u0302i = 0.3 would mean the classifier has determined that event i is more likely to be 1 than \u22121, but the confidence level is not high. We can thus conclude that the generalized smoothness prior can promote an appropriate amount of ambiguity in the classification solution instead of forcing the classifier to make hard binary decisions. As a result, the mean square error (MSE) of our solution with respect to the ground truth labels is small.\nE. Iterative Reweighted Least Squares Algorithm\nTo solve (44), we employ the following optimization strategy. We first replace the l0-norm in (44) with a weighted l2norm:\nmin x\n(y\u2212Hx)TB(y\u2212Hx)\u03b3 + \u03c3\u221220 x T Lg x + \u03c3 \u22122 1 x T (L+)2 x\n(45) where B is a K\u00d7K diagonal matrix with weights b1, . . . , bK on its diagonal. In other words, the fidelity term is now a weighted sum of label differences: (y\u2212Hx)TB(y\u2212Hx) =\u2211K i=1 bi(yi \u2212Hi,:x)2. The weights bi should be set so that the weighted l2norm mimics the l0-norm. To accomplish this, we employ the iterative reweighted least squares (IRLS) strategy [24], which has been proven to have superlinear local convergence, and\nsolve (45) iteratively, where the weights b(t+1)i of iteration t+ 1 is computed using solution x(t)i of the previous iteration t, i.e.,\nb (t+1) i =\n1\n(yi \u2212Hi,:x(t))2 + (46)\nfor a small > 0 to maintain numerical stability. Using this weight update, we see that the weighted quadratic term (y \u2212 Hx)TB(y\u2212Hx) mimics the original l0-norm \u2016y \u2212Hx\u20160 in the original objective (44) when the solution x converges.\n1) Closed-Form Solution per Iteration: For a given weight matrix B, it is clear that the objective (45) is an unconstrained quadratic programming problem with three quadratic terms. One can thus derive a closed-form solution by taking the derivative with respect to x and equating it to zero, resulting in:\nx\u2217 = ( \u03b3HTBH + \u03c3\u221220 Lg + \u03c3 \u22122 1 (L +)2 )\u22121 \u03b3HTBTy (47)\nF. Interpreting Computed Solution x\u0302\nAfter the IRLS algorithm converges to a solution x\u0302, we interpret the classification results as follows. We perform thresholding by a pre-defined value \u03c4 on x\u0302 to divide it into three parts, including the rejection option for ambiguous labels:\nxi =  1, x\u2217i > \u03c4\nRejection, \u2212\u03c4 < x\u2217i < \u03c4 \u22121, x\u2217i < \u2212\u03c4.\n(48)\nTypically, the fraction of tolerable rejection labels is set per application requirement. Clearly, eliminating more ambiguous labels leads to a larger tolerable rejection rate, resulting in a smaller classification error rate."}, {"heading": "VII. EXPERIMENTATION", "text": ""}, {"heading": "A. Experiment Setup", "text": "1) Datasets for Training and Testing: To evaluate the performances of different classification methods, we selected three two-class datasets from the KEEL (Knowledge Extraction based on Evolutionary Learning) database [46], which contains a rich collection of labelled and unlabeled datasets for data mining and analysis. The first dataset called \u201cSonar, Mines vs. Rocks\u201d contains various patterns obtained by bouncing sonar signals off metal cylinders and rocks at various angles (spanning 90 degrees for mines and 180 degrees for rocks) and under various conditions. Each pattern is a set of 60 numbers in the range 0.0 to 1.0, where each number represents the energy within a particular frequency band, integrated over a certain period of time. These patterns are used to classify whether an object is a metal cylinder or a rock.\nThe second dataset is the Banana dataset, an artificial dataset where 5300 instances belong to several clusters with a banana shape. In the dataset, two attributes were extracted to classify two kinds of banana shapes.\nThe third is the Titanic dataset that gives the values of four categorical attributes (social class, age, gender, and survival status) for each of the 2201 people on board the Titanic when it struck an iceberg and sank. This natural dataset was used\n10\nto study how survival (i.e., the two classes are survival and death) relates to the other three attributes.\nFor our experiment, we randomly sampled 210 instances from the first dataset and 300 instances from the second and third, and used 70% of the samples as training data and 30% as testing data. We repeated the process 100 times for each dataset and then calculated the average performance of the 100 experiments in terms of classification error rate.\n2) Graph Construction: To construct a graph with negative edge weights for our proposed methods, we first constructed a graph with non-negative edge weights. For each sample (node), we found its three nearest neighbors according to the Euclidean distances between the node and its neighbors, and connected these nodes using edges with non-negative weights that are normalized to [0,1] using the Gaussian kernel in (10). We performed clustering on the graph and found the centroids (chosen from labelled nodes) of clusters as explained in Section III-E. We then paired the cluster centroids with different labels, and assigned a negative edge weight between each pair with a value normalized to [\u22121,0] where the magnitude is inversely proportional to the Euclidean distance between the pair. In the experiments, we added two negative edge weights in each constructed graph.\n3) Comparison Schemes: We tested our proposed algorithm against five schemes: i) linear SVM, ii) SVM with a RBF kernel (named SVM-RBF), iii) a more robust version of the famed AdaBoost called RobustBoost [25] that claims robustness against label noise, iv) a graph classifier with the graph-signal smoothness prior (3) where the edge weights of the graph are all positive (named Graph-Pos), and v) a graph classifier with a graph containing negative edge weights where the graph Laplacian L is perturbed by the minimum-norm perturbation criteria in (12) to eliminate negative eigenvalues for numerical stability (named Graph-MinNorm).\nWe implemented two variants of our proposed graph classifier: i) our proposed minimum-variance perturbation method (named Proposed) using the generalized graph Laplacian Lg but without the generalized smoothness term (i.e., \u03c3\u221221 = 0 in (44) and \u03c4 = 0 in (48)), and vii) the proposed method in (44) with rejection (named Proposed-Rej) where the rejection rate is controlled to be within 9\u201310% by tuning parameters in (48)."}, {"heading": "B. Performance Evaluation", "text": "To test the robustness of different classification schemes against label error, we randomly selected a portion of samples\nfrom the training set and reversed their labels. All the classifiers were then trained using the same set of features and labels. Each test set was classified by the classifiers and the results are compared with the ground-truth labels."}, {"heading": "1) Numerical comparisons for different label noise:", "text": "The resulting classification error rates for the three datasets using different classifiers are presented in Tables I\u2013III, where the percentage of randomly erred training labels ranges from 0% to 25%. The comparisons show that our proposed scheme achieved lower classification error when compared to five competing schemes at almost all training label error rates. The parameters (\u03b3, \u03c30, \u03c31, \u03c4) used for the three datasets respectively are: (0.0002, 1, 7.8, [0.017, 0.025]), (0.0001, 3, 7.8, [0.01, 0.055]), (0.0001, 1.2, 10, [0.0303, 0.1019]). Compared to the graph classifier with all positive edge weights, our results show that adding negative edge weights can effectively improve the classification accuracy by 1\u20133%. We observe also that our proposed minimum-variance-based matrix perturbation scheme also outperformed the minimum-norm based scheme (Graph-MinNorm) in classification accuracy.\n11\nFig. 5. Comparison of the actual smallest eigenvalues and their lowerbounds using r = 50 and 70 for the \u201cSonar, Mines vs. Rocks\u201d dataset (N = 147), corresponding to 88% and 77% computation reduction.\nFig. 6. Comparison of the actual smallest eigenvalues and their lowerbounds using r = 70 and 100 for the Banana dataset (N = 210), corresponding to 89% and 77%, computation reduction, respectively.\n2) Graph classifier with non-zero rejection rate: By allowing a certain amount of ambiguous samples to remain unlabelled (less than 10% rejection rate in our experiments), our proposed generalized graph-signal smoothness prior can further improve the classification accuracy. We note that a user may define the desired classifier performance as a weighted sum of classification error and rejection rate for different applications, as done in [47]. Table IV shows the classification error and rejection rates in \u201cSonar, Mines vs. Rocks\u201d dataset for our proposed method with rejection under different training label error rates and \u03c3\u221221 , where the values of \u03c4 are set the same as that used in the first row. It shows that as \u03c3\u221221 for the generalized smoothness term increases, the rejection rate also increases, which is consistent with our explanation in Section VI-B that the second smoothness term promotes ambiguity in the solution instead of forcing the solution to be strictly binary. As a result, using our algorithm, one can thus tune \u03c3\u221221 and \u03c4 to adjust the preference of classification error versus rejection rate.\n3) Fast computation: In Section V-C, we proposed a fast eigen-decomposition scheme to lower-bound the smallest eigenvalue \u03bbmin of the graph Laplacian L for a graph with negative edge weights. The computed lower bound \u03bb#min is used to approximate the minimum-variance perturbation matrix \u2206 = \u2212\u03bb#minI in a much quicker manner. Fig. 5 to Fig. 7 compare the actual minimum eigenvalues (denoted Proposed) and their computed lower-bounds (denoted Fast) in the first 30 out of 100 sampled data subsets for the three datasets,\nFig. 7. Comparison of the actual smallest eigenvalues and their lowerbounds using r = 70 and 100 for the Titanic dataset (N = 210), corresponding to 89% and 77%, computation reduction, respectively.\nrespectively. In the experiment, we set parameter r to be about one half and one third of the number of samples N , which correspond to about 89% and 75% computation reduction, respectively, since the computation complexity is reduced from O(N3) to O(Nr2) as explained in Section V-E.\nThe results show that \u03bb#min computed by our proposed fast algorithm is an actual lower-bound for the true minimum eigenvalue \u03bbmin, i.e. \u03bb # min \u2264 \u03bbmin and the bound is tight most of the time. This ensures that the obtained Lg using matrix perturbation using \u2212\u03bb#minI is PSD, and not too far from the optimal \u2206. Table V compares the classification error rates of the proposed minimum-variance perturbation method (without rejection) with the actual minimum eigenvalues with the approximation computing the lower-bound eigenvalues by our proposed fast algorithm. Results show that the fast algorithm leads to slight performance differences compared to the full computation method."}, {"heading": "VIII. CONCLUSION", "text": "To address the semi-supervised learning problem, in this paper we view a classifier as a graph-signal in a highdimensional feature space, and pose a maximum a posteriori (MAP) problem to restore the classifier signal given partial and noisy labels. Unlike previous graph-based classifier works, we consider in addition edges with negative weights that signify anti-correlation between sample pairs. To achieve a stable signal smoothness prior, we derive an optimal perturbation matrix \u2206, so that when added to the graph Laplacian L, the matrix sum is positive semi-definite (PSD). We can compute a\n12\nfast approximation to \u2206 using a recursive algorithm based on the Haynsworth inertia additivity formula. Finally, we show that a generalized smoothness prior can promote ambiguity in the classifier signal, so that estimated labels with low confidence can be rejected. Experimental results show that our proposal outperforms SVM variants and previous graph-based classifiers using positive-edge graphs noticeably.\nAPPENDIX\nWe prove that if L1,1 is PD and L/L1,1 + \u03b4L is PSD for \u03b4 > 0, then given perturbed matrix L\u2032 = L + \u03b4I, L\u2032/L\u20321,1 is also PSD. By definition, L/L1,1 + \u03b4I is PSD means:\nxT ( L2,2 \u2212 LT1,2L\u221211,1L1,2 + \u03b4I ) x \u2265 0\nLet L1,1 be spectrally decomposed to L1,1 = V\u039bVT , where \u039b = diag(\u03bb1, . . . , \u03bbn) is a diagonal matrix containing all positive eigenvalues, since L1,1 is PD. We can thus rewrite above:\nxTL2,2x\u2212 xTLT1,2V\ufe38 \ufe37\ufe37 \ufe38 yT diag(\u03bb\u221211 , . . . , \u03bb \u22121 n ) V TL1,2x\ufe38 \ufe37\ufe37 \ufe38 y +\u03b4xTx\n= xTL2,2x + \u03b4x Tx\u2212 \u2211 i \u03bb\u22121i y 2 i (49)\nIf L is now perturbed by \u03b4I, we can similarly write the resulting SC L\u2032/L\u20321,1 in quadratic form:\nxT ( L2,2 + \u03b4I\u2212 LT1,2(L1,1 + \u03b4I)\u22121L1,2 ) x\n= xTL2,2x + \u03b4x Tx\u2212 xTLT1,2(L1,1 + \u03b4I)\u22121L1,2x (50)\nThe first two terms are the same as ones in (49). The third term can be rewritten as:\nxTLT1,2V\ufe38 \ufe37\ufe37 \ufe38 yT diag((\u03bb1 + \u03b4) \u22121, . . . , (\u03bbn + \u03b4) \u22121) VTL1,2x\ufe38 \ufe37\ufe37 \ufe38 y\n= \u2211 i (\u03bbi + \u03b4) \u22121y2i (51)\nSince \u03bbi > 0, we see that \u03bb\u22121i > (\u03bbi + \u03b4) \u22121. Hence this third term has magnitude strictly smaller than one in (49). Thus, non-negativity in (49) implies non-negativity in (50). Thus L\u2032/L\u20321,1 is PSD."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Label selection on graphs", "author": ["A. Guillory", "J. Bilmes"], "venue": "Twenty- Third Annual Conference on Neural Information Processing Systems, Vancouver, Canada, December 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning based on locally linear reconstruction", "author": ["L. Zhang", "C. Cheng", "J. Bu", "D. Cai", "X. He", "T. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no.10, October 2014, pp. 2026\u20132038.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal recovery on graphs: Variation minimization", "author": ["S. Chen", "A. Sandryhaila", "J. Moura", "J. Kovacevic"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no.17, September 2015, pp. 4609\u20134624.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Active semi-supervised learning using sampling theory for graph signals", "author": ["A. Gadde", "A. Anis", "A. Ortega"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, August 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral Graph Theory", "author": ["F. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, vol. 30, no.3, May 2013, pp. 83\u201398.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Image classifier learning from noisy labels via generalized graph smoothness priors", "author": ["Y. Mao", "G. Cheung", "C.-W. Lin", "Y. Ji"], "venue": "IEEE IVMSP Workshop, Bordeaux, France, July 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Depth map denoising using graphbased transform and group sparsity", "author": ["W. Hu", "X. Li", "G. Cheung", "O. Au"], "venue": "IEEE International Workshop on Multimedia Signal Processing, Pula, Italy, October 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Redefining self-similarity in natural images for denoising using graph signal gradient", "author": ["J. Pang", "G. Cheung", "W. Hu", "O.C. Au"], "venue": "APSIPA ASC, Siem Reap, Cambodia, December 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal graph Laplacian regularization for natural image denoising", "author": ["J. Pang", "G. Cheung", "A. Ortega", "O.C. Au"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, Brisbane, Australia, April 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Localized iterative methods for interpolation in graph structured data", "author": ["S.K. Narang", "A. Gadde", "E. Sanou", "A. Ortega"], "venue": "Symposium on Graph Signal Processing in IEEE Global Conference on Signal and Information Processing (GlobalSIP), Austin, TX, December 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Signal processing techniques for interpolation of graph structured data", "author": ["S.K. Narang", "A. Gadde", "A. Ortega"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada, May 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Expansion hole filling in depth-image-based rendering using graph-based interpolation", "author": ["Y. Mao", "G. Cheung", "A. Ortega", "Y. Ji"], "venue": "IEEE International Conference on Acousitics, Speech and Signal Processing, Vancouver, Canada, May 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Image interpolation during DIBR view synthesis using graph Fourier transform", "author": ["Y. Mao", "G. Cheung", "Y. Ji"], "venue": "3DTV-Conference, Budapest, Hungary, July 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "On constructing z-dimensional DIBR-synthesized images", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Multimedia, vol. 18, no.8, August 2016, pp. 1453\u2013 1468.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Image bitdepth enhancement via maximum-a-posteriori estimation of graph AC component", "author": ["P. Wan", "G. Cheung", "D. Florencio", "C. Zhang", "O. Au"], "venue": "IEEE International Conference on Image Processing, Paris, France, October 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Image bit-depth enhancement via maximum-a-posteriori estimation of AC signal", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Image Processing, vol. 25, no.6, June 2016, pp. 2896\u20132909.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Inter-block soft decoding of JPEG images with sparsity and graph-signal smoothness priors", "author": ["X. Liu", "G. Cheung", "X. Wu", "D. Zhao"], "venue": "IEEE International Conference on Image Processing, Quebec City, Canada, September 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph-based dequantization of block-compressed piecewise smooth images", "author": ["W. Hu", "G. Cheung", "M. Kazui"], "venue": "IEEE Signal Processing Letters, vol. 23, no.2, February 2016, pp. 242\u2013246.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "On the inertia of some classes of partitioned matrices", "author": ["E.V. Haynsworth", "A.M. Ostrowski"], "venue": "Linear Algebra and its Applications, vol. 1, no.2, 1968, pp. 299\u2013316.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1968}, {"title": "A TGV-based framework for variational image decompression, zooming and reconstruction. Part I: Analytics", "author": ["K. Bredies", "M. Holler"], "venue": "SIAM Jour, vol. 8, no.4, 2015, pp. 2814\u20132850.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. Devore", "M. Fornasier", "S. Gunturk"], "venue": "Communications on Pure and Applied Mathematics, vol. 63, no.1, January 2010, pp. 1\u201338.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "A more robust boosting algorithm", "author": ["Y. Freund"], "venue": "May 2009, https://arxiv.org/abs/0905.2138.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "The interaction between supervised learning and crowdsourcing", "author": ["A. Brew", "D. Greene", "P. Cunningham"], "venue": "Computational Social Science and the Wisdom of Crowds Workshop at NIPS, Whistler, Canada, December 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Editorial: Special issue on advances in learning with label noise", "author": ["B. Frenay", "A. Kaban"], "venue": "Elsevier: Neurocomputing, vol. 160, July 2015, pp. 1\u20132.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Twitter polarity classification with label propagation over lexical links and the follower graph", "author": ["M. Speriosu", "N. Sudan", "S. Upadhyay", "J. Baldridge"], "venue": "Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, July 2011.  13", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting emotions in social media: A constrained optimization approach", "author": ["Y. Wang", "A. Pal"], "venue": "Twenty-Fourh International Joint Conference on Artificial Intelligence, Buenos Aires, Argentina, July 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Discrete signal processing on graphs", "author": ["A. Sandryhaila", "J. Moura"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no.7, August 2013, pp. 1644\u20131656.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["\u2014\u2014"], "venue": "IEEE Signal Processing Magazine, vol. 31, no.5, August 2014, pp. 80\u2013 90.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal denoising on graphs via graph filtering", "author": ["S. Chen", "A. Sandryhaila", "J.M.F. Moura", "J. Kovacevic"], "venue": "IEEE Global Conference on Signal and Information Processing, Austin, TX, December 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, vol. 60, no.1-4, November 1992, pp. 259\u2013268.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1992}, {"title": "On the definiteness of the weighted laplacian and its connection to effective resistance", "author": ["D. Zelazo", "M. Burger"], "venue": "53rd IEEE Conference on Decision and Control, Los Angeles, CA, December 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "On the definiteness of graph laplacians with negative weights: Geometrical and passivity-based approaches", "author": ["Y. Cheng", "S.Z. Khong", "T.T. Georgiou"], "venue": "2016 American Control Conference, Boston, MA, July 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding gangs in war from signed networks", "author": ["L. Chu"], "venue": "22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining, San Francisco, CA, August 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Nodal domain theorems and bipartite subgraphs", "author": ["T. Biyikoglu", "J. Leydold", "P.F. Stadler"], "venue": "Electronic Journal of Linear Algebra, vol. 13, November 2005, pp. 344\u2013351.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.8, August 2000, pp. 888\u2013905.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2000}, {"title": "Automated variable weighting in k-means type clustering", "author": ["J.Z. Huang", "M.K. Ng", "H. Rong", "Z. Li"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no.5, May 2005, pp. 657\u2013 668.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Modifying the inertia of matrices arising in optimization", "author": ["N. Higham", "S.H. Cheng"], "venue": "ELSEVIER Linear Algebra and its Applications, vol. 275-279, May 1998, pp. 261\u2013279.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Numerical Linear Algebra", "author": ["L. Trefethen", "D. Bau"], "venue": "SIAM: Society for Industrial and Applied Mathematics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1997}, {"title": "Kron reduction of graphs with applications to electrical networks", "author": ["F. D\u00f6rfler", "F. Bullo"], "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 60, no.1, January 2013, pp. 150\u2013163.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Introduction to Algorithms", "author": ["Cormen", "Leiserson", "Rivest"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1986}, {"title": "Resistance distance", "author": ["D. Klein", "M. Randic"], "venue": "Journal of Mathematical Chemistry, vol. 12, no.1, December 1993, pp. 81\u201395.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1993}, {"title": "Graph Laplacian regularization for inverse imaging: Analysis in the continuous domain", "author": ["J. Pang", "G. Cheung"], "venue": "April 2016, https://arxiv.org/abs/1604.07948.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Keel: A software tool to assess evolutionary algorithms to data mining problems", "author": ["J.A.-F"], "venue": "Soft Computing, vol. 13, no.3, February 2009, pp. 307\u2013318.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C. Chow"], "venue": "IEEE Transactions on Information Theory, vol. 16, no.1, September 1970, pp. 41\u201346.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1970}], "referenceMentions": [{"referenceID": 0, "context": "A fundamental problem in machine learning is semisupervised learning [1]: given partially observed labels (possibly corrupted by noise) as input, train a classifier so that unclassified samples can also be appropriately assigned labels.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Among many approaches to the problem is a class of graphbased methods [2]\u2013[5] that model each sample as a node in a graph, connected to other nodes via undirected edges, with weights that reflect pairwise distances in a high-dimensional feature space.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "Among many approaches to the problem is a class of graphbased methods [2]\u2013[5] that model each sample as a node in a graph, connected to other nodes via undirected edges, with weights that reflect pairwise distances in a high-dimensional feature space.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": ", low graph frequencies that are eigenvectors of the graph Laplacian matrix) can be exploited for label assignment via spectral graph theory [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Conventional formulations in graph signal processing (GSP) [7] use non-negative edge weights that reflect inter-node correlation; an edge weight wi,j = 0 means samples xi and xj are conditionally independent.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "To study the implications of negative edge weights, we view a binary classifier as a piecewise constant (PWC) graph-signal and cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation [8].", "startOffset": 239, "endOffset": 242}, {"referenceID": 8, "context": "We show first that a graph Laplacian matrix L with negative edge weights can be indefinite, and a common graph-signal prior called graph Laplacian regularizer [9]\u2013[20] xLx for candidate signal x\u2014measuring signal smoothness with respect to the underlying graph\u2014can lead to pathological solutions.", "startOffset": 159, "endOffset": 162}, {"referenceID": 19, "context": "We show first that a graph Laplacian matrix L with negative edge weights can be indefinite, and a common graph-signal prior called graph Laplacian regularizer [9]\u2013[20] xLx for candidate signal x\u2014measuring signal smoothness with respect to the underlying graph\u2014can lead to pathological solutions.", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "To efficiently compute an approximate \u2206, we propose a fast recursive algorithm that identifies a lower bound for the smallest eigenvalue of L via a novel application of the Haynsworth Inertia Additivity formula [21].", "startOffset": 211, "endOffset": 215}, {"referenceID": 21, "context": "Second, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph\u2014an extension of total generalized variation (TGV) [22] to the graph-signal domain\u2014that promotes the right amount of ambiguity in the classifier signal.", "startOffset": 173, "endOffset": 177}, {"referenceID": 22, "context": "Finally, we propose an algorithm based on iterative reweighted least squares (IRLS) [24] that efficiently solves the posed MAP problem for the noisy label scenario.", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "Extensive simulation results show that our proposed algorithm outperforms SVM variants, a well-known robust classifier in the machine learning literature called RobustBoost [25], and graph-based classifiers using positive-edge graphs noticeably for both noiseless and noisy label scenarios.", "startOffset": 173, "endOffset": 177}, {"referenceID": 24, "context": "Classifier learning with label noise has garnered much interest, including a workshop1 in NIPS\u201910 [26] and a journal special issue in Neurocomputing [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "Classifier learning with label noise has garnered much interest, including a workshop1 in NIPS\u201910 [26] and a journal special issue in Neurocomputing [27].", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": ", label propagation in [28]) and application-specific (e.", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": ", emotion detection using inference algorithm based on multiplicative update rule [29]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "In this paper, similar to previous works [2]\u2013[5] we choose to build a graph-based classifier, where each acquired sample is represented as a node in a highdimensional feature space and connects to other sample nodes in its neighborhood.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "In this paper, similar to previous works [2]\u2013[5] we choose to build a graph-based classifier, where each acquired sample is represented as a node in a highdimensional feature space and connects to other sample nodes in its neighborhood.", "startOffset": 45, "endOffset": 48}, {"referenceID": 21, "context": "Second, we show how generalized graph smoothness notion\u2014extending TGV [22] to the graph-signal domain\u2014can be interpreted intuitively as Kirchoff\u2019s current law and used to promote ambiguity in the classifier solution.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 84, "endOffset": 87}, {"referenceID": 10, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 3, "context": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32].", "startOffset": 161, "endOffset": 164}, {"referenceID": 28, "context": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32].", "startOffset": 166, "endOffset": 170}, {"referenceID": 30, "context": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32].", "startOffset": 171, "endOffset": 175}, {"referenceID": 3, "context": "As an example, for graph-signal restoration, a smoothness prior \u2016x\u2212Wx\u2016pp for some positive integer p was proposed in [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 31, "context": "Suppose a total variation (TV) approach [33] is taken instead, so that a smoothness prior using L but based on l1norm is used instead: i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "Recent studies in the control community have examined the conditions where one or more negative edge weights would induce a graph Laplacian to be indefinite [34], [35].", "startOffset": 157, "endOffset": 161}, {"referenceID": 33, "context": "Recent studies in the control community have examined the conditions where one or more negative edge weights would induce a graph Laplacian to be indefinite [34], [35].", "startOffset": 163, "endOffset": 167}, {"referenceID": 34, "context": "[36] considered a signed social network where each edge denotes either a cohesive (positive edge weight) or oppositive (negative edge weight) relationship between two vertices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "A combinatorial graph Laplacian matrix L is then simply L = D\u2212W [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 35, "context": "Note that v1 = 1 has no zero-crossings, and higher frequency components vk have increasingly more zero-crossings according to the nodal domain theorem [37].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Mathematically, we can write that a signal x is smooth if its graph Laplacian regularizer xLx is small [10], [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Mathematically, we can write that a signal x is smooth if its graph Laplacian regularizer xLx is small [10], [11].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 36, "context": "This weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight.", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "This weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight.", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "This weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight.", "startOffset": 130, "endOffset": 133}, {"referenceID": 37, "context": "For more detailed optimization of feature parameters \u039ei,i\u2014which is not the focus of this paper\u2014see [39].", "startOffset": 99, "endOffset": 103}, {"referenceID": 38, "context": "1 in [40], which we rephrase as follows.", "startOffset": 5, "endOffset": 9}, {"referenceID": 38, "context": "1 in [40] states that the optimal perturbation matrix \u2206 with minimum norm \u2016\u2206\u2016, such that L + \u2206 is PSD, is:", "startOffset": 5, "endOffset": 9}, {"referenceID": 38, "context": "See [40] for a complete proof.", "startOffset": 4, "endOffset": 8}, {"referenceID": 35, "context": "To construct a solution for this case, we define a generalized graph Laplacian matrix Lg [37] as the sum of L and an identity matrix5 I scaled by \u2212\u03bbmin:", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "However, the computation of \u03bbmax is O(N ) in the worst case [41], and this lower bound is often loose in practice.", "startOffset": 60, "endOffset": 64}, {"referenceID": 39, "context": "The complexity of eigen-decomposition to compute the smallest eigenvalue \u03bbmin of graph Laplacian L is O(N) [41].", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "Note that partitioning a graph into two node sets to reduce complexity is also done in Kron reduction [42].", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "However, [42] considers only PSD L (possibly with self-loops), while we consider indefinite L that requires perturbation \u2206 to make L + \u2206 PSD.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "We can now relate the inertia of L with its sub-matrices using the Haynsworth Inertia Additivity formula [21]:", "startOffset": 105, "endOffset": 109}, {"referenceID": 41, "context": "N t 1 can be chosen by first randomly selecting a node in N , then perform breadth-first search (BFS) [43] until r nodes are discovered.", "startOffset": 102, "endOffset": 106}, {"referenceID": 24, "context": "To model binary label noise, we adopt a uniform noise model [26], where the probability of observing yi = xi,", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "This noise model is motivated by the following observation in social media analysis, when labels are often assigned manually by non-experts via crowd-sourcing [26]\u2014i.", "startOffset": 159, "endOffset": 163}, {"referenceID": 21, "context": "1) Positive Edge Weights for Generalized Smoothness: Like TGV for images [22], one can also define a higher-order notion of smoothness for graph-signals using positive edge weights [8].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "1) Positive Edge Weights for Generalized Smoothness: Like TGV for images [22], one can also define a higher-order notion of smoothness for graph-signals using positive edge weights [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "Specifically, positive edge graph Laplacian L is related to the second derivative of continuous functions [7], and so Lx computes the second-order difference on graph-signal x.", "startOffset": 106, "endOffset": 109}, {"referenceID": 32, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 135, "endOffset": 139}, {"referenceID": 40, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 141, "endOffset": 145}, {"referenceID": 42, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 147, "endOffset": 151}, {"referenceID": 40, "context": "As done in [42], a generalized graph Laplacian Lg with diagonal element Li,i \u2265 \u2211 j|j 6=i Li,j can be considered a conductance matrix, where an edge (i, j) has branch conductance \u2212Li,j and node i has shunt conductance Li,i\u2212 \u2211 j|j 6=i Li,j \u2265 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 31, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 43, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 242, "endOffset": 246}, {"referenceID": 18, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 248, "endOffset": 252}, {"referenceID": 19, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 254, "endOffset": 258}, {"referenceID": 21, "context": "Recall that the purpose of TGV [22] is to avoid oversmoothing a ramp (linear increase / decrease in pixel intensity) in an image, which would happen if only a TV prior is used.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "To accomplish this, we employ the iterative reweighted least squares (IRLS) strategy [24], which has been proven to have superlinear local convergence, and solve (45) iteratively, where the weights b i of iteration t+ 1 is computed using solution x i of the previous iteration t, i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 44, "context": "1) Datasets for Training and Testing: To evaluate the performances of different classification methods, we selected three two-class datasets from the KEEL (Knowledge Extraction based on Evolutionary Learning) database [46], which contains a rich collection of labelled and unlabeled datasets for data mining and analysis.", "startOffset": 218, "endOffset": 222}, {"referenceID": 0, "context": "For each sample (node), we found its three nearest neighbors according to the Euclidean distances between the node and its neighbors, and connected these nodes using edges with non-negative weights that are normalized to [0,1] using the Gaussian kernel in (10).", "startOffset": 221, "endOffset": 226}, {"referenceID": 23, "context": "3) Comparison Schemes: We tested our proposed algorithm against five schemes: i) linear SVM, ii) SVM with a RBF kernel (named SVM-RBF), iii) a more robust version of the famed AdaBoost called RobustBoost [25] that claims robustness against label noise, iv) a graph classifier with the graph-signal smoothness prior (3) where the edge weights of the graph are all positive (named Graph-Pos), and v) a graph classifier with a graph containing negative edge weights where the graph Laplacian L is perturbed by the minimum-norm perturbation criteria in (12) to eliminate negative eigenvalues for numerical stability (named Graph-MinNorm).", "startOffset": 204, "endOffset": 208}, {"referenceID": 45, "context": "We note that a user may define the desired classifier performance as a weighted sum of classification error and rejection rate for different applications, as done in [47].", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "In a semi-supervised learning scenario, (possibly noisy) partially observed labels are used as input to train a classifier, in order to assign labels to unclassified samples. In this paper, we study this classifier learning problem from a graph signal processing (GSP) perspective. Specifically, by viewing a binary classifier as a piecewise constant graph-signal in a highdimensional feature space, we cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation. Unlike previous graph-signal restoration works, we consider in addition edges with negative weights that signify anti-correlation between samples. One unfortunate consequence is that the graph Laplacian matrix L can be indefinite, and previously proposed graph-signal smoothness prior xLx for candidate signal x can lead to pathological solutions. In response, we derive an optimal perturbation matrix \u2206\u2014based on a fast lower-bound computation of the minimum eigenvalue of L via a novel application of the Haynsworth inertia additivity formula\u2014 so that L + \u2206 is positive semi-definite, resulting in a stable signal prior. Further, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph that promotes ambiguity in the classifier signal. Finally, we propose an algorithm based on iterative reweighted least squares (IRLS) that solves the posed MAP problem efficiently. Extensive simulation results show that our proposed algorithm outperforms both SVM variants and graph-based classifiers using positiveedge graphs noticeably.", "creator": "LaTeX with hyperref package"}}}