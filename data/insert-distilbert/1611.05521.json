{"id": "1611.05521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Robust Hashing for Multi-View Data: Jointly Learning Low-Rank Kernelized Similarity Consensus and Hash Functions", "abstract": "learning hash functions / codes for similarity search over multi - view data is attracting increasing attention, where similar hash codes are assigned to the paired data objects characterizing consistently neighborhood relationship across views. traditional methods in this category inherently suffer three limitations : 1 ) they commonly adopt primarily a two - stage scheme where similarity matrix is first constructed, followed by a subsequent hash function learning ; 2 ) these methods are commonly developed on the assumption that data samples with multiple representations are noise - sensor free, which is not practical in real - life applications ; 3 ) they often incur cumbersome training model caused by the neighborhood graph construction using all $ n $ points in the database ( $ w o ( n ) $ ). in this paper, we motivate the problem performance of jointly and efficiently training the robust hash functions over data objects with multi - feature representations which may be noise corrupted. to achieve both, the robustness and training efficiency, we propose an approach to effectively and efficiently learning low - rank kernelized \\ footnote { we use kernelized similarity rather here than kernel, as it is not a squared symmetric matrix for data - generated landmark affinity matrix. } hash functions shared across views. specifically, we utilize landmark graphs to construct tractable similarity matrices in multi - views to automatically discover neighborhood structure in the data. to learn robust hash functions, a latent low - rank kernel function is used to construct hash functions in order directly to accommodate compact linearly inseparable data. in particular, a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel - based similarity matrices. extensive experiments on real - world multi - view datasets validate the efficacy of our method in the presence of error corruptions.", "histories": [["v1", "Thu, 17 Nov 2016 01:21:26 GMT  (262kb,D)", "http://arxiv.org/abs/1611.05521v1", "Accepted to appear in Image and Vision Computing"]], "COMMENTS": "Accepted to appear in Image and Vision Computing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lin wu", "yang wang"], "accepted": false, "id": "1611.05521"}, "pdf": {"name": "1611.05521.pdf", "metadata": {"source": "CRF", "title": "Robust Hashing for Multi-View Data: Jointly Learning Low-Rank Kernelized Similarity Consensus and Hash Functions", "authors": ["Lin Wu", "Yang Wang"], "emails": ["lin.wu@adelaide.edu.au", "wangy@cse.unsw.edu.au"], "sections": [{"heading": null, "text": "Learning hash functions/codes for similarity search over multi-view data is attracting increasing attention, where similar hash codes are assigned to the data objects characterizing consistently neighborhood relationship across views. Traditional methods in this category inherently suffer three limitations: 1) they commonly adopt a two-stage scheme where similarity matrix is first constructed, followed by a subsequent hash function learning; 2) these methods are commonly developed on the assumption that data samples with multiple representations are noise-free,which is not practical in reallife applications; 3) they often incur cumbersome training model caused by the neighborhood graph construction using all N points in the database (O(N)). In this paper, we motivate the problem of jointly and efficiently training the robust hash functions over data objects with multi-feature representations which may be noise corrupted. To achieve both the robustness and training efficiency, we propose an approach to effectively and efficiently learning low-rank kernelized 1 hash functions shared across views. Specifically, we utilize landmark graphs to construct tractable similarity matrices in multi-views to automatically discover neighborhood structure in the data. To learn robust hash functions, a latent low-rank kernel function is used to construct hash functions in order to accommodate linearly inseparable data. In particular, a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel-based similarity matrices. Extensive experiments on realworld multi-view datasets validate the efficacy of our method in the presence of error corruptions.\n1We use kernelized similarity rather than kernel, as it is not a squared symmetric matrix for data-landmark affinity matrix."}, {"heading": "1 Introduction", "text": "Hashing is dramatically efficient for similarity search over low-dimensional binary codes with low storage cost. Intensive hashing methods valid on single data source have been proposed which can be classified into data-independent hashing such as locality sensitive hashing (LSH) [Datar et al., 2004] and data-dependent hashing or learning based hashing [Weiss et al., 2008; Wang et al., 2010].\nIn real-life situations, data objects can be decomposed of multi-view (feature) spaces where each view can characterize its individual property, e.g., an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a]. Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search. The critical issue is to ensure the learned hash codes can well preserve the original data similarities regarding viewdependent feature representations. To be specific, similar hash codes are assigned to data objects that consistently capture nearest neighborhood structure across all views."}, {"heading": "1.1 Motivation", "text": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:\n\u2022 The learning process is conducted by a two-stage mechanism where hash functions are learned based on preconstructed data similarity matrix. Their methods commonly assume that data samples are noise-free under multiple views whereas in real-world applications input data objects may be noisy (e.g., missing values in pixels), resulting in corresponding similarity matrices being corrupted by considerable noises [Wang et al., 2015d; Xia et al., 2014]. Moreover, the recovery of consen-\nar X\niv :1\n61 1.\n05 52\n1v 1\n[ cs\n.L G\n] 1\n7 N\nov 2\nsus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012]. This motivates us to deliver a framework to jointly and effectively learn similarity matrices and robust hash functions with kernel functions plugged because the kernel trick is able to tackle linearly inseparable data [Liu et al., 2012a]. To this end, a latent kernelized similarity matrix is recovered shared across views by using lowrank representation (LRR) [Liu et al., 2013] which is robust to corrupted observations. The recovered lowrank kernelized similarity matrix is consensus-reaching across views and can reveal the true underlying structures in data points.\n\u2022 State-of-the-art multi-view hashing methods is less efficiency in their learning procedure because the learning is performed by building and accessing a neighborhood graph using all N points (O(N2)). This action is intractable in off-line training when N is large. To this end, we are further motivated to employ an landmark graph to build an approximate neighborhood graph using landmarks [Liu et al., 2011; Liu et al., 2010b], in which the similarity between a pair of data points is measured with respect to a small number of landmarks (typically a few hundred). The resulting graph is built in O(N) time and sufficiently sparse with performance approaching to true k-NN graphs as the number of landmarks increases [Liu et al., 2011]."}, {"heading": "1.2 Our Method", "text": "In this paper, we propose a novel approach to robust multiview hashing by effectively and efficiently learning a set of hash functions and a low-rank kernelized similarity matrix shared by multiple views.\nWe remark that our method is fundamentally different from existing multi-view hashing methods that are conditioned on corruption-free similarities, which has diminished their application to real-world tasks. Instead, we propose to learn hash functions and kernel-based similarities under a more realistic scenario with noisy observations. Our method is advantageous in the aspect of efficiency due to the employment of approximate neighborhood with landmark graphs. We clarify the recovered low-rank similarity matrix in kernel functions to be the kernelized rather than kernel since it is not a symmetric matrix yet characterizes non-linear similarities. The proposed method is also different from partial view study [Kumar and III, 2011; Wang et al., 2015a], where they consider the case that data examples with some modalities are missing. Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.\nIn our framework, the low rank minimization is enforced to yield a consensus-reaching, kernelized similarity matrix\nshared by multiple views where larger similarity values indicate corresponding data objects from the same cluster, while smaller similarity values imply those come from distinct clusters. Thus, the learned low-rank similarity matrix against multi-views can reflect the underlying clustering information.\nTechnically, a nonlinear kernelized similarity matrix in the m-th view, denoted as K(m), can be decomposed into three components: (1) A latent low-rank kernelized similarity matrix K\u0302, representing the nonlinear requisite or consensus similarities shared across views; (2) a view-dependent redundancy characterizing its individual similarities; and (3) possible error corruptions for view-specific representations. We unify view redundancy and errors into E(m) and impose an `2,1-norm constraint on it, denoted as ||E(m)||2,1. This is because view redundancy and disturbing errors are always sparsely distributed, and minimizing ||E(m)||2,1 is able to identify non-zero sparse columns revealing corresponding redundancy/errors. Note that in this work, \u201cerror\u201d generally refers to error corruptions or perturbation, e.g., noise or missing values, in view-dependent feature values. These principles are formulated into an objective function, which is optimized based on the inexact Augmented Lagrangian Multiplier (ALM) scheme [Lin et al., 2010]. It allows us to jointly learn a latent low-rank nonlinear similarity with corruption free and optimal hash functions for multi-view data, where hash codes are restricted to well preserve local (neighborhood) geometric structures in each view. We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities. Our setting is fundamentally different from cross-view/modal hashing in the aspect that we aim to leverage multiple features to jointly learn hash functions and a latent nonlinear similarity matrix over a homogeneous data source. To the best of our knowledge, we are the first to systematically address the problem of multi-view hashing with possible data error corruptions."}, {"heading": "1.3 Contributions", "text": "The major contributions of this paper are three-fold.\n\u2022 We motivate the problem of robust hashing over multiview data with nonlinear data distribution, and propose to learn the robust hash functions and a low-rank kernelized similarity matrix shared by views.\n\u2022 An iterative low-rank recovery optimization technique is proposed to learn the robust hashing functions. For the sake of efficiency, the neighborhood graph is approximated by using landmark graphs with sparse connection between data points.\n\u2022 Extensive experiments conducted on real-world multiview datasets validate the efficacy of our method in the presence of error corruptions for multi-view feature representations."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Multi-view Learning based Hashing", "text": "The purpose of multi-view learning based hashing is to learn better hash codes by leveraging multiple views. Some recent representative works include Multiple Feature Hashing (MFH) [Song et al., 2011], Composite Hashing with Multiple Sources (CHMS) [Zhang et al., 2011], Compact Kernel Hashing with multiple features (CKH) [Liu et al., 2012b], and Multi-view Sequential Spectral Hashing (SSH) [Kim et al., 2012]. However, these methods have common drawbacks that they typically apply spectral graph technique (e.g., k-NN graph) to model a similarities between data points. In general, the complexity of constructing the similarity matrix isO(N2) for N data points, which is not pragmatic in large-scale applications. Moreover, the similarity matrix induced by graph construction is very sensitive to noise corruptions. To avoid the construction of similarity matrix, Shen et al. [Shen et al., 2015] present a Multi-View Latent Hashing (MVLH) to learn hash codes by performing matrix factorization on a unified kernel feature space over multiple views. Nonetheless, there are significant differences between MVLH and our approach. First, matrix factorization is performed on a unified kernel space which is formed by simply concatenating multiple kernel feature spaces. This would discard distinct local structures in individual views. By contrast, the kernelized similarity matrix is constructed with respect to the distinct characteristic in each view. Second, MVLH neglects the case of potential noise corruption in data samples. In this aspect, we attentively employ the low-rank representation (LRR) [Liu et al., 2013] to recover latent subspace structures from corrupted data."}, {"heading": "2.2 Low-rank Modeling", "text": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015]. It has striking success in many applications such as data compression [Wright et al., 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015]. For instance, in [Zhang et al., 2016], Zhang et al. consider a joint formulation of recovering low-rank and sparse subspace structures for robust representation.\nNowadays, data are usually collected from diverse domains or obtained from various feature extractors, and each group of features can be regarded as a particular view [Xu et al., 2013]. Moreover, these data can be easily corrupted by potential noises (e.g., missing pixels or outliers), or large variations (e.g., post variations in face images) in real applications. In practice, the underlying structure of data could be multiple subspaces, and thus Low-Rank Representation (LRR) is designed to find subspace structures in noisy data [Liu et al., 2013; Zhang et al., 2014]. The multi-view low-rank analysis [Li et al., 2015] is a recently proposed multi-view learning approach, which introduces low-rank constraint to reveal the\nintrinsic structure of data, and identifies outliers for the representation coefficients in low-rank matrix recovery.\nIn this paper, we are the first to apply low-rank learning to reveal structured kernalized similarity among multi-view data, and scale it up well to large-scale applications."}, {"heading": "3 Robust Multi-view Hashing", "text": ""}, {"heading": "3.1 Preliminary and Problem Definition", "text": "Let \u03c6(\u00b7) = {\u03c61(\u00b7), \u00b7\u00b7, \u03c6M (\u00b7)} be the embedding function for M nonlinear feature spaces, each of which corresponds to one view. Following the Kernelized Locality Sensitive Hashing [Kulis and Grauman, 2009], we uniformly select R samples from the training set X , denoted by Zr (r = 1, . . . , R), to construct kernelized similarity matrices under multiple views. Given a sample represented by its feature xi, the p-th hash bit can be generated via the linear projection:\nhp(xi) = sign(C T p \u03c6(xi) + bp), (1)\nwhere sign(\u00b7) denotes the element-wise function, which is 1 if it is larger or equal to 0 and -1 otherwise. Cp =\u2211R r=1Wrp\u03c6(Zr) indicates the linear combination of R landmarks, which can be the cluster centers [Liu et al., 2011] via scalable R-means clustering over the feature space with d dimensions. bp \u2208 R is a bias term. Then, we have\nhp(xi) = sign( R\u2211 r=1 WrpKi + bp), (2)\nwhere Ki denotes the i-th column of K \u2208 RR\u00d7N , such that K = \u2211M i=1K\n(m), and K(m) \u2208 RR\u00d7N (m = 1, . . . ,M) denotes the kernelized similarity matrix between R landmarks and N samples corresponding to the kernelized representation \u03c6(m)(\u00b7). Accordingly, the hash code of xi can be rewritten via the kernel form,\nyi = sign(W TKi + b), (3)\nwhere W \u2208 RR\u00d7P and b = [b1, . . . , bP ]. Given a set of training samplesX = [x1, . . . , xN ] that may contain errors, x(m)i \u2208 Rdm\u00d71 denotes them-th feature of xi, and dm is the dimensionality for the feature space regarding the m-th view. Then X(m) = [x(m)1 , x (m) 2 . . . , x (m) N ] \u2208 Rdm\u00d7N is the view matrix corresponding to the mth feature of all training data. xi = [(x1)Ti , . . . , (x M )Ti ] T \u2208 Rd\u00d71 is the vector representation of the ith training data using all features where d = \u2211M m=1 dm, andM is the number of views. We denote Y = [y1, y2, . . . , yN ] \u2208 RP\u00d7N as the hash codes of the training samples corresponding to all features, and Y (m) = [y\n(m) 1 , . . . , y (m) N ] \u2208 RP\u00d7N as the hash codes of the training data for the m-th view. We aim to learn a latent low-rank kernel matrix K\u0302 shared across multiple kernels, and construct a set of robust hashing functions H = {h1(\u00b7), . . . , hP (\u00b7)} for multi-view data where hp : Rd 7\u2192 {1,\u22121} (p = 1, 2, . . . , P ), and P is the number of hashing functions, i.e., the hash code length. The kernel function is plugged into hash function because the kernel trick has been theoretically and empirically proved to be able to tackle the data distribution that is almost linearly inseparable [Liu et al., 2012a]."}, {"heading": "3.2 Low-rank Kernelized Similarity Recovery from Multi-views", "text": "Given a collection of high-dimensional multi-view data samples that may contain certain errors for each viewspecific representation, we construct multiple nonlinear feature spaces K(m)(m = 1, . . . ,M), each of which represents one feature view. To leverage multiple complementary representations, we propose to derive a consensus low-rank kernelized similarity matrix K\u0302 recovered from corrupted data objects, and shared across views. This low-rank nonlinear similarity matrix is considered as the most requisite component, whilst each view also contains individual non-requisite information including redundancy and errors. We explicitly model the redundancy via sparsity since multi-view study suggests that each individual view is sufficient to identify most of the similarity structure, and the deviation between requisite component and data sample is sparse [Kumar et al., 2011]. In reality, data samples can be grossly corrupted due to the sensor failure or communication errors. Thus, an `2,1-norm is adopted to characterize errors since they usually cause column sparsity in an affinity matrix [Liu et al., 2013].\nIn our framework, the low-rank similarity matrix is constructed to be sparse by considering data samples and landmarks, thus ascertaining the efficiency of our approach. Therefore, the latent low-rank kernelized similarity matrix K\u0302 can be recovered from K(m)(m = 1, . . . ,M) through a lowrank constraint on K\u0302 and sparse constraint on eachE(m), that is,\nmin K\u0302,E(m) ||K\u0302||\u2217+\u03bb M\u2211 m=1 ||E(m)||2,1, s.t. K(m) = K\u0302+E(m), K\u0302 \u2265 0. (4) where \u03bb is the trade-off parameter andE(m) encodes the summation of error corruption and possible noise information regarding the m-th view."}, {"heading": "3.3 Objective Function", "text": "Many studies [Weiss et al., 2008; Song et al., 2011] have shown the benefits to exploit local structure of the training data to infer accurate and compact hash codes. However, all these algorithms are sensitive to error corruptions, hampering them to be effective in practical situations. By contrast, we propose to jointly learn hash codes by preserving local similarities in multiple views while being robust to errors. To exploit the local structure in each view, we define M affinity matrices S(m) \u2208 RN\u00d7N (m = 1, . . . ,M), one for each view, that is,\nS (m) ij =\n{ 1, x\n(m) i \u2208 Nk(x (m) j ) or x (m) j \u2208 Nk(x (m) i );\n0, else.\nwhere Nk(\u00b7) is the k-nearest neighbor set, and the Euclidean distance is employed in each feature space to determine the neighborhood. A reasonable criteria of learning hash codes y (m) i from the m-th view is to ensure similar objects in the original space should have similar binary hash codes. This\ncan be formulated as below:\nmin N\u2211 i,j=1 S (m) ij ||y (m) i \u2212 y (m) j || 2 F . (5)\nGiven a training sample xi, we expect the optimal hash code yi consistent with its distinct hash codes y (m) i derived from each view. In this way, the local geometric structure in a single view can be globally optimized. Therefore, we have\nmin M\u2211 m=1  N\u2211 i,j=1 S (m) ij ||y (m) i \u2212 y (m) j || 2 F + \u03b3 N\u2211 i=1 ||yi \u2212 y(m)i || 2 F  , (6) where \u03b3 is a trade-off parameter. The main bottleneck in the above formulation is computation where the cost of building the underlying graph and its associate affinity matrix S(m) is O(dmN2), which is intractable for large N . To avoid the computational bottleneck, we employ a landmark graph by using a small set of L points called landmarks to approximate the data neighborhood structure [Liu et al., 2011]. Similarities of all N database points are measured with respect to these L landmarks, and the true adjacency/similarity matrix S(m) in the m-th view is approximated using these similarities. First, K-means clustering 2 is performed on N data points to obtain L (L N ) clusters center U = {uj \u2208 Rdm}Lj=1 that act as landmark points. Next, the landmark graph defines the truncated similarities Fij\u2019s between all N data points and L landmarks as,\nFij =\n{ exp(\u2212D2(xi,uj)/t)\u2211\nj\u2032\u2208\u3008i\u3009 exp(\u2212D2(xi,u\u2032j)/t) , \u2200j \u2208 \u3008i\u3009\n0, elsewhere\nwhere \u3008i\u3009 \u2282 [1 : L] denotes the indices of k (k L) nearest landmarks of points xi in U according to a distance function D() such as `2 distance, and t denotes the bandwidth parameter. Note that the matrix F \u2208 RN\u00d7L is highly sparse. Each row of F contains only k non-zero entries which sum to 1. Thus, the landmark graph provides a powerful approximation to the adjacency matrix S(m) as S\u0302(m) = F\u039b\u22121FT where \u039b = diag(FT 1) \u2208 RL\u00d7L [Liu et al., 2011]. For ease of representation, we denote L(Y, Y (m)) =\u2211N i,j=1 S\u0302 (m) ij ||y (m) i \u2212 y (m) j ||2F + \u03b3 \u2211N i=1 ||yi \u2212 y (m) i ||2F . To learn a set of hashing functions and a consensus nonlinear representation in a joint framework, we formulate the objective function of robust multi-view hashing as follows\nmin W,K\u0302,b,E(m) M\u2211 m=1 L(Y, Y (m)) + \u03b1||K\u0302||\u2217 + \u03bb M\u2211 m=1 ||E(m)||2,1,\ns.t. K(m) = K\u0302 + E(m), K\u0302 \u2265 0,m = 1, . . . ,M, yi = sign(W\nT K\u0302i + b) \u2208 {\u22121, 1}P , Y Y T = I, (7)\nwhere \u03b1 is a trade-off parameter, yi \u2208 {\u22121, 1}P enforces the hash code yi to be binary codes, and the constraint Y Y T = I\n2In practice, running K-means algorithm on a small subsample of the database with very few iterations is sufficient.\nis imposed to encourage bit de-correlations while avoiding the trivial solution. Due to the discrete constraints and nonconvexity, the optimization problem in Eq.(7) is difficult to solve. Following spectral hashing [Weiss et al., 2008], we relax the constraints yi \u2208 {\u22121, 1}P to be yi = WT K\u0302i + b, then we have\nmin W,K\u0302,b,E(m) M\u2211 m=1 L(Y, Y (m)) + \u03b1||K\u0302||\u2217 + \u03bb M\u2211 m=1 ||E(m)||2,1,\ns.t. K(m) = K\u0302 + E(m), K\u0302 \u2265 0,m = 1, . . . ,M ; yi = W T K\u0302i + b, Y Y T = I.\nWe rewrite the objective function by further minimizing the least square error regarding Y while regularizing W coupled with trade-off parameters \u03b2 and \u03b4, it then has\nmin W,K\u0302,b,E(m) M\u2211 m=1 L(Y, Y (m)) + \u03b1||K\u0302||\u2217 + \u03bb M\u2211 m=1 ||E(m)||2,1\n+ \u03b2 ( ||K\u0302TW + 1b\u2212 Y ||2F + \u03b4||W ||2F ) s.t. K(m) = K\u0302 + E(m), K\u0302 \u2265 0,m = 1, . . . ,M ;Y Y T = I.\n(8)\nEq.(8) is still non-convex due to orthogonal constraint Y Y T = I . Fortunately with either W , b or K\u0302, E(m) fixed, the problem is convex with respect to the other variables. Therefore, we present an alternating optimization way that can efficiently find the optimum in a few steps. First, given K\u0302 and E(m), we show that computation expressions of W and b can be obtained. To compute K\u0302 and E(m), we employ an efficient optimization technique, the inexact augmented Lagrange multiplier (ALM) algorithm [Lin et al., 2010]."}, {"heading": "4 Optimization", "text": ""}, {"heading": "4.1 Compute W and b", "text": "With other variables fixed, and setting the derivative of Eq.(8) w.r.t. b to zero, we get\n1T ( K\u0302TW + 1b\u2212 Y ) = 0\n\u21d2 b = 1 N\n( 1TY \u2212 1T K\u0302TW ) .\n(9)\nSetting the derivative of Eq.(8) w.r.t. W to zero, we yield\nK\u0302(K\u0302TW + 1b\u2212 Y ) + \u03b4W = 0. (10)\nSubstituting b in Eq.(9) into Eq.(10), we have K\u0302K\u0302TW + K\u03021 ( 1\nN (1TY \u2212 1T K\u0302TW )\n) \u2212 K\u0302Y = 0\n\u21d2W = (K\u0302LcK\u0302T + \u03b4I)\u22121 + K\u0302LcY, (11)\nwhere Lc = I \u2212 1N 11 T is the centering matrix, and Lc = LTc = LcL T c .\n4.2 Compute K\u0302 and E(m) With variables W and b being fixed, the problem turns to be\nmin K\u0302,E(m) \u03b1||K\u0302||\u2217 + \u03bb M\u2211 m=1 ||E(m)||2,1,\ns.t. K(m) = K\u0302 + E(m), K\u0302 \u2265 0,m = 1, . . . ,M.\n(12)\nThe rank minimization problem has been well studied in literature [Liu et al., 2010a; Wright et al., 2009b]. By introducing an auxiliary variableQ such that K\u0302 = Q, Eq.(12) can be then converted into the following equivalent form: D(K\u0302,Q,E(m)) = \u03b1||Q||\u2217 + \u03bb M\u2211 m=1 ||E(m)||2,1\n+ M\u2211 m=1 ( \u3008A(m), K\u0302 + E(m) \u2212K(m)\u3009+ \u00b5 2 ||K\u0302 + E(m) \u2212K(m)||2F ) + \u3008B, K\u0302 \u2212Q\u3009+ \u00b5\n2 ||K\u0302 \u2212Q||2F ,\n(13)\nwhere A(m) and B represent the Lagrange multipliers, \u3008\u00b7, \u00b7\u3009 denotes the inner product of matrices, and \u00b5 > 0 is an adaptive penalty parameter. Next we will elaborate the update rules for each of K\u0302, Q, and E(m) by minimizing D while fixing the others.\nSolving for Q When the other variables are fixed, the subproblem w.r.t. Q is\nmin Q ||Q||\u2217 +\n\u00b5 2\u03b1 ||K\u0302 \u2212Q+ B \u00b5\u03b1 ||2F . (14)\nIt can be solved by the Singular Value Threshold method [Cai et al., 2010]. More specifically, let U\u03a3V T be the SVD form of (K\u0302 + B\u00b5\u03b1 ), the updating rule of Q using the SVD operator in each iteration will be\nQ = US1/\u00b5\u03b1(\u03a3)V T , (15)\nwhere S%(x) = max(x\u2212%, 0)+min(x+%, 0) is the shrinkage operator [Lin et al., 2015].\nSolving for E(m) The subproblem with respect to E(m), (m = 1, . . . ,M) can be simplified as\nmin E(m)\n\u03bb||E(m)||2,1+ \u00b5 2 ||E(m)\u2212(K(m)\u2212K\u0302\u2212A\n(m) \u00b5 )||2F , (16)\nwhich enjoys a closed form solution E(m) = S\u03bb/\u00b5(K(m) \u2212 K\u0302 \u2212 A (m)\n\u00b5 ).\nSolving for K\u0302 With the other variables being fixed, we update K\u0302 by solving"}, {"heading": "K\u0302 = arg min", "text": "K\u0302 ||K\u0302+E(m)\u2212K(m)+A\n(m) \u00b5 ||2F+ \u00b5 2 ||K\u0302\u2212Q+B \u00b5 ||2F\n(17)\nFor ease of representation, we define C = 1M (Q \u2212 B \u00b5 +\u2211M\nm=1(K (m)\u2212E(m)\u2212 A\n(m)\n\u00b5 )). Then, the problem in Eq.(17) can be rewritten as"}, {"heading": "K\u0302 = arg min", "text": "K\u0302\n1 2 ||K\u0302 \u2212 C||2F = arg min\nK\u03021,...,K\u0302N\n= 1\n2 N\u2211 i=1 ||K\u0302i \u2212 Ci||22.\ns.t.K\u0302 \u2265 0, K\u0302i(i=1,...,N) \u2265 0. (18)\nHence, the problem in Eq.(18) can be decomposed into N independent subproblems: minK\u0302i 1 2 ||K\u0302i \u2212 Ci|| 2 2, subject to K\u0302i \u2265 0. Each subproblem is a proximal operator problem, which can be efficiently solved by the projection algorithm in [Duchi et al., 2008]."}, {"heading": "4.3 Learning Hash Codes", "text": "Once the hashing function implemented by W and b is learned by exploiting the kernelized similarity consensus K\u0302, we can generate hash codes for both database and query samples, denoted as xt, via Eq. (19).\nyt = sign(W T [K(xt, Z1), . . . ,K(xt, ZR)]T + b), (19)\nwhere W \u2208 RR\u00d7P , K(xt, Zi) represents the similarity between xt and the i-th landmark using Gaussian RBF kernel over the concatenated feature space for all views."}, {"heading": "4.4 Out-of-Sample Extension", "text": "An essential part of hashing is to generate binary codes for new samples, which is known as out-of-sample problems. A widely used solution is the Nystro\u0308m extension [Bengio et al., 2004]. However, this is impractical for large-scale hashing since the Nystro\u0308m extension is as expensive as doing exhaustive nearest neighbor search with a complexity of O(N) for N data points. In order to address the out-of-sample extension problem, we employ a non-parametric regression approach, inspired by Shen et al. [Shen et al., 2013]. Specifically, given the hashing embedding Y = {y1, y1, . . . , yN} for the entire training set X = {x1, x2, . . . , xN}, for a new data point xq , we aim to generate a hashing embedding yq while preserving the local neighborhood relationships among its neighbors Nk(xq) in X . A simple inductive formulation can produce the embedding for a new data point by a sparse linear combination of the base embeddings:\nyq = \u2211N i=1 w(xq, xi)yi\u2211N i=1 w(xq, xi) , (20)\nwhere we define\nw(x1, xi) =\n{ exp(\u2212||xq \u2212 xi||2/\u03c32), xi \u2208 Nk(xq)\n0, elsewhere.\nHowever, Eq.(20) does not scale well for computing outof-sample extension (O(N)) for large-scale tasks. To this end, we employ a prototype algorithm [Shen et al., 2013] to approximate yq using only a small base set:\nh(xq) = sign (\u2211Z j=1 w(xq, cj)yj\u2211Z j=1 w(xq, cj) ) (21)\nwhere sign(\u00b7) is the sign function, and Y = {y1, y2, . . . , yZ} is the hashing embedding for the base set B = {c1, c2, . . . , cZ} which is the cluster centers obtained by Kmeans. In this stage, the major computation cost comes from K-means clustering, which is O(dlZN) in time (d is the feature dimension, and l is the number of iterations in K-means). The iteration number l can be set less than 50, thus, the Kmeans only costs O(dZN). Considering that Z is much less than N , the total time is linear in the size of training set. The computation of distance between B and X cost O(dZN). Thus, the overall time cost is O(dZN + dZN) = O(dZN)."}, {"heading": "5 Complexity Analysis", "text": "We analyze the time complexity regarding per iteration of the optimization strategy. The complexity of computing K\u0302LcK\u0302T and (K\u0302LcK\u0302T + \u03b4I)\u22121 in Eq.(11) is O(R2N) and O(R3), respectively. Commonly, R( N) landmarks are generated off-line via scalable K-means clustering for less than 50 iterations, keeping the complexity of computing W to be O(R2N)+O(R3). The complexity of computing hash codes for a new sample isO(dZN). Overall, the time complexity is O(dZN +R3 +R2N) \u2248 O(dZN +R2N) in one iteration, which is linear with respect to the training size."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Experimental Settings", "text": "Competitors We compare our method with recently proposed state-of-the-art multiple feature hashing algorithms:\n\u2022 Multiple feature hashing (MFH) [Song et al., 2011]: This method exploits local structure in each feature and global consistency in the optimization of hashing functions.\n\u2022 Composite hashing with multiple sources (CHMS) [Zhang et al., 2011]: This method treats a linear combination of view-specific similarities as an average similarity which can be plugged into a spectral hashing framework.\n\u2022 Compact kernel hashing with multiple features (CKH) [Liu et al., 2012b]: It is a multiple feature hashing framework where multiple kernels are linearly combined.\n\u2022 Sequential spectral hashing with multiple representations (SSH) [Kim et al., 2012]: This method constructs an average similarity matrix to assemble view-specific similarity matrices.\n\u2022 Multi-View Latent Hashing (MVLH) [Shen et al., 2015]: This is an unsupervised multi-view hashing approach where binary codes are learned by the latent factors shared by multiple views from an unified kernel feature space.\nDatasets We conduct the experiments on two image benchmarks: CIFAR-10 3 and NUS-WIDE.\n3http://www.cs.toronto.edu/ kriz/cifar.html\n\u2022 CIFAR-10 consists of 60K 32\u00d732 color images from ten object categories, each of which contains 6K samples. Every image is assigned to a mutually exclusive class label and for each image, we extract 512-dimensional GIST feature [Oliva and Torralba, 2001] and 300-dimensional bag-of-words quantized from dense SIFT features [Lowe, 2004] to be two views.\n\u2022 NUS-WIDE [Chua et al., 2009] contains 269,648 labeled images crawled from Flickr and is manually annotated with 81 categories. Three types of features are extracted: 128-dimensional wavelet texture, 225-dimensional block-wise color moments, and 500- dimensional bag-of-words to construct three views.\nMulti-view Corruption Setting In CIFAR-10, considering that missing features may have some structure, we remove a square patch of pixels from each image covering 25% of the total number of pixels. The location of the patch is uniformly sampled for each image. This will naturally deteriorate view-dependent feature representations. In NUS-WIDE, we consider the scenario where 20% of feature values in each view are corrupted with perturbation noise following a standard Gaussian distribution.\nParameter Setting In the training phase, we uniformly sample 30K and 100K images as training data from both datasets, and generate 300 and 500 landmarks. That is, we fix the graph construction parameters L = 300, k = 3 on CIFAR-10, and L = 500, k = 5 on NUS-WIDE, respectively. In the testing phase, we randomly select 1,000 query images in which the true neighbors of each image are defined as the semantic neighbors which share at least one common semantic label. For our method and CKH, we use Gaussian RBF kernelK(i)(x, y) = exp(\u2212||x\u2212y||2i /2\u03c32)(i = 1, \u00b7,M), where || \u00b7 ||2i represents the Euclidean distance within the i-th feature space. The parameter \u03c3 is learned via the self-tuning strategy [Zelnik-Manor and Perona, 2004].\nIn Eq.(8), there are five tunable parameters: \u03b3, \u03b4, \u03b1, \u03b2, and \u03bb. Parameters \u03b3 and \u03b4 controlling global hash code learning and regularization on hashing functions are set as 10\u22124 and 10\u22126, respectively. For \u03b1, \u03b2, and \u03bb, we tune their optimal combination, that is, \u03b1 = 10\u22121, \u03b2 = 100, and \u03bb = 10\u22123, as conducted in section 6.3.\nEvaluation Metric The mean precision-recall and mean average precision (MAP) are computed over the retrieved set consisting of the samples with the hamming distance [Liu et al., 2012a] using 8 to 32 bits to a specific query. We carry out hash lookup within a Hamming radius 2 and report the mean hash lookup precision over all queries. For a query q, the average precision (AP) is defined as AP (q) = 1 Lq \u2211l z=1 Pq(z)$q(z), where Lq is the number of groundtruth neighbors of q in database, l is the number of entities in database, Pq(z) denotes the precision of the top z retrieved entities, and $q(z) = 1 if the z-th retrieved entity is a ground-truth neighbor and $q(z) = 0, otherwise. Ground truth neighbors are defined as items which share at\nleast one semantic label. Given a query set of size F , the MAP is defined as the mean of the average precision for all queries: MAP = 1F \u2211F i=1AP (qi)."}, {"heading": "6.2 Results", "text": "We report the mean precision-recall curves of Hamming ranking, and mean average precision (MAP) w.r.t. different number of hashing bits over 1K query images. Results are shown in Fig.1, which are computed from top-100 retrieved samples. It can be seen from top subfigure of Fig.1 that our method achieves a performance gain in both precision and recall over all counterparts and the second best is MVLH. This can demonstrate the superiority of using nonlinear hashing functions in nonlinear space. More importantly, the latent consensus kernelized similarity matrix by low-rank minimization is not only effective in leveraging complementary information from multi-views, but also robust against the presence of errors. The subfigure (bottom) in Fig.1 shows that as the hashing bit number varies, our method consistently keeps superior performance. Specifically, it reaches the highest precision value for 48 bits and shows a relatively steady performance with more hashing bits. The results from the NUS-WIDE database are shown in Fig.2. Once again we can see performance gaps in precision-recall between our approach and competitors, as illustrated in top subfigure of Fig.2. This validates the advantage of our method by exploiting consensus of kernelized similarity to learn robust nonlinear hashing functions. In subfigure (bottom) of Fig.2, as the number of hashing bit increases, our method is able to keep high and steady MAP values.\nTo evaluate the impact of hashing bit numbers on performance of hash lookup, in Table 1, we report hash lookup mean precision with standard deviation (mean\u00b1std) in the case of 8, 32, 48, 128 bits on both databases. Similar to\nHamming ranking results, our method achieves the better performance than others and obviously increasing performance with less than 32 bits, which demonstrates that our approach with compact hashing codes can retrieve more semantically related images than all baselines in terms of hash lookup.\nIn Table 2, we report the comparison on training/test time over the two image benchmarks. CKH and our method are much more efficient by taking less than 15s and 20s respectively to train on CIFAR-10 and NUS-WIDE using 32 bits. The efficiency improvement comes from the usage of landmarks. While our method is slightly less efficient to CKH because of the low-rank kernelized similarity recovery, it is very comparable to CKH and consistently superior to CKH in other performance. MVLH is relatively costly due to its expensive matrix factorization in its kernel space. MFH and CHMS are time-consuming in training stage because they both involve the eigen-decomposition of a dense affinity matrix, which is not scalable to a large-scale setting. SSH has a gain in efficiency compared with MFH and CHMS on account of their approximation on the K-nearest graph construction [Kim et al., 2012]."}, {"heading": "6.3 Parameter Tuning", "text": "In this experiment, we test different parameter settings for our algorithm to study the performance sensitivity. We learn three parameters: \u03b1, \u03bb, and \u03b2, corresponding to the term of requisite component, non-requisite decomposition, and hashing function learning in Eq.(8). For these parameters, we tune them from {10\u22125, 10\u22123, 10\u22121, 100, 101, 102, 103}. We fix one of the parameters in \u03b1, \u03bb, and \u03b2 to report the MAP while the other two parameters are changing. The results are shown in Fig.3. In Fig.3 (a), by fixing \u03bb = 10\u22123, we show the performance variance on different pairs of \u03b1 and \u03b2. We can observe that our algorithms achieves a relatively higher MAP when \u03b1 = 0.1, and \u03b2 = 100. The similar performance can also be seen from Fig.3 (b) and Fig.3 (c). Thus, among different combinations, the method gains the best performance when \u03b1 = 10\u22121, \u03b2 = 1, and \u03bb = 10\u22123, while it is relatively insensitive to varied parameters setting. With optimal combi-\nnation of parameters, we study the issue of convergence. In Fig.4, we can observe that our algorithm becomes convergent in less than 40 iterations, demonstrating its fast convergence rate."}, {"heading": "6.4 Out-of-Sample Case", "text": "In this experiment, we study the property of out-of-sample extension. We take the CIFAR-10 dataset as the base benchmark to train base embeddings. Another dataset MNIST is considered as the testing bed. The MINIST dataset [LeCun et al., 1998] consists of 70K images, each of 784 dimensions, of handwritten digits from \u201c0\u201d to \u201c9\u201d. As in Fig.5, our method achieves the best results. On this dataset, we can clearly see that our method outperforms MVLH by a large margin, which increases as code length increases. This further demonstrates the advantage of kernelized low-rank embedding as a tool for hashing by embedding high dimensional data into a lower dimensional space. This dimensionality reduction procedure not only preserves the local neighborhood, but also reveals global structure."}, {"heading": "7 Conclusion", "text": "In this paper, we motivate the problem of robust hashing for similarity search over multi-view data objects under a practical scenario that error corruptions for view-dependent feature representations are presented. Unlike existing multi-view hashing methods that take a two-phase scheme of constructing similarity matrices and learning hash functions separately, we propose a novel technique to jointly learn hash functions and a latent, low-rank, corruption-free kernelized similarity under multiple representations with potential noise corruptions. Extensive experiments conducted on real-world multiview data sets demonstrate the superiority of our method in terms of efficacy."}], "references": [{"title": "Neural Comput", "author": ["Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux", "Jean-Franois Paiement", "Pascal Vincent", "Marie Ouimet. Learning eigenfunctions links spectral embedding", "kernel pca"], "venue": "16(10):2197\u2013 2219,", "citeRegEx": "Bengio et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A singular value thresholding algorithm", "author": ["Cai et al", "2010] Jian-Feng Cai", "Emmanuel J. Cands", "Zuowei Shen"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Foundations of Computational Mathmatics", "author": ["Emmanuel J. Candes", "Benjamin Recht. Exact matrix completion via convex optimization"], "venue": "9(6):717\u2013772,", "citeRegEx": "Candes and Recht. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Nus-wide: a real-world web image database from national university of singapore", "author": ["Tat-Seng Chua", "Jinhui Tang", "Richang Hong", "Haojie Li", "Zhiping Luo"], "venue": "ACM CIVR,", "citeRegEx": "Chua et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Locality-sensitive hashing scheme based on p-stable distribution", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S. Mirrokni"], "venue": "SOCG,", "citeRegEx": "Datar et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE Transactions on Neural Networks and Learing Systems", "author": ["Yue Deng", "Qionghai Dai", "Risheng Liu", "Zengke Zhang", "Sanqing Hu. Low-rank structure learning via nonconvex heuristic recovery"], "venue": "24(3):383\u2013396,", "citeRegEx": "Deng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra"], "venue": "ICML,", "citeRegEx": "Duchi et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In ECCV", "author": ["Saehoon Kim", "Yoonseop Kang", "Seungjin Choi. Sequential spectral learning to hash with multiple representations"], "venue": "pages 538\u2013551,", "citeRegEx": "Kim et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["Brian Kulis", "Kristen Grauman"], "venue": "ICCV,", "citeRegEx": "Kulis and Grauman. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "A co-training approach for multi-view spectral clustering", "author": ["Abhishek Kumar", "Hal Daume III"], "venue": "ICML,", "citeRegEx": "Kumar and III. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In IJCAI", "author": ["Shaishav Kumar", "Raghavendra Udupa. Learning hash functions for cross-view similarity search"], "venue": "pages 1360\u20131365,", "citeRegEx": "Kumar and Udupa. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized multi-view spectral clustering", "author": ["Abhishek Kumar", "Piyush Rai", "Hal Daum"], "venue": "NIPS,", "citeRegEx": "Kumar et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haaffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "LeCun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In SIAM Data Mining", "author": ["Sheng Li", "Ming Shao", "Yun Fu. Multiview low-rank analysis for outlier detection"], "venue": "pages 748\u2013756,", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Zhouchen Lin", "Minming Chen", "Yi Ma"], "venue": "arXiv:1009.5055,", "citeRegEx": "Lin et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning", "author": ["Zhouchen Lin", "Risheng Liu", "Huan Li. Linearized alternating direction method with parallel splitting", "adaptive penalty for separable convex programs in machine learning"], "venue": "(2):287\u2013325,", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["Guangcai Liu", "Zhuochen Lin", "Yong Yu"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2010a", "shortCiteRegEx": null, "year": 2010}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["Wei Liu", "Jun Wang", "Shih-Fu Chang"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2010b", "shortCiteRegEx": null, "year": 2010}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In CVPR", "author": ["Wei Liu", "Jun Wang", "Rongrong Ji", "Yugang Jiang", "Shih-Fu Chang. Supervised hashing with kernels"], "venue": "pages 2074 \u2013 2081,", "citeRegEx": "Liu et al.. 2012a", "shortCiteRegEx": null, "year": 2012}, {"title": "In ACM Multimedia", "author": ["Xianglong Liu", "Junfeng He", "Di Liu", "Bo Lang. Compact kernel hashing with multiple features"], "venue": "pages 881\u2013884,", "citeRegEx": "Liu et al.. 2012b", "shortCiteRegEx": null, "year": 2012}, {"title": "Mach", "author": ["Guangcan Liu", "Zhuochen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma. Robust recovery of subspace structures by low-rank representation. IEEE Trans. Pattern Anal"], "venue": "Intell., 35(1):171\u2013184,", "citeRegEx": "Liu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IJCV", "author": ["David Lowe. Distinctive image features from scale-invariant keypoints"], "venue": "60:91\u2013110,", "citeRegEx": "Lowe. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE TPAMI", "author": ["Jonathan Masci", "Michael M. Bronstein", "Alexander M. Bronstein", "Jurgen Schmidhuber. Multimodal similarity-preserving hashing"], "venue": "36(4):824\u2013830,", "citeRegEx": "Masci et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "IJCV, 42(3):145\u2013175,", "citeRegEx": "Oliva and Torralba. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Comparing apples to oranges: a scalable solution with heterogeneous hashing", "author": ["Mingdong Ou", "Peng Cui", "Fei Wang", "Jun Wang", "Wenwu Zhu", "Shiqiang Yang"], "venue": "ACM SIGKDD, pages 230\u2013238,", "citeRegEx": "Ou et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In CVPR", "author": ["Fumin Shen", "Chunhua Shen", "Qinfeng Shi", "Anton van den Hengel", "Zhenmin Tang. Inductive hashing on manifolds"], "venue": "pages 1562 \u2013 1569,", "citeRegEx": "Shen et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In ACM Multimedia", "author": ["Xiaobo Shen", "Fumin Shen", "Quan-Sen Sun", "Yun-Hao Yuan. Multi-view latent hashing for efficient multimedia search"], "venue": "pages 831\u2013834,", "citeRegEx": "Shen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ACM Multimedia", "author": ["Jingkuan Song", "Yi Yang", "Zi Huang", "Heng-Tao Shen", "Richang Hong. Multiple feature hashing for real-time large scale near-duplicate video retrieval"], "venue": "pages 423\u2013432,", "citeRegEx": "Song et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In CVPR", "author": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang. Semi-supervised hashing for scalable image retrieval"], "venue": "pages 3424 \u2013 3431,", "citeRegEx": "Wang et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards metric fusion on multi-view data: a crossview based graph random walk approach", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang"], "venue": "ACM CIKM, pages 805\u2013810,", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting correlation consensus: Towards subspace clustering for multi-modal data", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang"], "venue": "ACM Multimedia, pages 981\u2013984,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In IJCAI", "author": ["Qifan Wang", "Luo Si", "Bin Shen. Learning to hash on partial multi-modal data"], "venue": "pages 3904\u20133910,", "citeRegEx": "Wang et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective multi-query expansions: Robust landmark retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "ACM Multimedia, pages 79\u201388,", "citeRegEx": "Wang et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Lbmch: Learning bridging mapping for cross-modal hashing", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang"], "venue": "ACM SIGIR,", "citeRegEx": "Wang et al.. 2015c", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Image Processing", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang", "Xiaodi Huang. Robust subspace clustering for multi-view data by exploiting correlation consensus"], "venue": "24(11):3939\u20133949,", "citeRegEx": "Wang et al.. 2015d", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Neural Networks and Learning System", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Xiang Zhao. Unsupervised metric fusion over multiview data by graph random walk-based crossview diffusion"], "venue": "99:1\u201314,", "citeRegEx": "Wang et al.. 2015e", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge and Information Systems", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Qing Zhang", "Wenjie Zhang. Shifting multi-hypergraphs via collaborative probabilistic voting"], "venue": "46(3):515\u2013536,", "citeRegEx": "Wang et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Iterative views agreement: An iterative low-rank based structured optimization method to multi-view spectral clustering", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Meng Fang", "Shirui Pan"], "venue": "IJCAI,", "citeRegEx": "Wang et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "In ACM SIGKDD", "author": ["Ying Wei", "Yangqiu Song", "Yi Zhen", "Bo Liu", "Qiang Yang. Scalable heterogeneous translated hashing"], "venue": "pages 791\u2013800,", "citeRegEx": "Wei et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "NIPS,", "citeRegEx": "Weiss et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization", "author": ["John Wright", "Yigang Peng", "Yi Ma", "Arvind Ganesh", "Shankar Rao"], "venue": "NIPS,", "citeRegEx": "Wright et al.. 2009a", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust principal component analysis: exact recovery of corrupted low-rank matrices via convex optimization", "author": ["John Wright", "Yigang Peng", "Yi Ma", "Arvind Ganesh", "Shankar Rao"], "venue": "NIPS,", "citeRegEx": "Wright et al.. 2009b", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient image and tag co-ranking: a bregman divergence optimization method", "author": ["Lin Wu", "Yang Wang", "John Shepherd"], "venue": "ACM Multimedia,", "citeRegEx": "Wu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting attribute correlations: A novel trace lasso-based weakly supervised dictionary learning method", "author": ["Lin Wu", "Yang Wang", "Shirui Pan"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "Wu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In AAAI", "author": ["Rongkai Xia", "Yan Pan", "Lei Du", "Jian Yin. Robust multi-view spectral clustering via low-rank", "sparse decomposition"], "venue": "pages 2149\u20132155,", "citeRegEx": "Xia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on multi-view learning", "author": ["Chang Xu", "Dacheng Tao", "Chao Xu"], "venue": "arXiv:1304.5634,", "citeRegEx": "Xu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In CVPR", "author": ["Guangnan Ye", "Dong Liu", "I-Hong Jhuo", "Shih-Fu Chang. Robust late fusion with rank minimization"], "venue": "pages 3021\u20133028,", "citeRegEx": "Ye et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-tuning spectral clustering", "author": ["Lihi Zelnik-Manor", "Pietro Perona"], "venue": "NIPS,", "citeRegEx": "Zelnik.Manor and Perona. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In ACM SIGIR", "author": ["Dan Zhang", "Fei Wang", "Luo Si. Composite hashing with multiple information sources"], "venue": "pages 225\u2013234,", "citeRegEx": "Zhang et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural Networks", "author": ["Zhao Zhang", "Shuicheng Yan", "Mingbo Zhao. Similarity preserving low-rank representation for enhanced data representation", "effective subspace learning"], "venue": "53:81\u201394,", "citeRegEx": "Zhang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge-Based Systems", "author": ["Zhao Zhang", "Shuicheng Yan", "Mingbo Zhao", "Fanzhang Li. Bilinear low-rank coding framework", "extension for robust image recovery", "feature representation"], "venue": "86:143\u2013157,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Image Processing", "author": ["Zhao Zhang", "Fanzhang Li", "Mingbo Zhao", "Li Zhang", "Shuicheng Yan. Joint low-rank", "sparse principal feature coding for enhanced robust repersentation", "visual classification"], "venue": "25(6):2429\u20132443,", "citeRegEx": "Zhang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In AAAI", "author": ["Shuai Zheng", "Xiao Cai", "Chris Ding", "Feiping Nie", "Heng Huang. A closed form solution to multi-view low-rank regression"], "venue": "pages 1973\u2013 1979,", "citeRegEx": "Zheng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Xiaowei Zhou", "Can Yang", "Weichuan Yu. Moving object detection by detecting contiguous outliers in the low-rank representation"], "venue": "35(3):597\u2013 610,", "citeRegEx": "Zhou et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Intensive hashing methods valid on single data source have been proposed which can be classified into data-independent hashing such as locality sensitive hashing (LSH) [Datar et al., 2004] and data-dependent hashing or learning based hashing [Weiss et al.", "startOffset": 168, "endOffset": 188}, {"referenceID": 40, "context": ", 2004] and data-dependent hashing or learning based hashing [Weiss et al., 2008; Wang et al., 2010].", "startOffset": 61, "endOffset": 100}, {"referenceID": 29, "context": ", 2004] and data-dependent hashing or learning based hashing [Weiss et al., 2008; Wang et al., 2010].", "startOffset": 61, "endOffset": 100}, {"referenceID": 31, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 30, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 34, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 43, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 44, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 38, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 35, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 36, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 37, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 49, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 7, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 23, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 28, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 20, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 27, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 49, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 7, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 23, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 28, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 20, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 27, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 35, "context": ", missing values in pixels), resulting in corresponding similarity matrices being corrupted by considerable noises [Wang et al., 2015d; Xia et al., 2014].", "startOffset": 115, "endOffset": 153}, {"referenceID": 45, "context": ", missing values in pixels), resulting in corresponding similarity matrices being corrupted by considerable noises [Wang et al., 2015d; Xia et al., 2014].", "startOffset": 115, "endOffset": 153}, {"referenceID": 13, "context": "sus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012].", "startOffset": 147, "endOffset": 201}, {"referenceID": 53, "context": "sus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012].", "startOffset": 147, "endOffset": 201}, {"referenceID": 47, "context": "sus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012].", "startOffset": 147, "endOffset": 201}, {"referenceID": 19, "context": "This motivates us to deliver a framework to jointly and effectively learn similarity matrices and robust hash functions with kernel functions plugged because the kernel trick is able to tackle linearly inseparable data [Liu et al., 2012a].", "startOffset": 219, "endOffset": 238}, {"referenceID": 21, "context": "To this end, a latent kernelized similarity matrix is recovered shared across views by using lowrank representation (LRR) [Liu et al., 2013] which is robust to corrupted observations.", "startOffset": 122, "endOffset": 140}, {"referenceID": 18, "context": "To this end, we are further motivated to employ an landmark graph to build an approximate neighborhood graph using landmarks [Liu et al., 2011; Liu et al., 2010b], in which the similarity between a pair of data points is measured with respect to a small number of landmarks (typically a few hundred).", "startOffset": 125, "endOffset": 162}, {"referenceID": 17, "context": "To this end, we are further motivated to employ an landmark graph to build an approximate neighborhood graph using landmarks [Liu et al., 2011; Liu et al., 2010b], in which the similarity between a pair of data points is measured with respect to a small number of landmarks (typically a few hundred).", "startOffset": 125, "endOffset": 162}, {"referenceID": 18, "context": "The resulting graph is built in O(N) time and sufficiently sparse with performance approaching to true k-NN graphs as the number of landmarks increases [Liu et al., 2011].", "startOffset": 152, "endOffset": 170}, {"referenceID": 9, "context": "The proposed method is also different from partial view study [Kumar and III, 2011; Wang et al., 2015a], where they consider the case that data examples with some modalities are missing.", "startOffset": 62, "endOffset": 103}, {"referenceID": 32, "context": "The proposed method is also different from partial view study [Kumar and III, 2011; Wang et al., 2015a], where they consider the case that data examples with some modalities are missing.", "startOffset": 62, "endOffset": 103}, {"referenceID": 46, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 53, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 11, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 31, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 33, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 45, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 14, "context": "These principles are formulated into an objective function, which is optimized based on the inexact Augmented Lagrangian Multiplier (ALM) scheme [Lin et al., 2010].", "startOffset": 145, "endOffset": 163}, {"referenceID": 25, "context": "We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities.", "startOffset": 62, "endOffset": 120}, {"referenceID": 39, "context": "We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities.", "startOffset": 62, "endOffset": 120}, {"referenceID": 10, "context": "We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities.", "startOffset": 62, "endOffset": 120}, {"referenceID": 28, "context": "Some recent representative works include Multiple Feature Hashing (MFH) [Song et al., 2011], Composite Hashing with Multiple Sources (CHMS) [Zhang et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 49, "context": ", 2011], Composite Hashing with Multiple Sources (CHMS) [Zhang et al., 2011], Compact Kernel Hashing with multiple features (CKH) [Liu et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 20, "context": ", 2011], Compact Kernel Hashing with multiple features (CKH) [Liu et al., 2012b], and Multi-view Sequential Spectral Hashing (SSH) [Kim et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 7, "context": ", 2012b], and Multi-view Sequential Spectral Hashing (SSH) [Kim et al., 2012].", "startOffset": 59, "endOffset": 77}, {"referenceID": 27, "context": "[Shen et al., 2015] present a Multi-View Latent Hashing (MVLH) to learn hash codes by performing matrix factorization on a unified kernel feature space over multiple views.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "In this aspect, we attentively employ the low-rank representation (LRR) [Liu et al., 2013] to recover latent subspace structures from corrupted data.", "startOffset": 72, "endOffset": 90}, {"referenceID": 41, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 2, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 13, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 52, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 50, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 51, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 41, "context": "It has striking success in many applications such as data compression [Wright et al., 2009a], subspace clustering [Liu et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 21, "context": ", 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 5, "context": ", 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 50, "context": ", 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 54, "context": ", 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015].", "startOffset": 30, "endOffset": 89}, {"referenceID": 52, "context": ", 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015].", "startOffset": 30, "endOffset": 89}, {"referenceID": 51, "context": ", 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015].", "startOffset": 30, "endOffset": 89}, {"referenceID": 52, "context": "For instance, in [Zhang et al., 2016], Zhang et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 46, "context": "Nowadays, data are usually collected from diverse domains or obtained from various feature extractors, and each group of features can be regarded as a particular view [Xu et al., 2013].", "startOffset": 167, "endOffset": 184}, {"referenceID": 21, "context": "In practice, the underlying structure of data could be multiple subspaces, and thus Low-Rank Representation (LRR) is designed to find subspace structures in noisy data [Liu et al., 2013; Zhang et al., 2014].", "startOffset": 168, "endOffset": 206}, {"referenceID": 50, "context": "In practice, the underlying structure of data could be multiple subspaces, and thus Low-Rank Representation (LRR) is designed to find subspace structures in noisy data [Liu et al., 2013; Zhang et al., 2014].", "startOffset": 168, "endOffset": 206}, {"referenceID": 13, "context": "The multi-view low-rank analysis [Li et al., 2015] is a recently proposed multi-view learning approach, which introduces low-rank constraint to reveal the intrinsic structure of data, and identifies outliers for the representation coefficients in low-rank matrix recovery.", "startOffset": 33, "endOffset": 50}, {"referenceID": 8, "context": "Following the Kernelized Locality Sensitive Hashing [Kulis and Grauman, 2009], we uniformly select R samples from the training set X , denoted by Zr (r = 1, .", "startOffset": 52, "endOffset": 77}, {"referenceID": 18, "context": "Cp = \u2211R r=1Wrp\u03c6(Zr) indicates the linear combination of R landmarks, which can be the cluster centers [Liu et al., 2011] via scalable R-means clustering over the feature space with d dimensions.", "startOffset": 102, "endOffset": 120}, {"referenceID": 19, "context": "The kernel function is plugged into hash function because the kernel trick has been theoretically and empirically proved to be able to tackle the data distribution that is almost linearly inseparable [Liu et al., 2012a].", "startOffset": 200, "endOffset": 219}, {"referenceID": 11, "context": "We explicitly model the redundancy via sparsity since multi-view study suggests that each individual view is sufficient to identify most of the similarity structure, and the deviation between requisite component and data sample is sparse [Kumar et al., 2011].", "startOffset": 238, "endOffset": 258}, {"referenceID": 21, "context": "Thus, an `2,1-norm is adopted to characterize errors since they usually cause column sparsity in an affinity matrix [Liu et al., 2013].", "startOffset": 116, "endOffset": 134}, {"referenceID": 40, "context": "Many studies [Weiss et al., 2008; Song et al., 2011] have shown the benefits to exploit local structure of the training data to infer accurate and compact hash codes.", "startOffset": 13, "endOffset": 52}, {"referenceID": 28, "context": "Many studies [Weiss et al., 2008; Song et al., 2011] have shown the benefits to exploit local structure of the training data to infer accurate and compact hash codes.", "startOffset": 13, "endOffset": 52}, {"referenceID": 18, "context": "To avoid the computational bottleneck, we employ a landmark graph by using a small set of L points called landmarks to approximate the data neighborhood structure [Liu et al., 2011].", "startOffset": 163, "endOffset": 181}, {"referenceID": 18, "context": "Thus, the landmark graph provides a powerful approximation to the adjacency matrix S as \u015c = F\u039b\u22121FT where \u039b = diag(F 1) \u2208 RL\u00d7L [Liu et al., 2011].", "startOffset": 126, "endOffset": 144}, {"referenceID": 40, "context": "Following spectral hashing [Weiss et al., 2008], we relax the constraints yi \u2208 {\u22121, 1} to be yi = W K\u0302i + b, then we have", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "To compute K\u0302 and E, we employ an efficient optimization technique, the inexact augmented Lagrange multiplier (ALM) algorithm [Lin et al., 2010].", "startOffset": 126, "endOffset": 144}, {"referenceID": 16, "context": "The rank minimization problem has been well studied in literature [Liu et al., 2010a; Wright et al., 2009b].", "startOffset": 66, "endOffset": 107}, {"referenceID": 42, "context": "The rank minimization problem has been well studied in literature [Liu et al., 2010a; Wright et al., 2009b].", "startOffset": 66, "endOffset": 107}, {"referenceID": 15, "context": "where S%(x) = max(x\u2212%, 0)+min(x+%, 0) is the shrinkage operator [Lin et al., 2015].", "startOffset": 64, "endOffset": 82}, {"referenceID": 6, "context": "Each subproblem is a proximal operator problem, which can be efficiently solved by the projection algorithm in [Duchi et al., 2008].", "startOffset": 111, "endOffset": 131}, {"referenceID": 0, "context": "A widely used solution is the Nystr\u00f6m extension [Bengio et al., 2004].", "startOffset": 48, "endOffset": 69}, {"referenceID": 26, "context": "[Shen et al., 2013].", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "To this end, we employ a prototype algorithm [Shen et al., 2013] to approximate yq using only a small base set:", "startOffset": 45, "endOffset": 64}, {"referenceID": 28, "context": "\u2022 Multiple feature hashing (MFH) [Song et al., 2011]: This method exploits local structure in each feature and global consistency in the optimization of hashing functions.", "startOffset": 33, "endOffset": 52}, {"referenceID": 49, "context": "\u2022 Composite hashing with multiple sources (CHMS) [Zhang et al., 2011]: This method treats a linear combination of view-specific similarities as an average similarity which can be plugged into a spectral hashing framework.", "startOffset": 49, "endOffset": 69}, {"referenceID": 20, "context": "\u2022 Compact kernel hashing with multiple features (CKH) [Liu et al., 2012b]: It is a multiple feature hashing framework where multiple kernels are linearly combined.", "startOffset": 54, "endOffset": 73}, {"referenceID": 7, "context": "\u2022 Sequential spectral hashing with multiple representations (SSH) [Kim et al., 2012]: This method constructs an average similarity matrix to assemble view-specific similarity matrices.", "startOffset": 66, "endOffset": 84}, {"referenceID": 27, "context": "\u2022 Multi-View Latent Hashing (MVLH) [Shen et al., 2015]: This is an unsupervised multi-view hashing approach where binary codes are learned by the latent factors shared by multiple views from an unified kernel feature space.", "startOffset": 35, "endOffset": 54}, {"referenceID": 24, "context": "Every image is assigned to a mutually exclusive class label and for each image, we extract 512-dimensional GIST feature [Oliva and Torralba, 2001] and 300-dimensional bag-of-words quantized from dense SIFT features [Lowe, 2004] to be two views.", "startOffset": 120, "endOffset": 146}, {"referenceID": 22, "context": "Every image is assigned to a mutually exclusive class label and for each image, we extract 512-dimensional GIST feature [Oliva and Torralba, 2001] and 300-dimensional bag-of-words quantized from dense SIFT features [Lowe, 2004] to be two views.", "startOffset": 215, "endOffset": 227}, {"referenceID": 3, "context": "\u2022 NUS-WIDE [Chua et al., 2009] contains 269,648 labeled images crawled from Flickr and is manually annotated with 81 categories.", "startOffset": 11, "endOffset": 30}, {"referenceID": 48, "context": "The parameter \u03c3 is learned via the self-tuning strategy [Zelnik-Manor and Perona, 2004].", "startOffset": 56, "endOffset": 87}, {"referenceID": 19, "context": "Evaluation Metric The mean precision-recall and mean average precision (MAP) are computed over the retrieved set consisting of the samples with the hamming distance [Liu et al., 2012a] using 8 to 32 bits to a specific query.", "startOffset": 165, "endOffset": 184}, {"referenceID": 7, "context": "SSH has a gain in efficiency compared with MFH and CHMS on account of their approximation on the K-nearest graph construction [Kim et al., 2012].", "startOffset": 126, "endOffset": 144}, {"referenceID": 12, "context": "The MINIST dataset [LeCun et al., 1998] consists of 70K images, each of 784 dimensions, of handwritten digits from \u201c0\u201d to \u201c9\u201d.", "startOffset": 19, "endOffset": 39}], "year": 2016, "abstractText": "Learning hash functions/codes for similarity search over multi-view data is attracting increasing attention, where similar hash codes are assigned to the data objects characterizing consistently neighborhood relationship across views. Traditional methods in this category inherently suffer three limitations: 1) they commonly adopt a two-stage scheme where similarity matrix is first constructed, followed by a subsequent hash function learning; 2) these methods are commonly developed on the assumption that data samples with multiple representations are noise-free,which is not practical in reallife applications; 3) they often incur cumbersome training model caused by the neighborhood graph construction using all N points in the database (O(N)). In this paper, we motivate the problem of jointly and efficiently training the robust hash functions over data objects with multi-feature representations which may be noise corrupted. To achieve both the robustness and training efficiency, we propose an approach to effectively and efficiently learning low-rank kernelized 1 hash functions shared across views. Specifically, we utilize landmark graphs to construct tractable similarity matrices in multi-views to automatically discover neighborhood structure in the data. To learn robust hash functions, a latent low-rank kernel function is used to construct hash functions in order to accommodate linearly inseparable data. In particular, a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel-based similarity matrices. Extensive experiments on realworld multi-view datasets validate the efficacy of our method in the presence of error corruptions. We use kernelized similarity rather than kernel, as it is not a squared symmetric matrix for data-landmark affinity matrix.", "creator": "LaTeX with hyperref package"}}}