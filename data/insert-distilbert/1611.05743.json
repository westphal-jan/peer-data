{"id": "1611.05743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Relational Multi-Manifold Co-Clustering", "abstract": "co - clustering targets on grouping the samples ( e. g., documents, users ) and the features ( e. g., words, ratings ) simultaneously. it employs analyzing the dual relation and the bilateral information between matched the samples and all features. in many realworld applications, data usually reside on a submanifold of the ambient euclidean space, but it is nontrivial to estimate the intrinsic manifold of filling the data space in a principled way. in this study, we focus on improving the co - clustering performance via manifold ensemble learning, which is able to maximally approximate the intrinsic manifolds of both components the sample and feature spaces. to helps achieve this, we develop a novel co - clustering algorithm called relational multi - manifold co - clustering ( rmc ) based on symmetric nonnegative matrix tri - factorization, which decomposes the relational data matrix into three submatrices. this method considers the intertype relationship revealed by the relational data matrix, and also the intra - type information reflected by the affinity matrices encoded on the sample and feature manifold data distributions. specifically, we assume the intrinsic manifold of the sample or feature space lies in a convex hull of some pre - defined candidate manifolds. we ideally want to learn a convex combination of them to maximally approach the desired intrinsic manifold. to optimize the objective function, the multiplicative rules are utilized to update the potential submatrices alternatively. besides, both the entropic mirror descent algorithm and the coordinate descent algorithm are exploited to learn the manifold coefficient vector. extensive experiments on documents, images and gene expression data sets have demonstrated the superiority of the proposed valuation algorithm compared to other well - established methods.", "histories": [["v1", "Wed, 16 Nov 2016 05:33:04 GMT  (120kb)", "http://arxiv.org/abs/1611.05743v1", "11 pages, 4 figures, published in IEEE Transactions on Cybernetics (TCYB)"]], "COMMENTS": "11 pages, 4 figures, published in IEEE Transactions on Cybernetics (TCYB)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ping li", "jiajun bu", "chun chen", "zhanying he", "deng cai"], "accepted": false, "id": "1611.05743"}, "pdf": {"name": "1611.05743.pdf", "metadata": {"source": "CRF", "title": "Relational Multi-Manifold Co-Clustering", "authors": ["Ping Li", "Jiajun Bu"], "emails": ["ing}@zju.edu.cn).", "cai@cad.zju.edu.cn)"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n05 74\n3v 1\n[ cs\n.L G\n] 1\n6 N\nov 2\n01 6\nRelational Multi-Manifold Co-Clustering Ping Li, Jiajun Bu, Member, IEEE, Chun Chen, Member, IEEE, Zhanying He, and Deng Cai, Member, IEEE\nAbstract\u2014Co-clustering targets on grouping the samples (e.g., documents, users) and the features (e.g., words, ratings) simultaneously. It employs the dual relation and the bilateral information between the samples and features. In many realworld applications, data usually reside on a submanifold of the ambient Euclidean space, but it is nontrivial to estimate the intrinsic manifold of the data space in a principled way. In this study, we focus on improving the co-clustering performance via manifold ensemble learning, which is able to maximally approximate the intrinsic manifolds of both the sample and feature spaces. To achieve this, we develop a novel co-clustering algorithm called Relational Multi-manifold Co-clustering (RMC) based on symmetric nonnegative matrix tri-factorization, which decomposes the relational data matrix into three submatrices. This method considers the intertype relationship revealed by the relational data matrix, and also the intra-type information reflected by the affinity matrices encoded on the sample and feature data distributions. Specifically, we assume the intrinsic manifold of the sample or feature space lies in a convex hull of some pre-defined candidate manifolds. We want to learn a convex combination of them to maximally approach the desired intrinsic manifold. To optimize the objective function, the multiplicative rules are utilized to update the submatrices alternatively. Besides, both the entropic mirror descent algorithm and the coordinate descent algorithm are exploited to learn the manifold coefficient vector. Extensive experiments on documents, images and gene expression data sets have demonstrated the superiority of the proposed algorithm compared to other well-established methods.\nIndex Terms\u2014Relational co-clustering, manifold ensemble learning, nonnegative matrix tri-factorization, entropic mirror descent algorithm, coordinate descent algorithm.\nI. INTRODUCTION\nTHE last dozen years have witnessed the profoundchanges in human lifestyle with the rapid development of modern digital technologies. A huge amount of multitype relational data emerge every moment in a broad range of real-world applications [12], [29], e.g., numerous documents in online offices, various images or videos in the social networks, and gene expression data for medical diagnosis. Clustering has established itself as a very useful tool to handle a vast number of data with successful applications in knowledge management, information retrieval and bioinformatics, etc. As an unsupervised learning mechanism, clustering seeks the appropriate partitioning\nManuscript received August 7, 2012; revised November 25, 2012; accepted December 10, 2012.\nP. Li, J. Bu, C. Chen and Z. He are with the Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University, Hangzhou 310027, China (e-mail: {lpcs, bjj, chenc, hezhanying}@zju.edu.cn).\nD. Cai is with the State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou 310058, China (e-mail: dengcai@cad.zju.edu.cn)\nof the data with the rule that the data points within the same cluster should be more closely and mutually interdependent than those in different clusters. In general, traditional clustering belongs to the unilateral learning, namely it only emphasizes clustering along the sample or feature dimension. Recent works have shown that clustering samples and features simultaneously, i.e., co-clustering, is beneficial to further improving the clustering performance, in the sense that co-clustering fully makes use of the dual interdependence between samples and features to discover certain hidden clustering structures in data [17], [18], [34].\nActually, a great many of important applications require to co-cluster both the samples and features, which are often stacked in columns and rows of a dyadic data matrix, such as co-occurrence matrix, rating matrix and proximity matrix [30]. For instance, in the text and webpage analysis, the relations between words and documents can be reflected by a contingency table [16], [31]. In a recommendation system, users and ratings on items (e.g., movies, music) can be coclustered via collaborative filtering [11]. In the biological domain (e.g., cancer diagnosis, mircoarray analysis), we can cluster the genes and experimental conditions simultaneously [24]. To this end, many co-clustering algorithms have emerged, such as graph partitioning based model [17], [33], [42], pattern based model [41], information theory based model [18], [35], and matrix factorization based model [14], [19], [40].\nIn this work, we focus on matrix factorization based co-clustering, which models the sample-feature relationship from the data reconstruction perspective. A heuristic method is to use singular value decomposition (SVD), whose low rank singular vectors constitute a compact representation. However, its factorized matrices contain negative values, which lacks a nice interpretation for co-clustering on documents or facial images. Therefore, we take advantage of a popularized tool, namely nonnegative matrix tri-factorization [19], as the foundation of our approach. This method imposes the nonnegative constraints on the decomposed matrices, which leads to a parts-based representation [26]. On the other hand, previous studies have shown human generated data (e.g., documents) are usually drawn from a probability distribution that has support on or near to a submanifold of the ambient Euclidean space [6], [7], [9], and manifold learning has favorable applications in wide areas [3]. As a result, some researchers strive to consider manifold geometrical structure in co-clustering by dual graph regularization [23], [38], [43]. However, it is a nontrivial and challenging task to seek the intrinsic manifolds of different types of data objects (e.g., samples and features) for graph based co-clustering. To address this issue, inspired from the work in [21], we propose\nto approximate the optimal manifold by using a convex combination of some pre-given candidate manifolds, thus developing a novel method called Relational Multimanifold Co-clustering (RMC) to improve the clustering performance via manifold ensemble learning.\nOur approach employs the symmetric nonnegative matrix tri-factorization to decompose the relational data matrix into three matrices for co-clustering. We consider the inter-type relation through the relational data matrix and the intra-type information through the affinity matrices constructed on both the sample and feature spaces [38], [39]. In manifold ensemble learning, we assume that the intrinsic manifold of the sample or feature space lies in a convex hull of a group of pre-defined candidate manifolds [21]. It is strongly desirable to approximate the intrinsic manifolds of both the sample and feature spaces as much as possible, i.e., to learn an appropriate convex combination of a set of available manifolds. To optimize the objective function, the multiplicative update rules are adopted for the factorized matrices in an alternating manner. In this work, we exploit the entropic mirror descent algorithm and the coordinate descent algorithm to automatically learn the appropriate convex combination of candidate manifolds. To explore the performance of our method, we conducted extensive experiments on documents (e.g., webpages), images (e.g., handwritten digits and faces) and gene expression data. Experimental results suggest the efficacy of the proposed algorithm.\nThe remainder of this paper is organized as follows. In Section II, we briefly review the related works. Section III introduces our approach as well as two manifold coefficients optimization algorithms. Experimental results on several data sets from different domains are reported in Section IV involving parameter selection and results analysis. Finally, we provide some concluding remarks in Section V."}, {"heading": "II. RELATED WORKS", "text": "In this section, we primarily review some related works to our co-clustering method. In the last decade, coclustering has become a hotspot topic and received lots of attention from the multidisciplinary communities because of its promising applications to many practical problems, e.g., collaborative filtering [11], [22], microarray data analysis [15], [24].\nFrom the viewpoint of graph partitioning, two bipartite spectral graph partition methods were proposed in [17], [42] to co-cluster documents and words by finding minimum cut vertex partitions in a bipartite graph. More recently, authors in [33] put forward the isoperimetric co-clustering algorithm to partition the document-word bipartite graph, which minimizes the ratio of the perimeter of the bipartite graph partition and the partition area under a welldefined graph-theoretic area. For the information theory based co-clustering, samples and features are regarded as the instances of two random variables [43], e.g., an information bottleneck method was implemented in [35]\nto cluster documents by using word clusters. Besides, an information-theoretic co-clustering algorithm specially designed for contingency table was introduced in [18] and a more general co-clustering framework based on Bregman divergence was given in [1].\nMatrix factorization based co-clustering techniques have been widely studied. At the very beginning, nonnegative matrix factorization was proposed to learn a partsbased representation, and it approximates the original data matrix by the product of two decomposed nonnegative matrices [26], [44]. NMF has the intuitive interpretation for its results, but it focuses on the unilateral clustering and neglects the duality between rows and columns of a matrix. Motivated by this, the block value decomposition (BVD) was presented for co-clustering dyadic data [30]. It factorizes the data matrix into three components, i.e., the row and column coefficient matrices and the block value matrix. As an extension of NMF, orthogonal nonnegative matrix tri-factorization (ONMTF) was studied in [19], which emphasizes the role of bi-orthogonality in three-factor NMF. Thanks to the successful applications of manifold learning in recent years [3], [8], [10], some researchers consider the local geometrical structure in matrix factorization based co-clustering. For example, a dual regularized co-clustering (DRCC) method based on semi-nonnegative matrix tri-factorization was proposed in [23], which explores the geometrical structure of both data manifold and feature manifold. To reduce the computational complexity of DRCC, a fast nonnegative matrix trifactorization approach was shown in [40], which constrains two factorized nonnegative matrices to be cluster indicator matrices. Moreover, a symmetric nonnegative matrix trifactorization (SNMTF) framework was developed to cluster multi-type relational data [38], which incorporates the intratype information through manifold regularization. Based on this work, the authors further presented a fast nonnegative matrix tri-factorization approach to deal with large-scale data [39].\nHowever, these graph regularized methods have a common shortcoming that the manifold used for co-clustering might not be the true intrinsic manifold, and it will even deviates far from the desired in an adverse situation. It is a nontrivial task to seek the intrinsic manifold in reality. To alleviate this difficulty, in light of [21], we assume the intrinsic manifold of the sample or feature distribution lies in a convex hull of some candidate manifolds, and hope to maximally approximate the true intrinsic manifold using the convex combination of them. This way, the local geometrical structure can be better preserved in both the sample and feature spaces for co-clustering."}, {"heading": "III. RELATIONAL MULTI-MANIFOLD CO-CLUSTERING", "text": "In this part, we introduce our RMC approach. We briefly review the symmetric NMTF based co-clustering of the multi-type relational data and describe the manifold ensemble learning, from which we arrive at our objective. The multiplicative update rules of the decomposed matrices as\nwell as manifold coefficients optimization algorithms are provided. We begin with the problem formulation below."}, {"heading": "A. Problem Formulation", "text": "The general problem setting is to co-cluster multitype relational data. Given a K-type relational data set X = {X1,X2, ...,XK}, where each Xk represents the data objects of the k-th type, we define an inter-type relational matrix R with the sub-matrix Rij \u2208 Rni\u00d7nj , i 6= j, which reflects the inter-type relationship between the i-th type and the j-th type data objects. To model the intra-type structure information for each data type, we define an intra-type relational matrix W consisting of a set of affinity matrices Wk \u2208 R\nnk\u00d7nk encoded on the data, which indicates the intra-type relation of components within the k-th type data. Both of the inter-type and intra-type relationship among the different data objects are to be fully used for co-clustering.\nHowever, two-type relational data are omnipresent and frequently used in many real-world applications, e.g., simultaneously clustering documents and words, collaborative filtering in a recommendation system, gene expression data analysis under different experimental conditions. Hence, in this study we focus on the case K = 2, i.e., we employ both the sample type and the feature type data objects for co-clustering. The concerned matrices can be respectively formulated as\nR =\n[\n0 n1\u00d7n1 Rn1\u00d7n212 Rn2\u00d7n121 0 n2\u00d7n2\n]\n,\nW =\n[\nW n1\u00d7n11 0 n1\u00d7n2\n0 n2\u00d7n1 W n2\u00d7n22\n]\n,\nwhere 0 is a matrix with all zero entries of different sizes, the superscripts denote the matrix sizes. Here, R12 and R21 represent the feature and sample matrix respectively, they satisfy the condition R12 = RT21. Each column of R21 or R12 denotes a feature or sample vector."}, {"heading": "B. Symmetric NMTF", "text": "Symmetric nonnegative matrix tri-factorization (SNMTF) employs both the inter-type relationship and the intra-type information of multi-type relational data [38]. Similar to the previous works, i.e., Long\u2019s BVD [30], Ding\u2019s ONMTF [19] and Gu\u2019s DRCC [23], SNMTF shares an important property that they all decompose the data matrix into three low-rank matrices, i.e., (K = 2)\nR12 \u2248 G1S12G T 2 , (1)\nwhere G1 \u2208 Rn1\u00d7c1 and G2 \u2208 Rn2\u00d7c2 are the cluster indicator matrices for X1 and X2, respectively, c1 \u226a n1, c2 \u226a n2. The middle matrix S12 \u2208 Rc1\u00d7c2 can be treated as a compact representation of R12 [30], which absorbs the different scales of other matrices [19].\nNote that BVD and ONMTF impose nonnegative constraints on all three matrices G1, G2, S, while DRCC and SNMTF both relax the nonnegative constraint on the matrix S, thereby allowing negative entries. Different\nfrom BVD, ONMTF emphasizes the bi-orthogonality on G1, G2. Compared to ONMTF, both DRCC and SNMTF consider the local geometrical structure by incorporating the Laplacian regularization into the objective. Based on DRCC, SNMTF employs the symmetric matrix by virtue of the idea in [29], [37] to simultaneously cluster multi-type relational data, and its objective can be shown by\nmin G,S\n\u2016R\u2212GSGT \u20162F + 2\u03bbTr(G TLG), s.t. G 0, (2)\nwhere \u2016 \u00b7 \u2016F denotes the Frobenius norm of a matrix, L = D\u2212W is the graph Laplacian [3], D is the diagonal degree matrix with Dk(ii) = \u2211\nj Wk(ij) , and Lk = Dk \u2212Wk. The relational matrices G and S are designed similarly\nto W and R, respectively, i.e.,\nG =\n[\nGn1\u00d7c11 0 n1\u00d7c2 0 n2\u00d7c1 Gn2\u00d7c22\n]\n,\nS =\n[\n0 c1\u00d7c1 Sc1\u00d7c212 Sc2\u00d7c121 0 c2\u00d7c2\n]\n."}, {"heading": "C. Manifold Ensemble Learning", "text": "As can be observed from the existing graph based coclustering methods, the intrinsic manifold plays a crucial role in preserving the local geometrical structure in the data space. However, it is a nontrivial task to discover an appropriate intrinsic manifold in reality. Therefore, automatic and data-driven manifold approximation is said to be invaluable for manifold regularization based co-clustering. In this work, we adopt a novel learning paradigm named manifold ensemble learning to maximally approximate the true intrinsic manifold. This idea is inspired from the work in [21], which combines the automatic intrinsic manifold approximation and semi-supervised classification.\nOur assumption is that a series of initial guesses of graph Laplacian are available and the intrinsic manifold of the sample or feature space lies in the convex hull of these pregiven candidate manifolds. In some sense, this assumption constrains the search space of possible manifolds, since the optimal graph Laplacian is an discrete approximation to the intrinsic manifold [21], i.e.,\nL =\nq \u2211\ni=1\n\u00b5iL\u0303i, s.t.\nq \u2211\ni=1\n\u00b5i = 1, \u00b5i \u2265 0, (3)\nwhere a set of candidate graph Laplacians C = {L1, . . . ,Lq} is defined and the convex hull of this set is denoted by\nconvC = { q \u2211\ni\n\u00b5ixi| \u2211\ni\n\u00b5i = 1, xi \u2208 C, xi \u2265 0}.\nHere we use L\u0303i of the i-th candidate manifold to discriminate it from Lk of the k-th type data. Now, we have L \u2208 convC, which is also a graph Laplacian."}, {"heading": "D. Objective Function", "text": "Based on the above analysis, it is natural for us to take advantage of the manifold ensemble learning to approximate the intrinsic manifold in the sample and feature space, respectively. In concrete, we incorporate this idea into the symmetric nonnegative matrix tri-factorization framework and propose a novel co-clustering approach named Relational Multi-manifold Co-clustering (RMC).\nNow, it is easy to arrive at the objective function, i.e.,\nmin G,S,\u00b5\n\u2016R\u2212GSGT\u20162F + \u03b1Tr[G T (\nq \u2211\ni=1\n\u00b5iL\u0303i)G] + \u03b2\u2016\u00b5\u2016 2 2,\ns.t.\nq \u2211\ni=1\n\u00b5i = 1,\u00b5 0,G 0,\n(4) where \u03b1 > 0, \u03b2 > 0, the tradeoff parameter \u03b1 is used to govern the contribution of the ensemble manifold regularization to the objective, the l2-norm of \u00b5 is employed to avoid the coefficient parameter over-fitting to only one manifold and the factor \u03b2 acts as an over-fitting tolerance parameter for the manifold coefficients. Similar to [23], we do l2 normalization on columns of G and compensate its norm to S. The components of G, i.e., G1 and G2 represent the partition matrices of the feature matrix R21 and the sample matrix R12, respectively. We typically use the partition matrices to derive the co-clustering results."}, {"heading": "E. Optimization", "text": "In this section, we explore how to optimize the objective in Eq. (4). It can be readily found that the objective function is non-convex in G,S,\u00b5 jointly, but it is convex in them respectively. So it is unrealistic to find the global minimum since no closed-form solution can be obtained. We present an alternating scheme to optimize the objective as most of the previous works do [6], [23], [30], [38]. However, different from the existing schemes, it is more challenging to optimize our objective since it has a critical manifold coefficient vector to be solved. Details are shown below.\n1) Computation of S: When fixing G and \u00b5, the objective becomes minimizing JS = \u2016R \u2212 GSGT \u20162F . Taking its derivative to S, i.e.,\n\u2202JS \u2202S = \u22122GTRG + 2GTGSGTG,\nand setting it to 0, then we have the update rule\nS = (GTG)\u22121GTRG(GTG)\u22121. (5)\n2) Computation of G: When fixing S and \u00b5, the objective with respect to G reduces to minimizing\nJG = \u2016R\u2212GSG T \u20162F + \u03b1Tr(G TLG), s.t. G 0.\nTo solve this constrained optimization problem, we introduce the Lagrangian multiplier matrix \u039b and its Lagrangian function is formulated as\nL(G) = \u2016R\u2212GSGT\u20162F +\u03b1Tr(G TLG)+Tr(\u039bGT ). (6)\nRequiring its derivative to G be 0, we obtain\n\u039b = 4\u03b1LG\u2212 4A+ 4GB,\nwhere A = RGST , B = STGTGS. Since the Karush-Kuhn-Tucker (KKT) complementary condition [4] for the nonnegativity of Gij gives \u039bijGij = 0, we have\n(\u03b1LG\u2212A+GB)ijGij = 0. (7)\nSimilar to [20], we define L = L+\u2212L\u2212, A = A+\u2212A\u2212, B = B+ \u2212B\u2212, where\nL+ij = (|Lij |+Lij)\n2 , L\u2212ij =\n(|Lij | \u2212Lij)\n2 .\nWe substitute the decomposed positive and negative parts into Eq. (7), which leads to the update rule\nGij \u2190 Gij\n[\n(\u03b1L\u2212G+A+ +GB\u2212)ij (\u03b1L+G+A\u2212 +GB+)ij\n] 1 2\n. (8)\n3) Computation of \u00b5: When fixing G and S, the objective is simplified to\nmin \u00b5 f(\u00b5) =\nq \u2211\ni=1\n\u00b5isi + \u03b2\u2016\u00b5\u2016 2 2,\ns.t.\nq \u2211\ni=1\n\u00b5i = 1,\u00b5 0,\n(9)\nwhere si = Tr(GT L\u0303iG). It is easy to see that if \u03b2 = 0, then the trivial solution will be\n\u00b5i =\n{\n1, if si = mink=1,...,n sk, 0, otherwise.\nThis is extremely sparse and undesirable to learn a composite manifold. On the other hand, if \u03b2 \u2192 \u221e, all the candidate manifolds will receive identical weights, which is also unexpected. Therefore, it is essential to assign a reasonable value for the parameter \u03b2.\nTo optimize this problem, there are generally three possible ways. First, it can be solved by the generic Quadratic Programming (QP) method (e.g, CVX) [4], but this solver is often time-consuming for larger size problem and shows slow convergence. Second, it is actually an exactly well-defined problem, i.e., the convex minimization over the unit simplex, which can be elegantly solved by the Entropic Mirror Descent Algorithm (EMDA) with a global efficiency estimate proven to be mildly dependent on the problem size [2], as shown in Algorithm 1. Third, we can adopt the Coordinate Descent Algorithm (CDA) just like using sequential minimal optimization for support vector machines [32], which selects two variables to update in each iteration while keeping the others fixed, as shown in Algorithm 2. In this work, we put emphasis on EMDA and CDA, which are employed to learn appropriate manifold coefficients. The brief descriptions about them are given below.\nEntropic Mirror Descent Algorithm. It can be viewed as a nonlinear projected-subgradient type method, derived from using a general distance-like function instead of the\nAlgorithm 1 Entropic Mirror Descent Algorithm (EMDA) Input:\nThe Lipschitz constant Lf , The tradeoff parameter \u03b2, The ensemble manifold s.\nOutput: The manifold coefficient vector \u00b5. Procedure: 1: Initialize \u00b5i with the identical weight 1/q, where q is\nthe number of candidate manifolds. 2: for i = 1 to q do 3: repeat 4: Compute tm = \u221a 2 ln q mL2\nf\n, where m is the m-th\niteration. 5: Update each \u00b5i according to\n\u00b5m+1i \u2190 \u00b5mi exp[\u2212tmf \u2032(\u00b5mi )] \u2211q\ni=1 \u00b5 m i exp[\u2212tmf \u2032(\u00b5mi )] ,\nwhere f \u2032(\u00b5mi ) = 2\u03b2\u00b5 m i + si.\n6: until convergence 7: end for\nAlgorithm 2 Coordinate Descent Algorithm (CDA) Input:\nThe tradeoff parameter \u03b2, The ensemble manifold s.\nOutput: The manifold coefficient vector \u00b5. Procedure: 1: Initialize \u00b5i with the identical weight 1/q, where q is\nthe number of candidate manifolds. 2: for i = 1 to q do 3: for j = 1 to q (j 6= i) do 4: repeat 5: if 2\u03b2(\u00b5i + \u00b5j) + (sj \u2212 si) \u2264 0 then 6: \u00b5\u2217i = 0, \u00b5 \u2217\nj = \u00b5i + \u00b5j . 7: else 8: if 2\u03b2(\u00b5i + \u00b5j) + (si \u2212 sj) \u2264 0 then 9: \u00b5\u2217j = 0, \u00b5 \u2217 i = \u00b5i + \u00b5 \u2032\nj . 10: else 11: \u00b5\u2217i = 2\u03b2(\u00b5i+\u00b5j)+(sj\u2212si) 4\u03b2 , 12: \u00b5\u2217j = \u00b5i + \u00b5j \u2212 \u00b5 \u2217\ni . 13: end if 14: end if 15: until convergence 16: end for 17: end for\nusual Euclidean squared distance [2]. It has been shown that EMDA owns the natural advantage to solve this convex problem over the unit simplex \u25b3 = {\u00b5 \u2208 Rq : \u2211q\ni=1 \u00b5i = 1,\u00b5 0}. To apply this algorithm, the objective function f should be a convex Lipschitz continuous function with Lipschitz constant Lf with respect to a fixed given norm. In our approach, we derive this Lipschitz constant from \u2016\u2207f(\u00b5)\u20161 \u2264 2\u03b2 + \u2016s\u20161 = Lf , where s = {s1, . . . , sq}.\nAlgorithm 3 Relational Multi-manifold Co-clustering (RMC) Input:\nThe relational data matrices R and W , The number of co-clusters c1 and c2, The tradeoff parameters \u03b1 and \u03b2, The convergence rate \u01eb = 10\u22125.\nOutput: The partition matrix G. Procedure: 1: Initialize G using k-means. 2: repeat 3: Compute S = (GTG)\u22121GTRG(GTG)\u22121. 4: Learn the manifold coefficient vector \u00b5 using EMDA\nin Algorithm 1 or CDA in Algorithm 2. 5: Update the matrix G according to\nGij \u2190 Gij\n[\n(\u03b1L\u2212G+A+ +GB\u2212)ij (\u03b1L+G+A\u2212 +GB+)ij\n] 1 2\n.\n6: until convergence\nHere, we use \u2016 \u00b7 \u20161 norm as suggested in [2]. Coordinate Descent Algorithm. In this method, a pair of variables are selected to joint the update process while holding the others fixed in each iteration. It is true that the summation of each pair of elements will keep still after the previous iteration due to the constraint\n\u2211q i=1 \u00b5i = 1.\nAll pairs of elements in the coefficient vector \u00b5 will be iteratively scanned. Compared to EMDA, CDA costs more time since it has to traverse over all the element pairs while EMDA only goes over the elements in sequence."}, {"heading": "F. Our RMC Approach", "text": "In summary, we present the primary procedures of the proposed Relational Multi-manifold Co-clustering (RMC) approach in Algorithm 3.\nAs can be seen, we will finally obtain a partition matrix G used for co-clustering the samples and features simultaneously. Since the intrinsic manifold is maximally approximated via manifold ensemble learning, the local geometrical structure of the sample or feature space can be better respected, thus achieving more promising coclustering results. In particular, we define the RMC approach using EMDA to learn the manifold coefficient as RMC-E for short, and using CDA as RMC-C for short.\nNote that here we omit the convergence proof of the multiplicative update rules, since our method essentially follows the similar fashion of many existing co-clustering algorithms, e.g., DRCC [23], SNMTF [38]. To probe it deeply, we refer the readers to these literatures for details. Besides, for matrix factorization based co-clustering methods, there is an important issue to be considered, i.e.,, computational complexity. Luckily, there have been some research works focusing on this problem in [39], [40]. Therefore, to make our algorithm scalable, it is an ideal choice to adopt those speed-up strategies."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we aim to investigate the clustering performance of the proposed method on a broad range of collected data sets. A number of interesting experiments were carried out to demonstrate the effectiveness of our approach for document, image and gene expression data clustering, respectively. We first give brief descriptions about the data sets and evaluation criteria. Then, parameter settings for all the compared algorithms are presented and the corresponding results are reported. Finally, we explore the parameter selection and the distributions of the manifold coefficients."}, {"heading": "A. Data Corpora", "text": "We conduct the performance evaluations on several diverse data collections, i.e., two text corpora, three image databases and three gene expression data. Their important statistics are summarized in Table I and the brief descriptions about them are shown below.\nText corpora. NGroups5 is selected from the popular newsgroup data collection 20Newsgroups1. We use a subset of the 20news-bydate version, which removed the duplicates and some headers. This subset contains five different topics, which refer to 4,052 documents. RCV1-5 is a subset chosen from the Reuters Corpus Volume I2 (RCV1) collection [27]. We choose five topics contained in a smaller RCV1 database [13], and this subset is associated with the \u2018M141\u2019,\u2018GCAT\u2019,\u2018G151\u2019,\u2018G158\u2019, \u2018G159\u2019 topics. For the two data sets, we preprocess them using a standard feature selection mechanism in [35] for text data, i.e., we select 1,200 words with the highest contribution to the mutual information between the words and the documents [14].\nImage databases. The COIL203 image library contains 20 different objects viewed from varying angles. Each object has 72 gray scale images with the size of 32\u00d732. The AlphaDigit4 is a handwritten image database that contains 1,404 binary images, covering 20\u00d716 digits of \u201c0\u201d through \u201c9\u201d and capitals \u201cA\u201d through \u201cZ\u201d. UMIST5 is a face image database that contains 575 multi-view face images of 20 people, referring to a range of poses from profile to frontal views. Each image is rescaled to 28\u00d723 pixels.\n1http://people.csail.mit.edu/jrennie/20Newsgroups/ 2http://www.daviddlewis.com/resources/testcollections/rcv1/ 3http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php 4http://cs.nyu.edu/\u223croweis/data.html 5http://images.ee.umist.ac.uk/danny/database.html\nGene expression data. The three microarray gene expression data6 Leukemia2, LungCancer and SRBCT are often used to do multicategory cancer diagnosis in bioinformatics [36]. Leukemia2 and LungCancer are produced by oligonucleotide-based technology. SRBCT was obtained by using two-color cDNA platform with consecutive image analysis. Similar to [28], we removed the genes which vary little across samples so as to reduce the computational complexity. Table II shows the exact preprocessing strategy of the gene expression data. Details about each term can be referred to [28]. No operation is conducted on SRBCT since it has been originally preprocessed in [25].\nNotice that all data sets used here are normalized to unit Euclidean length in prior. Besides, the entries of each input data matrix are nonnegative, thereby all compared algorithms can be applied to yield the clustering results."}, {"heading": "B. Performance Comparison", "text": "To explore the clustering performance of the proposed algorithm, we compare it with some state-of-the-art approaches, which are clearly listed below.\n\u2022 KM: Conventional k-means method. \u2022 NMF: Nonnegative matrix factorization [26]. \u2022 GNMF: Graph regularized nonnegative matrix factor-\nization, which considers local geometrical structure by the sample graph regularization [6]. \u2022 DRCC: Dual regularized co-clustering, which employs both the sample and feature graph regularization [23]. \u2022 ONMTF: Orthogonal nonnegative matrix tri-factorization with the bi-orthogonality constraints imposed [19]. \u2022 SNMTF: Symmetric nonnegative matrix tri-factorization, which utilizes NMTF to simultaneously cluster different types of data [38]. \u2022 OSNTF: Orthogonal symmetric nonnegative matrix tri-factorization, which is designed for simultaneous clustering of multi-type relational data with the orthogonality constraint [39]. \u2022 RMC: Our relational multi-manifold co-clustering approach, which makes use of the entropic mirror descent algorithm (RMC-E) and the coordinate descent algorithm (RMC-C) respectively to learn a convex combination of a group of candidate manifolds.\nNote that among the above compared methods, the original update rule of G in SNMTF [38] and OSNTF [39] might appear negative elements due to the polarity uncertainty of the numerator (RGS +\u03bbWG). In consequence,\n6http://www.gems-system.org/datasets/\nwe follow [23] and use the similar update rule for the matrix G in SNMTF and OSNTF, so that the nonnegativity can be well guaranteed and the objective strictly reduces. For both SNMTF and OSNTF, the cluster membership of each sample is determined by the corresponding row vector of the cluster indicator matrix G. For the remaining methods except KM, we perform clustering using k-means in the derived low-dimensional data space. In the experiments, we ran k-means 20 times with different randomly generated starting points and the result in terms of the minimized objective function was recorded. For all the evaluated approaches, we repeat the clustering twenty times and the average results over 20 test runs are reported."}, {"heading": "C. Evaluation Criteria", "text": "We adopt two popular criteria to measure the clustering performance [5], [23], i.e., the accuracy (AC) and the normalized mutual information (NMI).\nAC denotes the percentage of correct labels estimated by the clustering algorithm. Given a data point xi, let ai and gi be the estimated and true label respectively, then we have\nAC =\n\u2211n i=1 \u03b4(gi,map(ai))\nn , (10)\nwhere n is the total number of samples, \u03b4(\u00b7, \u00b7) is an indicator function that equals one if the two entries are the same and equals zero otherwise. The permutation mapping function map(ai) maps each cluster label ai to the equivalent label of the data set.\nNMI evaluates how closely the clustering algorithm is able to reconstruct the underlying label distribution in the data corpus. Let C and C\u2032 be the cluster sets from the ground truth and the clustering method respectively, then\nNMI between them is defined as\nNMI(C,C\u2032) =\n\u2211\nci\u2208C,c \u2032 j \u2208C\u2032 p(ci, c\n\u2032 j) \u00b7 log2 p(ci,c\n\u2032 j)\np(ci)\u00b7p(c\u2032j)\nmax(H(C), H(C\u2032)) ,\n(11) where the probabilities p(ci), p(c\u2032j) indicate to what extent a sample belongs to the clusters ci and c\u2032j respectively. p(ci, c \u2032\nj) is the joint probability that the selected sample belongs to ci and c\u2032j simultaneously. H(C) and H(C\n\u2032) are the entropies of C and C\u2032. NMI(C,C\u2032) takes values between zero and one. The larger the NMI value is, the better clustering performance the algorithm will achieve."}, {"heading": "D. Parameter Settings", "text": "There are several parameters to be tuned in the evaluated algorithms. To ensure the fairness, we run them under different parameter settings, and report the best results.\nFor all methods except KM, the number of sample or feature clusters is set to the actual number of classes in all the collected data sets. Note that there is no parameter selection for KM, NMF and ONMTF, once the number of clusters is given. For the graphbased methods GNMF, DRCC, SNMTF, OSNTF and RMC, the number of the nearest neighbor is fixed to a small number 5 to ensure the locality preserving property. The regularization parameters are all searched from the grid {0.001, 0.01, 0.1, 1, 10, 100, 500, 1500}. As for the coclustering methods, the regularization parameters for the sample graph and the feature graph are set to the same. Except RMC, the other graph-based methods construct the graph Laplacian matrix using the binary weighting scheme as suggested in [6], [23].\nFor our RMC approach, the ensemble dual regularization parameter \u03b1 is in proportion to the over-fitting tolerance parameter \u03b2, which is empirically set to \u03b2 = 0.1\u03b1 [21]. To generate a group of diverse manifolds, we utilized three kinds of weighting schemes to construct the sample and\nfeature graphs, i.e., the binary weighting, the Gaussian kernel, and the cosine similarity. In particular, for the Gaussian kernel, we varied the bandwidth t in a broad range of area, i.e., t = { \u03c4100 , \u03c4 60 , \u03c4 30 , \u03c4 10 , \u03c4, 10\u03c4, 30\u03c4, 60\u03c4, 100\u03c4}. Here, \u03c4 is empirically set as the inverse of the mean square of Euclidean distances between all sample or feature pairs in the selected data [21], i.e., \u03c4 = (\n1 n2\n\u2211n i,j=1 \u2016xi\u2212xj\u2016 2 )\u22121\n. In total, eleven manifolds containing nine Gaussian graphs, one binary graph and one cosine similarity graph were employed in manifold ensemble learning for the sample and feature spaces."}, {"heading": "E. Results", "text": "The averaged results of different algorithms are tabulated in Table III and IV in terms of AC and NMI, respectively. The best results on each data set are highlighted in boldface. Besides, the asterisk symbol \u201d*\u201d aside our results indicates\nRMC is statistically and significantly better than the other well-established methods at a significance level of 0.01.\nFrom these experiments, several interesting points can be revealed below.\n\u2022 The clustering performances of RMC are systematically and consistently better than the compared algorithms, which verifies that the manifold ensemble learning is advantageous to the graph-based symmetric nonnegative matrix factorization methods for coclustering. \u2022 Except the gene expression data, RMC-E generally outperforms RMC-C on the remaining data sets. We attribute this to the fact that they respectively employ two different optimization methods to learn the manifold coefficients. Specifically, EMDA has a global efficiency estimate mildly dependent on the number of manifolds for the convex optimization over the unit simplex, and is provably very efficient for large scale\nproblems [2]. On the contrary, in each iteration of CDA, only two variables are selected to update while the others are fixed, which is a pairwise alternating optimization, leading to sub-optimal solutions. \u2022 Generally, both RMC-E and RMC-C perform more or less on the gene expression data, which might be due to the reason that their sample sizes are much smaller than those of text and image data sets, i.e., small sample size (SSS) problem. Thus, both EMDA and CDA are very likely to converge at a value around the optimal solution of the convex optimization problem, thereby achieving similar co-clustering results. \u2022 The graph regularization based methods, e.g., GNMF, DRCC, SNMTF, OSNTF and RMC, almost perform better than KM, NMF and OSNMTF. This is mainly for the reason that the graph-based approaches have considered the local geometrical structure, which is\nconducive to preserve the locality property in the lowdimensional data space. \u2022 The dual regularization based methods, e.g., DRCC and RMC, generally outperform the one-side graph regularization based method GNMF. This demonstrates that the geometric structures in both the sample and feature spaces are beneficial to further improve the clustering performance."}, {"heading": "F. Parameter Selection", "text": "There are three important parameters in our RMC method, i.e., the manifold regularization parameter \u03b1, the over-fitting tolerance parameter \u03b2 and the number of nearest neighbor p. Since the number of nearest neighbor has been fixed to 5 for all the graph-based approaches and no bias will occur during the comparison, it makes sense that here we neglect the model selection on the param-\neter p. As mentioned earlier in parameter settings, the over-fitting tolerance parameter \u03b2 was empirically set as \u03b2 = 0.1\u03b1 [21] to reduce the degree of the parameter freedom in the objective function of RMC. Hence, it is only desirable to explore the influence of the parameter \u03b1 on the clustering performance of our method. To do this, we varied the parameter \u03b1 in a broad real value range, i.e., {0.001, 0.01, 0.1, 1, 10, 100, 500, 1500}.\nFigure 1 and 2 respectively show the plots of accuracy and normalized mutual information versus the parameter \u03b1 for RMC using EMDA. Similar tendencies can be also observed for RMC using CDA. As vividly depicted in these figures, we can see that our approach enjoys the satisfactory performance when the parameter \u03b1 takes a higher value, e.g., around 500 or 1000. This indicates that the ensemble manifold regularization term should be imposed larger weights, such that it can make much more positive contributions to the objective function of the proposed approach."}, {"heading": "G. Study on Manifold Coefficients", "text": "Since manifold ensemble learning plays an essential role in the proposed RMC method, it is worthwhile to examine the manifold coefficients learned by two different optimization algorithms, i.e., EMDA and CDA. We use the histogram to draw the distribution of manifold coefficients derived from RMC under the best parameter settings. Figure 3 and 4 respectively illustrate the histogram of manifold coefficients for RMC-E and RMC-C on all data sets.\nFrom these bars, it is easy to find that CDA only choose two or three manifolds on most data sets, which suggests that there might be some important information loss, resulting that the improvement of the clustering performance seems not inspiring as shown in Table III and IV. In contrast, EMDA assigns each manifold a nonzero coefficient or weight, and the convex combination of all the candidate manifolds jointly contributes to approximate the true intrinsic manifolds of the sample and feature spaces as much as possible, thus achieving greater performance improvements. Comparing the corresponding figures in RMC-E and RMCC, we discover that the manifolds selected by RMC-C are the manifolds with the top two or three weights in RMC-E, which indicates that the two optimization methods almost select the similar manifolds as their main components. However, when it comes to RMC-C, in each iteration only two elements are selected for updating when fixing others, thereby resulting that some important manifolds like binary graph and cosine similarity graph are neglected by RMCC during the optimization process. Different from RMC-C, RMC-E updates each element in a global view and has a global efficiency estimate for the convex minimization problem on the standard simplex."}, {"heading": "V. CONCLUSIONS", "text": "This paper presents a novel co-clustering approach named Relational Multi-manifold Co-clustering (RMC),\nwhich is based on the symmetric nonnegative matrix trifactorization.\nOur approach takes into account the inter-type relation and the intra-type information of both the sample and feature data simultaneously. The basic idea is to make use of manifold ensemble learning to enhance the performance of co-clustering. To achieve this, we attempt to learn a sensible convex combination of candidate manifolds so that it can maximally approximate the true intrinsic manifolds of both the sample and feature spaces. In order to optimize the objective function, we adopt the popular alternating optimization method to update the factorized matrices. However, different from the existing matrix factorization based co-clustering methods, there exists a manifold coefficient vector to be optimized in our approach, which poses a challenging task. In this work, we utilize two optimization methods with totally different mechanisms to optimize this coefficient vector, i.e., the entropic mirror descent algorithm and the coordinate descent algorithm. The effectiveness of the proposed approach is demonstrated by a number of interesting experiments on data collections from diverse domains, i.e., text processing, image analysis and bioinformatics."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported in part by National Natural Science Foundation of China under Grants 91120302, 61222207, 60905001 and 61173186, National Basic Research Program of China (973 Program) under Grant 2013CB336500, the Fundamental Research Funds for the Central Universities under Grant 2012FZA5017 and the Zhejiang Province Key S&T Innovation Group Project under Grant 2009R50009."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Co-clustering targets on grouping the samples<lb>(e.g., documents, users) and the features (e.g., words, ratings)<lb>simultaneously. It employs the dual relation and the bilateral<lb>information between the samples and features. In many real-<lb>world applications, data usually reside on a submanifold of<lb>the ambient Euclidean space, but it is nontrivial to estimate<lb>the intrinsic manifold of the data space in a principled<lb>way. In this study, we focus on improving the co-clustering<lb>performance via manifold ensemble learning, which is able to<lb>maximally approximate the intrinsic manifolds of both the<lb>sample and feature spaces. To achieve this, we develop a<lb>novel co-clustering algorithm called Relational Multi-manifold<lb>Co-clustering (RMC) based on symmetric nonnegative matrix<lb>tri-factorization, which decomposes the relational data matrix<lb>into three submatrices. This method considers the inter-<lb>type relationship revealed by the relational data matrix,<lb>and also the intra-type information reflected by the affinity<lb>matrices encoded on the sample and feature data distributions.<lb>Specifically, we assume the intrinsic manifold of the sample<lb>or feature space lies in a convex hull of some pre-defined<lb>candidate manifolds. We want to learn a convex combination<lb>of them to maximally approach the desired intrinsic manifold.<lb>To optimize the objective function, the multiplicative rules<lb>are utilized to update the submatrices alternatively. Besides,<lb>both the entropic mirror descent algorithm and the coordinate<lb>descent algorithm are exploited to learn the manifold coeffi-<lb>cient vector. Extensive experiments on documents, images and<lb>gene expression data sets have demonstrated the superiority<lb>of the proposed algorithm compared to other well-established<lb>methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}