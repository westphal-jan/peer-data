{"id": "1611.05973", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "NCBO Ontology Recommender 2.0: An Enhanced Approach for Biomedical Ontology Recommendation", "abstract": "biomedical researchers use ontologies to annotate their data with ontology terms, enabling better data integration and interoperability. however, the changing number, variety and complexity of current biomedical ontologies make it cumbersome for researchers to determine which ones require to reuse for their specific user needs. ultimately to overcome this problem, in 2014 2010 the national center for biomedical ontology ( ncbo ) released available the ontology recommender, which designation is describes a service that receives a biomedical text corpus or a list of keywords and suggests ontologies appropriate for referencing the indicated terms. we developed a new version of the generalized ncbo ontology recommender. called ontology recommender 2. 0, it uses a new recommendation approach that evaluates the relevance of an ontology to biomedical text document data according to four criteria : ( 1 ) the extent to which the ontology covers the input data ; ( 2 ) the acceptance of the pharmaceutical ontology in the biomedical academic community ; ( 3 ) the level of detail of the ontology classes that usually cover the input data ; 4th and ( 4 ) the specialization map of the ontology to the domain of the input data. together our evaluation shows that the enhanced recommender provides higher quality clinical suggestions nowadays than the original approach, providing noticeably better coverage of the input data, more detailed source information about their recommendation concepts, reflecting increased broader specialization for the domain of integrating the input data, and even greater acceptance of and use in the community. in high addition, lastly it provides users with more explanatory information, along with specialized suggestions of not only individual printed ontologies but chapters also across groups of ontologies. it also can be customized to fit the needs of different search scenarios. ontology recommender 2. 0 combines the strengths of its predecessor formats with a range of adjustments and hopefully new features that improve its reliability and usefulness. ontology recommender 2. xl 0 recommends over some 500 biomedical ontologies from the ncbo bioportal platform, where it is openly available.", "histories": [["v1", "Fri, 18 Nov 2016 04:58:54 GMT  (5997kb)", "http://arxiv.org/abs/1611.05973v1", "29 pages, 8 figures, 11 tables"], ["v2", "Thu, 25 May 2017 21:32:40 GMT  (1393kb)", "http://arxiv.org/abs/1611.05973v2", "29 pages, 8 figures, 11 tables"]], "COMMENTS": "29 pages, 8 figures, 11 tables", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["marcos martinez-romero", "clement jonquet", "martin j o'connor", "john graybeal", "alejandro pazos", "mark a musen"], "accepted": false, "id": "1611.05973"}, "pdf": {"name": "1611.05973.pdf", "metadata": {"source": "CRF", "title": "NCBO Ontology Recommender 2.0: An Enhanced Approach for Biomedical Ontology Recommendation", "authors": ["Marcos Martinez-Romero", "Clement Jonquet", "Martin J. O'Connor", "John Graybeal", "Alejandro Pazos", "Mark A. Musen"], "emails": [], "sections": [{"heading": null, "text": "Background. Ontologies and controlled terminologies have become increasingly important in biomedical research. Researchers use ontologies to annotate their data with ontology terms, enabling better data integration and interoperability across disparate datasets. However, the number, variety and complexity of current biomedical ontologies make it cumbersome for researchers to determine which ones to reuse for their specific needs. To overcome this problem, in 2010 the National Center for Biomedical Ontology (NCBO) released the Ontology Recommender, which is a service that receives a biomedical text corpus or a list of keywords and suggests ontologies appropriate for referencing the indicated terms.\nMethods. We developed a new version of the NCBO Ontology Recommender. Called Ontology Recommender 2.0, it uses a new recommendation approach that evaluates the relevance of an ontology to biomedical text data according to four different criteria: (1) the extent to which the ontology covers the input data; (2) the acceptance of the ontology in the biomedical community; (3) the level of detail of the ontology classes that cover the input data; and (4) the specialization of the ontology to the domain of the input data.\nResults. Our evaluation shows that the enhanced recommender provides higher quality suggestions than the original approach, providing better coverage of the input data, more detailed information about their concepts, increased specialization for the domain of the input data, and greater acceptance and use in the community. In addition, it provides users with more explanatory information, along with suggestions of not only individual ontologies but also groups of ontologies to use together. It also can be customized to fit the needs of different ontology recommendation scenarios.\nConclusions. Ontology Recommender 2.0 suggests relevant ontologies for annotating biomedical text data. It combines the strengths of its predecessor with a range of adjustments and new features that improve its reliability and usefulness. Ontology Recommender 2.0 recommends over 500 biomedical ontologies from the NCBO BioPortal platform, where it is openly available (both via the user interface at http://bioportal.bioontology.org/recommender, and via a Web service API).\nKeywords Ontology selection, Ontology recommendation, Ontology evaluation, Semantic Web, Biomedical Ontologies, NCBO BioPortal."}, {"heading": "1 Background", "text": "During the last two decades, the biomedical community has grown progressively more interested in ontologies. Ontologies provide the common terminology necessary for biomedical researchers to describe their datasets, enabling better data integration and interoperability, and therefore facilitating translational discoveries [1, 2].\nBioPortal [3], developed by the National Center for Biomedical Ontology (NCBO) [4], serves as one of the primary platforms for hosting and sharing biomedical ontologies. BioPortal users can publish their ontologies as well as submit new versions. They can browse, search, review, and comment on ontologies, both interactively through a Web interface, and programmatically via Web services. In 2008, BioPortal1 contained 72 ontologies and 300.000 ontology classes. As of 2016, the number of ontologies exceeds 500, with more than 8.1 million classes, making it an indispensable resource for biomedical researchers.\nThe great number, complexity, and variety of ontologies in the biomedical field present a challenge for researchers: how to identify those ontologies that are most relevant for annotating, mining or indexing particular datasets. To address this problem, in 2010 the NCBO released the first version of its Ontology Recommender (henceforth \u2018Ontology Recommender 1.0\u2019 or \u2018original Ontology Recommender\u2019) [5], which informed the user of the most appropriate ontologies in BioPortal to annotate textual data. It was, to the best of our knowledge, the first biomedical ontology recommendation service, and it became widely known and used by the community2. However, the service has some limitations, and a significant amount of work has been done in the field of ontology recommendation since its release. This motivated us to analyze its weaknesses and to design a new recommendation approach.\nThis paper presents our new approach for biomedical ontology recommendation, which we have used to implement the NCBO Ontology Recommender 2.0 (henceforth \u2018Ontology Recommender 2.0\u2018 or \u2018new Ontology Recommender\u2018). Given biomedical text data as input, our enhanced approach makes it possible to identify the most appropriate ontologies for annotating input data. Our research is relevant both for researchers and developers who need to identify ontologies that are best suited to specific datasets."}, {"heading": "1.1 Related work", "text": "Much theoretical work has been done over the past two decades in the fields of ontology evaluation, selection, search, and recommendation. Ontology evaluation has been defined as the problem of assessing a given ontology from the point of view of a particular criterion, typically in order to determine which of several ontologies would best suit a particular purpose [6]. As a consequence, ontology recommendation is fundamentally an ontology evaluation task because it addresses the problem of evaluating and consequently selecting the most appropriate ontologies for a specific context or goal [7, 8].\nEarly contributions in the field of ontology evaluation date back to the early 1990s and were motivated by the necessity of having evaluation strategies to guide and improve the ontology engineering process [9\u201311]. Some years later, with the birth of the Semantic Web [12], the need for reusing ontologies across the Web motivated the development of the first ontology search engines [13\u201315], which made it possible to retrieve all ontologies satisfying some basic requirements (e.g., find all ontologies that contain the class gestational diabetes).\n1 http://bioportal.bioontology.org/ 2 At the time of writing this paper, there are 57 citations to the NCBO Ontology Recommender 1.0 paper [5]. The service has received 95 calls per month on average in 2013, according to BioPortal traffic logs.\nThe process of recommending ontologies involves more than traditional ontology search, however. It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.\nIn biomedicine, the great number, size, and complexity of ontologies have motivated strategies to help researchers find the best ontologies to describe their datasets. Tan and Lambrix [21] proposed a theoretical framework for selecting the best ontology for a particular text-mining application and manually applied it to a gene-normalization task. Alani et al. [22] developed an ontology-search strategy that uses query-expansion techniques to find ontologies related to a particular domain (e.g., Anatomy). Maiga and Williams [23] conceived a semi-automatic tool that makes it possible to find the ontologies that best match a list of user-defined task requirements.\nThe most relevant alternative to the NCBO Ontology Recommender is BiOSS [19, 24], which was released in 2011 by some of the authors of this paper. BiOSS evaluates each candidate ontology according to three criteria: (1) the input coverage; (2) the semantic richness of the ontology for the input; and (3) the acceptance of the ontology. However, this system has some weaknesses that make it insufficient to satisfy many ontology reuse needs in biomedicine. BiOSS\u2019 ontology repository is not updated regularly, so it does not take into account the most recent revisions to biomedical ontologies. Also, BiOSS evaluates ontology acceptance by counting the number of mentions of the ontology name in Web 2.0 resources, such as Twitter and Wikipedia. However, this method is not always appropriate because a large number of mentions do not always correspond to a high level of acceptance by the community (e.g., an ontology may be \u201cpopular\u201d on Twitter because of a high number of negative comments about it). Another drawback is that the input to BiOSS is limited to comma-delimited keywords; it is not possible to suggest ontologies to annotate raw text, which is a very common use case in biomedical informatics.\nIn this work, we have applied our previous experience in the development of the original Ontology Recommender and the BiOSS system to conceive a new approach for biomedical ontology recommendation. The new approach has been used to design and implement the Ontology Recommender 2.0. The new system combines the strengths of previous methods with a range of enhancements, including new recommendation strategies and the ability to handle new use cases. Because it is integrated within the NCBO BioPortal, this system works with a large corpus of current biomedical ontologies and can therefore be considered the most comprehensive biomedical ontology recommendation system developed to date.\nOur recommendations for the choice of appropriate ontologies centers around the use of ontologies to perform annotation of textual data. We define annotation as a correspondence or relationship between a term and an ontology class that specifies the semantics of that term. For instance, an annotation might relate leucocyte in some text to a particular ontology class leucocyte in the Cell Ontology. The annotation process will also relate textual data such as white blood cell and lymphocyte to the class leucocyte in the Cell Ontology, via synonym and subsumption relationships, respectively."}, {"heading": "1.2 Description of the original approach", "text": "The original NCBO Ontology Recommender supported two primary use cases: (1) corpus-based recommendation, and (2) keyword-based recommendation. In these scenarios, the system recommended appropriate ontologies from the BioPortal ontology repository to annotate a text corpus or a list of keywords, respectively.\nThe NCBO Ontology Recommender invoked the NCBO Annotator [25] to identify all annotations for the input data. The NCBO Annotator is a BioPortal service that annotates textual data with ontology classes. Then, the Ontology Recommender scored all BioPortal ontologies as a function of\nthe number and relevance of the annotations found, and ranked the ontologies according to those scores. The first ontology in the ranking would be the most appropriate for the input data.\nThe score for each ontology was computed according to the following formula:3\n!\"#$% !, ! = (!\"\"#$!$%#\"&'#()(!) + 2 \u2217 \u210e!\"#$#%\u210e!\"#$#% ! )!\"#10( ! ) \u2200! \u2208 !\"\"!\"#\"$!%&(!, !)\nHere o is the ontology that is being evaluated, t is the input text, annotationScore(a) is the relevance score for the annotation a; hierarchyLevel(a) is the position of the matched class in the ontology tree; |o| is the number of classes in o; and annotations(o,t) is the list of annotations (a) of t with o returned by the NCBO Annotator.\nThe annotationScore(a) would depend on whether the annotation was achieved with a class \u2018preferred name\u2019 or with a class synonym. A preferred name is the human readable label that the authors of the ontology suggested to be used when referring to the class (e.g., vertebral column), whereas synonyms are alternate names for the class (e.g., spinal column, backbone, spine). Each class in BioPortal has a single preferred name and it may have any number of synonyms. Because synonyms can be imprecise, this approach favored matches on preferred names4.\nThe normalization by ontology size was intended to discriminate between large ontologies that offer good coverage of the input data, and small ontologies with both correct coverage and better specialization for the input data\u2019s domain. The granularities of the matched classes (i.e., hierarchyLevel(a)) were also considered, so that annotations performed with granular classes (e.g., epithelial cell proliferation) would receive higher scores than those performed with more abstract classes (e.g., biological process).\nFor example, Table 1 shows the top five suggestions of the original Ontology Recommender for the text Melanoma is a malignant tumor of melanocytes which are found predominantly in skin but also in the bowel and the eye. In this example, the system considered that the best ontology for the input data is the National Cancer Institute Thesaurus (NCIT).\nTable 1. Ontologies suggested by the original Ontology Recommender for the sample input text\nMelanoma is a malignant tumor of melanocytes which are found predominantly in skin but also in the bowel and the eye. For each ontology, the table shows its position in the ranking, the acronym of the ontology in BioPortal5, the number of annotations returned by the NCBO Annotator for the sample input, the terms annotated (or \u2018covered\u2019) by those annotations and the ontology score.\nRank Ontology No. annotations Terms annotated Score 1 NCIT 21 melanoma, malignant tumor, melanocytes,\nfound, skin, bowel, eye 55.2\n2 EHDA 15 skin, eye 38.3 3 EFO 10 melanoma, malignant tumor, skin, bowel, eye 35.9 4 LOINC 18 melanoma, malignant, tumor, skin, bowel, eye 35.9 5 MP 9 melanoma, skin, bowel, eye 34.8\n3 This formula is slightly different from the scoring method presented in the paper describing the original Ontology Recommender Web service [5]. It corresponds to an upgrade done in the recommendation algorithm in December 2011, when BioPortal 3.5 was released, for which description and methodology was never published. The normalization strategy was improved by applying a logarithmic transformation to the ontology size to avoid a negative effect on very large ontologies. Mappings between ontologies, used to favor reference ontologies, were discarded due to the small number of manually created and curated mappings that could be used for such a purpose. The hierarchy-based semantic expansion was replaced by the position of the matched class in the ontology hierarchy. 4 annotationScore(a) was equal to 10 if a was achieved with a class preferred name, and to 8 if a was achieved with a class synonym. 5 See \"List of abbreviations\".\nIn the following sections, we summarize the most relevant shortcomings of the original approach, addressing input coverage, coverage of multi-word terms, input types and output information."}, {"heading": "1.2.1 Input coverage", "text": "Input coverage refers to the fraction of input data that is annotated with ontology classes. Given that the goal is to find the best ontologies to annotate the user\u2019s data, high input coverage is the main requirement for ontology-recommendation systems. One of the shortcomings of the original approach is that it did not ensure that ontologies that provide high input coverage were ranked higher than ontologies with lower coverage. The approach was strongly based on the total number of annotations returned by the NCBO Annotator. However, a large number of annotations does not always imply high coverage. Ontologies with low input coverage can contain a great many classes that match only a few input terms, or match many repeated terms in a large text corpus.\nIn the previous example (see Table 1), EHDA (Human Developmental Anatomy Ontology) was ranked at the second position. However, it covers only two input terms: skin and eye. Clearly, it is not an appropriate ontology to annotate the input when compared with LOINC or EFO, which have almost three times more terms covered. The reason that EHDA was assigned a high score is that it contains 11 different eye classes (e.g., EHDA:4732, EHDA:3808, EHDA:5701) and 4 different skin classes (e.g., EHDA:6531, EHDA:6530, EHDA:7501), which provide a total of 15 annotations. Since the recommendation score computed using the original approach is directly influenced by the number of annotations, EHDA obtains a high relevance score and thus the second position in the ranking. This issue was also identified by L\u00f3pez-Garc\u00eda et al. in their study of the efficiency of automatic summarization techniques [26]. These authors noticed that EHDA was the most recommended ontology for a broad range of topics that the ontology actually did not cover well."}, {"heading": "1.2.2 Multi-word terms", "text": "Biomedical texts frequently contain terms composed of several words, such as distinctive arrangement of microtubules, or dental disclosing preparation. Annotating a multi-word phrase or multi-word keyword with an ontological class that completely represents its semantics is a much better choice than annotating each word separately. The original recommendation approach was not designed to select the longest matches and consequently the results were affected.\nAs an example, Table 2 shows the top 5 ontologies suggested by the original Ontology Recommender for the phrase embryonic cardiac structure. Ideally, the first ontology in the ranking (SWEET) would contain the class embryonic cardiac structure. However, the SWEET ontology covers only the term structure. This ontology was ranked at the first position because it contains 3 classes matching the term structure and also because it is a small ontology (4549 classes).\nFurthermore, SNOMEDCT, which does contain a class that provides a precise representation of the input, was ranked in the 5th position. There are 3 other ontologies in BioPortal that contain the class embryonic cardiac structure: EP, BIOMODELS and FMA. However, they were ranked 8, 11 and 32, respectively. The recommendation algorithm should assign a higher score to an annotation that\ncovers all words in a multi-word term than it does to different annotations that cover all words separately."}, {"heading": "1.2.3 Input types", "text": "Related work in ontology recommendation highlights the importance of addressing two different input types: text corpora and lists of keywords [27]. The original Ontology Recommender, while offering users the possibility of selecting among these two recommendation scenarios, would treat the input data in the same manner. To satisfy users\u2019 expectations, the system should process these two input types differently, to better reflect the information coded in the input about multi-word boundaries."}, {"heading": "1.2.4 Output information", "text": "The output provided by the original Ontology Recommender consisted of a list of ontologies ranked by relevance score. For each ontology, the Web-based user interface displayed the number of classes matched and the size of each recommended ontology. In contrast, the Web service could additionally return the particular classes matched in each ontology. This information proved insufficient to assure users that a recommended ontology was appropriate and better than the alternatives. For example, it was not possible to know what specific input terms were covered by each class. The system should provide enough detail both to reassure users, and to give them information about alternative ontologies.\nIn this section we have described the fundamental limitations of the original Ontology Recommender and suggested methods for their improvement. Our goal is to design a new approach that enhances the advantages of the original one while addressing its shortcomings. The strategy for evaluating input coverage must be improved to ensure that ontologies that provide high input coverage are highly ranked. Annotations that cover all words in multi-word keywords should be prioritized over annotations that cover each word separately. The new approach must be also able to accept both plain text and keywords as input, but it should process the input data differently to better satisfy the users\u2019 expectations. Additionally, there is a diversity of other recently-proposed evaluation techniques [7, 17, 24] that could enhance our strategy. Particularly, there are two evaluation criteria that could substantially improve the output provided by the system: (1) ontology acceptance, which represents the degree of acceptance of the ontology by the community; and (2) ontology detail, which refers to the level of detail of the classes that cover the input data."}, {"heading": "2 Description of the new approach", "text": "In this section, we present our new approach to biomedical ontology recommendation. First, we describe our ontology evaluation criteria and explain how the recommendation process works. We then provide some implementation details and discuss improvements to the user interface.\nThe execution starts from the input data and a set of configuration settings. The NCBO Annotator [25] is then used to obtain all annotations for the input using BioPortal ontologies. Those ontologies that do not provide annotations for the input data are considered irrelevant and are ignored in further processing. The ontologies that provide annotations are evaluated one by one according to four evaluation criteria that address the following questions:\n1. Coverage: To what extent does the ontology represent the input data? 2. Acceptance: How well-known and trusted is the ontology by the biomedical community? 3. Detail: How rich is the ontology representation for the input data? 4. Specialization: How specialized is the ontology to the domain of the input data?\nAccording to our analysis of related work, these are the most relevant criteria for ontology recommendation. Note that other authors have referred to the coverage criterion as term matching\n[5], class match measure [17] and topic coverage [27]. Acceptance is related to criteria such as popularity [19, 24, 27], connectivity [5] and connectedness [16]. Detail is similar to structure measure [5], semantic richness [19, 24] , structure [16], and granularity [23].\nFor each of these evaluation criteria, a score in the interval [0,1] is typically obtained. Then, all the scores for a given ontology are aggregated into a composite relevance score, also in the interval [0,1]. This score represents the appropriateness of that ontology to describe the input data. The individual scores are combined in accordance with the following expression:\n!\"#$% !, ! = !! \u2217 !\"#$%&'$ !, ! + !! \u2217 !\"\"#$%!&\"# ! + !! \u2217 !\"#$%& !, ! + !! \u2217 !\"#$%&'%(&)%*+ !, !\nWhere o is the ontology that is being evaluated, t represents the input data, and {wc, wa, wd, ws} are a set of predefined weights that are used to give more or less importance to each evaluation criterion, such that wc + wa + wd + ws = 1. Note that acceptance is the only criterion independent from the input data. Ultimately, the system returns a list of ontologies ranked according to their relevance scores."}, {"heading": "2.1 Ontology evaluation criteria", "text": "The relevance score of each candidate ontology is calculated based on coverage, acceptance, detail, and specialization. We now describe these criteria in more detail."}, {"heading": "2.1.1 Ontology coverage", "text": "It is crucial that ontology recommendation systems suggest ontologies that provide high coverage of the input data. As with the original approach, the new recommendation process is driven by the annotations provided by the NCBO Annotator, but the method used to evaluate the candidate ontologies is different. In the new algorithm, each annotation is assigned a score computed in accordance with the following expression:6\n!\"\"#$!$%#\"&'#()2 ! = !\"\"#$!$%#\"&'()*+#,)(!) +!\"#$%&'()*+'(,(! ) \u2217 !!!\"#$#%&'\"!\"#(!) with:\n!\"\"#$!$%#\"&'()*+#,) ! = 10 !\" !\"\"#$!$%#\"&'() = !\"#$5 !\" !\"\"#$!$%#\"&'() = !\"#\n!\"#$%&'()*+'(, ! = 3 !\" !\"\"#$!$%&'#(&) ! > 10 !\"\u210e!\"#$%!\nIn this expression, annotationTypeScore(a) is a score based on the annotation type, which can be either \u2018PREF\u2019, if the annotation has been performed with a class preferred name, or \u2018SYN\u2019, if it has been performed with a class synonym. Our method assigns higher relevance to scores done with class preferred names than to those made with class synonyms because we have seen that many BioPortal ontologies contain synonyms that are not reliable (e.g., Other variants as a synonym of Other Variants of Basaloid Follicular Neoplasm of the Mouse Skin in the NCI Thesaurus).\nThe multiWordScore(a) score rewards multi-word annotations. It gives more importance to classes that annotate multi-word terms than to classes that annotate individual words separately (e.g., blood cell versus blood and cell). Such classes better reflect the input data than do classes that represent isolated words.\nThe annotatedWords(a) function represents the number of words matched by the annotation (e.g., 2 for the term blood cell).\nSometimes, an ontology provides overlapping annotations for the same input data. For instance, the text white blood cell may be covered by two different classes, white blood cell and blood cell. In the\n6 The function is called annotationScore2 to differentiate it from the original annotationScore function.\noriginal approach, ontologies with low input coverage were sometimes ranked among the top positions because they had multiple classes matching a few input terms, and all those annotations contributed to the final score. Our new approach addresses this issue. If an ontology provides several annotations for the same text fragment, only the annotation with the highest score is selected to contribute to the coverage score.\nThe coverage score for each ontology is computed as the sum of all the annotation scores, as follows:\n!\"#$%&'$ !, ! = !\"#$ !\"\"#$!$%#\"&'#()2(!) \u2200! \u2208 !\"#\"$%\"&'(()%*%+)(!(!)\nwhere A is the set of annotations performed with the ontology o for the input t, selectedAnnotations(A) is the set of annotations that are left after discarding overlapping annotations, and norm is a function that normalizes the coverage score to the interval [0,1].\nAs an example, Table 3 shows the annotations performed with SNOMEDCT for the input A thrombocyte is a kind of blood cell. In this example, the coverage score for SNOMEDCT would be calculated as 5+26=31, which would be normalized to the interval [0,1] by dividing it by the maximum coverage score. The maximum coverage score is obtained by adding the scores of all the annotations performed with all BioPortal ontologies, after discarding overlapping annotations.\nIt is important to note that this evaluation of ontology coverage takes into account term frequency. That is, matched terms with several occurrences are considered more relevant to the input data than terms that occur less frequently. If an ontology covers a term that appears several times in the input, its corresponding annotation score will be counted each time and the coverage score for the ontology accordingly will be higher. In addition, because we select only the matches with the highest score, the frequencies are not distorted by terms embedded in one another (e.g., white blood cell and blood cell).\nOur approach accepts two input types: free text and comma-delimited keywords. For the keyword input type, only those annotations that cover all the words in a multi-word term are considered. Partial annotations are immediately discarded."}, {"heading": "2.1.2 Ontology acceptance", "text": "In biomedicine, some ontologies have been developed and maintained by widely known institutions or research projects. The content of these ontologies is periodically curated are extensively used and accepted by the community. Examples of broadly accepted ontologies are SNOMEDCT [28] and Gene Ontology [29]. Some ontologies uploaded to BioPortal may be relatively less reliable, however. They may contain incorrect or poor quality content or simply be insufficiently up to date. It is important that an ontology recommender be able to distinguish between ontologies that are accepted as trustworthy and those that are less so.\nOur approach estimates the degree of acceptance of each BioPortal ontology based on two factors:\n1. The number of visits to the ontology in BioPortal in a recent period of time (e.g., the last 6 months). Ontologies that receive more visits (pageviews) in BioPortal are presumed to be more relevant to the biomedical community than ontologies that receive a lower number of visits. This method takes into account changes in ontology popularity over time. 2. The presence of the ontology in the Unified Medical Language System (UMLS) [30]. UMLS is a collection of more than 100 biomedical ontologies and controlled terminologies, developed and maintained by the National Library of Medicine (NLM). The ontologies that are included into UMLS are generally widely-accepted, and subjected to revision by the NLM. The current version of BioPortal contains 527 ontologies, 31 of which (5.9%) belong to UMLS.\nThe acceptance score for each ontology is calculated according to the following expression:\n!\"\"#$%!&\"# ! = !!\" \u2217 !\"#$%&$'()*+,$ ! + !!\"#$ \u2217 !\"#$%&'() ! where pageviewsScore(o) represents the number of visits to the ontology in BioPortal normalized to the interval [0,1]; umlsScore(o) is 1 if the ontology is included into UMLS and 0 if it is not. wpv and wumls are weights that are used to give more or less importance to each factor, with wpv + wumls = 1.\nFigure 1 shows the top 20 accepted BioPortal ontologies according to our approach at the time of writing this paper. Estimating the acceptance of an ontology by the community is inherently subjective, but the above ranking shows that our approach provides reasonable results. All ontologies in the ranking are widely known and accepted biomedical ontologies that are used in a variety of projects and applications."}, {"heading": "2.1.3 Ontology detail", "text": "Ontologies containing a richer representation for a specific input are potentially more useful to describe the input than less detailed ontologies. As an example, the class melanoma in the Human Disease Ontology contains a definition, two synonyms, and twelve properties. However, the class melanoma from the GALEN ontology does not contain any definition, synonyms, or properties. If a user needs an ontology to represent that concept, the Human Disease Ontology would probably be more useful than the GALEN ontology because of this additional information. An ontology\nrecommender should be able to analyze the level of detail of the classes that cover the input data and to give more or less weight to the ontology according to the degree to which its classes have been specified.\nWe evaluate the richness of the ontology representation for the input data based on a simplification of the \u201csemantic richness\u201d metric used by BiOSS [24]. For each annotation selected during the coverage evaluation step, we calculate the detail score as follows:\n!\"#$%&'()!\" ! = !\"#$%$&$'%()'*\" ! + !\"#$#\"%!&'$() ! + !\"#!$\"%&$'()#\"$(!)3\nwhere detailScore(a) is a value in the interval [0,1] that represents the level of detail provided by the annotation a. This score is based on three functions that evaluate the detail of the knowledge representation according to the number of definitions, synonyms, and other properties of the matched class:\n!\"#$%$&$'%()'*\" ! = 1 !\" |!| \u2265 !! |!|/!! !\"\u210e!!\"#$%\n!\"#$#\"%!&'$() ! = 1 !\" |!| \u2265 !! |!|/!! !\"\u210e!\"#$%! !\"#!$\"%&$'()#\"$ ! = 1 !\" |!| \u2265 !! |!|/!! !\"\u210e!\"#$%!\nwhere |D|, |S| and |P| are the number of definitions, synonyms, and other properties of the matched class, and kd, ks and kp are predefined constants that represent the number of definitions, synonyms, and other properties, respectively, necessary to get the maximum detail score. For example, using ks=4 means that, if the class has 4 or more synonyms, then it will be assigned the maximum synonyms score, which would be 1. If it has fewer than 4 synonyms, for example 3, the synonyms score will be computed proportionally according to the expression above (i.e., 3/4). Finally, the detail for the ontology would be calculated as the sum of the detail scores of the annotations done with the ontology, normalized to [0,1]:\n!\"#$%& !, ! = !\"#$%&'()*\"(!)! \u2200! \u2208 !\"#\"$%\"&'(()%*%+)(!(!)\nExample: Suppose that, for the input t = Penicillin is an antibiotic used to treat tonsillitis, there are two ontologies O1 and O2 with the classes shown in Table 4.\nAssuming that kd = 1, ks = 4 and kp = 10, the detail score for O1 and O2 would be calculated as follows:\n!\"#$%& !1, ! = !!! !!! !\" ! + !!!!!\n! 2 = 0.87\n!\"#$%& !2, ! = !!! !!! !\" ! + !!!!! !\"\n! 2 = 0.13\nGiven that O1 annotates the input with two classes that provide more detailed information than the classes from O2, the detail score for O1 is higher."}, {"heading": "2.1.4 Ontology specialization", "text": "Some biomedical ontologies aim to represent detailed information about specific subdomains or particular tasks. Examples include the Ontology for Biomedical Investigations [31], the Human Disease Ontology [32] and the Biomedical Resource Ontology [33]. These ontologies are usually much smaller than more general ones, with only several hundred or a few thousand classes, but they provide comprehensive knowledge for their fields.\nTo evaluate ontology specialization, an ontology recommender needs to quantify the extent to which a candidate ontology fits the specialized nature of the input data. To do that, we reused the evaluation approach applied by the original Ontology Recommender\u2014which was designed to identify small, specialized ontologies\u2014and adapted it to the new annotation scoring strategy. The specialization score for each candidate ontology is calculated according to the following expression:\n!\"#$%&'%(&)%*+ !, ! = !\"#$ (!\"\"#$!$%#\"&'#()2(!) + 2 \u2217 \u210e!\"#$#%\u210e!\"#$#% ! )!\"#!\"( ! ) \u2200! \u2208 !\nwhere o is the ontology being evaluated, t is the input text, annotationScore2(a) is the function that calculates the relevance score of an annotation (see Section 2.1.1), hierarchyLevel(a) returns the level of the matched class in the ontology hierarchy, and A is the set of all the annotations done with the ontology o for the input t. Unlike the coverage and detail criteria, which consider only selectedAnnotations(A), the specialization criterion takes into account all the annotations returned by the Annotator (i.e., A). This is appropriate because an ontology that provides multiple annotations for a specific text fragment is likely to be more specialized for that text than an ontology that provides only one annotation for it. The normalization by ontology size aims to assign a higher score to smaller, more specialized ontologies. Applying a logarithmic function decreases the impact of ontologies with a very large size. Finally, the norm function normalizes the score to the interval [0,1].\nUsing the same hypothetical ontologies, input, and annotations from the previous example, and taking into account the size and annotation details shown in Table 5, the specialization score for O1 and O2 would be calculated as follows:\n!\"#$%&'%(&)%*+ !1, ! = !\"#$ 10 + 2 \u2217 5 + (5 + 2 \u2217 3)!\"#!\"(120000) = !\"#$ 315.08 = !\"#$(6.10)\n!\"#$%&'%(&)%*+ !2, ! = !\"#$ 5 + 2 \u2217 6 + (10 + 2 \u2217 12)!\"#!\"(800) = !\"#$ 512.90 = !\"#$(17.59)\nIt is possible to see that the classes from O2 are located deeper in the hierarchy than are those from O1. Also, O2 is a much smaller ontology than O1. As a consequence, according to our ontologyspecialization method, O2 would be considered more specialized for the input than O1, and would be assigned a higher specialization score."}, {"heading": "2.1.5 Summary of the approach", "text": "In this section, we presented the four evaluation criteria used in our approach to recommending ontologies: ontology coverage, ontology acceptance, ontology detail, and ontology specialization. We explained how to calculate the evaluation score corresponding to each criterion and described the method used to aggregate the resulting four evaluation scores into a composite relevancy score for each ontology. The composite score represents the appropriateness of that ontology to describe the input data and is used to generate the ranking of recommended ontologies that is returned to the user."}, {"heading": "2.2 Evaluation of ontology sets", "text": "When annotating a biomedical text corpus or a list of biomedical keywords, it is often difficult to identify a single ontology that covers all terms. In practice, it is more likely that several ontologies will jointly cover the input [7]. Suppose that a researcher needs to find the best ontologies for a list of biomedical terms. If there is not a single ontology that provides an acceptable coverage it should then evaluate different combinations of ontologies and return a ranked list of ontology sets that, together, provide higher coverage. This is a multi-criteria optimization problem in which an ontology recommender should attempt to find the minimum set of ontologies that provides the maximum coverage. For instance, in our previous example (Penicillin is an antibiotic used to treat tonsillitis), O1 covers the terms penicillin and antibiotic and O2 covers penicillin and tonsillitis. None of those ontologies provides full coverage of all the relevant input terms. However, by using O1 and O2 together, it is possible to cover penicillin, antibiotic, and tonsillitis.\nOur method to evaluate ontology sets is based on the \u201contology combinations\u201d approach used by the BiOSS system [19]. The system generates all possible sets of 2 and 3 candidate ontologies (3 being the default maximum, though users may modify this limit according to their specific needs) and it evaluates them using the criteria presented previously. To improve performance, we use some heuristic optimizations to discard certain ontology sets without performing the full evaluation process for them. For example, a set containing two ontologies that cover exactly the same terms will be immediately discarded because that set\u2019s coverage will not be higher than that provided by each ontology individually.\nThe relevance score for each set of ontologies is calculated using the same approach as for single ontologies, in accordance with the following expression:\n!\"#$%!\"# !, ! = !! \u2217 !\"#$%&'$($) !, ! + !! \u2217 !\"\"#$%!&\"#'#% ! + !! \u2217 !\"#$%&'\"# !, ! + !! \u2217 !\"#$%&'%(&)%*+,#) !, !\nwhere O = {o | o is an ontology} and |O| > 1. The scores for the different evaluation criteria are calculated as follows:\n\u2022 coverageSet: It is computed the same way as for a single ontology, but takes into account all the annotations performed with all the ontologies in the ontology set. The system selects the best annotations, and the set\u2019s input coverage is computed based on them. \u2022 acceptanceSet, detailSet, and specializationSet: For each ontology, the system calculates its coverage contribution (as a percentage) to the set\u2019s coverage score. The recommender then uses this contribution to calculate all the other scores proportionally. By using this method, the impact (in terms of acceptance, detail and specialization) of a particular ontology on the set score will vary according to the coverage provided by such ontology."}, {"heading": "2.3 Implementation details", "text": "Figure 2 shows the architecture and information flow of Ontology Recommender 2.0. Like its predecessor, it has two interfaces: a REST Web service interface, which makes it possible to invoke\nthe recommender programmatically, and a Web-based user interface included in the NCBO BioPortal.\nThe backend has been developed using Ruby and interacts with the BioPortal infrastructure to obtain all the information required to provide a recommendation. This information includes all annotations for the input data (generated by the Annotator service) and details about the candidate ontologies and their classes, such as the number of classes in each ontology, the number of properties for each class, and the number of user visits to each ontology in a period of time. The system has four independent evaluation modules that assess each candidate ontology according to four criteria: coverage, acceptance, detail and specialization. Because of the system's modular design, new ontology evaluation components can be plugged in easily. Ontology Recommender 2.0 works with all the ontologies available in the BioPortal ontology repository. The source code is available in GitHub7 under a BSD License.\n7 https://github.com/ncbo/ncbo_ontology_recommender"}, {"heading": "2.3.1 User Interface", "text": "Figure 3 shows the Ontology Recommender 2.0 user interface. The system supports two input types: plain text and comma-separated keywords. It also provides two kinds of output: ranked ontologies and ranked ontology sets. As previously explained, the ranked ontology sets can be useful for researchers who seek maximum coverage of the input data, even if several ontologies have to be used. The advanced options section, which is initially hidden, allows the user to customize (1) the weights applied to the evaluation criteria, (2) the maximum number of ontologies in each set (when using the ontology sets output), and (3) the list of candidate ontologies to be evaluated.\nFigure 4 shows an example of the system's output when selecting \u201ckeywords\u201d as input and \u201contologies\u201d as output. For each ontology in the output, the user interface shows its final score, the scores for the four evaluation criteria used, and the number of annotations performed with the ontology on the input. For instance, the most highly recommended ontology in Figure 4 is the Symptom Ontology (SYMP), which covers 17 of the 21 input keywords. The column \u201chighlight annotations\u201d allows the user to select any of the suggested ontologies and see which specific input terms are covered. Also, clicking on a particular term in the input reveals the details of the matched class in BioPortal. All scores are translated from the interval [0, 1] to [0, 100] for better readability.\nFigure 5 shows the \u201cOntology sets\u201d output for the same keywords displayed in Figure 4. In this case, the system looks for the minimum number of ontologies that provide the highest input coverage. The output shows that using three ontologies (SYMP, SNOMEDCT and MEDDRA) it is possible to cover all the input keywords. Different colors for the input terms and for the recommended ontologies in Figure 5 distinguish the specific terms covered by each ontology in the selected set."}, {"heading": "3 Evaluation", "text": "To evaluate our approach, we compared the performance of Ontology Recommender 2.0 to Ontology Recommender 1.0 using data from a variety of well-known public biomedical databases. Examples of these databases are PubMed, which contains bibliographic information for the fields of biomedicine and health; the Gene Expression Omnibus (GEO), which is a repository of gene expression data; and ClinicalTrials.gov, which is a registry of clinical trials. We used the API provided by the NCBO Resource Index8 [34] to programmatically extract data from those databases."}, {"heading": "3.1 Experiment 1: Input Coverage", "text": "We selected 12 widely known biomedical databases and extracted 600 biomedical texts from them, with 127 words on average, and 600 lists of biomedical keywords, with 17 keywords on average, producing a total of 1200 inputs (100 inputs per database). The databases used are listed in Table 6.\nGiven the importance of input coverage, we first executed both systems for all inputs and compared the coverage provided by the top-ranked ontology. We focused on the top-ranked ontology because the majority of users always select the first result obtained [35]. The strategy we used to calculate the ontology coverage differed depending on the input type:\n\u2022 For texts, the coverage was computed as the percentage of input words covered by the ontology with respect to the total number of words that could be covered using all BioPortal ontologies together.\n\u2022 For keywords, the coverage was computed as the percentage of keywords covered by the ontology divided by the total number of keywords.\nFigure 6 and Figure 7 show a representation of the coverage provided by both systems for each database and input type. Table 7 and Table 8 provide a summary of the evaluation results.\nFor some inputs, the first ontology suggested by Ontology Recommender 1.0 provides very low coverage (under 20%). This results from one of the shortcomings previously described: Ontology Recommender 1.0 occasionally assigns a high score to ontologies that provide low coverage because they contain several classes matching the input. The new recommendation approach used by Ontology Recommender 2.0 addresses this problem: Virtually none of its executions provide such low coverage.\nFor example, Table 9 shows the ontologies recommended if we input the following description of a disease, extracted from the Integrated Disease View (IDV) database: Chronic fatigue syndrome refers to severe, continued tiredness that is not relieved by rest and is not directly caused by other medical conditions. See also: Fatigue. The exact cause of chronic fatigue syndrome (CFS) is unknown. The following may also play a role in the development of CFS: CFS most commonly occurs in women ages 30 to 50.\nOntology Recommender 1.0 suggests the Bone Dysplasia Ontology (BDO), whereas Ontology Recommender 2.0 suggests the NCI Thesaurus (NCIT). Because BDO covers only 4 of the input\nterms, while NCIT covers 17, the recommendation provided by Ontology Recommender 2.0 is more appropriate than that of its predecessor.\nOntology Recommender 2.0 also provides better mean coverage for both input types (i.e., text and keywords) across all the biomedical databases included in the evaluation. Compared to Ontology Recommender 1.0, the mean coverage reached using Ontology Recommender 2.0 was 14.9% higher for texts and 19.3% higher for keywords. That increase was even greater using the \u201contology sets\u201d output type provided by Ontology Recommender 2.0, which reached a mean coverage of 92.1% for texts (31.3% higher than the Ontology Recommender 1.0 ratings) and 89.8% for keywords (26.9% higher)."}, {"heading": "3.2 Experiment 2: Refining Recommendations", "text": "Our second experiment set out to examine whether Ontology Recommender 2.0 is effective at discerning how to make meaningful recommendations when ontologies exhibit similar coverage of the input text. Specifically, we were interested in analyzing how the new version uses ontology acceptance, detail and specialization to prioritize the most appropriate ontologies.\nWe started with the 1200 inputs (600 texts and 600 lists of keywords) from the previous experiment, and selected those inputs for which the two versions of Ontology Recommender suggested different ontologies with similar coverage. We considered two coverage values similar if the difference between them was less than 10%. This yielded a total of 284 inputs (32 input texts and 252 lists of keywords). We executed both systems for those 284 inputs and analyzed the ontologies obtained in terms of their acceptance, detail and specialization scores.\nFigure 8 and Table 10 show the results obtained. The ontologies suggested by Ontology Recommender 2.0 have higher acceptance (87.1) and detail scores (72.1) than those suggested by Ontology Recommender 1.0. Importantly, the graphs show peaks of low acceptance (<30%) and detail (<20%) for Ontology Recommender 1.0 that are addressed by Ontology Recommender 2.0.\nThe ontologies suggested by Ontology Recommender 2.0 have, on average, lower specialization scores (65.1) than those suggested by Ontology Recommender 1.0 (95.1). This is an expected result, given that the recommendation approach used by Ontology Recommender 1.0 is based on the relation between the number of annotations provided by each ontology and its size, which is our measure for ontology specialization.\nOntology Recommender 1.0 is better than Ontology Recommender 2.0 at finding small ontologies that provide multiple annotations for the user\u2019s input. However, those ontologies are not necessarily the most appropriate to describe the input data. As we have seen (see Section 1.2.1), a large number of annotations does not always indicate a high input coverage. Ontology Recommender 1.0 sometimes suggests ontologies with high specialization scores but with very low input coverage, which makes the ontologies inappropriate for the user\u2019s input. The multi-criteria evaluation\napproach used by Ontology Recommender 2.0 has been designed to address this issue by evaluating ontology specialization in combination with other criteria, including ontology coverage."}, {"heading": "3.3 Experiment 3: High Coverage and Specialized Ontologies", "text": "We set out to evaluate how well Ontology Recommender 2.0 prioritizes recommending small ontologies that provide appropriate coverage for the input data. We created 15 inputs, each of which contained keywords from a very specific domain (e.g., adverse reactions, dermatology, units of measurement), and executed both versions of the Ontology Recommender for those inputs.\nTable 11 shows the particular domain for each of the 15 inputs used, and the first ontology suggested by each version of Ontology Recommender, as well as the size of each ontology and the coverage provided.\nAnalysis of the results reveals that Ontology Recommender 2.0 is more effective than Ontology Recommender 1.0 for suggesting specialized ontologies that provide high input coverage. In 9 out of 15 inputs (60%), the first ontology suggested by Ontology Recommender 2.0 is more appropriate, in terms of its size and coverage provided, than the ontology recommended by Ontology Recommender 1.0. Ontology Recommender 2.0 considers input coverage in addition to ontology specialization, which Ontology Recommender 1.0 does not. In addition, Ontology Recommender 2.0 uses a different annotation scoring method (the function annotationScore2(a); see Section 2.1.1) that gives more weight to annotations that cover multi-word terms. There is one input (no. 13), for which the ontology suggested by Ontology Recommender 2.0 provides higher coverage (88% versus 80%), but it is bigger than the ontology recommended by Ontology Recommender 1.0 (324K classes versus 119K). In 5 out of 15 inputs (33%), both systems recommended the same ontology."}, {"heading": "4 Discussion", "text": "Recommending biomedical ontologies is a challenging task. The great number, size, and complexity of biomedical ontologies, as well as the diversity of user requirements and expectations, make it difficult to identify the most appropriate ontologies to annotate biomedical data. The analysis of the results demonstrates that ontologies suggested using our new recommendation approach are more appropriate than those recommended using the original method. Our acceptance evaluation method has proved to be successful, and it is currently used not only by the Ontology Recommender, but also by the BioPortal search engine. The classes returned when searching in BioPortal are ordered according to the general acceptance of the ontologies to which they belong.\nWe note that, because the system is designed in a modular way, it will be easy to add new evaluation criteria to extend its functionality. As a first priority, we intend to improve and extend the evaluation criteria currently used. In addition, we will investigate the effect of extending the Ontology Recommender to include relevant features not yet considered, such as the frequency of an ontology\u2019s updates, its levels of abstraction, formality, granularity, and the language in which the ontology is expressed.\nIndeed, using metadata information is a simple by often ignored approach to select ontologies. Coverage-based approaches often miss relevant results because they focus on the content of ontologies and ignore more general information about the ontology. For example, applying the new Ontology Recommender to the Wikipedia definition of anatomy9 will return some widely-known ontologies that contain the terms anatomy, structure, organism and biology, but the Foundational Model of Anatomy (FMA), which is the reference ontology about human anatomy will not show up in the top 25 results. To address this issue, we are currently refining, in collaboration with the AgroPortal ontology repository [36], the way BioPortal handles metadata for ontologies in order to support, in the future, even more ontology recommendation scenarios.\nOur coverage evaluation approach may be further enhanced by complementing our annotation scoring method (i.e., annotationScore2) with term extraction techniques. We plan to analyze the application of a term extraction measure, called C-value [37], which is specialized for multi-word term extraction, and that has already been applied to the results of the NCBO Annotator, leading to significant improvements [38].\nThere are some possible avenues for enhancing our assessment of ontology acceptance. These include considering the number of projects that use a specific ontology, the number of mappings created manually that point to a particular ontology, the number of user contributions (e.g., mappings, notes, comments), the metadata available per ontology, and the number, publication date and publication frequency of ontology versions. There are other indicators external to BioPortal that could be useful for performing a more comprehensive evaluation of ontology acceptance, such as the number of Google results when searching for the ontology name or the number of PubMed publications that contain the ontology name [19].\nThe current version of Ontology Recommender uses a set of default parameters to control how the different evaluation scores are calculated, weighted and aggregated. These parameters provide acceptable results for general ontology recommendation scenarios, but some users may need to modify the default settings to match their needs. In the future, we would like the system to use an automatic weight adjustment approach. We will investigate whether it is possible to develop methods of adjusting the weights dynamically for specific scenarios.\nOntology Recommender helps to identify all the ontologies that would be suitable for semantic annotation. However, given the number of ontologies in BioPortal, it would be difficult, computationally expensive, and often useless to annotate user inputs with all the ontologies in the repository. Ontology Recommender could function within BioPortal as a means to screen 9 https://en.wikipedia.org/wiki/Anatomy\nontologies for use with the NCBO Annotator. A user might be offered the possibility to \u201cRun the Ontology Recommender first\u201d before actually calling the Annotator. Then only the top-ranked ontologies would be used for annotations.\nA user-based evaluation would help us understand the system\u2019s utility in real-world settings. Our experience evaluating the original Ontology Recommender and BiOSS showed us that obtaining a user-based evaluation of an ontology recommender system is a challenging task. For example, the evaluators of BiOSS reported that they would need at least 50 minutes to perform a high-quality evaluation of the system for each test case. We plan to investigate whether crowd-sourcing methods, as an alternative, can be useful to evaluate ontology recommendation systems from a usercentered perspective.\nOur approach for ontology recommendation was designed for the biomedical field, but it can be adapted to work with ontologies from other domains so long as they have a resource equivalent to the NCBO Annotator, an API to obtain basic information about all the candidate ontologies, and their classes, and alternative resources for extracting information about the acceptance of each ontology. For example, AgroPortal [36] is an ontology repository based on NCBO BioPortal technology. AgroPortal uses Ontology Recommender 2.0 in the context of plant, agronomic and environmental sciences.10"}, {"heading": "5 Conclusions", "text": "Biomedical ontologies are crucial for representing knowledge and annotating data. However, the large number, complexity, and variety of biomedical ontologies make it difficult for researchers to select the most appropriate ontologies for annotating their data. In this paper, we presented a novel approach for recommending biomedical ontologies. This approach has been implemented as release 2.0 of the NCBO Ontology Recommender, a system that is able to find the best ontologies for a biomedical text or set of keywords. Ontology Recommender 2.0 combines the strengths of its predecessor with a range of adjustments and new features that improve its reliability and usefulness.\nOur evaluation shows that, on average, the new system is able to suggest ontologies that provide better input coverage, contain more detailed information, are more specialized, and are more widely accepted than those suggested by the original Ontology Recommender. In addition, the new version is able to evaluate not only individual ontologies, but also different ontology sets, in order to maximize input coverage. The new system can be customized to specific user needs and it provides more explanatory output information than its predecessor, helping users to understand the results returned. The new service, embedded into the NCBO BioPortal, will be a more valuable resource to the community of researchers, scientists, and developers working with ontologies.\nList of abbreviations\nBioPortal ontology acronyms:\nBIOMODELS BioModels Ontology (BIOMODELS) CPT Current Procedural Terminology EFO Experimental Factor Ontology EHDA Human Developmental Anatomy Ontology, timed version EP Cardiac Electrophysiology Ontology FMA Foundational Model of Anatomy HUPSON Human Physiology Simulation Ontology\n10 http://agroportal.lirmm.fr/recommender\nLOINC Logical Observation Identifier Names and Codes MEDDRA Medical Dictionary for Regulatory Activities MP Mammalian Phenotype Ontology NCIT National Cancer Institute Thesaurus NDDF National Drug Data File NDFRT National Drug File - Reference Terminology RXNORM RxNORM SNOMEDCT Systematized Nomenclature of Medicine - Clinical Terms SWEET Semantic Web for Earth and Environment Technology Ontology VSO Vital Sign Ontology MESH Medical Subject Headings ICD9CM International Classification of Diseases, Version 9 - Clinical Modification RCD Read Codes, Clinical Terms Version 3 OMIM Online Mendelian Inheritance in Man VANDF Veterans Health Administration National Drug File GO Gene Ontology CRISP Computer Retrieval of Information on Scientific Projects Thesaurus ICPC International Classification of Primary Care MEDLINEPLUS MedlinePlus Health Topics COSTART Coding Symbols for a Thesaurus of Adverse Reaction Terms PDQ Physician Data Query SYMP Symptom Ontology\nDeclarations\nEthics approval and consent to participate Not applicable.\nConsent for publication Not applicable.\nAvailability of data and materials \u2022 Project name: The Biomedical Ontology Recommender \u2022 Project home page: http://bioportal.bioontology.org/recommender \u2022 Project GitHub repository: https://github.com/ncbo/ncbo_ontology_recommender \u2022 REST service parameters: http://data.bioontology.org/documentation#nav_recommender \u2022 Operating system(s): Platform independent \u2022 Programming language: Ruby, Javascript, HTML \u2022 Other requirements: none \u2022 License: BSD (http://www.bioontology.org/BSD-license). \u2022 The datasets used in our evaluation are available at GitHub: https://git.io/vXIOC.\nCompeting interests The authors declare no competing interests.\nFunding This work was supported in part by the National Center for Biomedical Ontology as one of the National Centers for Biomedical Computing, supported by the NHGRI, the NHLBI, and the NIH Common Fund under grant U54 HG004028 from the U.S. National Institutes of Health. Additional support was provided by the \u201cXunta de Galicia\u201d (grants R2014/039 and GRC2014/049), the Carlos III Health Institute (grant PI13/00280), and European Regional Development Funds (FEDER). This project has also received support from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 701771 and the French National Research Agency (grant ANR-12-JS02-01001).\nAuthor\u2019s contributions MMR conceived the approach, designed and implemented the system, and drafted the initial manuscript. CJ participated in technical discussions and provided ideas to refine the approach. MAM supervised the work and gave advice and feedback at all stages. CJ, MJO, JG, and AP provided critical revision and edited the manuscript. All authors gave the final approval of the manuscript."}, {"heading": "Acknowledgments", "text": "We acknowledge the suggestions about the problem of recommending ontologies provided by the NCBO team, as well as their assistance and advice on integrating Ontology Recommender 2.0 into BioPortal. Natasha Noy and Vanessa Aguiar offered valuable feedback.\nEndnotes"}], "references": [{"title": "Bio-ontologies-fast and furious", "author": ["J Blake"], "venue": "Nat Biotechnol", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Bio-ontologies: Current trends and future directions", "author": ["O Bodenreider", "R Stevens"], "venue": "Briefings in Bioinformatics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "BioPortal: A Web repository for biomedical ontologies and data", "author": ["NF Noy", "M Dorf", "MJ Montegut", "NH Shah", "N Griffith", "DL Rubin", "a. Musen M", "B Dai", "C Jonquet", "C Youn"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "National Center for Biomedical Ontology: advancing biomedicine through structured organization of scientific knowledge", "author": ["DL Rubin", "SE Lewis", "CJ Mungall", "S Misra", "M Westerfield", "M Ashburner", "I Sim", "CG Chute", "H Solbrig", "M-A Storey", "B Smith", "J Day-Richter", "NF Noy", "MA Musen"], "venue": "OMICS", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Building a biomedical ontology recommender web service", "author": ["C Jonquet", "MA Musen", "NH Shah"], "venue": "J Biomed Semantics 2010,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A survey of ontology evaluation techniques. In Proceedings of the conference on data mining and data warehouses (SiKDD", "author": ["J Brank", "M Grobelnik", "D Mladeni\u0107"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Ontology selection for the real Semantic Web: How to cover the Queen\u2019s birthday dinner", "author": ["M Sabou", "V Lopez", "E Motta"], "venue": "In Proceedings of International Conference on Knowledge Engineering and Knowledge Management. Berlin,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Improving Ontology Recommendation and Reuse in WebCORE by Collaborative Assessments", "author": ["I Cantador", "M Fern\u00e1ndez", "P Castells"], "venue": "Work Soc Collab Constr Struct Knowl 16th Int World  28  Wide Web Conf", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Some ideas and examples to evaluate ontologies", "author": ["A Gomez-Perez"], "venue": "In Proceedings the 11th Conference on Artificial Intelligence for Applications (CAIA\u201994)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "From Knowledge Based Systems to Knowledge Sharing Technology: Evaluation and Assessment", "author": ["A G\u00f3mez-P\u00e9rez"], "venue": "Technical Report KSL 94-73", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Toward Principles for the Design of Ontologies Used for Knowledge Sharing", "author": ["T Gruber"], "venue": "In International Workshop on Formal Ontology;", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "The Semantic Web. Scientific American 2001:34\u201343", "author": ["T Berners-Lee", "J Hendler", "O Lassila"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Swoogle : A Search and Metadata Engine for the Semantic Web", "author": ["T Finin", "P Reddivari", "RS Cost", "J Sachs"], "venue": "In ACM conference on Information and knowledge", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "OntoKhoj: A Semantic Web Portal for Ontology Searching, Ranking, and Classification", "author": ["C Patel", "K Supekar", "Y Lee", "E Park"], "venue": "Proc 5th ACM CIKM Int Work Web Inf Data Manag", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "OntoSearch : An Ontology Search Engine", "author": ["Y Zhang", "W Vasconcelos", "D Sleeman"], "venue": "In Research and Development in Intelligent Systems XXI. London,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "OntoSelect: A dynamic ontology library with support for ontology selection", "author": ["P Buitelaar", "T Eigner", "T Declerck"], "venue": "In Proceedings of the Demo Session at the International Semantic Web Conference (ISWC);", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Ranking ontologies with AKTiveRank", "author": ["H Alani", "C Brewster", "N Shadbolt"], "venue": "In Proceedings of the International Semantic Web Conference (ISWC). Berlin,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "OntoQA: Metric-Based Ontology Quality Analysis", "author": ["S Tartir", "I Arpinar", "M Moore", "a Sheth", "B Aleman-Meza"], "venue": "IEEE Work Knowl Acquis from Distrib Auton Semant Heterog Data Knowl Sources", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "BiOSS: A system for biomedical ontology selection", "author": ["M Mart\u00ednez-Romero", "JM V\u00e1zquez-Naya", "J Pereira", "A Pazos"], "venue": "Comput Methods Programs Biomed", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Cupboard - A place to expose your ontologies to applications and the community", "author": ["M D\u2019Aquin", "H Lewen"], "venue": "Lect Notes Comput Sci (including Subser Lect Notes Artif Intell Lect Notes Bioinformatics)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Selecting an ontology for biomedical text mining", "author": ["H Tan", "P Lambrix"], "venue": "Proc Work Curr Trends Biomed Nat Lang Process", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Searching ontologies based on content: Experiments in the biomedical domain", "author": ["H Alani", "N Noy", "N Shah", "N Shadbolt", "M Musen"], "venue": "In Proceedings of the 4th international conference on Knowledge capture", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "A Flexible Biomedical Ontology Selection Tool", "author": ["G Maiga"], "venue": "Strength Role ICT Dev", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A multi-criteria approach for automatic ontology recommendation using collective knowledge", "author": ["M Mart\u00ednez-Romero", "JM V\u00e1zquez-Naya", "J Pereira", "A Pazos"], "venue": "In Recommender Systems for the Social Web. Volume", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "NCBO Annotator : Semantic Annotation of Biomedical Data", "author": ["C Jonquet", "NH Shah", "H Cherie", "MA Musen", "C Callendar", "M-A Storey"], "venue": "In International Semantic Web Conference (ISWC), poster and demo session;", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Automatic Summarization for Terminology Recommendation : the case of the NCBO Ontology Recommender", "author": ["P L\u00f3pez-Garc\u00eda", "S Schulz", "R Kern"], "venue": "In 7th International", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Ontology selection: ontology evaluation on the real Semantic Web", "author": ["M Sabou", "V Lopez", "E Motta", "V Uren"], "venue": "In 15th International World Wide Web Conference (WWW", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "SNOMED RT: a reference terminology for health care", "author": ["KA Spackman", "KE Campbell", "RA C\u00f4t\u00e9"], "venue": "Conf Proc Am Med Informatics Assoc Annu Fall Symp 1997:640\u2013644", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "Gene Ontology: tool for the unification of biology", "author": ["M Ashburner", "CA Ball", "JA Blake", "D Botstein", "H Butler", "JM Cherry", "AP Davis", "K Dolinski", "SS Dwight", "JT Eppig", "MA Harris", "DP Hill", "L Issel-Tarver", "A Kasarskis", "S Lewis", "JC Matese", "JE Richardson", "M Ringwald", "GM Rubin", "G Sherlock"], "venue": "Nat Genet", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "The Unified Medical Language System (UMLS): integrating biomedical terminology", "author": ["O Bodenreider"], "venue": "Nucleic Acids Res 2004,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Ontology for Biomedical Investigations", "author": ["B Peters", "T OBI Consortium"], "venue": "Nat Preced 2009:2007\u20132007", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Disease ontology: A backbone for disease semantic integration", "author": ["LM Schriml", "C Arze", "S Nadendla", "YWW Chang", "M Mazaitis", "V Felix", "G Feng", "WA Kibbe"], "venue": "Nucleic Acids Res", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "The Biomedical Resource Ontology (BRO) to enable resource discovery in clinical and translational research", "author": ["JD Tenenbaum", "PL Whetzel", "K Anderson", "CD Borromeo", "ID Dinov", "D Gabriel", "B Kirschner", "B Mirel", "T Morris", "N Noy", "C Nyulas", "D Rubenson", "PR Saxman", "H Singh", "N Whelan", "Z Wright", "BD Athey", "MJ Becich", "GS Ginsburg", "MA Musen", "KA Smith", "AF Tarantal", "DL Rubin", "P Lyster"], "venue": "J Biomed Inform", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "NCBO Resource Index: Ontology-based search and mining of biomedical resources", "author": ["C Jonquet", "P Lependu", "S Falconer", "A Coulet", "NF Noy", "a. Musen M", "NH Shah"], "venue": "J Web Semant", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "a.: Getting lucky in ontology search: A data-driven evaluation framework for ontology ranking", "author": ["NF Noy", "PR Alexander", "R Harpaz", "PL Whetzel", "RW Fergerson", "M Musen"], "venue": "Lect Notes Comput Sci (including Subser Lect Notes Artif Intell Lect Notes Bioinformatics)", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "AgroPortal : a proposition for ontology-based services in the agronomic domain", "author": ["C Jonquet", "E Dzal", "E Arnaud", "P Larmande", "E Dzal\u00e9-yeumo"], "venue": "In In IN-OVIVE\u201915: 3e\u0300me atelier INte\u0301gration de sources/masses de donne\u0301es he\u0301te\u0301roge\u0300nes et Ontologies, dans le domaine des sciences du VIVant et de l\u2019Environnement;", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Automatic recognition of multi-word terms: The Cvalue/NC-value method", "author": ["K Frantzi", "S Ananiadou", "H Mima"], "venue": "Int J Digit Libr", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Scoring semantic annotations returned by the NCBO Annotator. In Proceedings of the 7th International Workshop on Semantic Web Applications and Tools for Life Sciences (SWAT4LS\u201914)", "author": ["S Melzi", "C Jonquet"], "venue": "CEUR Workshop Proceedings. Volume 1320", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Ontologies provide the common terminology necessary for biomedical researchers to describe their datasets, enabling better data integration and interoperability, and therefore facilitating translational discoveries [1, 2].", "startOffset": 215, "endOffset": 221}, {"referenceID": 1, "context": "Ontologies provide the common terminology necessary for biomedical researchers to describe their datasets, enabling better data integration and interoperability, and therefore facilitating translational discoveries [1, 2].", "startOffset": 215, "endOffset": 221}, {"referenceID": 2, "context": "BioPortal [3], developed by the National Center for Biomedical Ontology (NCBO) [4], serves as one of the primary platforms for hosting and sharing biomedical ontologies.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "BioPortal [3], developed by the National Center for Biomedical Ontology (NCBO) [4], serves as one of the primary platforms for hosting and sharing biomedical ontologies.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "0\u2019 or \u2018original Ontology Recommender\u2019) [5], which informed the user of the most appropriate ontologies in BioPortal to annotate textual data.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Ontology evaluation has been defined as the problem of assessing a given ontology from the point of view of a particular criterion, typically in order to determine which of several ontologies would best suit a particular purpose [6].", "startOffset": 229, "endOffset": 232}, {"referenceID": 6, "context": "As a consequence, ontology recommendation is fundamentally an ontology evaluation task because it addresses the problem of evaluating and consequently selecting the most appropriate ontologies for a specific context or goal [7, 8].", "startOffset": 224, "endOffset": 230}, {"referenceID": 7, "context": "As a consequence, ontology recommendation is fundamentally an ontology evaluation task because it addresses the problem of evaluating and consequently selecting the most appropriate ontologies for a specific context or goal [7, 8].", "startOffset": 224, "endOffset": 230}, {"referenceID": 8, "context": "Early contributions in the field of ontology evaluation date back to the early 1990s and were motivated by the necessity of having evaluation strategies to guide and improve the ontology engineering process [9\u201311].", "startOffset": 207, "endOffset": 213}, {"referenceID": 9, "context": "Early contributions in the field of ontology evaluation date back to the early 1990s and were motivated by the necessity of having evaluation strategies to guide and improve the ontology engineering process [9\u201311].", "startOffset": 207, "endOffset": 213}, {"referenceID": 10, "context": "Early contributions in the field of ontology evaluation date back to the early 1990s and were motivated by the necessity of having evaluation strategies to guide and improve the ontology engineering process [9\u201311].", "startOffset": 207, "endOffset": 213}, {"referenceID": 11, "context": "Some years later, with the birth of the Semantic Web [12], the need for reusing ontologies across the Web motivated the development of the first ontology search engines [13\u201315], which made it possible to retrieve all ontologies satisfying some basic requirements (e.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "Some years later, with the birth of the Semantic Web [12], the need for reusing ontologies across the Web motivated the development of the first ontology search engines [13\u201315], which made it possible to retrieve all ontologies satisfying some basic requirements (e.", "startOffset": 169, "endOffset": 176}, {"referenceID": 13, "context": "Some years later, with the birth of the Semantic Web [12], the need for reusing ontologies across the Web motivated the development of the first ontology search engines [13\u201315], which made it possible to retrieve all ontologies satisfying some basic requirements (e.", "startOffset": 169, "endOffset": 176}, {"referenceID": 14, "context": "Some years later, with the birth of the Semantic Web [12], the need for reusing ontologies across the Web motivated the development of the first ontology search engines [13\u201315], which made it possible to retrieve all ontologies satisfying some basic requirements (e.", "startOffset": 169, "endOffset": 176}, {"referenceID": 4, "context": "0 paper [5].", "startOffset": 8, "endOffset": 11}, {"referenceID": 15, "context": "It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.", "startOffset": 258, "endOffset": 265}, {"referenceID": 16, "context": "It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.", "startOffset": 258, "endOffset": 265}, {"referenceID": 17, "context": "It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.", "startOffset": 258, "endOffset": 265}, {"referenceID": 18, "context": "It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.", "startOffset": 297, "endOffset": 301}, {"referenceID": 15, "context": "It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.", "startOffset": 316, "endOffset": 320}, {"referenceID": 19, "context": "It is a complex process that comprises not only enumerating a list of ontologies with class names matching a specific term, but also evaluating all candidate ontologies according to a variety of criteria, such as coverage, richness of the ontology structure [16\u201318], correctness, frequency of use [19], connectivity [16], formality, user ratings [20], and their suitability for the task at hand.", "startOffset": 346, "endOffset": 350}, {"referenceID": 20, "context": "Tan and Lambrix [21] proposed a theoretical framework for selecting the best ontology for a particular text-mining application and manually applied it to a gene-normalization task.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "[22] developed an ontology-search strategy that uses query-expansion techniques to find ontologies related to a particular domain (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Maiga and Williams [23] conceived a semi-automatic tool that makes it possible to find the ontologies that best match a list of user-defined task requirements.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "The most relevant alternative to the NCBO Ontology Recommender is BiOSS [19, 24], which was released in 2011 by some of the authors of this paper.", "startOffset": 72, "endOffset": 80}, {"referenceID": 23, "context": "The most relevant alternative to the NCBO Ontology Recommender is BiOSS [19, 24], which was released in 2011 by some of the authors of this paper.", "startOffset": 72, "endOffset": 80}, {"referenceID": 24, "context": "The NCBO Ontology Recommender invoked the NCBO Annotator [25] to identify all annotations for the input data.", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "3 This formula is slightly different from the scoring method presented in the paper describing the original Ontology Recommender Web service [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 25, "context": "in their study of the efficiency of automatic summarization techniques [26].", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "Related work in ontology recommendation highlights the importance of addressing two different input types: text corpora and lists of keywords [27].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "Additionally, there is a diversity of other recently-proposed evaluation techniques [7, 17, 24] that could enhance our strategy.", "startOffset": 84, "endOffset": 95}, {"referenceID": 16, "context": "Additionally, there is a diversity of other recently-proposed evaluation techniques [7, 17, 24] that could enhance our strategy.", "startOffset": 84, "endOffset": 95}, {"referenceID": 23, "context": "Additionally, there is a diversity of other recently-proposed evaluation techniques [7, 17, 24] that could enhance our strategy.", "startOffset": 84, "endOffset": 95}, {"referenceID": 24, "context": "The NCBO Annotator [25] is then used to obtain all annotations for the input using BioPortal ontologies.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "8 [5], class match measure [17] and topic coverage [27].", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": "8 [5], class match measure [17] and topic coverage [27].", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "8 [5], class match measure [17] and topic coverage [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "Acceptance is related to criteria such as popularity [19, 24, 27], connectivity [5] and connectedness [16].", "startOffset": 53, "endOffset": 65}, {"referenceID": 23, "context": "Acceptance is related to criteria such as popularity [19, 24, 27], connectivity [5] and connectedness [16].", "startOffset": 53, "endOffset": 65}, {"referenceID": 26, "context": "Acceptance is related to criteria such as popularity [19, 24, 27], connectivity [5] and connectedness [16].", "startOffset": 53, "endOffset": 65}, {"referenceID": 4, "context": "Acceptance is related to criteria such as popularity [19, 24, 27], connectivity [5] and connectedness [16].", "startOffset": 80, "endOffset": 83}, {"referenceID": 15, "context": "Acceptance is related to criteria such as popularity [19, 24, 27], connectivity [5] and connectedness [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Detail is similar to structure measure [5], semantic richness [19, 24] , structure [16], and granularity [23].", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "Detail is similar to structure measure [5], semantic richness [19, 24] , structure [16], and granularity [23].", "startOffset": 62, "endOffset": 70}, {"referenceID": 23, "context": "Detail is similar to structure measure [5], semantic richness [19, 24] , structure [16], and granularity [23].", "startOffset": 62, "endOffset": 70}, {"referenceID": 15, "context": "Detail is similar to structure measure [5], semantic richness [19, 24] , structure [16], and granularity [23].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "Detail is similar to structure measure [5], semantic richness [19, 24] , structure [16], and granularity [23].", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "For each of these evaluation criteria, a score in the interval [0,1] is typically obtained.", "startOffset": 63, "endOffset": 68}, {"referenceID": 0, "context": "Then, all the scores for a given ontology are aggregated into a composite relevance score, also in the interval [0,1].", "startOffset": 112, "endOffset": 117}, {"referenceID": 0, "context": "where A is the set of annotations performed with the ontology o for the input t, selectedAnnotations(A) is the set of annotations that are left after discarding overlapping annotations, and norm is a function that normalizes the coverage score to the interval [0,1].", "startOffset": 260, "endOffset": 265}, {"referenceID": 0, "context": "In this example, the coverage score for SNOMEDCT would be calculated as 5+26=31, which would be normalized to the interval [0,1] by dividing it by the maximum coverage score.", "startOffset": 123, "endOffset": 128}, {"referenceID": 27, "context": "Examples of broadly accepted ontologies are SNOMEDCT [28] and Gene Ontology [29].", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "Examples of broadly accepted ontologies are SNOMEDCT [28] and Gene Ontology [29].", "startOffset": 76, "endOffset": 80}, {"referenceID": 29, "context": "The presence of the ontology in the Unified Medical Language System (UMLS) [30].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "where pageviewsScore(o) represents the number of visits to the ontology in BioPortal normalized to the interval [0,1]; umlsScore(o) is 1 if the ontology is included into UMLS and 0 if it is not.", "startOffset": 112, "endOffset": 117}, {"referenceID": 23, "context": "We evaluate the richness of the ontology representation for the input data based on a simplification of the \u201csemantic richness\u201d metric used by BiOSS [24].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "For each annotation selected during the coverage evaluation step, we calculate the detail score as follows: !\"#$%&'()!\" ! = !\"#$%$&$'%()'*\" ! + !\"#$#\"%!&'$() ! + !\"#!$\"%&$'()#\"$(!) 3 where detailScore(a) is a value in the interval [0,1] that represents the level of detail provided by the annotation a.", "startOffset": 231, "endOffset": 236}, {"referenceID": 0, "context": "Finally, the detail for the ontology would be calculated as the sum of the detail scores of the annotations done with the ontology, normalized to [0,1]: !\"#$%& !, ! = !\"#$%&'()*\"(!) ! \u2200! \u2208 !\"#\"$%\"&'(()%*%+)(!(!)", "startOffset": 146, "endOffset": 151}, {"referenceID": 30, "context": "Examples include the Ontology for Biomedical Investigations [31], the Human Disease Ontology [32] and the Biomedical Resource Ontology [33].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "Examples include the Ontology for Biomedical Investigations [31], the Human Disease Ontology [32] and the Biomedical Resource Ontology [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 32, "context": "Examples include the Ontology for Biomedical Investigations [31], the Human Disease Ontology [32] and the Biomedical Resource Ontology [33].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "Finally, the norm function normalizes the score to the interval [0,1].", "startOffset": 64, "endOffset": 69}, {"referenceID": 6, "context": "In practice, it is more likely that several ontologies will jointly cover the input [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 18, "context": "Our method to evaluate ontology sets is based on the \u201contology combinations\u201d approach used by the BiOSS system [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "All scores are translated from the interval [0, 1] to [0, 100] for better readability.", "startOffset": 44, "endOffset": 50}, {"referenceID": 33, "context": "We used the API provided by the NCBO Resource Index [34] to programmatically extract data from those databases.", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "We focused on the top-ranked ontology because the majority of users always select the first result obtained [35].", "startOffset": 108, "endOffset": 112}, {"referenceID": 35, "context": "To address this issue, we are currently refining, in collaboration with the AgroPortal ontology repository [36], the way BioPortal handles metadata for ontologies in order to support, in the future, even more ontology recommendation scenarios.", "startOffset": 107, "endOffset": 111}, {"referenceID": 36, "context": "We plan to analyze the application of a term extraction measure, called C-value [37], which is specialized for multi-word term extraction, and that has already been applied to the results of the NCBO Annotator, leading to significant improvements [38].", "startOffset": 80, "endOffset": 84}, {"referenceID": 37, "context": "We plan to analyze the application of a term extraction measure, called C-value [37], which is specialized for multi-word term extraction, and that has already been applied to the results of the NCBO Annotator, leading to significant improvements [38].", "startOffset": 247, "endOffset": 251}, {"referenceID": 18, "context": "There are other indicators external to BioPortal that could be useful for performing a more comprehensive evaluation of ontology acceptance, such as the number of Google results when searching for the ontology name or the number of PubMed publications that contain the ontology name [19].", "startOffset": 283, "endOffset": 287}, {"referenceID": 35, "context": "For example, AgroPortal [36] is an ontology repository based on NCBO BioPortal technology.", "startOffset": 24, "endOffset": 28}], "year": 2016, "abstractText": null, "creator": "Word"}}}