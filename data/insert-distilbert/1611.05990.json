{"id": "1611.05990", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Monte Carlo Connection Prover", "abstract": "monte carlo tree approximation search ( mcts ) is a technique to guide search approaches in a large decision space by taking random samples and evaluating their outcome. in this work, as we may study mcts methods in expanding the main context of the connection calculus and implement them on top of the leancop prover. improving this includes proposing useful proof - state evaluation heuristics that are learned prior from rigorous previous proofs, and proposing and automatically improving suitable mcts comparison strategies in this context. the operating system is trained and evaluated on a large suite of related problems coming from the stanford mizar proof assistant, showing that it is capable to find new and different partial proofs. to our knowledge, this is the first time robust mcts has been applied to theorem proving.", "histories": [["v1", "Fri, 18 Nov 2016 06:30:09 GMT  (29kb)", "http://arxiv.org/abs/1611.05990v1", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["michael f\\\"arber", "cezary kaliszyk", "josef urban"], "accepted": false, "id": "1611.05990"}, "pdf": {"name": "1611.05990.pdf", "metadata": {"source": "META", "title": "Monte Carlo Connection Prover", "authors": ["Michael F\u00e4rber", "Cezary Kaliszyk"], "emails": ["michael.faerber@uibk.ac.at", "cezary.kaliszyk@uibk.ac.at", "josef.urban@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n05 99\n0v 1\n[ cs\n.L O\n] 1\n8 N\nov 2\n01 6\nMonte Carlo Tree Search (MCTS) is a technique to guide search in a large decision space by taking random samples and evaluating their outcome. In this work, we study MCTS methods in the context of the connection calculus and implement them on top of the leanCoP prover. This includes proposing useful proof-state evaluation heuristics that are learned from previous proofs, and proposing and automatically improving suitable MCTS strategies in this context. The system is trained and evaluated on a large suite of related problems coming from the Mizar proof assistant, showing that it is capable to find new and different proofs. To our knowledge, this is the first time MCTS has been applied to theorem proving."}, {"heading": "1. Introduction", "text": "Proof automation is critical in formal proof verification. In particular, the recent hammer integration of automated theorem provers (ATPs) such as E (Schulz 2013), Vampire (Kov\u00e1cs and Voronkov 2013), CVC4 (Barrett et al. 2011), leanCoP (Otten 2008), and interactive theorem provers (ITPs) such as Isabelle (Wenzel et al. 2008), HOL4 (Slind and Norrish 2008), HOL Light (Harrison 1996), and Mizar (Grabowski et al. 2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)). Today\u2019s automated theorem provers are however still quite weak in finding more complicated proofs over large formal developments. Their search typically blows up after several seconds, making the chance of finding proofs in longer times exponentially decreasing.\n[Copyright notice will appear here once \u2019preprint\u2019 option is removed.]\nThis behaviour is reminiscent of poorly guided search in games such as Chess and Go. The number of all possible variants there typically also grows exponentially, and intelligent guiding methods are needed to focus on exploring the most promising moves and positions. The guiding method that has recently very significantly improved computer Go is Monte Carlo Tree Search (MCTS), i.e., expanding the search based on its (variously guided) random sampling.\nIn this work, we study MCTS methods that can guide the search in automated theorem provers, and evaluate their impact on interactive theorem proving problems. We focus on the connection calculus where the notion of tree search occurs naturally, and on the leanCoP prover, which has a compact implementation that is easy to experiment with. We can also build on previous machine-learning extensions of leanCoP (Kaliszyk and Urban 2015b, Urban et al. (2011)). As shown for example in the recent AlphaGo system (Silver et al. 2016), machine learning can be used to train good position evaluation heuristics even in very complicated domains that were previously thought to be solely in the realm of \u201chuman intuition\u201d. This gives a hope for productive use of MCTS in theorem proving. While \u201cfinishing the randomly sampled game\u201d \u2013 as used in the most straightforward MCTS for games \u2013 is rarely possible in ATP (it would mean finishing the proof), there is a chance of learning good proof state evaluation heuristics that will guide the MCTS for ATPs in a similar way as e.g. in AlphaGo.\nThe rest of the paper is organized as follows. Section 2 introduces MCTS and in particular its recent UCT refinement. Section 3 introduces the connection calculus and proposes modifications that allow combining it with tree search. Section 4 proposes proof state evaluation heuristics. Section 5 describes our implementation, and section 6 evaluates the system on a set of Mizar Mathematical library problems."}, {"heading": "2. Monte Carlo Tree Search", "text": "Monte Carlo Tree Search (MCTS) is a procedure to search large spaces by random sampling, biased towards promising parts of the search space. It has been applied\n1 2016/11/21\nto a multitude of games and domains, often achieving state-of-the-art results (Browne et al. 2012). One of the most famous MCTS applications is AlphaGo, which is the first program to beat a professional Go player in a tournament without handicap (Silver et al. 2016).\nThe idea of MCTS is to run random play-outs of a game, called simulations, and to choose the move from where the play-outs were most successful. To obtain good results, it is crucial to bias the simulations such that they cover a diverse selection of the state space (exploration), but also that they concentrate on the parts where the most successful play-outs happened (exploitation). One of the most influential methods to balance exploration and exploitation in MCTS has been UCT (Kocsis and Szepesv\u00e1ri 2006). It contributes a strategy to select a subset of the decision space to search next, based on how often the subspace was already searched and what the average outcome of the searches was.\nWe next introduce terminology to describe MCTS problems and present MCTS and UCT algorithms in detail."}, {"heading": "2.1 Characterisation of MCTS Problems", "text": "To make a problem such as theorem proving tractable by MCTS, it is convenient to characterise the problem by rules and heuristics. Rules describe the starting state and the legal moves possible in any given state, while heuristics estimate the quality of a given move as well as of a (final) state.\nTo describe rules, we use:\n\u2022 A set of states S. \u2022 An initial state s0 \u2208 S. \u2022 A state transition function \u03b4 : S \u2192 2S , which maps\nevery state to its permitted successor states.\nTo describe heuristics:\n\u2022 A transition weight function \u03b4w : S \u2192 S \u2192 R\u22650, where \u03b4w(s, t) gives the weight of going from state s to t in a random simulation. \u2022 A reward function r : S \u2192 [0, 1]."}, {"heading": "2.1.1 Example: Travelling Salesman Problem", "text": "As an example of an MCTS application, consider the travelling salesman problem (TSP): Given a list of cities {c1, . . . , cn} with a distance function d, find a permutation X of [1, . . . , n] such that r(X) = \u2211\ni d(cXi , cXi+1) is minimal. We can formulate the rules of this problem as follows:\n\u2022 The set of states S are all possible (potentially unfinished) paths of the travelling salesman, i.e. permutations of subsets of {1, . . . , n}. \u2022 The initial state s0 is the empty sequence [].\n\u2022 The state transition function \u03b4 returns all possible paths where the travelling salesman has visited one more city, i.e. \u03b4([X1, . . . , Xi]) = {[X1, . . . , Xi, j] | 1 \u2264 j \u2264 n \u2227 j /\u2208 {X1, . . . , Xi}}.\nThe heuristics for the TSP could be set as follows:\n\u2022 The reward function is r(X) as given in the TSP definition. \u2022 The transition weight function is\n\u03b4w(s, t) = |{u \u2208 \u03b4(s) | r(u) \u2265 r(t)}| \u22121 ,\nwhich rates cities by how many cities are nearer to the current position of the travelling salesman.\nThis description is sufficient to run MCTS to find solutions to the TSP."}, {"heading": "2.2 Monte Carlo Tree Search Algorithm", "text": "MCTS maintains a decision tree in which nodes correspond to states S of the underlying problem (say, the current positions of stones on a Go board), and the children of a node s correspond to successor states \u03b4(s) (e.g., the board configurations after the other player has set a stone). In addition to the state, a node contains statistics about the number of times the node was traversed and about the rewards of simulations that were started from and below the node.\nWhen starting MCTS, the decision tree is initialised with a root node that corresponds to the initial state s0 \u2208 S. One iteration of the MCTS algorithm involves the following steps:\n1. Selection: The MCTS tree is traversed from the root until a leaf (with at least one unexplored successor state) is encountered. 2. Simulation: A (random) simulation is performed starting from the leaf. 3. Expansion: New nodes corresponding to the initial simulation steps are added to the leaf. 4. Backpropagation: The reward of the simulation is backpropagated from the leaf to all nodes up to the root.\nThe actual algorithms used in all four steps are referred to as strategies. Some commonly used strategies are:\n\u2022 Simulation strategy: Given a state s, choose as next state some state t \u2208 \u03b4(s) randomly biased by \u03b4w(s, t). If \u03b4(s) is empty or a maximal simulation depth has been reached, stop the simulation. \u2022 Expansion strategy: Add a node corresponding to the first state of the simulation to the leaf. \u2022 Backpropagation strategy: Add the reward of the simulation to the rewards of all ancestors and increase their visit counts by one. This allows calculat-\n2 2016/11/21\ning the average reward by dividing the reward sum by the number of visits.\nWe present one of the most influential selection strategies: UCT."}, {"heading": "2.3 UCT", "text": "UCT (Upper Confidence Bounds applied to Trees) is a version of MCTS with a selection strategy that attempts to balance exploitation and exploration: It selects the node j that maximises Xj + Cp \u221a\n2 ln n nj , where:\n\u2022 Xj is average reward for node j, \u2022 nj is number of times node j was chosen, \u2022 n is total number of times the parent node of j was\nchosen.\nUCT assumes that rewards are always in the range [0, 1]. This is important to keep the equilibrium between exploration and exploitation. We call Cp \u221a\n2 ln n nj the\nexploration term."}, {"heading": "3. Connection Proving and MCTS", "text": "In this section, we introduce the clausal connection calculus, discuss possible ways to model the connection calculus as game and describe how to integrate it with MCTS."}, {"heading": "3.1 Clausal Connection Calculus", "text": "The connection calculus is a goal-directed proof search procedure (Bibel 1993). It operates on a matrix M representing the set of clauses obtained from the Skolem normal form of a set of first-order formulas. A version that is used in the automated theorem prover leanCoP is shown in Figure 1. The proof search is initialised with the \u201cStart\u201d rule and is successful when all branches of the proof tree are closed by the application of the other rules. The \u201cstate\u201d of a branch is characterised by a triple (C,M,Path), where C is the current clause, M is the matrix containing all input clauses and Path is the active path."}, {"heading": "3.2 Connection Proving as a Game", "text": "MCTS has been applied to two-player games such as Go (Silver et al. 2016), as well as to single-player games such as SameGame (Schadd et al. 2012). It is possible to model proof search as game with either one or two players. We compare the two approaches.\nIn case of a single player, the player chooses an action to apply to an open proof branch, potentially adding new open proof branches. The player wins when proof branches have been closed, and loses if there exists a proof branch for which no proof rule is applicable.\nIn case of two players, the first player chooses an open proof branch and, if the clause of the branch is\nnot empty, a literal. The second player then applies an action to the chosen proof branch, which has to result either in closing the proof branch or producing at least one new open proof branch with the literal removed from the clause. If the second player is unable to do either of the two, then the first player wins, and if there is no open proof branch left for the first player to choose from, then the second player wins.\nModelling connection proving as a two-player game has the advantage that given an intelligent proof branch evaluation heuristic, the proof search can be directed by the first player towards \u201charder\u201d proof branches, thus more quickly resulting in the termination of proof search in branches that cannot be closed. On the other hand, a similar behaviour can be achieved by pre-ordering the proof branches in the single-player game. However, if we model connection proving as a two-player game following the default UCT selection policy, every possible move of the first player has to be considered at least once. Even if one restricts the first player to choose only literals from a single proof branch clause, this can considerably blow up the search space. We therefore focus on connection proving as a single-player game."}, {"heading": "3.3 Single-Player Connection Proving Game", "text": "To model single-player connection proving as MCTS problem, we introduce a non-branching version of the connection calculus (see Figure 2) where substitutions are explicit and every rule has only a single premise. As closed proof trees in the branching and non-branching connection calculi are isomorphic, the non-branching calculus preserves soundness and completeness. With this calculus, we can give the rules of connection proving as in subsection 2.1:\n\u2022 The set of states S is the set of tuples (Goals, \u03c3), where Goals is a set of clause-path pairs and \u03c3 is a substitution. \u2022 The initial state s0 is ({({\u22a4}, {})}, {}). \u00ac\u22a4 has to be added to all positive clauses in M to correctly emulate the start rule of the branching connection calculus. \u2022 The state transition function \u03b4(s) returns all possible premises of non-branching connection calculus rules that have s as conclusion.1\nIn the remainder of this and the following section, we discuss possible MCTS heuristics."}, {"heading": "3.4 Main Loop", "text": "To find good moves for two-player games, MCTS is usually run with a fixed time limit or number of iterations in order to select and use the best first move from the\n1 In practice, \u03b4(s) restricts to a finite subset of the possible calculus premises, just like leanCoP does.\n3 2016/11/21\nroot. It is possible to use MCTS in such a way as an oracle in a conventional theorem prover: Given a prover state with high uncertainty as to which proof step is most appropriate (for example when having to select between several extension clauses), one can run MCTS and obtain the most promising next proof step, resuming the conventional proof search from there.\nThis approach has two problems. First, it is unclear in which situations it is beneficial to run MCTS instead of searching all proof options exhaustively. Second, extracting only the first best successor node of the Monte Carlo search tree discards all the statistics in the search tree about the quality of follow-up states.\nFor this reason, we follow a different approach, namely we use MCTS as driving force to control the proof search: In every MCTS iteration, a short proof search is performed, and the reward function controls which parts of the search space the proof search concentrates on, depending on the outcomes of the previous proof searches. A similar approach is taken in (Schadd et al. 2012).\nAs selection strategy, we use UCT as described in subsection 2.3, and as backpropagation strategy, we use the MCTS default strategy as described in section 2."}, {"heading": "3.5 Simulation Strategies", "text": "Once the selection strategy has chosen a child node with at least one unexplored successor state, the simulation strategy chooses an unexplored successor state s and starts a random simulation. In every iteration of the simulation we calculate the set of successor states \u03b4(s), i.e. all possible proof steps that can be applied in the current prover state s. Next, one of the successor states is randomly picked, biased with the transition weight \u03b4w, and the simulation is continued with the picked successor state. During the simulation no backtracking is done, as it would require storing search history information in the prover state.\nThe simulation returns the sequence of traversed states up to the current state s if either \u03b4(s) is the empty set, i.e. no proof rule is applicable, or some maximum simulation depth is reached. In case that s has no more open subgoals, i.e. a proof has been found, the proof is returned.\n4 2016/11/21\nTo bias the selection of successor states, we assign a constant weight to reduction steps and focus on influencing the weight of extension steps in function of the corresponding extension clauses:\n\u2022 Constant: Potential extension steps have the same weight. \u2022 Inverse size: Extension clause is weighed with the inverse number of literals it contains. \u2022 Rank: Extension clauses are weighed by the inverse of the number of smaller extension clauses.\nSelection of extension steps using machine-learning, similar to FEMaLeCoP (Kaliszyk and Urban 2015b), is left as future work."}, {"heading": "3.6 Expansion Strategies", "text": "Once the simulation strategy has finished with a resulting sequence of states, the expansion strategy is responsible for adding a new child node to the node n the simulation has started from. Since the chance of revisiting a node sharply decreases with its depth, most deeper nodes will not be visited again and storing them all takes up significant amount of memory. Therefore, we add only a single leaf node to the starting node n. Considering deeper subtrees in function of the expected number of future visits to the node, in order to minimise the costly recalculation of successor states, could be considered in future work.\nOur leaf-adding expansion strategies are:\n\u2022 First-node expansion: This strategy adds the first node of the simulation, i.e. the previously unvisited action of the chosen node to its parent node. \u2022 Best-node expansion: This strategy takes the state of the simulation with the lowest number of subgoals and adds it to the chosen node. When a node has a lower number of subgoals than the starting node, at least one subgoal was closed. Adding only that node to the tree discards all other options to close the branch. This is similar to the cut heuristic in leanCoP, which makes proof search incomplete, but reduces the search space.\nThe first-node expansion strategy preserves completeness. Consider the leafs of a Monte Carlo search tree. They represent all possible ways to prove the original conjecture, and all of its successor states preserve this invariant. Since rewards are between 0 and 1, we can show that for Cp > 0, any leaf of the search tree will be selected at some point. This is because the exploration term of any node will become large enough for that node to be visited, regardless of its average reward."}, {"heading": "4. Proof State Evaluation Heuristics", "text": "One of the important advantages of MCTS for games is that is does not require an evaluation function that estimates the win likelihood for a non-final state. For example, in a chess game it is not required to find a function that estimates how likely a player will win the game, but it is only necessary to determine at the end of a game which player won and perhaps by how much. However, in theorem proving, we are very rarely able to play a game to the end with a clear outcome, meaning that we still need to estimate the success of branches. Even if we encounter a branch where we are not able to continue to proof, thus being in a final state, that may not be considered a \u201closs\u201d in the game sense because we might just have a single mistake that made us unable to finish the proof. It is clear that we need a more finegrained notion of \u201csuccess\u201d to estimate such situations\u2019 potential to lead to a proof by choosing slightly different decisions."}, {"heading": "4.1 Provability Estimates", "text": "We define the quality of a state as its provability estimate, i.e. the probability of finding a proof by pursuing that branch. We have devised several methods to estimate the quality of a branch. The linear combination of their estimates constitutes the reward function r(s) of our Monte Carlo Connection Prover.\nThe first method is based on the fact that towards the end of a proof, the number of open subgoals goes to zero, and the number of open subgoals is always smaller or equal to the number of previously opened subgoals. So the first reward function for a state is just the ratio of open and previously opened subgoals.\nThe second method is based on data collected during previous proof attempts. We assume that certain goals are statistically easier to close than others, and also certain literals of goals are easier than others. This is similar to the approach taken in e.g. FEMaLeCoP (Kaliszyk and Urban 2015b), which prefers proof steps involving previously \u201csuccessful\u201d clauses. However, where FEMaLeCoP only needs to establish an order on a finite set of clauses that are applicable in a particular situation, we need to establish an order on a potentially infinite set of states. Furthermore, while FEMaLeCoP estimates the usefulness of a clause for the current state, we have to evaluate the state itself, putting it into relation with any other state. We describe the method in the next section."}, {"heading": "4.2 Machine-learnt Provability Estimates", "text": "We say that a literal L1 was proved successfully if either the reduction rule was applied to C\u222a{L1} or the extension rule was applied and its left branch was completely closed (see Figure 1). A literal was proved unsuccessfully\n5 2016/11/21\nwhen the reduction rule was not applicable and for all extension rule that were applicable, their left branches were not closed.\nWe now define our literal provability estimate: If we have not seen the literal l in previous proofs, then the provability of l is 1, that is, for unseen literals we are optimistic that we are able to prove them. If we have seen it in proofs being successfully proved for p and unsuccessfully proved for n times, then the success rate is p\np+n . To account for literals that we have rarely seen,\nwe introduce the certainty\nc(x) = 1\u2212 C 1\nxD + 1\n(where C and D are constants) and calculate the total literal provability as\n1 + c(p+ n) \u00b7\n(\np\np+ n \u2212 1\n)\n.\nWith the literal provability defined, we can define clause provability. A clause is proved if all of its literals are proved, therefore it is natural to define the clause provability as the product of its literals\u2019 provabilities.\nWe can apply the same idea to obtain the total prover state refutability \u2013 to solve the whole problem, it is necessary to prove all clauses of the prover state, therefore the total provability is the product of the clause provabilities. However, in our experiments it became quite clear that this very frequently gave rewards of 0, because a single literal in a single clause that had a bad ranking could deteriorate the reward. For that reason, we tried combining clause provabilities with the minimum function and with the harmonic, geometric, and arithmetic means."}, {"heading": "5. Implementation", "text": "The implementation is based on the source code of the FEMaLeCoP prover (Kaliszyk and Urban 2015b), which is an OCaml version of leanCoP (Otten 2008) enhanced by machine learning techniques. For all the provers mentioned below, we implemented the usual leanCoP optimisations which are not part of the calculus, such as lemmas and the regularity check. Furthermore, all provers below use the same proof format as the leanCoP tactic described in (Kaliszyk et al. 2015) to automatically find proofs for HOL Light, therefore adapting the HOL Light proof tactic to use our Monte Carlo system should be straightforward."}, {"heading": "5.1 Lazy List Connection Prover", "text": "The FEMaLeCoP implementation uses continuationpassing style (CPS) to implement backtracking supporting the Prolog \u201ccut\u201d. While this is very efficient, it does not provide a simple representation of different ways to\nclose a subgoal, because this information is \u201chidden\u201d in the continuation. However, MCTS needs an explicit set of successor states to any prover state. To approach this, we first created a version of FEMaLeCoP that uses lazy lists instead of continuations to represent different proof options: To close a subgoal, there might be different reduction and extension steps. So for a given literal l, we return a lazy list of proof trees that prove l. We can then filter out only closed proof trees and backtrack by going through the list of proof trees. Cut can also be implemented by restricting the returned proof trees to the first closed one. We refer to the lazy list implementation as lazyCoP."}, {"heading": "5.2 Non-branching Calculus", "text": "lazyCoP served as a base for an implementation of the non-branching calculus: In the non-branching calculus, we return as list of actions to close a subgoal only the next nodes of the proof subtrees and not the whole proof subtree to close the subgoal. This distinction is important, because it allows us to perform rapid random play-outs of the proof search without branching. The resulting implementation of non-branching proof search works also independently from MCTS, for example by using iterative deepening together with a depthfirst search in the proof search decision tree."}, {"heading": "5.3 Monte Carlo Connection Prover", "text": "Because the non-branching proof search corresponds already closely to the MCTS problem description in subsection 2.1, it is now relatively easy to replace the depthfirst search tactic with MCTS. The largest adaptation necessary was the replacement of the global array-based substitution with a local substitution for every explored proof branch, where we chose a list-based substitution. We refer to the resulting system as monteCoP."}, {"heading": "5.4 Monte Carlo Tree Search", "text": "We implemented the UCT variant of MCTS and validated its function with the travelling salesman problem as shown in subsubsection 2.1.1.\nWe added an optimisation in MCTS specific to automated theorem proving: When the list of unvisited and visited child nodes of a node is empty, the node itself is deleted. Without this deletion, nodes which have no hope of ever contributing to the proof search would still be visited from time to time, as the UCT exploration term of the nodes would grow.\nFurthermore, we use the transition weight function \u03b4w not only to bias the selection of successor states during the simulation, but also when choosing an unexplored action of a Monte Carlo tree node.\n6 2016/11/21"}, {"heading": "5.5 Training Data", "text": "To collect literal refutation statistics as described in section 4, we created a lazyCoP version that reports statistics for the last iteration of the iterative deepening if a proof was found. To account for the same literal appearing among different clauses, we also register for each literal the clause it came from.\nAn important aspect is consistent Skolemisation, which ensures that constants introduced during Skolemisation bear the same name among different problems and prover runs. Initially, we stored the symbolic representations of clauses and literals in the literal statistics. However, the names of the Skolem constants can become so large that loading the training data took a considerable amount of time during proof search. We solved this problem by saving only hashes of the symbolic representations of literals and their clauses, which reduced the size of the training data by several magnitudes. To combine the training data from different examples, we add the positive and negative occurrences of each literal among all problems."}, {"heading": "5.6 Related Work", "text": "randoCoP (Raths and Otten 2008) is another connection prover based on randomness. It runs leanCoP multiple times, shuffling the order of clauses and literals at the beginning of every proof search. The difference to our approach is that in randoCoP, the randomness only plays a role at the beginning of each proof search, after which the behaviour is deterministic. In monteCoP, however, the restart behaviour is intrinsically given by UCT, and randomness influences the proof search at every point where multiple options are available to close a subgoal."}, {"heading": "6. Evaluation", "text": "We used the problems of the MZR@Turing division of CASC-J6 (Sutcliffe 2013). It consists of 1000 training problems that were known before the competition, as well as 400 testing problems that were used to evaluate the quality of the submitted competitors. All 1400 problems are taken from the MPTP2078 problem set (Alama et al. 2011), but sometimes augmented with facts that are not necessary for the proof to make proof attempts harder. We mapped the problems of the MZR@Turing division to their bushy counterparts (with minimised number of dependencies) of the MPTP2078 dataset and used these for evaluation."}, {"heading": "6.1 Parameter Optimisation", "text": "First, we ran lazyCoP with timeout of 300s on the 1000 training problems, yielding 314 solved problems together with literal statistics as described in subsection 5.5. We then divided the solved problems into a pa-\nrameter training set of 264 and a parameter testing set of 50 problems. On the parameter training set, we ran ParamILS (Hutter et al. 2009) with monteCoP, using a problem timeout of 2s. We ran 32 parallel ParamILS instances with different starting configurations, where each instance ran for a total time of 80000s. Next, we obtained the best resulting ParamILS configuration with respect to the number of inferences per solved problem on the parameter testing set."}, {"heading": "6.2 Results", "text": "As baseline comparison, we chose a monteCoP configuration that resembles breadth-first search, by choosing a constant reward function and a simulation depth of 1. This causes the search tree to grow uniformly and encourages only exploration, as the reward of all nodes is the same. This version of monteCoP proves 41 problems of the benchmark. Next, we chose a monteCoP configuration with a higher simulation depth, still keeping the constant reward function and choosing potential next proof steps with equal probability. This corresponds to randomised beam search with increasing beam depth. In this configuration, monteCoP solves 49 problems. Finally, we guide the proof search by biasing extension clause selection towards smaller clauses, using the simulation strategy in subsection 3.5, and with the reward function introduced in section 4, using the literal provability data obtained from the lazyCoP runs on the MZR@Turing training problems. The resulting monteCoP configuration solves 64 problems, which is an increase of 30.6% compared to the unguided monteCoP proof search. The single best lazyCoP strategy solves 84 problems. Still, of the 64 problems proved by monteCoP, 15 solutions are unique, which is 17.9% of the problems that lazyCoP solves."}, {"heading": "6.3 Inferences", "text": "We count the number of inferences as the number of successful extension steps. In monteCoP, we additionally count the number of MCTS iterations. This roughly corresponds to the number of nodes in the Monte Carlo search tree, i.e. the number of random proof searches, with the exception that our version of MCTS can also delete nodes during an iteration when they do not have any successor states.\nThe average number of monteCoP MCTS iterations is 5532. The average number of inferences is 320295 for lazyCoP and 117163 for monteCoP. On average, monteCoP performed 9.6 times as many inferences as lazyCoP on the problems that both provers solved. This can be explained by the fact that monteCoP always calculates all possible extension steps applicable, even if the extension step is not tried afterwards. Finding a way to compute fewer extension clauses might improve performance.\n7 2016/11/21"}, {"heading": "6.4 Experiments", "text": "In our experiments, the exploration constant Cp turned out to be very influential for proof search: Setting it high results in a strategy that resembles breadth-first search, while setting it low makes it resemble best-first search. Our hypothesis was that the optimal value of Cp is problem-dependent. For that reason, we experimented with an oscillating Cp(i) = Cp,0 + a sin(2\u03c0 i T ), where i is the number of the current MCTS iteration, a is the oscillation amplitude and T is the oscillation period. Unfortunately, this did not yield a substantial improvement.\nWe were surprised to see that the best-node expansion strategy (see subsection 3.6) corresponding to the leanCoP cut strategy solved only 14 problems.\nWe tried random rewards in the range [0, 1], which proved exactly as many problems as constant reward. This is because the selection strategy calculates the average reward from all previous simulations, which for the random rewards converges to 0.5 for all nodes.\n(Schadd et al. 2012) suggests running MCTS multiple times with different random seeds to avoid MCTS getting stuck in local maxima. We tried this in combination with rerunning the same problem with multiple values of the exploration constant Cp for a shorter time, however, this did not achieve any improvement."}, {"heading": "6.5 MPTP2078", "text": "In addition to the problems in the MZR@Turing dataset, we also ran an evaluation on all MPTP2078 problems. Here, the single best lazyCoP strategy solves 468 problems, whereas monteCoP solves 296. Of these 296, 56 are unique, i.e. 12.0% of the problems lazyCoP solves."}, {"heading": "7. Conclusion", "text": "We described and implemented a connection prover based on Monte Carlo Tree Search with machine-learnt guidance heuristic. We evaluated it on two datasets, where on one dataset, our system was able to prove 17.9% new problems compared to a conventional connection prover. One can use our system as advisor in a theorem prover to choose between different possible proof steps. As future work, new prover state evaluation heuristics, e.g. using neural networks, might help to better estimate promising regions of the proof search."}], "references": [{"title": "Premise selection for mathematics by corpus analysis and kernel methods", "author": ["J. Alama", "D. K\u00fchlwein", "E. Tsivtsivadze", "J. Urban", "T. Heskes"], "venue": "CoRR, abs/1108.3446,", "citeRegEx": "Alama et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2011}, {"title": "Deduction - automated logic", "author": ["W. Bibel"], "venue": null, "citeRegEx": "Bibel.,? \\Q1993\\E", "shortCiteRegEx": "Bibel.", "year": 1993}, {"title": "Hammering towards QED", "author": ["J. Blanchette", "C. Kaliszyk", "L. Paulson", "J. Urban"], "venue": "Journal of Formalized Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "A learning-based fact selector for Isabelle/HOL", "author": ["J.C. Blanchette", "D. Greenaway", "C. Kaliszyk", "D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Premise selection and external provers for HOL4", "author": ["T. Gauthier", "C. Kaliszyk"], "venue": "In Certified Programs and Proofs (CPP\u201915), LNCS. Springer,", "citeRegEx": "Gauthier and Kaliszyk.,? \\Q2015\\E", "shortCiteRegEx": "Gauthier and Kaliszyk.", "year": 2015}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Korni\u0142owicz", "A. Naumowicz"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "HOL Light: A tutorial introduction", "author": ["J. Harrison"], "venue": "FMCAD, volume 1166 of LNCS,", "citeRegEx": "Harrison.,? \\Q1996\\E", "shortCiteRegEx": "Harrison.", "year": 1996}, {"title": "ParamILS: an automatic algorithm configuration framework", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2014\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2014}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Certified connection tableaux proofs for HOL Light and TPTP", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Proceedings of the 2015 Conference on Certified Programs and Proofs,", "citeRegEx": "Kaliszyk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "Bandit based MonteCarlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin,", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "Single-player Monte-Carlo tree search for SameGame", "author": ["M.P.D. Schadd", "M.H.M. Winands", "M.J.W. Tak", "J.W.H.M. Uiterwijk"], "venue": "Knowl.-Based Syst.,", "citeRegEx": "Schadd et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schadd et al\\.", "year": 2012}, {"title": "A brief overview of HOL4", "author": ["K. Slind", "M. Norrish"], "venue": "TPHOLs 2008,", "citeRegEx": "Slind and Norrish.,? \\Q2008\\E", "shortCiteRegEx": "Slind and Norrish.", "year": 2008}, {"title": "The 6th IJCAR automated theorem proving system competition - CASC-J6", "author": ["G. Sutcliffe"], "venue": "AI Commun.,", "citeRegEx": "Sutcliffe.,? \\Q2013\\E", "shortCiteRegEx": "Sutcliffe.", "year": 2013}, {"title": "MaLeCoP machine learning connection prover", "author": ["J. Urban", "J. Vyskocil", "P. Step\u00e1nek"], "venue": "Automated Reasoning with Analytic Tableaux and Related Methods - 20th International Conference,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}, {"title": "The Isabelle framework", "author": ["M. Wenzel", "L.C. Paulson", "T. Nipkow"], "venue": "TPHOLs, volume 5170 of LNCS,", "citeRegEx": "Wenzel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wenzel et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "2011), leanCoP (Otten 2008), and interactive theorem provers (ITPs) such as Isabelle (Wenzel et al. 2008), HOL4 (Slind and Norrish 2008), HOL Light (Harrison 1996), and Mizar (Grabowski et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 5, "context": "2008), HOL4 (Slind and Norrish 2008), HOL Light (Harrison 1996), and Mizar (Grabowski et al. 2010) have led to a significant improvement of the usability of ITPs (Blanchette et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 121}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 151}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 178}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 206}, {"referenceID": 8, "context": "We can also build on previous machine-learning extensions of leanCoP (Kaliszyk and Urban 2015b, Urban et al. (2011)).", "startOffset": 70, "endOffset": 116}, {"referenceID": 13, "context": "2016), as well as to single-player games such as SameGame (Schadd et al. 2012).", "startOffset": 58, "endOffset": 78}, {"referenceID": 13, "context": "A similar approach is taken in (Schadd et al. 2012).", "startOffset": 31, "endOffset": 51}, {"referenceID": 10, "context": "Furthermore, all provers below use the same proof format as the leanCoP tactic described in (Kaliszyk et al. 2015) to automatically find proofs for HOL Light, therefore adapting the HOL Light proof tactic to use our Monte Carlo system should be straightforward.", "startOffset": 92, "endOffset": 114}, {"referenceID": 0, "context": "All 1400 problems are taken from the MPTP2078 problem set (Alama et al. 2011), but sometimes augmented with facts that are not necessary for the proof to make proof attempts harder.", "startOffset": 58, "endOffset": 77}, {"referenceID": 7, "context": "On the parameter training set, we ran ParamILS (Hutter et al. 2009) with monteCoP, using a problem timeout of 2s.", "startOffset": 47, "endOffset": 67}, {"referenceID": 13, "context": "(Schadd et al. 2012) suggests running MCTS multiple times with different random seeds to avoid MCTS getting stuck in local maxima.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Monte Carlo Tree Search (MCTS) is a technique to guide search in a large decision space by taking random samples and evaluating their outcome. In this work, we study MCTS methods in the context of the connection calculus and implement them on top of the leanCoP prover. This includes proposing useful proof-state evaluation heuristics that are learned from previous proofs, and proposing and automatically improving suitable MCTS strategies in this context. The system is trained and evaluated on a large suite of related problems coming from the Mizar proof assistant, showing that it is capable to find new and different proofs. To our knowledge, this is the first time MCTS has been applied to theorem proving.", "creator": "LaTeX with hyperref package"}}}