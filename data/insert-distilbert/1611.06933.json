{"id": "1611.06933", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Unsupervised Learning for Lexicon-Based Classification", "abstract": "in lexicon - based classification, documents are easily assigned labels by comparing the number of words classified that appear from two opposed lexicons, such as positive and negative sentiment. creating such words lists is often easier than labeling particular instances, and they can be debugged by non - experts if classification performance is unsatisfactory. however, there is little analysis or justification of this classification heuristic. this paper describes exactly a set of assumptions that can be used to derive a robust probabilistic justification sufficient for lexicon - frequency based classification, as well as an analysis of its expected accuracy. one key assumption behind lexicon - based classification is that all words in one each lexicon are equally predictive. this implication is rarely true in practice, which is why lexicon - based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. this paper shows that it is possible better to learn such weights without labeled data, by leveraging co - occurrence statistics across the lexicons. this offers the best of both worlds : light supervision in the form of lexicons, and data - driven classification with higher accuracy than traditional word - counting heuristics.", "histories": [["v1", "Mon, 21 Nov 2016 18:30:17 GMT  (41kb)", "http://arxiv.org/abs/1611.06933v1", "to appear in AAAI 2017"]], "COMMENTS": "to appear in AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["jacob eisenstein"], "accepted": true, "id": "1611.06933"}, "pdf": {"name": "1611.06933.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning for Lexicon-Based Classification", "authors": ["Jacob Eisenstein"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n06 93\n3v 1\n[ cs\n.L G\n] 2\n1 N\nov 2"}, {"heading": "Introduction", "text": "Lexicon-based classification refers to a classification rule in which documents are assigned labels based on the count of words from lexicons associated with each label (Taboada et al. 2011). For example, suppose that we have opposed labels Y \u2208 {0, 1}, and we have associated lexicons W0 and W1. Then for a document with a vector of word counts x, the lexicon-based decision rule is,\n(1) \u2211\ni \u2208W0\nxi \u2277 \u2211\nj \u2208W1\nxj ,\nwhere the \u2277 operator indicates a decision rule. Put simply, the rule is to select the label whose lexicon matches the most word tokens.\nLexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010). The popularity of this approach can be explained by its relative simplicity and ease of use: for domain experts, creating lexicons is intuitive, and, in comparison with labeling instances, it may offer a faster path towards a reasonably\naccurate classifier (Settles 2011). Furthermore, classification errors can be iteratively debugged by refining the lexicons.\nHowever, from a machine learning perspective, there are a number of drawbacks to lexicon-based classification. First, while intuitively reasonable, lexicon-based classification lacks theoretical justification: it is not clear what conditions are necessary for it to work. Second, the lexicons may be incomplete, even for designers with strong substantive intuitions. Third, lexicon-based classification assigns an equal weight to each word, but some words may be more strongly predictive than others.1 Fourth, lexiconbased classification ignores multi-word phenomena, such as negation (e.g., not so good) and discourse (e.g., the movie would be watchable if it had better acting). Supervised classification systems, which are trained on labeled examples, tend to outperform lexicon-based classifiers, even without accounting for multi-word phenomena (Liu 2015; Pang and Lee 2008).\nSeveral researchers have proposed methods for lexicon expansion, automatically growing lexicons from an initial seed set (Hatzivassiloglou and McKeown 1997; Qiu et al. 2011). There is also work on handling multi-word phenomena such as negation (Wilson, Wiebe, and Hoffmann 2005; Polanyi and Zaenen 2006), and discourse (Somasundaran, Wiebe, and Ruppenhofer 2008; Bhatia, Ji, and Eisenstein 2015). However, the theoretical foundations of lexiconbased classification remain poorly understood, and we lack principled means for automatically assigning weights to lexicon items without resorting to labeled instances.\nThis paper elaborates a set of assumptions under which lexicon-based classification is equivalent to Na\u0131\u0308ve Bayes classification. I then derive expected error rates under these assumptions. These expected error rates are not matched by observations on real data, suggesting that the underlying assumptions are invalid. Of key importance is the assumption that each lexicon item is equally predictive. To relax this assumption, I derive a principled method for estimating word probabilities under each label, using a method-of-moments estimator on cross-lexical co-occurrence counts.\n1Some lexicons attach coarse-grained predefined weights to each word. For example, the OpinionFinder Subjectivity lexicon labels words as \u201cstrongly\u201d or \u201cweakly\u201d subjective (Wilson, Wiebe, and Hoffmann 2005). This poses an additional burden on the lexicon creator.\nOverall, this paper makes the following contributions:\n\u2022 justifying lexicon-based classification as a special case of multinomial Na\u0131\u0308ve Bayes;\n\u2022 mathematically analyzing this model to compute the expected performance of lexicon-based classifiers;\n\u2022 extending the model to justify a popular variant of lexicon-based classification, which incorporates word presence rather than raw counts;\n\u2022 deriving a method-of-moments estimator for the parameters of this model, enabling lexicon-based classification with unique weights per word, without labeled data;\n\u2022 empirically demonstrating that this classifier outperforms lexicon-based classification and alternative approaches."}, {"heading": "Lexicon-Based Classification as Na\u0131\u0308ve Bayes", "text": "I begin by showing how the lexicon-based classification rule shown in (1) can be derived as a special case of Na\u0131\u0308ve Bayes classification. Suppose we have a prior probability PY for the label Y , and a likelihood function PX|Y , where X is a random variable corresponding to a vector of word counts. The conditional label probability can be computed by Bayesian inversion,\n(2)P (y | x) = P (x | y)P (y)\u2211 y\u2032 P (x | y\u2032)P (y\u2032) .\nAssuming that the costs for each type of misclassification error are identical, then the minimum Bayes risk classification rule is,\n(3)log Pr(Y = 0) + logP (x | Y = 0) \u2277 log Pr(Y = 1) + logP (x | Y = 1),\nmoving to the log domain for simplicity of notation. I now show that lexicon-based classification can be justified under this decision rule, given a set of assumptions about the probability distributions.\nLet us introduce some assumptions about the likelihood function, PX|Y . The random variable X is defined over vectors of counts, so a natural choice for the form of this likelihood is the multinomial distribution, corresponding to a multinomial Na\u0131\u0308ve Bayes classifier. For a specific vector of counts X = x, write P (x | y) , Pmultinomial(x; \u03b8y, N), where \u03b8y is a probability vector associated with label y, and N = \u2211V\ni=1 xi is the total count of tokens in x, and xi is the count of word i \u2208 {1, 2, . . . , V }. The multinomial likelihood is proportional to a product of likelihoods of categorical variables corresponding to individual words (tokens),\nPr(W = i | Y = y; \u03b8) = \u03b8y,i, (4)\nwhere the random variable W corresponds to a single token, whose probability of being word type i is equal to \u03b8y,i in a document with label y. The multinomial log-likelihood can be written as,\nlogP (x | y) = logPmultinomial(x; \u03b8y, N)\n=K(x) +\nV \u2211\ni=1\nxi log Pr(W = i | Y = y; \u03b8)\n=K(x) +\nV \u2211\ni=1\nxi log \u03b8y,i, (5)\nwhere K(x) is a function of x that is constant in y. The first necessary assumption about the likelihood function is that the lexicons are complete: words that are in neither lexicon have identical probability under both labels. Formally, for any word i /\u2208 W0 \u222aW1, we assume,\nPr(W = i | Y = 0) = Pr(W = i | Y = 1), (6)\nwhich implies that these words are irrelevant to the classification boundary.\nNext, we must assume that each in-lexicon word is equally predictive. Specifically, for words that are in lexicon y,\nPr(W = i | Y = y) Pr(W = i | Y = \u00acy) = 1 + \u03b3 1\u2212 \u03b3 , (7)\nwhere \u00acy is the opposite label from y. The parameter \u03b3 controls the predictiveness of the lexicon: for example, if \u03b3 = 0.5 in a sentiment classification problem, this would indicate that words in the positive sentiment lexicon are three times more likely to appear in documents with positive sentiment than in documents with negative sentiment, and vice versa. The word atrocious might be less likely overall than good, but still three times more likely in the negative class than in the positive class. In the limit, \u03b3 = 0 implies that the lexicons do not distinguish the classes at all, and \u03b3 = 1 implies that the lexicons distinguish the classes perfectly, so that the observation of a single in-lexicon word would completely determine the document label.\nThe conditions enumerated in (6) and (7) are ensured by the following definition,\n\u03b8y,i =\n\n\n (1 + \u03b3)\u00b5i, i \u2208 Wy (1\u2212 \u03b3)\u00b5i, i \u2208 W\u00acy \u00b5i, i /\u2208 Wy \u222aW\u00acy,\n(8)\nwhere \u00acy is the opposite label from y, and \u00b5 is a vector of baseline probabilities, which are independent of the label.\nBecause the probability vectors \u03b80 and \u03b81 must each sum to one, we require an assumption of equal coverage,\n\u2211\ni\u2208W0\n\u00b5i = \u2211\nj\u2208W1\n\u00b5j . (9)\nFinally, assume that the labels have equal prior likelihood, Pr(Y = 0) = Pr(Y = 1). It is trivial to relax this assumption by adding a constant term to one side of the decision rule in (1).\nWith these assumptions in hand, it is now possible to simplify the decision rule in (3). Thanks to the assumption of\nequal prior probability, we can drop the priors P (Y ), so that the decision rule is a comparison of the likelihoods,\nlogP (x | Y = 0) \u2277 logP (x | Y = 1) (10) K(x) + \u2211\ni\nxi log \u03b80,i \u2277 K(x) + \u2211\ni\nxi log \u03b81,i. (11)\nCanceling K(x) and applying the definition from (8),\n(12)\n\u2211\ni \u2208W0\nxi log((1 + \u03b3)\u00b5i) + \u2211\ni \u2208W1\nxi log((1\u2212 \u03b3)\u00b5i)\n\u2277 \u2211\ni \u2208W0\nxi log((1 \u2212 \u03b3)\u00b5i) + \u2211\ni \u2208W1\nxi log((1 + \u03b3)\u00b5i).\nThe \u00b5i terms cancel after distributing the log, leaving, \u2211\ni\u2208W0\nxi log 1 + \u03b3 1\u2212 \u03b3 \u2277 \u2211\ni\u2208W1\nxi log 1 + \u03b3\n1\u2212 \u03b3 . (13)\nFor any \u03b3 \u2208 (0, 1), the term log 1+\u03b31\u2212\u03b3 is a finite and positive constant. Therefore, (13) is identical to the counting-based classification rule in (1). In other words, lexicon-based classification is minimum Bayes risk classification in a multinomial probability model, under the assumptions of equal prior likelihood, lexicon completeness, equal predictiveness of words, and equal coverage."}, {"heading": "Analysis of Lexicon-Based Classification", "text": "One advantage of deriving a formal foundation for lexiconbased classification is that it is possible to analyze its expected performance. For a label y, let us write the count of in-lexicon words as my = \u2211\ni\u2208Wy xi, and the count of\nopposite-lexicon words as m\u00acy = \u2211\ni\u2208W\u00acy xi. Lexicon-\nbased classification makes a correct prediction whenever my > m\u00acy for the correct label y. To assess the likelihood that my > m\u00acy , it is sufficient to compute the expectation and variance of the difference my \u2212m\u00acy; under the central limit theorem, we can treat this difference as approximately normally distributed, and compute the probability that the difference is positive using the Gaussian cumulative distribution function (CDF).\nLet us use the convenience notation s\u00b5,\ns\u00b5 , \u2211\ni\u2208W0\n\u00b5i = \u2211\ni\u2208W1\n\u00b5i. (14)\nRecall that we have already taken the assumption that the sums of baseline word probabilities for the two lexicons are equal. Under the multinomial probability model, given a document with N tokens, the expected counts are,\nE[my] =N \u2211\ni\u2208Wy\n\u03b8y,i = N(1 + \u03b3)s\u00b5 (15)\nE[m\u00acy] =N \u2211\ni\u2208W\u00acy\n\u03b8\u00acy,i = N(1\u2212 \u03b3)s\u00b5 (16)\nE[my \u2212m\u00acy] =2N\u03b3s\u00b5. (17) Next we compute the variance of this margin,\nV [my \u2212m\u00acy] =V [my] + V [m\u00acy] + Cov(my ,m\u00acy). (18)\nEach of these terms is the variance of a sum of counts. Under the multinomial distribution, the variance of a single count is V [xi] = N\u03b8i(1\u2212 \u03b8i). The variance of the sum my is,\nV [my] = \u2211\ni\u2208Wy\nN\u03b8i(1\u2212 \u03b8i)\u2212 \u2211\nj\u2208Wy,j 6=i\nN\u03b8i\u03b8j\n= \u2211\ni\u2208Wy\nN\u03b8i \u2212N\u03b82i \u2212 \u2211\nj\u2208Wy ,j 6=i\nN\u03b8i\u03b8j (19)\n\u2264N \u2211\ni\u2208Wy\n\u03b8i = N \u2211\ni\u2208Wy\n(1 + \u03b3)\u00b5i\n=N(1 + \u03b3)s\u00b5. (20)\nAn equivalent upper bound can be computed for the variance of the count of opposite lexicon words,\nV [m\u00acy] \u2264 N(1\u2212 \u03b3)s\u00b5. (21) These bounds are fairly tight because the products of probabilities \u03b82i and \u03b8i\u03b8j are nearly always small, due to the fact that most words are rare. Because the covariance Cov(my,m\u00acy) is negative (and also involves a product of word probabilities), we can further bound the variance of the margin, obtaining the upper bound,\nV [my\u2212m\u00acy] \u2264 N(1+\u03b3)s\u00b5+N(1\u2212\u03b3)s\u00b5 = 2Ns\u00b5. (22)\nBy the central limit theorem, the margin my \u2212 m\u00acy is approximately normally distributed, with mean 2N\u03b3s\u00b5 and variance upper-bounded by 2Ns\u00b5. The probability of making a correct prediction (which occurs when my > m\u00acy) is then equal to the cumulative density of a standard normal distribution \u03a6(z), where the z-score is equal to the ratio of the expectation and the standard deviation,\nz = E[my \u2212m\u00acy] \u221a V [my \u2212m\u00acy] \u2265 2N\u03b3s\u00b5\u221a 2Ns\u00b5 = \u03b3 \u221a 2Ns\u00b5. (23)\nNote that by upper-bounding the variance, we obtain a lower bound on the z-score, and thus a lower bound on the expected accuracy.\nAccording to this approximation, accuracy is expected to increase with the predictiveness \u03b3, the document length N , and the lexicon coverage s\u00b5. This helps to explain a dilemma in lexicon design: as more words are added, the coverage increases, but the average predictiveness of each word decreases (assuming the most predictive words are added first). Thus, increasing the size of a lexicon by adding marginal words may not improve performance.\nThe analysis also predicts that longer documents should be easier to classify. This is because the expected size of the gap my \u2212 m\u00acy grows with N , while its standard deviation grows only with \u221a N . This prediction can be tested empirically, and on all four datasets considered in this paper, it is false: longer documents are harder to classify accurately. This is a clue that the underlying assumptions are not valid. The decreased accuracy for especially long reviews may be due to these reviews being more complex, perhaps requiring modeling of the discourse structure (Somasundaran, Wiebe, and Ruppenhofer 2008)."}, {"heading": "Justifying the Word-Appearance Heuristic", "text": "An alternative heuristic to lexicon-based classification is to consider only the presence of each word type, and not its count. This corresponds to the decision rule,\n(24) \u2211\ni \u2208W0\n\u03b4(xi > 0) \u2277 \u2211\nj \u2208W1\n\u03b4(xj > 0),\nwhere \u03b4(\u00b7) is a delta function that returns one if the Boolean condition is true, and zero otherwise. In the context of supervised classification, Pang, Lee, and Vaithyanathan (2002) find that word presence is a more predictive feature than word frequency. By ignoring repeated mentions of the same word, heuristic (24) emphasizes the diversity of ways in which a document covers a lexicon, and is more robust to document-specific idiosyncrasies \u2014 such as a review of The Joy Luck Club, which might include the positive words joy and luck many times even if the review is negative.\nThe word-appearance heuristic can also be explained in the framework defined above. The multinomial likelihood PX|Y can be replaced by a Dirichlet-compound multinomial (DCM) distribution, also known as a multivariate Polya distribution (Madsen, Kauchak, and Elkan 2005). This distribution is written Pdcm(x;\u03b1y), where \u03b1y is a vector of parameters associated with label y, with \u03b1y,i > 0 for all i \u2208 {1, 2, . . . , V }. The DCM is a \u201ccompound\u201d distribution because it treats the parameter of the multinomial as a latent variable to be marginalized out,\nPdcm(x;\u03b1y) =\n\u222b\n\u03bd\nPmultinomial(x | \u03bd)PDirichlet(\u03bd | \u03b1y)d\u03bd. (25)\nIntuitively, one can think of the DCM distribution as encoding a model in which each document has its own multinomial distribution over words; this document-specific distribution is itself drawn from a prior that depends on the class label y.\nSuppose we set the DCM parameter \u03b1 = \u03c4\u03b8, with \u03b8 as defined in (8). The constant \u03c4 > 0 is then the concentration of the distribution: as \u03c4 grows, the probability distribution over \u03b1 is more closely concentrated around the prior expectation \u03b8. Because \u2211V\ni \u03b8i = 1, the likelihood function under this model is,\nPdcm(x | y) = \u0393(\u03c4)\n\u0393(N + \u03c4)\n\u220f\ni\n\u0393(xi + \u03c4\u03b8y,i)\n\u0393(\u03c4\u03b8y,i) , (26)\nwhere \u0393(\u00b7) is the gamma function. Minimum Bayes risk classification in this model implies the decision rule:\n\u2211\ni\u2208W0\nlog rin(xi)\nrout(xi) \u2277\n\u2211\ni\u2208W1\nlog rin(xt,i)\nrout(xi) (27)\nwhere,\nrin(xi) , \u0393(xi + \u03c4(1 + \u03b3)\u00b5i)\n\u0393(\u03c4(1 + \u03b3)\u00b5i) (28)\nrout(xi) , \u0393(xi + \u03c4(1 \u2212 \u03b3)\u00b5i)\n\u0393(\u03c4(1 \u2212 \u03b3)\u00b5i) . (29)\nAs \u03c4 \u2192 \u221e, the prior on \u03bd is tightly linked to \u03b8, so that the model reduces to the multinomial defined above. Another\nway to see this is to apply the equality \u0393(x + 1) = x\u0393(x) to (28) and (29) when \u03c4\u00b5i \u226b xi. As \u03c4 \u2192 0, the prior on \u03bd becomes increasingly diffuse. Repeated counts of any word are better explained by document-specific variation from the prior, than by properties of the label. This situation is shown in Figure 1, which plots the \u201ceffective counts\u201d implied by the classification rule (27) for a range of values of the concentration parameter \u03c4 , holding the other parameters constant (\u00b5 = 10\u22123, \u03b3 = 0.5). For high values of \u03c4 , the effective counts track the observed counts linearly, as in the multinomial model; for low values of \u03c4 , the effective counts barely increase beyond 1.\nMinka (2012) presents a number of estimators for the concentration parameter \u03c4 from a corpus of text. When the label y is unknown, we cannot apply these estimators directly. However, as described above, out-of-lexicon words are assumed to have identical probability under both labels. This assumption can be exploited to estimate \u03c4 exclusively from the first and second moments of these out-of-lexicon words. Analysis of the expected accuracy of this model is left to future work."}, {"heading": "Estimating Word Predictiveness", "text": "A crucial simplification made by lexicon-based classification is that all words in each lexicon are equally predictive. In reality, words may be more or less predictive of class labels, for reasons such as sense ambiguity (e.g., well) and degree (e.g., good vs flawless). By introducing a per-word predictiveness factor \u03b3i into (8), we arrive at a model that is a restricted form of Na\u0131\u0308ve Bayes. (The restriction is that the probabilities of non-lexicon words are constrained to be identical across classes.) If labeled data were available, this model could be estimated by maximum likelihood. This section shows how to estimate the model without labeled data, using the method of moments.\nFirst, note that the baseline probabilities \u00b5i can be estimated directly from counts on an unlabeled corpus; the challenge is to estimate the parameters \u03b3i for all words in the two lexicons. The key intuition that makes this possible is that highly predictive words should rarely appear with words in the opposite lexicon. This idea can be formalized\nin terms of cross-label counts: the cross-label count ci is the co-occurrence count of word i with all words in the opposite lexicon,\nci = T \u2211\nt=1\n\u2211\nj\u2208W\u00acy\nx (t) i x (t) j , (30)\nwhere x(t) is the vector of word counts for document t, with t \u2208 {1 . . . T }. Under the multinomial model defined above, for a single document with N tokens, the expected product of counts for a word pair (i, j) is equal to,\nE[xixj ] =E[xi]E[xj ] + Cov(xi, xj)\n=N\u03b8iN\u03b8j \u2212N\u03b8i\u03b8j =N(N \u2212 1)\u03b8i\u03b8j . (31)\nLet us focus on the expected products of counts for crosslexicon word pairs (i \u2208 W0, j \u2208 W1). The parameter \u03b8 depends on the document label y, as defined in (8). As a result, we have the following expectations,\nE[xixj | Y = 0] =N(N \u2212 1)\u00b5i(1 + \u03b3i)\u00b5j(1\u2212 \u03b3j)\n=N(N \u2212 1)\u00b5i\u00b5j(1 + \u03b3i \u2212 \u03b3j \u2212 \u03b3i\u03b3j) (32)\nE[xixj | Y = 1] =N(N \u2212 1)\u00b5i(1\u2212 \u03b3i)\u00b5j(1 + \u03b3j)\n=N(N \u2212 1)\u00b5i\u00b5j(1\u2212 \u03b3i + \u03b3j \u2212 \u03b3i\u03b3j) (33)\nE[xixj ] =P (Y = 0)E[xixj | Y = 0]\n+ P (Y = 1)E[xixj | Y = 1]\n=N(N \u2212 1)\u00b5i\u00b5j(1\u2212 \u03b3i\u03b3j). (34)\nSumming over all words j \u2208 W1 and all documents t,\n(35)\nE[ci] =\nT \u2211\nt=1\n\u2211\nj\u2208W1\nE[x (t) i x (t) j ]\n=\nT \u2211\nt=1\nNt(Nt \u2212 1)\u00b5i \u2211\nj\u2208W1\n\u00b5j(1\u2212 \u03b3i\u03b3j)\nLet us write \u03b3(1) to indicate the vector of \u03b3j parameters for all j \u2208 W1, and \u03b3(0) for all i \u2208 W0. The expectation in (35) is a linear function of \u03b3i, and a linear function of the vector \u03b3(1). Analogously, for all j \u2208 W1, E[cj ] is a linear function of \u03b3j and \u03b3(1). Our goal is to choose \u03b3 so that the expectations E[ci] closely match the observed counts ci. This can be viewed as form of method of moments estimation, with the following objective,\nJ = 1\n2\n\u2211\ni\u2208W0\n(ci \u2212 E[ci])2 + 1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ])2, (36)\nwhich can be minimized in terms of \u03b3(0) and \u03b3(1). However, there is an additional constraint: the probability distributions \u03b80 and \u03b81 must still sum to one. We can express this as a linear constraint on \u03b3(0) and \u03b3(1),\n\u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) = 0, (37)\nwhere \u00b5(y) is the vector of baseline probabilities for words i \u2208 Wy , and \u00b5(0) \u00b7 \u03b3(0) indicates a dot product.\nWe therefore formulate the following constrained optimization problem,\nmin \u03b3(0),\u03b3(1)\n1\n2\n\u2211\ni\u2208W0\n(ci \u2212 E[ci])2 + 1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ])2\ns.t. \u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) = 0 \u2200i \u2208 (W0 \u222aW1), \u03b3i \u2208 [0, 1). (38)\nThis problem can be solved by alternating direction method of multipliers (Boyd et al. 2011). The equality constraint can be incorporated into an augmented Lagrangian,\nL\u03c1(\u03b3 (0),\u03b3(1)) =\n1\n2\n\u2211\ni\u2208W0\n(ci \u2212 E[ci])2 + 1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ])2\n+ \u03c1\n2 (\u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1))2, (39)\nwhere \u03c1 > 0 is the penalty parameter. The augmented Lagrangian is biconvex in \u03b3(0) and \u03b3(1), which suggests an iterative solution (Boyd et al. 2011, page 76). Specifically, we hold \u03b3(1) fixed and solve for \u03b3(0), subject to \u03b3i \u2208 [0, 1) for all i \u2208 W0. We then solve for \u03b3(1) under the same conditions. Finally, we update a dual variable u, representing the extent to which the equality constraint is violated. These updates are iterated until convergence. The unconstrained local updates to \u03b3(0) and \u03b3(1) can be computed by solving a system of linear equations, and the result can be projected back onto the feasible region. The penalty parameter \u03c1 is initialized at 1, and then dynamically updated based on the primal and dual residuals (Boyd et al. 2011, pages 20-21). More details are available in the appendix, and in the online source code.2"}, {"heading": "Evaluation", "text": "An empirical evaluation is performed on four datasets in two languages. All datasets involve binary classification problems, and performance is quantified by the area-under-thecurve (AUC), a measure of classification performance that is robust to unbalanced class distributions. A perfect classifier achieves AUC = 1; in expectation, a random decision rule gives AUC = 0.5.\nDatasets The proposed method relies on co-occurrence counts, and therefore is best suited to documents containing at least a few sentences each. With this in mind, the following datasets are used in the evaluation: Amazon English-language product reviews across four do-\nmains; of these reviews, 8000 are labeled and another 19677 are unlabeled (Blitzer, Dredze, and Pereira 2007).\nCornell 2000 English-language film reviews (version 2.0), labeled as positive or negative (Pang and Lee 2004).\nCorpusCine 3800 Spanish-language movie reviews, rated on a scale of one to five (Vilares, Alonson, and Go\u0301mezRodr\u0131\u0301guez 2015). Ratings of four or five are considered as positive; ratings of one and two are considered as negative. Reviews with a rating of three are excluded. 2https://github.com/jacobeisenstein/ probabilistic-lexicon-classification\nIMDB 50,000 English-language film reviews (Maas et al. 2011). This evaluation includes only the test set of 25,000 reviews, of which half are positive and half are negative.\nLexicons Preliminary evaluation compared several English-language sentiment lexicons. The Liu lexicon (Liu 2015) consistently obtained the best performance on all three English-language datasets, so it was made the focus of all subsequent experiments. Ribeiro et al. (2016) also found that the Liu lexicon is one of the strongest lexicons for review analysis. For the Spanish data, the ISOL lexicon was used (Molina-Gonza\u0301lez et al. 2013). It is a modified translation of the Liu lexicon.\nClassifiers The evaluation compares the following unsupervised classification strategies: LEXICON basic word counting, as in decision rule (1); LEX-PRESENCE counting word presence rather than fre-\nquency, as in decision rule (24);\nPROBLEX-MULT probabilistic lexicon-based classification, as proposed in this paper, using the multinomial likelihood model;\nPROBLEX-DCM probabilistic lexicon-based classification, using the Dirichlet Compound Multinomial likelihood to reduce effective counts for repeated words;\nPMI An alternative approach, discussed in the related work, is to impute document labels from a seed set of words, and then compute \u201csentiment scores\u201d for individual words from pointwise mutual information between the words and imputed labels (Turney 2002). The implementation of this method is based on the description from Kiritchenko, Zhu, and Mohammad (2014), using the lexicons as the seed word sets. As an upper bound, a supervised logistic regression classifier is also considered. This classifier is trained using fivefold cross validation. It is the only classifier with access to training data. For the PROBLEX-MULT and PROBLEXDCM methods, lexicon words which co-occur with the opposite lexicon at greater than chance frequency are eliminated from the lexicon in a preprocessing step.\nResults Results are shown in Table 1. The superior performance of the logistic regression classifier confirms the principle that supervised classification is far more accurate than lexicon-based classification. Therefore, supervised classification should be preferred when labeled data is available. Nonetheless, the probabilistic lexicon-based classifiers developed in this paper (PROBLEX-MULT and PROBLEXDCM) go a considerable way towards closing the gap, with improvements in AUC ranging from less than 1% on the CorpusCine data to nearly 8% on the IMDB data. The PMI approach performs poorly, improving over the simpler lexicon-based classifiers on only one of the four datasets. The word presence heuristic offers no consistent improvements, and the Bayesian adjustment to the classification rule (PROBLEX-DCM) offers only modest improvements on two of the four datasets."}, {"heading": "Related work", "text": "Turney (2002) uses pointwise mutual information to estimate the \u201csemantic orientation\u201d of all vocabulary words from co-occurrence with a small seed set. This approach has later been extended to the social media domain by using emoticons as the seed set (Kiritchenko, Zhu, and Mohammad 2014). Like the approach proposed here, the basic intuition is to leverage co-occurrence statistics to learn weights for individual words; however, PMI is a heuristic score that is not justified by a probabilistic model of the text classification problem. PMI-based classification underperforms PROBLEX-MULT and PROBLEX-DCM on all four datasets in our evaluation.\nThe method-of-moments has become an increasingly popular estimator in unsupervised machine learning, with applications in topic models (Anandkumar et al. 2014), sequence models (Hsu, Kakade, and Zhang 2012), and more elaborate linguistic structures (Cohen et al. 2014). Of particular relevance are \u201canchor word\u201d techniques for learning latent topic models (Arora, Ge, and Moitra 2012). In these methods, each topic is defined first by a few keywords, which are assumed to be generated only from a single topic. From these anchor words and co-occurrence statistics, the topic-word probabilities can be recovered. A key difference is that the strong anchor word assumption is not required in this work: none of the words are assumed to be perfectly predictive of either label. We require only the much weaker assumption that words in a lexicon tend to co-occur less frequently with words in the opposite lexicon."}, {"heading": "Conclusion", "text": "Lexicon-based classification is a popular heuristic that has not previously been analyzed from a machine learning perspective. This analysis yields two techniques for improving unsupervised binary classification: a method-of-moments estimator for word predictiveness, and a Bayesian adjustment for repeated counts of the same word. The method-ofmoments estimator yields substantially better performance than conventional lexicon-based classification, without requiring any additional annotation effort. Future work will consider the generalization to multi-class classification, and more ambitiously, the extension to multiword units.\nAcknowledgment This research was supported by the National Institutes of Health under award number R01GM112697-01, and by the Air Force Office of Scientific Research."}, {"heading": "Supplementary Material: Estimation Details", "text": "This supplement describes the estimation procedure in more detail. The paper uses the method of moments to derive the following optimization problem,\nmin \u03b3(0),\u03b3(1)\n1\n2\n\u2211\ni\u2208W0\n(ci \u2212 E[ci])2 + 1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ])2\ns.t. \u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) = 0 \u2200i\u2208W00 \u2264 \u03b3 (0) i < 1\n\u2200j\u2208W10 \u2264 \u03b3 (1) j < 1. (40)\nThis problem is biconvex in the parameters \u03b3(0),\u03b3(1). We optimize using the alternating direction method of multipliers (ADMM; Boyd et al. 2011). In the remainder of this document, x \u00b7 y is used to indicate a dot product between x and y, and x\u2299 y is used to indicate an elementwise product."}, {"heading": "ADMM for biconvex problems", "text": "In general, suppose that the function F (x, z) is biconvex in x and z, and that the constraint G(x, z) = 0 is affine in x and z,\nmin x,z F (x, z) (41)\ns.t.G(x, z) = 0. (42)\nWe can optimize via ADMM by the following updates (Boyd et al 2011, section 9.2),\nxk+1 \u2190argminxF (x, z) + (\u03c1/2)||G(x, zk) + uk||22 (43) zk+1 \u2190argminzF (x, z) + (\u03c1/2)||G(xk+1, z) + uk||22\n(44)\nuk+1 \u2190uk +G(xk+1, zk+1). (45)\nNow suppose we have a more general constrained optimization problem,\nmin x,z F (x, z) (46)\ns.t. G(x, z) = 0\nx \u2208 Cx z \u2208 Cz,\nwhere Cx and Cz are convex sets. We can solve via the updates,\nxk+1 \u2190argminx\u2208CxF (x, z) + (\u03c1/2)||G(x, z k) + uk||22 (47) zk+1 \u2190argminz\u2208CzF (x, z) + (\u03c1/2)||G(x k+1, z) + uk||22 (48) uk+1 \u2190uk +G(xk+1, zk+1), (49)\nwhere u is a dual variable and \u03c1 > 0 is a hyperparameter."}, {"heading": "Application to moment-matching", "text": "In the application to moment-matching estimation, we have:\nx ,\u03b3(0) (50)\nz ,\u03b3(1) (51)\nG(x, z) ,\u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) (52) Cx = Cz ,[0, 1) (53)\nF (x, z) , 1\n2\n\u2211\ni\u2208W0\n(ci \u2212 E[ci])2 + 1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ])2\n(54)\nE[ci] = \u2211\nj\u2208W1\nE[ci,j ] = s\u00b5i \u2211\nj\u2208W1\n\u00b5j(1\u2212 \u03b3i\u03b3j)\n=s\u00b5i \u2211\nj\u2208W1\n\u00b5j \u2212 s\u00b5i\u03b3i \u2211\nj\u2208W1\n\u00b5j\u03b3j (55)\nE[cj ] = \u2211\ni\u2208W0\nE[ci,j ] = s\u00b5j \u2211\ni\u2208W0\n\u00b5i(1\u2212 \u03b3i\u03b3j)\n=s\u00b5j \u2211\ni\u2208W0\n\u00b5i \u2212 s\u00b5j\u03b3j \u2211\ni\u2208W0\n\u00b5i\u03b3i (56)\ns , \u2211\nt\nNt(Nt \u2212 1). (57)\nWe now consider how to perform the updates to xk+1 as a quadratic closed-form expression (an identical derivation applies to zk+1). Specifically, if the overall objective for x can be written in the form,\nJ(x) = 1\n2 xTPx+ q \u00b7 x+ r, (58)\nthen the optimal value of x is found at,\nx\u0302 =\u2212 P\u22121q. (59) We will obtain this form by converting the objective F and the tersm relating to the equality constraint G the boundary constraint H into quadratic forms.\nObjective We define helper notation,\nri =ci \u2212 s\u00b5i \u2211\nj\u2208W1\n\u00b5j (60)\nrj =cj \u2212 s\u00b5j \u2211\ni\u2208W0\n\u00b5i, (61)\nrepresenting the residuals from a model in which \u03b3i = \u03b3j = 0 for all i and j. Using these residuals, we rewrite the objective from Equation 54,\nF (\u03b3(0),\u03b3(1)) = 1\n2\n\u2211\ni\u2208W0\n(ci \u2212 E[ci]) 2 +\n1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ]) 2\n(62)\n= 1\n2\n\u2211\ni\u2208W0\n(ri + s\u00b5i\n(\n\u2211\nj\u2208W1\n\u00b5j\u03b3j\n)\n\u03b3i) 2\n+ 1\n2\n\u2211\nj\u2208W1\n(rj + s\u00b5j\n(\n\u2211\ni\u2208W0\n\u00b5i\u03b3i\n)\n\u03b3j) 2. (63)\nSolving first for \u03b3(0), we can rewrite the left term as a quadratic function,\n1\n2\n\u2211\ni\u2208W0\n(ci \u2212E[ci]) 2 =\n1 2 (\u03b3(0))TP0\u03b3 (0) + q0 \u00b7 \u03b3 (0) + 1 2 \u2211\ni\u2208W0\nr2i\n(64)\n(P0)ii =(s 2( \u2211\nj\u2208W1\n\u00b5j\u03b3j)\u00b5 2 i ) (65)\n(P0)i6=j =0 (66)\nq0 =s( \u2211\nj\u2208W1\n\u00b5j\u03b3j)(r (0) \u2299 \u00b5(0)), (67)\nwhere the matrix P0 is diagonal. We can also rewrite the second term as a quadratic function of \u03b3(0),\n1\n2\n\u2211\nj\u2208W1\n(cj \u2212 E[cj ]) 2 =\n1 2 (\u03b3(0))TP1\u03b3 (0) + q1 \u00b7 \u03b3 (0) + 1 2 \u2211\nj\u2208W1\nr2j\n(68)\nP1 =s 2( \u2211\nj\n\u00b52j\u03b3 2 j )\u00b5 (0)(\u00b5(0))T (69)\nq1 =s( \u2211\nj\nrj\u00b5j\u03b3j)\u00b5 (0), (70)\nwhere the matrix P1 is rank one. To summarize the terms from the objective,\nPF =Diag\n( s2 [ \u2211\nj\u2208W1\n\u00b5j\u03b3j\n]\n\u00b5 (0) \u2299 \u00b5(0)\n)\n+ s2( \u2211\nj\u2208W1\n\u00b52j\u03b3 2 j )\u00b5 (0)(\u00b5(0))T (71)\nqF =s( \u2211\nj\u2208W1\n\u00b5j\u03b3j)(r (0) \u2299 \u00b5(0)) + s(\n\u2211\nj\u2208W1\nrj\u00b5j\u03b3j)\u00b5 (0) (72)\nWe get an analogous set of terms when solving for \u03b3(1), meaning that we can use the same code, with a change over arguments.\nEquality constraint The constraint G requires that equal weight be assigned to the two lexicons,\nG(\u03b3(0),\u03b3(1)) =\u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) (73) Thus, the augmented Lagrangian term (\u03c1/2)||G(\u03b3(0),\u03b3(1)) + uk||22 can be written as a quadratic function of \u03b3(0),\n(\u03c1/2)||G(\u03b3(0),\u03b3(1)) + uk||22 = (\u03c1/2)(\u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) + uk)2 (74)\n= 1\n2 (\u03b3(0))TPG\u03b3 (0) + qG \u00b7 \u03b3(0) + . . . . (75)\nThis quadratic form for \u03b3(0) has the parameters,\nP (0) G =\u03c1\u00b5 (0)(\u00b5(0))T (76)\nq (0) G =\u03c1(u k \u2212 \u00b5(1) \u00b7 \u03b3(1))\u00b5(0). (77)\nWhen solving for \u03b3(1), we have,\n(\u03c1/2)||G(\u03b3(0),\u03b3(1)) + uk||22 (78) = (\u03c1/2)(\u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1) + uk)2 (79)\n= 1\n2 (\u03b3(1))TP (1) G \u03b3 (1) + q (1) G \u00b7 \u03b3(1) + . . . , (80)\nso that,\nP (1) G =\u03c1\u00b5 (1)(\u00b5(1))T (81)\nq (1) G =\u2212 \u03c1(uk + \u00b5(0) \u00b7 \u03b3(0))\u00b5(1) (82)\n=\u03c1(\u2212uk \u2212 \u00b5(0) \u00b7 \u03b3(0))\u00b5(1), (83)\nmeaning that we can use the same code, but plug in \u2212uk instead of uk.\nUnconstrained solution The augmented Lagrangian for \u03b3(0) can be written as,\nJ(\u03b3(0)) = 1\n2 (\u03b3(0))TP (0)\u03b3(0) + q(0) \u00b7 \u03b3(0) + r (84)\nP (0) =P (0) diag + P (0) low-rank (85)\nP (0) Diag =Diag\n\ns2\n\n\n\u2211\nj\u2208W1\n\u00b5j\u03b3j\n \u00b5(0) \u2299 \u00b5(0)   (86)\nP (0) Low-rank =(s 2( \u2211\nj\u2208W1\n\u00b52j\u03b3 2 j ) + \u03c1)\u00b5 (0)(\u00b5(0))T (87)\nq(0) =s( \u2211\nj\u2208W1\n\u00b5j\u03b3j)(r (0) \u2299 \u00b5(0))\n+ s( \u2211\nj\u2208W1\nrj\u00b5j\u03b3j)\u00b5 (0)\n+ \u03c1(uk \u2212 \u00b5(1) \u00b7 \u03b3(1))\u00b5(0) (88)\nIgnoring the constraint set Cx, the solution for \u03b3(0) is given by,\n\u03b3(0) \u2190\u2212 (P (0)diag + P (0) low-rank) \u22121q(0). (89)\nThe solution can be computed using the Woodbury identity.\nConstrained solution Each update to \u03b3(0) and \u03b3(1) must lie within the constraint sets Cx and Cz . One way to ensure this is to apply boundary-constrained L-BFGS to the augmented Lagrangian in Equation 84. This solution requires the gradient, which is simply P\u03b3(0) + q(0). A slightly faster (and more general) solution is to apply ADMM again, using the following iterative updates (Boyd et al. 2011, page 33):\n\u03b3 (0) \u2190\u2212 (P (0) diag + P (0) low-rank + \u03c12I) \u22121(q(0) + \u03c12(v \u2212 a)) (90)\na \u2190\u03a0Cx(\u03b3 (0)) (91)\nv \u2190v + \u03b3(0) \u2212 a, (92)\nwhere \u03a0Cx projects on to the set Cx, and v is an additional dual variable. This requires only a minor change to the quadratic solution in Equation 89: we add \u03c12 to the diagonal of P , and we add \u03c12(v \u2212 a) to the vector q.\nAlgorithm 1 ADMM optimization for unsupervised lexicon-based classification\nwhile global primal and dual residuals are above threshold do\nP (0), q(0) \u2190 ComputeQuadraticForm(\u03b3(0),\u03b3(1), u) a \u2190 0, v \u2190 0 while local primal and dual residuals are above thresh-\nold do \u03b3(0) \u2190 (P (0) + \u03c12I)\u22121(q(0) + \u03c12(v \u2212 a)) a \u2190 \u03a0Cx(\u03b3(0) + v) v \u2190 v + \u03b3(0) \u2212 a\n\u03b3(0) \u2190 \u03a0Cx(\u03b3(0)) P (1), q(1) \u2190 ComputeQuadraticForm(\u03b3(1),\u03b3(0),\u2212u) b \u2190 0, w \u2190 0 while local primal and dual residuals are above thresh-\nold do \u03b3(1) \u2190 (P (1) + \u03c12I)\u22121(q(1) + \u03c12(w \u2212 b)) b \u2190 \u03a0Cz (\u03b3(1) + w) w \u2190 w + \u03b3(1) \u2212 b\n\u03b3(1) \u2190 \u03a0Cz (\u03b3(1)) u \u2190 u+ \u00b5(0) \u00b7 \u03b3(0) \u2212 \u00b5(1) \u00b7 \u03b3(1)\nThe overall algorithm is listed in Algorithm 1. Each loop terminates when the primal and dual residuals fall below a threshold (Boyd et al 2011, pages 19-20). We also use these residuals to dynamically adapt the penalties \u03c1 and \u03c12 (Boyd et al 2011, pages 20-21)."}, {"heading": "Computer and System Sciences 78(5):1460\u20131480.", "text": "Kiritchenko, S.; Zhu, X.; and Mohammad, S. M. 2014. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research 50:723\u2013762.\nLaver, M., and Garry, J. 2000. Estimating policy positions from political texts. American Journal of Political Science 619\u2013634.\nLiu, B. 2015. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge University Press.\nMaas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.; and Potts, C. 2011. Learning word vectors for sentiment analysis. In Proceedings of the Association for Computational Linguistics (ACL).\nMadsen, R. E.; Kauchak, D.; and Elkan, C. 2005. Modeling word burstiness using the dirichlet distribution. In Proceedings of the 22nd international conference on Machine learning, 545\u2013552. ACM.\nMinka, T. 2012. Estimating a dirichlet distribution. http://research.microsoft.com/ en-us/um/people/minka/papers/dirichlet/ minka-dirichlet.pdf.\nMolina-Gonza\u0301lez, M. D.; Mart\u0131\u0301nez-Ca\u0301mara, E.; Mart\u0131\u0301nValdivia, M.-T.; and Perea-Ortega, J. M. 2013. Semantic orientation for polarity classification in spanish reviews. Expert Systems with Applications 40(18):7250\u20137257.\nPang, B., and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the Association for Computational Linguistics (ACL), 271\u2013278.\nPang, B., and Lee, L. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval 2(1-2):1\u2013135.\nPang, B.; Lee, L.; and Vaithyanathan, S. 2002. Thumbs up?: sentiment classification using machine learning techniques."}, {"heading": "In Proceedings of Empirical Methods for Natural Language", "text": "Processing (EMNLP), 79\u201386.\nPolanyi, L., and Zaenen, A. 2006. Contextual valence shifters. In Computing attitude and affect in text: Theory and applications. Springer.\nQiu, G.; Liu, B.; Bu, J.; and Chen, C. 2011. Opinion word expansion and target extraction through double propagation. Computational linguistics 37(1):9\u201327.\nRibeiro, F. N.; Arau\u0301jo, M.; Gonc\u0327alves, P.; Gonc\u0327alves, M. A.; and Benevenuto, F. 2016. Sentibench-a benchmark comparison of state-of-the-practice sentiment analysis methods. EPJ Data Science 5(1):1\u201329.\nSettles, B. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1467\u20131478. Association for Computational Linguistics.\nSomasundaran, S.; Wiebe, J.; and Ruppenhofer, J. 2008. Discourse level opinion interpretation. In Proceedings\nof the 22nd International Conference on Computational Linguistics-Volume 1, 801\u2013808. Association for Computational Linguistics. Taboada, M.; Brooke, J.; Tofiloski, M.; Voll, K.; and Stede, M. 2011. Lexicon-based methods for sentiment analysis. Computational linguistics 37(2):267\u2013307. Tausczik, Y. R., and Pennebaker, J. W. 2010. The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology 29(1):24\u201354. Turney, P. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the Association for Computational Linguistics (ACL), 417\u2013424. Vilares, D.; Alonson, M. A.; and Go\u0301mez-Rodr\u0131\u0301guez, C. 2015. A syntactic approach for opinion mining on spanish reviews. Natural Language Engineering 21:139\u2013163. Wilson, T.; Wiebe, J.; and Hoffmann, P. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP), 347\u2013354."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "The Journal of Machine Learning Research 15(1):2773\u20132832.", "citeRegEx": "Anandkumar et al\\.,? 2014", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Learning topic models - going beyond SVD", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "FOCS, 1\u201310.", "citeRegEx": "Arora et al\\.,? 2012", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Better documentlevel sentiment analysis from rst discourse parsing", "author": ["P. Bhatia", "Y. Ji", "J. Eisenstein"], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 440\u2013447.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R", "citeRegEx": "Boyd et al\\.,? 2011", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity", "author": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "Journal of Machine Learning Research 15:2399\u20132449.", "citeRegEx": "Cohen et al\\.,? 2014", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Predicting the semantic orientation of adjectives", "author": ["V. Hatzivassiloglou", "K.R. McKeown"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 174\u2013181.", "citeRegEx": "Hatzivassiloglou and McKeown,? 1997", "shortCiteRegEx": "Hatzivassiloglou and McKeown", "year": 1997}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Journal of Computer and System Sciences 78(5):1460\u20131480.", "citeRegEx": "Hsu et al\\.,? 2012", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Sentiment analysis of short informal texts", "author": ["S. Kiritchenko", "X. Zhu", "S.M. Mohammad"], "venue": "Journal of Artificial Intelligence Research 50:723\u2013762.", "citeRegEx": "Kiritchenko et al\\.,? 2014", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Estimating policy positions from political texts", "author": ["M. Laver", "J. Garry"], "venue": "American Journal of Political Science 619\u2013634.", "citeRegEx": "Laver and Garry,? 2000", "shortCiteRegEx": "Laver and Garry", "year": 2000}, {"title": "Sentiment Analysis: Mining Opinions, Sentiments, and Emotions", "author": ["B. Liu"], "venue": "Cambridge University Press.", "citeRegEx": "Liu,? 2015", "shortCiteRegEx": "Liu", "year": 2015}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Modeling word burstiness using the dirichlet distribution", "author": ["R.E. Madsen", "D. Kauchak", "C. Elkan"], "venue": "Proceedings of the 22nd international conference on Machine learning, 545\u2013552. ACM.", "citeRegEx": "Madsen et al\\.,? 2005", "shortCiteRegEx": "Madsen et al\\.", "year": 2005}, {"title": "Estimating a dirichlet distribution", "author": ["T. Minka"], "venue": null, "citeRegEx": "Minka,? \\Q2012\\E", "shortCiteRegEx": "Minka", "year": 2012}, {"title": "Semantic orientation for polarity classification in spanish reviews. Expert Systems with Applications 40(18):7250\u20137257", "author": ["M.D. Molina-Gonz\u00e1lez", "E. Mart\u0131\u0301nez-C\u00e1mara", "M.-T. Mart\u0131\u0301nValdivia", "J.M. Perea-Ortega"], "venue": null, "citeRegEx": "Molina.Gonz\u00e1lez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Molina.Gonz\u00e1lez et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 271\u2013278.", "citeRegEx": "Pang and Lee,? 2004", "shortCiteRegEx": "Pang and Lee", "year": 2004}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and trends in information retrieval 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee,? 2008", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), 79\u201386.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Contextual valence shifters", "author": ["L. Polanyi", "A. Zaenen"], "venue": "Computing attitude and affect in text: Theory and applications. Springer.", "citeRegEx": "Polanyi and Zaenen,? 2006", "shortCiteRegEx": "Polanyi and Zaenen", "year": 2006}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["G. Qiu", "B. Liu", "J. Bu", "C. Chen"], "venue": "Computational linguistics 37(1):9\u201327.", "citeRegEx": "Qiu et al\\.,? 2011", "shortCiteRegEx": "Qiu et al\\.", "year": 2011}, {"title": "Sentibench-a benchmark comparison of state-of-the-practice sentiment analysis methods", "author": ["F.N. Ribeiro", "M. Ara\u00fajo", "P. Gon\u00e7alves", "M.A. Gon\u00e7alves", "F. Benevenuto"], "venue": "EPJ Data Science 5(1):1\u201329.", "citeRegEx": "Ribeiro et al\\.,? 2016", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances", "author": ["B. Settles"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1467\u20131478. Association for Computational Linguistics.", "citeRegEx": "Settles,? 2011", "shortCiteRegEx": "Settles", "year": 2011}, {"title": "Discourse level opinion interpretation", "author": ["S. Somasundaran", "J. Wiebe", "J. Ruppenhofer"], "venue": "Proceedings", "citeRegEx": "Somasundaran et al\\.,? 2008", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2008}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": "Computational linguistics 37(2):267\u2013307.", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "The psychological meaning of words: LIWC and computerized text analysis methods", "author": ["Y.R. Tausczik", "J.W. Pennebaker"], "venue": "Journal of Language and Social Psychology 29(1):24\u201354.", "citeRegEx": "Tausczik and Pennebaker,? 2010", "shortCiteRegEx": "Tausczik and Pennebaker", "year": 2010}, {"title": "Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews", "author": ["P. Turney"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 417\u2013424.", "citeRegEx": "Turney,? 2002", "shortCiteRegEx": "Turney", "year": 2002}, {"title": "A syntactic approach for opinion mining on spanish reviews", "author": ["D. Vilares", "M.A. Alonson", "C. G\u00f3mez-Rodr\u0131\u0301guez"], "venue": "Natural Language Engineering", "citeRegEx": "Vilares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilares et al\\.", "year": 2015}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 23, "context": "Introduction Lexicon-based classification refers to a classification rule in which documents are assigned labels based on the count of words from lexicons associated with each label (Taboada et al. 2011).", "startOffset": 182, "endOffset": 203}, {"referenceID": 16, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 145, "endOffset": 174}, {"referenceID": 10, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 145, "endOffset": 174}, {"referenceID": 9, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 230, "endOffset": 282}, {"referenceID": 24, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 230, "endOffset": 282}, {"referenceID": 21, "context": "The popularity of this approach can be explained by its relative simplicity and ease of use: for domain experts, creating lexicons is intuitive, and, in comparison with labeling instances, it may offer a faster path towards a reasonably accurate classifier (Settles 2011).", "startOffset": 257, "endOffset": 271}, {"referenceID": 10, "context": "Supervised classification systems, which are trained on labeled examples, tend to outperform lexicon-based classifiers, even without accounting for multi-word phenomena (Liu 2015; Pang and Lee 2008).", "startOffset": 169, "endOffset": 198}, {"referenceID": 16, "context": "Supervised classification systems, which are trained on labeled examples, tend to outperform lexicon-based classifiers, even without accounting for multi-word phenomena (Liu 2015; Pang and Lee 2008).", "startOffset": 169, "endOffset": 198}, {"referenceID": 6, "context": "Several researchers have proposed methods for lexicon expansion, automatically growing lexicons from an initial seed set (Hatzivassiloglou and McKeown 1997; Qiu et al. 2011).", "startOffset": 121, "endOffset": 173}, {"referenceID": 19, "context": "Several researchers have proposed methods for lexicon expansion, automatically growing lexicons from an initial seed set (Hatzivassiloglou and McKeown 1997; Qiu et al. 2011).", "startOffset": 121, "endOffset": 173}, {"referenceID": 18, "context": "There is also work on handling multi-word phenomena such as negation (Wilson, Wiebe, and Hoffmann 2005; Polanyi and Zaenen 2006), and discourse (Somasundaran, Wiebe, and Ruppenhofer 2008; Bhatia, Ji, and Eisenstein 2015).", "startOffset": 69, "endOffset": 128}, {"referenceID": 13, "context": "Minka (2012) presents a number of estimators for the concentration parameter \u03c4 from a corpus of text.", "startOffset": 0, "endOffset": 13}, {"referenceID": 4, "context": "(38) This problem can be solved by alternating direction method of multipliers (Boyd et al. 2011).", "startOffset": 79, "endOffset": 97}, {"referenceID": 15, "context": "0), labeled as positive or negative (Pang and Lee 2004).", "startOffset": 36, "endOffset": 55}, {"referenceID": 11, "context": "IMDB 50,000 English-language film reviews (Maas et al. 2011).", "startOffset": 42, "endOffset": 60}, {"referenceID": 10, "context": "The Liu lexicon (Liu 2015) consistently obtained the best performance on all three English-language datasets, so it was made the focus of all subsequent experiments.", "startOffset": 16, "endOffset": 26}, {"referenceID": 14, "context": "For the Spanish data, the ISOL lexicon was used (Molina-Gonz\u00e1lez et al. 2013).", "startOffset": 48, "endOffset": 77}, {"referenceID": 10, "context": "The Liu lexicon (Liu 2015) consistently obtained the best performance on all three English-language datasets, so it was made the focus of all subsequent experiments. Ribeiro et al. (2016) also found that the Liu lexicon is one of the strongest lexicons for review analysis.", "startOffset": 4, "endOffset": 188}, {"referenceID": 25, "context": "Classifiers The evaluation compares the following unsupervised classification strategies: LEXICON basic word counting, as in decision rule (1); LEX-PRESENCE counting word presence rather than frequency, as in decision rule (24); PROBLEX-MULT probabilistic lexicon-based classification, as proposed in this paper, using the multinomial likelihood model; PROBLEX-DCM probabilistic lexicon-based classification, using the Dirichlet Compound Multinomial likelihood to reduce effective counts for repeated words; PMI An alternative approach, discussed in the related work, is to impute document labels from a seed set of words, and then compute \u201csentiment scores\u201d for individual words from pointwise mutual information between the words and imputed labels (Turney 2002).", "startOffset": 751, "endOffset": 764}, {"referenceID": 25, "context": "Classifiers The evaluation compares the following unsupervised classification strategies: LEXICON basic word counting, as in decision rule (1); LEX-PRESENCE counting word presence rather than frequency, as in decision rule (24); PROBLEX-MULT probabilistic lexicon-based classification, as proposed in this paper, using the multinomial likelihood model; PROBLEX-DCM probabilistic lexicon-based classification, using the Dirichlet Compound Multinomial likelihood to reduce effective counts for repeated words; PMI An alternative approach, discussed in the related work, is to impute document labels from a seed set of words, and then compute \u201csentiment scores\u201d for individual words from pointwise mutual information between the words and imputed labels (Turney 2002). The implementation of this method is based on the description from Kiritchenko, Zhu, and Mohammad (2014), using the lexicons as the seed word sets.", "startOffset": 752, "endOffset": 871}, {"referenceID": 0, "context": "The method-of-moments has become an increasingly popular estimator in unsupervised machine learning, with applications in topic models (Anandkumar et al. 2014), sequence models (Hsu, Kakade, and Zhang 2012), and more elaborate linguistic structures (Cohen et al.", "startOffset": 135, "endOffset": 159}, {"referenceID": 5, "context": "2014), sequence models (Hsu, Kakade, and Zhang 2012), and more elaborate linguistic structures (Cohen et al. 2014).", "startOffset": 95, "endOffset": 114}, {"referenceID": 23, "context": "Related work Turney (2002) uses pointwise mutual information to estimate the \u201csemantic orientation\u201d of all vocabulary words from co-occurrence with a small seed set.", "startOffset": 13, "endOffset": 27}, {"referenceID": 4, "context": "We optimize using the alternating direction method of multipliers (ADMM; Boyd et al. 2011).", "startOffset": 66, "endOffset": 90}], "year": 2016, "abstractText": "In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics. Introduction Lexicon-based classification refers to a classification rule in which documents are assigned labels based on the count of words from lexicons associated with each label (Taboada et al. 2011). For example, suppose that we have opposed labels Y \u2208 {0, 1}, and we have associated lexicons W0 and W1. Then for a document with a vector of word counts x, the lexicon-based decision rule is, (1) \u2211", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}