{"id": "1611.07174", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition", "abstract": "clinical performance of end - to - end automatic speech recognition ( asr ) systems can significantly be improved by the increasing large speech corpus and deeper neural network. given the arising problem of training speed and recent success of innovative deep convolutional neural network in asr, we build a theoretically novel deep recurrent convolutional network for generalized acoustic modeling and hopefully apply deep residual learning framework to it, our experiments show that it has not only faster convergence converge speed but better recognition accuracy over traditional deep convolutional recurrent network. we mainly compare convergence speed of two acoustic models, which are novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. with comparable faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable repetition performance. we further show that applying deep residual learning can boost both convergence speed and recognition accuracy of our novel recurret convolutional networks. finally, we evaluate all our experimental networks by phoneme error rate ( per ) with newly proposed bidirectional statistical language model. our evaluation results show that our first model applied with deep residual distributed learning can reach the best per of 17. 33 % with fastest convergence speed in timit database.", "histories": [["v1", "Tue, 22 Nov 2016 07:36:21 GMT  (463kb,D)", "http://arxiv.org/abs/1611.07174v1", "10pages, 13figures"], ["v2", "Tue, 27 Dec 2016 04:53:56 GMT  (469kb,D)", "http://arxiv.org/abs/1611.07174v2", "11 pages, 13 figures"]], "COMMENTS": "10pages, 13figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zewang zhang", "zheng sun", "jiaqi liu", "jingwen chen", "zhao huo", "xiao zhang"], "accepted": false, "id": "1611.07174"}, "pdf": {"name": "1611.07174.pdf", "metadata": {"source": "CRF", "title": "An Experimental Comparison of Deep Neural Networks for End-to-end Speech Recognition", "authors": ["Zewang Zhang", "Zheng Sun", "Jiaqi Liu", "Jingwen Chen"], "emails": ["iao}@{mail2,mail2,mail2,mail2,mail}.sysu.edu.cn", "huozhao@cupl.edu.cn"], "sections": [{"heading": null, "text": "Index Terms\u2014Speech recognition, convergence speed, end-toend, recurrent neural network, convolutional neural network, residual learning framework, phoneme error rate\nI. INTRODUCTION\nAUTOMATIC speech recognition(ASR) is designed totranscript human speech into spoken phonemes. ASR has been investigated for several decades. Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4]. With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].\nRecurrent neural network (RNN) is a powerful tool for sequential modeling, and DNNs are gradually replaced by RNNs which have been successfully applied in ASR in the\nZ. Zhang, Z. Sun, J. Liu, J. Chen, X. Zhang are with the Department of Physics, Sun Yat-Sen University, Guangzhou, 510275 P.R.China. E-mail: {zhangzw3, sunzh6, liujq33, chenjw93, zhangxiao}@{mail2,mail2,mail2,mail2,mail}.sysu.edu.cn\nZ. Huo is with China University of Political Science and Law. Email: huozhao@cupl.edu.cn\nManuscript received xx/xx/xxxx; revised xx/xx/xxxx.\nlast several years. Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14]. Besides, sequence training of RNNs with connectionist temporal classification (CTC) has shown great performance in endto-end ASR [10], [15]. Traditional frame-wise cross entropy training needs pre-segmented data by hand, but CTC is an end-to-end training method for RNNs which decodes the output probability distribution into phoneme sequences without requiring pre-segmented training data. RNN has been widely used in ASR, but RNN can\u2019t depict very long time dependency because of its vanishing gradient problem, and deeper RNN seems to have little improvement when the number of layers reaches a limit. Although LSTM improves the performance of RNN, an disadvantage of LSTM is that it requires too much computation and stores multiple gating neural responses at each time-step, which will become a computational bottleneck.\nVery recently, some other novel neural networks structures have been proposed, of which convolutional neural network (CNN) is one of the most attractive models. CNN is an older deep neural network architecture [29], and has enjoyed the great popularity as a efficient approach in character recognition. Human speech, which is also a sequential signal, can be transformed into a feature map that we can take similarly as an image. Human speech signals are highly variable because of different speaking accents, different speaking styles and uncertain noises from the environment. For speech recognition, CNN has several advantages: (i) human speech signal has local correlations in both time and frequency, CNN is well suited to exploit these correlations explicitly through a local connectivity. (ii) CNN has the ability to capture the frequency shift in human speech signal.\nSome researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27]. Most of previous application of CNNs in speech recognition only used fewer convolutional layers. For example, Abdel-Hamid et al. [22] used one convolutional layer, one pooling layer and a few full-connected layers. Amodei et al. [9] also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26]. Generally, very small filters with 3*3\nar X\niv :1\n61 1.\n07 17\n4v 1\n[ cs\n.C L\n] 2\n2 N\nov 2\n01 6\n2 kernels have recently been successfully applied in acoustic modeling in hybrid NN-HMM speech recognition system, and pooling layer has been proved to be replaced by full-connected convolutional layers and pooling has no highlights for LVCSR tasks [30].\nCNN has the ability to exploit the internal dependency of speech sequences while having the advantage of fewer parameters than RNN. This is to say, we can use less complex computational cost to achieve the same performance as RNN. With the datasets of human speech becoming larger, it will come true that overfitting will be less important but the convergence speed is what we always care about. Meanwhile, we find there are few work that discusses the convergence speed of different configurations in deep CNNs for ASR. Typical architecture of deep CNNs for ASR usually contains some fully-connected convolutional layers at the bottom, followed by several recurrent layers and fully-connected feedforward layers, but we find that, in practice, it\u2019s too slow to train this type of architecture for acoustic modeling. To explore how can we speed up the convergence, we build another two architectures based on deep CNNs for acoustic modeling: (i) deep recurrent convolutional networks, and (ii) applying residual learning framework in deep CNNs. Deep residual learning framework can obtain compelling accuracy and good convergence performance in computer vision [31], which attributes to its identity mapping as the skip connection. We compare the convergence speed of above three different architectures, besides, we further present the evaluation results by PER of three architectures. Our work has meaningful reference for who are also applying deep CNNs for ASR to attain a fast convergence speed.\nThis paper is organized as follows. We first review the basics of commonly used models in ASR including recurrent neural network and convolutional neural network (Section II), then we explain how the end-to-end training method of connectionist temporal classification can be used to decode the output phonemes probability distribution (Section III). Besides, we propose a bi-directional statistical n-gram language model to rectify the output sequences of acoustic model in Section IV. Section V explains the experiment setup and some training details. Section VI presents the comparison of convergence speed between traditional deep convolutional recurrent networks, our novel deep recurrent convolutional networks and those applied with deep residual learning. Further, we show the evaluation results by minimum PER of all experimental architectures in Section VII."}, {"heading": "II. REVIEW OF NEURAL NETWORK", "text": ""}, {"heading": "A. ELU Nonlinearity", "text": "The most common functions applied to a neuron\u2019s output is ReLU [19] as a function of its input with f(x) = max(0, x). ReLU behaves better than traditional non-linear functions such as sigmoid or tanh, since the mean value of ReLU\u2019s activations is not zero, some neurons in practice always become dead during backpropagation. Exponential linear unit (ELU) was introduced in [20], in contrast to ReLU, ELU has negative values which pushes the mean of activations close to zero,\nthis is to say, ELU can decrease the gap between the normal gradient and unit natural gradient and, therefore, speed up training. The expression of ELU nonlinearity is shown below in Equation ??.\nf(x) = { x if x > 0 \u03b1(exp(x)\u2212 1) if x 6 0 (1)\nSince our work is based on deep CNNs, we can take ELU as the non-linear function to faster our network\u2019s convergence. Fast learning has a great impact on performance of training large datasets."}, {"heading": "B. Recurrent Neural Network", "text": "General forward neural network can\u2019t depict the time dependency for sequence modeling problems well, such as automatic speech recognition. One type of special forward neural network is recurrent neural network(RNN). When RNN is folded out in time, it can be considered as a DNN with many sequential layers. In contrast to forward neural network, RNN is used to build time dependency of input features with internal memory units. RNN can receive input and produce output at every layer, the general architecture of RNN is shown in Figure 1.\nAs shown in Figure 1, a very simple RNN is composed of an input layer, a hidden layer and an output layer. The recurrent hidden layer is designed to pass the forward information to backward time-steps. We can depict internal relationship of a general RNN in Equation (??).\nOt = f(yt) = f(Whh \u2217 ht\u22121 + Wxh \u2217 xt + bt) (2)\nwhere Whh is the weight matrix between adjacent hidden units, ht\u22121 is the hidden unit of previous time-step, Wxh is the weight matrix between input layer and hidden layer, xt is the input at time t, and the bias bt is added and finally an activation function f(\u00b7), typically sigmoid, tanh, ReLU or ELU, will be applied to generate the output of the recurrent layer. If several recurrent layers are stacked, the output of previous layer becomes the input of next layer."}, {"heading": "C. Convolutional Neural Network", "text": "Compared to standard fully-connected neural networks and RNNs, CNNs which are proposed in [16] have much fewer parameters so that they are easier to train. CNNs are pretty\n3 similar to ordinary neural networks, they are made of trainable weights and bias and they can also be stacked to a deep depth, which has been successfully applied in ImageNet competition [32].\nA typical architecture of CNN is composed of a convolutional layer and a pooling layer which is shown in Figure 2. In most cases, a typical convolutional layer contains several feature maps, each of which is a kind of filter with shared parameters. These filters are spatial and extend through the full depth of input volume. Pooling layer is designed for dimensionality reduction and full-connected layer can output the probability distribution of all different classes.\nIn our experimented model, we replace the pooling layer with the full convolutional layer. Especially, we pad the input layer with zeros in both dimensions, since that the output layer can be the same size as the input layer if we set the stride to be 1. The details of padding is shown in Figure 3."}, {"heading": "III. REVIEW OF CONNECTIONIST TEMPORAL CLASSIFICATION", "text": "We also replace the traditional HMM decoder for output sequences. Labelling unsegmented sequential data is very common in speech recognition, and CTC is good at achieving this. The basic idea of CTC is to interpret the outputs of network as a probability distribution over all possible phenomes. Given this distribution, we can derive the objective function of sequence labeling. Since the objective function is differentiable, we can train it by backpropagation through time algorithm.\nUsing the probability distributions learned by deep CNNs, we would then use a CTC loss layer to finally output the phenome sequence. For a given input sequence, the goal of CTC is to minimize the average edit distance from the output\nsequence to actual sequence. Suppose L is an alphabet of all output phonemes, the CTC network has one more units than there are labels in L, and the activations of first |L| labels are interpreted as the probability of observing the corresponding labels, the activation of the extra unit is the probability of the blank label. From the output probability distribution, we can compute probability of any sequence by summing all its different alignments.\nIn detail, if we define ytk as the probability of outputting label k at time t, which defines a distribution over the set L \u2032T of length T sequences over the alphabet L \u2032 = L \u2229 {blank}:\np(\u03c0 | x) = T\u220f t=1 yt\u03c0t ,\u2200\u03c0 \u2208 L \u2032T (3)\nIn (??), we take the elements of L \u2032T as different paths, which is denoted as \u03c0. Given a labelling l and a mapping function B which removes all blanks and repeated labels, we can compute its probability by summing all its possible paths:\np(l | x) = \u2211\n\u03c0\u2208B\u22121(l)\np(\u03c0 | x) (4)\nTherefore, the output sequence should be the most probable labelling, it\u2019s a decoding problem about how to find the output sequence. We require an efficient way of calculating the probabilities p(l | x) of each labelling. Since from (??) we may feel that it\u2019s very difficult to handle because there are many paths corresponding to a giving labelling. However, the problem can be converted to a dynamic programming problem, like what we use in HMMs. The key point of dynamic algorithm is to break down the sum over all paths into forward and backward variables.\nTo account for the blanks, CTC considers to add blanks between every pair of labels and beginning and end. Therefore, the length of modified label l\u2032 is 2|l|+1. Since the state transition space for dynamic programming shouldn\u2019t be too large for computational efficiency, we assume that state transition only occurs between blank and non-blank label or two distinct non-blank labels. Besides, we allow all prefixes begins with a blank or the first symbol in l and ends with a blank or the last symbol in l.\nThus, we can draw the expressions of dynamic program formulation from Figure 4. Define the forward variable\n\u03b1t(s) = \u03b1t\u22121(s) + \u03b1t(s\u2212 1) (5)\nWe can conclude that if the current label is blank or the current blank is the same as it was two steps ago, then the current forward variable is defined\n\u03b1t(s) = \u03b1t(s)y t l\u2032s\n(6)\nOtherwise, the current forward variable is defined\n\u03b1t(s) = (\u03b1t(s) + \u03b1t\u22121(s\u2212 2))ytl\u2032s (7)\nSince the last label must only be a blank or the last label in l, the final probability of a given label is calculated by forward variables as\np(l|x) = \u03b1T (|l|) + \u03b1T (|l| \u2212 1) (8)\n4 The forward variables are defined previously, and the backward variables can be defined similarly. In practice, in order to avoid any underflows on any digital computer, we must normalize both the forward variables and backward variables. Finally, we can calculate the maximum likelihood error as\nln(p(l|x)) = T\u2211 t=1 ln( \u2211 s \u03b1t(s)) (9)\nFor forward and backward variables defined above, we can calculate the probability of any labels occurred at any time t given a labelling l as\n\u03b1t(s)\u03b2t(s) = y t ls \u2211 \u03c0\u2208\u03b2\u22121(l) p(\u03c0|x) (10)\nSince p(l | x) can be obtained by summing over all s, differentiating this w.r.t ytk, we only consider paths that go through label k at time t.\np(l | x) ytk = 1 ytk 2 \u2211 s\u2208pos(k,l) \u03b1t(s)\u03b2t(s) (11)\npos(k, l) represents the set of positions where k occurs in l, so the objective function\u2019s gradient is\n1 ytk (\u2212ln(p(l | x))) = 1 p(l | x) 1 ytk 2\u2211\ns\u2208pos(k,l) \u03b1t(s)\u03b2t(s) (12)\nOur prediction of acoustic modeling is tied to CTC loss, and the loss takes the output of deep convolutional neural network as input. Using back propagation algorithm, we can update all parameters of acoustic model to reach a minimum of loss."}, {"heading": "IV. BIDIRECTIONAL N-GRAM LANGUAGE MODEL", "text": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37]. Traditional n-gram model always makes an assumption\nthat the probability of the current word depends only on the probability of the previous N-1 words, we propose a new bidirectional n-gram model which considers context probability of two sides. Computation of our bidirectional n-gram model is composed of two parts. First, We define forward n-gram going left to right in a sentence and we obtain the forward probability for each phoneme given previous phoneme phrase.\nPf (ph) = P (phn|phn\u22121, ..., phn\u2212(N\u22121)) (13)\nSecond, we reverse the whole training sentence to obtain the backward probability for each phoneme given future phoneme phrase.\nPb(ph) = P (phn|phn+1, ..., phn+(N\u22121)) (14)\nOur model takes as input bidirectional n-gram phoneme counts, thus it combines the bidirectional context information capacity and simple computation complexity. Besides, to make our model more robust, we perform bigram, trigram and four-gram including both forward probability and backward probabilitybased on TIMIT corpus. We shows an example how our language model rectify mislabeled phonemes of acoustic model in Figure 5."}, {"heading": "V. EXPERIMENTAL SETUP", "text": ""}, {"heading": "A. Dataset", "text": "We perform our speech recognition experiments on a public commonly used speech dataset: TIMIT. TIMIT is composed of 630 speakers with 6300 utterances of different sexes and dialects. Every audio clip is followed by phoneme transcriptions and sentence transcriptions, and we take the phoneme transcriptions as ground labels. The output is probability distribution of 63 labels including 61 non-blank phonemes, one space label and one blank label for CTC.\nSince a typical machine learning dataset contains training set, validation set and test set, we design to split 6300 utterances of TIMIT into 5000 training utterances, 1000 validation\n5 rectified sequences n-gram rectify predicted sequences\ndclae d er dcl dclae y er dcl\ndclae ? er dcl\nerdcl ? dcl ae\nFig. 5. Example of the bi-directional statistical n-gram language model rectifying the output phoneme sequence.\nutterances and 300 test utterances. For the generalization of our model, we randomly choose six different partitions of TIMIT and then select the best partition by cross validation. We build a simply baseline model of deep neural networks to evaluate the performance of six different partitions, and we choose the best partition whose test cost curve is going down both stably and fast."}, {"heading": "B. Feature Selection", "text": "At the stage of pre-processing, each piece of speech is analyzed using a 25-ms Hamming window with a fixed overlap of 10-ms. Each feature vector of a frame are calculated by Fourier-transform-based filter-bank analysis, which includes 39 log energy coefficients distributed on 13 mel frequency cepstral coefficients (MFCCs), along with their first and second temporal derivatives. Besides, all feature data are normalized so that each vector dimension has a zero mean and unit variance. Since the feature matrix of each audio speech differs in time length, we pad each feature matrix with zeros to a max length.\nC. Implementation\nWe build our deep neural network models based on opensource library Lasagne, which is a lightweight library to build and train neural networks in Theano. We take our experiment on GPU Tesla K80 to speed up our training. For CTC part, we choose to use a C++ implementation of Baidu Research and we write some Python code to wrap it in Lasagne. Besides, all cross validation, feature generation, data loading, result analysis, visualization are implemented in Python code by ourselves."}, {"heading": "D. Training Details", "text": "All our experimental models are trained by end-to-end stochastic gradient descent algorithm with a mini-batch of 32. In detail, since the Adam optimization method [21] is computationally efficient, requires little memory allocation and is well suitable for training deep learning models with large data, we adopt the Adam method with a learning rate of 0.00005 at the start of training. Instead of setting learning rate to be 0.001 as the paper said, we find that a learning rate of 0.00005 can make the divergence more stable in practice. As our experimental models are very deep, we would like to adopt some regulations to avoid overfitting. Recetnly, batch normalization [18] has shown a better regularization performance, however it would add extra parameters and needs heavy data augmentaion, which we would like to avoid. Instead, we add a dropout layer after the recurrent layers and\nafter the first full-connected feedforward layer to prevent it from overfitting. To keep the sequential information for better acoustic modeling, we reserve the whole context information for end-to-end training instead of splitting each audio into frames of same length for frame-wise cross-entropy training. Besides, on the top layer of our experimental models, we set the activation function to be linear, for that CTC decoding has wrapped the softmax layer inside."}, {"heading": "E. Evaluation", "text": "Since our proposed model is end-to-end and phonemelevel, we use phoneme error rate (PER) to evaluate the result. The PER is computed after the CTC network had decoded the whole output sentence into a sequence of phonemes. We then compute the Damerau-Levenshtein distance between our predicted sequence and the truth sequence to obtain the mistake we made. The average number of mistakes over the length of the phoneme sequence is just our PER. We evaluate our model on the test set finally."}, {"heading": "VI. ARCHITECTURE", "text": "To explore the convergence properties of deep convolutional networks with recurrent networks, we have conducted experiments with different configurations. In this section, we\u2019ll discuss three typical experiments we have tried, which are novel deep recurrent convolutional networks, traditional deep convolutional recurrent networks and residual networks. We\u2019ll compare the convergence speed of them according to different configurations including number of layers, number of parameters, number of feature maps and applying residual learning."}, {"heading": "A. End-to-End Training", "text": "Traditional training approach of acoustic modeling is based on frame-wise cross-entropy of predicted output and true label, which needs handy alignment between input frames and output labels. To avoid such high labor cost, our approach exploits the dynamic decoding method based on CTC which can perform supervised learning on sequence data and avoid alignment between input data and output label. We choose a commonly used dataset TIMIT to work on our acoustic model. TIMIT contains 6300 utterances of 630 speakers in 8 dialects, each audio contains phoneme transcription, word transcription and the whole sentence. We choose the phoneme transcription as labels for phoneme-level training.\n6"}, {"heading": "B. Deep convolutional recurrent networks", "text": "1) Details of experimental models: Since many recent ASR systems use deep CNNs for acoustic modeling as part of ASR, especially, deep CNNs are used for feature preprocessing followed by RNN CTC decoding network. Conventional deep CNNs contain convolution and pooling, but we find that pooling can be replaced by convolution with fewer feature maps in practice. Inspired by this, we also build four deep convolutional recurrent networks, which are composed of deep convolutional layers, four recurrent layers and two full-connected feedforward layers. They are distinguished by different number of feature maps at convolutional layer. As shown in Figure 7, the two deep fully-connected convolutional recurrent networks \u201cCR1\u201d and \u201cCR2\u201d differ in number of feature maps in bottom convolutional layers. We set the number of feature maps to go down gradually, which means that the number of feature maps becomes narrower from bottom layers to up layers. Both \u201cCR3\u201d and \u201cCR4\u201d have narrower deep convolutional layers, but they have different bottom convolutional layers. The parameters of each model are shown in Table I.\n2) Comparison of cost curves: The cost curves of four convolutional recurrent models are shown in Figure 8. Comparing to other three models, \u201cCR2\u201d converges fastest. Since \u201cCR1\u201d and \u201cCR2\u201d both have similar deep fully-connected\nconvolutional layers, but differ in number of feature maps. We can find that \u201cCR2\u201d behaves much better than \u201cCR1\u201d. \u201cCR3\u201d and \u201cCR4\u201d have narrower structures, they behave very similarly but much poorer than \u201cCR2\u201d. So we can draw a conclusion that for convolutional recurrent networks, deep fully-connected convolutional layers can be of much help to convergence. However, narrower structure doesn\u2019t improve the performance slightly here."}, {"heading": "C. Deep recurrent convolutional networks", "text": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9]. We propose a new architecture for acoustic modeling which is composed of several recurrent layers followed by deep CNNs. Our new architecture has some highlights: First, we make use of RNNs to depict short time dependency of the input feature. Second, CNN is able to depict the local correlations of small\n7 field, but our deep stacked CNNs can see the whole context information. Third, as opposite to the traditionally used 6*6 or 3*4 filter and 1*3 pooling in speech recognition [26], we use the small filters of 3*3 to build the full convolutional layers with no pooling layer. Filters of 3*3 is successfully used in computer vision, and we also find filters of 3*3 can effectively capture the high-level features along both time dimension and frequency dimension with little computational complexity. The stride of convolutional layer is set to be 1, and each convolutional layer is padded with zeros in both dimensions to keep the sizes of input and output feature maps unchanged.\n1) Details of experimental networks: We build six different networks which are combinations of RNNs and CNNs. The similarities of six networks are as follows. Continuous frames of 39 dimensional feature vectors are passed into the bottom recurrent layer as input of the network, and each recurrent layer has 128 neurons to store hidden information. Besides, at the top of each network, we add two full-connected feedforward layers, which have 256 hidden neurons and 63 output neurons, respectively. Since the CTC network has the softmax function, so we don\u2019t add any activation function on the top layer. Instead, we take the linear activation function as the output followed by CTC network.\nAs shown in Figure 9, \u201cRC1\u201d is a model with four recurrent layers at the bottom, followed by totally 12 convolutional layers, of which they are two convolutional layers with 24 feature maps, two convolutional layers with 48 feature maps, two convolutional layers with 24 feature maps, two convolutional layer with 12 feature maps, two convolutional layers with 6 feature maps and two convolutional layers with three feature maps. Compared with \u201cRC1\u201d, we also build \u201cRC2\u201d with fewer parameters, and the difference between both is that \u201cRC2\u201d has 6 convolutional layers with 16 feature maps, 2 convolutional layers with 8 feature maps, 2 convolutional layers with 4 feature maps and 2 convolutional layers with 2 feature maps. Besides, we also build \u201cRC3\u201d, which has the similar structure as \u201cRC1\u201d but with only 2 recurrent layers. Comparing with \u201cRC2\u201d, we also build a similar network as \u201cRC4\u201d, which has the same convolutional layers but with only 2 recurrent layers. Besides, we also build another two networks \u201cRC5\u201d and \u201cRC6\u201d, which have more full-connected convolutional layers than previous four networks. The parameters of six networks are shown in Table II.\n2) Comparison of cost curves: To investigate the convergence of different networks in Figure 9, we present the comparison of cost curves in Figure 10. The horizontal axis represents time, of which the unit is minute. The vertical axis represents training cost. We totally train each model for 42 epochs, and each marker denotes an epoch in Figure 10. Results show that \u201cRC1\u201d converges the slowest at both the beginning and the end, which is probably caused by too many\nparameters and too many convolutional layers of different feature maps. Differently, \u201cRC2\u201d performs much better than \u201cRC1\u201d, since both finished 42 epochs at same time, but there is a great gap between their cost curves. Comparing \u201dRC1\u201d with \u201cRC2\u201d, \u201cRC2\u201d has fewer parameters and more continuous full-connected convolutional layers. \u201cRC3\u201d has fewer recurrent layers than \u201cRC1\u201d, and \u201cRC3\u201d also behaves much better than \u201cRC1\u201d. Similarly, \u201cRC4\u201d has fewer recurrent layers than \u201cRC2\u201d, and it behaves much better than \u201dRC2\u201d. \u201cRC4\u201d finishes 42 epochs in only 900 minutes, but \u201dRC2\u201d takes around 1100 minutes. Since \u201cRC4\u201d, \u201cRC5\u201d and \u201cRC6\u201d have the similar number of parameters, we find that \u201cRC4\u201d behaves best among three, which probably attributes to the different structure of \u201cRC4\u201d. Based on \u201cRC5\u201d, \u201cRC6\u201d has more convolutional feature maps, but we find this degrades the performance of \u201cRC5\u201d.\nTotally, our experiments show that narrower fully-connected convolutional layers can help improve model to converge, as \u201cRC2\u201d and \u201cRC4\u201d. Besides, deep fully-connected convolutional layers with same number of feature maps have slightly inferior performance to narrower ones, as \u201cRC5\u201d and \u201cRC6\u201d. For convergence, narrower recurrent convolutional layers behave better.\n3) Comparison of \u201cRC2\u201d and \u201cCR2\u201d: To investigate the difference of convergence between deep recurrent convolutional network and deep convolutional recurrent network, we present a comparison of convergence curves between both. Recurrent convolutional network is our novel proposed network for acoustic modeling, and convolutional recurrent network is the conventional one, they have the similar number of parameters. Since we have discussed some different models of them previously, we especially select two better models \u201cRC2\u201d and \u201cCR2\u201d to be shown in Figure 11.\nAs shown in Figure 11, we count 88 epochs for both \u201cCR2\u201d and \u201cRC2\u201d. Observing the training cost curves along time, we can draw some conclusions. First, \u201cRC2\u201d finishes 88 epoches in less than 3000 minutes, while \u201cCR2\u201d finishes in nearly 4000 minutes, even though \u201cCR2\u201d has the same number of parameters as \u201cRC2\u201d, but it converges one quarter faster than \u201cRC2\u201d. Besides, in the first 2000 minutes, \u201cRC2\u201d behaves much better than \u201cCR2\u201d, but in the last half of training, \u201cCR2\u201d catches up with \u201cRC2\u201d. Therefore, our proposed deep recurrent convolutional network has the faster convergence performance at the beginning, and reach the same performance as conventional deep convolutional recurrent network later."}, {"heading": "D. Residual networks", "text": "Generally, deeper convolutional neural networks can have larger capacity for feature representation, however, it has been shown that deeper convolutional neural networks are more difficult to train for their degradation problem, although there are some modern optimization methods. Recently, a new residual learning framework has been proposed to ease the training of very deep convolutional neural networks, and deep residual networks [31] have been proved to improve convergence and higher accuracy in image classification with no more extra parameters. General deep residual networks\n8 Input:Fr*39 (RNN,128)*4 (conv 3*3,24)*2 (conv 3*3,48)*2 (conv 3*3,24)*2 (conv 3*3,12)*2 (conv 3*3,6)*2 (conv 3*3,3)*2 (DNN,256) (DNN,63) CTC Input:Fr*39 (RNN,128)*4 (conv 3*3,16)*6 (conv 3*3,8)*2 (conv 3*3,4)*2 (conv 3*3,2)*2 (DNN,256) (DNN,63) CTC Input:Fr*39 (RNN,128)*2 (DNN,256) (DNN,63) CTC Input:Fr*39 (RNN,128)*2 (conv 3*3,32)*2 (conv 3*3,64)*2 (conv 3*3,32)*2 (conv 3*3,16)*2 (conv 3*3,8)*2 (conv 3*3,4)*2 (DNN,256) (DNN,63) CTC Input:Fr*39 (RNN,128)*2 (DNN,256) (DNN,63) CTC Input:Fr*39 (RNN,128)*2 (conv 3*3,16)*6 (conv 3*3,8)*2 (conv 3*3,4)*2 (conv 3*3,2)*2 (DNN,256) (DNN,63) CTC RC1 RC2 RC5RC3 RC6RC4 (conv 3*3,16)*10 (conv 3*3,2)*2 (conv 3*3,24)*10 (conv 3*3,2)*2\nFig. 9. Architectures of different recurrent convolutional networks\n(ResNets) are composed of many stacked \u201cResidual Blocks\u201d, and each residual block can be expressed in following form:\nyl = h(xl) +z(xl,Wl) (15)\nxl+1 = f(yl) (16)\nwhere xl and xl+1 are input and output of the l-th residual block, and z is a residual mapping function. In general, h(xl) = xl is an identity mapping and f is an activation function, which we set to be elu [20] here. With the presence of short-cut connection, residual networks can ease the degradation problem of deeper convolutional neural networks\nbecause the additional layers can only simply perform an identity mapping. Although ResNets with over 100 layers have shown great accuracy for several challenging image classification tasks on ImageNet competitions [32] and MS COCO competitions [33], we also want to explore how the residual blocks behave in our experimental models.\nTo explore how ResNets behave in ASR, we propose a novel architecture which combines deep fully convolutional network with residual learning framework in ASR. We build two ResNets based on both \u201cCR2\u201d and \u201cRC2\u201d, which are shown in Figure 12. To avoid extra parameters, we build residual blocks only with layers of same dimension. \u201cCR2\u201d\n9 and \u201cRC2\u201d are two models with best performance among all discussed models above. For \u201cRes-RC2\u201d, we build four residual blocks based on \u201cRC2\u201d, and each residual block contains several layers with the same number of feature maps. In contrast, \u201cRes-CR2\u201d contains two residual blocks based on \u201cCR2\u201d.\nFigure 13 gives the comparison of cost curves between two plain networks and their residual versions. \u201cRes-RC2\u201d shows the best performance, it finishes 88 epochs in only 1500 minutes, and its cost also goes down very quickly. Comparing \u201cRes-RC2\u201d with \u201cRC2\u201d, we can draw a conclusion that \u201cRes-RC2\u201d converges twice as fast as \u201dRC2\u201d, which mainly attributes to the four shortcuts connection of identity mapping in \u201cRes-RC2\u201d, thus, the difficulty of training \u201cResCR2\u201d can be eased. However, compared to \u201cCR2\u201d, \u201cResCR2\u201d with only two residual blocks converges much slower. Concretely, \u201cCR2\u201d finishes 88 epochs in less than 4000\nminutes, while \u201cRes-CR2\u201d finished 88 epochs in nearly 7000 minutes. Although \u201cRes-CR2\u201d has two residual blocks based on \u201cCR2\u201d, but the degradation problem has been exposed out of expectation. For this degradation problem, we propose two possible reasons. (i) The bottom residual block of \u201cResCR2\u201d has ten convolutional layers, we think it\u2019s so deep that the power of deep residual learning may be restricted. (ii) \u201cRes-CR2\u201d has two recurrent layers on top of convolutional layers, so the convergence may be mainly influenced by the top recurrent layers and residual blocks make no difference here."}, {"heading": "VII. EVALUATION", "text": "TIMIT is a small 16 kHz speech corpus with 6300 utterances, from which the validation set of 300 utterances and the test set of 300 utterances are derived. We use 63 phonemes as output labels, including 61 non-blank labels, one space\n10\nlabel and one blank label for CTC. After acoustic modeling, a probability distribution of 63 labels will be decoded by the CTC network into final phoneme sequences. Then, our proposed bidirectional hybrid n-gram language model over phonemes, estimated from the whole training set, is used to rectify the final sequence.\nSince our acoustic models are phoneme-level, we evaluate them by PER on the test set. Each experiment of an architecture has been conducted several times, we present the minimum validation PER and minimum test PER for every architecture in Table III. According to evaluation result, our novel deep recurrent convolutional network \u201cRC2\u201d obtains a 20.71% PER, with accuracy competitive to traditional deep belief network acoustic model. Although, deep convolutional recurrent network \u201cCR2\u201d achieves a test PER of 18.73%, which is a 2% relative improvement over the novel \u201cRC2\u201d. However, when we apply the residual learning framework to them, we find that \u201cRes-RC2\u201d obtains a test PER of 17.33%, which is an obvious improvement over \u201cRC2\u201d. Furthermore, \u201cRes-CR2\u201d attains a PER of 18.90% with slightly improvement over \u201cCR2\u201d, which probably is caused by the heavy residual block as we discussed previously."}, {"heading": "VIII. CONCLUSIONS", "text": "Our paper presents a detailed experimental comparison of three different acoustic models in speech recognition, they\nare traditional deep convolutional recurrent networks, deep recurrent convolutional networks and deep residual networks. Traditional application of deep CNNs are used for feature preprocessing, followed by recurrent layers and CTC decoding layer, but in practice it takes too much time to converge. Our proposed deep recurrent convolutional network takes the recurrent networks as feature preprocessing, and deep CNNs are designed to depict high-level feature representation. Our Experiments show that, compared to traditional deep convolutional recurrent networks, our novel recurrent convolutional network can converge in less time in the first half and also attain a comparable PER. Besides, we try to apply residual learning framework in our acoustic models. We build an identity mapping as shortcut connection as residual block for each model, and experiments show that our proposed novel deep recurrent convolutional networks can benefit a lot from residual blocks both in accuracy and convergence, however, residual blocks seem to have some negative impacts on the traditional deep convolutional recurrent networks. We present a detailed analysis about their performance according to different training cost curves, and our proposed \u201cRes-RC2\u201d attains the best PER of 17.33%. Our experiments verify that the novel deep recurrent convolutional networks can take place of traditional deep convolutional recurrent networks in ASR with less training time, in particular, residual learning framework can also be applied in deep recurrent convolutional neural network to make great improvement in both convergence speed and recognition accuracy."}, {"heading": "ACKNOWLEDGMENT", "text": "The author would like to thank Chengyou Xie and Qionghaofeng Wu for helpful discussions on automatic speech recognition."}], "references": [{"title": "A Maximum Likelihood Approach to Continuous Speech Recognition.", "author": ["Ieee", "Lalit R. Bahl Member", "F.J.F. Ieee", "R.L. Mercer"], "venue": "Pattern Analysis & Machine Intelligence IEEE Transactions on 5.2(1983),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition.", "author": ["L Bahl"], "venue": "IEEE International Conference on Acoustics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Hidden Markov Models for Speech Recognition.", "author": ["Michaelson", "By S", "M. Steedman"], "venue": "Technometrics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "An Introduction to the Application of the Theory of Probabilistic Functions of a Markov Process to Automatic Speech Recognition.", "author": ["S.E. Levinson", "L.R. Rabiner", "M.M. Sondhi"], "venue": "Bell System Technical Journal", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition.", "author": ["Dahl", "George E"], "venue": "IEEE Transactions on Audio Speech & Language Processing", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition.", "author": ["G Hinton"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Deep Neural Networks for Single-Channel Multi- Talker Speech Recognition.", "author": ["Weng", "Chao"], "venue": "Audio Speech & Language Processing IEEE/ACM Transactions on 23.10(2015),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["A. Graves", "N. Jaitly"], "venue": "International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification.", "author": ["Hwang", "Kyuyeon", "W. Sung"], "venue": "Computer Science", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Adam and others, \u201cDeep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks.", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures.", "author": ["Graves", "Alex", "J. Schmidhuber"], "venue": "Neural Networks", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.", "author": ["Haim Sak", "Andrew Senior", "Franoise Beaufays"], "venue": "Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.", "author": ["Graves", "Alex"], "venue": "International Conference on Machine Learning ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Le Cun", "B Boser", "Denker", "John S", "D Henderson", "Howard", "Richard E", "W Hubbard", "Jackel", "Lawrence D"], "venue": "Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["Srivastava", "Nitish"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.", "author": ["Ioffe", "Sergey", "C. Szegedy"], "venue": "Computer Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines Vinod Nair.", "author": ["Nair", "Vinod", "G.E. Hinton"], "venue": "International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).", "author": ["Clevert", "Djork-Arn", "T. Unterthiner", "S. Hochreiter"], "venue": "Computer Science", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Adam: A method for stochastic optimization\u201d, arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Convolutional Neural Networks for Speech Recognition.", "author": ["Abdel-Hamid", "Ossama"], "venue": "IEEE/ACM Transactions on Audio Speech & Language Processing", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "very Deep Convolutional Neural Networks for Robust Speech Recognition.", "author": ["Qian", "Yanmin", "Philip C. Woodland"], "venue": "arXiv preprint arXiv:1610.00277", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Advances in Very Deep Convolutional Neural Networks for LVCSR.", "author": ["Sercu", "Tom", "Vaibhava Goel"], "venue": "arXiv preprint arXiv:1604.01792", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition.", "author": ["O Abdel-Hamid"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Deep convolutional neural networks for LVCSR.", "author": ["Sainath", "T. N"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Convolutional Neural Networks for Speech Recognition.", "author": ["Abdel-Hamid", "Ossama"], "venue": "IEEE/ACM Transactions on Audio Speech & Language Processing", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input.", "author": ["Palaz", "Dimitri", "Ronan Collobert"], "venue": "Proceedings of Interspeech. No. EPFL-CONF-210029", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.", "author": ["Fukushima", "Kunihiko"], "venue": "Biological Cybernetics", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1980}, {"title": "Improvements to deep convolutional neural networks for LVCSR.", "author": ["Sainath", "T. N"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming"], "venue": "Computer Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky", "Olga"], "venue": "International Journal of Computer Vision,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Microsoft COCO: Common Objects", "author": ["Lin", "Tsung Yi"], "venue": "in Context\u201d,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A variable-length category-based n-gram language model", "author": ["T.R. Niesler", "P.C. Woodland"], "venue": "Icassp IEEE,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1996}, {"title": "Neural Probabilistic Language Models", "author": ["Bengio", "Yoshua"], "venue": "Journal of Machine Learning Research", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Recurrent neural network based language model\u201d, INTERSPEECH 2010, Conference of the International Speech Communication Association, Makuhari", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "NN-grams: Unifying neural network and ngram language models for Speech", "author": ["Damavandi", "Babak"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 261, "endOffset": 264}, {"referenceID": 3, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 266, "endOffset": 269}, {"referenceID": 4, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 158, "endOffset": 161}, {"referenceID": 15, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 371, "endOffset": 375}, {"referenceID": 17, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 377, "endOffset": 381}, {"referenceID": 18, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 383, "endOffset": 387}, {"referenceID": 19, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 389, "endOffset": 393}, {"referenceID": 7, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "Besides, sequence training of RNNs with connectionist temporal classification (CTC) has shown great performance in endto-end ASR [10], [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Besides, sequence training of RNNs with connectionist temporal classification (CTC) has shown great performance in endto-end ASR [10], [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": "CNN is an older deep neural network architecture [29], and has enjoyed the great popularity as a efficient approach in character recognition.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 24, "context": "Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27].", "startOffset": 219, "endOffset": 223}, {"referenceID": 25, "context": "Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27].", "startOffset": 225, "endOffset": 229}, {"referenceID": 20, "context": "[22] used one convolutional layer, one pooling layer and a few full-connected layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 274, "endOffset": 278}, {"referenceID": 22, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 280, "endOffset": 284}, {"referenceID": 24, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 286, "endOffset": 290}, {"referenceID": 28, "context": "kernels have recently been successfully applied in acoustic modeling in hybrid NN-HMM speech recognition system, and pooling layer has been proved to be replaced by full-connected convolutional layers and pooling has no highlights for LVCSR tasks [30].", "startOffset": 247, "endOffset": 251}, {"referenceID": 29, "context": "Deep residual learning framework can obtain compelling accuracy and good convergence performance in computer vision [31], which attributes to its identity mapping as the skip connection.", "startOffset": 116, "endOffset": 120}, {"referenceID": 17, "context": "The most common functions applied to a neuron\u2019s output is ReLU [19] as a function of its input with f(x) = max(0, x).", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "Exponential linear unit (ELU) was introduced in [20], in contrast to ReLU, ELU has negative values which pushes the mean of activations close to zero, this is to say, ELU can decrease the gap between the normal gradient and unit natural gradient and, therefore, speed up training.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Compared to standard fully-connected neural networks and RNNs, CNNs which are proposed in [16] have much fewer parameters so that they are easier to train.", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "similar to ordinary neural networks, they are made of trainable weights and bias and they can also be stacked to a deep depth, which has been successfully applied in ImageNet competition [32].", "startOffset": 187, "endOffset": 191}, {"referenceID": 32, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 110, "endOffset": 114}, {"referenceID": 34, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 116, "endOffset": 120}, {"referenceID": 35, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "In detail, since the Adam optimization method [21] is computationally efficient, requires little memory allocation and is well suitable for training deep learning models with large data, we adopt the Adam method with a learning rate of 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "Recetnly, batch normalization [18] has shown a better regularization performance, however it would add extra parameters and needs heavy data augmentaion, which we would like to avoid.", "startOffset": 30, "endOffset": 34}, {"referenceID": 7, "context": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9].", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9].", "startOffset": 211, "endOffset": 215}, {"referenceID": 24, "context": "Third, as opposite to the traditionally used 6*6 or 3*4 filter and 1*3 pooling in speech recognition [26], we use the small filters of 3*3 to build the full convolutional layers with no pooling layer.", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "Recently, a new residual learning framework has been proposed to ease the training of very deep convolutional neural networks, and deep residual networks [31] have been proved to improve convergence and higher accuracy in image classification with no more extra parameters.", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "In general, h(xl) = xl is an identity mapping and f is an activation function, which we set to be elu [20] here.", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "Although ResNets with over 100 layers have shown great accuracy for several challenging image classification tasks on ImageNet competitions [32] and MS COCO competitions [33], we also want to explore how the residual blocks behave in our experimental models.", "startOffset": 140, "endOffset": 144}, {"referenceID": 31, "context": "Although ResNets with over 100 layers have shown great accuracy for several challenging image classification tasks on ImageNet competitions [32] and MS COCO competitions [33], we also want to explore how the residual blocks behave in our experimental models.", "startOffset": 170, "endOffset": 174}], "year": 2016, "abstractText": "Performance of end-to-end automatic speech recognition (ASR) systems can significantly be improved by the increasing large speech corpus and deeper neural network. Given the arising problem of training speed and recent success of deep convolutional neural network in ASR, we build a novel deep recurrent convolutional network for acoustic modeling and apply deep residual learning framework to it, our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network. We mainly compare convergence speed of two acoustic models, which are novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. With faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable performance. We further show that applying deep residual learning can boost both convergence speed and recognition accuracy of our novel recurret convolutional networks. Finally, we evaluate all our experimental networks by phoneme error rate (PER) with newly proposed bidirectional statistical language model. Our evaluation results show that our model applied with deep residual learning can reach the best PER of 17.33% with fastest convergence speed in TIMIT database.", "creator": "LaTeX with hyperref package"}}}