{"id": "1611.07661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Multigrid Neural Architectures", "abstract": "we propose a multigrid extension of convolutional neural networks ( called cnns ). rather than manipulating representations living on a single spatial grid, our network layers operate across grid scale space, on using a pyramid of tensors. they consume multigrid inputs and slowly produce multigrid distributed outputs ; convolutional filters themselves have both within - scale and cross - scale extent. this aspect is distinct from simple mesh multiscale designs, which only process the input at different scales. viewed in terms of information driven flow, a multigrid network passes messages across forming a spatial pyramid. as a consequence, receptive field size grows exponentially with growing depth, facilitating rapid integration of context. most critically, multigrid coding structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern cnns fail.", "histories": [["v1", "Wed, 23 Nov 2016 06:55:53 GMT  (815kb,D)", "http://arxiv.org/abs/1611.07661v1", "Code available:this http URL"], ["v2", "Thu, 11 May 2017 19:24:33 GMT  (818kb,D)", "http://arxiv.org/abs/1611.07661v2", "updated with ImageNet results; to appear at CVPR 2017"]], "COMMENTS": "Code available:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["tsung-wei ke", "michael maire", "stella x yu"], "accepted": false, "id": "1611.07661"}, "pdf": {"name": "1611.07661.pdf", "metadata": {"source": "CRF", "title": "Neural Multigrid", "authors": ["Tsung-Wei Ke", "Michael Maire", "Stella X. Yu"], "emails": ["twke@icsi.berkeley.edu", "mmaire@ttic.edu", "stellayu@berkeley.edu"], "sections": [{"heading": null, "text": "Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR image classification, flipping from single to multigrid within standard CNN architectures improves accuracy at modest compute and parameter increase. Multigrid is independent of other architectural choices; we show synergistic results in combination with residual connections. On tasks demanding per-pixel output, gains can be substantial. We show dramatic improvement on a synthetic semantic segmentation dataset. Strikingly, we show that relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid could replace virtually all existing CNN designs."}, {"heading": "1. Introduction", "text": "Since Fukushima\u2019s neocognitron [7], the basic architectural design of convolutional neural networks has persisted in form similar to that shown in the top of Figure 1. Processing begins on a high resolution input, of which filters examine small local pieces. Through stacking many layers, in combination with occasional pooling and subsampling, receptive fields slowly grow with depth, eventually\nencompassing the entire input. Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12]. Coupled with large datasets and compute power, this pipeline drives state-of-the-art vision systems.\nHowever, sufficiency of this design does not speak to its optimality. A revolution in performance may have blinded the community to investigating whether or not unfolding computation in this standard manner is the best choice. In fact, there are shortcomings to the typical CNN pipeline:\n\u2022 It conflates abstraction and scale. Early layers cannot see coarser scales, while later layers are forced to do so. For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].\n\u2022 The fine-to-coarse processing within a standard CNN is in opposition to a near universal principle for efficient algorithm design: coarse-to-fine processing. The first layer in a standard CNN consists of many filters independently looking at tiny, almost meaningless regions of the image. Would it not be more reasonable for the system to observe some coarse-scale context before deciding how to probe the details?\n\u2022 Communication is inefficient. A neuron\u2019s receptive field is determined by the units in input layer that could propagate a signal to it. Standard CNNs implement a slow propagation scheme, diffusing information across a single grid at rate proportional to convolutional filter size. This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.\nThese points can be summarized as inherent deficiencies in representation, computation, and communication. Our multigrid architecture (Figure 1, bottom) endows CNNs with additional structural capacity in order to dissolve these deficiencies. It is explicitly multiscale, pushing choices about scale-space representation into the training process.\nar X\niv :1\n61 1.\n07 66\n1v 1\n[ cs\n.C V\n] 2\n3 N\nov 2\n01 6\nComputation occurs in parallel at all scales; every layer process both coarse and fine representations. Section 3.2 also explores coarse-to-fine variants that transition from processing on a coarse pyramid to processing on a full pyramid as the network deepens. Pyramids provide not only an efficient computational model, but a unified one. Viewing the network as evolving a representation living on the pyramid, we can combine previous task-specific architectures. For classification, attach an output to the coarsest pyramid level; for segmentation, attach an output to the finest.\nMultigrid structure facilitates cross-scale information exchange, thereby destroying the long-established notion of receptive field. Most neurons have receptive field equivalent to the entire input; its size grows exponentially with depth, or, in progressive multigrid networks, begins with the full (coarse) input. Quick communication pathways exist throughout the network, and enable new capabilities.\nWe specifically demonstrate that multigrid CNNs, trained in a pure end-to-end fashion, can learn to attend and route information. Routing circuits, as articulated by Olshausen et al. [24], emerge within them. We construct a simple synthetic task that standard CNNs completely fail to learn, but that multigrid CNNs accomplish with ease. Probing network internals reveals attentional capacity to be key.\nAs Section 2 reviews, recent CNN architectural innovations ignore scale-space routing capacity, focusing instead on aspects like depth. Multigrid, as Section 3 details, complements such work. Section 4 measures performance improvements due to multigrid on (CIFAR) classification and (synthetic) semantic segmentation tasks. Multigrid boosts\nboth baseline and residual CNNs. On a synthetic spatial transformation task, we show that multigrid is more than a boost; it is required, as residual networks alone do not possess attentional capacity. Section 5 discusses implications."}, {"heading": "2. Related Work", "text": "In wake of AlexNet [18], exploration of CNNs across computer vision has distilled some rules of thumb for their design. Small, e.g. 3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9]. Feature channels should increase with spatial resolution reduction [18, 28], e.g. doubling as in Figure 1. Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12]. Width also matters [34].\nRecent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction. We focus on different aspects of efficiency, which, while traditionally influencing computer vision, have not yet been brought to bear on CNNs. Coarse-to-fine themes and cascades relate back to early face detection work, e.g. Viola and Jones [31]. Multigrid methods have a history of use in image segmentation algorithms [27, 22].\nThe desire to adapt image classification CNNs to more complex output tasks, such as semantic segmentation, has catalyzed development of ad-hoc architectural additions for restoring spatial resolution. These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-\nduce and re-expand spatial grids [1, 11, 23, 26]. Yet, all of these still fundamentally manipulate a single grid scale at a time. Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication."}, {"heading": "3. Multigrid Architectures", "text": "Figure 1 conveys our intention to wire cross-scale connections into network structure at the lowest level. We can think of a multigrid CNN as a standard CNN in which every grid is transformed into a pyramid. Every convolutional filter extends spatially within grids (x, y), across grids of multiple scales (s) within a pyramid, and over corresponding feature channels (c). A pyramidal slice, across scalespace, of the preceding layer contributes to the response at a particular corresponding neuron in the next.\nThis scheme enables any neuron to transmit a signal up the pyramid, from fine to coarse grids, and back down again. With signals jumping just one pyramid level per consecutive layer, the network can exploit this structure to quickly route information between two spatial locations on the finest grid. Communication time is logarithmic in spatial separation, rather than linear as in standard CNNs.\nThis difference in information routing capability is particularly dramatic in light of the recent trend towards stacking many layers of small 3\u00d7 3 convolutional filters [28, 9]. In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid. Multigrid allows for faster propagation without additional design complexity. Moreover, unlike fixed pooling/unpooling stages, multigrid allows the network to learn data-dependent routing strategies.\nInstead of directly implementing multigrid convolution as depicted in Figure 1, we implement a close alternative that can be easily built out of standard components in existing deep learning frameworks. Figure 2 illustrates the multigrid convolutional layer (mg-conv) we use as a drop-in replacement for standard convolutional (conv) lay-\ners when converting a network from grids to pyramids. With multigrid convolution, we may choose to learn independent filter sets for each scale within a layer, or alternatively, tie parameters between scales and learn shared filters. We learn independent filter sets, assuming that doing so affords the network the chance to squeeze maximum performance out of scale-specific representations. This means that channel counts between grids need not match up in any particular manner. Each of c0, c1, c2, c\u20320, c \u2032 1, c \u2032 2 in Figure 2 is an independent parameter that may be set at whim. For comparisons, we either set channel count on the finest grid to match baseline models, or calibrate channel counts so that total parameters are similar to the baselines. Section 4 reveals quite good performance is achievable by halving channel count with each coarser grid (e.g. c1 = 12c0, c2 = 1 4c0). As a consequence, multigrid adds minimal computational overhead; the coarse grids are cheap. Given recent widespread use of residual networks [9], we would be remiss not to consider a multigrid extension of them. The right side of Figure 2 shows a multigrid analogue of the original residual unit [9]. Convolution acts jointly across the multigrid pyramid, while batch normalization (BN) [15] and ReLU apply separately to each grid in the pyramid. Similar extensions are possible with alternative residual units [10], but we leave such exploration to future work. Our focus is on thorough comparison of baseline, residual, multigrid, and residual-multigrid networks."}, {"heading": "3.1. Pooling", "text": "Notice that within an mg-conv layer, max-pooling and subsampling acts as a lateral communication mechanism from fine to coarse grids. Similarly, upsampling facilitates lateral communication from coarse to fine grids. Rather than locating these operations at a few fixed stages in the pipeline, we are actually inserting them everywhere. However, their action is now lateral, between different-scale grids in the same layer, rather than between grids of different scales in two layers consecutive in depth.\nWhile Figure 1 is drawn with additional max-pooling\nand subsampling stages acting depth-wise on entire pyramids, we also consider pipelines that do not shrink pyramids. Rather, they simply attach an output to a particular grid (and possibly prune a few grids outside of its communication range). The success of this strategy may be motivation for rethinking the role of pooling in CNNs. Standard CNNs may actually misuse it as an explicit summarization mechanism to be employed at select stages. Multigrid yields a view of pooling as an implicit communication mechanism that should pervade the network."}, {"heading": "3.2. Progressive Multigrid", "text": "Thinking only of the communication pattern underlying a forward pass performed by a multigrid CNN, we find a strong analogy to the multiscale multigrid computational scheme of Maire and Yu [22]. Though in a different mathematical setting, one can view their proposed eigensolver as a linear diffusion process on a multiscale pyramid. We view a multigrid CNN as a nonlinear process, on a similar pyramid, with the same communication structure.\nThe analogy is strong enough that, in fact, we directly port their progressive multigrid computation scheme to our setting. A layer of a network is analogous to an iteration of a solver. Rather than starting work directly on the full pyramid, a progressive multigrid CNN can spend several layers processing only a coarse grid. This is followed by several additional layers processing a small pyramid consisting of coarse and medium grids together, before the remainder of the network commits to work with the full pyramid.\nIn all multigrid and progressive multigrid experiments, we set the fine-scale input grid to be the original image and simply feed in downsampled (lower resolution) versions to independent initial convolution layers. The outputs of these initial layers then form a multigrid pyramid which is processed coherently by the rest of the network."}, {"heading": "3.3. Model Zoo", "text": "Figure 3 diagrams the variety of network architectures we evaluate. For classification, we take a network with minor differences from that of Simonyan and Zisserman [28] as our baseline. In slight abuse of notation, we refer to the modified version in Figure 3 as VGG. Our 16-layer version, VGG-16, consists of 5 sections of 3 convolutional layers each, with pooling and subsampling between. A final softmax layer produces class prediction (on CIFAR-100). Convolutional layers all use 3 \u00d7 3 filters. We instantiate variants with different depth by changing the number layers per section (e.g. 2 for VGG-11, 4 for VGG-21). Residual baselines, e.g. RES-22, follow the same layout, but use residual units within each section. Remember that each residual unit contains two convolutional layers.\nMultigrid (NMG) and residual multigrid (R-NMG) networks, starting respectively from VGG or RES, simply flip\nfrom grids to pyramids and convolution to multigrid convolution. Progressive variants (P-NMG, PR-NMG) expand the first section in order to gradually work up to computation on the complete pyramid. Even as their depth increases, a significant portion of layers in progressive networks avoid processing the full pyramid. As diagrammed, multigrid networks match their fine-grid feature count with baselines, and hence add some capacity and parameters. For classification, we also consider smaller (denoted -sm) multigrid variants with fewer channels per grid in order to be closer in parameter count with baseline VGG and RES networks.\nFor the semantic segmentation and spatial transformation tasks detailed in the next section, we use networks that produce per-pixel output. As a baseline, we employ the UNET design of Ronneberger et al. [26]. Again, convolutional filters are 3 \u00d7 3, except for the layers immediately following upsampling; here we use 2 \u00d7 2 filters, following [26]. We examine progressive multigrid alternatives (PNMG, PR-NMG) that continue to operate on the multiscale pyramid. Unlike the classification setting, we do not reduce pyramid resolution. These networks drop some coarse grids towards the end of their pipelines for the sole reason that such grids do not communicate with the output."}, {"heading": "4. Experiments", "text": "Our experiments focus on systematic exploration and evaluation of the architectural design space.1 The goal is to quantify the relative benefits of multigrid and its synergistic combination with residual connections, rather than achieve absolute performance records on any particular dataset."}, {"heading": "4.1. Classification", "text": "We evaluate the array of network architectures listed in Table 1 on the task of CIFAR-100 [17] image classification.\nData preprocessing and augmentation. We first whiten the data and then enlarge each image to 36 \u00d7 36. We use random 32 \u00d7 32 patches from the training set and use the center 32\u00d7 32 patch from the test examples. Each training image is subject to random horizontal flipping. There are 50,000 training images and 10,000 test images.\nTraining procedure. We use SGD with training batch size 128, weight decay rate 0.0005, and 300 iterations per epoch for 200 epochs. For VGG, NMG, and P-NMG, our learning rate starts from 0.1 and exponentially decays to 0.0001; whereas for RES, R-NMG, and PR-NMG, our learning rate starts from 0.1 and decays at rate 0.2 every 60 epochs.\nResults. Deconstructing patterns in Table 1, we see that adding multigrid capacity consistently improves over both basic CNNs (e.g. compare green entries), as well as residual CNNs (e.g. blue entries).\nFurthermore, progressive multigrid nets of similar depth achieve better or comparable accuracy at substantially reduced computational expense, as quantified in terms of both parameters and floating point operations (e.g. red entries). This means they are able to intelligently probe details, using cheaply acquired global, but coarse context.\n1 Torch code for all of our experiments is available online."}, {"heading": "4.2. Semantic Segmentation", "text": "We generate synthetic data for semantic segmentation in the following steps. We first pick randomly about 5\u00b12 digits from the training set of MNIST [20], where the number of digits follows the Gaussian distribution with mean value 5 and standard deviation 0.5. Each digit is rescaled by a uniform random factor between 0.5 and 1.25 of the original size and rotated by a uniform random angle between \u221260\u25e6 and 60\u25e6. We randomly pick a location on a 64\u00d7 64 canvas for each transformed digit and paste it onto the canvas, making sure that each newly added digit does not overlap with existing ones by more than 30%. We generate 10,000 training images and 1,000 test images. For test images, we use digits from the MNIST test to prevent any instance overlap with training.\nIn addition to networks already mentioned, we consider a single grid (SG) baseline that mirrors P-NMG but removes all grids but the finest. We also consider U-NMG, which changes resolution like U-NET, but works on pyramids.\nTraining procedure. We use training batch size 64, weight decay rate 0.0005, and 150 iterations per epoch for 200 epochs. For U-NET, SG, P-NMG, our learning rate starts from 0.1 and exponentially decays to 0.0001; whereas for R-SG, PR-NMG, our learning rate starts from 0.1 and decays at rate 0.2 every 60 epochs.\nResults. Table 2 and Figure 4 show dramatic performance advantages of the progressive multigrid network. We report scores of mean intersection over union (IU) with the true output regions, as well as mean per-pixel multiclass labeling accuracy. These results are a strong indicator that continuous operation on a multigrid pyramid should replace\nnetworks that pass through a low resolution bottleneck."}, {"heading": "4.3. Spatial Transformers", "text": "Jaderberg et al. [16] engineer a system for inverting spatial distortions and translations by combining an neural network for estimating the parameters of the transformation with a dedicated unit for applying the inverse. We report here that such a split of the problem into two components appears necessary when using standard CNNs; they cannot learn the task end-to-end. However, progressive multigrid networks are able to learn such tasks.\nOur synthetic dataset for spatial transformation is generated in the same manner as the one for semantic segmentation, except that (1) there is only one digit, (2) the rescaling factor has a range of (0.5,1.5), and (3) there is an additional affine transformation with a uniformly random sheering transform angle in (60\u25e6, 60\u25e6). The task is no longer labeling, but rather to output the original, undistorted digit instance in the center of the canvas.\nTraining procedure. We use the same training procedure as for MNIST semantic segmentation, except that we use 800 iterations per epoch. We use 60,000 generated training images and 10,000 test images.\nResults. Table 3 and Figure 5 show that all networks except P-NMG and PR-NMG fail to learn the task. The right side of Figure 5 reveals the reason: U-NET (and others not shown) cannot learn translation. This makes sense for single grid methods, as propagating enough information across the fine grid would require them to be deeper than tested. U-NET\u2019s failure seems to stem from confusion due to pooling/upsampling. It appears to paste subregions of the target digit into output, but not in the correct arrangement. Figure 6 reveals that, unlike U-NET, P-NMG and PR-NMG exhibit attentional behavior."}, {"heading": "5. Conclusion", "text": "Our proposed multigrid extension to CNNs yields improved accuracy on classification and semantic segmentation tasks. Progressive multigrid variants open a new pathway towards optimizing CNNs for efficiency. Multigrid appears unique in extending the range of tasks CNNs can accomplish, by integrating into the network structure the capacity to learn routing and attentional mechanisms. These new abilities suggest that multigrid could replace some adhoc mechanisms in the current zoo of CNN architectures.\nTo conclude on a speculative note, multigrid neural networks might also have broader implications in neuroscience. Feedforward computation on a sequence of multi-\ngrid pyramids looks quite similar to combined bottomup/top-down processing across a single larger structure if all neurons are embedded in the larger computational substrate according to their spatial resolution, rather than their depth in the processing chain."}], "references": [{"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv:1511.00561", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "arXiv:1504.04788", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1980}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbelaez", "R. Girshick", "J. Malik"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1603.05027", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Bottom-up and top-down reasoning with convolutional latent-variable models", "author": ["P. Hu", "D. Ramanan"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "venue": "arXiv:1608.06993", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv:1603.09382", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": "arXiv:1602.07360", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "arXiv:1506.02025", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "arXiv:1605.07648", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Progressive multigrid eigensolvers for multiscale spectral segmentation", "author": ["M. Maire", "S.X. Yu"], "venue": "ICCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["A. Newell", "K. Yang", "J. Deng"], "venue": "arXiv:1603.06937", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information", "author": ["B.A. Olshausen", "C.H. Anderson", "D.C.V. Essen"], "venue": "Journal of Neuroscience", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv:1603.05279", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "arXiv:1505.04597", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchy and adaptivity in segmenting visual scenes", "author": ["E. Sharon", "M. Galun", "D. Sharon", "R. Basri", "A. Brandt"], "venue": "Nature, 442:810\u2013813", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "IJCV", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "ICCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "N", "author": ["Z. Yang", "M. Moczulski", "M. Denil"], "venue": "de Freitas, A. J. Smola, L. Song, and Z. Wang. Deep fried convnets. ICCV", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv:1605.07146", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Introduction Since Fukushima\u2019s neocognitron [7], the basic architectural design of convolutional neural networks has persisted in form similar to that shown in the top of Figure 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 29, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 8, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 184, "endOffset": 195}, {"referenceID": 9, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 184, "endOffset": 195}, {"referenceID": 33, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 184, "endOffset": 195}, {"referenceID": 18, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 205, "endOffset": 213}, {"referenceID": 11, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 205, "endOffset": 213}, {"referenceID": 20, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 7, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 1, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 0, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 29, "context": "This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 8, "context": "This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 18, "context": "This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 23, "context": "[24], emerge within them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Related Work In wake of AlexNet [18], exploration of CNNs across computer vision has distilled some rules of thumb for their design.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9].", "startOffset": 89, "endOffset": 100}, {"referenceID": 29, "context": "3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9].", "startOffset": 89, "endOffset": 100}, {"referenceID": 8, "context": "3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9].", "startOffset": 89, "endOffset": 100}, {"referenceID": 17, "context": "Feature channels should increase with spatial resolution reduction [18, 28], e.", "startOffset": 67, "endOffset": 75}, {"referenceID": 27, "context": "Feature channels should increase with spatial resolution reduction [18, 28], e.", "startOffset": 67, "endOffset": 75}, {"referenceID": 29, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 121, "endOffset": 129}, {"referenceID": 12, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 121, "endOffset": 129}, {"referenceID": 28, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 18, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 11, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 33, "context": "Width also matters [34].", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 128, "endOffset": 139}, {"referenceID": 2, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 128, "endOffset": 139}, {"referenceID": 13, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 128, "endOffset": 139}, {"referenceID": 24, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 153, "endOffset": 160}, {"referenceID": 3, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 153, "endOffset": 160}, {"referenceID": 30, "context": "Viola and Jones [31].", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Multigrid methods have a history of use in image segmentation algorithms [27, 22].", "startOffset": 73, "endOffset": 81}, {"referenceID": 21, "context": "Multigrid methods have a history of use in image segmentation algorithms [27, 22].", "startOffset": 73, "endOffset": 81}, {"referenceID": 20, "context": "These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-", "startOffset": 46, "endOffset": 53}, {"referenceID": 1, "context": "These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-", "startOffset": 46, "endOffset": 53}, {"referenceID": 7, "context": "These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "Right: A building block (res-mg-unit) for residual connectivity [9] in multigrid networks.", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 10, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 22, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 25, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 5, "context": "Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication.", "startOffset": 39, "endOffset": 49}, {"referenceID": 4, "context": "Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication.", "startOffset": 39, "endOffset": 49}, {"referenceID": 31, "context": "Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication.", "startOffset": 39, "endOffset": 49}, {"referenceID": 27, "context": "This difference in information routing capability is particularly dramatic in light of the recent trend towards stacking many layers of small 3\u00d7 3 convolutional filters [28, 9].", "startOffset": 169, "endOffset": 176}, {"referenceID": 8, "context": "This difference in information routing capability is particularly dramatic in light of the recent trend towards stacking many layers of small 3\u00d7 3 convolutional filters [28, 9].", "startOffset": 169, "endOffset": 176}, {"referenceID": 0, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 10, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 22, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 25, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 8, "context": "Given recent widespread use of residual networks [9], we would be remiss not to consider a multigrid extension of them.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "The right side of Figure 2 shows a multigrid analogue of the original residual unit [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 14, "context": "Convolution acts jointly across the multigrid pyramid, while batch normalization (BN) [15] and ReLU apply separately to each grid in the pyramid.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Similar extensions are possible with alternative residual units [10], but we leave such exploration to future work.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Bottom: Semantic segmentation (SEG) and spatial transformation (SPT) architectures: U-NET [26], and our progressive multigrid alternative (P-NMG).", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "Progressive Multigrid Thinking only of the communication pattern underlying a forward pass performed by a multigrid CNN, we find a strong analogy to the multiscale multigrid computational scheme of Maire and Yu [22].", "startOffset": 211, "endOffset": 215}, {"referenceID": 27, "context": "For classification, we take a network with minor differences from that of Simonyan and Zisserman [28] as our baseline.", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Again, convolutional filters are 3 \u00d7 3, except for the layers immediately following upsampling; here we use 2 \u00d7 2 filters, following [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Classification We evaluate the array of network architectures listed in Table 1 on the task of CIFAR-100 [17] image classification.", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "We first pick randomly about 5\u00b12 digits from the training set of MNIST [20], where the number of digits follows the Gaussian distribution with mean value 5 and standard deviation 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "[16] engineer a system for inverting spatial distortions and translations by combining an neural network for estimating the parameters of the transformation with a dedicated unit for applying the inverse.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of tensors. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail. Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR image classification, flipping from single to multigrid within standard CNN architectures improves accuracy at modest compute and parameter increase. Multigrid is independent of other architectural choices; we show synergistic results in combination with residual connections. On tasks demanding per-pixel output, gains can be substantial. We show dramatic improvement on a synthetic semantic segmentation dataset. Strikingly, we show that relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid could replace virtually all existing CNN designs.", "creator": "LaTeX with hyperref package"}}}