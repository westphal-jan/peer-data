{"id": "1611.07743", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Tunable Sensitivity to Large Errors in Neural Network Training", "abstract": "when humans somehow learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. we propose incorporating this idea of tunable training sensitivity for hard examples in numerical neural network learning, using a new generalization basis of the cross - entropy gradient step, which can be used in place of the gradient in any gradient - based training method. the generalized gradient is parameterized by a value that controls the sensitivity of the training process corresponds to harder training examples. we tested our method on several benchmark datasets. moreover we propose, and subsequently corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the distributed network. moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross - entropy gradient learner. we therefore conclude that appropriate tunable sensitivity can be helpful for neural grid network learning.", "histories": [["v1", "Wed, 23 Nov 2016 11:14:01 GMT  (63kb,D)", "http://arxiv.org/abs/1611.07743v1", "The paper is accepted to the AAAI 2017 conference"]], "COMMENTS": "The paper is accepted to the AAAI 2017 conference", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["gil keren", "sivan sabato", "bj\u00f6rn w schuller"], "accepted": true, "id": "1611.07743"}, "pdf": {"name": "1611.07743.pdf", "metadata": {"source": "META", "title": "Tunable Sensitivity to Large Errors in Neural Network Training", "authors": ["Gil Keren", "Sivan Sabato", "Bj\u00f6rn Schuller"], "emails": ["gil.keren@uni-passau.de"], "sections": [{"heading": "1 Introduction", "text": "In recent years, neural networks have become empirically successful in a wide range of supervised learning applications, such as computer vision (Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015), speech recognition (Hinton et al. 2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al. 2016). Standard implementations of training feed-forward neural networks for classification are based on gradient-based stochastic optimization, usually optimizing the empirical cross-entropy loss (Hinton 1989).\nHowever, the cross-entropy is only a surrogate for the true objective of supervised network training, which is in most cases to reduce the probability of a prediction error (or in some case BLEU score, word-error-rate, etc). When optimizing using the cross-entropy loss, as we show below, the effect of training examples on the gradient is linear in the prediction bias, which is the difference between the network-predicted class probabilities and the target class probabilities. In particular, a wrong confident prediction induces a larger gradient than a similarly wrong, but less confident, prediction.\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIn contrast, humans sometimes employ a different approach to learning: when learning new concepts, they might ignore the examples they feel they do not understand, and focus more on the examples that are more useful to them. When improving proficiency regarding a familiar concept, they might focus on the harder examples, as these can contain more relevant information for the advanced learner. We make a first step towards incorporating this ability into neural network models, by proposing a learning algorithm with a tunable sensitivity to easy and hard training examples. Intuitions about human cognition have often inspired successful machine learning approaches (Bengio et al. 2009; Cho, Courville, and Bengio 2015; Lake et al. 2016). In this work we show that this can be the case also for tunable sensitivity.\nIntuitively, the depth of the model should be positively correlated with the optimal sensitivity to hard examples. When the network is relatively shallow, its modeling capacity is limited. In this case, it might be better to reduce sensitivity to hard examples, since it is likely that these examples cannot be modeled correctly by the network, and so adjusting the model according to these examples might only degrade overall prediction accuracy. On the other hand, when the network is relatively deep, it has a high modeling capacity. In this case, it might be beneficial to allow more sensitivity to hard examples, thereby possibly improving the accuracy of the final learned model.\nOur learning algorithm works by generalizing the crossentropy gradient, where the new function can be used instead of the gradient in any gradient-based optimization method for neural networks. Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015). The proposed generalization is parameterized by a value k > 0, that controls the sensitivity of the training process to hard examples, replacing the fixed dependence of the cross-entropy gradient. When k = 1 the proposed update rule is exactly the cross-entropy gradient. Smaller values of k decrease the sensitivity during training to hard examples, and larger values of k increase it.\nWe report experiments on several benchmark datasets. These experiments show, matching our expectations, that in almost all cases prediction error is improved using large values of k for deep networks, small values of k for shallow net-\nar X\niv :1\n61 1.\n07 74\n3v 1\n[ st\nat .M\nL ]\n2 3\nN ov\n2 01\nworks, and values close to the default k = 1 for networks of medium depth. They further show that using a tunable sensitivity parameter generally improves the results of learning.\nThe paper is structured as follows: In Section 1.1 related work is discussed. Section 2 presents our setting and notation. A framework for generalizing the loss gradient is developed in Section 3. Section 4 presents desired properties of the generalization, and our specific choice is given in Section 5. Experiment results are presented in Section 6, and we conclude in Section 7. Some of the analysis, and additional experimental results, are deferred to the supplementary material due to lack of space."}, {"heading": "1.1 Related Work", "text": "The challenge of choosing the best optimization objective for neural network training is not a new one. In the past, the quadratic loss was typically used with gradient-based learning in neural networks (Rumelhart, Hinton, and Williams 1988), but a line of studies demonstrated both theoretically and empirically that the cross-entropy loss has preferable properties over the quadratic-loss, such as better learning speed (Levin and Fleisher 1988), better performance (Golik, Doetsch, and Ney 2013) and a more suitable shape of the error surface (Glorot and Bengio 2010). Other cost functions have also been considered. For instance, a novel cost function was proposed in (Silva et al. 2006), but it is not clearly advantageous to cross-entropy. The authors of (Bahdanau et al. 2015) address this question in a different setting of sequence prediction.\nOur method allows controlling the sensitivity of the training process to examples with a large prediction bias. When this sensitivity is low, the method can be seen as a form of implicit outlier detection or noise reduction. Several previous works attempt to explicitly remove outliers or noise in neural network training. In one work (Smith and Martinez 2011), data is preprocessed to detect label noise induced from overlapping classes, and in another work (Jeatrakul, Wong, and Fung 2010) the authors use an auxiliary neural network to detect noisy examples. In contrast, our approach requires a minimal modification on gradient-based training algorithms for neural networks and allows emphasizing examples with a large prediction bias, instead of treating these as noise.\nThe interplay between \u201ceasy\u201d and \u201chard\u201d examples during neural network training has been addressed in the framework of Curriculum Learning (Bengio et al. 2009). In this framework it is suggested that training could be more successful if the network is first presented with easy examples, and harder examples are gradually added to the training process. In another work (Kumar, Packer, and Koller 2010), the authors define easy and hard examples based on the fit to the current model parameters. They propose a curriculum learning algorithm in which a tunable parameter controls the proportions of easy and hard examples presented to a learner at each phase. Our method is simpler than curriculum learning approaches, in that the examples can be presented at random order to the network. In addition, our method allows also a heightened sensitivity to harder examples. In a more recent work (Zaremba and Sutskever 2014), the authors indeed find\nthat a curriculum in which harder examples are presented in early phases outperforms a curriculum that at first uses only easy examples."}, {"heading": "2 Setting and Notation", "text": "For any integer n, denote [n] = {1, . . . , n}. For a vector v, its i\u2019th coordinate is denoted v(i).\nWe consider a standard feed-forward multilayer neural network (Svozil, Kvasnicka, and Pospichal 1997), where the output layer is a softmax layer (Bridle 1990), with n units, each representing a class. Let \u0398 denote the neural network parameters, and let zj(x; \u0398) denote the value of output unit j when the network has parameters \u0398, before the applying the softmax function. Applying the softmax function, the probability assigned by the network to class j is\npj(x; \u0398) := e zj/ n\u2211 i=1 ezi . The label predicted by the network for example x is y\u0302(x; \u0398) = argmaxj\u2208[n] pj(x; \u0398). We consider the task of supervised learning of \u0398, using a labeled training sample S = {(xi, yi)}mi=1, , where yi \u2208 [n], by\noptimizing the loss function: L(\u0398) := m\u2211 i=1 `((xi, yi); \u0398). A popular choice for ` is the cross-entropy cost function, defined by `((x, y); \u0398) := \u2212 log py(x; \u0398)."}, {"heading": "3 Generalizing the gradient", "text": "Our proposed method allows controlling the sensitivity of the training procedure to examples on which the network has large errors in prediction, by means of generalizing the gradient. A na\u0131\u0308ve alternative towards the same goal would be using an exponential version of the cross-entropy loss: ` = \u2212| log(py)k|, where py is the probability assigned to the correct class and k is a hyperparameter controlling the sensitivity level. However, the derivative of this function with respect to py is an undesired term since it is not monotone in k for a fixed py , resulting in lack of relevant meaning for small or large values of k. The gradient resulting from the above form is of a desired form only for k = 1, due to cancellation of terms from the derivatives of l and the softmax function. Another na\u0131\u0308ve option would be to consider l = \u2212 log(pky), but this is only a scaled version of the cross-entropy loss and amounts to a change in the learning rate.\nIn general, controlling the loss function alone is not sufficient for controlling the relative importance to the training procedure of examples on which the network has large and small errors in prediction. Indeed, when computing the gradients, the derivative of the loss function is being multiplied by the derivative of the softmax function, and the latter is a term that also contains the probabilities assigned by the model to the different classes. Alternatively, controlling the parameters updates themselves, as we describe below, is a more direct way of achieving the desired effect.\nLet (x, y) be a single labeled example in the training set, and consider the partial derivative of `(\u0398; (x, y)) with respect to some parameter \u03b8 in \u0398. We have\n\u2202`((x, y); \u0398)\n\u2202\u03b8 = n\u2211 j=1 \u2202` \u2202zj \u2202zj \u2202\u03b8 ,\nwhere zj is the input to the softmax layer when the input example is x, and the network parameters are \u0398.\nIf ` is the cross-entropy loss, we have \u2202`\u2202zj = \u2202` \u2202py \u2202py \u2202zj and\n\u2202` \u2202py = \u2212 1 py ,\n\u2202py \u2202zj = { py(1\u2212 py) j = y, \u2212pypj j 6= y.\nHence \u2202`\n\u2202zj = { pj \u2212 1 y = j pj otherwise.\nFor given x, y,\u0398, define the prediction bias of the network for example x on class j, denoted by j , as the (signed) difference between the probability assigned by the network to class j and the probability that should have been assigned, based on the true label of this example. We get j = pj \u2212 1 for j = y, and j = pj otherwise. Thus, for the crossentropy loss,\n\u2202` \u2202\u03b8 = n\u2211 j=1 \u2202zj \u2202\u03b8 j . (1)\nIn other words, when using the cross entropy loss, the effect of any single training example on the gradient is linear in the prediction bias of the current network on this example.\nAs discussed in Section 1, it is likely that in many cases, the results of training could be improved if the effect of a single example on the gradient is not linear in the prediction bias. Therefore, we propose a generalization of the gradient that allows non-linear dependence in .\nFor given x, y,\u0398 and for j \u2208 {1, . . . , n}, define f : [\u22121, 1]n \u2192 Rn, let = ( 1, . . . , n), and consider the following generalization of \u2202`\u2202\u03b8 :\ng(\u03b8) := n\u2211 j=1 \u2202zj \u2202\u03b8 fj( ). (2)\nHere fj is the j\u2019th component of f . When f is the identity, we have fj( ) \u2261 \u2202`\u2202zj , and g(\u03b8) = \u2202` \u2202\u03b8 . However, we are now at liberty to study other assignments for f . We call the vector of values of g(\u03b8) for \u03b8 in \u0398 a pseudogradient, and propose to use g in place of the gradient within any gradient-based algorithm. In this way, optimization of the cross-entropy loss is replaced by a different algorithm of a similar form. However, as we show in Section 5.2, g is not necessarily the gradient of any loss function.\n4 Properties of f Consider what types of functions are reasonable to use for f instead of the identity. First, we expect f to be monotonic non-decreasing, so that a larger prediction bias never results in a smaller update. This is a reasonable requirement if we cannot identify outliers, that is, training examples that have a wrong label. We further expect f to be positive when j 6= y and negative otherwise.\nIn addition to these natural properties, we introduce an additional property that we wish to enforce. To motivate\nthis property, we consider the following simple example. Assume a network with one hidden layer and a softmax layer (see Figure 1), where the inputs to the softmax layer are zj = \u3008wj , h\u3009 + bj and the outputs of the hidden layer are h(i) = \u3008w\u2032i, x\u3009 + b\u2032i, where x is the input vector, and b\u2032i, w \u2032 i are the scalar bias and weight vector between the input layer and the hidden layer. Suppose that at some point during training, hidden unit i is connected to all units j in the softmax layer with the same positive weight a. In other words, for all j \u2208 [n], wj(i) = a. Now, suppose that the training process encounters a training example (x, y), and let l be some input coordinate.\nWhat is the change to the weight w\u2032i(l) that this training example should cause? Clearly it need not change if x(l) = 0, so we consider the case x(l) 6= 0. Only the value h(i) is directly affected by changing w\u2032i(l). From the definition of pj(x; \u0398), the predicted probabilities are fully determined by the ratios ezj/ezj\u2032 , or equivalently, by the differences zj \u2212 zj\u2032 , for all j, j\u2032 \u2208 [n]. Now, zj \u2212 zj\u2032 = \u3008wj , h\u3009 + bj \u2212 \u3008wj\u2032 , h\u3009 + bj\u2032 . Therefore, \u2202(zj\u2212zj\u2032 ) \u2202h(i) = wj(i) \u2212 wj\u2032(i) = a\u2212 a = 0, and therefore \u2202(zj \u2212 zj\u2032) \u2202w\u2032i(l) = \u2202(zj \u2212 zj\u2032) \u2202h(i) \u2202h(i) \u2202w\u2032i(l) = 0. We conclude that in the case of equal weights from unit i to all output units, there is no reason to change the weightw\u2032i(l) for any l. Moreover, preliminary experiments show that in these cases it is desirable to keep the weight stationary, as otherwise it can cause numerical instability due to explosion or decay of weights.\nTherefore, we would like to guarantee this behavior also for our pseudo-gradients. Therefore, we require g(w\u2032i(l)) = 0 in this case. It follows that\n0 = g(w\u2032i(l)) = n\u2211 j=1 \u2202zj \u2202w\u2032i(l) f( j)\n= n\u2211 j=1 \u2202zj \u2202h(i) \u2202h(i) \u2202w\u2032i(l) f( j) = n\u2211 j=1 a \u00b7 x(l) \u00b7 f( j).\nDividing by a \u00b7x(l), we get the following desired property for the function f , for any vector of prediction biases:\nfy( ) = \u2212 \u2211 j 6=y fj( ). (3)\nNote that this indeed holds for the cross-entropy loss, since\u2211 j\u2208[n] j = 0, and in the case of cross-entropy, f is the identity.\n5 Our choice of f In the case of the cross-entropy, f is the identity, leading to a linear dependence on . A natural generalization is to consider higher order polynomials. Combining this approach with the requirement in Eq. (3), we get the following assignment for f , where k > 0 is a parameter.\nfj( ) = \u2212| y| k j = y,\n| y|k\u2211 i6=y ki \u00b7 kj otherwise. (4)\nThe expression | y| k\u2211\ni6=y ki\nis a normalization term which makes\nsure Eq. (3) is satisfied. Setting k = 1, we get that g(\u03b8) is the gradient of the cross-entropy loss. Other values of k result in different pseudo-gradients.\nTo illustrate the relationship between the value of k and the effect of prediction biases of different sizes on the pseudo-gradient, we plot fy( ) as a function of y for several values of k (see Figure 2). Note that absolute values of the pseudo-gradient are of little importance, since in gradient-based algorithms, the gradient (or in our case, the pseudo-gradient) is usually multiplied by a scalar learning rate which can be tuned.\nAs the figure shows, when k is large, the pseudo-gradient is more strongly affected by large prediction biases, compared to small ones. This follows since | | k\n| \u2032|k is monotonic increasing in k for > \u2032. On the other hand, when using a small positive k we get that | | k\n| \u2032|k tends to 1, therefore, the pseudo-gradient in this case would be much less sensitive to examples with large prediction biases. Thus, the choice of f , parameterized by k, allows tuning the sensitivity of the training process to large errors. We note that there could be other reasonable choices for f which have similar desirable properties. We leave the investigation of such other choices to future work."}, {"heading": "5.1 A Toy Example", "text": "To further motivate our choice of f , we describe a very simple example of a distribution and a neural network. Consider a neural network with no hidden layers, and only one input unit connected to two softmax units. Denoting the input by x, the input to softmax unit i is zi = wix+ bi, where wi and bi are the network weights and biases respectively.\nIt is not hard to see that the set of possible prediction functions x 7\u2192 y\u0302(x; \u0398) that can be represented by this network is exactly the set of threshold functions of the form y\u0302(x; \u0398) = sign(x\u2212 t) or y\u0302(x; \u0398) = \u2212sign(x\u2212 t).\nFor convenience assume the labels mapped to the two softmax units are named {\u22121,+1}. Let \u03b1 \u2208 ( 12 , 1), and suppose that labeled examples are drawn independently at random from the following distribution D over R\u00d7 {\u22121,+1}: Examples are uniform in [\u22121, 1]; Labels of examples in [0, \u03b1] are deterministically 1, and they are \u22121 for all other examples. For this distribution, the prediction function with the smallest prediction error that can be represented by the network is x 7\u2192 sign(x).\nHowever, optimizing the cross-entropy loss on the distribution, or in the limit of a large training sample, would result in a different threshold, leading to a larger prediction error (for a detailed analysis see Appendix A in the supplementary material). Intuitively, this can be traced to the fact that the examples in (\u03b1, 1] cannot be classified correctly by this network when the threshold is close to 0, but they still affect the optimal threshold for the cross-entropy loss.\nThus, for this simple case, there is motivation to move away from optimizing the cross-entropy, to a different update rule that is less sensitive to large errors. This reduced sensitivity is achieved by our update rule with k < 1. On the other hand, larger values of k would result in higher sensitivity to large errors, thereby degrading the classification accuracy even more.\nWe thus expect that when training the network using our new update rule, the prediction error of the resulting network should be monotonically increasing with k, hence values of k which are smaller than 1 would give a smaller error. We tested this hypothesis by training this simple network on a synthetic dataset generated according to the distribution D described above, with \u03b1 = 0.95.\nWe generated 30,000 examples for each of the training, validation and test datasets. The biases were initialized to 0 and the weights were initialized from a uniform distribution on (\u22120.1, 0.1). We used batch gradient descent with a learning rate of 0.01 for optimization of the four parameters, where the gradient is replaced with the pseudo-gradient\nfrom Eq. (2), using the function f defined in Eq. (4). f is parameterized by k, and we performed this experiment using values of k between 0.0625 and 4. After each epoch, we computed the prediction error on the validation set, and training was stopped after 3000 epochs in which this error was not changed by more than 0.001%. The values of the parameters at the end of training were used to compute the misclassification rate on the test set.\nTable 2 reports the results for these experiments, averaged over 10 runs for each value of k. The results confirm our hypothesis regarding the behavior of the network for the different values of k, and further motivate the possible benefits of using k 6= 1. Note that while the prediction error is monotonic in k in this experiment, the cross-entropy is not, again demonstrating the fact that optimizing the cross-entropy is not optimal in this case."}, {"heading": "5.2 Non-existence of a Cost Function for f", "text": "It is natural to ask whether, with our choice of f in Eq. (4), g(\u03b8) is the gradient of another cost function, instead of the cross-entropy. The following lemma demonstrates that this is not the case.\nLemma 1. Assume f as in Eq. (4) with k 6= 1, and g(\u0398) the resulting pseudo-gradient. There exists a neural network for which the g(\u0398) is not a gradient of any cost function.\nThe proof of is lemma is left for the supplemental material. Note that the above lemma does not exclude the possi-\nbility that a gradient-based algorithm that uses g instead of the gradient still somehow optimizes some cost function."}, {"heading": "6 Experiments", "text": "For our experiments, we used four classification benchmark datasets from the field of computer vision: The MNIST dataset (LeCun et al. 1998), the Street View House Numbers dataset (SVHN) (Netzer et al. 2011) and the CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton 2009). A more detailed description of the datasets can be found in Appendix C.1 in the supplementary material.\nThe neural networks we experimented with are feedforward neural networks that contain one, three or five hidden layers of various layer sizes. For optimization, we used stochastic gradient descent with momentum (Sutskever et al. 2013) with several values of momentum and a minibatch size of 128 examples. For each value of k, we replaced the gradient in the algorithm with the pseudo-gradient from Eq. (2), using the function f defined in Eq. (4). For the multilayer experiments we also used Gradient-Clipping (Pascanu, Mikolov, and Bengio 2013) with a threshold of 100. In the hidden layers, biases were initialized to 0 and for the weights we used the initialization scheme from (Glorot and Bengio 2010). Both biases and weights in the softmax layer were initialized to 0.\nIn each experiment, we used cross-validation to select the best value of k. The learning rate was optimized using crossvalidation for each value of k separately, as the size of the pseudo-gradient can be significantly different between different values of k, as evident from Eq. (4). We compared the test error between the models using the selected k and k = 1, each with its best performing learning rate. Additional details about the experiment process can be found in Appendix C.2 in the supplementary material.\nWe report the test error of each of the trained models for MNIST, SVHN, CIFAR-10 and CIFAR-100 in Tables 1, 3 and 4 for networks with one, three and five layers respectively. Additional experiments are reported in Appendix C.1 in the supplementary material. We further report the crossentropy values using the selected k and the default k = 1.\nSeveral observations are evident from the experiment results. First, aligned with our hypothesis, the value of k selected by the cross-validation scheme was almost always smaller than 1, for the shallow networks, larger than one for the deep networks, and close to one for networks with medium depth. Indeed, the capacity of network is positively correlated with the optimal sensitivity to hard examples.\nSecond, for the shallow networks the cross-entropy loss on the test set was always worse for the selected k than for k = 1. This implies that indeed, by using a different value of k we are not optimizing the cross-entropy loss, yet are improving the success of optimizing the true prediction error. On the contrary, in the experiments with three and five layers, the cross entropy is also improved by selecting the larger k. This is an interesting phenomenon, which might be explained by the fact that examples with a large prediction bias have a high cross-entropy loss, and so focusing training on these examples reduces the empirical cross-entropy loss, and therefore also the true cross-entropy loss.\nTo summarize, our experiments show that overall, crossvalidating over the value of k usually yields improved results over k = 1, and that, as expected, the optimal value of k grows with the depth of the network."}, {"heading": "7 Conclusions", "text": "Inspired by an intuition in human cognition, in this work we proposed a generalization of the cross-entropy gradient step in which a tunable parameter controls the sensitivity of the training process to hard examples. Our experiments show that, as we expected, the optimal level of sensitivity to hard examples is positively correlated with the depth of the network. Moreover, the experiments demonstrate that selecting the value of the sensitivity parameter using cross validation leads overall to improved prediction error performance on a variety of benchmark datasets.\nThe proposed approach is not limited to feed-forward neural networks \u2014 it can be used in any gradient-based training algorithm, and for any network architecture. In future work, we plan to study this method as a tool for improving training in other architectures, such as convolutional networks and recurrent neural networks, as well as experimenting with different levels of sensitivity to hard examples in different stages of the training procedure, and combining the predictions of models with different levels of this sensitivity."}, {"heading": "Acknowledgments", "text": "This work has been supported by the European Communitys Seventh Framework Programme through the ERC Starting Grant No. 338164 (iHEARu). Sivan Sabato was supported in part by the Israel Science Foundation (grant No. 555/15)."}, {"heading": "A Proof for Toy Example", "text": "Consider the neural network from the toy example in Section 5.1. In this network, there exists one classification threshold such that examples above or below it are classified to different classes. We prove that for a large enough training set, the value of the cross-entropy cost is not minimal when the threshold is at 0.\nSuppose that there is an assignment of network parameters that minimizes the cross-entropy which induces a threshold at 0. The output of the softmax layer is determined uniquely by e z0\nez1 , or equivalently by z0\u2212z1 = x(w0\u2212w1)+ b0\u2212b1. Therefore, we can assume without loss of generality that w1 = b1 = 0. Denote w := w0, b := b0. If w = 0 in the minimizing assignment, then all examples are classified as members of the same class and in particular, the classification threshold is not zero. Therefore we may assume w 6= 0. In this case, the classification threshold is \u2212bw . Since we assume a minimal solution at zero, the minimizing assignment must have b = 0.\nWhen the training set size approaches infinity, the crossentropy on the sample approaches the expected crossentropy on D. Let CE(w, b) be the expected cross-entropy on D for network parameter values w, b. Then\nCE(w, b) = \u22121 2 ( \u222b 0 \u22121 log(p0(x))dx+ \u222b \u03b1 0 log(p1(x))dx\n+ \u222b 1 \u03b1 log(p0(x))dx ) .\nAnd we have:\nlog(p0(x)) = log ewx+b\newx+b + 1 = wx+ b\u2212 log(ewx+b + 1),\nlog(p1(x)) = log 1\newx+b + 1 = \u2212 log(ewx+b + 1).\nTherefore \u2202CE(w, b)\n\u2202b =\n\u2212 1 2 \u2202 \u2202b (\u222b 0 \u22121 (wx+ b)dx\u2212 \u222b 0 \u22121 log(ewx+b + 1)dx\n\u2212 \u222b \u03b1\n0\nlog(ewx+b + 1)dx\n+ \u222b 1 \u03b1 (wx+ b)dx\u2212 \u222b 1 \u03b1 log(ewx+b + 1)dx )\n= \u22121 2 (1\u2212 \u2202 \u2202b (\u222b 1 \u22121 log(ewx+b + 1)dx ) + 1\u2212 \u03b1).\nDifferentiating under the integral sign, we get\n\u2202CE(w, b) \u2202b = \u22121 2\n( 2\u2212 \u03b1\u2212 \u222b 1 \u22121 ewx+b ewx+b + 1 )\nSince we assume the cross-entropy has a minimal solution with b = 0, we have\n0 = \u22122\u2202CE(w, b = 0) \u2202b\n= 2\u2212 \u03b1\u2212 1 w (log(ew + 1)\u2212 log(e\u2212w + 1)).\nTherefore\nw(2\u2212 \u03b1) = log e w + 1\ne\u2212w + 1 = log(ew) = w.\nSince \u03b1 6= 1, it must be that w = 0. This contradicts our assumption, hence the cross-entropy does not have a minimal solution with a threshold at 0."}, {"heading": "B Proof of Lemma 1", "text": "Proof. Consider a neural network with three units in the output layer, and at least one hidden layer. Let (x, y) be a labeled example, and suppose that there exists some cost function \u00af\u0300((x, y); \u0398), differentiable in \u0398, such that for g as defined in Eq. (2) and f defined in Eq. (4) for some k > 0, we have g(\u03b8) = \u2202 \u00af\u0300\u2202\u03b8 for each parameter \u03b8 in \u0398. We now show that this is only possible if k = 1.\nUnder the assumption on \u00af\u0300, for any two parameters \u03b81, \u03b82,\n\u2202\n\u2202\u03b82\n( \u2202 \u00af\u0300\n\u2202\u03b81\n) = \u22022 \u00af\u0300\n\u2202\u03b81\u03b82 =\n\u2202\n\u2202\u03b81\n( \u2202 \u00af\u0300\n\u2202\u03b82\n) ,\nhence \u2202g(\u03b81)\n\u2202\u03b82 = \u2202g(\u03b82) \u2202\u03b81 . (5)\nRecall our notations: h(i) is the output of unit i in the last hidden layer before the softmax layer, wj(i) is the weight between the hidden unit i in the last hidden layer, and unit j in the softmax layer, zj is the input to unit j in the softmax layer, and bj is the bias of unit j in the softmax layer.\nLet (x, y) such that y = 1. From Eq. (5) and Eq. (2), we have\n\u2202\n\u2202w2(1) n\u2211 j=1 \u2202zj \u2202w1(1) fj( ) = \u2202 \u2202w1(1) n\u2211 j=1 \u2202zj \u2202w2(1) fj( ).\nPlugging in f as defined in Eq. (4), and using the fact that \u2202zj\n\u2202wi(1) = 0 for i 6= j, we get:\n\u2212 \u2202 \u2202w2(1)\n( \u2202z1\n\u2202w1(1) \u00b7 | 1|k\n) =\n\u2202\n\u2202w1(1)\n( \u2202z2\n\u2202w2(1) \u00b7\nk 2\nk2 + k 3\n\u00b7 | 1|k ) .\nSince y = 1, we have = (p1 \u2212 1, p2, p3). In addition, \u2202zj\n\u2202wj(1) = h(1) and \u2202h(1)\u2202wj(1) = 0 for j \u2208 [2]. Therefore\n\u2212 \u2202 \u2202w2(1)\n( (1\u2212 p1)k ) = (6)\n\u2202\n\u2202w1(1)\n( pk2\npk2 + p k 3\n(1\u2212 p1)k ) .\nNext, we evaluate each side of the equation separately, using the following:\n\u2202pj \u2202wj(1) = \u2202pj \u2202zj \u2202zj \u2202wj(1) = h(1)pj(1\u2212 pj),\n\u2200j 6= i, \u2202pj \u2202wi(1) = \u2202pj \u2202zi \u2202zi \u2202wi(1) = \u2212h(1)pipj .\nFor the LHS of Eq. (6), we have\n\u2212 \u2202 \u2202w2(1) (1\u2212 p1)k = \u2212k(1\u2212 p1)k\u22121h(1)p1p2.\nFor the RHS,\n\u2202\n\u2202w1(1) pk2 pk2 + p k 3 (1\u2212 p1)k = \u2212 kh(1)p1p k 2(1\u2212 p1)k pk2 + p k 3 .\nHence Eq. (6) holds if and only if:\n1 = pk\u221212 (1\u2212 p1) pk2 + p k 3 .\nFor k = 1, this equality holds since p1 + p2 + p3 = 1. However, for any k 6= 1, there are values of p1, p2, p3 such that this does not hold. We conclude that our choice of f does not lead to a pseudo-gradient g which is the gradient of any cost function."}, {"heading": "C Additional Experiment details and Results", "text": "C.1 datasets The MNIST dataset (LeCun et al. 1998), consisting of grayscale 28x28 pixel images of handwritten digits, with 10 classes, 60,000 training examples and 10,000 test examples, the Street View House Numbers dataset (SVHN) (Netzer et al. 2011), consisting of RGB 32x32 pixel images of digits cropped from house numbers, with 10 classes 73,257 training examples and 26,032 test examples and the CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton 2009), consisting of RGB 32x32 pixel images of 10/100 object classes, with 50,000 training examples and 10,000 test examples. All datasets were linearly transformed such that all features are in the interval [\u22121, 1].\nC.2 Choosing the value of k In each experiment, we used cross-validation to select the best value of k. For networks with one hidden layer, k was selected out of the values {4, 2, 1, 0.5, 0.25, 0.125, 0.0625}. For networks with 3 or 5 hidden layers, k was selected out of the values {4, 2, 1, 0.5, 0.25}, removing the smaller values of k due to performance considerations (in preliminary experiments, these small values yielded poor results for deep networks). The learning rate was optimized using cross-validation for each value of k separately, as the size of the pseudo-gradient can be significantly different between different values of k, as evident from Eq. (4).\nFor each experiment configuration, defined by a dataset, network architecture and momentum, we selected an initial learning rate \u03b7, based on preliminary experiments on the training set. Then the following procedure was carried out for \u03b7/2, \u03b7, 2\u03b7, for every tested value of k:\n1. Randomly split the training set into 5 equal parts, S1, . . . , S5.\n2. Run the iterative training procedure on S1 \u222a S2 \u222a S3, until there is no improvement in test prediction error for 15 epochs on the early stopping set, S4.\n3. Select the network model that did the best on S4. 4. Calculate the validation error of the selected model on the\nvalidation set, S5. 5. Repeat the process for t times after permuting the roles\nof S1, . . . , S5. We set t = 10 for MNIST, and t = 7 for CIFAR-10/100 and SVHN.\n6. Let errk,\u03b7 be the average of the t validation errors. We then found argmin\u03b7 errk,\u03b7 . If the minimum was found with the minimal or the maximal \u03b7 that we tried, we also performed the above process using half the \u03b7 or double the \u03b7, respectively. This continued iteratively until there was no need to add learning rates. At the end of this process we selected (k\u2217, \u03b7\u2217) = argmink,\u03b7 errk,\u03b7 , and retrained the network with parameters k\u2217, \u03b7\u2217 on the training sample, using one fifth of the sample as an early stopping set. We compared the test error of the resulting model to the test error of a model retrained in the same way, except that we set k = 1 (leading to standard cross-entropy training), and the learning rate to \u03b7\u22171 = argmin\u03b7 err1,\u03b7 . The final learning rates in the selected models were in the range [10\u22121, 10] for MNIST, and [10\u22124, 1] for the other datasets.\nC.3 Results Additional experiment results with momentum values other than 0.5 are reported in Table 5, Table 6."}], "references": [{"title": "Task loss estimation for sequence prediction", "author": ["D. Bahdanau", "D. Serdyuk", "P. Brakel", "N.R. Ke", "J. Chorowski", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1511.06456.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proc. of the 26th annual International Conference on Machine Learning (ICML), 41\u2013", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "Neurocomputing. Springer. 227\u2013236.", "citeRegEx": "Bridle,? 1990", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["K. Cho", "A. Courville", "Y. Bengio"], "venue": "IEEE Transactions on Multimedia 17(11):1875\u2013 1886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. of International Conference on Artificial Intelligence and Statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Cross-entropy vs", "author": ["P. Golik", "P. Doetsch", "H. Ney"], "venue": "squared error training: a theoretical and experimental comparison. In Proc. of INTERSPEECH, 1756\u20131760.", "citeRegEx": "Golik et al\\.,? 2013", "shortCiteRegEx": "Golik et al\\.", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. N Sainath"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Connectionist learning procedures", "author": ["G.E. Hinton"], "venue": "Artificial intelligence 40(1):185\u2013234.", "citeRegEx": "Hinton,? 1989", "shortCiteRegEx": "Hinton", "year": 1989}, {"title": "Data cleaning for classification using misclassification analysis", "author": ["P. Jeatrakul", "K.W. Wong", "C.C. Fung"], "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics 14(3):297\u2013302.", "citeRegEx": "Jeatrakul et al\\.,? 2010", "shortCiteRegEx": "Jeatrakul et al\\.", "year": 2010}, {"title": "Convolutional RNN: an enhanced model for extracting features from sequential data", "author": ["G. Keren", "B. Schuller"], "venue": "Proc. of 2016 International Joint Conference on Neural Networks (IJCNN), 3412\u20133419.", "citeRegEx": "Keren and Schuller,? 2016", "shortCiteRegEx": "Keren and Schuller", "year": 2016}, {"title": "Convolutional neural networks with data augmentation for classifying speakers native language", "author": ["G. Keren", "J. Deng", "J. Pohjalainen", "B. Schuller"], "venue": "Proc. of INTERSPEECH, 2393\u20132397.", "citeRegEx": "Keren et al\\.,? 2016", "shortCiteRegEx": "Keren et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. of Advances in Neural Information Processing Systems (NIPS), 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "Proc. of Advances in Neural Information Processing Systems (NIPS), 1189\u20131197.", "citeRegEx": "Kumar et al\\.,? 2010", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv preprint arXiv:1604.00289.", "citeRegEx": "Lake et al\\.,? 2016", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Accelerated learning in layered neural networks", "author": ["E. Levin", "M. Fleisher"], "venue": "Complex systems 2:625\u2013640.", "citeRegEx": "Levin and Fleisher,? 1988", "shortCiteRegEx": "Levin and Fleisher", "year": 1988}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning. Granada, Spain.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), 1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics 4(5):1\u201317.", "citeRegEx": "Polyak,? 1964", "shortCiteRegEx": "Polyak", "year": 1964}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling 5:3.", "citeRegEx": "Rumelhart et al\\.,? 1988", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "New developments of the Z-EDM algorithm", "author": ["L.M. Silva", "J.M. De Sa", "L Alexandre"], "venue": "In Intelligent Systems Design and Applications,", "citeRegEx": "Silva et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2006}, {"title": "Improving classification accuracy by identifying and removing instances that should be misclassified", "author": ["M.R. Smith", "T. Martinez"], "venue": "The 2011 International Joint Conference on Neural Networks (IJCNN), 2690\u20132697.", "citeRegEx": "Smith and Martinez,? 2011", "shortCiteRegEx": "Smith and Martinez", "year": 2011}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proc. of the 30th International Conference on Machine Learning (ICML), 1139\u20131147.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to multi-layer feed-forward neural networks", "author": ["D. Svozil", "V. Kvasnicka", "J. Pospichal"], "venue": "Chemometrics and intelligent laboratory systems 39(1):43\u201362.", "citeRegEx": "Svozil et al\\.,? 1997", "shortCiteRegEx": "Svozil et al\\.", "year": 1997}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever,? 2014", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "In recent years, neural networks have become empirically successful in a wide range of supervised learning applications, such as computer vision (Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015), speech recognition (Hinton et al.", "startOffset": 145, "endOffset": 206}, {"referenceID": 6, "context": "2015), speech recognition (Hinton et al. 2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 9, "context": "2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al. 2016).", "startOffset": 103, "endOffset": 147}, {"referenceID": 10, "context": "2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al. 2016).", "startOffset": 103, "endOffset": 147}, {"referenceID": 7, "context": "Standard implementations of training feed-forward neural networks for classification are based on gradient-based stochastic optimization, usually optimizing the empirical cross-entropy loss (Hinton 1989).", "startOffset": 190, "endOffset": 203}, {"referenceID": 1, "context": "Intuitions about human cognition have often inspired successful machine learning approaches (Bengio et al. 2009; Cho, Courville, and Bengio 2015; Lake et al. 2016).", "startOffset": 92, "endOffset": 163}, {"referenceID": 15, "context": "Intuitions about human cognition have often inspired successful machine learning approaches (Bengio et al. 2009; Cho, Courville, and Bengio 2015; Lake et al. 2016).", "startOffset": 92, "endOffset": 163}, {"referenceID": 20, "context": "Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015).", "startOffset": 82, "endOffset": 95}, {"referenceID": 28, "context": "Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015).", "startOffset": 105, "endOffset": 131}, {"referenceID": 11, "context": "Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015).", "startOffset": 142, "endOffset": 162}, {"referenceID": 17, "context": "In the past, the quadratic loss was typically used with gradient-based learning in neural networks (Rumelhart, Hinton, and Williams 1988), but a line of studies demonstrated both theoretically and empirically that the cross-entropy loss has preferable properties over the quadratic-loss, such as better learning speed (Levin and Fleisher 1988), better performance (Golik, Doetsch, and Ney 2013) and a more suitable shape of the error surface (Glorot and Bengio 2010).", "startOffset": 318, "endOffset": 343}, {"referenceID": 4, "context": "In the past, the quadratic loss was typically used with gradient-based learning in neural networks (Rumelhart, Hinton, and Williams 1988), but a line of studies demonstrated both theoretically and empirically that the cross-entropy loss has preferable properties over the quadratic-loss, such as better learning speed (Levin and Fleisher 1988), better performance (Golik, Doetsch, and Ney 2013) and a more suitable shape of the error surface (Glorot and Bengio 2010).", "startOffset": 442, "endOffset": 466}, {"referenceID": 22, "context": "For instance, a novel cost function was proposed in (Silva et al. 2006), but it is not clearly advantageous to cross-entropy.", "startOffset": 52, "endOffset": 71}, {"referenceID": 0, "context": "The authors of (Bahdanau et al. 2015) address this question in a different setting of sequence prediction.", "startOffset": 15, "endOffset": 37}, {"referenceID": 23, "context": "In one work (Smith and Martinez 2011), data is preprocessed to detect label noise induced from overlapping classes, and in another work (Jeatrakul, Wong, and Fung 2010) the authors use an auxiliary neural network to detect noisy examples.", "startOffset": 12, "endOffset": 37}, {"referenceID": 1, "context": "The interplay between \u201ceasy\u201d and \u201chard\u201d examples during neural network training has been addressed in the framework of Curriculum Learning (Bengio et al. 2009).", "startOffset": 139, "endOffset": 159}, {"referenceID": 29, "context": "In a more recent work (Zaremba and Sutskever 2014), the authors indeed find that a curriculum in which harder examples are presented in early phases outperforms a curriculum that at first uses only easy examples.", "startOffset": 22, "endOffset": 50}, {"referenceID": 2, "context": "We consider a standard feed-forward multilayer neural network (Svozil, Kvasnicka, and Pospichal 1997), where the output layer is a softmax layer (Bridle 1990), with n units, each representing a class.", "startOffset": 145, "endOffset": 158}, {"referenceID": 16, "context": "For our experiments, we used four classification benchmark datasets from the field of computer vision: The MNIST dataset (LeCun et al. 1998), the Street View House Numbers dataset (SVHN) (Netzer et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 18, "context": "1998), the Street View House Numbers dataset (SVHN) (Netzer et al. 2011) and the CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton 2009).", "startOffset": 52, "endOffset": 72}, {"referenceID": 12, "context": "2011) and the CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton 2009).", "startOffset": 46, "endOffset": 74}, {"referenceID": 24, "context": "For optimization, we used stochastic gradient descent with momentum (Sutskever et al. 2013) with several values of momentum and a minibatch size of 128 examples.", "startOffset": 68, "endOffset": 91}, {"referenceID": 4, "context": "In the hidden layers, biases were initialized to 0 and for the weights we used the initialization scheme from (Glorot and Bengio 2010).", "startOffset": 110, "endOffset": 134}], "year": 2016, "abstractText": "When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning.", "creator": "TeX"}}}