{"id": "1611.07804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "ATR4S: Toolkit with State-of-the-art Automatic Terms Recognition Methods in Scala", "abstract": "automatically accurately recognized terminology is widely used for various extended domain - specific texts - processing tasks, such as machine translation, information retrieval or sentiment analysis. however, there is presumably still no agreement on which methods are best suited for relatively particular settings and, moreover, finally there is no reliable comparison of 11 already currently developed methods. we believe that one of amongst the main reasons is the lack standardization of state - of - the - art methods implementations, which are usually non - trivial to recreate. in order to address these issues, we present atr4s, an open - source software written in scala that comprises more than 15 methods, for automatic terminology recognition ( atr ) and implements for the whole pipeline from text document preprocessing, to term candidates collection, term candidates scoring, and finally, term candidates ranking. it is similarly highly scalable, modular and configurable robotic tool with support of automatic caching. we also compare 10 state - of - for the - art methods on 7 open datasets by average nominal precision and normal processing time. experimental comparison reveals that no single formal method demonstrates best average global precision for all datasets and says that other available domain tools for atr do perhaps not contain the best methods.", "histories": [["v1", "Wed, 23 Nov 2016 14:14:52 GMT  (20kb)", "http://arxiv.org/abs/1611.07804v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["n astrakhantsev"], "accepted": false, "id": "1611.07804"}, "pdf": {"name": "1611.07804.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nikita Astrakhantsev"], "emails": ["astrakhantsev@ispras.ru"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n07 80\n4v 1\n[ cs\n.C L\n] 2\n3 N\nov 2\nIn order to address these issues, we present ATR4S, an open-source software written in Scala that comprises more than 15 methods for automatic terminology recognition (ATR) and implements the whole pipeline from text document preprocessing, to term candidates collection, term candidates scoring, and finally, term candidates ranking. It is highly scalable, modular and configurable tool with support of automatic caching.\nWe also compare 13 state-of-the-art methods on 7 open datasets by average precision and processing time. Experimental comparison reveals that no single method demonstrates best average precision for all datasets and that other available tools for ATR do not contain the best methods.\nKeywords automatic term recognition \u00b7 terminology extraction \u00b7 open source software\nThe reported study was supported by RFBR, research project No. 14-07-00692\nN. Astrakhantsev ul. Solzhenitsyna 25, Moscow, 109004 Russia Tel.: +7(495)912-56-59 (ext. 461) Fax: +7(495)912-15-24 E-mail: astrakhantsev@ispras.ru"}, {"heading": "1 Introduction", "text": "Automatic terminology recognition (ATR) aims at extraction of terms \u2014 words and collocations designating domain-specific concepts \u2014 from a collection of text documents belonging to that domain. Extracted terms then can be used for many tasks including machine translation [19], information retrieval [28], sentiment analysis [31], ontology construction and ontology enrichment [5].\nDespite this importance, the ATR task is still far from being solved: researches continue to propose new methods for ATR, which usually show average precision below 80% even on top 500-1000 terms [17,46,47] and thus are hardly used in practice. Moreover, there is still no fair and reliable comparison of already developed methods. Most works compare 1-2 newly proposed methods with several old baselines on 1-3 datasets only.\nThe reason is that a lot efforts are required both to obtain datasets for comparison and to reimplement modern methods, which are usually non-trivial and dependent on proprietary modules.\nTo address these issues, we present an open-source implementation of ATR methods and their comparison on many datasets. In more detail, our main contributions are the following:\n1. ATR4S1: an open-source implementation of more than 10 state-of-the-art methods for ATR written in Scala; note that it can be easily used in any language working on Java Virtual Machine including Java itself. 2. Modification of KeyConceptRelatedness method [3] by replacing proprietary module for semantic relatedness computation with open-source tool word2vec [34]. 3. Comparison of 13 state-of-the-art methods on 7 datasets by average precision and processing time. In particular, more correct evaluation of unsupervised methods with many parameters (namely, PU-ATR [3] and aforementioned KeyConceptRelatedness) by adapting the cross-validation strategy.\nThis paper is organized as follows. Section 2 discuss ATR methods and overview existing software tools for ATR. Section 3 describes ATR4S architecture and implemented methods, including proposed modifications of KeyConceptRelatedness. Section 4 presents experimental evaluation. The last section outlines the paper.\n2 Related work\n2.1 ATR methods\nThe first survey by Kageura and Umino [22] devoted to ATR distinguished all methods into linguistic and statistical. Then, in a survey by Pazienza et al. [37] of 2005, it was argued that all modern algorithms include linguistic methods\n1 https://github.com/ispras/atr4s\nas a filtering step. Finally, the most recent survey by Astrakhantsev et al. [5] identified the general pipeline of ATR methods: preprocessing, term candidates collection, term candidates scoring, and term candidates ranking.\nPreprocessing transforms input text into a sequence of elements needed for further term candidates extraction; most often, each of such elements consists of lemmatized token with attached part of speech tag; some works [21,47] use instead noun phrases obtained by shallow parsing.\nTerm candidates extraction can be seen as a filtering step: it should throw out such words and collocations that are almost certainly not terms, based on simple linguistic and statistical criteria like presence of stop word or minimal frequency of occurrence.\nTerm candidates scoring, i.e. assigning a number to each term candidate reflecting its likelihood of being a term, is the most important and sophisticated step in the whole pipeline. Considering the type of information used to score term candidate, we can form the following groups: methods based on frequencies of term candidate occurrences (with large subgroup of word association measures); on occurrences contexts; on reference corpora; on topic modeling; on Wikipedia. ATR4S includes most promising methods for each group; see details in section 3.3.\nTerm candidates ranking is the final step, which lets to take the top candidates and thus distinguish terms from not terms. It is trivial in case of only one term scoring method, because we can simply rank by that score; and the vast majority of works belongs to this group. Some works use linear combination, voting algorithm or semi-supervised learning, we discuss them in section 3.4. Another set of works [4,17,35] apply supervised machine learning.\n2.2 ATR software tools\nThere are many software tools developed for ATR to date. However, most of them provide only 1 or 2 methods, which are usually outdated. For example, TerMine2 is based on CValue/NC-Value methods (and academic usage only); FlexiTerm contains C-Value and \u201da simple term variant normalisation method\u201d [41]; TOPIA3 lists only one method without algorithm description and it is not updated since 2009; TermRider4 utilizes TF-IDF only; TermSuite [11] ranks candidates by Weirdness method, but focuses on recognizing term variants based on syntactic and morphological patterns.\nSome tools are limited by searching for mentions of (named) entities (for example, OpenCalais5) or named entites and Wikipedia concepts (Texterra [42]). Another tool6 supports only supervised recognition of 1-word and 2-words terms.\n2 http://www.nactem.ac.uk/software/termine 3 https://pypi.python.org/pypi/topia.termextract 4 https://gate.ac.uk/projects/neon/termraider.html 5 http://www.opencalais.com/about-open-calais/ 6 https://bitbucket.org/Meister17/term-extraction\nJATE 2.0 [47] is the most similar tool to ATR4S: it is written in Java and also can be natively used in any JVM-based language, contains many ATR algorithms and multiple methods for term candidates collection; it is highly modular and adaptable. However, it lacks a lot of actual state-of-the-art methods, namely those based on occurrences contexts, topic models, Wikipedia, and non-trivial ranking algorithms such as Voting [46] and PU-ATR [3]. It also depends on Apache Solr7, which may simplify its integration to the application that already uses Solr, but may as well complicate its usage as a library."}, {"heading": "3 Architecture", "text": "ATR4S follows general pipeline of term recognition: texts preprocessing, term candidates collection, term candidates scoring, and term candidates ranking. Subsections below describe each step in details.\n3.1 Preprocessing\nAt the preprocessing step, ATR4s splits input text documents into sentences, tokenizes obtained sentences, and finds part of speech tags and lemmas for obtained tokens. In order to perform these tasks, ATR4S incorporated 3 external NLP libraries: Stanford CoreNLP [30], Emory nlp4j8 and Apache OpenNLP9. We use the first one in all experiments10.\n3.2 Term candidates collection\nATR4S extracts consecutive word n-grams of specified orders (by default, from 1 to 4) as term candidates.\nThree basic filters can be applied before formation of term candidate occurrence (or term occurrence, for brevity):\n1. Noise word filter: keeps term occurrence if all of its lemmas have length not less than the predefined limit and match the predefined regular expression (by default, length limit is 3 characters and regular expression filters out words containing non-alphanumeric characters). This filter is most useful for texts obtained from automatic parsing (e.g. PDF or HTML) and thus containing a lot of noise words.\n7 http://lucene.apache.org/solr/ 8 https://emorynlp.github.io/nlp4j/ 9 http://opennlp.apache.org/\n10 Preliminary experiments show drop of 1-5% in average precision in case of switching from Stanford CoreNLP, mainly because of part of speech tagging errors; however, note that Stanford CoreNLP is distributed under GPL, while others are licensed under the Apache License, Version 2.0.\n2. Stop word filter: keeps term occurrence if the predefined set of stop words contains no lemma of the term occurrence. By default, we use stop words list from the SMART retrieval system [40]. 3. Part of speech (PoS) tags pattern: keeps term occurrence if its PoS tags match the pattern encoded as the regular expression. By default, we apply the commonly-used pattern [7] extended by allowing prepositions between nouns: (NN(S)?|JJ|NNP|NN(S?)IN)*(NN(S)?)\nThen ATR4S combines occurrences with the same canonical representation (lemmas joined by underscore symbol, e.g. information_processing) as belonging to the same term candidate.\nFinally, ATR4S filters out term candidates occurring rarer than the predefined number of times (by default, 2), in order to (a) reduce noise occurring due to errors in preprocessing steps or input data preparation; (b) improve quality of scoring methods: most of them use occurrences frequency; and (c) reduce computation efforts.\n3.3 Term candidates scoring\nATR4S includes more than 15 methods for term candidates scoring; below we describe them grouped by the type of information used to score term candidate."}, {"heading": "3.3.1 Methods based on occurrences frequencies", "text": "Most methods for term candidates scoring are based on the intuition that the more frequently some word or collocation occur in the domain-specific text collection, the more likely it is a term for this domain. This subsection describes methods that utilize this intuition only, i.e. those considering only frequencies of words constituting term candidate and ignoring other information.\nBesides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].\nATF simply normalizes term frequency by number of documents containing this term candidate.\nTF-IDF is a classical information retrieval measure showing high values for term candidates that occur frequently in few documents:\nTF \u00b7 IDF (t) = TF (t) \u00b7 log D\nDTF (t) , (1)\nwhere D is a total number of document in collection, DTF (t) is a number of documents containing term candidate t.\nRIDF was originally proposed for keywords extraction [8] and than re-used for term recognition [47]. It is based on the assumption that the deviation of observed IDF from the IDF modeled by Poisson distribution is higher for keywords than for ordinary words.\nRIDF (t) = TF (t) \u00b7 log D\nDTF (t) + log(1\u2212 e\u2212ATF (t)), (2)\nC-Value, one of the most popular methods, promotes term candidates that occur frequently, but not as parts of other term candidates. This method was supposed to work with multi-word term candidates only; ATR4S includes modification proposed by Ventura et al. [43] that supports one-word term candidates as well:\nC-V alue(t) =\n{\nlog2(|t|+ 0.1) \u00b7 TF (t), if {s : t \u2282 s} = \u2205; log2(|t|+ 0.1) \u00b7 ( TF (t)\u2212 \u2211 s TF (s)\n|{s:t\u2282s}|\n)\n, else.\n(3) where |t| is a length of term candidate t (number of words), s is a set of term candidates containing t, i.e. such candidates that t is a their substring. Basic is a modification of C-Value for intermediate level (of specificity) terms extraction. Like original C-Value, it can extract multi-word terms only; however, unlike C-Value, Basic promotes term candidates that are part of other term candidates, because such terms are usually served for creation of more specific terms.\nBasic(t) = |t| log f(t) + \u03b1et, (4)\nwhere et is a number of term candidates containing t. ComboBasic modifies Basic further, so that the level of term specificity can\nbe customized by changing parameters of the method:\nComboBasic(t) = |t| log f(t) + \u03b1et + \u03b2e \u2032 t, (5)\nwhere e\u2032t is a number of term candidates that are contained in t. Therefore, by increasing \u03b2, one can extract more specific terms and vice versa.\nNote that ATR4S does not include methods based on word association measures like\nz-test [12], t-test [9], \u03c72-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47]."}, {"heading": "3.3.2 Methods based on occurrences contexts", "text": "Methods from this group follows the distributional hypothesis [20] and try to distinguish terms from non-terms by considering their contexts. We are aware of only 2 such methods: NC-Value [18] and DomainCoherence [6]; since the latter is a modification of the former and was shown to work better [6], ATR4S includes only it.\nDomainCoherence works in 3 steps. First, it extracts 200 best term candidates by using Basic method.\nThen, words from contexts of previously extracted 200 terms are filtered: it keeps only nouns, adjectives, verbs and adverbs that occur in at least 1 quarter of all documents and are similar to these 200 term candidates, i.e. ranked in the top 50 by averaged Normalized PMI:\ns(w) = 1\n|T |\n\u2211\nt\u2208T\nNPMI(t, w) = 1\n|T |\n\u2211\nt\u2208T\nlog (\nP (t,w) P (t)P (w)\n)\nlog(P (t, w)) , (6)\nwhere w is a context word; T is a set of 200 best term candidates extracted by Basic; P (t, w) is a probability of occurrence of word w in the context of t; P (t) and P (w) are probabilities of occurrences of term t and word w, correspondingly. These probabilities are estimated on the basis of occurrence frequencies in the input collection; context is considered to be a 5 words window.\nFinally, as a weight of a term candidate, DomainCohrerence takes the average of the same NPMI measures computed with each of 50 context words extracted at the previous step."}, {"heading": "3.3.3 Methods based on reference corpora", "text": "There are multiple methods based on the assumption that terms can be distinguished from other words and collocations by comparing occurrences statistics of considered domain-specific collection with statistics of some reference corpus (usually, from general domain).\nDomainPertinence [33] is the simplest implementation of this idea:\nDomainPertinence(t) = TFtarget(t)\nTFreference(t) , (7)\nwhere TFtarget(t) is a frequency of term candidate t in target (domain-specific) collection; TFreference is a frequency in reference (general) collection.\nWeirdness [1] normalizes it by sizes (in number of words) of document collections:\nWeirdness(t) = NTFtarget(t)\nNTFreference(t) , (8)\nwhere NTFtarget(t) and NTFreference are frequencies of t normalized by sizes of target and reference collections, respectively11.\nRelevance [38] further updates it by taking into account fraction of documents, where term candidate occur:\nRelevance(t) = 1\u2212\n(\nlog2\n(\n2 + NTFtarget(t) \u00b7DFtarget(t)\nNTFreference(t)\n))\u22121\n(9)\n11 Note that in case of simple ranking Weirdness and DomainPertinence show exact the same results, because scores for the same term candidate computed by these 2 methods differ by a constant multiplier only.\nATR4S uses Corpus of Historical American English12 as a reference collection."}, {"heading": "3.3.4 Methods based on topic modeling", "text": "These methods are based on the idea that topic modeling uncovers semantic information useful for term recognition; in particular, that distribution of words over topics found by topic modeling is a less noisy signal than simple frequency of occurrences.\nTo the best of our knowledge, this group contains only one method capable to extract terms of arbitrary length, that is Novel Topic Model [27].\nFirst, it obtains probability distribution of words over the following topics: \u03c6t \u2013 general topics (1 \u2264 t \u2264 20); \u03c6B \u2013 background topic; \u03c6D \u2013 documentspecific topic. Then, it extracts 200 words most probable for each topic: Vt, VB, VD, correspondingly; finally, for each term candidate ci its weight is computed as a sum of maximal probabilities for each of its Li words (wi1wi2...wiLi):\nNTM(ci) = log(TFi) \u00b7 \u2211\n1\u2264j\u2264Li,wj\u2208\u222a{Vt}t\u2208T\u222a{B,D}\n\u03c6 mtwj wj , (10)\nwhere mtwj = argmaxt\u2208T\u222a{B,D} \u03c6 t wj\nFor topic modeling, ATR4S uses open source framework13."}, {"heading": "3.3.5 Methods based on Wikipedia", "text": "All methods mentioned above require large collection of text documents, otherwise statistics of term candidates occurrences is too noisy. The only way to overcome it in case of small collection is to use external resources. Such a resource should satisfy two requirements: (a) it should be specific enough to contain domain-specific information needed to distinguish terms from not terms; (b) it should be general enough to be applicable for many domains in practice. Wikipedia14 satisfy these requirements: it is multilingual (English version contains more than 5 million articles), covers a lot of domains and keeps growing.\nOne of the simplest method in this group is WikiPresence: it returns 1 if term candidate occurs in Wikipedia pages as hyperlink caption; 0 otherwise. Example of its usage is an additional filter for other methods [3].\nLinkProbability [2] is a normalized frequency of being hyperlink in Wikipedia pages:\nLinkProbT (t) =\n{\n0 - if Wikipedia does not contain t or H(t) W (t) < T ; H(t) W (t) - else,\n(11)\n12 http://www.ngrams.info/download_coha.asp 13 https://github.com/ispras/tm 14 https://www.wikipedia.org/\nwhere H(t) is a number of occurrences of term candidate t as a hyperlink caption; W (t) is a total number of occurrences in Wikipedia pages; T is a method parameter needed to filter out too small values, because they occur due to markup errors in most cases; experimentally chosen value T = 0.018 is used by default.\nThis method propagates term candidates that are specific enough to be provided with a hyperlink; however, it is able to distinguish terms from general words and collocations, but not from terms of other domains.\nKeyConceptsRelatedness [2] interprets domain-specific terms as words and collocations that are semantically related to knowingly domain-specific concepts. This method assumes concepts that are key for many documents in the input collection to be a good approximation for such knowingly domain-specific concepts.\nOriginally, it was based on computation of semantic relatedness between two Wikipedia concepts (i.e. pages) by Dice measure, which considers numbers of common and total neighbors of these pages. We modified this: instead of Dice measure, we use cosine distance between word embedding vectors [34] corresponding to Wikipedia concepts. More precisely, we preprocess15 Wikipedia dump by removing markup, while keeping occurrences of Wikipedia concepts (replace each hyperlink by special token that includes title of link\u2019s target concept), and by tokenizing and stemming, then we build16 word embedding model and, finally, use it for semantic relatedness computation.\nAnother modification relates to extraction of key concepts from a text document. Initially, KeyConceptsRelatedness used algorithm based on semantic graph construction and clustering, but it works too slowly: in particular, because it requires full word sense disambiguation of all texts.\nWe propose to use simplified version of KP-Miner [14]: In order to be considered as a candidate to key concept, a word or a collocation must: (a) occur at least twice; (b) have an occurrence among the first 80017 words; (c) be a valid term candidate, i.e. satisfy requirements listed in Section 3.2; and (d) be contained in the vocabulary of constructed word embedding model (i.e. Wikipedia dump should contain at least 5 hyperlinks to the concept with the same title as the word/collocation). Then we rank such candidates by the product of their length (in words) and number of occurrences in the document.\nIn summary, the modified algorithm for KeyConceptsRelatedness is the following:\n1. Extract key concepts for the whole document collection: (a) extract d key concepts from each document (see the algorithm above); (b) keep N key concepts with maximal frequency (number of being chosen as a key concept). 2. For each term candidate: if the word embedding model does not contain\nthe term candidate, then return 0; otherwise compute semantic relatedness\n15 https://github.com/phdowling/wiki2vec 16 https://github.com/RaRe-Technologies/gensim 17 We keep original constant from KP-Miner algorithm.\nto extracted N key concepts by weighted kNN adapted for the case with only positive instances:\nsimk(c, CN ) = 1\nk\nk \u2211\ni=1\ncos(vc, vi) (12)\nwhere c is a term candidate; CN is a set of N key concepts sorted by semantic relatedness to c in descending order; k is a parameter from kNN (should be much smaller than N); vc is an embedding vector corresponding to the term candidate; vi is an embedding vector corresponding to the key concept i.\nThis method propagates a term candidate that has correspondingWikipedia article, which is semantically related to key concepts of the whole document collection.\n3.4 Term candidates ranking\nAs we already mentioned in section 2, term candidates ranking becomes nontrivial in case of multiple methods for term candidates scoring. (Following terminology of machine learning, we will refer such methods for scoring as features, for brevity.) General idea for this problem is to aggregate values of multiple features into one number (usually, between 0 and 1), thus reducing the task to ranking by one method.\nOne of the most popular method is a linear combination of features with some predefined (usually, equal) coefficients. Examples include PostRankDC [6] and GlossEx [36].\nNote that linear combination does not require scores of other term candidates to be computed in advance, so it is simpler and faster18, but misses potentially useful information. Voting algorithm [46] considers values of all term candidates and it was shown [46] to outperform single methods and weighted average (i.e. linear combination):\nV (t) =\nn \u2211\ni=1\n1\nr(fi(t)) , (13)\nwhere r(fi(t)) is a rank of term candidate t among all candidates sorted by feature fi only; n is a total number of aggregated features.\nMore sophisticated approach is PU-ATR [2], which is based on the ideas of bootstrapping (like NC-Value and DomainCoherence) and positive unlabeled (PU) learning.\nIt extracts top 50-200 terms by single method (seed method); then computes values for multiple features for all term candidates; learns positiveunlabeled classifier by considering these seed terms as positive instances and all\n18 In particular, because it can be easily parallelized by term candidates.\nother term candidates as unlabeled instances, where each instance is a vector of feature values; and, finally, applies learned classifier to each term candidate, so that the obtained classifier\u2019s confidence is a final aggregated value.\nComboBasic is recommended [3] as a seed method, because (a) it lets to adjust the level of specificity of seed terms and thus indirectly affects the level of specificity of all terms; (b) it is simple enough to include terms of different nature, i.e. terms that can be extracted by using different types of information (see Section 3.3), so that PU algorithm overfits less probably.\nNote that we perform probabilistic classification, which can suffer from the problem of multicollinearity. Thus, following the previous work [3], we assume that scoring methods from different groups weakly correlate and choose them as features: C-Value (occurrences frequencies), DomainCoherence (occurrences contexts), Relevance (reference corpora), NovelTopicModel (topic modeling), LinkProbability (Wikipedia, domain-independent specificity), KeyConceptRelatedness (Wikipedia, domain-specificity).\nDifferent Positive-Unlabeled algorithms were shown [3] to work similarly for this task, so we chose the simplest one [29], with Logistic Regression19 as an internal probabilistic classifier (during preliminary experiments we found it outperforming Random Forest classifier).\n3.5 Tool features\nATR4S is highly scalable (by CPUs of one machine), modular and configurable tool that supports automatic caching.\nScalability is provided by storing documents and candidates in Scala parallel collection: preprocessing and most steps of candidates collection can be parallelized by documents; candidates scoring can be parallelized by candidates themselves.\nThe whole pipeline is instantiated by its own configuration class, which contains corresponding configurations for each constituent step, which are configurable in the same way, i.e. by constructor injection, until the final configurations with constants only, not instances of other configuration classes. This configuration can be serialized/deserialized to/from a human-readable JSON file that can be manually edited, so the tool can be easily configured without a necessity to rebuild it.\nDescribed architecture enables automatic caching: since configuration for each step uniquely determines result of such step, we can cache that result and address it by the corresponding configuration20. Considering the observation that ATR methods usually require fine-tuning for optimal quality and thus are often launched many times, such caching can significantly speed up unchanged (previous) steps, e.g. dataset preprocessing or candidates collection, and therefore, speed up the whole process.\n19 We use Apache Spark MLlib for supervised ML: http://spark.apache.org/mllib/ 20 See details in the source code, class ru.ispras.atr.utils.Cacher.\n4 Evaluation\n4.1 Experiments design\nWe evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.0 [39], EuroParl [24] with Eurovoc thesaurus21. See table 1 for summary statistics.\nWe extract term candidates by using default parameters and filters described in section 3.2; table 2 shows summary statistics of collected candidates.\nWe use a standard metric for ATR, that is average precision at level K (AvP):\nAvP (K) =\nK \u2211\ni=1\nP (i)(R(i)\u2212R(i\u2212 1)), (14)\nwhere P (i) is precision at level i; R(i) is recall at level i.\n21 http://eurovoc.europa.eu/drupal\nWe choose K to be equal to the number of expected terms among extracted term candidates for the dataset (see the last column in table 2): in this case, perfect algorithm for ranking term candidates reaches 100% quality by AvP.\nIn order to find best parameters of the methods modified in this work, that are KeyConceptRelatedness and PU-method, we adapt cross-validation strategy in the following way: each dataset is considered to be a fold; one fold is test \u2013 we use it for computing average precision of the parameter set chosen as the best one; other folds are validation \u2013 we use them for choosing the best parameter set as follows:\n1. evaluate each parameter set on each dataset, i.e. associate each parameter set with the AvP obtained on this dataset; 2. find the maximum AvP on that dataset; 3. compute relative goodness of each parameter set for each dataset by divid-\ning AvP of this parameter set on the maximum AvP for this dataset; 4. choose the best (CV) parameter set as the one with maximum product of\nrelative goodnesses over all (validation) datasets.\nWe prefer this strategy to the commonly used optimization over all datasets, because it is more similar to the real setting, when we apply ATR to the new dataset without any labeled data and have to use (or at least start from) parameters that were optimized on some previous datasets. It is especially relevant for the methods with many parameters like KeyConceptRelatedness and PU-method due to their potentially higher chances of overfitting. For the same reason, we also keep 1 dataset, Europarl, from using it in any experiments except for the final comparison.\n4.2 Quality results (average precision)\nTo optimize parameters of KeyConceptRelatedness method we perform grid search with the following set of possible values: count of key concepts per document d = {3, 5, 10, 15, 20, 30}; total count of key concepts N = {50, 100, 200, 300, 500}; count of nearest keys k = {1, 2, 3, 5, 10}.\nAs we can see from table 3, parameters found by cross-validation are quite stable: parameter set d = 15, N = 500, k = 2 shows the highest result in 4\nof 5 cases and in the same 4 cases difference between test AvP and best AvP is about 1-2%; at the same time, parameter sets optimized for one dataset (columns named Best parameters) predictably vary a lot. By optimizing over all 6 datasets we have the same parameters set: d = 15, N = 500, k = 2, which is used for Europarl dataset in the final comparison methods.\nTo optimize parameters of PU-ATR method we perform grid search with the following set of possible values: coefficient used in ComboBasic method for the number of containing terms \u03b1 = {0, 0.1, 0.5, 0.75, 1}; coefficient used in ComboBasic method for the number of contained terms \u03b2 = {0, 0.1, 0.5, -0.1, -0.25, -0.5}; threshold used in PU algorithm for determining reliable negative instances t = {0.1, 0.05, 0.025}.\nTable 4 shows that parameters are not so stable, but the difference between test AvP and best AvP is about 1-2% in all cases. By optimizing over all 6 datasets we have the following parameter set: \u03b1 = 0.75, \u03b2 = 0.1, t = 0.05, which is used for Europarl dataset in the final comparison methods.\nTable 5 presents comparison of all methods over all datasets. Note that we use parameter set chosen by cross-validation for KeyConceptRelatedness\nand PU-ATR and default parameters for other methods. Voting aggregates the same 5 features as PU-ATR, see section 3.4.\nPU-ATR seems to be the most stable: it is the best for 4 datasets and in top 3 methods for all datasets. However, it is the most computationally intensive method, see the next subsection.\nNote also that none of the methods showing best results in this experiment are implemented in other tools.\n4.3 Performance results (time)\nWe estimated performance of ATR4S on a machine with Intel Core i5-2500 (3.3GHz, 4 cores) and 32 Gb RAM, from which 12 Gb was set as a maximum memory allocation pool for Java Virtual Machine, see Table 6. Since preprocessing and candidates collection steps are the same for all methods, we show them in the first 2 rows and ignore that time for scoring/ranking methods.\nAs we can see, methods from the first 3 groups, i.e. those based on occurrences frequencies, contexts and reference corpora, are the fastest. Methods based on Wikipedia require constant 15 sec (LinkProbability) or 1 min (KeyConceptRelatedness) for initialization, then their times depend on dataset size almost linearly. NovelTopicModel is the slowest for big datasets; however, its average precision is not good for big datasets anyway. Time required for PUATR is almost the sum of used features times and Spark start time (about 30 secs)."}, {"heading": "5 Conclusion", "text": "This paper presents ATR4S, an open-source tool for automatic terms recognition, and experimental comparison of 13 state-of-the-art methods for ATR on 7 datasets. ATR4S comprises more than 15 methods for ATR, supports caching and human-readable configuration; it is written in Scala with parallel collections wherever appropriate, so it utilizes all CPU cores.\nExperimental comparison confirms observations that (1) no single method is best for all datasets [46] and (2) multiple features should be combined for better quality [17]; also it shows that other available tools lack the best methods, i.e. actual state-of-the-art methods, namely PU-ATR [3], KeyConceptRelatedness [3], NovelTopicModel [27], and Basic [6].\nIt is obvious that ATR4S does not include all methods capable to outperform already implemented ones on some settings, but we believe that these implementations can be used as a basis for development of other methods or, at least, for easy comparison. Nevertheless, the addition of new methods and their experimental evaluation are the main directions of further improvement.\nRegarding practical aspects of ATR task, in particular noisy input datasets, which often contain documents from multiple domains, and scenarios assuming terminology enrichment instead of extraction, we believe that incorporation of document clustering and more sophisticated semi-supervised methods are among the most promising research topics.\nAcknowledgements The author would like to thank Yaroslav Nedumov and Denis Turdakov for their valuable comments and Denis Fedorenko for his help in implementing the previous versions of ATR tool."}], "references": [{"title": "University of surrey participation in trec8: Weirdness indexing for logical document extrapolation and retrieval (wilder)", "author": ["K. Ahmad", "L. Gillam", "L Tostevin"], "venue": "The Eighth Text REtrieval Conference (TREC-8)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Automatic term acquisition from domain-specific text collection by using wikipedia", "author": ["N. Astrakhantsev"], "venue": "Proceedings of the Institute for System Programming 26(4), 7\u201320", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods and software for terminology extraction from domainspecific text collection", "author": ["N. Astrakhantsev"], "venue": "Ph.D. thesis, Institute for System Programming of Russian Academy of Sciences", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic enrichment of informal ontology by analyzing a domain-specific text collection", "author": ["N. Astrakhantsev", "D. Fedorenko", "D. Turdakov"], "venue": "Computational Linguistics and Intellectual Technologies: Papers from the Annual International Conference Dialogue 13, 29\u201342", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods for automatic term recognition in domain-specific text collections: A survey", "author": ["N. Astrakhantsev", "D. Fedorenko", "D.Y. Turdakov"], "venue": "Programming and Computer Software 41(6), 336\u2013349", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-independent term extraction through domain modelling", "author": ["G. Bordea", "P. Buitelaar", "T. Polajnar"], "venue": "the 10th International Conference on Terminology and Artificial Intelligence (TIA 2013), Paris, France", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Expertise mining from scientific literature", "author": ["P. Buitelaar", "T. Eigner"], "venue": "Proceedings of the Fifth International Conference on Knowledge Capture, K-CAP \u201909, pp. 171\u2013172. ACM, New York, NY, USA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Inverse document frequency (idf): A measure of deviations from poisson", "author": ["K. Church", "W. Gale"], "venue": "Natural language processing using very large corpora, pp. 283\u2013295. Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "6", "author": ["K. Church", "W. Gale", "P. Hanks", "D. Kindle"], "venue": "using statistics in lexical analysis. Lexical acquisition: exploiting on-line resources to build a lexicon p. 115", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "Word association norms, mutual information, and lexicography", "author": ["K.W. Church", "P. Hanks"], "venue": "Computational linguistics 16(1), 22\u201329", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1990}, {"title": "Termsuite: Terminology extraction with term variant detection", "author": ["D. Cram", "B. Daille"], "venue": "ACL 2016 p. 13", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "The construction of a thesaurus automatically from a sample of text", "author": ["S.F. Dennis"], "venue": "Proceedings of the Symposium on Statistical Association Methods For Mechanized Documentation, Washington, DC, pp. 61\u2013148", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1965}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["T. Dunning"], "venue": "Computational linguistics 19(1), 61\u201374", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Kp-miner: Participation in semeval-2", "author": ["S.R. El-Beltagy", "A. Rafea"], "venue": "Proceedings of the 5th international workshop on semantic evaluation, pp. 190\u2013193. Association for Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Clarit-trec experiments", "author": ["D.A. Evans", "R.G. Lefferts"], "venue": "Information processing & management 31(3), 385\u2013395", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Growing multi-domain glossaries from a few seeds using probabilistic topic models", "author": ["S. Faralli", "R. Navigli"], "venue": "EMNLP, pp. 170\u2013181", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic recognition of domainspecific terms: an experimental evaluation", "author": ["D. Fedorenko", "N. Astrakhantsev", "D. Turdakov"], "venue": "Proceedings of SYRCoDIS 2013, pp. 15\u201323", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic recognition of multi-word terms", "author": ["K. Frantzi", "S. Ananiadou", "H. Mima"], "venue": "the c-value/nc-value method. International Journal on Digital Libraries 3(2), 115\u2013130", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Flow network models for word alignment and terminology extraction from bilingual corpora", "author": ["\u00c9. Gaussier"], "venue": "Proceedings of the 17th international conference on Computational linguistics-Volume 1, pp. 444\u2013450. Association for Computational Linguistics", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word 10(2-3), 146\u2013162", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1954}, {"title": "Unsupervised training set generation for automatic acquisition of technical terminology in patents", "author": ["A. Judea", "H. Sch\u00fctze", "S. Bruegmann"], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pp. 290\u2013300. Dublin City University and Association for Computational Linguistics, Dublin, Ireland", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods of automatic term recognition: A review", "author": ["K. Kageura", "B. Umino"], "venue": "Terminology 3(2), 259\u2013289", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Genia corpus\u2013a semantically annotated corpus for bio-textmining", "author": ["J.D. Kim", "T. Ohta", "Y. Tateisi", "J. Tsujii"], "venue": "Bioinformatics 19(Suppl 1), i180\u2013i182", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "MT summit, vol. 5, pp. 79\u201386", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Glossary extraction and utilization in the information search and delivery system for ibm technical support", "author": ["L. Kozakov", "Y. Park", "T. Fin", "Y. Drissi", "Y. Doganata", "T. Cofino"], "venue": "IBM Systems Journal 43(3), 546\u2013563", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Large dataset for keyphrases extraction", "author": ["M. Krapivin", "A. Autaeu", "M. Marchese"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "A novel topic model for automatic term extraction", "author": ["S. Li", "J. Li", "T. Song", "W. Li", "B. Chang"], "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pp. 885\u2013888. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving retrieval effectiveness by using key terms in top retrieved documents", "author": ["Y. Lingpeng", "J. Donghong", "Z. Guodong", "N. Yu"], "venue": "Advances in Information Retrieval, pp. 169\u2013184. Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Partially supervised classification of text documents", "author": ["B. Liu", "W.S. Lee", "P.S. Yu", "X. Li"], "venue": "ICML, vol. 2, pp. 387\u2013394. Citeseer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pp. 55\u201360", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A high precision method for aspect extraction in russian", "author": ["V. Mayorov", "I. Andrianov", "N. Astrakhantsev", "V. Avanesov", "I. Kozlov", "D. Turdakov"], "venue": "Proceedings of International Conference Dialog, vol. 2", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-independent automatic keyphrase indexing with small training sets", "author": ["O. Medelyan", "I.H. Witten"], "venue": "Journal of the American Society for Information Science and Technology 59(7), 1026\u20131040", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "A semantic approach for extracting domain taxonomies from text", "author": ["K. Meijer", "F. Frasincar", "F. Hogenboom"], "venue": "Decision Support Systems 62, 78\u201393", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pp. 3111\u20133119", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "An experimental study of term extraction for real information-retrieval thesauri", "author": ["M. Nokel", "N. Loukachevitch"], "venue": "Proceedings of 10th International Conference on Terminology and Artificial Intelligence, pp. 69\u201376", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic glossary extraction: beyond terminology identification", "author": ["Y. Park", "R. Byrd", "B. Boguraev"], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pp. 1\u20137. Association for Computational Linguistics", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Terminology extraction: an analysis of linguistic and statistical approaches", "author": ["M. Pazienza", "M. Pennacchiotti", "F. Zanzotto"], "venue": "Knowledge Mining pp. 255\u2013279", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Corpus-based terminology extraction applied to information access", "author": ["A. Pe\u00f1as", "F. Verdejo", "J Gonzalo"], "venue": "Proceedings of Corpus Linguistics, vol. 2001. Citeseer", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "The ACL RD-TEC", "author": ["B. QasemiZadeh", "A.K. Schumann"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "The SMART Retrieval System\u2014Experiments in Automatic Document Processing", "author": ["G. Salton"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1971}, {"title": "Flexiterm: a flexible term recognition method", "author": ["I. Spasi\u0107", "M. Greenwood", "A. Preece", "N. Francis", "G. Elwyn"], "venue": "Journal of biomedical semantics 4(1), 1", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Texterra: A framework for text analysis", "author": ["D.Y. Turdakov", "N. Astrakhantsev", "Y.R. Nedumov", "A. Sysoev", "I. Andrianov", "V. Mayorov", "D. Fedorenko", "A. Korshunov", "S.D. Kuznetsov"], "venue": "Programming and Computer Software 40(5), 288\u2013295", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining c-value and keyword extraction methods for biomedical terms extraction", "author": ["J.A.L. Ventura", "C. Jonquet", "M. Roche", "M Teisseire"], "venue": "LBM\u20192013: International Symposium on Languages in Biology and Medicine, pp. 45\u201349", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "You can\u2019t beat frequency (unless you use linguistic knowledge): a qualitative evaluation of association measures for collocation and term extraction", "author": ["J. Wermter", "U. Hahn"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pp. 785\u2013792. Association for Computational Linguistics", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "The acl rd-tec: A dataset for benchmarking terminology extraction and classification in computational linguistics", "author": ["B.Q. Zadeh", "S. Handschuh"], "venue": "COLING 2014: Proceedings of the 4th International Workshop on Computational Terminology (CompuTerm\u201914). Dublin, Ireland", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparative evaluation of term recognition algorithms", "author": ["Z. Zhang", "C. Brewster", "F. Ciravegna"], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC08), Marrakech, Morocco", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Jate 2.0: Java automatic term extraction with apache solr", "author": ["Z. Zhang", "J. Gao", "F. Ciravegna"], "venue": "The LREC 2016 Proceedings", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Extracted terms then can be used for many tasks including machine translation [19], information retrieval [28], sentiment analysis [31], ontology construction and ontology enrichment [5].", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "Extracted terms then can be used for many tasks including machine translation [19], information retrieval [28], sentiment analysis [31], ontology construction and ontology enrichment [5].", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "Extracted terms then can be used for many tasks including machine translation [19], information retrieval [28], sentiment analysis [31], ontology construction and ontology enrichment [5].", "startOffset": 131, "endOffset": 135}, {"referenceID": 4, "context": "Extracted terms then can be used for many tasks including machine translation [19], information retrieval [28], sentiment analysis [31], ontology construction and ontology enrichment [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 16, "context": "Despite this importance, the ATR task is still far from being solved: researches continue to propose new methods for ATR, which usually show average precision below 80% even on top 500-1000 terms [17,46,47] and thus are hardly used in practice.", "startOffset": 196, "endOffset": 206}, {"referenceID": 45, "context": "Despite this importance, the ATR task is still far from being solved: researches continue to propose new methods for ATR, which usually show average precision below 80% even on top 500-1000 terms [17,46,47] and thus are hardly used in practice.", "startOffset": 196, "endOffset": 206}, {"referenceID": 46, "context": "Despite this importance, the ATR task is still far from being solved: researches continue to propose new methods for ATR, which usually show average precision below 80% even on top 500-1000 terms [17,46,47] and thus are hardly used in practice.", "startOffset": 196, "endOffset": 206}, {"referenceID": 2, "context": "Modification of KeyConceptRelatedness method [3] by replacing proprietary module for semantic relatedness computation with open-source tool word2vec [34].", "startOffset": 45, "endOffset": 48}, {"referenceID": 33, "context": "Modification of KeyConceptRelatedness method [3] by replacing proprietary module for semantic relatedness computation with open-source tool word2vec [34].", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "In particular, more correct evaluation of unsupervised methods with many parameters (namely, PU-ATR [3] and aforementioned KeyConceptRelatedness) by adapting the cross-validation strategy.", "startOffset": 100, "endOffset": 103}, {"referenceID": 21, "context": "The first survey by Kageura and Umino [22] devoted to ATR distinguished all methods into linguistic and statistical.", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "[37] of 2005, it was argued that all modern algorithms include linguistic methods", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] identified the general pipeline of ATR methods: preprocessing, term candidates collection, term candidates scoring, and term candidates ranking.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Preprocessing transforms input text into a sequence of elements needed for further term candidates extraction; most often, each of such elements consists of lemmatized token with attached part of speech tag; some works [21,47] use instead noun phrases obtained by shallow parsing.", "startOffset": 219, "endOffset": 226}, {"referenceID": 46, "context": "Preprocessing transforms input text into a sequence of elements needed for further term candidates extraction; most often, each of such elements consists of lemmatized token with attached part of speech tag; some works [21,47] use instead noun phrases obtained by shallow parsing.", "startOffset": 219, "endOffset": 226}, {"referenceID": 3, "context": "Another set of works [4,17,35] apply supervised machine learning.", "startOffset": 21, "endOffset": 30}, {"referenceID": 16, "context": "Another set of works [4,17,35] apply supervised machine learning.", "startOffset": 21, "endOffset": 30}, {"referenceID": 34, "context": "Another set of works [4,17,35] apply supervised machine learning.", "startOffset": 21, "endOffset": 30}, {"referenceID": 40, "context": "For example, TerMine is based on CValue/NC-Value methods (and academic usage only); FlexiTerm contains C-Value and \u201da simple term variant normalisation method\u201d [41]; TOPIA lists only one method without algorithm description and it is not updated since 2009; TermRider utilizes TF-IDF only; TermSuite [11] ranks candidates by Weirdness method, but focuses on recognizing term variants based on syntactic and morphological patterns.", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "For example, TerMine is based on CValue/NC-Value methods (and academic usage only); FlexiTerm contains C-Value and \u201da simple term variant normalisation method\u201d [41]; TOPIA lists only one method without algorithm description and it is not updated since 2009; TermRider utilizes TF-IDF only; TermSuite [11] ranks candidates by Weirdness method, but focuses on recognizing term variants based on syntactic and morphological patterns.", "startOffset": 300, "endOffset": 304}, {"referenceID": 41, "context": "Some tools are limited by searching for mentions of (named) entities (for example, OpenCalais) or named entites and Wikipedia concepts (Texterra [42]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 46, "context": "0 [47] is the most similar tool to ATR4S: it is written in Java and also can be natively used in any JVM-based language, contains many ATR algorithms and multiple methods for term candidates collection; it is highly modular and adaptable.", "startOffset": 2, "endOffset": 6}, {"referenceID": 45, "context": "However, it lacks a lot of actual state-of-the-art methods, namely those based on occurrences contexts, topic models, Wikipedia, and non-trivial ranking algorithms such as Voting [46] and PU-ATR [3].", "startOffset": 179, "endOffset": 183}, {"referenceID": 2, "context": "However, it lacks a lot of actual state-of-the-art methods, namely those based on occurrences contexts, topic models, Wikipedia, and non-trivial ranking algorithms such as Voting [46] and PU-ATR [3].", "startOffset": 195, "endOffset": 198}, {"referenceID": 29, "context": "In order to perform these tasks, ATR4S incorporated 3 external NLP libraries: Stanford CoreNLP [30], Emory nlp4j and Apache OpenNLP.", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "By default, we use stop words list from the SMART retrieval system [40].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "By default, we apply the commonly-used pattern [7] extended by allowing prepositions between nouns: (NN(S)?|JJ|NNP|NN(S?)IN)*(NN(S)?)", "startOffset": 47, "endOffset": 50}, {"referenceID": 46, "context": "Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].", "startOffset": 98, "endOffset": 102}, {"referenceID": 46, "context": "Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].", "startOffset": 137, "endOffset": 141}, {"referenceID": 5, "context": "Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 7, "context": "RIDF was originally proposed for keywords extraction [8] and than re-used for term recognition [47].", "startOffset": 53, "endOffset": 56}, {"referenceID": 46, "context": "RIDF was originally proposed for keywords extraction [8] and than re-used for term recognition [47].", "startOffset": 95, "endOffset": 99}, {"referenceID": 42, "context": "[43] that supports one-word term candidates as well:", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 158, "endOffset": 162}, {"referenceID": 35, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 181, "endOffset": 185}, {"referenceID": 24, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 201, "endOffset": 205}, {"referenceID": 43, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 293, "endOffset": 303}, {"referenceID": 34, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 293, "endOffset": 303}, {"referenceID": 46, "context": "Note that ATR4S does not include methods based on word association measures like z-test [12], t-test [9], \u03c7-test, Loglikelihood [13], Mutual Information (MI) [10], Lexical Cohesion [36], Term Cohesion [25], because they were repeatedly shown to obtain not better results than simple frequency [44,35,47].", "startOffset": 293, "endOffset": 303}, {"referenceID": 19, "context": "Methods from this group follows the distributional hypothesis [20] and try to distinguish terms from non-terms by considering their contexts.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "We are aware of only 2 such methods: NC-Value [18] and DomainCoherence [6]; since the latter is a modification of the former and was shown to work better [6], ATR4S includes only it.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "We are aware of only 2 such methods: NC-Value [18] and DomainCoherence [6]; since the latter is a modification of the former and was shown to work better [6], ATR4S includes only it.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "We are aware of only 2 such methods: NC-Value [18] and DomainCoherence [6]; since the latter is a modification of the former and was shown to work better [6], ATR4S includes only it.", "startOffset": 154, "endOffset": 157}, {"referenceID": 32, "context": "DomainPertinence [33] is the simplest implementation of this idea:", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "Weirdness [1] normalizes it by sizes (in number of words) of document collections:", "startOffset": 10, "endOffset": 13}, {"referenceID": 37, "context": "Relevance [38] further updates it by taking into account fraction of documents, where term candidate occur:", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "To the best of our knowledge, this group contains only one method capable to extract terms of arbitrary length, that is Novel Topic Model [27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "Example of its usage is an additional filter for other methods [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "LinkProbability [2] is a normalized frequency of being hyperlink in Wikipedia pages:", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "KeyConceptsRelatedness [2] interprets domain-specific terms as words and collocations that are semantically related to knowingly domain-specific concepts.", "startOffset": 23, "endOffset": 26}, {"referenceID": 33, "context": "We modified this: instead of Dice measure, we use cosine distance between word embedding vectors [34] corresponding to Wikipedia concepts.", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "We propose to use simplified version of KP-Miner [14]: In order to be considered as a candidate to key concept, a word or a collocation must: (a) occur at least twice; (b) have an occurrence among the first 800 words; (c) be a valid term candidate, i.", "startOffset": 49, "endOffset": 53}, {"referenceID": 5, "context": "Examples include PostRankDC [6] and GlossEx [36].", "startOffset": 28, "endOffset": 31}, {"referenceID": 35, "context": "Examples include PostRankDC [6] and GlossEx [36].", "startOffset": 44, "endOffset": 48}, {"referenceID": 45, "context": "Voting algorithm [46] considers values of all term candidates and it was shown [46] to outperform single methods and weighted average (i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 45, "context": "Voting algorithm [46] considers values of all term candidates and it was shown [46] to outperform single methods and weighted average (i.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "More sophisticated approach is PU-ATR [2], which is based on the ideas of bootstrapping (like NC-Value and DomainCoherence) and positive unlabeled (PU) learning.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "ComboBasic is recommended [3] as a seed method, because (a) it lets to adjust the level of specificity of seed terms and thus indirectly affects the level of specificity of all terms; (b) it is simple enough to include terms of different nature, i.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Thus, following the previous work [3], we assume that scoring methods from different groups weakly correlate and choose them as features: C-Value (occurrences frequencies), DomainCoherence (occurrences contexts), Relevance (reference corpora), NovelTopicModel (topic modeling), LinkProbability (Wikipedia, domain-independent specificity), KeyConceptRelatedness (Wikipedia, domain-specificity).", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Different Positive-Unlabeled algorithms were shown [3] to work similarly for this task, so we chose the simplest one [29], with Logistic Regression as an internal probabilistic classifier (during preliminary experiments we found it outperforming Random Forest classifier).", "startOffset": 51, "endOffset": 54}, {"referenceID": 28, "context": "Different Positive-Unlabeled algorithms were shown [3] to work similarly for this task, so we chose the simplest one [29], with Logistic Regression as an internal probabilistic classifier (during preliminary experiments we found it outperforming Random Forest classifier).", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "We evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "We evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.", "startOffset": 49, "endOffset": 53}, {"referenceID": 25, "context": "We evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "We evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.", "startOffset": 78, "endOffset": 82}, {"referenceID": 44, "context": "We evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.", "startOffset": 95, "endOffset": 99}, {"referenceID": 38, "context": "0 [39], EuroParl [24] with Eurovoc thesaurus.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "0 [39], EuroParl [24] with Eurovoc thesaurus.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Krapivin Computer Science 2304 21189 8766 Authors\u2019 keywords and Protodog [16] glossary Patents Engineering 12 120 1595 Manual markup", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "Experimental comparison confirms observations that (1) no single method is best for all datasets [46] and (2) multiple features should be combined for better quality [17]; also it shows that other available tools lack the best methods, i.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "Experimental comparison confirms observations that (1) no single method is best for all datasets [46] and (2) multiple features should be combined for better quality [17]; also it shows that other available tools lack the best methods, i.", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "actual state-of-the-art methods, namely PU-ATR [3], KeyConceptRelatedness [3], NovelTopicModel [27], and Basic [6].", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "actual state-of-the-art methods, namely PU-ATR [3], KeyConceptRelatedness [3], NovelTopicModel [27], and Basic [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 26, "context": "actual state-of-the-art methods, namely PU-ATR [3], KeyConceptRelatedness [3], NovelTopicModel [27], and Basic [6].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "actual state-of-the-art methods, namely PU-ATR [3], KeyConceptRelatedness [3], NovelTopicModel [27], and Basic [6].", "startOffset": 111, "endOffset": 114}], "year": 2016, "abstractText": "Automatically recognized terminology is widely used for various domain-specific texts processing tasks, such as machine translation, information retrieval or ontology construction. However, there is still no agreement on which methods are best suited for particular settings and, moreover, there is no reliable comparison of already developed methods. We believe that one of the main reasons is the lack of state-of-the-art methods implementations, which are usually non-trivial to recreate. In order to address these issues, we present ATR4S, an open-source software written in Scala that comprises more than 15 methods for automatic terminology recognition (ATR) and implements the whole pipeline from text document preprocessing, to term candidates collection, term candidates scoring, and finally, term candidates ranking. It is highly scalable, modular and configurable tool with support of automatic caching. We also compare 13 state-of-the-art methods on 7 open datasets by average precision and processing time. Experimental comparison reveals that no single method demonstrates best average precision for all datasets and that other available tools for ATR do not contain the best methods.", "creator": "LaTeX with hyperref package"}}}