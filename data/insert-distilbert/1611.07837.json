{"id": "1611.07837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Adaptive Feature Abstraction for Translating Video to Language", "abstract": "a new model for video captioning is developed, using a deep three - dimensional convolutional neural network ( c3d ) implemented as essentially an encoder for videos and a recurrent neural network ( rnn ) as a decoder for captions. we consider both \" hard \" representations and \" soft \" attention mechanisms, to adaptively and sequentially focus on different layers of features ( levels of feature \" abstraction \" ), particularly as damn well as local spatiotemporal regions of the aforementioned feature maps at each layer. the proposed approach is evaluated on three benchmark video datasets : youtube2text, channel m - vad and msr - vtt. along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of modifying the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences languages with rich semantics.", "histories": [["v1", "Wed, 23 Nov 2016 15:21:48 GMT  (4421kb,D)", "http://arxiv.org/abs/1611.07837v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["yunchen pu", "martin renqiang min", "zhe gan", "lawrence carin"], "accepted": false, "id": "1611.07837"}, "pdf": {"name": "1611.07837.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yunchen Pu", "Martin Renqiang Min"], "emails": ["yunchen.pu@duke.edu", "renqiang@nec-labs.com", "zhe.gan@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Videos represent among the most widely used forms of data, and their accurate characterization poses an important challenge for computer vision and machine learning. Generating a naturallanguage description of a video, termed video captioning, is an important component of video analysis. Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al., 2015; Venugopalan et al., 2015a;b; Pan et al., 2016b; Yu et al., 2016) employs a twodimensional (2D) or three-dimensional (3D) Convolutional Neural Network (CNN) as an encoder, mapping an input video to a compact feature-vector representation. A Recurrent Neural Network (RNN) is typically employed as a decoder, unrolling the feature vector to generate a sequence of words of arbitrary length.\nDespite achieving encouraging success in video captioning, previous models suffer important limitations. Often the rich video content is mapped to a single feature vector for caption generation; this approach is prone to miss detailed and localized spatiotemporal information. To mitigate this, one may employ methods to focus attention on local regions of the feature map, but typically this is done with features from a selected (usually top) CNN layer. By employing features from a fixed CNN layer, the algorithm is limited in its ability to model rich, context-aware semantics that requires focusing on different feature abstraction levels. As investigated in Mahendran & Vedaldi\n\u2217Most of this work was done when the author was an intern at NEC Labs America.\nar X\niv :1\n61 1.\n07 83\n7v 1\n[ cs\n.C V\n(2015); Zeiler & Fergus (2014), the feature characteristics/abstraction is correlated with the CNN layer: features from layers at or near the top of a CNN tend to focus on global (extended) visual percepts, while features from lower CNN layers provide more local, fine-grained information. It is desirable to select/weight features from different CNN layers adaptively when decoding a caption, selecting different levels of feature abstraction by sequentially emphasizing features from different CNN layers. In addition to focusing on features from different CNN layers, it is also desirable to emphasize local spatiotemporal regions in feature maps at particular layers.\nTo realize these desiderata, the proposed decoding process for generating a sequence of words dynamically emphasizes different levels (CNN layers) of 3D convolutional features, to model important coarse or fine-grained spatiotemporal structure. Additionally, the model adaptively attends to different locations within the feature maps at particular layers. While some previous models use 2D CNN features to generate video representations, our model adopts features from a deep 3D convolutional neural network (C3D). Such features have been shown to be effective for video representations, action recognition and scene understanding (Tran et al., 2015), by learning the spatiotemporal features that can provide better appearance and motion information. In addition, the proposed model is inspired by the recent success of attention-based models that mimic human perception (Mnih et al., 2014; Xu et al., 2015). Adaptive spatiotemporal feature representation, with dynamic feature abstraction, involves comparing and evaluating different levels of 3D convolutional feature maps. A challenge is that the features from different C3D levels have distinct dimensions. For example, low-level features provide fine resolution on localized spatiotemporal regions, while high-level features capture extended spatiotemporal space with less resolution. To enable direct comparisons between layers, we employ convolution operations to map different levels of features to the same semantic-space dimension.\nThe principal contributions of this paper are: (i) A new video-caption-generation model is developed, based on two distinct means (\u201chard\u201d and \u201csoft\u201d) of imposing attention. (ii) Our model empolys the attention to adaptively and sequentially emphasize different levels of feature abstraction (CNN layers), while also imposing attention within local spatiotemporal regions of the feature maps at each layer. (iii) A 3D convolutional transformation is introduced to achieve spatiotemporal and semantic feature consistency across different layers. (iv) The proposed model achieves state-of-the-art performance on several benchmark datasets. We call the proposed algorithm Adaptive SpatioTemporal representation with dynAmic abstRaction (ASTAR)."}, {"heading": "2 RELATED WORK", "text": "Early work on video captioning used a two-step approach, employing role-word detection (e.g., subject, verb and object) and rules for language grammar. In such work, the sentence for video description is first split into several parts, each of which is aligned with visual content. For example, Rohrbach et al. (2013; 2014) learn a Conditional Random Field (CRF) to infer high-level concepts such as object and action; in Guadarrama et al. (2013) semantic hierarchies are used to choose an appropriate level of sentence fragments. Statistical language models, learned from large text corpora, are used to translate the semantic representation to a grammatically correct sentence.\nRecent work often develops a probabilistic model of the caption, conditioned on a video. Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al. (2016a) applied a 2D CNN pretrained on ImageNet to video frames, with the top-layer output of the CNN used as features. Given the sequence of features extracted from video frames, the video representation is then obtained by a CRF (Donahue et al., 2015), mean pooling (Venugopalan et al., 2015b), weighted mean pooling with attention (Yu et al., 2016), or via the last hidden state of an RNN encoder (Venugopalan et al., 2015a). These works were followed by Pan et al. (2016b), which jointly embedded the 2D CNN features and spatiotemporal features extracted from a 3D CNN (Tran et al., 2015). However, all of these previous models utilize features extracted from the top layer of the CNN.\nThe proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al. (2016). In addition, a powerful decoder enhanced with two attention mechanisms is constructed for generating captions, while a simple RNN decoder is employed in Ballas et al. (2016). Finally, we use features extracted from C3D instead of a 2D CNN."}, {"heading": "3 METHOD", "text": "Consider N training videos, the nth of which is denoted X(n), with associated caption Y(n). The length-Tn caption is represented Y(n) = (y (n) 1 , . . . ,y (n) Tn\n), with y(n)t a 1-of-V (\u201cone hot\u201d) encoding vector, with V the size of the vocabulary.\nFor each video, the C3D feature extractor (Tran et al., 2015) produces a set of features A(n) = {a(n)1 , . . . ,a (n) L ,a (n) L+1}, where {a (n) 1 , . . . ,a (n) L } are feature maps extracted from L convolutional layers, and a(n)L+1 is a vector obtained from the top fully-connected layer.\nThe convolutional-layer features, {a(n)1 , . . . ,a (n) L }, are extracted by feeding the entire video into C3D, and hence the dimensions of {a(n)1 , . . . ,a (n) L } are dependent on the video length (number of frames). As discussed below, we employ spatiotemporal attention at each layer (and between layers), and therefore it is not required that the sizes of {a(n)1 , . . . ,a (n) L } be the same for all videos. However, the fully connected layer at the top, responsible for a(n)L+1, assumes that a (n) L is of the same size for all videos (like the 16-frame-length videos in (Tran et al., 2015)). To account for variablelength videos, and maintain the same fully-connected layer at the top, we employ mean pooling to a (n) L , based on a window of length 16 (as in (Tran et al., 2015)) with an overlap of 8 frames. The particular form of pooling used here (one could also use max pooling) is less important than the need to make the dimension of the top-layer features the same for feeding into the final fully-connected layer."}, {"heading": "3.1 CAPTION MODEL", "text": "For notational simplicity, henceforth we omit superscript n. The t-th word in a caption, yt, is mapped to an M -dimensional vector wt = Weyt, where We \u2208 RM\u00d7V is a learned wordembedding matrix, i.e., wt is a column of We chosen by the one-hot yt. The probability of caption Y = {yt}t=1,T is defined as\np(Y|A) = p(y1|A) \u220fT t=2 p(yt|y<t,A) . (1)\nSpecifically, the first word y1 is drawn from p(y1|A) = softmax(Vh1), where h1 = tanh(CaL+1). Bias terms are omitted for simplicity throughout the paper. All the other words in the caption are then sequentially generated using an RNN, until the end-sentence symbol is generated. Conditional distribution p(yt|y<t,A) is specified as softmax(Vht), where ht is recursively updated as ht = H(wt\u22121,ht\u22121, zt). V is a matrix connecting the RNN hidden state to a softmax, for computing a distribution over words. zt = \u03c6(ht\u22121,a1, . . . ,aL) is the context vector used in the attention mechanism, capturing the relevant visual features associated with the spatiotemporal attention (also weighting level of feature abstraction), as detailed in Sec. 3.2.\nNote that the output of the fully-connected-layer, aL+1, is only used to generate the first word (encapsulating overall-video features). We found that only using aL+1 there works better in practice\nthan using it at each time step of the RNN, as also found in Vinyals et al. (2015); Venugopalan et al. (2015b).\nThe transition function H(\u00b7) is implemented with Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997). At time t, the LSTM unit consists of a memory cell ct and three gates: the input it, forget f t, and output ot gates. The memory cell transmits the information from the previous step to the current step, while the gates control reading or writing the memory unit through sigmoid functions. Specifically, the hidden units ht are updated as follows:\nit = \u03c3(Wiwwt\u22121 +Wihht\u22121 +Wizzt), f t = \u03c3(Wfwwt\u22121 +Wfhht\u22121 +Wfzzt),\not = \u03c3(Wowwt\u22121 +Wohht\u22121 +Wozzt), c\u0303t = tanh(Wcwwt\u22121 +Wchht\u22121 +Wczzt),\nct = f t ct\u22121 + it c\u0303t, ht = ot tanh(ct), (2) where \u03c3(\u00b7) and denote the sigmoid function and the element-wise multiply operator, respectively. Matrices W{i,f,o,c}, V and C represent the set of LSTM parameters that will be learned (plus associated biases).\nGiven the video X (with features A) and associated caption Y, the objective function is the sum of the log-likelihood of the caption conditioned on the video representation:\nlog p(Y|A) = log p(y1|A) + \u2211T t=2 log p(yt|y<t,A) , (3)\nEquation (3) is a function of all model parameters to be learned; they are not explicitly depicted in (3) for notational simplicity. Further, (3) corresponds to a single video-caption pair, and when training we sum over all such training pairs. Our proposed model is illustrated in Figure 1."}, {"heading": "3.2 ATTENTION MECHANISM", "text": "We first define notation needed to describe attention mechanism \u03c6(ht\u22121,a1, . . . ,aL). Each feature map, al, is a 4D tensor, with elements corresponding to two spatial coordinates (i.e., vertical and horizontal dimensions in a given frame), third tensor index in the frame-index dimension, and a fourth dimension associated with the filter index (for the convolutional filters). To be explicit, at CNN layer l, the number of dimensions of this tensor are denoted nlx\u00d7nly\u00d7nlz\u00d7nlF , with respective dimensions corresponding to vertical, horizontal, frame, and filter (e.g., nlF convolutional filters at layer l). Note that dimensions nlx, n l y and n l z vary with layer level l (getting smaller with increasing l, due to pooling).\nWe define ai,l as a nlF -dimensional vector, corresponding to a fixed coordinate in three of the tensor dimensions, i.e., i \u2208 [1, . . . , nlx] \u00d7 [1, . . . , nly] \u00d7 [1, . . . , nlz], while sweeping across all nlF feature/filter dimensions. Further, define al(k) as a 3D tensor, associated with 4D tensor al. Specifically, al(k) corresponds to the 3D tensor manifested from al, with k \u2208 {1, . . . , nlF } a fixed coordinate in the dimension of the filter index. Hence, al(k) corresponds to the 3D feature map at layer l, due to the kth filter at that layer.\nWe introduce two attention mechanisms when predicting word yt: (i) spatiotemporal-localization attention, and (ii) abstraction-level attention; these, respectively, measure the relative importance of a particular spatiotemporal location and a particular CNN layer (feature abstraction) for producing yt, based on the word-history information y<t. The most straightforward way to generate the attention weights is to employ a multi-layer perceptron (MLP) (Xu et al., 2015; Bahdanau et al.,\n2015) on ai,l, as this corresponds to the ith spatial-spatial-frame location at layer l. However, this has three challenges: (i) the dimension of al depends on layer index l, undermine use of a single MLP for all ai,l; (ii) the features represented in each layer by al are not spatiotemporally aligned (i.e., there is no correspondence between i across layers); and (iii) the semantic meaning of each feature vector ai,l corresponds the convolutional filters in each layer of C3D, which varies across layers. Issue (ii) undermines the ability to quantify the value of attention at a specific spatiotemporal location, based on information from all CNN layers. Issue (iii) implies that the layer-dependent features are in different semantic spaces, which makes it difficult to compare the value of each layer for predicting the next word in the caption and feed the features into the same LSTM decoder.\nTo address this, we seek to map al \u2192 a\u0302l, where 4D tensors a\u0302l all have the same dimensions, are embedded into same semantic spaces, and are aligned spatialtemporally. Specifically, a\u0302l, l = 1, . . . , L\u22121 are aligned in the above ways with aL. To achieve this, we filter each al, l = 1, . . . , L\u2212 1, and then apply max-pooling; the filters seek semantic alignment of the features (including feature dimension), and the pooling is used to spatiotemporally align the features with aL. Specifically, consider\na\u0302l = f( \u2211nlF k=1 al(k) \u2217Uk,l), (4)\nfor l = 1, . . . , L\u2212 1, and with a\u0302L = aL. As discussed above, al(k) is the 3D feature map (tensor) for dictionary k \u2208 {1, . . . , nlF } at layer l, and Uk,l is a 4D tensor. The convolution \u2217 in (4) operates in the three shift dimensions, and al(k)\u2217Uk,l manifests a 4D tensor. Specifically, each of the nLF 3D \u201cslices\u201d of Uk,l are spatiotemporally convolved (3D convolution) with ak,l, and after summing over the nlF convolutional filters, followed by f(\u00b7), this manifests each of the nLF 3D slices of a\u0302l. Function f(\u00b7) is an element-wise nonlinear activation function, followed by max pooling, with the pooling dimensions meant to realize final dimensions consistent with aL, i.e., dimension nLx\u00d7nLy \u00d7nLz \u00d7nLF . Consequently, a\u0302i,l \u2208 Rn L F is a feature vector with i \u2208 [1, . . . , nLx ]\u00d7 [1, . . . , nLy ]\u00d7 [1, . . . , nLz ]. Note that it may only require one 3D tensor Ul applied on each 3D slices al(k) for each layer to achieve spatiotemporal alignment of the layer-dependent features. However, the features from two distinct layers will not be in the same \u201csemantic\u201d space, making it difficult to assess the value of the layerdependent features. The multiple tensors in set {Uk,l} provide the desired semantic alignment between layers, allowing analysis of the value of features from different layers via a single MLP, in the following (5) and (6).\nWith {a\u0302l}l=1,L semantically and spatiotemporally aligned, we now seek to jointly quantify the value of a particular spatiotemporal region and a particular feature layer (\u201cabstraction\u201d) for prediction of the next word. For each a\u0302i,l, the attention mechanism generates two positive weights, \u03b1ti and \u03b2tl, which measure the relative importance of location i and layer l for producing yt based y<t. Attention weights \u03b1ti and \u03b2tl and context vector zt are computed as\neti = w T \u03b1 tanh(Wa\u03b1a\u0302i +Wh\u03b1ht\u22121), \u03b1ti = softmax({eti}), st = \u03c8({a\u0302i}, {\u03b1ti}), (5)\nbtl = w T \u03b2 tanh(Ws\u03b2stl +Wh\u03b2ht\u22121), \u03b2tl = softmax({btl}), zt = \u03c8({stl}, {\u03b2tl}), (6)\nwhere a\u0302i is a vector composed by stacking {a\u0302i,l}l=1,L (all features at position i). eti and btl are scalars reflecting the importance of spatiotemporal region i and layer t to predicting yt, while \u03b1ti and \u03b2tl are relative weights of this importance, reflected by the softmax output. \u03c8(\u00b7) is a function that returns a single feature vector when given a set of feature vectors, and their corresponding weights across all i or l. Vector stl reflects the sub-portion of st associated with layer l.\nIn (5) we provide attention in the spatiotemporal dimensions, with that spatiotemporal attention shared across all L (now aligned) CNN layers. In (6) the attention is further refined, focusing attention in the layer dimension.\nTo make the following discussion concrete, we describe the attention function within the context of zt = \u03c8({stl}, {\u03b2tl}). This function setup is applied in the same way to st = \u03c8({a\u0302i}, {\u03b1ti}).\nSoft attention Following Bahdanau et al. (2015), we formulate the soft attention model by computing a weighted sum of the input features\nzt = \u03c8({stl}, {\u03b2tl}) = \u2211L l=1 \u03b2tlstl. (7)\nThe model is differentiable for all parameters and can be learned end-to-end using standard backpropagation.\nHard attention Let mt \u2208 {0, 1}L be a vector of all zeros, and a single one, and the location of the non-zero element of mt identifies the location to extract features for generating the next word. We impose\nmt \u223c Mult(1, {\u03b2tl}), zt = \u2211L l=1mtlstl. (8)\nIn this case, optimizing the objective function in (3) is intractable. However, the marginal loglikelihood can be lower-bounded as\nlog p(Y|A) = log \u2211 m p(m|A)p(Y|m,A) \u2265 \u2211\nm p(m|A) log p(Y|m,A), (9) where m = {mt}t=1,...,T . Inspired by importance sampling, the multi-sample stochastic lower bound has been recently used for latent variable models (Ba et al., 2015; Burda et al., 2016), defined as\nLK(Y) = \u2211\nm1:K p(m 1:K |A) [ log 1K \u2211K k=1 p(Y|mk,A) ] , (10)\nwhere m1, . . . ,mK are independent samples. This lower bound is guaranteed to be tighter with the increase of the number of samples K (Burda et al., 2016), thus providing a better approximation of the objective function than (9). As shown in Mnih & Rezende (2016), the gradient of LK(Y) with respect to the model parameters is\n\u2207LK(Y) = \u2211\nm1:K p(m 1:K |A) \u2211K k=1 [ L(m1:K)\u2207 log p(mk|A) + \u03c9k\u2207p(Y|mk,A) ] , (11)\nwhere L(m1:K) = log 1K \u2211K k=1 p(Y|mk,A) and \u03c9k = p(Y|mk,A)\u2211 j p(Y|mj ,A)\n. A variance reduction technique is introduced in Mnih & Rezende (2016) by replacing the above gradient with an unbiased estimator\n\u2207LK(Y) \u2248 \u2211K k=1 [ L\u0302(mk|m\u2212k)\u2207 log p(mk|A) + \u03c9k\u2207p(Y|mk,A) ] , (12)\nwhere L\u0302(mk|m\u2212k) = L(m1:K)\u2212 log 1K (\u2211 j 6=k p(Y|mj ,A) + f(Y,m\u2212k,A)) ) , (13)\nf(Y,m\u2212k,A) = exp( 1K\u22121 \u2211 j 6=k log p(Y|mj ,A). (14)\nWhen learning the model parameters, the lower bound (10) is optimized via the gradient approximation in (12).\nAs an alternative method, one may first produce abstraction-level attention weights \u03b2l, followed by generating spatiotemporal attention weights \u03b1i, i.e., switching the order of (5) and (6). However, we empirically found that this method provides slightly worse performance (we implemented and tested both) in our experiments, partially due to the increase of the number of parameters."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 DATASETS", "text": "We present results on three benchmark datasets: Microsoft Research Video Description Corpus (YouTube2Text) (Chen & Dolan, 2011), Montreal Video Annotation Dataset (M-VAD) (Torabi et al., 2015), and Microsoft Research Video to Text (MSR-VTT) (Xu et al., 2016).\nThe Youtube2Text contains 1970 Youtube clips, and each video is annotated with around 40 sentences. For fair comparison, we used the same splits as provided in Venugopalan et al. (2015b), with 1200 videos for training, 100 videos for validation, and 670 videos for testing.\nThe M-VAD is a large-scale movie description dataset, which is composed of 46587 movie snippets annotated with 56752 sentences. We follow the setting in Torabi et al. (2015), taking 36920 videos for training, 4950 videos for validation, and 4717 videos for testing.\nThe MSR-VTT is a newly collected large-scale video dataset, consisting of 20 video categories. The dataset was split into 6513, 2990 and 497 clips in the training, testing and validation sets. Each video has about 20-sentence descriptions. The ground-truth captions in the testing set are not available now. Thus, we split the original training dataset into a training set of 5513 clips and a testing set of 1000 clips.\nWe convert all captions to lower case and remove the punctuation, yielding vocabulary sizes V = 12594 for Youtube2Text, V = 13276 for M-VAD, and V = 8071 for MSR-VTT."}, {"heading": "4.2 TRAINING PROCEDURE", "text": "We consider the RGB frames of videos as input, and all videos are resized to 112 \u00d7 112 spatially, with 2 frames per second; note that the temporal sample rate of the videos are consequently consistent, but not necessarily the total number of frames. The C3D (Tran et al., 2015) is pretrained on Sports-1M dataset Karpathy et al. (2014), consisting of 1.1 million sports videos belonging to 487 categories. We extract the features from four convolutional layers and one fully connected layer, named as pool2, pool3, pool4, pool5 and fc-7 in the C3D (Tran et al., 2015), respectively. The model architecture of C3D is provided in Appendix B. The kernel sizes of the convolutional transformation in (4) are 7 \u00d7 7 \u00d7 7, 5 \u00d7 5 \u00d7 5 and 3 \u00d7 3 \u00d7 3 for layer pool2, pool3 and pool4 with 3 \u00d7 3 \u00d7 3, 2\u00d7 2\u00d7 2 and 1\u00d7 1\u00d7 1 zero padding, respectively. f(\u00b7) is implemented by ReLU (Nair & Hinton, 2010), followed by 3D max-pooling with 8\u00d7 8\u00d7 8, 4\u00d7 4\u00d7 4 and 2\u00d7 2\u00d7 2 ratios. More details for achieving spatiotemporal alignment are provided in Appendix C .\nAll recurrent matrices in the LSTM are initialized with orthogonal initialization (Saxe et al., 2014). We initialize non-recurrent weights from a uniform distribution in [\u22120.01, 0.01] and all the bias terms are initialized to zero. Word embedding vectors are initialized with the publicly available word2vec vectors that were trained on 100 billion words from Google News, which have dimensionality 300, and were trained using a continuous bag-of-words architecture (Mikolov et al., 2013). The embedding vectors of words not present in the pre-trained set are initialized randomly. The number of hidden units in the LSTM is set as 512 and we use mini-batches of size 32. Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014). The number of samples for multi-sample stochastic lower bound is set to 10. We do not perform any datasetspecific tuning and regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets. The Adam algorithm Kingma & Ba (2014) with learning rate 0.0002 is utilized for optimization. All experiments are implemented in Torch (Collobert et al., 2011)."}, {"heading": "4.3 EVALUATION", "text": "The widely used BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and CIDEr (Vedantam et al., 2015) metrics are employed to quantitatively evaluate the performance of our video caption generation model, and other models in the literature. For each dataset, we show three types of results, using part of or all of our model to illustrate the role of each component:\n1. C3D fc7 + LSTM : The LSTM caption decoder is employed, only using features extracted from the top fully-connected layer. No context vector zt is generated from intermediate convolutional layer features.\n2. Spatiotemporal attention + LSTM: The context vector zt is included, but only features extracted from a certain convolutional layer are employed, i.e., zt is equal to st in (5). The spatiotemporal attention is implemented with the soft attention in (7). For example, \u201cC3D fc7 + pool2\u201d means we leverage the features from layer pool2 to generate attention weights and context vector.\n3. ASTAR: This is our proposed model developed in Sec. 3.2. We present results based on both soft attention and hard attention.\nTo verify the effectiveness of our video caption generation model and C3D features, we also implement a strong baseline method based on the LSTM encoder-decoder network (Cho et al., 2014), where ResNet He et al. (2016) is employed as the feature extractor on each frame. We denote results using this method as ResNet + LSTM ."}, {"heading": "4.4 QUANTITATIVE RESULTS", "text": "Results are summarized in Tables 1 and 2, and we obtain state-of-theart results on both Youtube2Text and M-VAD datasets. On the MSRVTT dataset, our results also significantly outperform the strong baselines, demonstrating the effectiveness of the proposed model. Note that no comparative results exist in the literature for MSR-VTT. As a reference, with similar settings, Xu et al. (2016) using C3D fc7 + LSTM achieved 39.9 on BLEU-4 and 29.3 on METEOR using the standard testing set, while state-of-the-art results are 40.8 on BLEU-4, 28.2 on METEOR, and 44.8 on CIDEr, achieved by team \u201cv2t navigator\u201d from RUC and CMU 1.\nIt is worth noting that our model consistently yields significant improvements over models with only spatiotemporal attention, which further achieve better performance than only taking the C3D top fully-connected layer features; this demonstrates the importance of leveraging intermediate convolutional layer features. In addition, we achieve these results using a single model, without averaging over an ensemble of such models."}, {"heading": "4.5 QUALITATIVE RESULTS", "text": "Following Xu et al. (2015), we visualize the attention components learned by our model on Youtube2Text in Figure 3. As can be seen from Figure 3, the spatiotemporal attention aligns the objects in the video well with the corresponding words. In addition, the abstraction-level attention tends to focus on low level features when the model will generate a noun and high level features when an article or a preposition is to be generated. More results are provided in Appendix A.1.\nExamples of generated captions from unseen videos on Youtube2Text are shown in Figure 4. We find the results with abstraction-layer attention (indicated as \u201csoft attention\u201d or \u201c hard attention\u201d) is generally equal to or better than the best results, compared to those only taking a certain convolutional-layer feature (indicated as \u201cPool2\u201d etc.) . It demonstrates the effectiveness of our abstraction layer attention. More results are provided in Appendix A.2 and http://www.cs.toronto.edu/pub/cuty/videocaption/.\n1http://ms-multimedia-challenge.com/leaderboard"}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "We have proposed a novel video captioning model, that adaptively selects/weights the feature abstraction (CNN layer), as well as the location within a layer-dependent feature map. We have implemented the attention using both \u201chard\u201d and \u201csoft\u201d mechanisms, with the latter typically delivering better performance. Our model achieves excellent video caption generation performance, and has the capacity to provide interpretable alignments seemingly analogous to human perception.\nWe have focused on analysis of videos and associated captions. Similar ideas may be applied in the future to image captioning. Additionally, the CNN parameters were learned separately as a first step, prior to analysis of the captions. It is also of interest to consider CNN-parameter refinement conditioned on observed training captions."}, {"heading": "A MORE RESULTS", "text": "A.1 VISUALIZATION OF ATTENTION WEIGHTS\nA.2 GENERATED CAPTIONS\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na man is smiling\na man is eating a banana\na man is talking\na man is talking\na man is eating a banana\na man is eating a banana\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na monkey is running\na monkey is doing martial arts\na monkey is fighting\na monkey is fighting\na monkey is doing martial arts\na monkey is doing martial arts\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na puppy is playing\na puppy is playing with a toy\na puppy is playing\na baby panda is playing\na pig is eating a carrot\na baby pig is eating a carrot\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na man is lifting weights\na man is sharpening a knife\na man is doing something\na man is lifting weights\na man is putting a knife in a vice\na man is sharpening a knife\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na man is kicking a basketball\na boy is hitting a basketball\na man is hitting a basketball\na man is dribbling a basketball\na man is dribbling a basketball\na man is dribbling a basketball\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na man is dancing\na man is performing on stage\na man is playing a guitar\na man is playing a guitar\na band is performing on a stage\na band is playing a guitar\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na person is cutting a vegetable\na person is cutting a vegetable\na person is cutting a vegetable\na man is cutting a vegetable\na woman is slicing a vegetable\na woman is slicing a vegetable\nPool2:\nPool3:\nPool4:\nPool5:\nSoft attention:\nHard attention:\na man is talking\na man is talking\na man is singing\na man is singing\na man and woman are talking\na man and woman are sitting on\na motorcycle"}, {"heading": "B MODEL ARCHITECTURE OF C3D", "text": "The 3D pooling layers are denoted from pool1 to pool5. All pooling kernels are 2\u00d7 2\u00d7 2, except for pool1 is 1\u00d7 2\u00d7 2. Each fully\nconnected layer has 4096 output units.\nMethod Number of Nets Clip hit@1 Video hit@1 Video hit@5\nDeepVideo\u2019s Single-Frame + Multires [18] 3 nets 42.4 60.0 78.5\nDeepVideo\u2019s Slow Fusion [18] 1 net 41.9 60.9 80.2 Convolution pooling on 120-frame clips [29] 3 net 70.8* 72.4 90.8 C3D (trained from scratch) 1 net 44.9 60.0 84.4 C3D (fine-tuned from I380K pre-trained model) 1 net 46.1 61.1 85.2\nTable 2. Sports-1M classification result. C3D outperforms [18] by 5% on top-5 video-level accuracy. (*)We note that the method of [29] uses long clips, thus its clip-level accuracy is not directly comparable to that of C3D and DeepVideo.\nextract C3D feature, a video is split into 16 frame long clips with a 8-frame overlap between two consecutive clips. These clips are passed to the C3D network to extract fc6 activations. These clip fc6 activations are averaged to form a 4096-dim video descriptor which is then followed by an L2-normalization. We refer to this representation as C3D video descriptor/feature in all experiments, unless we clearly specify the difference.\nWhat does C3D learn? We use the deconvolution method explained in [46] to understand what C3D is learning internally. We observe that C3D starts by focusing on appearance in the first few frames and tracks the salient motion in the subsequent frames. Figure 4 visualizes deconvolution of two C3D conv5b feature maps with highest activations projected back to the image space. In the first example, the feature focuses on the whole person and then tracks the motion of the pole vault performance over the rest of the frames. Similarly in the second example it first focuses on the eyes and then tracks the motion happening around the eyes while applying the makeup. Thus C3D differs from standard 2D ConvNets in that it selectively attends to both\nmotion and appearance. We provide more visualizations in\nthe supplementary material to give a better insight about the\nlearned feature.\n4. Action recognition Dataset: We evaluate C3D features on UCF101 dataset [38]. The dataset consists of 13, 320 videos of 101 human action categories. We use the three split setting provided with this dataset.\nClassification model: We extract C3D features and in-\nput them to a multi-class linear SVM for training models.\ntiple nets setting, we concatenate the L2-normalized C3D descriptors of these nets.\nBaselines: We compare C3D feature with a few baselines: the current best hand-crafted features, namely improved dense trajectories (iDT) [44] and the popular-used deep image features, namely Imagenet [16], using Caffe\u2019s Imagenet pre-train model. For iDT, we use the bag-of-word representation with a codebook size of 5000 for each feature channel of iDT which are trajectories, HOG, HOF, MBHx, and MBHy. We normalize histogram of each channel separately using L1-norm and concatenate these normalized histograms to form a 25K feature vector for a video. For Imagenet baseline, similar to C3D, we extract Imagenet fc6 feature for each frame, average these frame features to make video descriptor. A multi-class linear SVM is also used for these two baselines for a fair comparison.\nResults: Table 3 presents action recognition accuracy of C3D compared with the two baselines and current best methods. The upper part shows results of the two baselines. The middle part presents methods that use only RGB frames as inputs. And the lower part reports all current best\nmethods using all possible feature combinations (e.g. opti-\ncal flows, iDT).\nC3D fine-tuned net performs best among three C3D nets described previously. The performance gap between these three nets, however, is small (1%). From now on, we refer to the fine-tuned net as C3D, unless otherwise stated. C3D using one net which has only 4, 096 dimensions obtains an accuracy of 82.3%. C3D with 3 nets boosts the accuracy to 85.2% with the dimension is increased to 12, 288. C3D when combined with iDT further improves the accuracy to\n90.4%, while when it is combined with Imagenet, we ob-"}, {"heading": "C DIMENSIONS AND RECEPTIVE FIELD OF C3D FEATURES", "text": "The dimensions for features extracted from pool2, pool3, pool4 and pool5 are 28\u00d728\u00d7N/2\u00d7128, 14 \u00d7 14 \u00d7 N/4 \u00d7 256, 7 \u00d7 7 \u00d7 N/8 \u00d7 512 and 4 \u00d7 4 \u00d7 N/16 \u00d7 512, respectively. N is the number of frames of input video. After the convolutional transformation, the dimensions will be all 4\u00d7 4\u00d7N/16\u00d7 512. To prove these features are spatiotemporal aligned, we first provide the receptive field for 3D convolutional layer and 3D pooling layer. Let Y = 3D-Conv(X), where 3D-Conv is the 3D convolutional layer with kernel size 3\u00d73\u00d73. The features indexed by i = [ix, iy, iz] in Y is obtained by convolving a subset of X indexed by j = [jx, jy, jz] with convolutional kernel, where jx \u2208 [ix\u22121, ix, ix+1], jy \u2208 [iy \u2212 1, iy, iy + 1] and jz \u2208 [iz \u2212 1, iz, iz + 1]. Then, we call that the receptive field of i = [ix, iy, iz] in Y is [ix\u22121, . . . , ix+1]\u00d7 [iy\u22121, . . . , iy+1]\u00d7 [iz\u22121, . . . , iz+1] in X. Similarly, if Y = 3D-pooling(X) with pooling ratio 2 \u00d7 2 \u00d7 2, the receptive field of i = [ix, iy, iz] in Y is [2ix \u2212 1, 2ix]\u00d7 [2iy \u2212 1, 2iy]\u00d7 [2iz \u2212 1, 2iz] in X. We then provide the receptive field of features al from each layer in the input video in Table 3 and receptive field of features after convolutional transformation, a\u0302l, in the original feature al in Tabel 4. The features are all indexed by i = [ix, iy, iz]. Combining Table 3 and Tabel 4, we can find the receptive field of a\u0302l indexed by i = [ix, iy, iz] for all l in the input video are all [32ix \u2212 63, . . . , 32ix + 30]\u00d7 [32iy \u2212 63, . . . , 32iy + 30]\u00d7 [16iz \u2212 32, . . . , 16iz + 15]. We index the top-left element in the first frame as [1, 1, 1]. Note that the index of receptive field could be negative due to padding.\nTable 3: Receptive field of al in input video\nLayer name Receptive field\nPool2 [4ix \u2212 7, . . . , 4ix + 2]\u00d7 [4iy \u2212 7, . . . , 4iy + 2]\u00d7 [2iz \u2212 4, . . . , 2iz + 1]\nPool3 [8ix \u2212 15, . . . , 8ix + 6]\u00d7 [8iy \u2212 15, . . . , 8iy + 6]\u00d7 [4iz \u2212 8, . . . , 4iz + 3]\nPool4 [16ix \u2212 31, . . . , 16ix + 14]\u00d7 [16iy \u2212 31, . . . , 16iy + 14]\u00d7 [8iz \u2212 16, . . . , 8iz + 7]\nPool5 [32ix \u2212 63, . . . , 32ix + 30]\u00d7 [32iy \u2212 63, . . . , 32iy + 30]\u00d7 [16iz \u2212 32, . . . , 16iz + 15]"}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["J. Ba", "R. Grosse", "R. Salakhutdinov", "B. Frey"], "venue": "In NIPS,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Delving deeper into convolutional networks for learning video representations", "author": ["N. Ballas", "L. Yao", "C. Pal", "A. Courville"], "venue": "In ICLR,", "citeRegEx": "Ballas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballas et al\\.", "year": 2016}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "In ACL workshop,", "citeRegEx": "Banerjee and Lavie.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D. Chen", "W.B. Dolan"], "venue": "In ACL,", "citeRegEx": "Chen and Dolan.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "Sun J"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "In CVPR,", "citeRegEx": "Karpathy and Li.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In CVPR,", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "In ICLR,", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Variational inference for monte carlo objectives", "author": ["A. Mnih", "D.J. Rezende"], "venue": "In ICML,", "citeRegEx": "Mnih and Rezende.,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende.", "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectied linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["P. Pan", "Z. Xu", "Y. Yang", "F. Wu", "Y. Zhuang"], "venue": "In CVPR,", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "In CVPR,", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Coherent multisentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "GCPR", "citeRegEx": "Rohrbach et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In ICCV,", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "In ICLR,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["A Torabi", "H Larochelle C Pal", "A Courville"], "venue": "In arXiv preprint arXiv:1503.01070,", "citeRegEx": "Torabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "In ICCV,", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "Z.C. Lawrence", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "In NAACL,", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Msr-vtt: A large video description dataset for bridging video and language", "author": ["J. Xu", "T. Mei", "T. Yao", "Y. Rui"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J.L. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al.", "startOffset": 81, "endOffset": 146}, {"referenceID": 1, "context": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al.", "startOffset": 81, "endOffset": 146}, {"referenceID": 29, "context": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al.", "startOffset": 81, "endOffset": 146}, {"referenceID": 15, "context": ", 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al.", "startOffset": 37, "endOffset": 118}, {"referenceID": 35, "context": ", 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al.", "startOffset": 37, "endOffset": 118}, {"referenceID": 17, "context": ", 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al.", "startOffset": 37, "endOffset": 118}, {"referenceID": 8, "context": ", 2015), most recent work on video captioning (Donahue et al., 2015; Venugopalan et al., 2015a;b; Pan et al., 2016b; Yu et al., 2016) employs a twodimensional (2D) or three-dimensional (3D) Convolutional Neural Network (CNN) as an encoder, mapping an input video to a compact feature-vector representation.", "startOffset": 46, "endOffset": 133}, {"referenceID": 38, "context": ", 2015), most recent work on video captioning (Donahue et al., 2015; Venugopalan et al., 2015a;b; Pan et al., 2016b; Yu et al., 2016) employs a twodimensional (2D) or three-dimensional (3D) Convolutional Neural Network (CNN) as an encoder, mapping an input video to a compact feature-vector representation.", "startOffset": 46, "endOffset": 133}, {"referenceID": 31, "context": "Such features have been shown to be effective for video representations, action recognition and scene understanding (Tran et al., 2015), by learning the spatiotemporal features that can provide better appearance and motion information.", "startOffset": 116, "endOffset": 135}, {"referenceID": 20, "context": "In addition, the proposed model is inspired by the recent success of attention-based models that mimic human perception (Mnih et al., 2014; Xu et al., 2015).", "startOffset": 120, "endOffset": 156}, {"referenceID": 37, "context": "In addition, the proposed model is inspired by the recent success of attention-based models that mimic human perception (Mnih et al., 2014; Xu et al., 2015).", "startOffset": 120, "endOffset": 156}, {"referenceID": 9, "context": "(2013; 2014) learn a Conditional Random Field (CRF) to infer high-level concepts such as object and action; in Guadarrama et al. (2013) semantic hierarchies are used to choose an appropriate level of sentence fragments.", "startOffset": 111, "endOffset": 136}, {"referenceID": 8, "context": "Given the sequence of features extracted from video frames, the video representation is then obtained by a CRF (Donahue et al., 2015), mean pooling (Venugopalan et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 38, "context": ", 2015b), weighted mean pooling with attention (Yu et al., 2016), or via the last hidden state of an RNN encoder (Venugopalan et al.", "startOffset": 47, "endOffset": 64}, {"referenceID": 31, "context": "(2016b), which jointly embedded the 2D CNN features and spatiotemporal features extracted from a 3D CNN (Tran et al., 2015).", "startOffset": 104, "endOffset": 123}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al.", "startOffset": 0, "endOffset": 70}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al. (2016a) applied a 2D CNN pretrained on ImageNet to video frames, with the top-layer output of the CNN used as features.", "startOffset": 0, "endOffset": 90}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al. (2016a) applied a 2D CNN pretrained on ImageNet to video frames, with the top-layer output of the CNN used as features. Given the sequence of features extracted from video frames, the video representation is then obtained by a CRF (Donahue et al., 2015), mean pooling (Venugopalan et al., 2015b), weighted mean pooling with attention (Yu et al., 2016), or via the last hidden state of an RNN encoder (Venugopalan et al., 2015a). These works were followed by Pan et al. (2016b), which jointly embedded the 2D CNN features and spatiotemporal features extracted from a 3D CNN (Tran et al.", "startOffset": 0, "endOffset": 559}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways.", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al.", "startOffset": 33, "endOffset": 172}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al. (2016). In addition, a powerful decoder enhanced with two attention mechanisms is constructed for generating captions, while a simple RNN decoder is employed in Ballas et al.", "startOffset": 33, "endOffset": 260}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al. (2016). In addition, a powerful decoder enhanced with two attention mechanisms is constructed for generating captions, while a simple RNN decoder is employed in Ballas et al. (2016). Finally, we use features extracted from C3D instead of a 2D CNN.", "startOffset": 33, "endOffset": 435}, {"referenceID": 31, "context": "For each video, the C3D feature extractor (Tran et al., 2015) produces a set of features A = {a 1 , .", "startOffset": 42, "endOffset": 61}, {"referenceID": 31, "context": "However, the fully connected layer at the top, responsible for a L+1, assumes that a (n) L is of the same size for all videos (like the 16-frame-length videos in (Tran et al., 2015)).", "startOffset": 162, "endOffset": 181}, {"referenceID": 31, "context": "To account for variablelength videos, and maintain the same fully-connected layer at the top, we employ mean pooling to a (n) L , based on a window of length 16 (as in (Tran et al., 2015)) with an overlap of 8 frames.", "startOffset": 168, "endOffset": 187}, {"referenceID": 33, "context": "than using it at each time step of the RNN, as also found in Vinyals et al. (2015); Venugopalan et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 33, "context": "(2015); Venugopalan et al. (2015b). The transition function H(\u00b7) is implemented with Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997).", "startOffset": 8, "endOffset": 35}, {"referenceID": 1, "context": "Soft attention Following Bahdanau et al. (2015), we formulate the soft attention model by computing a weighted sum of the input features", "startOffset": 25, "endOffset": 48}, {"referenceID": 0, "context": "Inspired by importance sampling, the multi-sample stochastic lower bound has been recently used for latent variable models (Ba et al., 2015; Burda et al., 2016), defined as L(Y) = \u2211 m1:K p(m 1:K |A) [ log 1 K \u2211K k=1 p(Y|m,A) ] , (10)", "startOffset": 123, "endOffset": 160}, {"referenceID": 4, "context": "Inspired by importance sampling, the multi-sample stochastic lower bound has been recently used for latent variable models (Ba et al., 2015; Burda et al., 2016), defined as L(Y) = \u2211 m1:K p(m 1:K |A) [ log 1 K \u2211K k=1 p(Y|m,A) ] , (10)", "startOffset": 123, "endOffset": 160}, {"referenceID": 4, "context": "This lower bound is guaranteed to be tighter with the increase of the number of samples K (Burda et al., 2016), thus providing a better approximation of the objective function than (9).", "startOffset": 90, "endOffset": 110}, {"referenceID": 4, "context": "This lower bound is guaranteed to be tighter with the increase of the number of samples K (Burda et al., 2016), thus providing a better approximation of the objective function than (9). As shown in Mnih & Rezende (2016), the gradient of L(Y) with respect to the model parameters is \u2207L(Y) = \u2211 m1:K p(m 1:K |A) \u2211K k=1 [ L(m)\u2207 log p(m|A) + \u03c9k\u2207p(Y|m,A) ] , (11)", "startOffset": 91, "endOffset": 220}, {"referenceID": 30, "context": "We present results on three benchmark datasets: Microsoft Research Video Description Corpus (YouTube2Text) (Chen & Dolan, 2011), Montreal Video Annotation Dataset (M-VAD) (Torabi et al., 2015), and Microsoft Research Video to Text (MSR-VTT) (Xu et al.", "startOffset": 171, "endOffset": 192}, {"referenceID": 36, "context": ", 2015), and Microsoft Research Video to Text (MSR-VTT) (Xu et al., 2016).", "startOffset": 56, "endOffset": 73}, {"referenceID": 33, "context": "For fair comparison, we used the same splits as provided in Venugopalan et al. (2015b), with 1200 videos for training, 100 videos for validation, and 670 videos for testing.", "startOffset": 60, "endOffset": 87}, {"referenceID": 30, "context": "We follow the setting in Torabi et al. (2015), taking 36920 videos for training, 4950 videos for validation, and 4717 videos for testing.", "startOffset": 25, "endOffset": 46}, {"referenceID": 30, "context": "[1,2,3,4] represent Venugopalan et al. (2015a); Pan et al.", "startOffset": 20, "endOffset": 47}, {"referenceID": 21, "context": "(2015a); Pan et al. (2016b); Ballas et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 2, "context": "(2016b); Ballas et al. (2016); Yu et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 2, "context": "(2016b); Ballas et al. (2016); Yu et al. (2016), respectively.", "startOffset": 9, "endOffset": 48}, {"referenceID": 31, "context": "The C3D (Tran et al., 2015) is pretrained on Sports-1M dataset Karpathy et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 31, "context": "We extract the features from four convolutional layers and one fully connected layer, named as pool2, pool3, pool4, pool5 and fc-7 in the C3D (Tran et al., 2015), respectively.", "startOffset": 142, "endOffset": 161}, {"referenceID": 13, "context": ", 2015) is pretrained on Sports-1M dataset Karpathy et al. (2014), consisting of 1.", "startOffset": 43, "endOffset": 66}, {"referenceID": 27, "context": "All recurrent matrices in the LSTM are initialized with orthogonal initialization (Saxe et al., 2014).", "startOffset": 82, "endOffset": 101}, {"referenceID": 18, "context": "Word embedding vectors are initialized with the publicly available word2vec vectors that were trained on 100 billion words from Google News, which have dimensionality 300, and were trained using a continuous bag-of-words architecture (Mikolov et al., 2013).", "startOffset": 234, "endOffset": 256}, {"referenceID": 29, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).", "startOffset": 68, "endOffset": 92}, {"referenceID": 28, "context": "We do not perform any datasetspecific tuning and regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets.", "startOffset": 83, "endOffset": 108}, {"referenceID": 7, "context": "All experiments are implemented in Torch (Collobert et al., 2011).", "startOffset": 41, "endOffset": 65}, {"referenceID": 17, "context": "Word embedding vectors are initialized with the publicly available word2vec vectors that were trained on 100 billion words from Google News, which have dimensionality 300, and were trained using a continuous bag-of-words architecture (Mikolov et al., 2013). The embedding vectors of words not present in the pre-trained set are initialized randomly. The number of hidden units in the LSTM is set as 512 and we use mini-batches of size 32. Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014). The number of samples for multi-sample stochastic lower bound is set to 10. We do not perform any datasetspecific tuning and regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets. The Adam algorithm Kingma & Ba (2014) with learning rate 0.", "startOffset": 235, "endOffset": 795}, {"referenceID": 24, "context": "The widely used BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and CIDEr (Vedantam et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 32, "context": ", 2002), METEOR (Banerjee & Lavie, 2005) and CIDEr (Vedantam et al., 2015) metrics are employed to quantitatively evaluate the performance of our video caption generation model, and other models in the literature.", "startOffset": 51, "endOffset": 74}, {"referenceID": 6, "context": "To verify the effectiveness of our video caption generation model and C3D features, we also implement a strong baseline method based on the LSTM encoder-decoder network (Cho et al., 2014), where ResNet He et al.", "startOffset": 169, "endOffset": 187}, {"referenceID": 6, "context": "To verify the effectiveness of our video caption generation model and C3D features, we also implement a strong baseline method based on the LSTM encoder-decoder network (Cho et al., 2014), where ResNet He et al. (2016) is employed as the feature extractor on each frame.", "startOffset": 170, "endOffset": 219}, {"referenceID": 36, "context": "As a reference, with similar settings, Xu et al. (2016) using C3D fc7 + LSTM achieved 39.", "startOffset": 39, "endOffset": 56}, {"referenceID": 36, "context": "Following Xu et al. (2015), we visualize the attention components learned by our model on Youtube2Text in Figure 3.", "startOffset": 10, "endOffset": 27}], "year": 2016, "abstractText": "A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. We consider both \u201chard\u201d and \u201csoft\u201d attention mechanisms, to adaptively and sequentially focus on different layers of features (levels of feature \u201cabstraction\u201d), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.", "creator": "LaTeX with hyperref package"}}}