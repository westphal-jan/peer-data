{"id": "1611.08070", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2016", "title": "Multiscale Inverse Reinforcement Learning using Diffusion Wavelets", "abstract": "this work simultaneously presents a multiscale empirical framework to solve mainly an inverse reinforcement learning ( irl ) problem for continuous - time / state stochastic systems. we take advantage of a diffusion wavelet representation of across the associated markov chain to abstract the state space. this not only allows for effectively handling the large ( and geometrically complex ) decision space equation but also provides more interpretable representations of together the demonstrated state trajectories and also of the resulting policy of irl. in assuming the proposed framework, understanding the problem is divided into the global and local irl, where the global approximation of to the optimal value functions are obtained using coarse features and the local details assumptions are quantified using fine local features. an illustrative structured numerical example on robot automated path control in a simple complex environment structure is presented repeatedly to verify the proposed method.", "histories": [["v1", "Thu, 24 Nov 2016 05:20:52 GMT  (979kb,D)", "http://arxiv.org/abs/1611.08070v1", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"]], "COMMENTS": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jung-su ha", "han-lim choi"], "accepted": false, "id": "1611.08070"}, "pdf": {"name": "1611.08070.pdf", "metadata": {"source": "CRF", "title": "Multiscale Inverse Reinforcement Learning using Diffusion Wavelets", "authors": ["Jung-Su Ha"], "emails": ["hanlimc}@kaist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we address an inverse reinforcement learning (IRL) problem (or often equivalently called an inverse optimal control problem) for robots operated in a complex environment over a long time horizon, where its forward problem is a continuous time/continuous stochastic optimal control problem called linearly solvable optimal control (LSOC). The objective of an IRL problem is to recover the value and cost functions as well as the optimal policy when experts\u2019 demonstrations are given. A general method to solve IRL problem for standard MDP often involves the procedure of solving the corresponding forward problem in every iteration [7], while a method for LSOC does not [2]. The IRL solution method for LSOC is formulated as a convex optimization problem, where its gradient and Hessian are obtained analytically. However, the optimization tends to be intractable as the size of the problem increases and moreover, in real situation, a demonstration data set may be not sufficient to represent whole state space. Finding a sparse structure of the problem and representing the problem with few meaningful bases are essential for obtaining the solution efficiently.\nTo address a large-scale problems effectively, it is conceivable that the hallmark of human intelligence could provide some insights - in particular, this work notes multiscale and hierarchical structure of human decision making. Suppose that someone currently writing a paper at his/her office desk wants to get out of the building; the office is located in the third floor of the building and there is one set of staircases and an elevator. Then, what would this person\u2019s control policy look like? This person would not try to figure out what he/she should do for all possible situations he/she could face like the standard value function-based approach; instead, he/she would figure out which building gate he/she would use, whether to take the elevator or the stairs, which door he would exit from the room (if there are more than one), etc. A detailed plans such as \u201cwhich particular start he/she should put their left foot,\u201d would be determined later in the process of executing a piece of overall plan, for example, \u201cgo downstairs using the staircase.\u201d It should be noted that this human-like decision takes advantage of the underlying (multiscale) hierarchical structure of state space; in the above example, detailed aspects such a particular certain sequence of stairs to put on is abstracted by just a single notion of staircase.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n08 07\n0v 1\n[ cs\n.L G\n] 2\n4 N\nov 2\nSame intuition can be applied to the inverse inference problem. By utilizing it, the key contribution of this paper is to present a systematic framework for solving IRL, as depicted in Fig. 1. The framework consists of five phases: (i) In the discretization phase, Markov chain associated to the robot dynamics is constructed by sampling a set finite set of states in state-space. (ii) In the abstraction phase, the hierarchical bases structure is obtained using the diffusion wavelet method. (iii) In global IRL phase, an IRL problem constructed only using the coarse bases (or on \u201cabstract-state\u201d) is solved; this IRL is much more tractable to handle than using the original bases set. (iv) In the local planning phase, the finer bases located in focused regions where the demonstration data visit frequently are sought for and detailed solution associated with these focused regions are computed. (vi) In the control phase, a continuous control sequence is computed and applied to the robot in a receding horizon fashion. Rest of the paper is primarily focused on elaborating the details of this framework, followed by numerical example for validation of the method."}, {"heading": "2 Inverse Reinforcement Learning", "text": ""}, {"heading": "2.1 Linearly-solvable optimal control", "text": "Consider a stochastic dynamics whose deterministic drift term is affine in control input:\ndx = f(x)dt+G(x)(udt+ \u03c3dw) (1)\nwhere x \u2208 X \u2282 Rdx , u \u2208 Rdu and w are a state, control vector and an du-dimensional Brownian motion process, respectively. Let functions q : X \u2192 R and \u03c0c : X \u2192 Rdu be an instantaneous state cost rate and a control policy, respectively and define an instantaneous cost rate as l(x,u) := q(x) + 12\u03c32u Tu. Then the cost functional is given as:\nJ\u03c0cconti(x) = limtf\u2192\u221e\n1 tf E [\u222b tf 0 l(x(t), \u03c0c(x(t))dt ] . (2)\nThe problem with the cost function (2) and dynamics (1) is called the infinite horizon average cost stochastic optimal control (SOC) problem or is referred as linearly-solvable optimal control (LSOC) problem since its solution is obtained from the linear partial differential equation [6].\nIf the time-axis is discretized by a time step h, the transition probability of one step without/with any control input is defined as:\nx[k + 1] \u223c p(\u00b7|x[k]), x[k + 1] \u223c \u03c0(\u00b7|x[k]), (3) which are called the passive and controlled dynamics, respectively. The passive and controlled dynamics are approximated as N (y;\u00b5(h),\u03a3(h)), where N is a Gaussian distribution with a mean \u00b5(h) and covariance \u03a3(h). For small h, the Kullback-Leibler divergence between two distributions is approximated as DKL(\u03c0(\u00b7|x)||p(\u00b7|x)) = h2\u03c32u\n\u2032u. Therefore, the cost functional (2) is written in the discrete time setting:\nJ\u03c0(x) = lim K\u2192\u221e\n1\nK E [ K\u2211 k=0 hq(x[k]) +DKL (\u03c0(\u00b7|x[k])||p(\u00b7|x[k])) ] . (4)\nMoreover, the state space can be discretized by sampling a set of states X = {xn} [3]. Transition probability matrix for passive dynamics P , where Pnm means a transition probability from xn to xm, is approximated via Gaussian distribution as:\nPnm = N (xm : \u00b5(h),\u03a3(h))\u2211 m\u2032 N (xm\u2032 : \u00b5(h),\u03a3(h) , (5)\nwhere \u00b5(h) and \u03a3(h) are computed by integrating moment dynamics of linearized SDE for t \u2208 [0, h]:\n\u00b5\u0307(t) = A\u00b5(t) + c, \u03a3\u0307(t) = A\u03a3(t) + \u03a3(t)A\u2032 +BB\u2032 (6) from \u00b5(0) = xn, \u03a3(0) = 0, where A = dfdx \u2223\u2223 x=xn\n, B = \u03c3G(xn) and c = f(xn) \u2212 Axn. One can truncate tails of Gaussian distribution to make P sparse. With a set of discrete states, X , the state-space as well as time-axis discretized version of SOC is formulated as the Markov decision process (MDP) of which cost function is given by (4). This type of MDP is called the linearly-solvable MDP [6] and its solution is known to converge to SOC solution as |X| \u2192 \u221e and h\u2192 0. Define the optimal cost-to-go value c := min\u03c0 J\u03c0(x), the differential cost-to-go function v(x), the (differential) desirability function z(x) = exp(\u2212v(x)), and the linear operator G[z](x) =\u2211\nx\u2032 p(x \u2032|x)z(x\u2032). Then z(x) satisfies the following linear Bellman equation:\nexp(\u2212c)z(x) = exp(\u2212hq(x))G[z](x), (7)\nand the optimal policy is obtained analytically:\n\u03c0\u2217(x\u2032|x) = p(x \u2032|x)z(x\u2032) G[z](x) . (8)\nFor more details of problem formulation and discrete approximation method, we would refer the reader to [3] and references therein."}, {"heading": "2.2 Inverse reinforcement learning for LSOC problem", "text": "While the objective of (forward) SOC problem is to find the optimal control policy for the given system and cost function, the objective of inverse reinforcement learning (IRL) problem is to recover the value and cost functions as well as the optimal policy when experts\u2019 demonstrations are given [2, 7]. Suppose a dataset of transitions {xn,x\u2032n}n=1,\u00b7\u00b7\u00b7 ,N is obtained from the optimal policy (8). Let v be a vector representation of value function v(\u00b7). Then, the negative log-likelihood of the dataset is given as: L[v] = aTv + bT log(P exp(\u2212v)), (9) where each component of a and b represent visitation counts of x\u2032n and xn, respectively. Since L is convex and its gradient and Hessian are computed analytically, it can be minimized by applying iterative second order convex optimization methods. Once the value function v is obtained, the cost function q(\u00b7) and the optimal policy \u03c0\u2217(\u00b7|\u00b7) can be recovered directly from (7) and (8), respectively. In real situation, however, a and b are sparse since the dataset is not sufficient, so it is impossible to compute v(\u00b7) over whole state space. Also, the optimization procedure in (9) gets intractable for problems having the large size of the state space. To represent the problem efficiently, a linear value function approximation can be used: v\u0302 = \u03a6w, (10) where each column of \u03a6 represents a feature (or basis) and w is weight. Note that L[w] is also convex. In this work, we obtain the hierarchical structure of feature sets which is naturally induced from the passive (diffusion) dynamics of the system and utilize those set of features to solve IRL problem efficiently."}, {"heading": "3 Multiscale Inverse Reinforcement Learning", "text": ""}, {"heading": "3.1 Multiscale feature extraction: Diffusion Wavelets", "text": "From now on, we consider T = P \u2032 for notion simplicity; then Tnm represents a transition probability from xm to xn. The Markov chain, T , obtained by discretizing a diffusion process ((1) with u = 0) is known to have some interesting properties: local, smoothing and contractive [1]. From any initial\npoint, \u03b4m, the state (numerically) transitions to only a few its neighbors (i.e., T\u03b4m has a small support) and T j\u03b4m is a smooth probability distribution. Also since ||T ||2 \u2264 1, a dimension of a subspace, Vj , which is -spanned by {T j\u03b4m}xm\u2208X monotonically decreases as j increases and V0 \u2287 V1 \u2287 \u00b7 \u00b7 \u00b7 \u2287 Vj \u2287 \u00b7 \u00b7 \u00b7 ; especially for an irreducible Markov chain, dim(Vj)\u2192 1 as j increases and a limit of Vj corresponds to the stationary distribution of the Markov chain.\nLet Wj be an orthogonal complement of Vj+1 into Vj , i.e., Vj = Vj+1 \u2295 Wj and suppose the orthonormal bases \u03a6j and \u03a8j span Vj and Wj , respectively. By using aforementioned properties of T , Diffusion wavelets constructs a hierarchical structure of a set of well-localized bases \u03a6j and \u03a8j called scaling and wavelet functions, respectively, in order that the subspace spanned by feature set [\u03a6j ]\u03a60 is j -close to the subspace spanned by {T 1+2+2 2+\u00b7\u00b7\u00b7+2j\u22121\u03b4m = T 2j\u22121\u03b4m}xm\u2208X . Roughly speaking, \u03a6j and \u03a8j represent smooth bump function and oscillatory function, respectively. We omit the procedure of Diffusion wavelets algorithm because of the space limitation and would refer the readers to [1, 3] for more details.\nLet [B]\u03a6j be a set of vectors B represented on a basis \u03a6j , where the columns of [B]\u03a6j are the coordinates of the vectors B in the coordinates \u03a6j . A set of features at level j can be written in the original coordinate (or can be unpacked) as:\n\u03a6j = [\u03a6j ]\u03a60 = [\u03a6j\u22121]\u03a60 [\u03a6j ]\u03a6j\u22121 = [\u03a61]\u03a60 \u00b7 \u00b7 \u00b7 [\u03a6j\u22121]\u03a6j\u22122 [\u03a6j ]\u03a6j\u22121 , (11) which is represented as a |X| \u00d7 |Xj | matrix. Note that each column of [\u03a6j ]\u03a60 can be viewed as an \u201cabstract-state\" of the original Markov chain. At the scale j, there are only |Xj | meaningful combinations of states and each combination, [\u03a6j ]\u03a60 , represents \u201cabstract-state\u201d."}, {"heading": "3.2 IRL with hierarchical multiscale feature sets", "text": "Rather than solving original |X|-dimensional optimization problem, we can treat the lowerdimensional coarsened problem. Suppose a set of \u201cabstract-state\" at level j, \u03a6j , is utilized as a set of features, which means the problem is viewed in a lower resolution with 2j time scale. Then, v is approximated as a linear combination of this feature set as v\u0302j = \u03a6jwj , and the optimization problem (9) is also written as:\nL[wj ] = a T\u03a6jwj + b T log(P exp(\u2212\u03a6jwj)). (12) The compressed problem (12) is much more tractable than the original problem (9) if |Xj | << |X|. Note that due to its localization property, the features are naturally interpretable (see Fig. 2 (a)\u2013(d)); thus user can choose the appropriate level, considering trade-off between the size of the problem and the solution quality. Also, the hierarchical structure of diffusion wavelet tree can be utilized to solve the problem more efficiently; the solution of jth level, wj , can provide an initial guess to (j \u2212 1)th level problem; that is, the optimization at (j \u2212 1)th level starts from w\u0303j\u22121 = [\u03a6j ]\u03a6j\u22121wj by unpacking the jth level solution. If v\u0302j is not sharply changed through the scale j, this initial guess would be near the optimum of (j \u2212 1)th level problem and the optimization procedure would rapidly converge to the minimum.\nSuppose we solve lth level problem and have the approximate solution vl by using features \u03a6l. Then, we can achieve more exact solution by considering additional features from wavelet functions, \u03a81:(l\u22121) which are orthogonal to \u03a6l (note that V0 = Vl \u2295Wl\u22121 \u2295Wl\u22122 \u2295 \u00b7 \u00b7 \u00b7 \u2295W0). The wavelet bases are also built as being well-localized. In this work, we utilize the intuition that the important region where optimal policy frequently visits should have highest resolution [3, 2]. The wavelet functions are evaluated as: s = bT |\u03a81:(l\u22121)|, (13) where the score s \u2208 R|X|\u2212|Xl| represents how each feature overlap with visitation counts of x\u2032n. By adding features with high scores and solving the corresponding problem, the value, cost functions and policy will have higher resolution in important region. Also, user can easily choose the additional features for their objective since the wavelet features are also interpretable.\nFinally, the continuous-time optimal policy can be extracted from the multiscale quantification of the optimal value structure for the interval \u03c4 = h \u00d7 kRHC in a receding horizon control fashion. The control can be computed by matching the 1st order moment of original SDE (1) and the optimal policy (8) for the MDP:\nu\u2217(t) = \u2212\u03c32GT eA T (\u03c4\u2212t)\u03a3(\u03c4)\u22121(\u00b5(\u03c4)\u2212 ynew), (14)\nwhere ynew =\n\u2211\ny\u2208X yPr(x[kRHC ] = y|x[0] = xcur, \u03c0\u0302\u2217) denotes the expected state after \u03c4 when the system follows the (approximate) optimal policy (8) from xcur."}, {"heading": "4 Numerical Example", "text": "We consider a simple two-dimensional stochastic single integrator in the fractal-like environment. The environment consists of 5 groups of rooms where one group is made of 5 square rooms as shown in Fig. 2; one can observe that the environment has 2 level self-similarity, which makes the problem have a multiscale nature. The dynamics is given by f(x) = 0, G(x) = I2; that is, the position of a robot in the configuration space, x \u2208 X , is controlled by the velocity input, u \u2208 R2 while being disturbed by white noise. We set h = 0.1 and \u03c3 = 1. In order to discretize the state space, 100 samples are obtained from each room, therefore there are 2500 discrete state in total. The transition data set is obtained from true occupancy measure induced by the optimal policy, which is equivalent to using infinite number of samples.\nFig. 2 (a) - (d) shows multiscale features: some scaling functions in the diffusion wavelet tree at level 3 and 8. It is seen that at level 3 and 8, where roughly 8h and 256h are considered as 1-step, scaling functions roughly represent each small room and one group of 5-rooms, respectively; it has no meaning to make a distinction within a room or a group of 5 rooms at those levels. Fig. 2 (g, i) and (h, j) depict the approximated value and cost functions at level 3 and 8, respectively and Fig. 2 (k) shows the number of features and the RMS error of value functions at each scale. At level 3, only 421 basis functions out of 3000 are used, but the value and cost functions are recovered quite exactly; at level 8, where 25 bases are used, even though the solution contains some error, it interprets the information of optimal policy and the preference of cost function between the groups of 5 rooms. We omit the computational time at each scale because of the space limitation but we observe that the computational time decreases as the number of feature increases; It is obvious that there is a trade-off between the solution quality and the computational cost and the appropriate level can be chosen by observing the features at that level."}, {"heading": "Acknowledgments", "text": "This work was supported by Agency for Defense Development (under contract #UD150047JD)."}], "references": [{"title": "Diffusion wavelets", "author": ["Ronald R Coifman", "Mauro Maggioni"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Inverse optimal control with linearly-solvable mdps", "author": ["Krishnamurthy Dvijotham", "Emanuel Todorov"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Multiscale abstraction, planning and control using diffusion wavelets for stochastic optimal control problems", "author": ["Jung-Su Ha", "Han-Lim Choi"], "venue": "In arXiv preprint,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Fast direct policy evaluation using multiscale analysis of markov diffusion processes", "author": ["Mauro Maggioni", "Sridhar Mahadevan"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Value function approximation with diffusion wavelets and laplacian eigenfunctions", "author": ["Sridhar Mahadevan", "Mauro Maggioni"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Efficient computation of optimal actions", "author": ["Emanuel Todorov"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "A general method to solve IRL problem for standard MDP often involves the procedure of solving the corresponding forward problem in every iteration [7], while a method for LSOC does not [2].", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "A general method to solve IRL problem for standard MDP often involves the procedure of solving the corresponding forward problem in every iteration [7], while a method for LSOC does not [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 5, "context": "The problem with the cost function (2) and dynamics (1) is called the infinite horizon average cost stochastic optimal control (SOC) problem or is referred as linearly-solvable optimal control (LSOC) problem since its solution is obtained from the linear partial differential equation [6].", "startOffset": 285, "endOffset": 288}, {"referenceID": 2, "context": "Moreover, the state space can be discretized by sampling a set of states X = {xn} [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "This type of MDP is called the linearly-solvable MDP [6] and its solution is known to converge to SOC solution as |X| \u2192 \u221e and h\u2192 0.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "For more details of problem formulation and discrete approximation method, we would refer the reader to [3] and references therein.", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "2 Inverse reinforcement learning for LSOC problem While the objective of (forward) SOC problem is to find the optimal control policy for the given system and cost function, the objective of inverse reinforcement learning (IRL) problem is to recover the value and cost functions as well as the optimal policy when experts\u2019 demonstrations are given [2, 7].", "startOffset": 347, "endOffset": 353}, {"referenceID": 6, "context": "2 Inverse reinforcement learning for LSOC problem While the objective of (forward) SOC problem is to find the optimal control policy for the given system and cost function, the objective of inverse reinforcement learning (IRL) problem is to recover the value and cost functions as well as the optimal policy when experts\u2019 demonstrations are given [2, 7].", "startOffset": 347, "endOffset": 353}, {"referenceID": 0, "context": "The Markov chain, T , obtained by discretizing a diffusion process ((1) with u = 0) is known to have some interesting properties: local, smoothing and contractive [1].", "startOffset": 163, "endOffset": 166}, {"referenceID": 0, "context": "We omit the procedure of Diffusion wavelets algorithm because of the space limitation and would refer the readers to [1, 3] for more details.", "startOffset": 117, "endOffset": 123}, {"referenceID": 2, "context": "We omit the procedure of Diffusion wavelets algorithm because of the space limitation and would refer the readers to [1, 3] for more details.", "startOffset": 117, "endOffset": 123}, {"referenceID": 2, "context": "In this work, we utilize the intuition that the important region where optimal policy frequently visits should have highest resolution [3, 2].", "startOffset": 135, "endOffset": 141}, {"referenceID": 1, "context": "In this work, we utilize the intuition that the important region where optimal policy frequently visits should have highest resolution [3, 2].", "startOffset": 135, "endOffset": 141}], "year": 2016, "abstractText": "This work presents a multiscale framework to solve an inverse reinforcement learning (IRL) problem for continuous-time/state stochastic systems. We take advantage of a diffusion wavelet representation of the associated Markov chain to abstract the state space. This not only allows for effectively handling the large (and geometrically complex) decision space but also provides more interpretable representations of the demonstrated state trajectories and also of the resulting policy of IRL. In the proposed framework, the problem is divided into the global and local IRL, where the global approximation of the optimal value functions are obtained using coarse features and the local details are quantified using fine local features. An illustrative numerical example on robot path control in a complex environment is presented to verify the proposed method.", "creator": "LaTeX with hyperref package"}}}