{"id": "1611.08321", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2016", "title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images", "abstract": "starting in this paper, we focus on training and evaluating effective word embeddings with both text and visual information. more specifically, we introduce a large - scale dataset with 300 million sentences accurately describing over 40 million images previously crawled and mostly downloaded from publicly available pins ( i. e. an image with sentence descriptions uploaded by users ) on pinterest. this dataset is noticeably more than 200 times larger than ms coco, the standard large - scale image dataset aligned with sentence descriptions. in addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. the word / phrase pairs in this evaluation dataset are collected from the click data with millions composed of users in an image search system, thus contain rich semantic relationships. based on these datasets, we propose and compare several alternative recurrent neural networks ( rnns ) based multimodal ( text and image ) models. many experiments show that our model implementations benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. the project page is :", "histories": [["v1", "Thu, 24 Nov 2016 23:15:56 GMT  (1378kb,D)", "http://arxiv.org/abs/1611.08321v1", "Appears in NIPS 2016. The datasets introduced in this work will be gradually released on the project page"]], "COMMENTS": "Appears in NIPS 2016. The datasets introduced in this work will be gradually released on the project page", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["junhua mao", "jiajing xu", "kevin jing", "alan l yuille"], "accepted": true, "id": "1611.08321"}, "pdf": {"name": "1611.08321.pdf", "metadata": {"source": "CRF", "title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images", "authors": ["Junhua Mao", "Jiajing Xu", "Yushi Jing", "Alan Yuille"], "emails": ["mjhustc@ucla.edu,", "jiajing@pinterest.com,", "jing@pinterest.com,", "alan.l.yuille@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings are dense vector representations of words with semantic and relational information. In this vector space, semantically related or similar words should be close to each other. A large-scale training dataset with billions of words is crucial to train effective word embedding models. The trained word embeddings are very useful in various tasks and real-world applications that involve searching for semantically similar or related words and phrases.\nA large proportion of the state-of-the-art word embedding models are trained on pure text data only. Since one of the most important functions of language is to describe the visual world, we argue that the effective word embeddings should contain rich visual semantics. Previous work has shown that visual information is important for training effective embedding models. However, due to the lack of large training datasets of the same scale as the pure text dataset, the models are either trained on relatively small datasets (e.g. [13]), or the visual contraints are only applied to limited number of pre-defined visual concepts (e.g. [21]). Therefore, such work did not fully explore the potential of visual information in learning word embeddings.\nIn this paper, we introduce a large-scale dataset with both text descriptions and images, crawled and collected from Pinterest, one of the largest database of annotated web images. On Pinterest, users save web images onto their boards (i.e. image collectors) and supply their descriptions of the images. More descriptions are collected when the same images are saved and commented by other users. Compared to MS COCO (i.e. the image benchmark with sentences descriptions [22]), our dataset is much larger (40 million images with 300 million sentences compared to 0.2 million images and 1 million sentences in the current release of MS COCO) and is at the same scale as the standard pure text training datasets (e.g. Wikipedia Text Corpus). Some sample images and their descriptions are\n1The datasets introduced in this work will be gradually released on the project page.\nar X\niv :1\n61 1.\n08 32\n1v 1\n[ cs\n.L G\n] 2\n4 N\nov 2\nshown in Figure 1 in Section 3.1. We believe training on this large-scale dataset will lead to richer and better generalized models. We denote this dataset as the Pinterest40M dataset.\nOne challenge for word embeddings learning is how to directly evaluate the quality of the model with respect to the tasks (e.g. the task of finding related or similar words and phrases). State-ofthe-art neural language models often use the negative log-likelihood of the predicted words as their training loss, which is not always correlated with the effectiveness of the learned embedding. Current evaluation datasets (e.g. [5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.\nThe challenge of constructing large-scale evaluation datasets is partly due to the difficulty of finding a large number of semantically similar or related word/phrase pairs. In this paper, we utilize user click information collected from Pinterest\u2019s image search system to generate millions of these candidate word/phrase pairs. Because user click data are somewhat noisy, we removed inaccurate entries in the dataset by using crowdsourcing human annotations. This led to a final gold standard evaluation dataset consists of 10,674 entries.\nEquipped with these datasets, we propose, train and evaluate several Recurrent Neural Network (RNN [10]) based models with input of both text descriptions and images. Some of these models directly minimize the Euclidean distance between the visual features and the word embeddings or RNN states, similar to previous work (e.g. [13, 21]). The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts. This strategy imposes soft constraints between the visual features and all the related words in the sentences. Our experiments validate the effectiveness and importance of incorporating visual information into the learned word embeddings.\nWe make three major contributions: Firstly, we constructed a large-scale multimodal dataset with both text descriptions and images, which is at the same scale as the pure text training set. Secondly, we collected and labeled a large-scale evaluation dataset for word and phrase similarity and relatedness evaluation. Finally, we proposed and compared several RNN based models for learning multimodal word embeddings effectively. To facilitate research in this area, we will gradually release the datasets proposed in this paper on our project page."}, {"heading": "2 Related Work", "text": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning. Because it takes lots of resources to label images with sentences descriptions, the scale of these datasets are relatively small (MS COCO, the largest dataset among them, only contains 1 million sentences while our Pinterest40M dataset has 300 million sentences). In addition, the language used to describe images in these datasets is relatively simple (e.g. MS COCO only has around 10,000 unique words appearing at least 3 times while there are 335,323 unique words appearing at least 50 times in Pinterest40M). The Im2Text dataset proposed in [28] adopts a similar data collection process to ours by using 1 million images with 1 million user annotated captions from Flickr. But its scale is still much smaller than our Pinterest40M dataset.\nRecently, [34] proposed and released the YFCC100M dataset, which is a large-scale multimedia dataset contains metadata of 100 million Flickr images. It provides rich information about images, such as tags, titles, and locations where they were taken. The users\u2019 comments can be obtained by querying the Flickr API. Because of the different functionality and user groups between Flickr and Pinterest, the users\u2019 comments of Flickr images are quite different from those of Pinterest (e.g. on Flickr, users tend to comment more on the photography techniques). This dataset provides complementary information to our Pinterest40M dataset.\nWord Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores. The word pairs are composed by asking human subjects to write the first related, or similar, word that comes into their mind when presented with a concept word (e.g. [27, 11]), or by randomly selecting frequent words in large text corpus and manually searching for useful pairs (e.g. [5]). In this work, we are able to collect a large number of word/phrase pairs with good quality by mining them from the click data of Pinterest\u2019s image search system used by\nmillions of users. In addition, because this dataset is collected through a visual search system, it is more suitable to evaluate multimodal embedding models. Another related evaluation is the analogy task proposed in [25]. They ask the model questions like \u201cman to woman is equal king to what?\u201d as their evaluation. But such questions do not directly measure the word similarity or relatedness, and cannot cover all the semantic relationships of million of words in the dictionary.\nRNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7]. We adopt Gated Recurrent Units (GRUs [7]), a variation of the simple RNN model.\nMultimodal Word Embedding Models For pure text, one of the most effective approaches to learn word embeddings is to train neural network models to predict a word given its context words in a sentence (i.e. the continuous bag-of-word model [4]) or to predict the context words given the current word (i.e. the skip-gram model [25]). There is a large literature on word embedding models that utilize visual information. One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17]. [13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25]. However, because of the lack of large-scale multimodal datasets, they only associate visual content with a pre-defined set of nouns (e.g. [21]) or perception domains (e.g. [14]) in the sentences, or focus on abstract scenes (e.g. [19]). By contrast, our best performing model places a soft constraint between visual features and all the words in the sentences by a weight sharing strategy as shown in Section 4."}, {"heading": "3 Datasets", "text": "We constructed two datasets: one for training our multimodal word-embeddings (see Section 3.1) and another one for the evaluation of the learned word-embeddings (see Section 3.2).\n3.1 Training Dataset\nTable 1: Scale comparison with other image descriptions benchmarks.\nImage Sentences\nFlickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M\nPinterest is one of the largest repository of Web images. Users commonly tag images with short descriptions and share the images (and desriptions) with others. Since a given image can be shared and tagged by multiple, sometimes thousands of users, many images have a very rich set of descriptions, making this source of data ideal for training model with both text and image inputs.\nThe dataset is prepared in the following way: first, we crawled the public available data on Pinterest to construct our training dataset of more than 40 million images. Each image is associated with an average of 12 sentences, and we removed duplicated or short sentences with less than 4 words. The duplication\ndetection is conducted by calculating the overlapped word unigram ratios. Some sample images and descriptions are shown in Figure 1. We denote this dataset as the Pinterest40M dataset.\nOur dataset contains 40 million images with 300 million sentences (around 3 billion words), which is much larger than the previous image description datasets (see Table 1). In addition, because the descriptions are annotated by users who expressed interest in the images, the descriptions in our dataset are more natural and richer than the annotated image description datasets. In our dataset, there are 335,323 unique words with a minimum number of occurence of 50, compared with 10,232 and 65,552 words appearing at least 3 times in MS COCO and IM2Text dataset respectively. To the best of our knowledge, there is no previous paper that trains a multimodal RNN model on a dataset of such scale."}, {"heading": "3.2 Evaluation Datasets", "text": "This work proposes to use labeled phrase triplets \u2013 each triplet is a three-phrase tuple containing phrase A, phrase B and phrase C, where A is considered as semantically closer to B than A is to C. At testing time, we compute the distance in the word embedding space between A/B and A/C, and consider a test triplet as positive if d(A,B) < d(A,C). This relative comparison approach was commonly used to evaluate and compare different word embedding models [30].\nIn order to generate large number of phrase triplets, we rely on user-click data collected from Pinterest image search system. At the end, we construct a large-scale evaluation dataset with 9.8 million triplets (see Section 3.2.1), and its cleaned up gold standard version with 10 thousand triplets (see Section 3.2.2)."}, {"heading": "3.2.1 The Raw Evaluation Dataset from User Clickthrough Data", "text": "It is very hard to obtain a large number of semantically similar or related word and phrase pairs. This is one of the challenges for constructing a large-scale word/phrase similarity and relatedness evaluation dataset. We address this challenge by utilizing the user clickthrough data from Pinterest image search system, see Figure 2 for an illustration.\nMore specifically, given a query from a user (e.g. \u201chair styles\u201d), the search system returns a list of items, and each item is composed of an image and a set of annotations (i.e. short phrases or words that describe the item). Please note that the same annotation can appear in multiple items, e.g., \u201chair tutorial\u201d can describe items related to prom hair styles or ponytails. We derive a matching score for each annotation by aggregating the click frequency of the items containing the annotation. The annotations are then ranked according to the matching scores, and the top ranked annotations are considered as the positive set of phrases or words with respect to the user query.\nTo increase the difficulty of this dataset, we remove the phrases that share common words with the user query from the initial list of positive phrases. E.g. \u201chair tutorials\u201d will be removed because the word \u201chair\u201d is contained in the query phrase \u201chair styles\u201d. A stemmer in Python\u2019s \u201cstemmer\u201d package is also adopted to find words with the same root (e.g. \u201ccake\u201d and \u201ccakes\u201d are considered as the same word). This pruning step also prevents giving bias to methods which measure the similarity between the positive phrase and the query phrase by counting the number of overlapping words between them. In this way, we collected 9,778,508 semantically similar phrase pairs.\nPrevious word similarity/relatedness datasets (e.g. [11, 14]) manually annotated each word pair with an absolute score reflecting how much the words in this pair are semantically related. In the testing stage, a predicted similarity score list of the word pairs generated by the model in the dataset is compared with the groundtruth score list. The Spearman\u2019s rank correlation between the two lists is calculated as the score of the model. However, it is often too hard and expensive to label the absolute related score and maintain the consistency across all the pairs in a large-scale dataset, even if we average the scores of several annotators.\nWe adopt a simple strategy by composing triplets for the phrase pairs. More specifically, we randomly sample negative phrases from a pool of 1 billion phrases. The negative phrase should not contain any overlapping word (a stemmer is also adopted) with both of the phrases in the original phrase pair. In this way, we construct 9,778,508 triplets with the format of (base phrase, positive phrase, negative phrase). In the evaluation, a model should be able to distinguish the positive phrase from the negative phrase by calculating their similarities with the base phrase in the embedding space. We denote this dataset as Related Phrase 10M (RP10M) dataset."}, {"heading": "3.2.2 The Cleaned-up Gold Standard Dataset", "text": "Because the raw Related Query 10M dataset is built upon user click information, it contains some noisy triplets (e.g. the positive and base phrase are not related, or the negative phrase is strongly related to the base phrase). To create a gold standard dataset, we conduct a clean up step using the crowdsourcing platform CrowdFlower [1] to remove these inaccurate triplets. A sample question and choices for the crowdsourcing annotators are shown in Figure 3. The positive and negative phrases in a triplet are randomly given as choice \u201cA\u201d or \u201cB\u201d. The annotators are required to choose which phrase is more related to the base phrase, or if they are both related or unrelated. To help the annotators understand the meaning of the phrases, they can click on the phrases to get Google search results.\nWe annotate 21,000 triplets randomly sampled from the raw Related Query 10M dataset. Three to five annotators are assigned to each question. A triplet is accepted and added in the final cleaned up dataset only if more than 50% of the annotators agree with the original positive and negative label of the queries (note that they do not know which one is positive in the annotation process). In practice, 70% of the selected phrases triplets have more than 3 annotators to agree. This leads to a\ngold standard dataset with 10,674 triplets. We denote this dataset as Gold Phrase Query 10K (Gold RP10K) dataset.\nThis dataset is very challenging and a successfully model should be able to capture a variety of semantic relationships between words or phrases. Some sample triplets are shown in Table 2."}, {"heading": "4 The Multimodal Word Embedding Models", "text": "We propose three RNN-CNN based models to learn the multimodal word embeddings, as illustrated in Figure 4. All of the models have two parts in common: a Convolutional Neural Network (CNN [20]) to extract visual representations and a Recurrent Neural Network (RNN [10]) to model sentences.\nFor the CNN part, we resize the images to 224\u00d7 224, and adopt the 16-layer VGGNet [32] as the visual feature extractor. The binarized activation (i.e. 4096 binary vectors) of the layer before its SoftMax layer are used as the image features and will be mapped to the same space of the state of RNN (Model A, B) or the word embeddings (Model C), depends on the structure of the model, by a fully connected layer and a Rectified Linear Unit function (ReLU [26], ReLU(x) = max(0, x)).\nFor the RNN part, we use a Gated Recurrent Unit (GRU [7]), an recently very popular RNN structure, with a 512 dimensional state cell. The state of GRU ht for each word with index t in a sentence can be represented as:\nrt = \u03c3(Wr[et, ht\u22121] + br) (1)\nut = \u03c3(Wu[et, ht\u22121] + bu) (2)\nct = tanh(Wc[et, rt ht\u22121] + bc) (3) ht = ut ht\u22121 + (1\u2212 ut) ct (4)\nwhere represents the element-wise product, \u03c3(.) is the sigmoid function, et denotes the word embedding for the word wt, rt and ut are the reset gate and update gate respectively. The inputs of the GRU are words in a sentence and it is trained to predict the next words given the previous words.\nWe add all the words that appear more than 50 times in the Pinterest40M dataset into the dictionary. The final vocabulary size is 335,323. Because the vocabulary size is very huge, we adopt the sampled SoftMax loss [8] to accelerate the training. For each training step, we sample 1024 negative words according to their log frequency in the training data and calculate the sampled SoftMax loss for the positive word. This sampled SoftMax loss function of the RNN part is adopted with Model A, B and C. Minimizing this loss function can be considered as approximately maximizing the probability of the sentences in the training set.\nAs illustrated in Figure 4, Model A, B and C have different ways to fuse the visual information in the word embeddings. Model A is inspired by the CNN-RNN based image captioning models [36, 23]. We map the visual representation in the same space as the GRU states to initialize them (i.e. set h0 = ReLU(WIfI)). Since the visual information is fed after the embedding layer, it is usually hard to ensure that this information is fused in the learned embeddings. We adopt a transposed weight\nsharing strategy proposed in [23] that was originally used to enhance the models\u2019 ability to learn novel visual concepts. More specifically, we share the weight matrix of the SoftMax layer UM with the matrix Uw of the word embedding layer in a transposed manner. In this way, UTw is learned to decode the visual information and is enforced to incorporate this information into the word embedding matrix Uw. In the experiments, we show that this strategy significantly improve the performance of the trained embeddings. Model A is trained by maximizing the log likelihood of the next words given the previous words conditioned on the visual representations, similar to the image captioning models.\nCompared to Model A, we adopt a more direct way to utilize the visual information for Model B and Model C. We add direct supervisions of the final state of the GRU (Model B) or the word embeddings (Model C), by adding new loss terms, in addition to the negative log-likelihood loss from the sampled SoftMax layer:\nLstate = 1\nn \u2211 s \u2016 hls \u2212 ReLU(WIfIs) \u2016 (5)\nLemb = 1\nn \u2211 s 1 ls \u2211 t \u2016 et \u2212 ReLU(WIfIs) \u2016 (6)\nwhere ls is the length of the sentence s in a mini-batch with n sentences, Eqn. 5 and Eqn. 6 denote the additional losses for model B and C respectively. The added loss term is balanced by a weight hyperparameter \u03bb with the negative log-likehood loss from the sampled SoftMax layer."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Training Details", "text": "We convert the words in all sentences of the Pinterest40M dataset to lower cases. All the nonalphanumeric characters are removed. A start sign \u3008bos\u3009 and an end sign \u3008eos\u3009 are added at the beginning and the end of all the sentences respectively.\nWe use the stochastic gradient descent method with a mini-batch size of 256 sentences and a learning rate of 1.0. The gradient is clipped to 10.0. We train the models until the loss does not decrease on a small validation set with 10,000 images and their descriptions. The models will scan the dataset for roughly five 5 epochs. The bias terms of the gates (i.e. br and bu in Eqn. 1 and 2) in the GRU layer are initialized to 1.0."}, {"heading": "5.2 Evaluation Details", "text": "We use the trained embedding models to extract embeddings for all the words in a phrase and aggregate them by average pooling to get the phrase representation. We then check whether the cosine distance between the (base phrase, positive phrase) pair are smaller than the (base phrase, negative phrase) pair. The average precision over all the triplets in the raw Related Phrases 10M (RP10M) dataset and the Gold standard Related Phrases 10K (Gold RP10K) dataset are reported."}, {"heading": "5.3 Results on the Gold RP10K and RP10M datasets", "text": "We evaluate and compare our Model A, B, C, their variants and several strong baselines on our RP10M and Gold RP10K datasets. The results are shown in Table 3. \u201cPure Text RNN\u201d denotes the baseline model without input of the visual features trained on Pinterest40M. It have the same model structure as our Model A except that we initialize the hidden state of GRU with a zero vector.\n\u201cModel A without weight sharing\u201d denotes a variant of Model A where the weight matrix Uw of the word embedding layer is not shared with the weight matrix UM of the sampled SoftMax layer (see Figure 4 for details). 2 \u201cWord2Vec-GoogleNews\u201d denotes the state-of-the-art off-the-shelf word embedding models of Word2Vec [25] trained on the Google-News data (about 300 billion words). \u201cGloVe-Twitter\u201d denotes the GloVe model [29] trained on the Twitter data (about 27 billion words). They are pure text models, but trained on a very large dataset (our model only trains on 3 billion words). Comparing these models, we can draw the following conclusions:\n\u2022 Under our evaluation criteria, visual information significantly helps the learning of word embeddings when the model successfully fuses the visual and text information together. E.g., our Model A outperforms the Word2Vec model by 9.5% and 9.2% on the Gold RP10K and RP10M datasets respectively. Model C also outperforms the pure text RNN baselines. \u2022 The weight sharing strategy is crucial to enhance the ability of Model A to fuse visual information\ninto the learned embeddings. E.g., our Model A outperforms the baseline without this sharing strategy by 7.0% and 4.4% on Gold RP10K and RP10M respectively. \u2022 Model A performs the best among all the three models. It shows that soft supervision imposed\nby the weight-sharing strategy is more effective than direct supervision. This is not surprising since not all the words are semantically related to the content of the image and a direct and hard constraint might hinder the learning of the embeddings for these words. \u2022 Model B does not perform very well. The reason might be that most of the sentences have more\nthan 8 words and the gradient from the final state loss term Lstate cannot be easily passed to the embedding of all the words in the sentence. \u2022 All the models trained on the Pinterest40M dataset performs better than the skip-gram model [25]\ntrained on a much larger dataset of 300 billion words."}, {"heading": "6 Discussion", "text": "In this paper, we investigate the task of training and evaluating word embedding models. We introduce Pinterest40M, the largest image dataset with sentence descriptions to the best of our knowledge, and construct two evaluation dataset (i.e. RP10M and Gold RP10K) for word/phrase similarity and relatedness evaluation. Based on these datasets, we propose several CNN-RNN based multimodal models to learn effective word embeddings. Experiments show that visual information significantly helps the training of word embeddings, and our proposed model successfully incorporates such information into the learned embeddings.\nThere are lots of possible extensions of the proposed model and the dataset. E.g., we plan to separate semantically similar or related phrase pairs from the Gold RP10K dataset to better understand the performance of the methods, similar to [3]. We will also give relatedness or similarity scores for the pairs (base phrase, positive phrase) to enable same evaluation strategy as previous datasets (e.g. [5, 11]). Finally, we plan to propose better models for phrase representations.\n2We also try to adopt the weight sharing strategy in Model B and C, but the performance is very similar to the non-weight sharing version.\nAcknowledgement We are grateful to James Rubinstein for setting up the crowdsourcing experiments for dataset cleanup. We thank Veronica Mapes, Pawel Garbacki, and Leon Wong for discussions and support. We appreciate the comments and suggestions from anonymous reviewers of NIPS 2016. This work is partly supported by the Center for Brains, Minds and Machines NSF STC award CCF-1231216 and the Army Research Office ARO 62250-CS."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pa\u015fca", "A. Soroa"], "venue": "NAACL HLT, pages 19\u201327", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Multimodal distributional semantics", "author": ["E. Bruni", "N.-K. Tran", "M. Baroni"], "venue": "JAIR, 49(1-47)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S.J.K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "ACL", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1990}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "WWW, pages 406\u2013414. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P. Clough", "H. M\u00fcller", "T. Deselaers"], "venue": "International Workshop OntoImage, pages 13\u201323", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["F. Hill", "A. Korhonen"], "venue": "EMNLP, pages 255\u2013265. Citeseer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 3128\u20133137", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["D. Kiela", "L. Bottou"], "venue": "EMNLP, pages 36\u201345. Citeseer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes", "author": ["S. Kottur", "R. Vedantam", "J.M. Moura", "D. Parikh"], "venue": "arXiv preprint arXiv:1511.07067", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "arXiv preprint arXiv:1501.02598", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["J. Mao", "X. Wei", "Y. Yang", "J. Wang", "Z. Huang", "A.L. Yuille"], "venue": "ICCV, pages 2533\u20132541", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The university of south florida free association", "author": ["D.L. Nelson", "C.L. McEvoy", "T.A. Schreiber"], "venue": "rhyme, and word fragment norms. Behavior Research Methods, Instruments, & Computers, 36(3):402\u2013407", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS, pages 1143\u20131151", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u201343", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "EMNLP, pages 298\u2013307", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["C. Silberer", "M. Lapata"], "venue": "ACL, pages 721\u2013732", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, 59(2):64\u201373", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, pages 3156\u20133164", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "ACL, pages 479\u2013488", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "[13]), or the visual contraints are only applied to limited number of pre-defined visual concepts (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "the image benchmark with sentences descriptions [22]), our dataset is much larger (40 million images with 300 million sentences compared to 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "[5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "[5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 8, "context": "[5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "Equipped with these datasets, we propose, train and evaluate several Recurrent Neural Network (RNN [10]) based models with input of both text descriptions and images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "[13, 21]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[13, 21]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 21, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 33, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 20, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 118, "endOffset": 122}, {"referenceID": 19, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 136, "endOffset": 140}, {"referenceID": 25, "context": "The Im2Text dataset proposed in [28] adopts a similar data collection process to ours by using 1 million images with 1 million user annotated captions from Flickr.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Recently, [34] proposed and released the YFCC100M dataset, which is a large-scale multimedia dataset contains metadata of 100 million Flickr images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 90, "endOffset": 97}, {"referenceID": 0, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 90, "endOffset": 97}, {"referenceID": 2, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "[27, 11]), or by randomly selecting frequent words in large text corpus and manually searching for useful pairs (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[27, 11]), or by randomly selecting frequent words in large text corpus and manually searching for useful pairs (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Another related evaluation is the analogy task proposed in [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 21, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 33, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 13, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 3, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 15, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 20, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 30, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 212, "endOffset": 219}, {"referenceID": 4, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 212, "endOffset": 219}, {"referenceID": 4, "context": "We adopt Gated Recurrent Units (GRUs [7]), a variation of the simple RNN model.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "the continuous bag-of-word model [4]) or to predict the context words given the current word (i.", "startOffset": 33, "endOffset": 36}, {"referenceID": 22, "context": "the skip-gram model [25]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17].", "startOffset": 165, "endOffset": 168}, {"referenceID": 28, "context": "One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 14, "context": "One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17].", "startOffset": 226, "endOffset": 230}, {"referenceID": 10, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 18, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "[21]) or perception domains (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14]) in the sentences, or focus on abstract scenes (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 47, "endOffset": 51}, {"referenceID": 9, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "This relative comparison approach was commonly used to evaluate and compare different word embedding models [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "[11, 14]) manually annotated each word pair with an absolute score reflecting how much the words in this pair are semantically related.", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[11, 14]) manually annotated each word pair with an absolute score reflecting how much the words in this pair are semantically related.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "All of the models have two parts in common: a Convolutional Neural Network (CNN [20]) to extract visual representations and a Recurrent Neural Network (RNN [10]) to model sentences.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "All of the models have two parts in common: a Convolutional Neural Network (CNN [20]) to extract visual representations and a Recurrent Neural Network (RNN [10]) to model sentences.", "startOffset": 156, "endOffset": 160}, {"referenceID": 29, "context": "For the CNN part, we resize the images to 224\u00d7 224, and adopt the 16-layer VGGNet [32] as the visual feature extractor.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "4096 binary vectors) of the layer before its SoftMax layer are used as the image features and will be mapped to the same space of the state of RNN (Model A, B) or the word embeddings (Model C), depends on the structure of the model, by a fully connected layer and a Rectified Linear Unit function (ReLU [26], ReLU(x) = max(0, x)).", "startOffset": 303, "endOffset": 307}, {"referenceID": 4, "context": "For the RNN part, we use a Gated Recurrent Unit (GRU [7]), an recently very popular RNN structure, with a 512 dimensional state cell.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "Because the vocabulary size is very huge, we adopt the sampled SoftMax loss [8] to accelerate the training.", "startOffset": 76, "endOffset": 79}, {"referenceID": 33, "context": "Model A is inspired by the CNN-RNN based image captioning models [36, 23].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "Model A is inspired by the CNN-RNN based image captioning models [36, 23].", "startOffset": 65, "endOffset": 73}, {"referenceID": 22, "context": "687 128 Word2Vec-GoogleNews [25] 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "596 300 GloVe-Twitter [29] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "sharing strategy proposed in [23] that was originally used to enhance the models\u2019 ability to learn novel visual concepts.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "2 \u201cWord2Vec-GoogleNews\u201d denotes the state-of-the-art off-the-shelf word embedding models of Word2Vec [25] trained on the Google-News data (about 300 billion words).", "startOffset": 101, "endOffset": 105}, {"referenceID": 26, "context": "\u201cGloVe-Twitter\u201d denotes the GloVe model [29] trained on the Twitter data (about 27 billion words).", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "\u2022 All the models trained on the Pinterest40M dataset performs better than the skip-gram model [25] trained on a much larger dataset of 300 billion words.", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": ", we plan to separate semantically similar or related phrase pairs from the Gold RP10K dataset to better understand the performance of the methods, similar to [3].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "[5, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[5, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 32, "context": "Figure 5: t-SNE [35] visualization of the 500 most frequent words learned by our Model A.", "startOffset": 16, "endOffset": 20}], "year": 2016, "abstractText": "In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest [2]. This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www. stat.ucla.edu/~junhua.mao/multimodal_embedding.html1.", "creator": "LaTeX with hyperref package"}}}