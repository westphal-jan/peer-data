{"id": "1611.08366", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2016", "title": "Local Discriminant Hyperalignment for Multi-Subject fMRI Data Alignment", "abstract": "multivariate pattern ( mvp ) classification can map different cognitive states to the brain tasks. one of the main challenges in mvp analysis is validating coupling the generated results across subjects. however, tightly analyzing multi - subject fmri data requires accurate functional alignments between neuronal activities of different subjects, which can rapidly increase the performance and robustness of the final results. diffuse hyperalignment ( ha ) is already one of the most effective functional alignment methods, which can be fairly mathematically formulated by the canonical correlation analysis ( cca ) methods. since ha mostly uses the unsupervised cca techniques, its analytical solution may not be optimized for mvp analysis. by incorporating the emerging idea of local discriminant analysis ( lda ) into cca, this paper proposes local discriminant hyperalignment ( ldha ) as a novel supervised ha method, which can provide better functional alignment for mvp analysis. indeed, the locality is defined based on the stimuli categories in the train - set, where the correlation percentage between all stimuli in the same candidate category will be maximized and the correlation between distinct categories of stimuli approaches to near zero. experimental studies on multi - subject mvp analysis confirm that the rigorous ldha method achieves superior performance to other state - of - the - art ha algorithms.", "histories": [["v1", "Fri, 25 Nov 2016 07:27:34 GMT  (319kb,D)", "http://arxiv.org/abs/1611.08366v1", "Published in the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)"]], "COMMENTS": "Published in the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["muhammad yousefnezhad", "daoqiang zhang"], "accepted": true, "id": "1611.08366"}, "pdf": {"name": "1611.08366.pdf", "metadata": {"source": "CRF", "title": "Local Discriminant Hyperalignment for multi-subject fMRI data alignment", "authors": ["Muhammad Yousefnezhad", "Daoqiang Zhang"], "emails": ["dqzhang}@nuaa.edu.cn"], "sections": [{"heading": "Introduction", "text": "As an imaging technology, functional Magnetic Resonance Imaging (fMRI) measures neural activity by employing the Blood-Oxygen-Level-Dependent (BOLD) contrast as a proxy for neural activation. The main idea is utilizing these measurements of neural activities to shed light on cognitive processes. Indeed, fMRI enables us to ask what information is represented in a region of the human brain and how that information is encoded, instead of asking what is a regions function (Haxby, Connolly, and Guntupalli 2014). Multivariate Pattern (MVP) classification is one of the main techniques in fMRI analysis, which can extract and decode brain patterns by applying the classification methods (Haxby et al. 2011; Chen et al. 2015; Oswal et al. 2016). In fact, it can predict patterns of neural activities associated with different cognitive states (Mohr et al. 2015; Chen et al. 2015; Figueiredo and Nowak 2016) and also can define decision surfaces to distinguish different stimuli for decoding the brain and understanding how it works (Haxby et al. 2011;\nHaxby, Connolly, and Guntupalli 2014). MVP analysis can be used to find novel treatments for mental diseases or even to create a new generation of the user interface.\nOne of the main challenges in fMRI studies, especially MVP analysis, is using multi-subject datasets. On the one hand, the multi-subject analysis is critical to figure out the generality and validity of the generated results across subjects. On the other hand, analyzing multi-subject fMRI data requires accurate functional and anatomical alignments between neuronal activities of different subjects in order to increase the performance of the final results (Haxby et al. 2011; Chen et al. 2014). Indeed, the fMRI datasets must be aligned across subjects in multi-subject studies in order to take between-subject variability into account. As mentioned before, there are two main alignment approaches, i.e. anatomical alignment and functional alignment, which can work in unison. The anatomical alignment is the most common method for aligning fMRI images based on anatomical features by employing structural MRI images, e.g. Talairach alignment (Talairach and Tournoux 1988). However, this method generated limited accuracy since the size, shape and anatomical location of functional loci differ across subjects (Watson et al. 1993; Rademacher et al. 1993). Indeed, anatomical alignment is just used in many fMRI studies as a preprocessing step. By contrast, functional alignment seeks to directly align the brain neural responses across subjects.\nHyperalignment (HA) (Haxby et al. 2011) is one of the most famous methods for functional alignment. HA can be mathematically formulated by Canonical Correlation Analysis (CCA). As a result, HA for multi-subject fMRI studies can be defined as a multiple-set CCA (Xu et al. 2012; Lorbert and Ramadge 2012; Chen et al. 2014). Since the unsupervised CCA techniques are employed for solving HA problems, the solution may not be optimized for MVP analysis. In other words, CCA just finds a set of mappings to maximize the correlation between same time-points of functional activities (in voxel-level) for all subjects, while it must maximize the correlation between homogeneous stimuli (from the same category) and also remove the correlation between different categories of stimuli. Indeed, this is a common problem in Machine Learning. For instance, Linear Discriminant Analysis (LDA) is mostly used rather than Principal Component Analysis (PCA) in the classification analysis, where LDA uses the supervision information such as class ar X iv :1\n61 1.\n08 36\n6v 1\n[ st\nat .M\nL ]\n2 5\nN ov\n2 01\nlabels or similarity between samples for improving the performance of classification methods.\nAs the main contribution of this paper, we introduce Local Discriminant Hyperalignment (LDHA) method, which incorporates the idea of Local Discriminate Analysis (LDA) into CCA (Peng, Zhang, and Zhang 2010) in order to improve the performance of the hyperalignment solution. In brief, the idea of locality is defined based on the stimuli categories (class labels) in the train-set, where the proposed method firstly generates two sets for each category of stimuli, i.e. the set of nearest homogeneous stimuli as withinclass neighborhoods and the set of stimuli from distinct categories as between-class neighborhoods. Then, these two sets are used to provide a better HA solution, where the correlation between the within-class neighborhoods is maximized, and also the correlation among between-class neighborhoods approaches to near zero.\nThe rest of this paper is organized as follows: In Section 2, this study briefly reviews some related works. Then, it introduces the proposed method in Section 3. Experimental results are reported in Section 4; and finally, this paper presents conclusion and pointed out some future works in Section 5."}, {"heading": "Related Works", "text": "There are several studies, which used functional and anatomical features for alignment. Sabuncu et al. (2010) employed cortical warping for maximizing the inter-subject correlation between functional responses across subjects. Conroy et al. (2009) also developed a method to maximize the alignment of intra-subject patterns of cortical functional connectivity by using a cortical warping.\nHyperalignment (HA) is proposed by Haxby et al. (2011), which is an \u2018anatomy free\u2019 alignment method based on functional features. HA utilized the Procrustean transformation (Scho\u0308nemann 1966) to map each the functional responses of each subject into a common high-dimensional model (template). The performance of MVP analysis by using the Hyperalignment is rapidly increased in comparison with the methods that just use the anatomical alignment (Haxby et al. 2011; Haxby, Connolly, and Guntupalli 2014).\nLorbert et al. (2012) developed Kernel Hyperalignment (KHA) to conduct nonlinear hyperalignment in an embedding space. Xu et al. (2012) introduced the regularized Hyperalignment, which makes connections to Canonical Correlation Analysis (CCA) (Gower and Dijksterhuis 2004). Dmochowski et al. (2012) applied correlated component analysis to maximize inter-subject correlation by aggregating the subjects data into an individual matrix. Sui et al. (2011; 2013) proposed a \u2018multimodal CCA + joint independent component analysis (ICA)\u2019 on multimodal data to identify the unique and shared variance associated with each imaging modality. Chen et al. (2014) examined a two-phase joint SVD-Hyperalignment algorithm, where a joint Singular Value Decomposition (SVD) is employed to provide dimensionality reduction, then HA aligns the subjects\u2019 responses in the lower dimensional feature space.\nMicheal et al. (2015) developed the GICA, IVA algorithms, which do not assume time-synchronized stimulus.\nHence, they concatenate data along the time dimension (implying spatial consistency) and learn spatial independent components. Recently, Guntupalli et al. (2016) proposed a linear model of shared representational spaces in human cortex. This model can capture fine-scale distinctions among population responses with response-tuning basis functions that are common across brains and models cortical patterns of neural responses with individual-specific topographic basis functions."}, {"heading": "The Proposed Method", "text": "This paper proposes a supervised version of hyperalignment method for applying the MVP classification. The procedure is so simple: such as all classification problems, there are two sets of data points, i.e. train-set and test-set. The trainset is used for generating the classification model. Then, the generated model is evaluated on the test-set. Since functional activities in different brains are originally unaligned in comparison with each other, the previous HA methods just used the data points in the train-set to generate a template for functional aligning in both the train and test sets before the MVP analysis. By contrast, our proposed method uses data points as well as class labels in the train-set for generating the HA template (which will be denoted by G). In train-set, the preprocessed fMRI time series collected for S subjects can be defined by X(i) = { x (i) mn } \u2208 RT\u00d7V , i = 1:S,m =\n1:T, n = 1:V , where T denotes the number of time points in unites of TRs (Time of Repetition), V is the number of voxels, and x(i)mn \u2208 R denotes the functional activity for the i\u2212 th subject in the m\u2212 th time point and the n\u2212 th voxel. In test-set, we have similar notations where the data points are defined by X\u0304(i) = { x\u0304 (i) mn } \u2208 RT\u00d7V , i = 1:S\u0304,m =\n1:T, n = 1:V . Here, S\u0304 is the number of subjects in the testset. In addition, the class labels in the train-set is denoted by Y = { ym } \u2208 NT , m = 1:T . Since there are more voxels\nthan TRs in most of the fMRI studies, X(i) and the voxel correlation map (X(i))>X(j) may not be full rank (Chen et al. 2014; Conroy et al. 2009; Lorbert and Ramadge 2012; Xu et al. 2012). In addition, time synchronized stimulus ensures temporal alignment, i.e. them\u2212th time point for all of the subjects represents the same simulation (Xu et al. 2012; Lorbert and Ramadge 2012). Indeed, the main goal of HA methods is aligning the columns of X(i) across subjects (Conroy et al. 2009; Xu et al. 2012), where the column representation of the functional activities for i\u2212 th subject and n\u2212 th voxel can be also defined as follows:\nx(i).n \u2208 RT = { x(i)mn|x(i)mn \u2208 X(i) and m = 1:T }\n(1)\nWe firstly need a metric to qualify the functional alignment. Inter-Subject Correlation (ISC) is a classical metric for functional alignment, which can be defined for two different subjects as follows (Haxby et al. 2011; Xu et al. 2012;\nChen et al. 2014):\nISC(X(i),X(j)) = (1/V )tr((X(i))>X(j)) =\n1\nV V\u2211 n=1 ( x(i).n )> x(j).n = 1 V V\u2211 m=1 V\u2211 n=1 x(i)mnx (j) mn\n(2)\nwhere tr() is the trace function. If the functional activities are column-wise standardized (X(i) \u223c N (0, 1)), the ISC lies in [\u22121,+1], where the large values represent better alignment (Conroy et al. 2009; Lorbert and Ramadge 2012; Xu et al. 2012; Chen et al. 2014). Based on (2), the hyperalignment can be formulated as follows:\n\u03c1 = arg max i,j=1:S \u2211 i<j ISC(X(i)R(i),X(j)R(j))\n= arg max i,j=1:S \u2211 i<j V\u2211 m=1 V\u2211 n=1 x(i)mnr (i) nmx (j) mnr (j) nm\n(3)\nwhere R(i) = { r (i) mn } \u2208 RV\u00d7V is the HA solution for\ni \u2212 th subject. Constrains must be imposed in R(i) to avoid overfitting (Xu et al. 2012). The general assumption in the basic hyperalignment is that the R(i), i = 1:S are noisy \u2018rotation\u2019 of a common template (Haxby et al. 2011; Guntupalli et al. 2016; Xu et al. 2012). This assumption leads us to define following problem:\n\u03c1 = arg min i,j=1:S \u2211 i<j \u2016X(i)R(i) \u2212X(j)R(j)\u20162F\nsubject to (R(`))>A(`)R(`) = I, ` = 1:S (4)\nwhere I denotes the identity matrix and the matrices A(`) \u2208 RV\u00d7V , ` = 1:S are symmetric and positive definite. Generally, if A(`) = I, then we have hyperalignment or a multiset orthogonal Procrustes problem, which is commonly used in share analysis. In addition, if A(`) = (X(`))>X(`), then (4) denotes a form of multi-set Canonical Correlation Analysis (CCA) (Lorbert and Ramadge 2012; Xu et al. 2012; Lorbert and Ramadge 2012; Chen et al. 2014). Lemma 1. The equation (4) is equivalent to:\n\u03c1 = arg min S\u2211 i=1 \u2016X(i)R(i) \u2212G\u20162F\nsubject to (R(`))>A(`)R(`) = I, ` = 1:S\n(5)\nwhere G \u2208 RT\u00d7V is the HA template:\nG = 1\nS S\u2211 j=1 X(j)R(j) (6)\nProof. Please refer to (Gower and Dijksterhuis 2004; Lorbert and Ramadge 2012) for the proof.\nIndeed, the HA template (G) can be used for functional alignment in the test-set before MVP analysis. Most of previous studies have used CCA for finding this template (Xu et al. 2012; Haxby et al. 2011; Chen et al. 2014).\nLemma 2. Canonical Correlation Analysis (CCA) finds an optimum solution for solving (4) by exploiting the objective function max\ni,j=1:S\n( (R(i))>C(i,j)R(j) ) , and then G also can\nbe calculated based on (6). Briefly, the CCA solution can be formulated as follows:\n\u03c1 = arg max i,j=1:S\n( (R(i))>C(i,j)R(j)\u221a\n((R(i))>C(i)R(i))((R(j))>C(j)R(j)) ) (7)\nwhere C(i) \u2208 RV\u00d7V = E [ (X(i))>X(i) ] = (X(i))>X(i),\nC(j) \u2208 RV\u00d7V = E [ (X(j))>X(j) ] = (X(j))>X(j), and\nC(i,j) \u2208 RV\u00d7V = E [ (X(i))>X(j) ] = (X(i))>X(j). The\nsolution of CCA can be obtained by computing a generalized eigenvalue decomposition problem (Hardoon, Szedmak, and Shawe Taylor 2004; Peng, Zhang, and Zhang 2010). Proof. Equation (4) can be written as follows:\n\u2016X(i)R(i) \u2212X(j)R(j)\u20162F = \u22122(R(i))>(X(i))>X(j)R(j)\n+(R(i))>(X(i))>X(i)R(i) + (R(j))>(X(j))>X(j)R(j) \u2261 (R(i))>(X(i))>X(j)R(j)\u221a\n((R(i))>(X(i))>X(i)R(i))((R(j))>(X(j))>X(j)R(j)) (8)\nRemark 1. The HA solution generated by unsupervised CCA may not be optimum for MVP analysis. We just explain two issues in the unsupervised solutions. Consider fMRI time series included visual stimuli, where two subjects watch two photos of cats as well as two photos of human faces. In this example, the sequence of stimuli is demonstrated by [cat1, face1, cat2, face2] for each subject after preprocessing steps. The unsupervised solution finds two mappings to maximize the correlation in the voxel-level, where the voxels for each subject are only compared with the voxels for other subjects with the same locations. As a result, the sequence of the mentioned comparison in the stimulus-level is shown by:\n(S1:cat1 \u2191 S2:cat1) ; (S1:face1 \u2191 S2:face1); (S1:cat2 \u2191 S2:cat2) ; (S1:face2 \u2191 S2:face2)\nwhere \u2191 denotes the operator for maximizing correlation and the S1 and S2 are the indices of subjects 1 and 2, respectively. Now, we can explain two issues. Indeed, the CCA solution here just maximized the correlation for the stimuli in the same locations, while they must also maximize the correlation between all stimuli in the same category and minimize the correlation between different categories of stimuli. Our approach for solving mentioned issues can be illustrated by:\n(S1:cat1,2 \u2191 S2:cat1,2); (S1:face1,2 \u2191 S2 : face1,2); (S1:cat1,2 \u2193 S2:face1,2); (S1:face1,2 \u2193 S2:cat1,2)\nwhere \u2193 denotes the operator for minimizing correlation. This paper proposes Local Discriminant Hyperalignment (LDHA), which combines the idea of locality into CCA (Peng, Zhang, and Zhang 2010) in order to provide a better\nHA solution in the MVP analysis. Since unaligned (before applying the HA method) functional activities in different subjects cannot be directly compared with each other, the neighborhoods matrix \u03b1 = { \u03b1mn } \u2208 RT\u00d7T is defined by\nusing class labels (Y) in the train-set as follows:\n\u03b1nm = \u03b1mn = { 0 ym 6= yn 1 ym = yn , m, n = 1:T , m < n\n(9) where the number of within-class neighborhoods is the permutation of all stimuli in each category, and the number of between-classes neighborhoods denotes by the permutation of all stimuli in distinct categories. The covariance matrices for within-class W(i,j) = { w (i,j) mn } \u2208 RV\u00d7V and between-\nclasses B(i,j) = { b (i,j) mn } \u2208 RV\u00d7V are defined as follows:\nw(i,j)mn = T\u2211 `=1 T\u2211 k=1 \u03b1`kx (i) `mx (j) kn + \u03b1`kx (i) `nx (j) km (10)\nb(i,j)mn = T\u2211 `=1 T\u2211 k=1 (1\u2212\u03b1`k)x(i)`mx (j) kn+(1\u2212\u03b1`k)x (i) `nx (j) km (11)\nwhere m,n = 1:V . The Local Discriminant Hyperalignment (LDHA) objective function is defined by max i,j=1:S ( (R(i))>C\u0303(i,j)R(j) ) where C\u0303(i,j) = W(i,j) \u2212 (\u03b7/T 2)B(i,j). Here, \u03b7 is the number of non-zero cells in the matrix \u03b1, and T is the number of time points in unites of TRs. In addition, the solution of the LDHA can be reformulated as follows:\n\u03c1 = arg max i,j=1:S,i<j (R(i))>C\u0303(i,j)R(j)\u221a ((R(i))>C(i)R(i))((R(j))>C(j)R(j))\nsubject to (R(`))>C(`)R(`) = I, ` = 1:S (12)\nThe main difference between LHDA and HA is the supervised covariance matrix (C\u0303(i,j)). Indeed, LDHA can be considered equivalent of the classical CCA (Lemma 2), where the correlations of non-homogeneous stimuli (B(i,j)) are participated to the CCA problem with a negative sign and all of the homogeneous stimuli in each category will be compared (W(i,j)) with each other. In addition, LDHA is related to the LDCCA method (Peng, Zhang, and Zhang 2010), where we have a mechanism in the LDCCA to manually select the relevant neighborhoods for each category of stimuli based on class-labels and also the balance factor is dynamically assigned based on the data structure.\nLemma 3. Same as the classical CCA, LDHA can be solved as a generalized eigenvalue decomposition problem. Proof.\nC\u0303(i,j) ( C(j) )\u22121 C\u0303(j,i)R(i) = ( \u039b(i,j) )2 C(i)R(i)\nC\u0303(j,i) ( C(i) )\u22121 C\u0303(i,j)R(j) = ( \u039b(i,j) )2 C(j)R(j) (13)\nAlgorithm 1 Local Discriminate Hyperalignment (LDHA) Input: Data points X(i) and X(j), class labels Y: Output: Hyperalignment parameters R(i) and R(j): Method:\n1. Generate \u03b1 by (9). 2. Calculate W(i,j), B(i,j) by using (10) and (11). 3. Calculate C\u0303(i,j).\n4. Compute H(i,j) = ( C(i) ) \u2212 1/2 C\u0303(i,j) ( C(j) ) \u2212 1/2 .\n5. Perform SVD: H(i,j) = P(i,j)\u039b(i,j) ( Q(i,j) )> .\n6. Return R(i) = ( C(i) ) \u2212 1/2 P(i,j)\nand R(j) = ( C(j) ) \u2212 1/2 Q(i,j).\nThis paper uses the Singular Value Decomposition (SVD) to solve LDHA problem, where H(i,j) =( C(i) ) \u2212 1/2 C\u0303(i,j) ( C(j) ) \u2212 1/2 , P(i,j) = ( C(i) )1/2 R(i),\nand Q(i,j) = ( C(j) )1/2 R(j). By considering SVD parameters, the (13) is equivalent to:H (i,j) ( H(i,j) )> P(i,j) = ( \u039b(i,j) )2 P(i,j)( H(i,j) )> H(i,j)Q(i,j) = ( \u039b(i,j) )2 Q(i,j) (14) where H(i,j) = P(i,j)\u039b(i,j) ( Q(i,j) )> . The Hyperalignment solution also can be defined as follows:R (i) = ( C(i) ) \u2212 1/2 P(i,j) R(j) = ( C(j) ) \u2212 1/2 Q(i,j) (15)\nAlgorithm 1 illustrates the LDHA procedure for solving the HA problem between two different subjects. As mentioned before, the LDHA is used for MVP analysis. Algorithm 2 demonstrates a general template for MVP analysis based on LDHA method. As this algorithm depicted, the procedure of generating the HA template (G) in the train stage is changed, while the template is used in the test stage such as the unsupervised HA methods. Therefore, we do not need the class labels in the test stage. Indeed, the proposed method in comparison with the unsupervised solutions just generates more optimum HA template for aligning functional neural activities, where this template can maximize the correlation between all stimuli in the same category and minimize the correlation between different categories of stimuli."}, {"heading": "Experiments", "text": "The empirical studies are presented in this section. Same as previous studies (Chen et al. 2014; Xu et al. 2012; Lorbert and Ramadge 2012; Haxby et al. 2011), this paper generates the classification model by using the \u03bd-SVM algorithms (Smola and Scho\u0308lkopf 2004), i.e. the binary \u03bd-SVM\nAlgorithm 2 A general template for MVP analysis by using Local Discriminate Hyperalignment (LDHA)\nInput: Train Set X(i), i = 1:S, Test Set X\u0302(j), j = 1:S\u0302: Output: Classification Performance (ACC, AUC): Method:\n01. Initiate R(i), i = 1:S. 02. Do 03. Foreach subject X(i), i = 1:S: 04. Update R(i) by Alg. 1 and X(`), ` = i+1:S. 05. End Foreach 06. Until X(i)R(i), i = 1:S do not change in this step. 07. Train a classifier by X(i)R(i), i = 1:S 08. Initiate R\u0302(j), j = 1:S\u0302. 09. Generate G based on (6) by using R(i), i = 1:S 10. Foreach subject X\u0302(j), j = 1:S\u0302: 11. Compute R\u0302(j) by classical HA (Eq. 5,7) and G. 12. End Foreach 13. Evaluate the classifier by using X\u0302(j)R\u0302(j), j = 1:S\u0302.\nfor datasets with just two categories of stimuli, and multilabel \u03bd-SVM (Smola and Scho\u0308lkopf 2004; Lorbert and Ramadge 2012) as multi-class approach. All employed datasets in this paper are separately preprocessed by SPM 12 (6685) (www.fil.ion.ucl.ac.uk/spm/), i.e. slice timing, anatomical alignment, normalization, smoothing. Regions of Interests (ROIs) are also defined by using the main references of each dataset. The features (voxels in the ROIs) are partitioned to train set and test set by using Leave-One-Out (LOO) crossvalidation across subjects (leave-one-subject-out). The HA methods are applied for functional aligning the neural activities and generating the general template (G). Finally, the classification model is generated for evaluating the performance of different methods. Performance of LDHA is compared with the \u03bd-SVM algorithm as the baseline (it just uses anatomical alignment without the hyperalignment mapping), the standard hyperalignment (HA) (Haxby et al. 2011; Guntupalli et al. 2016), Kernel-based hyperalignment (KHA) (Lorbert and Ramadge 2012), Regularized hyperalignment (SCCA) (Xu et al. 2012), and Joint SVD hyperalignment (SVD-HA) (Chen et al. 2014). Further, KHA algorithm is employed by the Gaussian kernel, which generated the best results in the original paper (Lorbert and Ra-\nmadge 2012). In addition, regularized parameters (\u03b1, \u03b2) in SCCA are considered optimum based on (Xu et al. 2012). All algorithms are implemented in the MATLAB R2016b (9.1) on a PC with certain specifications1 by authors in order to generate experimental results."}, {"heading": "Simple Tasks Analysis", "text": "This paper utilizes 4 datasets, shared by openfmri.org, for running empirical studies of this section. These datasets contain simple tasks such as watching a gray-scale photo or tapping a key, etc. As the first dataset, \u2018Visual Object Recognition\u2019 (DS105) includes 6 subjects and 71 sessions. It also contains 8 classes (categories) of visual stimuli, i.e. gray-scale images of faces, houses, cats, bottles, scissors, shoes, chairs, and scrambles (nonsense patterns). Please see (Haxby et al. 2011; Haxby, Connolly, and Guntupalli 2014) for more information. As the second dataset, \u2018Multisubject, multi-modal human neuroimaging dataset\u2019 (DS117) includes MEG and fMRI images for 19 subjects and 171 sessions. This paper just uses the fMRI images of this dataset. It also contains 2 classes of visual stimuli, i.e. human faces, and scrambles. Please see (Wakeman and Henson 2015) for more information. The responses of voxels in the ventral temporal cortex (VT) are analyzed for these two datasets. As the third dataset, \u2018Word and Object Processing\u2019 (DS107) includes 49 subjects and 98 sessions. It contains 4 classes of visual stimuli, i.e. words, objects, scrambles, consonants. ROIs and technical information are defined based on (Duncan et al. 2009). As the last dataset, \u2018Mixed-gambles task\u2019 (DS005) includes 16 subjects and 48 sessions. It also contains 2 classes of risk tasks in the human brain, where the chance of selection is 50/50. Further, the ROIs for functional alignment are selected based on the original paper (Tom et al. 2007).\nTable 1 and 2 respectively demonstrate the classification Accuracy and Area Under the ROC Curve (AUC) in percentage (%) for the predictors. These tables report the performance of predictors based on the categories of the stimuli. As these tables demonstrate, the proposed algorithm has generated better performance in comparison with other methods because it provided a better functional alignment of\n1DEL , CPU = Intel Xeon E5-2630 v3 (8\u00d72.4 GHz), RAM = 64GB, OS = Elementary OS 0.4 Loki\nneural activities by exploiting the locality properties of the fMRI datasets. This issue is more significant when the number of classes is more than 2 such as datasets DS105 and DS107."}, {"heading": "Complex Tasks Analysis", "text": "This section employs two fMRI datasets, which are related to watching movies. As the first dataset, \u2018A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie\u2019 (DS113) includes the fMRI data of 20 subjects, who watched \u2018Forrest Gump (1994)\u2019 movie during the experiment. This dataset provided by www.openfmri.org. Please see (Hanke et al. 2014) for more information. In the second dataset, subjects watched \u2018Raiders of the Lost Ark (1981)\u2019, where whole brain volumes are 48 and the number of subjects are 10. Please see (Chen et al. 2014; Chen et al. 2015; Lorbert and Ramadge 2012; Sabuncu et al. 2010) for more information. The responses of voxels in the ventral temporal cortex (VT) are collected to align the data while subjects watched the movie. Figure 1 illustrates the generated results for these two datasets. As\ndepicted in this figure, we rank order the voxels by employing the voxel selection method in (Haxby et al. 2011; Chen et al. 2014); and the experiments are repeated by using the different number of ranked voxels, i.e. 100, 200, 400, 600, 800, 1000, and 1200. In addition, the empirical studies are reported by using the first 100 TRs, 400 TRs, 800 TRs, and 2000 TRs in both datasets. Figure 1 illustrates that the LDHA achieves superior performance to other HA algorithms. As mentioned before, our method can generate a better functional alignment of neural activities when the concept of the locality used for functional alignments. This improvement is more significant when the number of TRs in figure 1 is limited."}, {"heading": "Conclusion", "text": "One of the main challenges in fMRI studies, especially MVP analysis, is using multi-subject datasets. On the one hand, the multi-subject analysis is necessary to estimate the validity of the generated results across subjects. On the other hand, analyzing multi-subject fMRI data requires accurate functional alignment between neuronal activities of different subjects for improving the performance of the final results. Hyperalignment (HA) is one of the most effective functional alignment methods, which can be formulated as a CCA problem for aligning neural activities of different subjects to a common space. The HA solution in MVP analysis may not be optimum because it mostly utilizes the unsupervised CCA techniques for functional alignment. This paper proposes the Local Discriminant Hyperalignment (LDHA) as a novel supervised HA solution, which employs the concept of locality in machine learning for improving the performances of both functional alignment and MVP analysis. Indeed, this paper defines the locality based on the stimuli categories (class labels) in the train-set. In a nutshell, the proposed method firstly generates two sets for each category of stimuli, i.e. the set of homogeneous stimuli as withinclass neighborhoods and the set of stimuli from distinct categories as between-class neighborhoods. Then, these two sets are used to provide a better HA solution, where the correlation between the homogeneous stimuli is maximized, and also the correlation between different categories of stimuli is near to zero. Experimental studies on multi-subject MVP analysis demonstrate that the LDHA method achieves superior performance to other state-of-the-art HA algorithms. In the future, we will plan to develop a kernel-based version of LDHA for improving its performance in non-linear problems."}, {"heading": "Acknowledgment", "text": "We thank the anonymous reviewers for comments. This work was supported in part by the National Natural Science Foundation of China (61422204 and 61473149), Jiangsu Natural Science Foundation for Distinguished Young Scholar (BK20130034) and NUAA Fundamental Research Funds (NE2013105)."}], "references": [{"title": "P", "author": ["P.H. Chen", "J.S. Guntupalli", "J.V. Haxby", "Ramadge"], "venue": "J.", "citeRegEx": "Chen et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["P.H.C. Chen", "J. Chen", "Y. Yeshurun", "U. Hasson", "J. Haxby", "Ramadge"], "venue": "J.", "citeRegEx": "Chen et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["B. Conroy", "B. Singer", "J. Haxby", "Ramadge"], "venue": "J.", "citeRegEx": "Conroy et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "L", "author": ["J.P. Dmochowski", "P. Sajda", "J. Dias", "Parra"], "venue": "C.", "citeRegEx": "Dmochowski et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["K.J. Duncan", "C. Pattamadilok", "I. Knierim", "Devlin"], "venue": "T.", "citeRegEx": "Duncan et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "R", "author": ["M.A. Figueiredo", "Nowak"], "venue": "D.", "citeRegEx": "Figueiredo and Nowak 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["J.C. Gower", "Dijksterhuis"], "venue": "B.", "citeRegEx": "Gower and Dijksterhuis 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "J", "author": ["J.S. Guntupalli", "M. Hanke", "Y.O. Halchenko", "A.C. Connolly", "P.J. Ramadge", "Haxby"], "venue": "V.", "citeRegEx": "Guntupalli et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "F", "author": ["M. Hanke", "F.J. Baumgartner", "P. Ibe", "Kaule"], "venue": "R.; Pollmann, S.; Speck, O.; Zinke, W.; and Stadler, J.", "citeRegEx": "Hanke et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["Hardoon"], "venue": "R.; Szedmak, S.; and Shawe Taylor, J.", "citeRegEx": "Hardoon. Szedmak. and Shawe Taylor 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "P", "author": ["J.V. Haxby", "J.S. Guntupalli", "A.C. Connolly", "Y.O. Halchenko", "B.R. Conroy", "M.I. Gobbini", "M. Hanke", "Ramadge"], "venue": "J.", "citeRegEx": "Haxby et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "J", "author": ["J.V. Haxby", "A.C. Connolly", "Guntupalli"], "venue": "S.", "citeRegEx": "Haxby. Connolly. and Guntupalli 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["A. Lorbert", "Ramadge"], "venue": "J.", "citeRegEx": "Lorbert and Ramadge 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "V", "author": ["A.M. Michael", "M. Anderson", "R.L. Miller", "T. Adal\u0131", "Calhoun"], "venue": "D.", "citeRegEx": "Michael et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse regularization techniques provide novel insights into outcome integration processes", "author": ["Mohr"], "venue": null, "citeRegEx": "Mohr,? \\Q2015\\E", "shortCiteRegEx": "Mohr", "year": 2015}, {"title": "Representational similarity learning with application to brain networks", "author": ["Oswal"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Oswal,? \\Q2016\\E", "shortCiteRegEx": "Oswal", "year": 2016}, {"title": "A new canonical correlation analysis algorithm with local discrimination", "author": ["Zhang Peng", "Y. Zhang 2010] Peng", "D. Zhang", "J. Zhang"], "venue": "Neural Processing Letters 31(1):1\u201315", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "V", "author": ["Rademacher, J.", "Caviness"], "venue": "S.; Steinmetz, H.; and Galaburda, A.", "citeRegEx": "Rademacher et al. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "J", "author": ["M.R. Sabuncu", "B.D. Singer", "B. Conroy", "R.E. Bryan", "P.J. Ramadge", "Haxby"], "venue": "V.", "citeRegEx": "Sabuncu et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "P", "author": ["Sch\u00f6nemann"], "venue": "H.", "citeRegEx": "Sch\u00f6nemann 1966", "shortCiteRegEx": null, "year": 1966}, {"title": "and Sch\u00f6lkopf", "author": ["A.J. Smola"], "venue": "B.", "citeRegEx": "Smola and Sch\u00f6lkopf 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "V", "author": ["J. Sui", "G. Pearlson", "A. Caprihan", "T. Adali", "K.A. Kiehl", "J. Liu", "J. Yamamoto", "Calhoun"], "venue": "D.", "citeRegEx": "Sui et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["J. Sui", "H. He", "G.D. Pearlson", "T. Adali", "K.A. Kiehl", "Q. Yu", "V.P. Clark", "E. Castro", "T. White", "Mueller"], "venue": "A.; et al.", "citeRegEx": "Sui et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Tournoux", "author": ["J. Talairach"], "venue": "P.", "citeRegEx": "Talairach and Tournoux 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "R", "author": ["S.M. Tom", "C.R. Fox", "C. Trepel", "Poldrack"], "venue": "A.", "citeRegEx": "Tom et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "R", "author": ["D.G. Wakeman", "Henson"], "venue": "N.", "citeRegEx": "Wakeman and Henson 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["J.D. Watson", "R. Myers", "R.S.J. Frackowiak", "J.V. Hajnal", "R.P. Woods", "Mazziotta"], "venue": "C.; Shipp, S.; and Zeki, S.", "citeRegEx": "Watson et al. 1993", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [], "year": 2016, "abstractText": "Multivariate Pattern (MVP) classification can map different cognitive states to the brain tasks. One of the main challenges in MVP analysis is validating the generated results across subjects. However, analyzing multi-subject fMRI data requires accurate functional alignments between neuronal activities of different subjects, which can rapidly increase the performance and robustness of the final results. Hyperalignment (HA) is one of the most effective functional alignment methods, which can be mathematically formulated by the Canonical Correlation Analysis (CCA) methods. Since HA mostly uses the unsupervised CCA techniques, its solution may not be optimized for MVP analysis. By incorporating the idea of Local Discriminant Analysis (LDA) into CCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel supervised HA method, which can provide better functional alignment for MVP analysis. Indeed, the locality is defined based on the stimuli categories in the train-set, where the correlation between all stimuli in the same category will be maximized and the correlation between distinct categories of stimuli approaches to near zero. Experimental studies on multi-subject MVP analysis confirm that the LDHA method achieves superior performance to other state-of-the-art HA algorithms.", "creator": "LaTeX with hyperref package"}}}