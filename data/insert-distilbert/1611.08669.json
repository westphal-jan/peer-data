{"id": "1611.08669", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Visual Dialog", "abstract": "visually we introduce the task of visual dialog, which requires an ai agent to hold a meaningful dialog with humans in natural, conversational modeling language reading about visual content. specifically, given an image, considering a dialog history, map and a question about the correct image, the agent has to ground forward the question in image, infer context from history, and answer the question accurately. visual dialog is disentangled enough from a specific downstream task so profound as to serve as a general test of machine intelligence, while never being grounded in overall vision enough to allow objective evaluation of individual responses and benchmark progress. we develop a novel two - person chat data - challenge collection protocol to curate a large - scale visual dialog dataset ( visdial ). data collection tool is underway and on completion, visdial will contain 1 dialog with total 10 question - answer pairs mounted on them all ~ 200k images archived from coco, compare with a total of 2m dialog question - questions answer pairs.", "histories": [["v1", "Sat, 26 Nov 2016 06:39:28 GMT  (6982kb,D)", "https://arxiv.org/abs/1611.08669v1", "22 pages, 16 figures"], ["v2", "Mon, 5 Dec 2016 02:00:49 GMT  (6982kb,D)", "http://arxiv.org/abs/1611.08669v2", "22 pages, 17 figures, Webpage:this http URL"], ["v3", "Fri, 21 Apr 2017 16:29:55 GMT  (9069kb,D)", "http://arxiv.org/abs/1611.08669v3", "22 pages, 17 figures, Webpage:this http URL"], ["v4", "Mon, 24 Apr 2017 02:10:49 GMT  (9069kb,D)", "http://arxiv.org/abs/1611.08669v4", "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9 dataset, Webpage:this http URL"], ["v5", "Tue, 1 Aug 2017 22:04:37 GMT  (9068kb,D)", "http://arxiv.org/abs/1611.08669v5", "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9 dataset, Webpage:this http URL"]], "COMMENTS": "22 pages, 16 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["abhishek das", "satwik kottur", "khushi gupta", "avi singh", "deshraj yadav", "jos\\'e m f moura", "devi parikh", "dhruv batra"], "accepted": false, "id": "1611.08669"}, "pdf": {"name": "1611.08669.pdf", "metadata": {"source": "CRF", "title": "Visual Dialog", "authors": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra"], "emails": ["dbatra}@gatech.edu", "moura}@andrew.cmu.edu", "avisingh@cs.berkeley.edu", "deshraj@vt.edu"], "sections": [{"heading": null, "text": "We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders \u2013 Late Fusion, Hierarchical Recurrent Encoder and Memory Network \u2013 and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrievalbased evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first \u2018visual chatbot\u2019! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org."}, {"heading": "1. Introduction", "text": "We are witnessing unprecedented advances in computer vision (CV) and artificial intelligence (AI) \u2013 from \u2018low-level\u2019 AI tasks such as image classification [20], scene recogni-\n*Work done while KG and AS were interns at Virginia Tech.\ntion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]! What lies next for AI? We believe that the next generation of visual intelligence systems will need to posses the ability to hold a meaningful dialog with humans in natural language about visual content. Applications include: \u2022 Aiding visually impaired users in understanding their sur-\nroundings [7] or social media content [66] (AI: \u2018John just uploaded a picture from his vacation in Hawaii\u2019, Human: \u2018Great, is he at the beach?\u2019, AI: \u2018No, on a mountain\u2019). \u2022 Aiding analysts in making decisions based on large quantities of surveillance data (Human: \u2018Did anyone enter this room last week?\u2019, AI: \u2018Yes, 27 instances logged on camera\u2019, Human: \u2018Were any of them carrying a black bag?\u2019),\nar X\niv :1\n61 1.\n08 66\n9v 5\n[ cs\n.C V\n] 1\nA ug\n\u2022 Interacting with an AI assistant (Human: \u2018Alexa \u2013 can you see the baby in the baby monitor?\u2019, AI: \u2018Yes, I can\u2019, Human: \u2018Is he sleeping or playing?\u2019). \u2022 Robotics applications (e.g. search and rescue missions) where the operator may be \u2018situationally blind\u2019 and operating via language [40] (Human: \u2018Is there smoke in any room around you?\u2019, AI: \u2018Yes, in one room\u2019, Human: \u2018Go there and look for people\u2019).\nDespite rapid progress at the intersection of vision and language \u2013 in particular, in image captioning and visual question answering (VQA) \u2013 it is clear that we are far from this grand goal of an AI agent that can \u2018see\u2019 and \u2018communicate\u2019. In captioning, the human-machine interaction consists of the machine simply talking at the human (\u2018Two people are in a wheelchair and one is holding a racket\u2019), with no dialog or input from the human. While VQA takes a significant step towards human-machine interaction, it still represents only a single round of a dialog \u2013 unlike in human conversations, there is no scope for follow-up questions, no memory in the system of previous questions asked by the user nor consistency with respect to previous answers provided by the system (Q: \u2018How many people on wheelchairs?\u2019, A: \u2018Two\u2019; Q: \u2018How many wheelchairs?\u2019, A: \u2018One\u2019). As a step towards conversational visual AI, we introduce a novel task \u2013 Visual Dialog \u2013 along with a large-scale dataset, an evaluation protocol, and novel deep models. Task Definition. The concrete task in Visual Dialog is the following \u2013 given an image I , a history of a dialog consisting of a sequence of question-answer pairs (Q1: \u2018How many people are in wheelchairs?\u2019, A1: \u2018Two\u2019, Q2: \u2018What are their genders?\u2019, A2: \u2018One male and one female\u2019), and a natural language follow-up question (Q3: \u2018Which one is holding a racket?\u2019), the task for the machine is to answer the question in free-form natural language (A3: \u2018The woman\u2019). This task is the visual analogue of the Turing Test. Consider the Visual Dialog examples in Fig. 2. The question \u2018What is the gender of the one in the white shirt?\u2019 requires the machine to selectively focus and direct atten-\ntion to a relevant region. \u2018What is she doing?\u2019 requires co-reference resolution (whom does the pronoun \u2018she\u2019 refer to?), \u2018Is that a man to her right?\u2019 further requires the machine to have visual memory (which object in the image were we talking about?). Such systems also need to be consistent with their outputs \u2013 \u2018How many people are in wheelchairs?\u2019, \u2018Two\u2019, \u2018What are their genders?\u2019, \u2018One male and one female\u2019 \u2013 note that the number of genders being specified should add up to two. Such difficulties make the problem a highly interesting and challenging one. Why do we talk to machines? Prior work in language-only (non-visual) dialog can be arranged on a spectrum with the following two end-points: goal-driven dialog (e.g. booking a flight for a user) \u2190\u2192 goal-free dialog (or casual \u2018chit-chat\u2019 with chatbots). The two ends have vastly differing purposes and conflicting evaluation criteria. Goal-driven dialog is typically evaluated on task-completion rate (how frequently was the user able to book their flight) or time to task completion [14,44] \u2013 clearly, the shorter the dialog the better. In contrast, for chit-chat, the longer the user engagement and interaction, the better. For instance, the goal of the 2017 $2.5 Million Amazon Alexa Prize is to \u201ccreate a socialbot that converses coherently and engagingly with humans on popular topics for 20 minutes.\u201d We believe our instantiation of Visual Dialog hits a sweet spot on this spectrum. It is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded enough in vision to allow objective evaluation of individual responses and benchmark progress. The former discourages taskengineered bots for \u2018slot filling\u2019 [30] and the latter discourages bots that put on a personality to avoid answering questions while keeping the user engaged [64]. Contributions. We make the following contributions: \u2022 We propose a new AI task: Visual Dialog, where a ma-\nchine must hold dialog with a human about visual content. \u2022 We develop a novel two-person chat data-collection pro-\ntocol to curate a large-scale Visual Dialog dataset (VisDial). Upon completion1, VisDial will contain 1 dialog each (with 10 question-answer pairs) on \u223c140k images from the COCO dataset [32], for a total of \u223c1.4M dialog question-answer pairs. When compared to VQA [6], VisDial studies a significantly richer task (dialog), overcomes a \u2018visual priming bias\u2019 in VQA (in VisDial, the questioner does not see the image), contains free-form longer answers, and is an order of magnitude larger.\n1VisDial data on COCO-train (\u223c83k images) and COCOval (\u223c40k images) is already available for download at https:// visualdialog.org. Since dialog history contains the ground-truth caption, we will not be collecting dialog data on COCO-test. Instead, we will collect dialog data on 20k extra images from COCO distribution (which will be provided to us by the COCO team) for our test set.\n\u2022 We introduce a family of neural encoder-decoder models for Visual Dialog with 3 novel encoders \u2013 Late Fusion: that embeds the image, history, and ques-\ntion into vector spaces separately and performs a \u2018late fusion\u2019 of these into a joint embedding. \u2013 Hierarchical Recurrent Encoder: that contains a dialoglevel Recurrent Neural Network (RNN) sitting on top of a question-answer (QA)-level recurrent block. In each QA-level recurrent block, we also include an attentionover-history mechanism to choose and attend to the round of the history relevant to the current question. \u2013 Memory Network: that treats each previous QA pair as a \u2018fact\u2019 in its memory bank and learns to \u2018poll\u2019 the stored facts and the image to develop a context vector. We train all these encoders with 2 decoders (generative and discriminative) \u2013 all settings outperform a number of sophisticated baselines, including our adaption of state-ofthe-art VQA models to VisDial. \u2022 We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a list of candidate answers and evaluated on metrics such as meanreciprocal-rank of the human response. \u2022 We conduct studies to quantify human performance. \u2022 Putting it all together, on the project page we demonstrate\nthe first visual chatbot!"}, {"heading": "2. Related Work", "text": "Vision and Language. A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69]. However, all of these involve (at most) a single-shot natural language interaction \u2013 there is no dialog. Concurrent with our work, two recent works [13, 43] have also begun studying visually-grounded dialog. Visual Turing Test. Closely related to our work is that of Geman et al. [18], who proposed a fairly restrictive \u2018Visual Turing Test\u2019 \u2013 a system that asks templated, binary questions. In comparison, 1) our dataset has free-form, openended natural language questions collected via two subjects chatting on Amazon Mechanical Turk (AMT), resulting in a more realistic and diverse dataset (see Fig. 5). 2) The dataset in [18] only contains street scenes, while our dataset has considerably more variety since it uses images from COCO [32]. Moreover, our dataset is two orders of magnitude larger \u2013 2,591 images in [18] vs \u223c140k images, 10 question-answer pairs per image, total of \u223c1.4M QA pairs. Text-based Question Answering. Our work is related to text-based question answering or \u2018reading comprehension\u2019 tasks studied in the NLP community. Some recent\nlarge-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46]. VisDial can be viewed as a fusion of reading comprehension and VQA. In VisDial, the machine must comprehend the history of the past dialog and then understand the image to answer the question. By design, the answer to any question in VisDial is not present in the past dialog \u2013 if it were, the question would not be asked. The history of the dialog contextualizes the question \u2013 the question \u2018what else is she holding?\u2019 requires a machine to comprehend the history to realize who the question is talking about and what has been excluded, and then understand the image to answer the question. Conversational Modeling and Chatbots. Visual Dialog is the visual analogue of text-based dialog and conversation modeling. While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61]. A recent large-scale conversation dataset is the Ubuntu Dialogue Corpus [35], which contains about 500K dialogs extracted from the Ubuntu channel on Internet Relay Chat (IRC). Liu et al. [33] perform a study of problems in existing evaluation protocols for free-form dialog. One important difference between free-form textual dialog and VisDial is that in VisDial, the two participants are not symmetric \u2013 one person (the \u2018questioner\u2019) asks questions about an image that they do not see; the other person (the \u2018answerer\u2019) sees the image and only answers the questions (in otherwise unconstrained text, but no counter-questions allowed). This role assignment gives a sense of purpose to the interaction (why are we talking? To help the questioner build a mental model of the image), and allows objective evaluation of individual responses."}, {"heading": "3. The Visual Dialog Dataset (VisDial)", "text": "We now describe our VisDial dataset. We begin by describing the chat interface and data-collection process on AMT, analyze the dataset, then discuss the evaluation protocol. Consistent with previous data collection efforts, we collect visual dialog data on images from the Common Objects in Context (COCO) [32] dataset, which contains multiple objects in everyday scenes. The visual complexity of these images allows for engaging and diverse conversations. Live Chat Interface. Good data for this task should include dialogs that have (1) temporal continuity, (2) grounding in the image, and (3) mimic natural \u2018conversational\u2019 exchanges. To elicit such responses, we paired 2 workers on AMT to chat with each other in real-time (Fig. 3). Each worker was assigned a specific role. One worker (the \u2018questioner\u2019) sees only a single line of text describing an im-\nage (caption from COCO); the image remains hidden to the questioner. Their task is to ask questions about this hidden image to \u2018imagine the scene better\u2019. The second worker (the \u2018answerer\u2019) sees the image and caption. Their task is to answer questions asked by their chat partner. Unlike VQA [6], answers are not restricted to be short or concise, instead workers are encouraged to reply as naturally and \u2018conversationally\u2019 as possible. Fig. 3c shows an example dialog. This process is an unconstrained \u2018live\u2019 chat, with the only exception that the questioner must wait to receive an answer before posting the next question. The workers are allowed to end the conversation after 20 messages are exchanged (10 pairs of questions and answers). Further details about our final interface can be found in the supplement. We also piloted a different setup where the questioner saw a highly blurred version of the image, instead of the caption. The conversations seeded with blurred images resulted in questions that were essentially \u2018blob recognition\u2019 \u2013 \u2018What is the pink patch at the bottom right?\u2019. For our full-scale data-collection, we decided to seed with just the captions since it resulted in more \u2018natural\u2019 questions and more closely modeled the real-world applications discussed in Section 1 where no visual signal is available to the human.\nBuilding a 2-person chat on AMT. Despite the popularity of AMT as a data collection platform in computer vision, our setup had to design for and overcome some unique challenges \u2013 the key issue being that AMT is simply not designed for multi-user Human Intelligence Tasks (HITs). Hosting a live two-person chat on AMT meant that none of the Amazon tools could be used and we developed our own backend messaging and data-storage infrastructure based on Redis messaging queues and Node.js. To support data quality, we ensured that a worker could not chat with themselves (using say, two different browser tabs) by maintaining a pool of worker IDs paired. To minimize wait time for one worker while the second was being searched for, we ensured that there was always a significant pool of available HITs. If\none of the workers abandoned a HIT (or was disconnected) midway, automatic conditions in the code kicked in asking the remaining worker to either continue asking questions or providing facts (captions) about the image (depending on their role) till 10 messages were sent by them. Workers who completed the task in this way were fully compensated, but our backend discarded this data and automatically launched a new HIT on this image so a real two-person conversation could be recorded. Our entire data-collection infrastructure (front-end UI, chat interface, backend storage and messaging system, error handling protocols) is publicly available2."}, {"heading": "4. VisDial Dataset Analysis", "text": "We now analyze the v0.9 subset of our VisDial dataset \u2013 it contains 1 dialog (10 QA pairs) on \u223c123k images from COCO-train/val, a total of 1,232,870 QA pairs."}, {"heading": "4.1. Analyzing VisDial Questions", "text": "Visual Priming Bias. One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial. Specifically, in all previous datasets, subjects saw an image while asking questions about it. As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers. This allows language-only models to perform remarkably well on VQA and results in an inflated sense of progress [19, 69]. As one particularly perverse example \u2013 for questions in the VQA dataset starting with \u2018Do you see a . . . \u2019, blindly answering \u2018yes\u2019 without reading the rest of the question or looking at the associated image results in an average VQA accuracy of 87%! In VisDial, questioners do not see the image. As a result, this bias is reduced.\n2https://github.com/batra-mlp-lab/ visdial-amt-chat\nDistributions. Fig. 4a shows the distribution of question lengths in VisDial \u2013 we see that most questions range from four to ten words. Fig. 5 shows \u2018sunbursts\u2019 visualizing the distribution of questions (based on the first four words) in VisDial vs. VQA. While there are a lot of similarities, some differences immediately jump out. There are more binary questions3 in VisDial as compared to VQA \u2013 the most frequent first question-word in VisDial is \u2018is\u2019 vs. \u2018what\u2019 in VQA. A detailed comparison of the statistics of VisDial vs. other datasets is available in Table 1 in the supplement. Finally, there is a stylistic difference in the questions that is difficult to capture with the simple statistics above. In VQA, subjects saw the image and were asked to stump a smart robot. Thus, most queries involve specific details, often about the background (\u2018What program is being utilized in the background on the computer?\u2019). In VisDial, questioners did not see the original image and were asking questions to build a mental model of the scene. Thus, the questions tend to be open-ended, and often follow a pattern: \u2022 Generally starting with the entities in the caption:\n\u2018An elephant walking away from a pool in an exhibit\u2019, \u2018Is there only 1 elephant?\u2019,\n\u2022 digging deeper into their parts or attributes:\n\u2018Is it full grown?\u2019, \u2018Is it facing the camera?\u2019,\n\u2022 asking about the scene category or the picture setting:\n\u2018Is this indoors or outdoors?\u2019, \u2018Is this a zoo?\u2019,\n\u2022 the weather:\n\u2018Is it snowing?\u2019, \u2018Is it sunny?\u2019,\n\u2022 simply exploring the scene:\n\u2018Are there people?\u2019, \u2018Is there shelter for elephant?\u2019, 3 Questions starting in \u2018Do\u2019, \u2018Did\u2019, \u2018Have\u2019, \u2018Has\u2019, \u2018Is\u2019, \u2018Are\u2019, \u2018Was\u2019,\n\u2018Were\u2019, \u2018Can\u2019, \u2018Could\u2019.\n\u2022 and asking follow-up questions about the new visual entities discovered from these explorations:\n\u2018There\u2019s a blue fence in background, like an enclosure\u2019, \u2018Is the enclosure inside or outside?\u2019."}, {"heading": "4.2. Analyzing VisDial Answers", "text": "Answer Lengths. Fig. 4a shows the distribution of answer lengths. Unlike previous datasets, answers in VisDial are longer and more descriptive \u2013 mean-length 2.9 words (VisDial) vs 1.1 (VQA), 2.0 (Visual 7W), 2.8 (Visual Madlibs). Fig. 4b shows the cumulative coverage of all answers (yaxis) by the most frequent answers (x-axis). The difference between VisDial and VQA is stark \u2013 the top-1000 answers in VQA cover \u223c83% of all answers, while in VisDial that figure is only\u223c63%. There is a significant heavy tail in VisDial \u2013 most long strings are unique, and thus the coverage curve in Fig. 4b becomes a straight line with slope 1. In total, there are 337,527 unique answers in VisDial v0.9. Answer Types. Since the answers in VisDial are longer strings, we can visualize their distribution based on the starting few words (Fig. 5c). An interesting category of answers emerges \u2013 \u2018I think so\u2019, \u2018I can\u2019t tell\u2019, or \u2018I can\u2019t see\u2019 \u2013 expressing doubt, uncertainty, or lack of information. This is a consequence of the questioner not being able to see the image \u2013 they are asking contextually relevant questions, but not all questions may be answerable with certainty from that image. We believe this is rich data for building more human-like AI that refuses to answer questions it doesn\u2019t have enough information to answer. See [48] for a related, but complementary effort on question relevance in VQA. Binary Questions vs Binary Answers. In VQA, binary questions are simply those with \u2018yes\u2019, \u2018no\u2019, \u2018maybe\u2019 as answers [6]. In VisDial, we must distinguish between binary questions and binary answers. Binary questions are those starting in \u2018Do\u2019, \u2018Did\u2019, \u2018Have\u2019, \u2018Has\u2019, \u2018Is\u2019, \u2018Are\u2019, \u2018Was\u2019, \u2018Were\u2019, \u2018Can\u2019, \u2018Could\u2019. Answers to such questions can (1) contain only \u2018yes\u2019 or \u2018no\u2019, (2) begin with \u2018yes\u2019, \u2018no\u2019, and contain additional information or clarification, (3) involve ambiguity (\u2018It\u2019s hard to see\u2019, \u2018Maybe\u2019), or (4) answer the question without explicitly saying \u2018yes\u2019 or \u2018no\u2019 (Q: \u2018Is there any type of design or pattern on the cloth?\u2019, A: \u2018There are circles and lines on the cloth\u2019). We call answers that contain \u2018yes\u2019 or \u2018no\u2019 as binary answers \u2013 149,367 and 76,346 answers in subsets (1) and (2) from above respectively. Binary answers in VQA are biased towards \u2018yes\u2019 [6, 69] \u2013 61.40% of yes/no answers are \u2018yes\u2019. In VisDial, the trend is reversed. Only 46.96% are \u2018yes\u2019 for all yes/no responses. This is understandable since workers did not see the image, and were more likely to end up with negative responses."}, {"heading": "4.3. Analyzing VisDial Dialog", "text": "In Section 4.1, we discussed a typical flow of dialog in VisDial. We analyze two quantitative statistics here.\nCoreference in dialog. Since language in VisDial is the result of a sequential conversation, it naturally contains pronouns \u2013 \u2018he\u2019, \u2018she\u2019, \u2018his\u2019, \u2018her\u2019, \u2018it\u2019, \u2018their\u2019, \u2018they\u2019, \u2018this\u2019, \u2018that\u2019, \u2018those\u2019, etc. In total, 38% of questions, 19% of answers, and nearly all (98%) dialogs contain at least one pronoun, thus confirming that a machine will need to overcome coreference ambiguities to be successful on this task. We find that pronoun usage is low in the first round (as expected) and then picks up in frequency. A fine-grained perround analysis is available in the supplement. Temporal Continuity in Dialog Topics. It is natural for conversational dialog data to have continuity in the \u2018topics\u2019 being discussed. We have already discussed qualitative differences in VisDial questions vs. VQA. In order to quantify the differences, we performed a human study where we manually annotated question \u2018topics\u2019 for 40 images (a total of 400 questions), chosen randomly from the val set. The topic annotations were based on human judgement with a consensus of 4 annotators, with topics such as: asking about a particular object (\u2018What is the man doing?\u2019) , scene (\u2018Is it outdoors or indoors?\u2019), weather (\u201cIs the weather sunny?\u2019), the image (\u2018Is it a color image?\u2019), and exploration (\u2018Is there anything else?\u201d). We performed similar topic annotation for questions from VQA for the same set of 40 images, and compared topic continuity in questions. Across 10 rounds, VisDial question have 4.55 \u00b1 0.17 topics on average, confirming that these are not independent questions. Recall that VisDial has 10 questions per image as opposed to 3 for VQA. Therefore, for a fair comparison, we compute average number of topics in VisDial over all subsets of 3 successive questions. For 500 bootstrap samples of batch size 40, VisDial has 2.14\u00b1 0.05 topics while VQA has 2.53\u00b1 0.09. Lower mean suggests there is more continuity in VisDial because questions do not change topics as often."}, {"heading": "4.4. VisDial Evaluation Protocol", "text": "One fundamental challenge in dialog systems is evaluation. Similar to the state of affairs in captioning and machine translation, it is an open problem to automatically evaluate the quality of free-form answers. Existing metrics such as BLEU, METEOR, ROUGE are known to correlate poorly with human judgement in evaluating dialog responses [33]. Instead of evaluating on a downstream task [9] or holistically evaluating the entire conversation (as in goal-free chitchat [5]), we evaluate individual responses at each round (t = 1, 2, . . . , 10) in a retrieval or multiple-choice setup. Specifically, at test time, a VisDial system is given an image I , the \u2018ground-truth\u2019 dialog history (including the image caption) C, (Q1, A1), . . . , (Qt\u22121, At\u22121), the question Qt, and a list of N = 100 candidate answers, and asked to return a sorting of the candidate answers. The model is evaluated on retrieval metrics \u2013 (1) rank of human response (lower is better), (2) recall@k, i.e. existence of the human response in top-k ranked responses, and (3) mean reciprocal rank (MRR) of the human response (higher is better). The evaluation protocol is compatible with both discriminative models (that simply score the input candidates, e.g. via a softmax over the options, and cannot generate new answers), and generative models (that generate an answer string, e.g. via Recurrent Neural Networks) by ranking the candidates by the model\u2019s log-likelihood scores. Candidate Answers. We generate a candidate set of correct and incorrect answers from four sets: Correct: The ground-truth human response to the question. Plausible: Answers to 50 most similar questions. Similar questions are those that start with similar tri-grams and mention similar semantic concepts in the rest of the question. To capture this, all questions are embedded into a vector space by concatenating the GloVe embeddings of the first three words with the averaged GloVe embeddings of the remaining words in the questions. Euclidean distances\nare used to compute neighbors. Since these neighboring questions were asked on different images, their answers serve as \u2018hard negatives\u2019. Popular: The 30 most popular answers from the dataset \u2013 e.g. \u2018yes\u2019, \u2018no\u2019, \u20182\u2019, \u20181\u2019, \u2018white\u2019, \u20183\u2019, \u2018grey\u2019, \u2018gray\u2019, \u20184\u2019, \u2018yes it is\u2019. The inclusion of popular answers forces the machine to pick between likely a priori responses and plausible responses for the question, thus increasing the task difficulty. Random: The remaining are answers to random questions in the dataset. To generate 100 candidates, we first find the union of the correct, plausible, and popular answers, and include random answers until a unique set of 100 is found."}, {"heading": "5. Neural Visual Dialog Models", "text": "In this section, we develop a number of neural Visual Dialog answerer models. Recall that the model is given as input \u2013 an image I , the \u2018ground-truth\u2019 dialog history (including the image caption) H = ( C\ufe38\ufe37\ufe37\ufe38\nH0 , (Q1, A1)\ufe38 \ufe37\ufe37 \ufe38 H1 , . . . , (Qt\u22121, At\u22121)\ufe38 \ufe37\ufe37 \ufe38 Ht\u22121 ),\nthe question Qt, and a list of 100 candidate answers At = {A(1)t , . . . , A (100) t } \u2013 and asked to return a sorting of At.\nAt a high level, all our models follow the encoder-decoder framework, i.e. factorize into two parts \u2013 (1) an encoder that converts the input (I,H,Qt) into a vector space, and (2) a decoder that converts the embedded vector into an output. We describe choices for each component next and present experiments with all encoder-decoder combinations. Decoders: We use two types of decoders: \u2022 Generative (LSTM) decoder: where the encoded vector\nis set as the initial state of the Long Short-Term Memory (LSTM) RNN language model. During training, we maximize the log-likelihood of the ground truth answer sequence given its corresponding encoded representation (trained end-to-end). To evaluate, we use the model\u2019s loglikelihood scores and rank candidate answers. Note that this decoder does not need to score options during training. As a result, such models do not exploit the biases in option creation and typically underperform models that do [25], but it is debatable whether exploiting such biases is really indicative of progress. Moreover, generative decoders are more practical in that they can actually be deployed in realistic applications. \u2022 Discriminative (softmax) decoder: computes dot product similarity between input encoding and an LSTM encoding of each of the answer options. These dot products are fed into a softmax to compute the posterior probability over options. During training, we maximize the log-likelihood of the correct option. During evaluation, options are simply ranked based on their posterior probabilities.\nEncoders: We develop 3 different encoders (listed below) that convert inputs (I,H,Qt) into a joint representation.\nIn all cases, we represent I via the `2-normalized activations from the penultimate layer of VGG-16 [56]. For each encoder E, we experiment with all possible ablated versions: E(Qt), E(Qt, I), E(Qt, H), E(Qt, I,H) (for some encoders, not all combinations are \u2018valid\u2019; details below). \u2022 Late Fusion (LF) Encoder: In this encoder, we treat H\nas a long string with the entire history (H0, . . . ,Ht\u22121) concatenated. Qt and H are separately encoded with 2 different LSTMs, and individual representations of participating inputs (I,H,Qt) are concatenated and linearly transformed to a desired size of joint representation. \u2022 Hierarchical Recurrent Encoder (HRE): In this encoder, we capture the intuition that there is a hierarchical nature to our problem \u2013 each question Qt is a sequence of words that need to be embedded, and the dialog as a whole is a sequence of question-answer pairs (Qt, At). Thus, similar to [54], as shown in Fig. 6, we propose an HRE model that contains a dialog-RNN sitting on top of a recurrent block (Rt). The recurrent block Rt embeds the question and image jointly via an LSTM (early fusion), embeds each round of the history Ht, and passes a concatenation of these to the dialog-RNN above it. The dialog-RNN produces both an encoding for this round (Et in Fig. 6) and a dialog context to pass onto the next round. We also add an attention-over-history (\u2018Attention\u2019 in Fig. 6) mechanism allowing the recurrent block Rt to choose and attend to the round of the history relevant to the current question. This attention mechanism consists of a softmax over previous rounds (0, 1, . . . , t \u2212 1) computed from the history and question+image encoding.\n\u2022 Memory Network (MN) Encoder: We develop a MN encoder that maintains each previous question and answer as a \u2018fact\u2019 in its memory bank and learns to refer to the stored facts and image to answer the question. Specifically, we encode Qt with an LSTM to get a 512-d vector, encode each previous round of history (H0, . . . ,Ht\u22121) with another LSTM to get a t \u00d7 512 matrix. We com-\npute inner product of question vector with each history vector to get scores over previous rounds, which are fed to a softmax to get attention-over-history probabilities. Convex combination of history vectors using these attention probabilities gives us the \u2018context vector\u2019, which is passed through an fc-layer and added to the question vectorto construct the MN encoding. In the language of Memory Network [9], this is a \u20181-hop\u2019 encoding.\nWe use a \u2018[encoder]-[input]-[decoder]\u2019 convention to refer to model-input combinations. For example, \u2018LF-QI-D\u2019 has a Late Fusion encoder with question+image inputs (no history), and a discriminative decoder. Implementation details about the models can be found in the supplement."}, {"heading": "6. Experiments", "text": "Splits. VisDial v0.9 contains 83k dialogs on COCO-train and 40k on COCO-val images. We split the 83k into 80k for training, 3k for validation, and use the 40k as test. Data preprocessing, hyperparameters and training details are included in the supplement. Baselines We compare to a number of baselines: Answer Prior: Answer options to a test question are encoded with an LSTM and scored by a linear classifier. This captures ranking by frequency of answers in our training set without resolving to exact string matching. NN-Q: Given a test question, we find k nearest neighbor questions (in GloVe space) from train, and score answer options by their meansimilarity with these k answers. NN-QI: First, we find K nearest neighbor questions for a test question. Then, we find a subset of size k based on image feature similarity. Finally, we rank options by their mean-similarity to answers to these k questions. We use k = 20,K = 100. Finally, we adapt several (near) state-of-art VQA models (SAN [67], HieCoAtt [37]) to Visual Dialog. Since VQA is posed as classification, we \u2018chop\u2019 the final VQA-answer softmax from these models, feed these activations to our discriminative decoder (Section 5), and train end-to-end on VisDial. Note that our LF-QI-D model is similar to that in [36]. Altogether, these form fairly sophisticated baselines. Results. Tab. 5 shows results for our models and baselines on VisDial v0.9 (evaluated on 40k from COCO-val). A few key takeaways \u2013 1) As expected, all learning based models significantly outperform non-learning baselines. 2) All discriminative models significantly outperform generative models, which as we discussed is expected since discriminative models can tune to the biases in the answer options. 3) Our best generative and discriminative models are MN-QIH-G with 0.526 MRR, and MN-QIH-D with 0.597 MRR. 4) We observe that naively incorporating history doesn\u2019t help much (LF-Q vs. LF-QH and LF-QI vs. LF-QIH) or can even hurt a little (LF-QI-G vs. LF-QIH-\nG). However, models that better encode history (MN/HRE) perform better than corresponding LF models with/without history (e.g. LF-Q-D vs. MN-QH-D). 5) Models looking at I ({LF,MN,HRE }-QIH) outperform corresponding blind models (without I). Human Studies. We conduct studies on AMT to quantitatively evaluate human performance on this task for all combinations of {with image, without image}\u00d7{with history, without history}. We find that without image, humans perform better when they have access to dialog history. As expected, this gap narrows down when they have access to the image. Complete details can be found in supplement."}, {"heading": "7. Conclusions", "text": "To summarize, we introduce a new AI task \u2013 Visual Dialog, where an AI agent must hold a dialog with a human about visual content. We develop a novel two-person chat datacollection protocol to curate a large-scale dataset (VisDial), propose retrieval-based evaluation protocol, and develop a family of encoder-decoder models for Visual Dialog. We quantify human performance on this task via human studies. Our results indicate that there is significant scope for improvement, and we believe this task can serve as a testbed for measuring progress towards visual intelligence."}, {"heading": "8. Acknowledgements", "text": "We thank Harsh Agrawal, Jiasen Lu for help with AMT data collection; Xiao Lin, Latha Pemula for model discussions; Marco Baroni, Antoine Bordes, Mike Lewis, Marc\u2019Aurelio Ranzato for helpful discussions. We are grateful to the developers of Torch [2] for building an excellent framework. This work was funded in part by NSF CAREER awards to DB and DP, ONR YIP awards to DP and DB, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, an Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and DP, Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donations to DB. SK was supported by ONR Grant N00014-12-1-0903. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."}, {"heading": "Appendix Overview", "text": "This supplementary document is organized as follows:\n\u2022 Sec. A studies how and why VisDial is more than just a collection of independent Q&As.\n\u2022 Sec. B shows qualitative examples from our dataset.\n\u2022 Sec. C presents detailed human studies along with comparisons to machine accuracy. The interface for human studies is demonstrated in a video4.\n\u2022 Sec. D shows snapshots of our two-person chat datacollection interface on Amazon Mechanical Turk. The interface is also demonstrated in the video3.\n\u2022 Sec. E presents further analysis of VisDial, such as question types, question and answer lengths per question type. A video with an interactive sunburst visualization of the dataset is included3.\n\u2022 Sec. F presents performance of our models on VisDial v0.5 test.\n\u2022 Sec. G presents implementation-level training details including data preprocessing, and model architectures.\n\u2022 Putting it all together, we compile a video demonstrating our visual chatbot3 that answers a sequence of questions from a user about an image. This demo uses one of our best generative models from the main paper, MN-QIH-G, and uses sampling (without any beam-search) for inference in the LSTM decoder. Note that these videos demonstrate an \u2018unscripted\u2019 dialog \u2013 in the sense that the particular QA sequence is not present in VisDial and the model is not provided with any list of answer options.\nA. In what ways are dialogs in VisDial more than just 10 visual Q&As?\nIn this section, we lay out an exhaustive list of differences between VisDial and image question-answering datasets, with the VQA dataset [6] serving as the representative. In essence, we characterize what makes an instance in VisDial more than a collection of 10 independent questionanswer pairs about an image \u2013 what makes it a dialog. In order to be self-contained and an exhaustive list, some parts of this section repeat content from the main document.\nA.1. VisDial has longer free-form answers\nFig. 7a shows the distribution of answer lengths in VisDial. and Tab. 2 compares statistics of VisDial with existing image question answering datasets. Unlike previous datasets,\n4https://goo.gl/yjlHxY\nanswers in VisDial are longer, conversational, and more descriptive \u2013 mean-length 2.9 words (VisDial) vs 1.1 (VQA), 2.0 (Visual 7W), 2.8 (Visual Madlibs). Moreover, 37.1% of answers in VisDial are longer than 2 words while the VQA dataset has only 3.8% answers longer than 2 words.\n(a) (b)\nFigure 7: Distribution of lengths for questions and answers (left); and percent coverage of unique answers over all answers from the train dataset (right), compared to VQA. For a given coverage, VisDial has more unique answers indicating greater answer diversity.\nFig. 7b shows the cumulative coverage of all answers (yaxis) by the most frequent answers (x-axis). The difference between VisDial and VQA is stark \u2013 the top-1000 answers in VQA cover \u223c83% of all answers, while in VisDial that figure is only \u223c63%. There is a significant heavy tail of answers in VisDial \u2013 most long strings are unique, and thus the coverage curve in Fig. 7b becomes a straight line with slope 1. In total, there are 337,527 unique answers in VisDial (out of the 1,232,870 answers currently in the dataset).\nA.2. VisDial has co-references in dialogs\nPeople conversing with each other tend to use pronouns to refer to already mentioned entities. Since language in VisDial is the result of a sequential conversation, it naturally contains pronouns \u2013 \u2018he\u2019, \u2018she\u2019, \u2018his\u2019, \u2018her\u2019, \u2018it\u2019, \u2018their\u2019, \u2018they\u2019, \u2018this\u2019, \u2018that\u2019, \u2018those\u2019, etc. In total, 38% of questions, 19% of answers, and nearly all (98%) dialogs contain at least one pronoun, thus confirming that a machine will need to overcome coreference ambiguities to be successful on this task. As a comparison, only 9% of questions and 0.25% of answers in VQA contain at least one pronoun. In Fig. 8, we see that pronoun usage is lower in the first round compared to other rounds, which is expected since there are fewer entities to refer to in the earlier rounds. The pronoun usage is also generally lower in answers than questions, which is also understandable since the answers are generally shorter than questions and thus less likely to contain pronouns. In general, the pronoun usage is fairly consistent across rounds (starting from round 2) for both questions and answers.\nA.3. VisDial has smoothness/continuity in \u2018topics\u2019\nQualitative Example of Topics. There is a stylistic difference in the questions asked in VisDial (compared to the questions in VQA) due to the nature of the task assigned to the subjects asking the questions. In VQA, subjects saw the image and were asked to \u201cstump a smart robot\u201d. Thus, most queries involve specific details, often about the background (Q: \u2018What program is being utilized in the background on the computer?\u2019). In VisDial, questioners did not see the original image and were asking questions to build a mental model of the scene. Thus, the questions tend to be openended, and often follow a pattern: \u2022 Generally starting with the entities in the caption:\n\u2018An elephant walking away from a pool in an exhibit\u2019, \u2018Is there only 1 elephant?\u2019,\n\u2022 digging deeper into their parts, attributes, or properties:\n\u2018Is it full grown?\u2019, \u2018Is it facing the camera?\u2019,\n\u2022 asking about the scene category or the picture setting:\n\u2018Is this indoors or outdoors?\u2019, \u2018Is this a zoo?\u2019,\n\u2022 the weather:\n\u2018Is it snowing?\u2019, \u2018Is it sunny?\u2019,\n\u2022 simply exploring the scene:\n\u2018Are there people?\u2019, \u2018Is there shelter for elephant?\u2019,\n\u2022 and asking follow-up questions about the new visual entities discovered from these explorations:\n\u2018There\u2019s a blue fence in background, like an enclosure\u2019, \u2018Is the enclosure inside or outside?\u2019.\nSuch a line of questioning does not exist in the VQA dataset, where the subjects were shown the questions already asked about an image, and explicitly instructed to ask about different entities [6].\nCounting the Number of Topics. In order to quantify these qualitative differences, we performed a human study where we manually annotated question \u2018topics\u2019 for 40 images (a total of 400 questions), chosen randomly from the val set. The topic annotations were based on human judgement with a consensus of 4 annotators, with topics such as: asking about a particular object (\u2018What is the man doing?\u2019), the scene (\u2018Is it outdoors or indoors?\u2019), the weather (\u201cIs the weather sunny?\u2019), the image (\u2018Is it a color image?\u2019), and exploration (\u2018Is there anything else?\u201d). We performed similar topic annotation for questions from VQA for the same set of 40 images, and compared topic continuity in questions. Across 10 rounds, VisDial questions have 4.55 \u00b1 0.17 topics on average, confirming that these are not 10 independent questions. Recall that VisDial has 10 questions per image as opposed to 3 for VQA. Therefore, for a fair comparison, we compute average number of topics in VisDial over all \u2018sliding windows\u2019 of 3 successive questions. For 500 bootstrap samples of batch size 40, VisDial has 2.14\u00b1 0.05 topics while VQA has 2.53\u00b1 0.09. Lower mean number of topics suggests there is more continuity in VisDial because questions do not change topics as often.\nTransition Probabilities over Topics. We can take this analysis a step further by computing topic transition probabilities over topics as follows. For a given sequential dialog exchange, we now count the number of topic transitions between consecutive QA pairs, normalized by the total number of possible transitions between rounds (9 for VisDial and 2 for VQA). We compute this \u2018topic transition probability\u2019 (how likely are two successive QA pairs to be about two different topics) for VisDial and VQA in two different settings \u2013 (1) in-order and (2) with a permuted sequence\nof QAs. Note that if VisDial were simply a collection of 10 independent QAs as opposed to a dialog, we would expect the topic transition probabilities to be similar for inorder and permuted variants. However, we find that for 1000 permutations of 40 topic-annotated image-dialogs, inorder-VisDial has an average topic transition probability of 0.61, while permuted-VisDial has 0.76\u00b1 0.02. In contrast, VQA has a topic transition probability of 0.80 for in-order vs. 0.83\u00b1 0.02 for permuted QAs. There are two key observations: (1) In-order transition probability is lower for VisDial than VQA (i.e. topic transition is less likely in VisDial), and (2) Permuting the order of questions results in a larger increase for VisDial, around 0.15, compared to a mere 0.03 in case of VQA (i.e. in-orderVQA and permuted-VQA behave significantly more similarly than in-order-VisDial and permuted-VisDial). Both these observations establish that there is smoothness in the temporal order of topics in VisDial, which is indicative of the narrative structure of a dialog, rather than independent question-answers.\nA.4. VisDial has the statistics of an NLP dialog dataset\nIn this analysis, our goal is to measure whether VisDial behaves like a dialog dataset. In particular, we compare VisDial, VQA, and Cornell Movie-Dialogs Corpus [11]. The Cornell Movie-Dialogs corpus is a text-only dataset extracted from pairwise interactions between characters from approximately 617 movies, and is widely used as a standard dialog corpus in the natural language processing (NLP) and dialog communities. One popular evaluation criteria used in the dialog-systems research community is the perplexity of language models trained on dialog datasets \u2013 the lower the perplexity of a model, the better it has learned the structure in the dialog dataset. For the purpose of our analysis, we pick the popular sequence-to-sequence (Seq2Seq) language model [24] and use the perplexity of this model trained on different datasets as a measure of temporal structure in a dataset. As is standard in the dialog literature, we train the Seq2Seq model to predict the probability of utterance Ut given the previous utterance Ut\u22121, i.e. P(Ut | Ut\u22121) on the Cornell corpus. For VisDial and VQA, we train the Seq2Seq model to predict the probability of a question Qt given the previous question-answer pair, i.e. P(Qt | (Qt\u22121, At\u22121)). For each dataset, we used its train and val splits for training and hyperparameter tuning respectively, and report results on test. At test time, we only use conversations of length 10 from Cornell corpus for a fair comparison to VisDial (which has 10 rounds of QA). For all three datasets, we created 100 permuted versions of\ntest, where either QA pairs or utterances are randomly shuffled to disturb their natural order. This allows us to compare datasets in their natural ordering w.r.t. permuted orderings. Our hypothesis is that since dialog datasets have linguistic structure in the sequence of QAs or utterances they contain, this structure will be significantly affected by permuting the sequence. In contrast, a collection of independent question-answers (as in VQA) will not be significantly affected by a permutation. Tab. 3 compares the original, unshuffled test with the shuffled testsets on two metrics:\nPerplexity: We compute the standard metric of perplexity per token, i.e. exponent of the normalized negative-logprobability of a sequence (where normalized is by the length of the sequence). Tab. 3 shows these perplexities for the original unshuffled test and permuted test sequences. We notice a few trends. First, we note that the absolute perplexity values are higher for the Cornell corpus than QA datasets. We hypothesize that this is due to the broad, unrestrictive dialog generation task in Cornell corpus, which is a more difficult task than question prediction about images, which is in comparison a more restricted task. Second, in all three datasets, the shuffled test has statistically significant higher perplexity than the original test, which indicates that shuffling does indeed break the linguistic structure in the sequences. Third, the absolute increase in perplexity from natural to permuted ordering is highest in the Cornell corpus (3.0) fol-\nlowed by our VisDial with 0.7, and VQA at 0.35, which is indicative of the degree of linguistic structure in the sequences in these datasets. Finally, the relative increases in perplexity are 3.64% in Cornell, 10.13% in VisDial, and 4.21% in VQA \u2013 VisDial suffers the highest relative increase in perplexity due to shuffling, indicating the existence of temporal continuity that gets disrupted.\nClassification: As our second metric to compare datasets in their natural vs. permuted order, we test whether we can reliably classify a given sequence as natural or permuted. Our classifier is a simple threshold on perplexity of a sequence. Specifically, given a pair of sequences, we compute the perplexity of both from our Seq2Seq model, and predict that the one with higher perplexity is the sequence in permuted ordering, and the sequence with lower perplexity is the one in natural ordering. The accuracy of this simple classifier indicates how easy or difficult it is to tell the difference between natural and permuted sequences. A higher classification rate indicates existence of temporal continuity in the conversation, thus making the ordering important. Tab. 3 shows the classification accuracies achieved on all datasets. We can see that the classifier on VisDial achieves the highest accuracy (73.3%), followed by Cornell (61.0%). Note that this is a binary classification task with the prior probability of each class by design being equal, thus chance performance is 50%. The classifiers on VisDial and Cornell both significantly outperforming chance. On the other hand, the classifier on VQA is near chance (52.8%), indicating a lack of general temporal continuity. To summarize this analysis, our experiments show that VisDial is significantly more dialog-like than VQA, and behaves more like a standard dialog dataset, the Cornell Movie-Dialogs corpus.\nA.5. VisDial eliminates visual priming bias in VQA\nOne key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial. Specifically, in all previous datasets, subjects saw an image while asking questions about it. As described in [69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers. This allows languageonly models to perform remarkably well on VQA and results in an inflated sense of progress [69]. As one particularly perverse example \u2013 for questions in the VQA dataset starting with \u2018Do you see a . . . \u2019, blindly answering \u2018yes\u2019 without reading the rest of the question or looking at the associated image results in an average VQA accuracy of 87%! In VisDial, questioners do not see the image. As a result,\nthis bias is reduced. This lack of visual priming bias (i.e. not being able to see the image while asking questions) and holding a dialog with another person while asking questions results in the following two unique features in VisDial.\nUncertainty in Answers in VisDial. Since the answers in VisDial are longer strings, we can visualize their distribution based on the starting few words (Fig. 9). An interesting category of answers emerges \u2013 \u2018I think so\u2019, \u2018I can\u2019t tell\u2019, or \u2018I can\u2019t see\u2019 \u2013 expressing doubt, uncertainty, or lack of information. This is a consequence of the questioner not being able to see the image \u2013 they are asking contextually relevant questions, but not all questions may be answerable with certainty from that image. We believe this is rich data for building more human-like AI that refuses to answer questions it doesn\u2019t have enough information to answer. See [48] for a related, but complementary effort on question relevance in VQA.\nBinary Questions 6= Binary Answers in VisDial. In VQA, binary questions are simply those with \u2018yes\u2019, \u2018no\u2019, \u2018maybe\u2019 as answers [6]. In VisDial, we must distinguish between binary questions and binary answers. Binary questions are those starting in \u2018Do\u2019, \u2018Did\u2019, \u2018Have\u2019, \u2018Has\u2019, \u2018Is\u2019, \u2018Are\u2019, \u2018Was\u2019, \u2018Were\u2019, \u2018Can\u2019, \u2018Could\u2019. Answers to such questions can (1) contain only \u2018yes\u2019 or \u2018no\u2019, (2) begin with \u2018yes\u2019, \u2018no\u2019, and contain additional information or clarification (Q: \u2018Are there any animals in the image?\u2019, A: \u2018yes, 2 cats and a dog\u2019), (3) involve ambiguity (\u2018It\u2019s hard to see\u2019,\n\u2018Maybe\u2019), or (4) answer the question without explicitly saying \u2018yes\u2019 or \u2018no\u2019 (Q: \u2018Is there any type of design or pattern on the cloth?\u2019, A: \u2018There are circles and lines on the cloth\u2019). We call answers that contain \u2018yes\u2019 or \u2018no\u2019 as binary answers \u2013 149,367 and 76,346 answers in subsets (1) and (2) from above respectively. Binary answers in VQA are biased towards \u2018yes\u2019 [6,69] \u2013 61.40% of yes/no answers are \u2018yes\u2019. In VisDial, the trend is reversed. Only 46.96% are \u2018yes\u2019 for all yes/no responses. This is understandable since workers did not see the image, and were more likely to end up with negative responses."}, {"heading": "B. Qualitative Examples from VisDial", "text": "Fig. 10 shows random samples of dialogs from the VisDial dataset."}, {"heading": "C. Human-Machine Comparison", "text": "Model MRR R@1 R@5 Mean\nH um\nan  Human-Q 0.441 25.10 67.37 4.19 Human-QH 0.485 30.31 70.53 3.91 Human-QI 0.619 46.12 82.54 2.92\nHuman-QIH 0.635 48.03 83.76 2.83\nM ac\nhi ne { HREA-QIH-G 0.477 31.64 61.61 4.42\nMN-QIH-G 0.481 32.16 61.94 4.47 MN-QIH-D 0.553 36.86 69.39 3.48\nTable 4: Human-machine performance comparison on VisDial v0.5, measured by mean reciprocal rank (MRR), recall@k for k = {1, 5} and mean rank. Note that higher is better for MRR and recall@k, while lower is better for mean rank.\nWe conducted studies on AMT to quantitatively evaluate human performance on this task for all combinations of {with image, without image}\u00d7{with history, without history} on 100 random images at each of the 10 rounds. Specifically, in each setting, we show human subjects a jumbled list of 10 candidate answers for a question \u2013 top-9 predicted responses from our \u2018LF-QIH-D\u2019 model and the 1 ground truth answer \u2013 and ask them to rank the responses. Each task was done by 3 human subjects. Results of this study are shown in the top-half of Tab. 4. We find that without access to the image, humans perform better when they have access to dialog history \u2013 compare the Human-QH row to Human-Q (R@1 of 30.31 vs. 25.10). As perhaps expected, this gap narrows down when humans have access to the image \u2013 compare Human-QIH to HumanQI (R@1 of 48.03 vs. 46.12). Note that these numbers are not directly comparable to machine performance reported in the main paper because models are tasked with ranking 100 responses, while humans are asked to rank 10 candidates. This is because the task of\nranking 100 candidate responses would be too cumbersome for humans. To compute comparable human and machine performance, we evaluate our best discriminative (MN-QIH-D) and generative (HREA-QIH-G, MN-QIH-G)5 models on the same 10 options that were presented to humans. Note that in this setting, both humans and machines have R@10 = 1.0, since there are only 10 options. Tab. 4 bottom-half shows the results of this comparison. We can see that, as expected, humans with full information (i.e. Human-QIH) perform the best with a large gap in human and machine performance (compare R@5: Human-QIH 83.76% vs. MN-QIH-D 69.39%). This gap is even larger when compared to generative models, which unlike the discriminative models are not actively trying to exploit the biases in the answer candidates (compare R@5: Human-QIH 83.76% vs. HREA-QIH-G 61.61%). Furthermore, we see that humans outperform the best machine even when not looking at the image, simply on the basis of the context provided by the history (compare R@5: Human-QH 70.53% vs. MN-QIH-D 69.39%). Perhaps as expected, with access to the image but not the history, humans are significantly better than the best machines (R@5: Human-QI 82.54% vs. MN-QIH-D 69.39%). With access to history humans perform even better. From in-house human studies and worker feedback on AMT, we find that dialog history plays the following roles for humans: (1) provides a context for the question and paints a picture of the scene, which helps eliminate certain answer choices (especially when the image is not available), (2) gives cues about the answerer\u2019s response style, which helps identify the right answer among similar answer choices, and (3) disambiguates amongst likely interpretations of the image (i.e., when objects are small or occluded), again, helping identify the right answer among multiple plausible options.\nD. Interface\nIn this section, we show our interface to connect two Amazon Mechanical Turk workers live, which we used to collect our data. Instructions. To ensure quality of data, we provide detailed instructions on our interface as shown in Fig. 11a. Since the workers do not know their roles before starting the study, we provide instructions for both questioner and answerer roles. After pairing: Immediately after pairing two workers, we assign them roles of a questioner and a answerer and display role-specific instructions as shown in Fig. 11b. Observe that\n5 We use both HREA-QIH-G, MN-QIH-G since they have similar accuracies.\nthe questioner does not see the image while the answerer does have access to it. Both questioner and answerer see the caption for the image."}, {"heading": "E. Additional Analysis of VisDial", "text": "In this section, we present additional analyses characterizing our VisDial dataset.\n(a) Detailed instructions for Amazon Mechanical Turkers on our interface\n(b) Left: What questioner sees; Right: What answerer sees.\nE.1. Question and Answer Lengths\nFig. 12 shows question lengths by type and round. Average length of question by type is consistent across rounds. Questions starting with \u2018any\u2019 (\u2018any people?\u2019, \u2018any other fruits?\u2019, etc.) tend to be the shortest. Fig. 13 shows answer lengths by type of question they were said in response to and round. In contrast to questions, there is significant variance in answer lengths. Answers to binary questions (\u2018Any people?\u2019, \u2018Can you see the dog?\u2019, etc.) tend to be short while answers to \u2018how\u2019 and \u2018what\u2019 questions tend to be more explanatory and long. Across question types, answers tend to be the longest in the middle of conversations.\nE.2. Question Types\nFig. 14 shows round-wise coverage by question type. We see that as conversations progress, \u2018is\u2019, \u2018what\u2019 and \u2018how\u2019 questions reduce while \u2018can\u2019, \u2018do\u2019, \u2018does\u2019, \u2018any\u2019 questions occur more often. Questions starting with \u2018Is\u2019 are the most popular in the dataset."}, {"heading": "F. Performance on VisDial v0.5", "text": "Tab. 5 shows the results for our proposed models and baselines on VisDial v0.5. A few key takeaways \u2013 First, as expected, all learning based models significantly outperform non-learning baselines. Second, all discriminative models significantly outperform generative models, which as we discussed is expected since discriminative models can tune to the biases in the answer options. This improvement comes with the significant limitation of not being able to actually generate responses, and we recommend the two decoders be viewed as separate use cases. Third, our best generative and discriminative models are MN-QIH-G with 0.44 MRR, and MN-QIH-D with 0.53 MRR that outperform a suite of models and sophisticated baselines. Fourth, we observe that models with H perform better than Q-only models, highlighting the importance of history in VisDial. Fifth, models looking at I outperform both the blind models (Q, QH) by at least 2% on recall@1 in both decoders. Finally, models that use both H and I have best performance.\nDialog-level evaluation. Using R@5 to define round-level \u2018success\u2019, our best discriminative model MN-QIH-D gets 7.01 rounds out of 10 correct, while generative MN-QIHG gets 5.37. Further, the mean first-failure-round (under R@5) for MN-QIH-D is 3.23, and 2.39 for MN-QIH-G. Fig. 16a and Fig. 16b show plots for all values of k in R@k."}, {"heading": "G. Experimental Details", "text": "In this section, we describe details about our models, data preprocessing, training procedure and hyperparameter selection.\nG.1. Models\nLate Fusion (LF) Encoder. We encode the image with a VGG-16 CNN, question and concatenated history with separate LSTMs and concatenate the three representations. This is followed by a fully-connected layer and tanh nonlinearity to a 512-d vector, which is used to decode the response. Fig. 17a shows the model architecture for our LF encoder.\nHierarchical Recurrent Encoder (HRE). In this encoder, the image representation from VGG-16 CNN is early fused with the question. Specifically, the image representation is concatenated with every question word as it is fed to an LSTM. Each QA-pair in dialog history is independently encoded by another LSTM with shared weights. The image-question representation, computed for every round from 1 through t, is concatenated with history representation from the previous round and constitutes a sequence of\nquestion-history vectors. These vectors are fed as input to a dialog-level LSTM, whose output state at t is used to decode the response to Qt. Fig. 17b shows the model architecture for our HRE.\nMemory Network. The image is encoded with a VGG16 CNN and question with an LSTM. We concatenate the representations and follow it by a fully-connected layer and tanh non-linearity to get a \u2018query vector\u2019. Each caption/QApair (or \u2018fact\u2019) in dialog history is encoded independently by an LSTM with shared weights. The query vector is then used to compute attention over the t facts by inner product. Convex combination of attended history vectors is passed through a fully-connected layer and tanh non-linearity, and added back to the query vector. This combined representation is then passed through another fully-connected layer and tanh non-linearity and then used to decode the response. The model architecture is shown in Fig. 17c. Fig. 18 shows\nsome examples of attention over history facts from our MN encoder. We see that the model learns to attend to facts relevant to the question being asked. For example, when asked \u2018What color are kites?\u2019, the model attends to \u2018A lot of people stand around flying kites in a park.\u2019 For \u2018Is anyone on bus?\u2019, it attends to \u2018A large yellow bus parked in some grass.\u2019 Note that these are selected examples, and not always are these attention weights interpretable.\nG.2. Training\nSplits. Recall that VisDial v0.9 contained 83k dialogs on COCO-train and 40k on COCO-val images. We split the 83k into 80k for training, 3k for validation, and use the 40k as test.\nPreprocessing. We spell-correct VisDial data using the Bing API [41]. Following VQA, we lowercase all questions and answers, convert digits to words, and remove contractions, before tokenizing using the Python NLTK [1]. We then construct a dictionary of words that appear at least five times in the train set, giving us a vocabulary of around 7.5k.\nHyperparameters. All our models are implemented in Torch [2]. Model hyperparameters are chosen by early stopping on val based on the Mean Reciprocal Rank (MRR) metric. All LSTMs are 2-layered with 512-dim hidden states. We learn 300-dim embeddings for words and images. These word embeddings are shared across question, history, and decoder LSTMs. We use Adam [28]\nwith a learning rate of 10\u22123 for all models. Gradients at each iterations are clamped to [\u22125, 5] to avoid explosion. Our code, architectures, and trained models are available at https://visualdialog.org."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sort story: Sorting jumbled images and captions into stories", "author": ["H. Agrawal", "A. Chandrasekaran", "D. Batra", "D. Parikh", "M. Bansal"], "venue": "EMNLP", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "VizWiz: Nearly Real-time Answers to Visual Questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "UIST", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Largescale Simple Question Answering with Memory Networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning End-to-End Goal- Oriented Dialog", "author": ["A. Bordes", "J. Weston"], "venue": "arXiv preprint arXiv:1605.07683", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Resolving language and vision ambiguities together: Joint segmentation and prepositional attachment resolution in captioned scenes", "author": ["G. Christie", "A. Laddha", "A. Agrawal", "S. Antol", "Y. Goyal", "K. Kochersberger", "D. Batra"], "venue": "EMNLP", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs", "author": ["C. Danescu-Niculescu-Mizil", "L. Lee"], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "and D", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh"], "venue": "Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In EMNLP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "GuessWhat?! Visual object discovery through multi-modal dialogue", "author": ["H. de Vries", "F. Strub", "S. Chandar", "O. Pietquin", "H. Larochelle", "A.C. Courville"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "author": ["J. Dodge", "A. Gane", "X. Zhang", "A. Bordes", "S. Chopra", "A. Miller", "A. Szlam", "J. Weston"], "venue": "ICLR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "20 Figure 18: Selected examples of attention over history facts from our Memory Network encoder", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "The intensity of color in each row indicates the strength of attention placed on that round by the model. Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering. In NIPS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A Visual Turing Test for Computer Vision Systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "PNAS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "author": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation from natural language expressions", "author": ["R. Hu", "M. Rohrbach", "T. Darrell"], "venue": "ECCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual storytelling", "author": ["T.-H. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R. Girshick", "X. He", "P. Kohli", "D. Batra", "L. Zitnick", "D. Parikh", "L. Vanderwende", "M. Galley", "M. Mitchell"], "venue": "NAACL HLT", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Oriol Vinyals", "author": ["Q.V.L. Ilya Sutskever"], "venue": "Sequence to Sequence Learning with Neural Networks. In NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["A. Jabri", "A. Joulin"], "venue": "van der Maaten. Revisiting visual question answering baselines. In ECCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["A. Kannan", "K. Kurach", "S. Ravi", "T. Kaufmann", "A. Tomkins", "B. Miklos", "G. Corrado", "L. Luk\u00e1cs", "M. Ganea", "P. Young"], "venue": "Smart Reply: Automated Response Suggestion for Email. In KDD", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "What are you talking about? text-to-image coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "CVPR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system", "author": ["O. Lemon", "K. Georgila", "J. Henderson", "M. Stuttle"], "venue": "EACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep Reinforcement Learning for Dialogue Generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "EMNLP", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "P", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan"], "venue": "Doll\u00c3\u00a1r, and C. L. Zitnick. Microsoft COCO: Common Objects in Context. In ECCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["C.-W. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": "EMNLP", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "SSD: Single Shot MultiBox Detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A.C. Berg"], "venue": "ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "SIGDIAL", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeper LSTM and Normalized CNN Visual Question Answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA_LSTM_CNN", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Un-  certain Input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "author": ["N. Mostafazadeh", "C. Brockett", "B. Dolan", "M. Galley", "J. Gao", "G.P. Spithourakis", "L. Vanderwende"], "venue": "arXiv preprint arXiv:1701.08251", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Empirical methods for evaluating dialog systems", "author": ["T. Paek"], "venue": "Proceedings of the workshop on Evaluation for Language and Dialogue Systems-Volume 9", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "ICCV", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "and P", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev"], "venue": "Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In EMNLP", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Linking people with \"their\" names using coreference resolution", "author": ["V. Ramanathan", "A. Joulin", "P. Liang", "L. Fei-Fei"], "venue": "ECCV", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions", "author": ["A. Ray", "G. Christie", "M. Bansal", "D. Batra", "D. Parikh"], "venue": "EMNLP", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring Models and Data for Image Question Answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele"], "venue": "ECCV", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "A dataset for movie description", "author": ["A. Rohrbach", "M. Rohrbach", "N. Tandon", "B. Schiele"], "venue": "CVPR", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "author": ["I.V. Serban", "A. Garc\u00eda-Dur\u00e1n", "\u00c7. G\u00fcl\u00e7ehre", "S. Ahn", "S. Chandar", "A.C. Courville", "Y. Bengio"], "venue": "ACL", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "AAAI", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1605.06069", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, 22  V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CVPR", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint Video and Text Parsing for Understanding Events and Answering Queries", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.C. Zhu"], "venue": "IEEE MultiMedia", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to Sequence - Video to Text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R.J. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "NAACL HLT", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "A Neural Conversational Model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs", "author": ["L. Wang", "S. Guo", "W. Huang", "Y. Xiong", "Y. Qiao"], "venue": "arXiv preprint arXiv:1610.01119", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "ICLR", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Using Artificial Intelligence to Help Blind People \u2018See\u2019 Facebook", "author": ["S. Wu", "H. Pique", "J. Wieland"], "venue": "http://newsroom.fb.com/news/2016/04/using-artificialintelligence-to-help-blind-people-see-facebook/", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CVPR", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "ICCV", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2015}, {"title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring machine intelligence through visual question answering", "author": ["C.L. Zitnick", "A. Agrawal", "S. Antol", "M. Mitchell", "D. Batra", "D. Parikh"], "venue": "AI Magazine", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "We are witnessing unprecedented advances in computer vision (CV) and artificial intelligence (AI) \u2013 from \u2018low-level\u2019 AI tasks such as image classification [20], scene recogni-", "startOffset": 155, "endOffset": 159}, {"referenceID": 58, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 28, "endOffset": 32}, {"referenceID": 37, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 103, "endOffset": 107}, {"referenceID": 50, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 194, "endOffset": 202}, {"referenceID": 59, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 194, "endOffset": 202}, {"referenceID": 2, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 35, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 44, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 65, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 52, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 273, "endOffset": 281}, {"referenceID": 53, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 273, "endOffset": 281}, {"referenceID": 3, "context": "\u2022 Aiding visually impaired users in understanding their surroundings [7] or social media content [66] (AI: \u2018John just uploaded a picture from his vacation in Hawaii\u2019, Human: \u2018Great, is he at the beach?\u2019, AI: \u2018No, on a mountain\u2019).", "startOffset": 69, "endOffset": 72}, {"referenceID": 60, "context": "\u2022 Aiding visually impaired users in understanding their surroundings [7] or social media content [66] (AI: \u2018John just uploaded a picture from his vacation in Hawaii\u2019, Human: \u2018Great, is he at the beach?\u2019, AI: \u2018No, on a mountain\u2019).", "startOffset": 97, "endOffset": 101}, {"referenceID": 36, "context": "search and rescue missions) where the operator may be \u2018situationally blind\u2019 and operating via language [40] (Human: \u2018Is there smoke in any room around you?\u2019, AI: \u2018Yes, in one room\u2019, Human: \u2018Go there and look for people\u2019).", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Goal-driven dialog is typically evaluated on task-completion rate (how frequently was the user able to book their flight) or time to task completion [14,44] \u2013 clearly, the shorter the dialog the better.", "startOffset": 149, "endOffset": 156}, {"referenceID": 39, "context": "Goal-driven dialog is typically evaluated on task-completion rate (how frequently was the user able to book their flight) or time to task completion [14,44] \u2013 clearly, the shorter the dialog the better.", "startOffset": 149, "endOffset": 156}, {"referenceID": 26, "context": "The former discourages taskengineered bots for \u2018slot filling\u2019 [30] and the latter discourages bots that put on a personality to avoid answering questions while keeping the user engaged [64].", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "Upon completion1, VisDial will contain 1 dialog each (with 10 question-answer pairs) on \u223c140k images from the COCO dataset [32], for a total of \u223c1.", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "When compared to VQA [6], VisDial studies a significantly richer task (dialog), overcomes a \u2018visual priming bias\u2019 in VQA (in VisDial, the questioner does not see the image), contains free-form longer answers, and is an order of magnitude larger.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 12, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 23, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 57, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 46, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 157, "endOffset": 169}, {"referenceID": 54, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 157, "endOffset": 169}, {"referenceID": 55, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 157, "endOffset": 169}, {"referenceID": 6, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 18, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 25, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 40, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 42, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 45, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 1, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 253, "endOffset": 260}, {"referenceID": 19, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 253, "endOffset": 260}, {"referenceID": 0, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 2, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 8, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 13, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 15, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 33, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 34, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 35, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 44, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 63, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 9, "context": "Concurrent with our work, two recent works [13, 43] have also begun studying visually-grounded dialog.", "startOffset": 43, "endOffset": 51}, {"referenceID": 38, "context": "Concurrent with our work, two recent works [13, 43] have also begun studying visually-grounded dialog.", "startOffset": 43, "endOffset": 51}, {"referenceID": 14, "context": "[18], who proposed a fairly restrictive \u2018Visual Turing Test\u2019 \u2013 a system that asks templated, binary questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "2) The dataset in [18] only contains street scenes, while our dataset has considerably more variety since it uses images from COCO [32].", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "2) The dataset in [18] only contains street scenes, while our dataset has considerably more variety since it uses images from COCO [32].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Moreover, our dataset is two orders of magnitude larger \u2013 2,591 images in [18] vs \u223c140k images, 10 question-answer pairs per image, total of \u223c1.", "startOffset": 74, "endOffset": 78}, {"referenceID": 47, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 130, "endOffset": 133}, {"referenceID": 17, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 59, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 206, "endOffset": 210}, {"referenceID": 41, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 260, "endOffset": 264}, {"referenceID": 5, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 10, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 22, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 27, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 48, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 49, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 56, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 31, "context": "A recent large-scale conversation dataset is the Ubuntu Dialogue Corpus [35], which contains about 500K dialogs extracted from the Ubuntu channel on Internet Relay Chat (IRC).", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "[33] perform a study of problems in existing evaluation protocols for free-form dialog.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Consistent with previous data collection efforts, we collect visual dialog data on images from the Common Objects in Context (COCO) [32] dataset, which contains multiple objects in everyday scenes.", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Unlike VQA [6], answers are not restricted to be short or concise, instead workers are encouraged to reply as naturally and \u2018conversationally\u2019 as possible.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 87, "endOffset": 90}, {"referenceID": 64, "context": "One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 15, "endOffset": 24}, {"referenceID": 15, "context": "As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 15, "endOffset": 24}, {"referenceID": 63, "context": "As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 15, "endOffset": 24}, {"referenceID": 15, "context": "This allows language-only models to perform remarkably well on VQA and results in an inflated sense of progress [19, 69].", "startOffset": 112, "endOffset": 120}, {"referenceID": 63, "context": "This allows language-only models to perform remarkably well on VQA and results in an inflated sense of progress [19, 69].", "startOffset": 112, "endOffset": 120}, {"referenceID": 43, "context": "See [48] for a related, but complementary effort on question relevance in VQA.", "startOffset": 4, "endOffset": 8}, {"referenceID": 2, "context": "In VQA, binary questions are simply those with \u2018yes\u2019, \u2018no\u2019, \u2018maybe\u2019 as answers [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6, 69] \u2013 61.", "startOffset": 47, "endOffset": 54}, {"referenceID": 63, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6, 69] \u2013 61.", "startOffset": 47, "endOffset": 54}, {"referenceID": 29, "context": "Existing metrics such as BLEU, METEOR, ROUGE are known to correlate poorly with human judgement in evaluating dialog responses [33].", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "Instead of evaluating on a downstream task [9] or holistically evaluating the entire conversation (as in goal-free chitchat [5]), we evaluate individual responses at each round (t = 1, 2, .", "startOffset": 43, "endOffset": 46}, {"referenceID": 21, "context": "As a result, such models do not exploit the biases in option creation and typically underperform models that do [25], but it is debatable whether exploiting such biases is really indicative of progress.", "startOffset": 112, "endOffset": 116}, {"referenceID": 51, "context": "In all cases, we represent I via the `2-normalized activations from the penultimate layer of VGG-16 [56].", "startOffset": 100, "endOffset": 104}, {"referenceID": 49, "context": "Thus, similar to [54], as shown in Fig.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "In the language of Memory Network [9], this is a \u20181-hop\u2019 encoding.", "startOffset": 34, "endOffset": 37}, {"referenceID": 61, "context": "Finally, we adapt several (near) state-of-art VQA models (SAN [67], HieCoAtt [37]) to Visual Dialog.", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "Finally, we adapt several (near) state-of-art VQA models (SAN [67], HieCoAtt [37]) to Visual Dialog.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "Note that our LF-QI-D model is similar to that in [36].", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "In this section, we lay out an exhaustive list of differences between VisDial and image question-answering datasets, with the VQA dataset [6] serving as the representative.", "startOffset": 138, "endOffset": 141}, {"referenceID": 34, "context": "DAQUAR [38] 12,468 1,447 11.", "startOffset": 7, "endOffset": 11}, {"referenceID": 62, "context": "4% Visual Madlibs [68] 56,468 9,688 4.", "startOffset": 18, "endOffset": 22}, {"referenceID": 44, "context": "9% COCO-QA [49] 117,684 69,172 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "0% 100% Baidu [17] 316,193 316,193 VQA [6] 614,163 204,721 6.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "0% 100% Baidu [17] 316,193 316,193 VQA [6] 614,163 204,721 6.", "startOffset": 39, "endOffset": 42}, {"referenceID": 64, "context": "Visual7W [70] 327,939 47,300 6.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Such a line of questioning does not exist in the VQA dataset, where the subjects were shown the questions already asked about an image, and explicitly instructed to ask about different entities [6].", "startOffset": 194, "endOffset": 197}, {"referenceID": 7, "context": "In particular, we compare VisDial, VQA, and Cornell Movie-Dialogs Corpus [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "For the purpose of our analysis, we pick the popular sequence-to-sequence (Seq2Seq) language model [24] and use the perplexity of this model trained on different datasets as a measure of temporal structure in a dataset.", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "One key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 87, "endOffset": 90}, {"referenceID": 64, "context": "One key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "One key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 118, "endOffset": 122}, {"referenceID": 63, "context": "As described in [69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 16, "endOffset": 20}, {"referenceID": 63, "context": "This allows languageonly models to perform remarkably well on VQA and results in an inflated sense of progress [69].", "startOffset": 111, "endOffset": 115}, {"referenceID": 43, "context": "See [48] for a related, but complementary effort on question relevance in VQA.", "startOffset": 4, "endOffset": 8}, {"referenceID": 2, "context": "In VQA, binary questions are simply those with \u2018yes\u2019, \u2018no\u2019, \u2018maybe\u2019 as answers [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6,69] \u2013 61.", "startOffset": 47, "endOffset": 53}, {"referenceID": 63, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6,69] \u2013 61.", "startOffset": 47, "endOffset": 53}, {"referenceID": 24, "context": "We use Adam [28]", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on \u223c120k images from COCO, with a total of\u223c1.2M dialog questionanswer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders \u2013 Late Fusion, Hierarchical Recurrent Encoder and Memory Network \u2013 and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrievalbased evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first \u2018visual chatbot\u2019! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org.", "creator": "LaTeX with hyperref package"}}}