{"id": "1611.08765", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Fill it up: Exploiting partial dependency annotations in a minimum spanning tree parser", "abstract": "unsupervised models full of dependency parsing typically merely require large amounts of clean, homogeneous unlabeled data plus gold - standard part - of - speech tags. adding indirect subject supervision ( e. g. language universals operators and rules ) can help, but we show that obtaining small amounts of direct supervision - here, partial variable dependency annotations - each provides a strong balance between zero and full supervision. we normally adapt the unsupervised open convexmst dependency parser to learn from partial dependencies expressed in the graph fragment language. with less than half 24 successive hours of total annotation, we obtain 7 % and 17 % absolute improvement in unlabeled dependency scores for english and spanish, respectively, performance compared to the same goal parser using only universal implicit grammar preserving constraints.", "histories": [["v1", "Sat, 26 Nov 2016 23:39:41 GMT  (192kb,D)", "http://arxiv.org/abs/1611.08765v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liang sun", "jason mielens", "jason baldridge"], "accepted": false, "id": "1611.08765"}, "pdf": {"name": "1611.08765.pdf", "metadata": {"source": "CRF", "title": "Fill it up: Exploiting partial dependency annotations in a minimum spanning tree parser", "authors": ["Liang Sun", "Jason Mielens"], "emails": ["sally722@utexas.edu", "jmielens@utexas.edu", "jbaldrid@utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised parsing solutions are simultaneously an attractive yet troublesome method for handling low-data scenarios. The performance of unsupervised parsers has increased dramatically in recent years (Klein and Manning, 2004; Naseem et al., 2010), making them a potentially viable option for constructing labeled corpora on limited budgets. However, their performance is often outmatched by small amounts of labeled data (Blunsom and Cohn, 2010; Spitkovsky et al., 2012). Further, recent work using linguistically-informed error analysis on unsupervised Combinatory Categorial Grammar parsing shows that entire syntactic phenomena are outside the scope of existing unsupervised parsers (Bisk\nand Hockenmaier, 2015). Accordingly, most recent work in this area has focused on methods of providing sources of indirect annotation, whether via linguistic world-knowledge (Naseem et al., 2010; Grave and Elhadad, 2015), partial annotations (Flannery et al., 2011; Mielens et al., 2015) or crosslingual information transfer (Naseem et al., 2012).\nWith unsupervised parsing, data collection is not entirely eliminated: a large amount of clean, relevant data is needed. Also, evaluations of unsupervised techniques typically rely on gold part-ofspeech tags. Obtaining clean data for many languages is actually a difficult process\u2013complicated by issues such as language identification, digitization, and varying or absent orthographies. This challenge also exists in many domain adaptation scenarios.\nWe explore the effectiveness of creating small amounts of labeled data using the Graph Fragment Language (GFL), an annotation scheme designed for speed and ease (Schneider et al., 2013; Mordowanec et al., 2014). We create 270 English and 2297 Spanish partial sentence annotations using GFL, using a mix of expert and non-expert annotators. We then adapt the minimum spanning tree based parsing technique of Grave & Elhadad (2015) to use these partial annotations in addition to universal dependency rules it already exploits. Throughout this work we will refer to this parser as ConvexMST.1\nWe present parsing results with and without gold part-of-speech tags. When using predicted POS tags, our experiments show that exploiting cheap, incomplete direct supervision in addition to language universals provides large absolute performance im-\n1Code available at github.com/jmielens/convex-mst\nar X\niv :1\n61 1.\n08 76\n5v 1\n[ cs\n.C L\n] 2\n6 N\nov 2\nprovements for both English and Spanish: 6.3% for the former and 17.3% for the latter. Furthermore, the ConvexMST parser dramatically outperforms the Gibbs sampler parser of Mielens et al. (2015) using the supervision (English: +5.2%; Spanish: +14.4%). We also show that the extra supervision provided by gold POS tags heavily influences results; in particular, it inflates performance when only using language universals. Experiments that rely on gold POS tags alone are thus not reliable indicators of performance in true low-resource settings."}, {"heading": "2 Annotation", "text": ""}, {"heading": "2.1 Graph Fragment Language", "text": "We use the Graph Fragment Language (GFL) (Schneider et al., 2013) to allow for light-weight, simple annotations that our annotators can easily learn and use confidently. The choice of annotation scheme is particularly important: we seek to optimize annotation speed rather than full-specification or high accuracy. In previous studies, the use of GFL has allowed for annotation rates of 2-3 times that of traditional dependency annotations while still maintaining a useful level of annotation density (Mordowanec et al., 2014; Mielens et al., 2015). Hwa (1999) demonstrated that it is most effective to provide high-level sentence constituents to a parser and allow it to fill in the low-level information itself.\nThe GFL annotation in Figure 1 shows two distinct notations. Constituent brackets are specified by parentheses and direct dependencies by angle brackets. Many words and phrases are underspecified. Allowing partial annotations dramatically increases the speed at which annotators can work, while simultaneously reducing error rates. These two effects both arise from being able to leave difficult or tedious portions of a sentence unspecified."}, {"heading": "2.2 Filling in Partial Dependencies", "text": "A partial annotation produces a set of dependency tree fragments. Compared to an unlabeled sentence, this can substantially reduce the work a parser must do. When working with partial dependencies, there are two paths that can be taken with regard to overall model-building. In a \u2018Fill-then-Parse\u2019 setup, the partial dependencies are first filled-in to produce full dependencies that are then used to train a standard dependency parser. In a \u2018Fill+Parse\u2019 setup, one model both fills in and parses new sentences.\nWe use a Fill+Parse setup, while previous work focused on Fill-then-Parse. The major benefit of the former is that learning can be sensitive to the source of an arc in the training data\u2014e.g., whether it came from an annotator or a universal rule. Fill-then-Parse obscures this distinction and not knowing how trustworthy an arc is can lead to additional errors. Indeed, Fill+Parse method produces better results for our datasets than Fill-then-Parse (see Section 4.2)."}, {"heading": "2.3 Simulated Cost Comparison", "text": "Many factors influence the cost of creating a corpus. Our goal is to minimize cost relative to the performance of a parser trained with the corpus. The actual cost of finding and paying annotators is the most obvious factor, and it will typically be higher for a lowresource language or highly specialized domain. Using a light-weight partial annotation scheme like GFL has the potential to increase the pool of qualified annotators and alleviate this challenge.\nGiven a partial annotation scheme like GFL, an additional cost factor is that of obtaining a particular level of completion for each sentence. Consider that for any sentence there are both \u2018low-hanging fruit\u2019 dependencies such as determiner attachment, and more difficult dependencies such as preposition attachment and long-distance relations. Harder dependencies take longer to annotate (and thus cost more), so it is worth considering cost metrics that incorporate completion percentage. In the absence of timing/expense data, we can simulate this intuition with a variable cost model for which each an additional dependency annotated in a sentence is more expensive than the previous one.\nFigure 2 demonstrates the impact of completion cost. Parsing accuracies (for our parser introduced in\nthe next section) are shown at different costs, under (a) simple equal (per arc) cost and (b) variable cost. We simulated the construction of various corpora by deriving partial dependencies from gold standard annotations), and show the cost curves for different sentence completion rates. 100% completion produces the best performance with equal costs, but under the more realistic variable cost model, 30% and 50% completion win. We show later that this pattern holds under actual timed annotation.\nGarrette (2015) demonstrated the benefit of partial annotations for CCG parsing. They focused on the number of (partial) bracket annotations (as a proxy for annotation time), holding this fixed while varying the number of sentences. Strikingly, they found that having 40% of brackets across the full dataset was better than full brackets for 80% of the corpus. This result uses an equal cost-per-bracket assumption, so the difference would be even more favorable to partial annotations with a variable cost."}, {"heading": "2.4 Unsupervised vs. Partial Annotations", "text": "Without any direct annotations, we must rely on indirect supervision such as universal grammar rules, cross-lingual information transfer, and domain adaptation. Following Grave & Elhadad (2015), we use the universal grammar rules in Table 1. Indirect supervision via these rules is achieved by biasing produced trees to conform to the rules. This is the\nonly form of dependency supervision considered by Grave & Elhadad, though they do provide additional direct supervision via gold part-of-speech tags."}, {"heading": "2.5 Data", "text": "We use two sources of data. To compare with prior work, we use the universal treebanks (version 2.0), which cover ten languages from a variety of language families (McDonald et al., 2013). We obtained GFL annotations for a subset of the English data, originally from WSJ Section 03 of the Penn Treebank, and we use simulation techniques to produce partial dependencies for the other languages.\nOur second data source is the Spanish dependency treebank from the AnCora corpus (Taule\u0301 et al., 2008). For 1410 unique sentences of AnCora, we have partial dependencies specified in GFL by twelve annotators. Most sentences received a single partial annotation from a single annotator, but one section of the corpus was annotated by all annotators. As the original corpus is fully-specified for\ngold dependencies, we can measure annotator agreement with a gold standard.\nThe background and experience of the annotators varied considerably. Roughly one third were native Spanish speakers, with the rest ranging from fluent non-native speakers to a few with just a single year of formal study. This was done intentionally to provide a large variance in the types and quality of annotations that they were able to provide.\nEach annotator was trained for just 30 minutes. The nature of the annotations was explained and a small number of guidelines were provided. For instance, annotators were told that typically adjectives are dependents of nouns, nouns are dependents of verbs, and so on. These guidelines amounted to a summary of the rules in Table 1. During the annotation sessions, annotators were told to ask as many clarifying questions as needed, although in practice they needed very little guidance. Post-experiment debriefing interviews suggested that the straightforward nature of the GFL notation was very helpful and became clear within a few example sentences.\nDespite minimal training time, annotators were able to produce relatively consistent annotations that agreed in large part with other annotators. Table 3 shows both pair-wise and overall agreement between annotators when considering arcs that each of the annotators in the pair had provided a head for. Overall agreement was high, with most pairwise numbers in the 70-80\u2019s, and agreement for individual annotators to the group is even higher \u2013 mostly in the 80\u2019s.\nThe partial annotation task proved helpful in terms of speed; our annotators were able to cover 750 tokens/hr, which compares favorably to the processes of the Penn Treebank, which achieved rates of 750-1000 tokens/hr for English (Marcus et al., 1993), and 300-400 tokens/hr for Chinese (Xue et al., 2005), both making use of initial parse suggestions from an existing parser. Efforts not using an existing parser proceed even slower; for instance the Ancient Greek Dependency Treebank reported rates of 100-200 tokens/hr (Bamman and Crane, 2011)."}, {"heading": "2.6 POS-Tagging", "text": "Our goal is to minimize real-world costs associated with producing a finished parsing model. To this end, we trained our own POS taggers using type label annotations (Garrette and Baldridge, 2013)\nrather than using gold-standard tags. We use universal POS tags rather than the finer-grained sets the source corpora use, both for simplicity and crosslanguage comparisons (Petrov et al., 2011).\nWe trained taggers for all languages using a limited amount of the available gold data\u2014ensuring that the accuracy is comparable with low-resource human-sourced taggers. We extract types from the corpus, rank them by frequency, and take the most frequent types to train the tagger. The cutoff on how many types to take is derived from the number of types the annotators in Garrette et al. (2013) were able to produce in two hours. The taggers all obtain around 80% accuracy."}, {"heading": "3 Method", "text": ""}, {"heading": "3.1 Convex-MST", "text": "This section provides a brief overview of the core parsing algorithm; for full details, see Grave & Elhadad (2015). We begin by considering a binary vector y that encodes all of the dependencies in our corpus, such that yijk = 1 if sentence i has an arc with dependent j and head k. This representation leads to the problem formulation in Equation 1, where Y is the convex hull of all the valid tree assignments for y, n is the number of possible dependency arcs in the corpus, u is a penalty vector that penalizes potential dependency arcs that are not in the set of universal dependency rules, and w is a weight vector learned during training\nmin y\u2208Y min w\n1\n2n \u2016y \u2212Xw\u201622 +\n\u03bb 2 \u2016w\u201622 \u2212 \u00b5uTy (1)\nThis problem can be solved using Algorithm 1 (Grave and Elhadad, 2015)."}, {"heading": "3.2 Partial Dependency Features", "text": "The main modification we make is to add an additional term to penalize arcs that disagree with partial\nAlgorithm 1 Optimization algorithm from Grave & Elhadad (2015)\n1: for r 6= 0 do Compute the optimal w: wt = argminw 1 2n\u2016yt \u2212Xw\u2016 2 2 + \u03bb 2\u2016w\u2016 2 2\nCompute the gradient w.r.t. y: gt = 1 n(yt \u2212Xwt)\u2212 \u00b5u Solve the linear program: st = mins\u2208Y s\nTgt Take the Franke-Wolfe step: yt = \u03b3tst + (1\u2212 \u03b3t)yt\nannotations. Let S be the set of all indices on y where that head-dependent pair conforms to one of the universal rules. Then we can require that some proportion of the arcs in the corpus satisfy:\n1\nn \u2211 i\u2208S yi \u2265 c\nThis is equivalent to uTy \u2265 c, where:\nui = { 1/n, if i \u2208 S. 0, otherwise.\nThis is how the penalty term \u00b5uTy from (1) is derived. Similarly, we can add another penalty term that ensures a certain percentage of the arcs conform to the arcs specified by annotators. If we let G be the set of all indicies on y where the word pair conforms to the GFL annotations, then it is simple to construct an additional penalty term \u03bevTy.\nThere is a slight difference between the GFL penalty term and the universal rule penalty term.\nWhereas the universal rule penalty is based simply on whether the arc conforms or does not conform to the rules, the GFL annotations naturally lead to a three-way distinction: the annotation can specify that an arc should be present, should not be present, or make no commitment.\nAccordingly, we modify G to be two sets, Gw and Gb, where Gw is the set of all indicies on y where the word pair should have an arc, and Gb is the set of all indicies on y where the word pair should not have an arc. We refer to these as the whitelist and blacklist, accordingly. Under this formulation, the GFL-based penalty term \u03bevTy is now made with:\nvi =  1/n, if i \u2208 Gw \u22121/n, if i \u2208 Gb 0, otherwise\nThis leads to the modified objective function in (2), which now seeks to find a solution that minimizes the number of arcs that violate both universal rules and the annotator-specified fragments.\nmin y\u2208Y min w\n1\n2n \u2016y\u2212Xw\u201622+\n\u03bb 2 \u2016w\u201622\u2212\u00b5uTy\u2212\u03bevTy\n(2) When no GFL annotations are specified for the corpus, the GFL penalty term goes to zero and the objective function reverts to its original formulation.\nSpecific arcs are added to Gw and Gb in a number of ways, based on the different types of GFL annotation. Consider the GFL annotation in Figure 3. Here, the annotator has specified a direct dependency with \u2018passed\u2019 as the head of \u2018congress\u2019. The\narc \u2018passed \u2190 congress\u2019 is added to Gw, while all other arcs of the form \u2018X \u2190 congress\u2019 are added to Gb because \u2018congress\u2019 may only have a single head.\nBrackets may also result in additions to the whitelist and blacklist. In Figure 3, \u2018a comprehensive plan\u2019 is bracketed. In this case, no arcs can be whitelisted, but many can be blacklisted. For instance, no word external to the bracket may be headed by a word in the bracket. This means arcs such as \u2018plan\u2190 congress\u2019 must be in Gb.\nAlso, \u2018passed\u2019 is indicated as the head of the entire bracket. We cannot whitelist any specific arcs with this information (since we do not know the head of the bracketed expression), but we know that no word internal to the bracket is headed by any word external to it, other than \u2018passed\u2019. Hence, arcs such as \u2018congress\u2190 plan\u2019 must be in Gb."}, {"heading": "4 Experiments and Discussion", "text": "We consider both simulated and actual partial annotations. Results based on actual annotation are the most important as they provide our best measure of performance under a realistic annotation setting. However, our Spanish annotators had only six hours each, and there was no inter-annotator communication or creation of annotation conventions, and no attempt to have them adopt the conventions in the gold-standard AnCora dependencies we evaluate against. Because of this, we include simulation results to eliminate this source of divergence to better measure the effectiveness of different methods for filling in missing arcs in a partial annotation. It of course also allows us to measure this for all the languages in the Universal Dependencies treebanks.\nWe consider three different supervision settings for ConvexMST:\n\u2022 UG uses just the universal grammar based features, which is equivalent to the method used by Grave & Elhadad (2015).\n\u2022 GFL uses just the human specified features.\n\u2022 GFL+UG uses both.\nThese three methods correspond with \u03be 6= 0, \u00b5 6= 0, and \u03be\u00b5 6= 0 in Equation 2. The training sets correspond with the \u2018Partial EN\u2019 and \u2018Partial ES\u2019 sets from Table 2. The set of sentences annotated with GFL is used as the training set for the GFL, UG, and GFL+UG methods."}, {"heading": "4.1 Simulated partial dependencies", "text": "Simulated partial dependencies are produced by removing dependencies via a stochastic process that approximates how we instructed human annotators to focus their efforts. Arcs are removed top-down, with arcs lower in the tree being more likely to be deleted. This results in trees with more high-level structures and less lower-level information. Figure 4 demonstrates the stability of our parser under varying levels of such gold tree degradation. Missing arcs were recovered using our parse imputation scheme (using GFL+UG features), and the resulting parser was applied to the evaluation sentences. Accuracy decreases slightly to around 60% removal, and then degrades more rapidly after that. Table 4 provides numeric data for the simulations."}, {"heading": "4.2 Annotator-sourced partial dependencies", "text": "Table 5 gives semi-supervised parsing results on the English and Spanish treebanks for sentences with 10 or fewer words. To investigate the impact of POS taggers on parsing results, we conducted two series of experiments using POS tags trained by our own tagger as discussed in Section 2.6 (Predicted Tag) and gold POS tags extracted from treebank (Gold Tag). We compare against a right-branching baseline and the Gibbs parser of Mielens et al. (2015).\nAll the parsing methods handily beat the rightbranching baseline. ConvexMST-UG (the model of Grave and Elhadad (2015)) beats the Gibbs parser with gold POS tags, but the ranking switches with predicted POS tags. This shows the effectiveness of ConvexMST, but highlights its brittleness with respect to tagging errors: bad tags lead to poor guidance from language universals. ConvexMSTGFL easily beats both these approaches: it exploits partial annotations much more effectively than the Gibbs parser and learns effectively without language universals. The difference is especially marked for predicted POS tags: ConvexMST-GFL beats ConvexMST-UG by 4.3% for English and 17.1% for Spanish. (Recall that there were 8 hours of annotation for English and 72 hours for Spanish.)\nThe best method of all uses both partial annotations and language universals: ConvexMSTUG+GFL improves on ConvexMST-GFL for both languages and POS conditions. The impact of the combination is greater for English, which has less GFL annotation. Overall, these results show that this combination is robust to varying amounts of partial annotations: the UG constraints are strong on their own and provide a strong basis without annotations, they contribute when there are not many annotations available, and eventually become less essential (but remain unharmful) as more are provided.\nIt is important to recall that the GFL annotations have no specific conformity to the gold standards of either original corpus. Our goal was to understand the overall behavior of different methods given the same free-wheeling, diverse annotations; it is likely that higher numbers would have been achieved had we guided annotators to use corpus conventions, or used full annotations provided by our annotators as the evaluation set. The former defeats the spirit of our exercise, and we did not have sufficient budget for the latter.\nFor Spanish, we also considered the performance of individual annotators alongside the full training set. The learning curves for individual annotators are shown in Figure 5. There is substantial variation in the curves for the individual annotators; however, the curve based on the union of all annotations at\neach time step is smooth and is better than any individual past the three hour mark. One way to consider this is in terms of building an accurate parser quickly with multiple, diverse annotators, where wall clock time matters. Another way is to consider robustness with respect to possibly bad annotators. The next obvious steps would be to use active learning and to detect disagreement in annotators to either drop some or intervene to improve their quality. (Again, keep in mind that we are considering a \u201ccold start\u201d to this process, so there can be no gold standard for checking annotator quality.)\nComparison to Full Annotation To this point, all performance comparisons have been between different parse feature sets; we have demonstrated that the GFL features are complimentary to the UG features, and that when standing alone the GFL features are stronger than the UG features. The question of whether it might be more effective to simply have annotators produce full annotations is not addressed by these comparisons. To answer this question, we had our most experienced annotator fully annotate the same section that the other annotators did partially. Producing these full annotations required roughly 13 hours of time from the single expert annotator. In comparison, the other annotators were able to partially annotate the same section in roughly two hours each \u2013 a total of 24 hours. However, the theoretical wall clock time of the group of annotators could be as low as two hours if the sessions were run in parallel. These different training sets were once again used to train ConvexMST models that were evaluated on a held out test set. Table 6 contains the results of this experiment, demonstrating that the group of inexperienced annotators producing partial annotations was able to achieve similar performance levels to the single annotator producing full annotations. It should be noted that this comparison does not weight the results using the extrinsic costs associated with the production of the training data. In a real-world environment, the expert annotator would likely be more expensive than the inexperienced annotators, and possibly all of them combined (especially in a crowd-sourcing scenario). This makes the performance per unit cost for partial annotators even higher than Table 6 indicates. See Section 2.3 for discussion and modeling\nof these extrinsic cost effects."}, {"heading": "4.3 Longer Sentences", "text": "We also evaluated ConvexMST with longer sentences: those with 20 words or less. For this, the right-branching baseline is 25.8%. When using all the annotations on the common set for all annotators, the scores for ConvexMST with UG, GFL, and GFL+UG are 47.6%, 54.4%, and 55.3%, respectively. The values are worse than for shorter sentences, as expected, but the pattern observed in Table 5 still holds: GFL annotations best UG alone, and their combination is the best of all."}, {"heading": "4.4 Discussion & Error Analysis", "text": "POS-Tagging Impact We thought it important to consider imperfect POS-taggings because this entire framework is based off of the assumption that the user is working from essentially no pre-existing resources. Assuming the availability of gold-standard POS tags is antithetical to this idea, and is one way in which direct supervision can show up in otherwise unsupervised (or indirectly supervised) systems.\nMany tagger errors are not likely to cause major problems during parsing; for instance mislabeling pronouns as nouns, or adverbs as adjectives, is unlikely to lead to major structural issues. However, more unlikely errors can cause more dramatic effects, as shown in Figure 6. Here, the phrase \u2018beating politically\u2019 (gold tags \u2018NOUN ADV\u2019) is mistagged as \u2018ADJ VERB\u2019, leading to the attachment of \u2018politically\u2019 to the root word and the reorganization of a substantial chunk of the sentence.\nWeighting Constraint Violations For feature sets with both GFL and UG-based constraints, a weighting factor can bias the parser towards being more likely to respect either GFL or UG constraints. We experimented with this, and found that for the\ndatasets we considered, the best results were obtained when we weighted violations of GFL constraints as worse than violations of UG constraints. This result is not entirely unexpected given the relative performances of the constraints on their own, but it provides more evidence that direct supervision even in small amounts can beat indirect supervision."}, {"heading": "5 Conclusion", "text": "We have shown that human-sourced partial annotations can be exploited to learn effective dependency parsers in short period of time. The ConvexMST method we adapt from Grave and Elhadad easily combines constraints from both language universals and partial annotations, providing greater robustness from starting annotation until one runs out of budget or time. We demonstrate this with actual annotations produced for English and Spanish, using annotators with a range of experience.\nOverall, we present a case for working in realistic settings by paying close attention to the various sources of annotation and tracking the real costs associated with that supervision. We believe that overreliance on creeping supervision of this type may lead to an inaccurate picture of the cross-lingual and low-resource applicability of various models, and are encouraged by recent work on character-based models by Gillick et al. (2015) and Ballesteros et al (2015), among others. Their work shows viable models can be produced without relying on having annotations a priori, but rather learning representations on the fly that need not conform to any one set of standards."}, {"heading": "Acknowledgments", "text": "Supported by the U.S. Army Research Office under grant number W911NF-10-1-0533. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the U.S. Army Research Office."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "The ancient Greek and Latin dependency treebanks", "author": ["Bamman", "Crane2011] David Bamman", "Gregory Crane"], "venue": "In Language Technology for Cultural Heritage,", "citeRegEx": "Bamman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2011}, {"title": "Probing the linguistic strengths and limitations of unsupervised grammar induction", "author": ["Bisk", "Hockenmaier2015] Yonatan Bisk", "Julia Hockenmaier"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bisk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bisk et al\\.", "year": 2015}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Blunsom", "Cohn2010] Phil Blunsom", "Trevor Cohn"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Blunsom et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2010}, {"title": "Training dependency parsers from partially annotated corpora", "author": ["Yusuke Miayo", "Graham Neubig", "Shinsuke Mori"], "venue": "In IJCNLP,", "citeRegEx": "Flannery et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Flannery et al\\.", "year": 2011}, {"title": "Learning a part-of-speech tagger from two hours of annotation", "author": ["Garrette", "Baldridge2013] Dan Garrette", "Jason Baldridge"], "venue": "In HLT-NAACL,", "citeRegEx": "Garrette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garrette et al\\.", "year": 2013}, {"title": "Inducing Grammars from Linguistic Universals and Realistic Amounts of Supervision", "author": ["Dan Garrette"], "venue": "Ph.D. thesis,", "citeRegEx": "Garrette.,? \\Q2015\\E", "shortCiteRegEx": "Garrette.", "year": 2015}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "A convex and feature-rich discriminative approach to dependency grammar induction", "author": ["Grave", "Elhadad2015] Edouard Grave", "No\u00e9mie Elhadad"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Grave et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grave et al\\.", "year": 2015}, {"title": "Supervised grammar induction using training data with limited constituent information", "author": ["Rebecca Hwa"], "venue": "In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "Hwa.,? \\Q1999\\E", "shortCiteRegEx": "Hwa.", "year": 1999}, {"title": "Corpus-based induction of syntactic structure: Models of dependency and constituency", "author": ["Klein", "Manning2004] Dan Klein", "Christopher D Manning"], "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2004}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Universal dependency annotation", "author": ["Joakim Nivre", "Yvonne Quirmbach-Brundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar T\u00e4ckstr\u00f6m"], "venue": null, "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Parse imputation for dependency annotations", "author": ["Liang Sun", "Jason Baldridge"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-", "citeRegEx": "Mielens et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mielens et al\\.", "year": 2015}, {"title": "Simplified dependency annotations with GFLWeb", "author": ["Nathan Schneider", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Mordowanec et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mordowanec et al\\.", "year": 2014}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Naseem et al.2010] Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Naseem et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Selective sharing for multilingual dependency parsing", "author": ["Naseem et al.2012] Tahira Naseem", "Regina Barzilay", "Amir Globerson"], "venue": null, "citeRegEx": "Naseem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2011] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "arXiv preprint arXiv:1104.2086", "citeRegEx": "Petrov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "A framework for (under) specifying dependency syntax without overloading annotators", "author": ["Brendan O\u2019Connor", "Naomi Saphra", "David Bamman", "Manaal Faruqui", "Noah A Smith", "Chris Dyer", "Jason Baldridge"], "venue": null, "citeRegEx": "Schneider et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 2013}, {"title": "Three dependencyand-boundary models for grammar induction", "author": ["Hiyan Alshawi", "Daniel Jurafsky"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-", "citeRegEx": "Spitkovsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2012}, {"title": "AnCora: Multilevel annotated corpora for catalan and spanish", "author": ["Taul\u00e9 et al.2008] Mariona Taul\u00e9", "Maria Ant\u00f2nia Mart\u0131", "Marta Recasens"], "venue": null, "citeRegEx": "Taul\u00e9 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Taul\u00e9 et al\\.", "year": 2008}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005] Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer"], "venue": "Natural language engineering,", "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "The performance of unsupervised parsers has increased dramatically in recent years (Klein and Manning, 2004; Naseem et al., 2010), making them a potentially viable option for constructing labeled corpora on limited budgets.", "startOffset": 83, "endOffset": 129}, {"referenceID": 20, "context": "However, their performance is often outmatched by small amounts of labeled data (Blunsom and Cohn, 2010; Spitkovsky et al., 2012).", "startOffset": 80, "endOffset": 129}, {"referenceID": 16, "context": "linguistic world-knowledge (Naseem et al., 2010; Grave and Elhadad, 2015), partial annotations (Flannery et al.", "startOffset": 27, "endOffset": 73}, {"referenceID": 4, "context": ", 2010; Grave and Elhadad, 2015), partial annotations (Flannery et al., 2011; Mielens et al., 2015) or crosslingual information transfer (Naseem et al.", "startOffset": 54, "endOffset": 99}, {"referenceID": 14, "context": ", 2010; Grave and Elhadad, 2015), partial annotations (Flannery et al., 2011; Mielens et al., 2015) or crosslingual information transfer (Naseem et al.", "startOffset": 54, "endOffset": 99}, {"referenceID": 17, "context": ", 2015) or crosslingual information transfer (Naseem et al., 2012).", "startOffset": 45, "endOffset": 66}, {"referenceID": 19, "context": "amounts of labeled data using the Graph Fragment Language (GFL), an annotation scheme designed for speed and ease (Schneider et al., 2013; Mordowanec et al., 2014).", "startOffset": 114, "endOffset": 163}, {"referenceID": 15, "context": "amounts of labeled data using the Graph Fragment Language (GFL), an annotation scheme designed for speed and ease (Schneider et al., 2013; Mordowanec et al., 2014).", "startOffset": 114, "endOffset": 163}, {"referenceID": 15, "context": ", 2013; Mordowanec et al., 2014). We create 270 English and 2297 Spanish partial sentence annotations using GFL, using a mix of expert and non-expert annotators. We then adapt the minimum spanning tree based parsing technique of Grave & Elhadad (2015) to use these partial annotations in addition to universal dependency rules it already exploits.", "startOffset": 8, "endOffset": 252}, {"referenceID": 14, "context": "Furthermore, the ConvexMST parser dramatically outperforms the Gibbs sampler parser of Mielens et al. (2015) using the supervision (English: +5.", "startOffset": 87, "endOffset": 109}, {"referenceID": 19, "context": "We use the Graph Fragment Language (GFL) (Schneider et al., 2013) to allow for light-weight, simple annotations that our annotators can easily learn and use confidently.", "startOffset": 41, "endOffset": 65}, {"referenceID": 15, "context": "In previous studies, the use of GFL has allowed for annotation rates of 2-3 times that of traditional dependency annotations while still maintaining a useful level of annotation density (Mordowanec et al., 2014; Mielens et al., 2015).", "startOffset": 186, "endOffset": 233}, {"referenceID": 14, "context": "In previous studies, the use of GFL has allowed for annotation rates of 2-3 times that of traditional dependency annotations while still maintaining a useful level of annotation density (Mordowanec et al., 2014; Mielens et al., 2015).", "startOffset": 186, "endOffset": 233}, {"referenceID": 9, "context": "Hwa (1999) demonstrated that it is most effective to provide high-level sentence constituents to a parser and allow it to fill in the low-level information itself.", "startOffset": 0, "endOffset": 11}, {"referenceID": 13, "context": "0), which cover ten languages from a variety of language families (McDonald et al., 2013).", "startOffset": 66, "endOffset": 89}, {"referenceID": 21, "context": "Our second data source is the Spanish dependency treebank from the AnCora corpus (Taul\u00e9 et al., 2008).", "startOffset": 81, "endOffset": 101}, {"referenceID": 12, "context": "terms of speed; our annotators were able to cover 750 tokens/hr, which compares favorably to the processes of the Penn Treebank, which achieved rates of 750-1000 tokens/hr for English (Marcus et al., 1993), and 300-400 tokens/hr for Chinese (Xue et al.", "startOffset": 184, "endOffset": 205}, {"referenceID": 22, "context": ", 1993), and 300-400 tokens/hr for Chinese (Xue et al., 2005), both making use of initial parse suggestions from an existing parser.", "startOffset": 43, "endOffset": 61}, {"referenceID": 18, "context": "We use universal POS tags rather than the finer-grained sets the source corpora use, both for simplicity and crosslanguage comparisons (Petrov et al., 2011).", "startOffset": 135, "endOffset": 156}, {"referenceID": 5, "context": "The cutoff on how many types to take is derived from the number of types the annotators in Garrette et al. (2013) were", "startOffset": 91, "endOffset": 114}, {"referenceID": 14, "context": "We compare against a right-branching baseline and the Gibbs parser of Mielens et al. (2015).", "startOffset": 70, "endOffset": 92}, {"referenceID": 7, "context": "lead to an inaccurate picture of the cross-lingual and low-resource applicability of various models, and are encouraged by recent work on character-based models by Gillick et al. (2015) and Ballesteros et al (2015), among others.", "startOffset": 164, "endOffset": 186}, {"referenceID": 7, "context": "lead to an inaccurate picture of the cross-lingual and low-resource applicability of various models, and are encouraged by recent work on character-based models by Gillick et al. (2015) and Ballesteros et al (2015), among others.", "startOffset": 164, "endOffset": 215}], "year": 2016, "abstractText": "Unsupervised models of dependency parsing typically require large amounts of clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect supervision (e.g. language universals and rules) can help, but we show that obtaining small amounts of direct supervision\u2014here, partial dependency annotations\u2014provides a strong balance between zero and full supervision. We adapt the unsupervised ConvexMST dependency parser to learn from partial dependencies expressed in the Graph Fragment Language. With less than 24 hours of total annotation, we obtain 7% and 17% absolute improvement in unlabeled dependency scores for English and Spanish, respectively, compared to the same parser using only universal grammar constraints.", "creator": "LaTeX with hyperref package"}}}