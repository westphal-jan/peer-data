{"id": "1611.09621", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Associative Memory Using Dictionary Learning and Expander Decoding", "abstract": "an associative memory is a framework of content - addressable memory that stores a collection of authentic message vectors ( or a dataset ) over a neural behavioral network while enabling a neurally feasible mechanism to recover any message in isolation the dataset from testing its eponymous noisy version. designing uniquely an associative memory requires addressing two main tasks : 1 ) basic learning phase : children given a dataset, learn a true concise representation of the dataset in the form of a graphical model ( or a neural network ), 2 ) recall phase : given a noisy version of a message character vector borrowed from the dataset, students output the correct correct message vector knowledge via a neurally motivated feasible algorithm over the network learnt during the learning phase. this paper studies the problem of designing a class of stationary neural associative memories which learns a network representation for a large dataset that ensures correction against a single large number of adversarial errors during probing the recall phase. specifically, the associative network memories designed embedded in this paper can store immediate dataset containing $ \\ exp ( n ) $ $ n $ - length - message vectors over a network with $ o ( n ) $ nodes and typically can tolerate $ \\ omega ( \\ frac { n } { { \\ rm polylog } n } ) $ adversarial errors. this innovative paper carries out this memory design by mapping solving the initial learning phase and recall phase to the cognitive tasks challenges of dictionary learning with a square dictionary and iterative error correction technique in an expander code, respectively.", "histories": [["v1", "Tue, 29 Nov 2016 13:27:18 GMT  (47kb,D)", "http://arxiv.org/abs/1611.09621v1", "To appear in AAAI 2017"]], "COMMENTS": "To appear in AAAI 2017", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["arya mazumdar", "ankit singh rawat"], "accepted": true, "id": "1611.09621"}, "pdf": {"name": "1611.09621.pdf", "metadata": {"source": "CRF", "title": "Associative Memory using Dictionary Learning and Expander Decoding", "authors": ["Arya Mazumdar", "Ankit Singh Rawat"], "emails": ["arya@cs.umass.edu,", "asrawat@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Associative memories aim to address a problem that naturally arises in many information processing systems: given a dataset M which consists of n-length vectors, design a mechanism to concisely store this dataset so that any future query corresponding to a noisy version of one of the vectors in the dataset can be mapped to the correct vector. An associative memory based solution to this problem is broadly required to have two key components: 1) dataset must be stored in the form of a neural network (graph) and 2) the mechanism to map a noisy query to the associated valid vector should be implementable in an iterative neurally feasible manner over the network (a neurally feasible algorithm employs only local computations at the nodes of the corresponding network based on the information obtained from their neighboring nodes). The tasks of learning the graph representation from the dataset and mapping erroneous vectors to the associated correct vectors are referred to as learning phase and recall phase, respectively.\nThe overarching goal of designing an associative memory that can store a large dataset (ideally containing exp(n) message vectors using a neural network with O(n) nodes) while ensuring robustness to a large number of errors (ideally \u2126(n) errors) during the recall phase has led to multiple research efforts in the \u2217This work was done when the author was with the Computer Science Department, Carnegie Mellon University, PA, USA.\nar X\niv :1\n61 1.\n09 62\n1v 1\n[ st\nat .M\nL ]\nliterature. The binary Hopfield networks, as studied in [13, 22], provide one of the earliest designs for the associative memories. Given a dataset containing binary vectors from {\u00b11}n, Hopfield networks learn this dataset in the form of an n-node weighted graph by employing Hebbian learning [11], i.e., the weighted adjacency matrix of the graph is defined by summing the outer products of all message vectors in the dataset. However, in their most general form, these networks suffer from small capacity. In [22], McEliece et al. show that these networks can only store O ( n\nlogn) message vectors when these messages correspond to arbitrary n-length binary vectors and the recall phase is required to tolerate linear \u2126(n) random errors. This has motivated the researchers to look at various generalizations of Hopfield networks (see, [9,16,21,23,31] and references therein). However, these solutions again fail to simultaneously achieve both large capacity and error tolerance.\nOne remedy to small capacity is to design associative memories with structural assumptions on the dataset. This approach has been considered in [8, 12, 18\u201320, 27]. In particular in [8], Gripon et al. store a dataset comprising O(n2) sparse vectors in the form of cliques in a neural network. In [12], Hillar and Tran design a Hopfield network with n nodes that can store \u223c 2 \u221a 2n/n1/4 message vectors and is robust against n/2 random errors. In [18\u201320, 27], the message vectors that need to be stored are assumed to constitute a subspace. In [18, 19, 27], the task is to learn a bipartite factor graph of the linear constraints satisfied by the dataset subspace. The error correction during recall phase is then performed by running a belief propagation algorithm [26] over the bipartite graph. In [18], Karbasi et al. work with a model where the message vectors in the dataset have overlapping sets of coordinates so that shortened vectors obtained by restricting the original message vectors to each of these overlapping sets belong to a subspace. Under this model, they design associative memories that can store exponential number (in n) of message vectors while correcting linear number (in n) of random errors during the recall phase.\nThe results in [18] hinge on the fact that the learning phase of their memory design recovers a bipartite graph which has certain desirable structural properties that are required for belief propagation type decoders to converge. However, no guarantee of recovering such a bipartite graph during the learning phase is provided in [18] even when we assume the subspace associated with the dataset has one such graphical representation to begin with. Recognizing the requirement of learning correct bipartite graph during the learning phase, Mazumdar and Rawat explore a sparse recovery based approach to design associative memories with the subspace dataset model in [20]. This approach assumes that the dataset belongs to a subspace whose orthogonal subspace has null space property, a sufficient condition for sparse signal recovery. This allows one to learn any basis for the orthogonal subspace during the learning phase and then recast the recall phase as a sparse recovery problem [4, 6]. The approach in [20] also allows for the strong error model containing adversarial errors. Specifically, [20] considers two candidate signal models which contain n-length message vectors and utilize O(n) sized neural networks to store the signals. The two models have the datasets of sizes exp(n3/4) and exp(r) with 1 \u2264 r \u2264 n, respectively. Furthermore, the designed associative memories based on these two signal models respectively allow for recovery from \u2126(n1/4) and \u2126 ( n\u2212r\nlog6 n\n) adversarial\nerrors in a neurally feasible manner. In this paper, we also follow the subspace model as in [18\u201320, 27]. We assume the dataset to form a subspace which is defined by sparse linear constraints. The model of sparse linear constraints are quite natural and less restrictive than the previous models of works such as [20]. Note that this signal model is similar to the model explored in Karbasi et al. [18]. However, our approach and contributions differ from [18], as we ensure that the learning phase provably generates the correct bipartite graph which can guarantee the error correction from a large number of errors using an iterative algorithm during the recall phase. We also note that similar to [20] we work with the stronger error model involving adversarial errors, but our scheme is superior to that of [20] in terms of storage capacity (see, Theorems 1, 3) and number of correctable adversarial errors (improvement by poly-log factors, see, Theorems 1, 2, 3). We want to point out that the main technical challenge in associative memory is not to individually design the learning or\nrecall phases, but to interface them in a way that is consistent with the operations of both phases, and to give an end-to-end performance guarantee.\nHere, we note that the problem of designing an associative memory is closely related to the well studied nearest neighbor search (NNS) problem and its relaxation approximate nearest neighbor search (A-NNS) problem [2, 14, 28, 32]. The solutions to the A-NNS problem enable one to store a dataset in such a manner that noisy versions of the vectors in a dataset (with bounded noise) can be mapped to the correct vectors. Additionally, the A-NNS solutions do not put assumptions on the dataset. However, this comes at the cost of removing the requirement of having a fast iterative or neurally feasible recall phase. Furthermore, the A-NNS solutions, especially based on locally sensitive hashing [10, 14] have large space complexity, i.e., polynomial in size of dataset. We note that the A-NNS solutions are very much aligned to the vector (image) retrieval task [7, 17, 33] which need not have a neurally feasible retrieval algorithm.\nThe rest of the paper is organized as follows. In Sec. 2, we define the dataset model considered in this paper and present the main results of this paper along with key techniques and ideas involved in establishing those results. Sec. 3 is dedicated to the proof of the main theorem. In Sec. 3.1, we describe the learning phase of the associative memory design results along with the relevant technical details. In Sec. 3.2, we present an iterative error correction algorithm which is employed during the recall phase of the designed associative memory. This analysis of the algorithm relies on the expansion properties of the bipartite graph which defines the dataset and is learnt during the learning phase. We conclude the paper with some comments on performance in Sec. 5."}, {"heading": "2 Main results and techniques", "text": ""}, {"heading": "2.1 Model for datasets", "text": "We focus on the associative memories based on the operations on R, the set of real numbers. In our first model, we consider the message patterns to be vectors over R. In the second model we comment on neural associative memories storing binary message patterns that are obtained by our approach."}, {"heading": "2.1.1 Dataset over real numbers: the sparse-sub-Gaussian model", "text": "We assume the message set to form a linear subspace defined by sparse linear constraints over R. Let M \u2286 Rn denote the set of message vectors (signals) that need to be stored on the associative memory. Let B be an m \u00d7 n matrix comprising the linear constraints that define the message setM. In particular, we have\nBx = 0 \u2200 x = (x1, x2, . . . , xn) \u2208M. (1)\nIn order to fully specify the message setM, we still need to provide a stochastic model for the matrix B. Towards this, we consider a random ensemble of sparse matrices. For each j \u2208 [n] := {1, 2, . . . , n}, we consider the following experiment. We pick d element uniformly at random with replacement from the set [m]. Let Nj denote the set comprising these randomly picked elements. For 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n, we define\n\u03bei,j = { 1 if i \u2208 Nj \u2282 [m] 0 otherwise.\n(2)\nLet { Ri,j } 1\u2264i\u2264m, 1\u2264j\u2264n be a collection of independent and identically distributed (i.i.d.) sub-Gaussian\nrandom variables. Given the random variables, { \u03bei,j , Ri,j } 1\u2264i\u2264m, 1\u2264j\u2264n, we assume that the (i, j)-th entry\nof the matrix B is defined as\nBi,j = \u03bei,jRi,j \u2208 R for 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n. (3)\nThrough out this paper, we refer to this model for the dataset to be stored on a neural associative memory as sparse-sub-Gaussian model. We work with various values of d which we specify while stating different parameters that we obtain for the designed associative memories in Sec. 2.2.\nThis model is a quite natural random model of bipartite graphs that allow for multi-edges. Indeed, consider a bipartite graph with disjoint sets of vertices [n] (variable nodes) and [m] (check nodes). There are d edges out of each variable node, being incident on uniformly and independently chosen vertices from the check nodes.\nRemark 1. The requirement on Ri,j is quite generic as it allows for many distributions. For example, we can assume that Ri,j belongs to a finite set of integers {\u2212L,\u2212L + 1, . . . ,\u22121, 1, . . . , L \u2212 1, L}. Similarly, in another setup, Ri,j can be assumed to be a Gaussian random variable."}, {"heading": "2.1.2 Binary dataset", "text": "Our model of binary dataset is same as above except for the fact that 1)M \u2286 {+1,\u22121}n, and 2) Ri,j is uniform over {+1,\u22121} in (3). The condition of (1) must be satisfied for any x \u2208M."}, {"heading": "2.2 Our main results", "text": "We establish that, for a datasetM corresponding to the null-space defined by the matrix B, the said matrix B can be exactly recovered from the dataset in polynomial time. Recall that there can be many sets of basis-vectors for the null-space ofM. Still, we claim that it is possible to accurately recover the matrix B that has been generated by the sparse-sub-Gaussian model described above.\nIt is essential for us that we recover the matrix B exactly. Being generated by the random model defined above, B exhibits certain graph expansion property that is necessary for our recall phase to be successful. This matrixB enables the error correction during the recall phase with the help of a simple iterative (neurally feasible algorithm). We summarize the parameters achieved by such memory as follows.\nTheorem 1. Suppose that c, c\u2032, c\u2032\u2032 > 0 are three constants. Let n be a large enough integer and m = c nlogn . Assume that B is an m \u00d7 n matrix generated from the sparse-sub-Gaussian model described in Sec. 2.1.1 with c\u2032 \u2264 d \u2264 c\u2032\u2032 log n, andM = {x \u2208 Rn : Bx = 0}. Then, with high probability (w.h.p.) M is an n\u2212m = n(1\u2212 c/ log n) dimensional subspace that can be stored in a neural network (learned in poly-time in the learning phase) while allowing for correct recovery from \u2126( n\nd2 log2 n ) adversarial errors during the\nrecall phase with a neurally feasible algorithm.\nThe proof of this theorem has been provided in Sec. 3. This result is obtained by utilizing a novel connection between recovering the matrix B defining the underlying datasetM and the dictionary learning problem with a square dictionary as studied in [1,3,30]. Given access to the datasetM, we can easily find a basis for the null-space ofM containingm = n\u2212dim(M) n-length vectors. LetA denote them\u00d7nmatrix which has the m vectors in this basis as its rows. Note that the row vectors of B also span the subspace orthogonal to the dataset M. Moreover, w.h.p., B is a full rank matrix. This implies that the following relationship holds w.h.p.,\nA = DB, (4)\nwhere D is an invertible m \u00d7 m matrix. Note that recovering the matrix B from A is now equivalent to dictionary learning problem [24] where n columns of A and B corresponds to n observations and the associated coefficients, respectively. Furthermore the matrix D corresponds to a square dictionary [30].\nAs for the recall phase, we rely on the observations (as shown in Sec. 3.2) that w.h.p. the bipartite graph associated with the sparse random matrix B is an expander graph. Assume that we are given a noisy version y of a valid message vector x \u2208M such that we have\ny = x + e (5)\nwhere e denotes the error vector. Recovering x from the observation y can be cast as a sparse recovery problem of recovering e from\nz = By = B(x + e) = Be.\nIf the bipartite graphs associated with B is an expander graph (which holds w.h.p.), we can solve this sparse recovery problem by an efficient and iterative algorithm [15] which is motivated by the decoding algorithm of expander codes [29] in coding theory literature.\nDue to the sample complexity requirements for efficient square-dictionary learning algorithms [1,3,30], the above model allows us to store datasets that satisfy at most O\n( n\nlogn\n) linear constraints. However if we\nallow for a learning-phase that takes quasi-polynomial time, then it is possible to store restricted datasets that satisfy m = \u0398(n) sparse-linear constraints. We summarize the result below.\nTheorem 2. Let n be a large enough integer and m = cn for a some constant c < 1/200. For a large enough constant C > 0, letB be anm\u00d7n matrix generated from the sparse-sub-Gaussian model described in Sec. 2.1.1 with d = C log n and M = {x \u2208 Rn : Bx = 0}. Then w.h.p., M is an n \u2212 m = n(1\u2212 c) dimensional subspace that can be stored in a neural network (learned in quasi-polynomial-time in the learning phase) while allowing for error correction from \u2126( n\nlog2 n ) adversarial errors during the recall\nphase with a neurally feasible algorithm.\nWhile in terms of storage capacity this theorem is inferior to that of Theorem 1, it may represent some datasets better, and has better error correction capability. While the recall phase of this algorithm works same as above, for the learning phase we can no longer rely on the dictionary-learning algorithms. Instead we do an exhaustive search over all possible sparse vectors to find out a sparse basis for the null-space ofM which end up taking a quasi-polynomial time, if we choose parameters suitable for the recall phase. We here crucially use the fact that for m = cn and d = C log n such a sparse basis is unique, which can be obtained from the results of [30]. The proof of the recall phase for this theorem remains same as that of Theorem 1.\nFinally, while both Theorems 1 and 2 have their counterparts when storing binary vectors, we present only one result for brevity. A sketch of the proof of the following theorem has been given in Sec. 4.\nTheorem 3 (Binary dataset). Suppose that c, c\u2032, c\u2032\u2032 > 0 are three constants. Let n be a large enough integer such that m = c nlogn . Assume that B is an m \u00d7 n matrix generated from the binary dataset model described in Sec. 2.1.2 with c\u2032 \u2264 d \u2264 c\u2032\u2032 log n and M = {x \u2208 {\u00b11}n : Bx = 0}. Then w.h.p., |M| = exp(n\u2212\u03b1n log(d log n)/ log n) for a constant \u03b1 andM can be stored in a neural network (learned in polynomial-time in the learning phase) while allowing for error correction from O( n\nd2 log2 n ) adversarial\nerrors during the recall phase with a neurally feasible algorithm."}, {"heading": "3 Proof of Theorem 1", "text": ""}, {"heading": "3.1 Learning phase of associative memory design", "text": "As discussed in the previous section, under the dataset model considered in this paper, the learning phase of the associative memory design can be mapped to the problem of dictionary learning with a square dictionary. The very same dictionary learning problem with slightly different random model for the coefficient vector has been studied in [1,3,30]. In Appendix A, we briefly describe this line of work along with the results that\nare used in this paper. We then utilize the dictionary learning algorithm used in [1] to exactly learn the matrix B which define our dataset and comment on the modifications required in the analysis of Adamczak [1] to obtain guarantees on the performance of this algorithm."}, {"heading": "3.1.1 Exact recovery of the matrix B", "text": "Our learning phase constitutes learning the matrix B exactly from the datasetM. Utilizing the dictionary learning algorithm from [1], we design the learning phase for an associative memory storing the message set described in Sec. 2.1. The learning phase consists of the following two steps.\n1. Given the message vectors from the datasetM, first construct a basis for the subspace orthogonal to the dataset subspaceM = {x : Bx = 0} \u2282 Rn with dim(M) = n\u2212m.\n2. Let A \u2208 Rm\u00d7n denote the basis obtained in the previous step. Since w.h.p. B is a full-rank matrix, we have\nA = DB,\nwhere D \u2208 Rm\u00d7m is a non-singular matrix. Now employ the modified ERSpUD dictionary learning algorithm [1] with the matrix A as its input. Note that the algorithm outputs candidates for the matrices D and B. The method of this square-dictionary learning and the algorithm are summarized in Appendix A.\nNext, we show that the proposed learning phase w.h.p. exactly recovers the matrix B. Note that the sparse-sub-Gaussian model used to generateB (cf. Sec. 2.1) slightly differs from the Bernoulli-sub-Gaussian model studied in [1, 30] (cf. Appendix A). In particular, for every j \u2208 [n], the distribution of the random variables {\u03bei,j : i \u2208 [m]} and {\u03b7i,j : i \u2208 [m]} is different1. However, this difference is not very crucial for the success of the learning algorithm as we still have independence among the random variables \u03bei,js which are indexed by different values of j \u2208 [n]. We formalize the exact recovery guarantees for the matrix B in the following result.\nTheorem 4. Let B \u2208 Rm\u00d7n be a matrix generated by the sparse-sub-Gaussian model (cf. Sec. 2.1) andM be the associated dataset, i.e.,M = {x : Bx = 0}. Then there exists a constant c > 0 such that whenever we have n \u2265 cm logm the two step learning phase of the associative memory as described above exactly recovers the linear constraints in B with probability at least 1\u2212 1/n.\nWe refer the reader to Appendix A.1 for the proof of Theorem 4."}, {"heading": "3.2 Recall phase of associative memory design", "text": "In this section we present an iterative algorithm which recovers the correct message vector among the dataset M from its noisy version. The noisy observation is assumed to be corrupted at adversarially chosen coordinates. The correctness of the iterative algorithm relies on the observation that the bipartite graph associated with the matrix B which defines our datasetM is a good expander graph. We first formalize this expansion property in the following result. We then present the iterative algorithm and show that it can provably tolerate \u2126 ( n\npolylogn\n) adversarial errors.\n1We focus on the sparse-sub-Gaussian model as opposed to the Bernoulli-sub-Gaussian model as the bipartite graph associated with the matrix B generated by the sparse-sub-Gaussian model is a good expander w.h.p. We utilize this fact while designing the recall phase for the proposed associative memory in Sec. 3.2."}, {"heading": "3.2.1 Expansion property of the bipartite graph defined by B", "text": "Let GB = (L = [n],R = [m], EB) be a bipartite graph where L andR denote the index sets of left and right vertices, respectively. The matrix B which defines our datasetM gives the m \u00d7 n adjacency matrix of the graph G, i.e., for ` \u2208 L and r \u2208 R, we have an edge (`, r) \u2208 EB iff Br,` 6= 0. More specifically, the weight of the edge (`, r) \u2208 EB is w`,r = Br,`. It follows from the sparse-sub-Gaussian model (cf. Sec. 2.1) which generates the random matrix B that every vertex in L has degree d and each of the d neighbors for a vertex in L are chosen uniformly at random from the set of right verticesR with replacement. The following result states that expansion properties that hold for such a graph with high probability.\nProposition 1. Assume that > 0 and d = O( nm logn). Let G = (L,R, E) be a random d-left regular graph where each of the d neighbors for a left vertex are chosen uniformly at random from the set of right vertices with replacement. Then, for a large enough n, w.h.p., G is an ( m2\nd2n , (1 \u2212 )d\n) -expander graph, where a\nbipartite graph is (t, l)-expander, if for every S \u2286 L such that |S| \u2264 t, we have |N (S)| \u2265 l|S|. Here, N (S) \u2286 R denotes the vertices inR that are neighbors of vertices in S.\nProof. Let\u2019s consider a set S \u2286 L such that |S| = s \u2264 m2 d2n . Let T \u2286 R be a set of right vertices such that |T | < (1 \u2212 )ds. The probability that N (S) \u2286 T is upper bounded by (\n(1\u2212 )ds m\n)ds . Now, taking the\nunion bound over all the sets S \u2286 L such that |S| = s and the sets T \u2286 R such that |T | < (1 \u2212 )ds, the probability Ps that the the graph G has a non-expanding set of size s, is upper bounded as follows.\nPs \u2264 ( n\ns\n)( m\n(1\u2212 )ds\n) ((1\u2212 )ds/m)ds\n\u2264 es+(1\u2212 )ds (n/s)s ((1\u2212 )ds/m) ds . (6)\nWe can rewrite (6) as,\nPs \u2264 es+(1\u2212 )ds (dn/m)s (ds/m) ds\u2212s . (7)\nNow, using our assumption that s \u2264 m2 d2n , we obtain that\nPs \u2264 es+(1\u2212 )ds (m/dn) ds\u22122s . (8)\nUsing union bound, we have that G is not an ( m2\nd2n , (1\u2212 )d\n) -expander with probability at most\nm2 d2n\u2211 s=1 Ps \u2264 m2 d2n es+(1\u2212 )ds (m dn ) ds\u22122s . (9)\nNow, for large enough n, the R.H.S. of (9) vanishes as we have mdn = O( 1 logn)"}, {"heading": "3.2.2 Iterative decoding algorithm", "text": "Remember that during the recall phase we are given an n-length observation vector y which is noisy version of one of the message vectors from the datasetM, i.e.,\ny = x + e, for some x \u2208M. (10)\nExpander decoding algorithm\nInput: The vector z = Be and the matrix B. 1: Define Nj := {i \u2208 [m] : Bi,j 6= 0} \u2200 j \u2208 [n]. 2: Initialize e\u0302 = 0. 3: if z = Be\u0302 then 4: End the decoding and output e\u0302. 5: else 6: Find an index j \u2208 [n] such that the multiset { giBi,j }i\u2208Nj has at least (1\u2212 2 )d identical elements, say\nAssuming that we have exactly learnt the m \u00d7 n matrix B during the learning phase of the associative memory (as described in Sec. 3.1), we obtain an m-length vector as follows.\nz = By = B(x + e) = Be, (11)\nwhere the last equality follows as we have x \u2208 M = {x \u2208 Rn : Bx = 0}. Note that we have reduced the problem of recovery of the correct message vector x from y to the task of recovering e from z. Assuming that the error vector e satisfies certain sparsity constraint, the latter problem is exactly the problem of recovering the sparse vector e from its linear measurements via the measurement matrix B. As shown in Proposition 1, w.h.p., the matrix B corresponds to the adjacency matrix of an expander graph. In [15], Jafarpour et al. have adapted the iterative error correction algorithm for expander codes from [29] to the problem of sparse recovery problem when the measurement matrix corresponds the adjacency matrix of a good expander graph. Here we propose to employ this iterative algorithm to recover e from z. The algorithm requires calculation of gap for each of the linear constraints defined by the matrix B (or rows of the matrix B) which we formally define below.\nDefinition 1. Let e be an error vector and z = Be. Given an estimate e\u0302 for e, for each linear constraint indexed by i \u2208 [m], we define a gap gi as follows.\ngi = zi \u2212 n\u2211 j=1 Bi,j e\u0302j . (12)\nWe describe the algorithm in Fig. 1 and present the theoretical guarantees for the performance of the algorithm from [15] as follows.\nProposition 2 ( [15]). LetB be anm\u00d7nmatrix which is the adjacency matrix for a (2k, (1\u2212 )d) expander bipartite graph with \u2264 14 . Then, given the measurement vector z = Be for any k-sparse vector e, the expander decoding algorithm (cf. Fig. 1) successfully recovers e in at most 2k iterations.\nWe now employ Proposition 2 to characterize the error correction performance of the designed associative memories during the recall phase.\nTheorem 5. Let B be the m\u00d7 n matrix generated by the sparse-sub-Gaussian model described in Sec. 2.1 andM denote the dataset associated with the matrix B. Then, with probability at least 1\u2212 o(1), the recall phase based on the iterative decoding algorithm described in Fig. 1 can correct at least m 2\n2d2n adversarial\nerrors.\nProof. It follows from Proposition 1 that with probability at least 1 \u2212 o(1), the matrix B corresponds to the adjacency matrix of an ( m2\nd2n , (1 \u2212 )d\n) -expander graph. Combining the expansion parameters for this\nexpander graph with the result in Proposition 2, we obtain that the iterative decoding algorithm (cf. Fig. 1) can recover the error vector e from z = Be as long as e has at most m 2\n2d2n non-zero coordinates. Given y\nand e, it is straightforward to obtain the correct message vector as x = y\u2212e. This completes the proof."}, {"heading": "4 Proof sketch of Theorem 3: Associative memory storing binary vectors", "text": "Since the graph defined by B is still an expander (with edge weights {+1,\u22121}), for the recall phase we rely on the same expander decoding algorithm. We just want to guarantee that |M| = |{x \u2208 {\u00b11}n : Bx = 0}| is of size about exp(n\u2212\u03b1n log(d log n)/ log n) w.h.p. The algorithm to learn B is same as that of Theorem 1.\nInstead of the random model that we have considered in Sec. 2.1.2, consider a random matrix B \u2208 {+1, 0,\u22121}m\u00d7n whose each row has independently and uniformly chosen d\u2032 nonzero ({+1,\u22121}) values. This model allows us to come up with a straight-forward analysis of number of binary vectors in the nullspace, while the original model gives the same estimate but with significantly lengthier analysis, that we omit for the interest of space. Note that d\u2032 \u223c d nm w.h.p. Now for a randomly and uniformly chosen \u00b11 vector y of length n, and for some constant c\u2032 > 0,\nP { By = 0 } =\n(( d\u2032\nd\u2032 2\n) /2d \u2032 )m \u2265 ( 1 c\u2032d\u2032 )m/2 .\nThis means E [ |M| ] \u2265 2n \u00b7 ( 1/(c\u2032d\u2032) )m/2 = 2n\u2212 m 2\nlog(c\u2032d\u2032). Substituting, m = c nlogn , we get the promised size ofM."}, {"heading": "5 Simulation results", "text": "Though our main contribution is theoretical, in this section we evaluate the proposed associative memory on synthetic dataset to verify if our methods works. Only a representative figure is presented here (Fig. 2). We consider three sets of system parameters (m,n, d) for the dataset to be stored. For each set of parameters, we first generate an m \u00d7 n random matrix B according to the sparse-sub-Gaussian model (cf. Sec. 2.1). Each non-zero entry of the matrix B is drawn uniformly at random from the set {\u00b11,\u00b12,\u00b13}. We then generate multiple message vectors which belong to the subspace orthogonal to all the rows of the matrix B and provide the learning phase with these vectors. Given these vectors we employ the dictionary learning based approach described in Sec. 3.1.1 to obtain an estimate B\u0302 for the matrix B. As guaranteed by Theorem 4, in our simulations, B\u0302 contains all the rows of the original matrix B (however, in a different order). For all three sets of parameters under consideration, we then utilize the estimate B\u0302 to evaluate the performance of the expander decoding based recall phase (cf. Sec. 3.2). For a fixed number E of errors, we generate 100 error vectors e \u2208 Rn with the number of non-zero entries in each error vector equal to E. The non-zero entries in these vectors are uniformly generated from the set {\u00b11, . . . ,\u00b14}. The positions of the non-zeros entries in each of these vectors are chosen according to a uniform random permutation on the set [n].\nStudent Version of MATLAB\nThe performance of the recall algorithm in our simulations is illustrated in Fig. 2 where we plot the fraction of incorrectly recovered error vectors as we increase the number of errors. As expected from Theorem 5, increasing d while keeping m and n fixed degrades the performance of the recall phase. On the other hand, increasing m while keeping d and the ratio mn fixed improves the performance of the recall phase.\nConcluding remarks While we use dictionary learning as a tool in the learning phase, the model of our datasets are subspace models. A large number of datasets on the other hand are also modeled by the sparse dictionary model (or union of subspaces). It is of interest to design associative memories, where the datasets are modeled as such. One other possible direction of future research would be to consider a subspace model with a mixture of sparse and dense constraints, which potentially will be inclusive of larger classes of real datasets. For such datasets, under suitable assumption on the generative model, one can potentially employ the techniques of recovering planted sparse vectors in a subspace spanned by dense random sub-Gaussian vectors [5,25] and utilize the recovered sparse constraints to design an iterative recall phase similar to the one presented in this paper. As in the case of [18], the networks (graphs) appearing in our associative memory design share some similarities with the neural networks used for classification tasks. It is an interesting problem to further explore such connections."}, {"heading": "A The modified ER-SpUD algorithm and proof of Theorem 4", "text": "In [30], Spielman et al. consider the following problem of exact dictionary learning. Let D \u2208 Rm\u00d7m be an invertible matrix also referred to as the dictionary. Given n observations\nuj = Dvj for j \u2208 [n], (13)\nthe task is to exactly learn the dictionary D and the coefficient matrix\nV = [v1,v2, . . . ,vn] \u2208 Rm\u00d7n.\nSpielman et al. assume that the coefficient vectors of the observation are randomly generate so that the entries of the coefficient matrix V are independent and identically distributed [30]. In particular, let\nVi,j = \u03b7i,jRi,j ,\nwhere \u03b7i,j \u2208 {0, 1} and Ri,j \u2208 R are independent random variables. In particular, for some constant \u03b1, they assume that\nP { \u03b7i,j = 1 } = 1\u2212 P { \u03b7i,j = 0 } = \u03b8 \u2208 [ 2 m , \u03b1\u221a m ] , (14)\nand Ri,j is a zero mean sub-Gaussian random variable such that\nE [ |Ri,j | ] \u2265 1\n10 and P\n{ |Ri,j | \u2265 t } \u2264 2 exp(\u2212t2/2).\nThis random generative model for the coefficients V is referred to as Bernoulli-sub-Gaussian model. Under the Bernoulli-sub-Gaussian model, Spielman et al. show that the dictionary learning problem is well defined. In particular, as long as n \u2265 \u2126(m logm), for an alternative representation of the observations\nU = [u1,u2, . . . ,un] = D \u2032V \u2032,\nwhereA\u2032 \u2208 Rm\u00d7m is an invertible matrix and V \u2032 \u2208 Rm\u00d7n is coefficient matrix with the per-column sparsity bounded by that of the original coefficient matrix V , we have\nD\u2032 = D\u03a0\u039b\nand V \u2032 = \u039b\u22121\u03a0V.\nHere, \u039b \u2208 Rm\u00d7m and \u03a0 \u2208 Rm\u00d7m denote a diagonal matrix and a permutation matrix, respectively. This implies that for n \u2265 \u2126(m logm), any other representation of the observations which is explained by a square dictionary and the sparsest coefficient vectors have its dictionary and coefficient matrix as some permutation and scaling of the columns and rows of the original dictionary D and the coefficient matrix V , respectively. Furthermore, Spielman et al. also present an algorithm for the exact dictionary learning problem that recovers the dictionaryD (up to scaling and permutations of the columns ofD) and the coefficient matrix V (up to scaling and permutations of the rows of V ) provided that n = O(m2 log2m) samples. Recently, Adamczak further improve the sample complexity of the dictionary learning algorithm2 to n = O(m logm) [1]. Since we rely on the dictionary learning algorithm in this paper, we describe the algorithm in Fig. 3 and present the exact recovery guarantees from [1].\n2In [1], Adamczak analyze a slight modification of the dictionary learning algorithm proposed by Spielman et al.\nModiefied ER-SpUD (DC): Exact recovery of sparsely-used dictionaries using the sum of two columns of U as constraint vectors.\nInput: n observations U = [u1,u2, . . . ,un] \u2208 Rm\u00d7n. 1: Initialize the set V = \u2205. 2: for i = 1, . . . , n\u2212 1 do 3: for j = i+ 1, . . . , n do 4: Let rij = ui + uj . 5: Solve minimizew\u2208Rm\u2016w\nTU\u20161 subject to rTijw = 1, and set sij = wTY \u2208 Rn. 6: V = V \u222a {sij} 7: end for 8: end for 9: for i = 1, . . . ,m do\n10: Repeat 11: vi \u2190 argminv\u2208V\u2016v\u20160, breaking ties arbitrarily 12: V = V\\{vi}. 13: Until rank([v1,v2, . . . ,vi]) = i. 14: end for Output: V = [v1,v2, . . . ,vm]T and D = UUT (V V T )\u22121.\nFigure 3: Description of the dictionary learning algorithm from [1].\nProposition 3. There exists absolute constants c, \u03b1 \u2208 (0,\u221e) such that if\n2 m \u2264 \u03b8 \u2264 \u03b1\u221a m\nand V follows the Bernoulli-sub-Gaussian model with parameter \u03b8, then for n \u2265 cm logm, with probability at least 1 \u2212 1/n the modified ER-SpUD algorithm (cf. Fig. 3) successfully recovers all the rows of V , i.e., multiples of all the rows of U are present among the set V .\nA.1 Proof of Theorem 4\nIn this section we highlight the proof of Theorem 4 which provides the guarantees for the exact recovery of the matrix B using the learning algorithm described in Fig. 3. In [1, Theorem 1.1], Adamczak establishes the analogue of Theorem 4 for m\u00d7n matrices generated by the Bernoulli-sub-Gaussian model (cf. Sec. A). Theorem 4 can be established by suitably modifying the analysis of Adamczak which comprises four main steps as highlighted in [1, Sec. 2.1]. Due to the small differences between the sparse-sub-Gaussian model (cf. Sec. 2.1) for the matrixB considered in this paper and the Bernoulli-sub-Gaussian model from [1], these steps continue to work after small modifications in the analysis. In the rest of this section, we demonstrate this by establishing Lemma 1 for the sparse-sub-Gaussian model which is analogue to [1, Lemma 2.4] for the Bernoulli-sub-Gaussian model. The analogue to other key lemmas from [1] can be similarly obtained.\nLet\u2019s first define the required notation. In what follows, for p \u2265 1,\n\u2016v\u2016p := ( m\u2211 i=1 vpi )1/p\ndenotes the `p-norm of the vector v \u2208 Rm. Moreover, we use Bm1 \u2282 Rm to denote the set of m-length vectors with unit `1-norm, i.e.,\nBm1 := { v \u2208 Rm : \u2016v\u20161 = 1 } .\nIn [1], Adamczak proves the following concentration result using Bernstein\u2019s inequality and Talagrand\u2019s contraction principle. Here, we restate this result as it is utilized in the proof of Lemma 1 below.\nProposition 4. [1, Proposition 2.1] Let R1, R2, . . . , Rn \u2208 Rm and \u03be1, \u03be2, . . . , \u03ben \u2208 {0, 1}m be two sets of independent random vectors. Assume that for some constant L, we have\nE [ e|Ri,j |/L ] \u2264 2 \u2200 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n. (15)\nFurthermore, assume that we have P { \u03bei,j = 1 } \u2264 \u03b8 \u2200 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n. (16)\nLet Z1, Z2, . . . , Zn \u2208 Rm be n random vectors defined as follows.\nZj = ( R1,j\u03be1,j , R2,j\u03be2,j , . . . , Rm,j\u03bem,j )T \u2200 1 \u2264 j \u2264 n. (17) Consider the random variable\nW = supv\u2208Bm1 \u2223\u2223\u2223\u2223\u2223\u2223 1n n\u2211 j=1 ( vTZj \u2212 E [ vTZj ])\u2223\u2223\u2223\u2223\u2223\u2223 . (18) Then, for some universal constant C and every q \u2265 max(2, logm), we have\n\u2016W\u2016q \u2264 C\nn\n(\u221a nq\u03b8 + q ) L (19)\nand\nP { W \u2265 Ce\nn\n(\u221a nq\u03b8 + q ) L } \u2264 e\u2212q. (20)\nBefore we proceed, we make the following simple claim about our generative model.\nClaim 1. For the random matrix ensemble generated by the sparse-sub-Gaussian model (cf. Sec. 2.1), whenever d = o(m), we have have the following\n( 1\u2212 o(1) ) d m \u2264 P { \u03bei,j = 1 } = 1\u2212 ( 1\u2212 1 m )d \u2264 d m .\nWe now present the following result which is analogue to [1, Lemma 2.4].\nLemma 1. Let S \u2286 [n] be a fixed subset of size |S| < n4 . Let X \u2208 R m\u00d7n be an m \u00d7 n matrix which is generated as follows.\n(i) For every j \u2208 S\u0304 := [n]\\S, we pick d elements uniformly at random from [m] with replacement. Let Nj \u2286 [m] denote the set of picked elements. LetRj = (R1,j , R2,j , . . . , Rm,j)T \u2208 Rm be a vector containing i.i.d. sub-Gaussian random variables and \u03bej \u2208 {0, 1}m denote the indicator vector for the set Nj \u2286 [m]. Now the j-th column of the matrixX is defined asXj = (\u03be1,jR1,j , \u03be2,jR2,j , . . . , \u03bem,jRm,j)T \u2208 Rm.\n(ii) Let s \u2264 2d. For every j \u2208 S, we pick d elements uniformly at random from the set [m + s] with replacement. Let N\u0303j \u2286 [m + s] denote the set of picked elements. We take a subset Nj = N\u0303j \u2229 [m]. LetRj = (R1,j , R2,j , . . . , Rm,j)T \u2208 Rm be a vector containing i.i.d. sub-Gaussian random variables and \u03bej \u2208 {0, 1}m denote the indicator vector for the setNj \u2286 [m]. Now the j-th column of the matrix X is defined as Xj = (\u03be1,jR1,j , \u03be2,jR2,j , . . . , \u03bem,jRm,j)T \u2208 Rm.\nLet XS denote the sub-matrix of X comprising the columns indexed by the set S \u2286 [n]. Then, with probability at least 1\u2212 n\u22128, for any v \u2208 Rm, we have\n\u2016vTX\u20161 \u2212 2\u2016vTXS\u20161 \u2265 \u2126 ( n\u00b5 \u221a \u03b8\nm\n) \u2016v\u20161, (21)\nwhere \u03b8 = dm and E [ |Ri,j | ] \u2264 \u00b5.\nProof. It follows from Proposition 4 that, with probability at least 1\u2212 n\u22128, we have that\nsup v\u2208Bm1 \u2223\u2223\u2016vTX\u20161 \u2212 E[\u2016vTX\u2016]1\u2223\u2223 \u2264 C (\u221an\u03b8 log n+ log n) \u2264 2C \u221a n\u03b8 log n\nand\nsup v\u2208Bm1 \u2223\u2223\u2016vTXS\u20161 \u2212 E[\u2016vTXS\u2016]1\u2223\u2223 \u2264 2C\u221an\u03b8 log n. This implies that for any v \u2208 Rm, we have that\n\u2016vTX\u20161 \u2265 E [ \u2016vTX\u2016 ] 1 \u2212 2C \u221a n\u03b8 log n\u2016v\u20161\nand\n\u2016vTXS\u20161 \u2264 E [ \u2016vTXS\u2016 ] 1 + 2C \u221a n\u03b8 log n\u2016v\u20161.\nCombining these two inequalities, we obtain that the following holds for each v \u2208 Rm.\n\u2016vTX\u20161 \u2212 2\u2016vTXS\u20161 \u2265 E [ \u2016vTX\u2016 ] 1 \u2212 2E [ \u2016vTXS\u2016 ] 1\n\u2212 6C \u221a n\u03b8 log n\u2016v\u20161\n= \u2211 j\u2208S E [ \u2223\u2223vTXj\u2223\u2223 ]+\u2211\nj\u2208S\u0304\nE [ \u2223\u2223vTXj\u2223\u2223 ]\u2212 2\u2211 j\u2208S E [ \u2223\u2223vTXj\u2223\u2223 ]\n\u2212 6C \u221a n\u03b8 log n\u2016v\u20161\n= \u2211 j\u2208S\u0304 E [ \u2223\u2223vTXj\u2223\u2223 ]\u2212\u2211 j\u2208S E [ \u2223\u2223vTXj\u2223\u2223 ]\n\u2212 6C \u221a n\u03b8 log n\u2016v\u20161. (22)\nWe now bound E [ \u2223\u2223vTXj\u2223\u2223 ] for j \u2208 S. Recall that all the columns of the matrix X indexed by the set S are identically distributed. Similarly, all the columns of the matrix X indexed by the set S\u0304 = [n]\\S are identically distributed. In the following, we use Z = (Z1, . . . , Zm)T and Z\u0302 = (Z\u03021, . . . , Z\u0302m)T to denote two random vectors with their distribution identical to the columns of the matrixX indexed by the set S\u0304 and S, respectively.\nE [\u2223\u2223vT Z\u0302\u2223\u2223] = E[\u2223\u2223vT (Z\u0302 + Z \u2212 Z)\u2223\u2223]\n\u2264 E [\u2223\u2223vTZ\u2223\u2223]+ E[\u2223\u2223vT (Z \u2212 Z\u0302)\u2223\u2223]\n\u2264 E [\u2223\u2223vTZ\u2223\u2223]+ \u00b5E[ m\u2211\ni=1\n|vi|Wi ] . (23)\nHere, for 1 \u2264 i \u2264 m, Wi = \u2211d l=1 Y l i denotes the sum of d indicator random variable which are defined as follows.\nY li =\n{ 1 with probability 2dm+2d 1 m\n0 with probability 1\u2212 2dm+2d 1 m .\n(24)\nCombining (23) and (24), we obtain\nE [\u2223\u2223vT Z\u0302\u2223\u2223] \u2264 E[\u2223\u2223vTZ\u2223\u2223]+ \u00b5 2d2\n(m+ 2d)m \u2016v\u20161\n\u2264 E [\u2223\u2223vTZ\u2223\u2223]+ \u00b52d2\nm2 \u2016v\u20161. (25)\nNote that we have\nE [\u2223\u2223vTXj\u2223\u2223] = E[\u2223\u2223vT Z\u0302\u2223\u2223] \u2200 j \u2208 S (26)\nE [\u2223\u2223vTXj\u2223\u2223] = E[\u2223\u2223vTZ\u2223\u2223] \u2200 j \u2208 S\u0304 = [n]\\S. (27)\nTherefore, combining (22) and (25), we obtain that for any v \u2208 Rm,\n\u2016vTX\u20161 \u2212 2\u2016vTXS\u20161 \u2265 (p\u2212 2|S|)E [\u2223\u2223vTZ\u2223\u2223]\n\u2212 |S|\u00b52d 2\nm2 \u2016v\u20161 \u2212 4C\n\u221a n\u03b8 log n\u2016v\u20161. (28)\nNow using m = c nlogn , d \u2264 c \u2032\u2032 log n and the lower bound on E [\u2223\u2223vTZ\u2223\u2223] from [1, Lemma 2.3], one can argue that\n\u2016vTX\u20161 \u2212 2\u2016vTXS\u20161 \u2265 \u2126 ( n\u00b5 \u221a \u03b8\nm\n) \u2016v\u20161. (29)"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>An associative memory is a framework of content-addressable memory that stores a collection of<lb>message vectors (or a dataset) over a neural network while enabling a neurally feasible mechanism to<lb>recover any message in the dataset from its noisy version. Designing an associative memory requires<lb>addressing two main tasks: 1) learning phase: given a dataset, learn a concise representation of the<lb>dataset in the form of a graphical model (or a neural network), 2) recall phase: given a noisy version of<lb>a message vector from the dataset, output the correct message vector via a neurally feasible algorithm<lb>over the network learnt during the learning phase. This paper studies the problem of designing a class<lb>of neural associative memories which learns a network representation for a large dataset that ensures<lb>correction against a large number of adversarial errors during the recall phase. Specifically, the associa-<lb>tive memories designed in this paper can store dataset containing exp(n) n-length message vectors over<lb>a network with O(n) nodes and can tolerate \u03a9( n<lb>polylogn ) adversarial errors. This paper carries out this<lb>memory design by mapping the learning phase and recall phase to the tasks of dictionary learning with<lb>a square dictionary and iterative error correction in an expander code, respectively.", "creator": "LaTeX with hyperref package"}}}