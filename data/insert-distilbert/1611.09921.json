{"id": "1611.09921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Less is More: Learning Prominent and Diverse Topics for Data Summarization", "abstract": "statistical topic models efficiently facilitate the exploration of large - scale data sets. many models have been developed and broadly used to summarize the semantic fluid structure in news, science, social media, and digital humanities. however, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly adequately extract representative ones that broadly cover even the content of the corpus, i. e., a few topics that serve as a good summary of the data. most existing topic models presently fit exactly the same number of topics as a user specifies, which have imposed still an unnecessary burden lifting to the users who have limited prior knowledge. we instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization. we propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics commonly extracted. with this reinforced random walk as a general process embedded in classical topic models, we obtain \\ textit { diverse topic models } that are able additionally to extract the most prominent and diverse topics from data. the inference procedures of these diverse topic models remain as simple and efficient as the classical models. experimental correlation results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal / prior knowledge of the users.", "histories": [["v1", "Tue, 29 Nov 2016 22:24:30 GMT  (6346kb,D)", "http://arxiv.org/abs/1611.09921v1", null], ["v2", "Thu, 1 Dec 2016 02:45:34 GMT  (6345kb,D)", "http://arxiv.org/abs/1611.09921v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["jian tang", "cheng li", "ming zhang", "qiaozhu mei"], "accepted": false, "id": "1611.09921"}, "pdf": {"name": "1611.09921.pdf", "metadata": {"source": "CRF", "title": "Less is More: Learning Prominent and Diverse Topics for Data Summarization", "authors": ["Jian Tang", "Ming Zhang", "Qiaozhu Mei"], "emails": ["tangjian@net.pku.edu.cn", "mzhang@net.pku.edu.cn", "qmei@umich.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Text Mining\nGeneral Terms Algorithms, Experimentation\nKeywords data summarization, topic modeling, diversity, random walk\n\u2217This study is done when the first author is visiting University of Michigan.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. xxx xxxxx Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00."}, {"heading": "1. INTRODUCTION", "text": "A huge amount of unstructured data has been continuously generated from various online information sources at an unprecedented speed, usually referred to as the \u201cbig data.\u201d A major challenge for data scientists has emerged along with this trend, which is concerned with how to facilitate the understanding and exploration of the big data.\nStatistical topic models [4], e.g., the probabilistic latent semantic analysis (PLSA) [9] and the latent Dirichlet allocation (LDA) [5], have been widely recognized as effective tools to assist the real users in understanding and exploring the data. This family of probabilistic models are designed to automatically infer the hidden themes (a.k.a. topics) that are salient in the data collection. The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].\nThese models commonly rely on a strong assumption that the user knows the actual number of topics in the data. This assumption may hold for small and restricted data sets, but becomes impractical when the data is big and when the domain is open, thus has limited the usefulness of topic models in practical data exploration tasks. Sophisticated treatments have to be applied to topic models in order to relax this assumption, which lead to various nonparametric [24] and hierarchical versions of the models [12] that are much more complicated. While these treatments successfully advanced the theoretical understanding of topic models, they have not completely solved the problem. In practice, the quality of topics extracted by nonparametric models is usually compromised, while how to find the hierarchical structure of topics remains mysterious.\nClearly, all the aforementioned challenges are due to the practice that we wanted to extract all the topics in the data. Is this, however, a necessary practice at all? We find that in many real world scenarios, the user actually wants to explore a few most prominent topics out there, instead of enumerating all possible topics in the data. Indeed, when searching for news reports about an event, although an investigator may still want to enumerate every aspect or detail of the event, an ordinary Web user only needs to read a few articles that well summarize the major aspects of the event. When exploring the literature of a new field, a researcher may start with digesting several major trends of research instead of investigating every research topic in that field. That says, different from investigation tasks where the concern is to enumerate all the topics in a data set, in most data exploration tasks the concern is to find the top K\nar X\niv :1\n61 1.\n09 92\n1v 1\n[ cs\n.L G\n] 2\n9 N\nov 2\n01 6\ntopics that reasonably summarize, or cover the entire data collection. This is analogical to text summarization tasks where a summary of a limited length is created to cover the most important points of the original document(s), or to information retrieval tasks where a limited number of results are presented to represent the big picture of content relevant to an information need. Comparing to the conventional applications of topic models, the number K in this practice does not rely on the impractical assumption about the user\u2019s prior knowledge, but rather depends on the budget or the personal need of the user (e.g., I have time to digest three research topics, or I want to know the two most representative perspectives in a debate). In practice, this number, K, can be much smaller than the actual (unknown) number of topics in the data.\nAs a good summary of the data collection, the K topics extracted should not only be meaningful and interpretable, but also cover as much content of the original collection as possible. This naturally requires the topics extracted to be the most prominent ones in the data collection, while at the meantime they should cover diverse aspects of the collection. A straightforward way to generate such a summary is to simply fit exactly K topics through a classical topic model. However, fitting only a few topics to a big data collection is likely to under-fit the data, making the extracted topics less meaningful or interpretable.\nAn alternative approach may first fit a large number of topics to the data and then pick the most important ones, e.g., the ones with the largest sizes. These topics are likely to be meaningful to the user. However, learning too many topics runs the risk of over-fitting the data, making the extracted topics either too small or too similar to each other. As a result, even the top ones may only cover partial aspects of the collection. This challenge motivates us to investigate new ways to extract the most representative, meaningful, and yet non-redundant topics from the data.\nIn this paper, we propose a reinforced random walk among a \u201csocial network\u201d of the topics that allows prominent topics to absorb tokens from similar and smaller topics. Specifically, during the inference procedure, a topic network is built to model the correlations, or interconnections among the topics. For each word token in the collection, after it is assigned to one of the topics, it is allowed to take a random walk in the topic network, and possibly transit to other topics. The probability of transition from one topic to another is initialized based on the similarity between the topics and then reinforced by the size of the target topic, i.e., the number of tokens already assigned or transited to it. During this process, the tokens belonging to the smaller topics are likely to transit to the larger and similar topics. In other words, the larger topics will absorb resource from its local neighborhood (of similar but smaller topics). While the process continues, a few prominent topics stand out from their local neighborhood and become the most representative ones in the topic network. At the meantime, these topics tend to represent different neighborhoods in the network, thus enhances the diversity among the top topics (see Figure 1).\nWe embedded this reinforced random walk process into the classical topic models PLSA and LDA, and obtained two diverse topic models DivPLSA (abbreviation for diverse PLSA) and DivLDA(abbreviation for diverse LDA). The learning procedures of the two models remain as simple and efficient as the classical ones but are able to learn the promi-\nnent and diverse topics pervading the data. Top topics extracted by the diverse topic models are not only meaningful but also serving as good summaries of the collection. Experimental results using four real-world datasets demonstrate the effectiveness of the two diverse topic models for data summarization, compared with classical topic models and alternative approaches. Organization. The rest of this paper is organized as follows. Section 2 discusses the related work. Section 3 formally defines the problem of topic modeling for data summarization. Section 4 introduces the reinforced random walk and the proposed diverse topic models. The effectiveness of the diverse topic models are empirically evaluated in Section 5, and the study is concluded in Section 6."}, {"heading": "2. RELATED WORK", "text": "Our work presents a good analogy to extractive text summarization [7], the goal of which is to construct a short summary of an individual document or multiple documents by extracting the most representative sentences in the document(s). Instead of summarizing a single document or a few documents with sentences, topic modeling is used to summarize a large document collection with latent topics pervading the collection, in which the topics serve as the role of \u201csentences\u201d in document summarization. Analogical to text summarization, the coverage of the extracted topics in the original data should be maximized and the redundancy among the extracted topics should be minimized. In other words, the extracted topics should be both prominent and diverse. The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].\nMany algorithms for enhancing the diversity of the results of a retrieval or a mining system have been proposed. In [6], Carbonell et al. proposed the Maximal Marginal Relevance (MMR) criterion for the problem of document retrieval, which aims to reduce the redundancy among the retrieved documents while preserving their relevance to the query. The MMR algorithm greedily selects the document that has the largest marginal relevance to the query, which is the relevance of the document to the query penalized by\nthe largest similarity between the document and the selected documents. In [26], a\u201csoft\u201dversion of MMR algorithm called Grasshopper based on absorbing random walk is proposed. In [17], Mei et al. proposed a unified process DivRank to generate the entire ranked list of vertices in information networks by balancing the prestige and diversity. The DivRank algorithm is built on top of the vertex-reinforced random walk, in which the transition probabilities between the vertices are reinforced by the number of visits to the target vertex. Neighboring vertices are competing resources against each other, and finally the probability mass is distributed among some diverse vertices. Our proposed reinforced random walk shares similar idea with the DivRank algorithm. Instead of reinforced by the number of visits to each vertex (topic in our case), the transition probabilities of the random walk by the word tokens are reinforced by the size of the topics. By doing this the larger topics would gradually absorb tokens from the smaller similar ones and end up with the most prominent and diverse topics in the data.\nAnother related direction is to learn diversified mixture components in area of mixture models [21, 28]. In most of the existing mixture models, the mixture components are identically drawn from a prior distribution. When the data is over-fitted, multiple mixture components are used to model one underlying group in the data, which results in many redundant mixture components. In [21], Petralia and Rao proposed a repulsive mixture prior which penalizes redundant components. Zou and Adams replaced the underlying i.i.d. prior for mixture components with a determinantal point process (DPP), which defines a probability measure over the entire set of probability distributions [28]. However, though these models are able to obtain more diverse mixture components than classical mixture models, multiple similar mixture components are still used to model one group in the data. In our proposed diverse topic models, the larger mixture components are able to fully absorb the smaller duplicated ones, and this indicates our models can reduce the number of unnecessary mixture components."}, {"heading": "3. PROBLEM DEFINITION", "text": "Statistical topical models represent and summarize the data through the discovery of hidden topics pervading the data collection. Each topic is formally defined as follows:\nDefinition 1. A topic \u03c6 is defined as a multinomial distribution over words in a vocabulary V , i.e. {p(w|\u03c6)}w\u2208V . Without loss of generality, we assume there are K topics in total in a given data collection, where K is unknown.\nIn the practice of data exploration, instead of enumerating all the topics, a more realistic need is to extract a given number of representative or prominent topics in the data. Based on these representative topics, users can have a quick understanding of the semantic structure of the data collection. Therefore, it is desirable to be able to extract a list of topics ranked according to their importance in the data. Formally, we define the problem as follows:\nDefinition 2. Given a data collection D, the problem of learning top-K topics aims to infer a list of topics {\u03c6j}j=1,\u00b7\u00b7\u00b7K ranked according to their prominence in D, where K is a number specified by the user.\nNote that different from conventional applications of topic modeling, the number K here is specified completely based\non the user\u2019s need instead of the prior knowledge of the data, and can be substantially smaller than the true number of topics K (K << K), which is unknown. To extract top-K representative topics in the data, a straightforward way is to apply a classical topic model (e.g., LDA) directly to fit exactly K topics into the data (i.e., set K = K). In this case, one would expect the inferred topics to be the most representative topics in the data. However, fitting only a few topics to the data usually results in topics that are actually a mixture of multiple topics, the coherence or interpretability of which is compromised.\nAs it is difficult for a user to find an appropriate number of topics K, an alternative way is to fit the data with a large number of topics, which usually yields semantically coherent topics. These topics can then be ranked by the proportions of the data they cover (e.g., number of word tokens assigned to each topic). Although these topics are likely to be coherence and meaningful, a large K may over-fit the data, returning many duplicated topics or smallish topics. As a result, even the top-ranked topics may have a small coverage of the original data, due to the fact that they are either too small or redundant. To provide a good summary of the data, it is desirable that the selected topics are both coherent and interpretable, and have a high coverage of the data. Formally, we have\nDefinition 3. Given a data collection D and a given integer K, the problem of topic summarization aims to find a set of K topics {\u03c6j}j=1,\u00b7\u00b7\u00b7K such that every \u03c6j is a coherent topic and the set of topics {\u03c6j} cover as much information in D as possible.\nApparently, neither of the simple approaches above is suitable for topic summarization. In next section, we introduce a novel approach, diverse topic modeling, to infer prominent and diverse topics which better summarize the data."}, {"heading": "4. DIVERSE TOPIC MODELING", "text": "In most existing topic models or mixture models, the topics or mixture components are generally i.i.d. according to a prior distribution. No restriction is placed upon the entire set of topics. This becomes problematic when the data is over-fitted, in which case each underlying theme of original data may be represented by multiple similar topics or be split into topics with a smaller granularity.\nTo tackle this problem, the diversity, or redundancy among topics must be considered. Recent work replaces the independent assumption with priors favoring diversified topics (e.g., [21, 28]), usually through a regularization term over the entire set of topics. A set that contains similar topics will be penalized. Although these treatments can infer more diverse topics than the classical models, they still make the same assumption on the number of topics, and multiple topics are still being used to represent each underlying theme.\nA more reasonable way is to merge similar topics into bigger ones, so that the top-ranked topics become more prominent and also more diverse. In this paper we propose to let the prominent topics merge with or absorb the smaller and similar topics. Naturally, we need a process under which topics can communicate with each other and compete for word tokens. If a topic absorbs all the word tokens belonging to another topic, the latter topic is completely absorbed by the former. In the following, we introduce a reinforced random walk as such a process."}, {"heading": "4.1 Reinforced Random Walk", "text": "To model how the topics are competing word tokens from each other, the relationships among the topics must be considered. Specifically, we introduce a \u201csocial\u201d network of topics, or a topic network, which is an undirected graph G = (V,E), where V is the set of topics and there is an edge e(i, j) \u2208 E between each pair of topics (i, j). The weight of e(i, j), w(i, j) is defined as the cosine similarity between the word distributions \u03c6i and \u03c6j , i.e.,\nw(i, j) = \u03c6i \u00b7 \u03c6j\n||\u03c6i||2 \u00b7 ||\u03c6j ||2 . (1)\nWe formulate the absorbing process among the topics as a random walk process by the word tokens on the topic network. Specifically, if one token belonging to one topic transits to another topic, we can say a token of this topic is absorbed by the other topic. If all the tokens belonging to one topic are absorbed by other topics, this topic dies out.\nAs we want the larger topics to gradually absorb the smaller and similar topics, the tokens belonging to the smaller topics should be more likely to transit to the larger ones. This can be achieved through reinforcing the transition probability among the topics by the size of the topics, i.e. the number of tokens belonging to the topic. Specifically, the transition probability p(i, j) from topic i to j is defined as\np(i, j) = p0(i, j)N\n\u03b3 j\nDi , (2)\nwhere p0(i, j) is the \u201corganic\u201d transition probability among the topics, Nj is the size of the topics j, \u03b3 is the parameter used to control the reinforcement intensity by the topic size, and the normalizing factor Di is calculate as\nDi = \u2211 j p0(i, j)N \u03b3 j . (3)\nThe \u201corganic\u201d transition probabilities p0(i, j) among the topics can be calculated based on the similarity between the topics. Meanwhile, it is also reasonable to assume that each token can also have some probability to stay on the current topic instead of transiting to its neighbors. In this case, tokens belonging to the large topics can still stay on the topic itself. Specifically, we define the p0(i, j) as follows:\np0(i, j) =\n{ \u03b1 w(i,j)\u2211\nj\u2032 w(i,j \u2032) , if j 6= i\n1\u2212 \u03b1, if j = i , (4)\nwhere \u03b1 is the probability of transiting to neighboring topics. The above random walk process is related to the stochastic process vertex-reinforced random walk [20] and DivRank [17], in both of which the transition probabilities among the vertices in the network are reinforced by the number of visits to the vertices. In these processes, the neighboring vertices are actually competing resources from each other. The reinforcement leads to the rich-get-richer phenomena and finally the resources are distributed across the prominent and diverse vertices in the network.\nWe embed the above random walk process into the inference processes of PLSA and LDA, and proposed two diverse topic models DivPLSA and DivLDA. During the inference processes, the topic assignment for each token is first calculated based on the EM algorithm (for PLSA) or Gibbs sampling (for LDA), and then a new topic assignment is obtained based on the reinforced random walk."}, {"heading": "4.2 Diverse Topic Models", "text": "4.2.1 DivPLSA The PLSA model assumes each document is a mixture of\ntopics with different proportions. Given a collection D, the log-likelihood of D under PLSA assumption is calculated as:\nL(D) = \u2211 d \u2211 w n(d,w) log K\u2211 k=1 \u03b8dk\u03c6kw, (5)\nwhere n(d,w) is the frequency of word w in document d, \u03b8dk is the probability of topic k in d, \u03c6kw is the probability of word w being generated by topic k and K is the number of topics specified. The parameters of the model, i.e. {\u03b8dk}, and {\u03c6kw}, are estimated by maximizing the log-likelihood. An EM algorithm is generally applied to solve the problem. In the E-step, it calculates the posterior distribution of the topic assignment of each token, i.e. p(z|d,w) based on the current model parameters. In the M-step, it updates the model parameters based on the posterior probability of topic assignments calculated in the E-step.\nWe embedded the reinforced random walk process into the E-step. For each token w, we first obtain the posterior distribution of topic assignments p(z|d,w) (Eqn. (6)). Then one-step reinforced random walk on the topic network is conducted to determine whether this topic is absorbed by other topics or staying on the current topic, which obtains the new topic assignments p(z\u0302|d,w) (Eqn. (7)). The transition probabilities among the topics P = {p(i, j)} can be periodically updated in the M-step. We summarize the detailed updating equations as below:\nE-step:\np(z = k|d,w) = \u03b8dk\u03c6kw\u2211K\nk\u2032=1 \u03b8dk\u2032\u03c6k\u2032w (6)\np(z\u0302 = k\u0302|d,w) = K\u2211 k=1 p(z = k|d,w)p(k, k\u0302) (7)\nM-step: \u03b8dk\u0302 \u221d \u2211 w n(d,w)p(z\u0302 = k\u0302|d,w) (8)\n\u03c6k\u0302w \u221d \u2211 d n(d,w)p(z\u0302 = k\u0302|d,w) (9)\nNk\u0302 = \u2211 d,w n(d,w)p(z\u0302 = k\u0302|d,w) (10)\nIn Alg. 1, we present the detailed learning procedure of the DivPLSA model. Users still need to specify the starting number of topics K, which can be strategically set very large and let the data be over-fitted. During the learning process, the small topics will be eventually absorbed by the larger ones, i.e. the sizes of those topics become 0. We can monitor the number of active topics (topics whose sizes are greater than 0) K\u2217 during the process. When K\u2217 converges, the whole algorithm stops.\n4.2.2 DivLDA The LDA model is a Bayesian treatment of the PLSA\nby placing the Dirichlet priors upon both the documenttopic and topic-word distributions, which results the model is computationally intractable. Collapsed Gibbs sampling [8] algorithm is widely used for the inference due to its simplicity and effectiveness. In Gibbs sampling, each dimension of\nAlgorithm 1: The DivPLSA model\nInput: Training data D, the starting number of topics K, the parameter to control the reinforcement intensity by the topic size \u03b3; Output: Number of diverse topics K\u2217, the word distributions of topics {\u03c6k}k=1,...,K\u2217 ; initialization: randomly initialize the document-topic and topic-word distributions; calculate the sizes of the topics and the transition probabilities among the topics according to (2); while K\u2217 no convergence do\nE-step: for each word w of each document d in {1, . . . , |D|}\n\u2022 calculate the posterior probability of topic assignments p(z|d,w) according to (6);\n\u2022 conduct one-step reinforced random walk on the topic network according to (7);\nM-step:\n\u2022 update document-topic, topic-word distributions and topic sizes according to (8), (9) and (10);\n\u2022 update the transition probability among the topics according to (2);\n\u2022 calculate the number of active topics K\u2217;\nend\nthe joint distribution is sampled alternatively based on the distribution conditioned on all the other variables. Specifically, in LDA the conditional distribution of the topic assignment zi associated with word wi is calculated via:\np(zi = k|wi = w,w\u2212i, z\u2212i) \u221d (nd,k + \u03b1k) nk,w + \u03b2\nnk + V \u03b2 . (11)\nz\u2212i represents the topic assignments of all the words excluding the current word. nd,k is the number of words assigned to the kth topic in document d; nk,w is the number of times that the word w is assigned to the kth topic; and nk is the number of tokens assigned to the kth topic, i.e., the size of topic k. For all these counts, the current token is excluded.\nIn DivLDA, instead of calculating the expectation of the topic assignments as done in the DivPLSA, sampling is used to obtain the topic assignments. For each token, we first sample the topic assignment zi based on equation (11), and then conduct one-step reinforced random walk starting from zi to obtain a new topic assignment z\u0302i according to\np(z\u0302i = k\u0302|zi = k) = p(k, k\u0302) (12)\nThe final document-topic and topic-word distributions can be estimated via:\n\u03b8dk = ndk + \u03b1k\u2211\nk\u2032(ndk\u2032 + \u03b1k\u2032) (13)\n\u03c6kw = nkw + \u03b2\nnk + V \u03b2 (14)\nThe detailed inference process is summarized in Alg. 2.\nAlgorithm 2: The DivLDA model\nInput: Training data D, the starting number of topics K, the parameter to control the reinforcement intensity by the topic size \u03b3, the total number of Gibbs sampling iterations M ; Output: Number of diverse topics K\u2217, the word distributions of topics {\u03c6k}k=1,...,K\u2217 ; initialization: randomly sample the topic assignments for all the word tokens.; calculate the sizes of the topics and the transition probabilities among the topics according to (2); while iter \u2264 M do\nfor each token w of each document d in {1, . . . , |D|}\n\u2022 for the current assignment k\u0303 of token w, decrement counts and sums: \u2212\u2212 ndk\u0303, \u2212\u2212 nk\u0303w, \u2212\u2212 nk\u0303;\n\u2022 sample the topic assignments zi = k for the token according to (11);\n\u2022 conduct one-step reinforced random walk on the topic network according to (12);\n\u2022 for the latest assignment k\u0302, increment counts and sums: + + ndk\u0302, + + nk\u0302w,+ + nk\u0302;\nupdate the transition probability among the topics according to (2); optimize the parameter ~\u03b1 according to [5];\nend"}, {"heading": "4.3 Discussion", "text": "We discuss some practical issues of the two models. Convergence. In the above two models, after plugging in the reinforced random walk process within the EM or Gibbs sampling process, there are no explicit objective functions any more. We empirically prove that both the number of active topics and the data likelihood will converge (see Figure 2(d) and 2(h)). We leave the theoretical justification of the convergence as the future work. How to set the parameter \u03b1? The parameter \u03b1 controls the transition probability to the neighbor topics or staying on itself. It takes similar effect as the step size in the gradient descent method, and hence in practice a small \u03b1 (e.g., 0.1) can be used. How to set the parameters \u03b3 and K? We empirically show that the performance of the two models are not sensitive to the parameters \u03b3 and K (See Figure 5 and 6). In practice, K can be set to be very large to let the data overfitted. \u03b3 can be usually set within [1, 2] for DivPLSA and [0.6, 1.5] for DivLDA. Scalability. Both DivPLSA and DivLDA can be easily scaled by making use of existing large-scale topic modeling techniques. The E-step of DivPLSA can be parallelized by assigning the documents to different processors or nodes. A scaled version of DivLDA can be built on top of the existing large scale LDA model, e.g., the yahoo-LDA model in [2]."}, {"heading": "5. EXPERIMENT", "text": "In this section, we move forward to evaluate the effectiveness of our proposed diverse topic models. We evaluate the performances on four real-world data sets."}, {"heading": "5.1 Datasets", "text": "4CONF. We start with a small data set, the 4CONF data set as in [16]. The data set is constructed from papers published in four conferences including KDD, SIGIR, NIPS, and WWW. Every document corresponds to an author by aggregating the titles of the author\u2019s papers. Stop words and words appearing in less than 10 documents are removed. This small data set allows us to interpret the topics intuitively and visually. 20NG. This is the widely used 20 newsgroup data set in text mining. Stop words and words appearing in less than 20 documents are removed. We sample 1,000 documents from the set as a holdout data set. WIKIPEDIA. This includes 10,000 articles randomly sampled from 4,636,797 Wikipedia articles in English. Stop words and words appearing in less than 100 documents are removed. We also hold out a sample of 1,000 articles. DBLP. The larger data set consists of all papers with abstracts in the computer science bibliography as in [23]. Stop words and words appearing in less than 50 documents are removed.\nTable 1 summarizes the statistics of all the data sets."}, {"heading": "5.2 Evaluation Metrics", "text": "To provide a good summary of the data, the topics need to be highly coherent and also provide a high coverage of the information in the original data. We introduce metrics to evaluate the semantic coherence and the coverage of the topics respectively. The quality of a topic is measured through the semantic coherence of the word distribution while the information coverage of a set of topics is measured through their predictive performance on the holdout data set, using the well-adopted perplexity metric.\nTopic Semantic Coherence. We measure the semantic quality of the topics through the semantic coherence of the topics. In [19], Newman et al. measures the semantic coherence of each topic as the average point-wise mutual information (PMI) of every word pair among the top-ranked words in the topic. Specifically, the overall semantic topic coherence of a set of topics \u03a6 = {\u03c6k}Kk=1 is calculated as:\nPMI(\u03a6) = 1\nK K\u2211 k=1\n2 N(N \u2212 1) \u2211\n1\u2264i<j\u2264N log\np(wki, wkj)\np(wki)p(wkj) , (15)\nwhere wk = (wk1, . . . , wkN ) are the words ranked at the top N positions in topic \u03c6k. p(wki, wki) is the probability that the pair of words co-occur in the same document while p(wki) is probability of a word wki appearing in a single document. The top ranked 20 (N = 20) words are used in our experiments.\nTo calculate the PMI, generally a large dataset has to be used. The entire 20NG data set (12,267 documents) and the\nentire English Wikipedia (4.6 million documents) are used in calculating the PMI for the experiments on the 20NG data set and on the WIKIPEDIA data set, respectively.\nPerplexity. We measure the information coverage of a set of topics by perplexity, which measures the predictive performance of these topics on the holdout data set. Specifically, each document wi in the held-out data set is split into two parts wi = (wi1, wi2). The likelihood of the second part of the documents wi2 (20% of the document) is calculated based on the training data D and the first 80% words of the document wi1. Specifically, we have\nPerplexity = exp { \u2212 \u2211 i log p(wi2|wi1, Dtrain)\u2211 i |wi2| }\n(16)"}, {"heading": "5.3 Algorithms for Comparison", "text": "We compare the following algorithms for selecting top-K topics for data summarization.\n\u2022 PLSA/LDA. The classical PLSA or LDA model is directly utilized to learn exactly K topics.\n\u2022 PLSA/LDA-TopK. We first train PLSA/LDA with a large number of K topics, and then pick top-K topics with the largest sizes.\n\u2022 PLSA/LDA-MMR-TopK. PLSA/LDA is used to train a large number of K topics. Then the MMR algorithm [6] is used to select top-K topics from the K topics. Although the MMR algorithm is proposed in a query-dependent setting, we adapt it to our scenario. The relevance between the query and each topic is measured as the coverage of this topic in the whole data set, and the similarity among each pair of topics is calculated as the cosine similarity of the word distributions. The best results are reported by empirically tuning the parameter \u03bb in the MMR algorithm.\n\u2022 PLSA/LDA-DivRank-TopK. PLSA/LDA is used to train a large number of K topics, and then the DivRank algorithm [17] is used to select top-K topics. In the DivRank algorithm, we treat the proportions of the topics as the preference vector, and the weight between each pair of topics is calculated as the cosine similarity of the corresponding word distributions. The best results are reported by empirically tuning the parameter \u03b1 and \u03bb in the DivRank algorithm.\n\u2022 DivPLSA/DivLDA-TopK. DivPLSA/DivLDA is applied on the data set and then the top-K topics with the largest sizes are selected."}, {"heading": "5.4 Learning Behavior of Diverse Topic Models", "text": "We start the experimental results by investigating the learning behavior of our proposed diverse topic models. We take the DivPLSA model as an example. Similar behavior of the DivLDA is observed and hence we do not include here. In Figure 2, we show how the topic assignments of the documents and words change along the iterations in the 4CONF dataset. We first build a document-word bipartite network with the edge weight as the word frequency in the document. A DrL [15] layout algorithm is applied on this bipartite network to calculate the two-dimensional coordinates for both\nPlay\nfatemah_a._alqallaf\nlearning\nweb\nretrieval\ndata\ninformation\nneural\nmining\nnetworksmodel\nsearch\nmodelsclassification analysis\nsystem\nclustering\ntext\nnetwork\napproach\nquery based\ndocument\ndetection\nsemantic\nsystems\nalgorithm\nlarge\nefficient\nevaluation\nknowledge\nautomatic\nmodelingrecognition\nalgorithms probabilistic\nadaptive\ndiscovery\nsupport\nframework\nimage\nlanguage\nbayesian\nappli ation\nselection\nvisual\nuser\nvectorfeaturefast\ndynamic structure\nTopic Distribution\nHighcharts.com\n1: 5.2 %\n2: 5.3 %\n3: 4.9 %\n4: 5.1 %\n5: 5.1 %\n6: 5.2 %\n7: 4.7 %\n8: 5.1 %\n9: 4.9 %\n10: 4.9 %11: 5.0 %\n12: 5.0 %\n13: 4.8 %\n14: 4.8 %\n15: 5.1 %\n16: 4.8 %\n17: 5.1 %\n18: 5.1 %\n19: 5.1 %\n20: 4.9 %\nDivPLSA http://localhost:8080/DocNetwork/DivPLSA.html\n1 of 1 8/18/13 9:42 AM\n(a) Iteration 1\nPlay\nlearning\nweb retrieval\ndata\ninformation\nneural\nmining\nnetworksmodel\nsearch\nmodelsclassificationanalysis\nsystem\nclustering\ntex\nnetwork\napproach\nqu ry based document\ndetection\nsemantic\nsystems\nalgorithm large\neffici nt\nevaluation\nknowledge\nautomatic\nmodelingrec gnition\nalgorithms probabilistic\nadaptive\ndiscovery support\nframework\nimage\nla guage\nbayesian applicat on\nselection\nvisual\nuser\nvectorf atu efast\ndynamic structure\nTopic Distribution\nHighcharts.com\n1: 6.5 %\n2: 4.5 %\n3: 4.9 %\n4: 3.9 %\n5: 4.6 % 6: 3.9 %\n7: 5.4 %\n8: 4.6 %\n9: 4.9 %\n10: 4.8 %11: 4.6 %\n12: 5.4 %\n13: 4.7 % 14: 4.1 %\n15: 6.0 %\n16: 5.9 %\n17: 5.9 %\n18: 7.0 %\n19: 3.4 %\n20: 5.0 %\nDivPLSA http://localhost:8080/DocNetwork/DivPLSA.html\n1 of 1 8/18/13 12:52 PM\n(b) Iteration 20\nPlay\nlearning\nweb retrieval\ndata\ninformation\nneural\nmining\nnetworksmodel\nsearch\nmodelsclassificationanalysis\nsystem\nclustering\ntext\nnetwork\napproach\nquerybased document\ndetection\nsemantic\nsystems\nalgorithm large\neffici nt\nevaluation\nknowledge\nautomatic\nmodelingrecognition\nalgorithms probabilistic\nadaptive\ndiscovery support\nframework\nimage\nla guage\nbayesian applicat on\nse ection\nvisual\nuser\nvectorfeatu efast dynamic structure\nTopic Distribution\nHighcharts.com\n1: 6.8 %\n2: 4.2 %\n3: 4.6 %\n4: 4.0 %\n5: 4.8 % 6: 3.7 % 7: 5.2 %\n8: 4.8 %\n9: 4.6 %\n10: 5.2 %11: 4.3 %\n12: 5.7 %\n13: 4.6 % 14: 3.9 %\n15: 6.2 %\n16: 5.8 %\n17: 6.1 %\n18: 7.7 %\n19: 3.4 %\n20: 4.4 %\nDivPLSA http://localhost:8080/DocNetwork/DivPLSA.html\n1 of 1 8/18/13 12:52 PM\n(c) Iteration 50\n0 50 100 150 200\n5 10\n15 20\n#iterations\n#t op\nic s\n(d) #topics v.s. #iterations\nPlay\nlearning\nweb retrieval\ndata\ninformation\nneural\nmining\nnetworksmodel\nsearch\nmodelsclassificationanalysis\nsystem\nclustering\ntex\nnetwork\napproach\nquery based\ndocument\ndetection\nsemantic\nsystems\nalgorithm\nlarge\nefficient\nevaluation\nknowledge\nautomatic\nmodelingreco nition\nalgorithms\nprobabilistic adaptive\ndiscovery support\nframework\nimage\nlanguage\nbayesian\napplicatio\nselection\nvisual\nuser\nvectofeaturefast\ndynamic structure\nTopic Distribution\nHighcharts.com\n1: 7.2 %\n2: 1.0 %\n3: 3.2 %\n4: 0.7 % 5: 2.1 %\n6: 0.3 % 7: 5.7 % 8: 2.3 % 9: 1.4 % 10: 4.2 % 11: 1.1 %\n12: 9.4 % 13: 0.2 %\n14: 0.3 %\n15: 3.4 %16: 16.7 %\n17: 21.2 %\n18: 17.8 %\n20: 1.8 %\nDivPLSA http://localhost:8080/DocNetwork/DivPLSA.html\n1 of 1 8/18/13 12:53 PM\n(e) Iteration 70\nPlay\nlearning\nweb retrieval\ndata\ninformation\nneural\nmining\nnetworksmodel\nsearch\nmodelsclassification analysis\nsystem\nclustering\ntext\nnetwork\napproach\nquery based\ndocument\ndetection\nsemantic\nsystems\nalgorithm\nlarge\nefficient\nevaluation\nknowledge\nautomatic\nmodelingrecognition\nalgorithms probabilistic\nadaptive\ndiscovery support\nframework\nimage\nlanguage\nbayesian\napplication\nselection\nvisual\nuser\nvectorfeaturefast\ndynamic\nstructure\nTopic Distribution\nHighcharts.com\n1: 5.2 %\n7: 0.6 %\n12: 14.7 %\n16: 18.9 %\n17: 37.7 %\n18: 22.9 %\nDivPLSA http://localhost:8080/DocNetwork/DivPLSA.html\n1 of 1 8/18/13 9:44 AM\n(f) Iteration 90\nPlay\nlearning\nweb retrieval\ndata\ninformation\nneural\nmining\nnetworksmodel\nsearch\nmodelsclassification analysis\nsystem\nclustering\ntext\nnetwork\napproach\nquery based\ndocument\ndetection\nsemantic\nsystems\nalgorithm\nlarge\nefficient\nevaluation\nknowledge\nautomatic\nmodelingrecognition\nalgorithms probabilistic\nadaptive\ndiscovery support\nframework\nimage\nlanguage\nbayesian\napplication\nselection\nvisual\nuser\nvect rfeaturefast\ndynamic\nstructure\nTopic Distribution\nHighcharts.com\n1: 14.2 %\n12: 16.2 %\n16: 20.5 %\n17: 26.0 %\n18: 23.0 %\nDivPLSA http://localhost:8080/DocNetwork/DivPLSA.html\n1 of 8/18/13 9:45 AM\n(g) Iteration 200\n0 50 100 150 200\n\u2212 56\n00 00\n\u2212 54\n00 00\n\u2212 52\n00 00\n\u2212 50\n00 00\n\u2212 48\n00 00\n\u2212 46\n00 00\n#iterations\nlik el\nih oo\nd\n(h) likelihood v.s. #iterations\nFigure 2: The learning behavior of DivPLSA in 4CONF dataset.\nthe documents and words. In Figure 2(a), each data point represents a document, and the most popular 100 words in the dataset are shown. Different colors represent different topics, and each document or word is assigned to its most probable topic. The size of each word is determined based on the probability of the word in the assigned topic. We expect that documents and words belonging to the same topics (colors) are likely to lie within a dense area.\nWe start with 20 topics in the DivPLSA model with \u03b1 = 0.1, \u03b3 = 1.9. In the first 50 EM iterations, the same EM iteration for PLSA is conducted and the reinforced random walk process kicks in after the 50th iteration. In the first EM iteration (Figure 2(a)), the topic assignments of all the tokens in the data set are randomly initialized and we can see that the c lors in the entire plot are totally mixed. There is no dense area taking the same color. As time goes on, dense area with the same color gradually emerges, e.g., the 20th iteration (Figure 2(b)). The results become better when reaching the 50th iteration. We can see the emergence of topics such as\u201cinformation retrieval\u201d, \u201cWeb semantic\u201d. However, we can see that too many topics are fitted into the data, and many of them are small and similar to each other. No clear topic semantic structure is discovered. After the 50th iteration, the reinforced random walk process kicks in and the smaller topics are gradually absorbed by the larger and similar ones. We can see better topical structure in 70th iterations, in which fewer than 20 topics are active. In the 200th iteration, the whole procedure stops and five active topics remain (the other 15 topics are fully absorbed) including \u201cinformation retrieval\u201d, \u201cweb\u201d, \u201cdata mining\u201d, \u201cneural network\u201d and \u201clearning, Bayesian\u201d. Recall that the dataset is collected from the four conferences \u201cSIGIR\u201d, \u201cWWW\u201d, \u201cKDD\u201d and \u201cNIPS\u201d, the five topics are a good summary of the original data. It is also interesting to notice that two diverse topics\n\u201cneural network\u201d and \u201clearning, Bayesian\u201d are discovered in the \u201cNIPS\u201d conference, which indicates there are two distinctive research communities in the machine learning area. The final list of topics is represented in Table 2.\nIn Figure 2(d), we show how the number of active topics changes along the iterations. In the first 50 iterations, the number of active topics remains 20 as the random walk process has not kicked in yet. The number of active topics begins to decrease in around the 70th iterations as the smaller topics are absorbed by the larger and similar ones, and dramatically decreases to 5 in around the 90th iterations. The number of active topics converges after 90 iterations. During the process, we can see that the absorbing process converges quite quickly, taking around 20 EM iterations.\nFigure 2(h) shows how the likelihood of the training data changes over iterations. The likelihood keeps increasing at first and starts to decrease in around the 70th iteration, as the number of active topics decreases. In around the 90 iterations, the likelihood stops decreasing as the number of active topics converges. After this, the likelihood keeps increasing and converges in around the 150th iterations.\nOverall, we can see that when the data is over-fitted, the inferred topics by the classical topic models tend to be small and duplicated, which is not good for data summarization purpose. The reinforced random walk is able to merge the\nsimilar topics and end up with some large diverse topics. Both the number of topics and the likelihood of the data will finally converge."}, {"heading": "5.5 Evaluation for Data Summarization", "text": "Next, we move forward to evaluate the performances of different algorithms for the task of topic summarization, which aims to learn and select top-K topics for summarizing the data. The performances are evaluated in terms of the semantic coherence and information coverage of the topics, which are measured by PMI and perplexity respectively.\nThe results on the 20NG dataset are presented in Figure 3. Figure 3(a) presents the information coverage of the topics learned by various algorithms in terms of perplexity. The lower the perplexity, the larger the information coverage. Overall, the more topics are selected, the higher the information coverage. All the methods except PLSA/LDA are trained with 50 topics (the starting number of topics is also 50 for DivPLSA/DivLDA). The PLSA-TopK algorithm, which selects the largest top-K topics in the 50 topics trained by the PLSA model, achieves the worst performance. In this case, the data is over-fitted and the inferred topics are small and similar to each other, which would have a low information coverage of the data. Both PLSA-MMR-TopK and PLSA-DivRank-TopK outperform PLSA-TopK by selecting the non-redundant largest top-K topics. LDA-TopK achieves better performance than PLSA-TopK, which may due to the fact that the largest topics learned by LDA is larger than the ones learned by PLSA through an examination of the sizes of the largest topics learned by the two models. Similarly, LDA-MMR-TopK and LDA-DivRank-TopK further improve the information coverage by taking into the diversity among the selected topics into consideration.\nAs there are 20 categories in the 20NG dataset, it is reasonable to think there are 20 topics in the dataset. Therefore, we trained PLSA/LDA with 20 topics and the topics with the largest size are selected (marked as PLSA/LDA20-TopK). We can see that the topics selected from 20 topics (PLSA/LDA-20-TopK) has a high information coverage than from 50 topics (PLSA/LDA-TopK). This indicates that if we can have a good estimation of the appropriate number of topics in the data, it is likely to infer better topics with a high information coverage. Our DivPLSA and DivLDA models (also starting from 50 topics) outperform all these models by learning the most prominent and diverse topics without worrying about estimating the appropriate number of topics. Though starting from a large number of topics,\nthe DivPLSA/DivLDA model is able to merge those similar topics and end up with the diverse ones. We are also surprised to see that deploying PLSA/LDA to train just K topics (denoted as PLSA/LDA) obtains the best performance in terms of information coverage. However, though the discovered topics have a high information coverage in terms of perplexity, the semantic coherence of the topics is pretty low and the topics are not interpretable to users.\nIn Figure 3(b), we compare the semantic coherence of the topics inferred by different algorithms. Though the K topics directly learned by PLSA/LDA have a high information coverage, the semantic coherence of the topics is the worst. This is because the data is under-fitted and the inferred topics are a mixture of multiple topics. Therefore, the topics are more likely to be the background topic of the whole data collection (See the first row in Figure 3(c)) and do not have any semantic information. The semantic quality of the top-K topics selected from 50 topics learned by PLSA using various ways (PLSA-TopK, PLSA-MMR-TopK, PLSADivRank-TopK) are the best among all the models. This may due to the granularity of these topics is quite small1, which can be examined from the second row in Figure 3(c). The topics learned by the DivPLSA and DivLDA have a reasonable good semantic quality meanwhile have a high information coverage, both of which are desirable for a good summarization.\nFigure 4 compares the performance on the WIKIPEDIA data set. To select an appropriate number of topics within the data, we vary different numbers of topics and choose the best one 100 based on the predictive performance on the holdout data set. All the models (except PLSA and LDA directly trained with a few topics) are trained with 100 topics. Similar behavior of different models can be observed in the data set as in the 20NG data set.\nTo summarize, our DivPLSA and DivLDA models give a good summarization of the data by presenting top-K topics with a high information coverage and a reasonable good semantic quality. The K topics directly trained with PLSA/LDA are not interpretable and cannot convey useful semantic information to users at all. The top-K topics selected from a large number of topics trained by PLSA/LDA have good semantic quality but low information coverage."}, {"heading": "5.6 Parameter Sensitivity", "text": "In this part, we investigate the performance sensitivity of\n1Topics with a small granularity tend to yield a larger PMI\nDivPLSA and DivLDA for data summarization w.r.t the parameters \u03b3 and K. The parameter \u03b3 controls the intensity of reinforcement by topic size on the transition probabilities among the topics. The larger \u03b3 is, the more likely the larger topics will absorb the smaller topics. Therefore, the model will return fewer but larger topics with a larger \u03b3, which tend to have a higher information coverage and lower semantic coherence. This can be observed from the results shown in Figure 5, in which the performances w.r.t \u03b3 for both DivPLSA and DivLDA are presented. We can see that overall the performances are not sensitive to \u03b3. Note that DivLDA tends to use a smaller \u03b3 than DivPLSA, this may due to the Gibbs sampling algorithm and the use of hyperparameter ~\u03b1, which is periodically optimized.\nThe performance sensitivities w.r.t K for both DivPLSA\nand DivLDA in terms of perplexity and semantic coherence are presented in Figure 6. We can see that the performances are also not sensitive to K."}, {"heading": "5.7 Summarization for DBLP", "text": "Finally, we give a summarization for the entire DBLP dataset with our diverse topic models. The DivPLSA model is applied on the dataset with K = 100 and \u03b3 = 1.1, and end up with 36 topics. Table 3 shows the most prominent 10 topics in the dataset."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed two diverse topic models DivPLSA and DivLDA to learn the prominent and diverse\ntopics for data summarization. The two models are built on top of a reinforced random walk on the topic network, which allows the prominent topics to absorb tokens from smaller and similar topics and improves the diversity among the extracted topics. The inference procedures for the two models remain as simple and efficient as the classical ones and are appropriate for big data analysis. Experiments on four real-world datasets prove the effectiveness of the two models for data summarization.\nThe future work are two-fold. First, we plan to investigate the theoretical convergence of the two diverse topic models. Currently the convergence of the two models is empirically proved through the likelihood of the training data and the number of active topics. We believe there is an underlying objective function which tradeoffs between the data likelihood and the diversity among the topics. Second, we plan to apply the reinforced random walk into more scenarios such as unsupervised clustering, which will result in prominent and diverse clusters in the data."}, {"heading": "7. REFERENCES", "text": "[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.\nDiversifying search results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 5\u201314. ACM, 2009.\n[2] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable inference in latent variable models. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 123\u2013132. ACM, 2012. [3] A. Ahmed, Y. Low, M. Aly, V. Josifovski, and A. J. Smola. Scalable distributed inference of dynamic user interests for behavioral targeting. In KDD, pages 114\u2013122, 2011. [4] D. M. Blei. Probabilistic topic models. Communications of the ACM, 55(4):77\u201384, 2012. [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022, 2003. [6] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335\u2013336. ACM, 1998. [7] G. Erkan and D. R. Radev. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457\u2013479, 2004. [8] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National academy of Sciences of the United States of America, 101(Suppl 1):5228\u20135235, 2004. [9] T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289\u2013296. Morgan Kaufmann Publishers Inc., 1999.\n[10] A. Karandikar. Clustering short status messages: A topic model based approach. PhD thesis, University of Maryland,\n2010. [11] S. Lacoste-Julien, F. Sha, and M. I. Jordan. Disclda:\nDiscriminative learning for dimensionality reduction and classification. In Advances in neural information processing systems, pages 897\u2013904, 2008.\n[12] W. Li and A. McCallum. Pachinko allocation: Dag-structured mixture models of topic correlations. In Proceedings of the 23rd international conference on Machine learning, pages 577\u2013584. ACM, 2006. [13] C. Lin and Y. He. Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 375\u2013384. ACM, 2009. [14] H. Ma, M. R. Lyu, and I. King. Diversifying query suggestion results. In Proc. of AAAI, volume 10, 2010. [15] S. Martin, W. Brown, R. Klavans, and K. Boyack. Drl: Distributed recursive (graph) layout. SAND2008-2936J: Sandia National Laboratories, 2008. [16] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic modeling with network regularization. In Proceedings of the 17th international conference on World Wide Web, pages 101\u2013110. ACM, 2008. [17] Q. Mei, J. Guo, and D. Radev. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1009\u20131018. ACM, 2010. [18] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171\u2013180. ACM, 2007. [19] D. Newman, J. H. Lau, K. Grieser, and T. Baldwin. Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100\u2013108. Association for Computational Linguistics, 2010. [20] R. Pemantle. Vertex-reinforced random walk. Probability Theory and Related Fields, 92(1):117\u2013136, 1992. [21] F. Petralia, V. Rao, and D. B. Dunson. Repulsive mixtures. In NIPS, pages 1898\u20131906, 2012. [22] D. Ramage, P. Heymann, C. D. Manning, and H. Garcia-Molina. Clustering the tagged web. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 54\u201363. ACM, 2009. [23] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: Extraction and mining of academic social networks. In KDD\u201908, pages 990\u2013998, 2008. [24] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical dirichlet processes. Journal of the american statistical association, 101(476), 2006. [25] J. Zhu, A. Ahmed, and E. P. Xing. Medlda: maximum margin supervised topic models for regression and classification. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1257\u20131264. ACM, 2009. [26] X. Zhu, A. B. Goldberg, J. Van Gael, and D. Andrzejewski.\nImproving diversity in ranking using absorbing random walks. In HLT-NAACL, pages 97\u2013104, 2007.\n[27] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and G. Lausen. Improving recommendation lists through topic diversification. In Proceedings of the 14th international conference on World Wide Web, pages 22\u201332. ACM, 2005. [28] J. Zou and R. Adams. Priors for diversity in generative latent variable models. In Advances in Neural Information Processing Systems 25, pages 3005\u20133013, 2012."}], "references": [{"title": "Diversifying search results", "author": ["R. Agrawal", "S. Gollapudi", "A. Halverson", "S. Ieong"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "In Proceedings of the fifth ACM international conference on Web search and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Scalable distributed inference of dynamic user interests for behavioral targeting", "author": ["A. Ahmed", "Y. Low", "M. Aly", "V. Josifovski", "A.J. Smola"], "venue": "In KDD,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National academy of Sciences of the United States of America,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Clustering short status messages: A topic model based approach", "author": ["A. Karandikar"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Pachinko allocation: Dag-structured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["C. Lin", "Y. He"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Diversifying query suggestion results", "author": ["H. Ma", "M.R. Lyu", "I. King"], "venue": "In Proc. of AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Drl: Distributed recursive (graph", "author": ["S. Martin", "W. Brown", "R. Klavans", "K. Boyack"], "venue": "layout. SAND2008-2936J: Sandia National Laboratories,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Topic modeling with network regularization", "author": ["Q. Mei", "D. Cai", "D. Zhang", "C. Zhai"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Divrank: the interplay of prestige and diversity in information networks", "author": ["Q. Mei", "J. Guo", "D. Radev"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Topic sentiment mixture: modeling facets and opinions in weblogs", "author": ["Q. Mei", "X. Ling", "M. Wondra", "H. Su", "C. Zhai"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100\u2013108", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Vertex-reinforced random walk", "author": ["R. Pemantle"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Repulsive mixtures", "author": ["F. Petralia", "V. Rao", "D.B. Dunson"], "venue": "In NIPS, pages 1898\u20131906,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Clustering the tagged web", "author": ["D. Ramage", "P. Heymann", "C.D. Manning", "H. Garcia-Molina"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Arnetminer: Extraction and mining of academic social networks", "author": ["J. Tang", "J. Zhang", "L. Yao", "J. Li", "L. Zhang", "Z. Su"], "venue": "In KDD\u201908,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the american statistical association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Medlda: maximum margin supervised topic models for regression and classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Improving diversity in ranking using absorbing random walks", "author": ["X. Zhu", "A.B. Goldberg", "J. Van Gael", "D. Andrzejewski"], "venue": "In HLT-NAACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Improving recommendation lists through topic diversification", "author": ["C.-N. Ziegler", "S.M. McNee", "J.A. Konstan", "G. Lausen"], "venue": "In Proceedings of the 14th international conference on World Wide Web,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Priors for diversity in generative latent variable models", "author": ["J. Zou", "R. Adams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Statistical topic models [4], e.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": ", the probabilistic latent semantic analysis (PLSA) [9] and the latent Dirichlet allocation (LDA) [5], have been widely recognized as effective tools to assist the real users in understanding and exploring the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": ", the probabilistic latent semantic analysis (PLSA) [9] and the latent Dirichlet allocation (LDA) [5], have been widely recognized as effective tools to assist the real users in understanding and exploring the data.", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 96, "endOffset": 104}, {"referenceID": 24, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 96, "endOffset": 104}, {"referenceID": 9, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 117, "endOffset": 125}, {"referenceID": 21, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 146, "endOffset": 154}, {"referenceID": 12, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 146, "endOffset": 154}, {"referenceID": 2, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 23, "context": "Sophisticated treatments have to be applied to topic models in order to relax this assumption, which lead to various nonparametric [24] and hierarchical versions of the models [12] that are much more complicated.", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Sophisticated treatments have to be applied to topic models in order to relax this assumption, which lead to various nonparametric [24] and hierarchical versions of the models [12] that are much more complicated.", "startOffset": 176, "endOffset": 180}, {"referenceID": 6, "context": "RELATED WORK Our work presents a good analogy to extractive text summarization [7], the goal of which is to construct a short summary of an individual document or multiple documents by extracting the most representative sentences in the document(s).", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 166, "endOffset": 169}, {"referenceID": 13, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 188, "endOffset": 192}, {"referenceID": 16, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 226, "endOffset": 230}, {"referenceID": 26, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 256, "endOffset": 260}, {"referenceID": 5, "context": "In [6], Carbonell et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "In [26], a\u201csoft\u201dversion of MMR algorithm called Grasshopper based on absorbing random walk is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], Mei et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Another related direction is to learn diversified mixture components in area of mixture models [21, 28].", "startOffset": 95, "endOffset": 103}, {"referenceID": 27, "context": "Another related direction is to learn diversified mixture components in area of mixture models [21, 28].", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "In [21], Petralia and Rao proposed a repulsive mixture prior which penalizes redundant components.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "prior for mixture components with a determinantal point process (DPP), which defines a probability measure over the entire set of probability distributions [28].", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": ", [21, 28]), usually through a regularization term over the entire set of topics.", "startOffset": 2, "endOffset": 10}, {"referenceID": 27, "context": ", [21, 28]), usually through a regularization term over the entire set of topics.", "startOffset": 2, "endOffset": 10}, {"referenceID": 19, "context": "The above random walk process is related to the stochastic process vertex-reinforced random walk [20] and DivRank [17], in both of which the transition probabilities among the vertices in the network are reinforced by the number of visits to the vertices.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "The above random walk process is related to the stochastic process vertex-reinforced random walk [20] and DivRank [17], in both of which the transition probabilities among the vertices in the network are reinforced by the number of visits to the vertices.", "startOffset": 114, "endOffset": 118}, {"referenceID": 7, "context": "Collapsed Gibbs sampling [8] algorithm is widely used for the inference due to its simplicity and effectiveness.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "\u2022 for the latest assignment k\u0302, increment counts and sums: + + ndk\u0302, + + nk\u0302w,+ + nk\u0302; update the transition probability among the topics according to (2); optimize the parameter ~ \u03b1 according to [5]; end", "startOffset": 196, "endOffset": 199}, {"referenceID": 0, "context": "\u03b3 can be usually set within [1, 2] for DivPLSA and [0.", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": "\u03b3 can be usually set within [1, 2] for DivPLSA and [0.", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": ", the yahoo-LDA model in [2].", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "We start with a small data set, the 4CONF data set as in [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The larger data set consists of all papers with abstracts in the computer science bibliography as in [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "In [19], Newman et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Then the MMR algorithm [6] is used to select top-K topics from the K topics.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "PLSA/LDA is used to train a large number of K topics, and then the DivRank algorithm [17] is used to select top-K topics.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "A DrL [15] layout algorithm is applied on this bipartite network to calculate the two-dimensional coordinates for both", "startOffset": 6, "endOffset": 10}], "year": 2016, "abstractText": "Statistical topic models efficiently facilitate the exploration of large-scale data sets. Many models have been developed and broadly used to summarize the semantic structure in news, science, social media, and digital humanities. However, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly extract representative ones that broadly cover the content of the corpus, i.e., a few topics that serve as a good summary of the data. Most existing topic models fit exactly the same number of topics as a user specifies, which have imposed an unnecessary burden to the users who have limited prior knowledge. We instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization. We propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics extracted. With this reinforced random walk as a general process embedded in classical topic models, we obtain diverse topic models that are able to extract the most prominent and diverse topics from data. The inference procedures of these diverse topic models remain as simple and efficient as the classical models. Experimental results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal prior knowledge of the users.", "creator": "LaTeX with hyperref package"}}}