{"id": "1612.00212", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Training Bit Fully Convolutional Network for Fast Semantic Segmentation", "abstract": "wired fully embedded convolutional neural networks give accurate, per - pixel prediction for input images and have applications simple like semantic segmentation. however, a typical fcn pipeline usually requires lots of floating point computation components and large run - time memory, which effectively limits its usability. we propose a method to train bit fully convolution network ( cf bfcn ), a fully convolutional neural network that has low bit - width weights and activations. because most of its computation - intensive convolutions are accomplished precisely between low bit - width numbers, a bfcn can be accelerated by forming an efficient compiler bit - convolution implementation. on cpu, efficiently the overlapping dot product operation between two bit vectors can be reduced back to bitwise operations and include popcounts, which can efficiently offer much higher read throughput than 32 - bit multiplications independently and add additions.", "histories": [["v1", "Thu, 1 Dec 2016 11:56:15 GMT  (1730kb,D)", "http://arxiv.org/abs/1612.00212v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["he wen", "shuchang zhou", "zhe liang", "yuxiang zhang", "dieqiao feng", "xinyu zhou", "cong yao"], "accepted": false, "id": "1612.00212"}, "pdf": {"name": "1612.00212.pdf", "metadata": {"source": "CRF", "title": "Training Bit Fully Convolutional Network for Fast Semantic Segmentation", "authors": ["He Wen", "Shuchang Zhou", "Zhe Liang", "Yuxiang Zhang", "Cong Yao"], "emails": ["yaocong}@megvii.com"], "sections": [{"heading": "Introduction", "text": "Deep convolutional neural networks (DCNN), with its recent progress, has considerably changed the landscape of computer vision (Krizhevsky, Sutskever, and Hinton 2012) and many other fields.\nTo achieve close to state-of-the-art performance, a DCNN usually has a lot of parameters and high computational complexity, which may easily overwhelm resource capability of embedded devices. Substantial research efforts have been invested in speeding up DCNNs on both general-purpose (Vanhoucke, Senior, and Mao 2011; Gong et al. 2014; Han et al. 2015) and specialized computer hardware (Farabet et al. 2009; Farabet et al. 2011; Pham et al. 2012; Chen et al. 2014b; Chen et al. 2014c; Zhang et al. 2015a).\nRecent progress in using low bit-width networks has considerably reduced parameter storage size and computation burden by using 1-bit weight and low bit-width activations. In particular, in BNN (Kim and Smaragdis 2016) and XNOR-net (Rastegari et al. 2016), during the forward pass the most computationally expensive convolutions can\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbe done by combining xnor and popcount operations, thanks to the following equivalence when x and y are bit vectors:\nn\u2211 i xiyi = n\u2212 2 popcount(xnor(xi, yi)), xi, yi \u2208 {\u22121, 1},\u2200i.\nSpecifically, an FPGA implementation of neural network can take more benefit from low bit-width computation, because the complexity of a multiplier is proportional to the square of bit-widths.\nHowever, most of previous researches on low bit-width networks have been focused on classification networks. In this paper, we are concerned with fully convolutional networks (FCN), which can be thought of as performing pixelwise classification of the input images and have applications in tasks like semantic segmentation (Long, Shelhamer, and Darrell 2015). Techniques developed in this paper can also be applied to other variants like RPN (Ren et al. 2015), FCLN (Johnson, Karpathy, and Fei-Fei 2015) and Densebox (Huang et al. 2015). Compared to a typical classification network, the following properties of FCN make it a better candidate to apply low bit-width quantizations.\n1. An FCN typically has large feature maps, and some of them may need to be stored for later combination, which pushes up its peak memory usage. As BFCN uses low bit-width feature maps, the peak memory usage is significantly reduced.\n2. An FCN usually accepts large input image and taps into a powerful classification network like VGGNet (Simonyan and Zisserman 2014) or ResNet (He et al. 2015) to boost performance. The acceleration offered by exploiting bitconvolution kernel, together with memory savings, would\nar X\niv :1\n61 2.\n00 21\n2v 1\n[ cs\n.C V\n] 1\nD ec\n2 01\n6\nallow BFCN to be run on devices with limited computation resources.\nConsidering the method of training a low bit-wdith network is still under exploration, it remains a challenge to find a way to train a BFCN efficiently as well.\nOur paper makes the following contributions:\n1. We propose BFCN, an FCN that has low bit-width weights and activations, which is an extension to the combination of methods from Binarized Neural Network (Courbariaux, Bengio, and David 2014), XNOR-net (Rastegari et al. 2016) and DoReFa-net (Zhou et al. 2016).\n2. We replace the convolutional filter in reconstruction with residual blocks to better suit the need of low bit-width network. We also propose a novel bit-width decay method to train BFCN with better performance. In our experiment, 2-bit BFCN with residual reconstruction and linear bit-width decay achieves a 67.0% mean intersection-overunion score, which is 7.4% better than the vanilla variant.\n3. Based on an ImageNet pretrained ResNet-50 with bounded weights and activations, we train a semantic segmentation network with 2-bit weights and activations except for the first layer, and achieves a mean IoU score of 67.0% on PASCAL VOC 2012 (Everingham et al. 2015) and 60.3% on Cityscapes (Cordts et al. 2016), both on validation set as shown in 1. For comparison, the baseline full-precision model is 69.8% and 62.1% respectively. Our network can run at 5x speed on CPU compared to full-precision, and can be implemented on FPGA with only few percents resource consumption."}, {"heading": "Related Work", "text": "Semantic segmentation helps computer to understand the structure of images, and usually serves as a basis of other computer vision applications. Recent state-of-the-art networks for semantic segmentation are mostly fully convolutional networks (Long, Shelhamer, and Darrell 2015) and adopt the architecture of encoder-decoder with multi-stage refinement (Badrinarayanan, Handa, and Cipolla 2015). In order to achieve best performance, powerful classification models are often embedded as part of the FCNs, which pushes up computational complexity together with large decoders.\nTo further refine the results from neural networks, CRFs are widely used in post-processing to improve local predictions (Chen et al. 2014a) by reconstructing boundaries more accurately. Since CRF can be integrated with most methods as post-processing step, which contributes little to our main topic, it will not be discussed in this paper.\nRecent success of residual network has shown that very deep networks can be trained efficiently and performs better than any other previous network. There also exists successful attempt (Wu, Shen, and Hengel 2016) to combine FCN and ResNet, which achieves considerable improvement in semantic segmentation.\nTo utilize scene-parsing network in low-latency or realtime application, the computational complexity need to be significantly reduced. Some methods (Paszke et al. 2016;\nKim et al. 2016) are proposed to reduce demand of computation resources of FCN by simplifying or redesigning the architecture of network.\nWe also note that our low bit-width method can be integrated with almost all other speed-up methods and achieves even further acceleration. For example, low-rank approaches (Zhang et al. 2015b) is orthogonal to our approach and may be integrated to BFCN."}, {"heading": "Method", "text": "In this section we first introduce the design of our bit fully convolutional network, and then propose our method for training a BFCN."}, {"heading": "Network design", "text": "A standard approach to perform semantic segmentation includes a feature extractor to produce feature maps from input image, and convolutions with upsampling operations to predict per-pixel labels from those feature maps. We use ResNet as feature extractor and adopt the multi-resolution reconstruction structure from Laplacian Reconstruction and Refinement (Ghiasi and Fowlkes 2016) to perform per-pixel classification over feature maps in different scales (see Figure 1).\nHowever, while the network works well in full-precision, we observe a great loss in accuracy while converting it to low bit-width, indicating that this architecture is not suitable for low bit-width network. To address this issue, we evaluate different variations of BFCN, so as to figure out the cause of performance degeneration. As shown in Table 2, low bit-width network with single convolution in reconstruction structure suffer great performance degeneration. We also discover that adding more channels in reconstruction filter helps improve performance considerably, indicating a low bit-width convolution is not enough to extract spatial context from feature maps. In short, we need a more low bit-width friendly architecture in reconstruction to eliminate the bottleneck.\nIntuitively, we may add more channels to the filter in reconstruction. But it also pushes up computational complexity a lot. Fortunately, ResNet (He et al. 2015) allows us to go deeper instead of wider. It has been shown that a deeper residual block can often performs better than a wider convolution. Therefore, we address the issue by replacing the\nlinear convolution with residual blocks. As shown in Table 2, our residual block variant even outperforms the original full-precision network.\nIn our approach, residual reconstruction structure can not only achieve better performance with similar complexity to a wide convolution, but also accelerate training by reduce the length of shortest path in reconstruction."}, {"heading": "Bit-width allocation", "text": "It is important to decide how many bits to allocate for weights and feature maps, because bit-width has a crucial impact on both performance and computational complexity of a network. Since our goal is to speed-up semantic segmentation networks without losing much performance, we need to allocate bit-width carefully and wisely.\nFirst we note it has been observed (Gupta et al. 2015) that 8-bit fixed point quantization is enough for a network to achieve almost the same performance as 32-bit floating point counterpart. Therefore, we focus our attention to bitwidths less than eight, which can provide us with further acceleration.\nIn order to extend bit-convolution kernel to m-bit weights and n-bit feature maps, we notice that:\nA\u00d7B =(A0 + 2A1 + ...+ 2mAm)(B0 + ...+ 2nBn) =A0B0 + ...+ 2 i+jAiBj + ...+ 2 m+nAmBn\nwhere Ai, Bi represent the i-th bit of A and B. Therefore, it is pretty straightforward to compute the dot product using m \u00b7 n bit-convolution kernels for m-bit weights and n-bit feature maps. It shows that the complexity of bit-width allocation, which is our primary goal to optimize, is proportional to the product of bit-widths allocated to weights and activations. Specifically, bit-width allocation becomes vital on FPGA implementation since it is the direct restriction of network size.\nWith fixed product of bit-widths, we still need to allocate bits between weights and activations. Intuitively, we would\nallocate bits equally as it keeps a balance between weights and activations, and error analysis confirms this intuition.\nWe first note that the error of a number introduced by k-bit quantization is 1/2k. As the errors are accumulated mainly by multiplication in convolution, it can be estimated as follow:\nE = 1\n2kW +\n1\n2kA =\n2kW + 2kW\n2kW+kA (1)\nWhen c = kW \u00b7 kA is constant, we have the following inequality:\nE \u2265 2\u00d7 2 \u221a c\n22 \u221a c\n(2)\nThe equality holds iff kW = kA = \u221a c, thus a balanced bit-width allocation is needed so as to minimize errors. For the first layer, since the input image is 8-bit, we also fix bit-width of weights to 8. The bit-width of activations is still the same as other layers.\nRoute to low bit-width\nAs shown in Figure 2, there are two ways to adapt the procedure of training a full-precision fully convolutional network to produce BFCN, denote as P1 and P2.\nThe only difference between P1 and P2 is the initialization. P1 uses full-precision FCN as initialization while\nP2 uses low bit-width classification network. Here fullprecision FCN serves as a intermediate stage in the procedure of training.\nWe evaluate the two routes and find the former one performs significantly better as the mean IoU scores indicate in Table 3. We then add one more intermediate stage to the procedure, the 8-bit BFCN, and achieve a slightly better result. We conjecture that utilizing intermediate network helps to preserve more information in the process of converting to low bit-width."}, {"heading": "Bit-width decay", "text": "We notice that cutting off bit-width directly from fullprecision to very low bit-width will lead to significant performance drop. To support this observation, we perform a simple experiment by training a 2-bit network initialized by a pretrained network of different number of bits. The training process (Figure 3) shows that networks initialized from lower bit-width converge faster.\nThis phenomenon can be explained by looking at the errors in quantization. Obviously, with higher original precision, a quantization step introduced larger error, and as as result the model benefit less from the initialization. However, introducing intermediate stages can help resolve it since networks with closer bit-widths tend to be more similar, hence more noise-tolerant when cutting off bit-width.\nOur experiments show that BFCN can not recover from the quantization loss very well, if directly initialized from full-precision models. To extend the idea of utilizing intermediate models during training low bit-width network, we add more intermediate steps to train BFCN. We propose a method called bit-width decay, which cuts off bit-width stepby-step to avoid the overwhelming quantization error caused by large numeric precision drop.\nWe detail the procedure of bit-width decay method as follow:\n1. Pretrain a full-precision network N1.\n2. Quantize N1 to produce N2 in 8-bit, which has been proved to be lossless, and fine-tune until its convergence.\n3. Initialize N3 with N2.\n4. Decrease bit-width of N3, and fine-tune for enough iterations.\n5. Repeat step 4 until desired bit-width is reached.\nIn this way, we can reduce the unrecoverable loss of quantization and the adverse impact of quantization can be mostly eliminated."}, {"heading": "Experiments", "text": "In this section, we first describe the datasets we evaluate on and the experiment setup, then demonstrate the results of our method. Note that we conduct most of our experiments in our in-house machine learning system."}, {"heading": "Datasets", "text": "We benchmarked the performance of our BFCN on PASCAL VOC 2012 and Cityscapes, two popular datasets for semantic segmentation.\nThe PASCAL VOC 2012 dataset on semantic segmentation consists of 1464 labelled images for training, and 1449 for validation. There are 20 categories to be predicted, including aeroplane, bus, chair, sofa, etc. All images in the dataset are not larger than 500x500. Following the convention of literature (Long, Shelhamer, and Darrell 2015; Wu, Shen, and Hengel 2016), we use the augmented dataset from (Hariharan et al. 2011), which gives us 10582 images for training in total. We also utilized reflection, resizing and random crop to augment the training data.\nThe Cityscapes dataset consists of 2975 street photos with fine annotation for training and 500 for validation. There are 19 classes of 7 categories in total. All images are in resolution of 2048x1536. In our experiment, the input of BFCN is random-cropped to 1536x768 due to GPU memory restriction, while validation is performed in its original size. We train our models with fine-annotated images only.\nFor performance evaluation, we report the mean classwise intersection-over-union score (mean IoU), which is the mean of IoU scores among classes."}, {"heading": "Experiment Setup", "text": "All experiments are initialized from a ImageNet pretrained ResNet-50 with bounded activations and weights. We then use stochastic gradient descend with momentum of 0.9 to fine-tune the BFCN on semantic segmentation dataset.\nSince the prediction on higher resolution feature maps in laplacian reconstruction and refinement structure depends on the prediction on lower resolutions, we use stage-wise losses to train the network. At first, we only define loss on 32x upsampling branch and fine-tune the network until convergence. Then losses of 16x, 8x and 4x upsampling branches are added one by one.\nIn order to overcome the huge imbalance of classes in Cityscapes dataset, we utilize a class weighing scheme introduced by ENet, which is defined as Wclass = 1/ ln(c + pclass). We choose c = 1.4 to bound class weights in [1, 3].\nResults of different bit-width allocations"}, {"heading": "32 / 32 69.8% -", "text": "First we evaluate the impact of different bit-width allocations on PASCAL VOC 2012 dataset (see Table 5).\nWe observe the performance of network degenerates while bit-width is decreasing, which correspond to our intuition. While 8-8 model performs exactly the same as the\nfull-precision model, decreasing bit-width from 4-4 to 2-2 continuously incurs degeneration in performance. The performance degeneration is at first minor compared to bitwidth saving, but suddenly becomes non-negligible around 4-4. We also discover that allocating different bit-widths to weights and activations harms performance compared to equally-allocated model with the same complexity.\nFrom the results we conclude that 4-4 and 2-2 are favorable choices in different scenes. The 4-4 model can offer comparable performance with full-precision model but with considerable 75% resource savings compared to 8-8 on FPGA. In a more resource-limited situation, the 2-2 model can still offer good performance with only 6.25% hardware complexity of 8-8 model."}, {"heading": "Results of bit-width decay", "text": "We then show how bit-width decay affects performance of networks on PASCAL VOC 2012.\nIt can be seen from Table 6 that bit-width decay does help to achieve a better performance compared to directly cutting off bit-width.\nBesides, we evaluate the impact of \u201ddecay rate\u201d, which is the number of bits in a step. For a decay rate of r, we have kW = c \u2212 r \u00b7 t and kA = c \u2212 r \u00b7 t after t steps of decay, where c = 8 is the initial bit-width. The results of different decay rates are also presented in Table 6.\nWe discover with decay rate less than 2 we can achieve almost the same performance, but increasing it to 3 leads to a sudden drop in performance. It indicates network with 3 less bits starts diverging from its high bit-width couterpart."}, {"heading": "Analysis of class-wise results", "text": "We demonstrate our class-wise results of PASCAL VOC 2012 and Cityscapes in Table 4 and 7.\nAs can be observed that most performance degeneration occur in classes which are more difficult to classify. In PASCAL VOC 2012, we observe that on fine-grained classes like\ncar and bus, cat and dog, BFCN is less powerful than its 32- bit counterpart, however on classes like sofa and bike, 2-bit BFCN even outperforms the full-precision network.\nIt can be seen more clearly on Cityscapes dataset: classes with low mean IoU scores in full-precision network become worse after quantization (like wall and train), while those large-scale, frequent classes such as sky and car remain in nearly the same accuracy.\nThe observation correspond to our intuition that a low bitwidth quantized network is usually less powerful and thus harder to train on difficult tasks. It also suggest that we may use class balancing or bootstrapping to improve performance in these cases."}, {"heading": "Analysis of run-time performance", "text": "We then analyze the run-time performance of BFCN on Tegra K1\u2019s CPU. We have implemented a custom runtime on arm, and all our results on CPU are measured directly in the runtime.\nWe note that 1 single precision operation is equivalent to 1024 bitOps on FPGA in terms of resource consumption, and roughly 18 bitOps on CPU according to the inference speed measured in our custom runtime. Thus, a network with m-bit weights and n-bit activations is expected to be 18m\u00b7n faster than its 32-bit counterpart ignoring the overheads.\nAs shown in Table 8, our 1-2 BFCN can run 7.8x faster than full-precision network with only 1/32 storage size."}, {"heading": "Discussion", "text": "We present some example outputs on PASCAL VOC 2012 in Figure 4. From predictions we can see that BFCNs perform well on easy tasks. But on difficult tasks, which mostly consist of small objects or rare classes like bottle and sofa, BFCN will fail and have worse boundary performance. It also seems that BFCN has difficulties in reconstructing fine\nstructures of the input image. However, low bit-width networks seldom misclassify the whole object, which effectively allow them to be used in real applications."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we propose and study methods for training bit fully convolutional network, which uses low bit-width weights and activations to accelerate inference speed and reduce memory footprint. We also propose a novel method to train a low bit-width network, which decreases bit-width step by step to reduce performance loss resulting from quantization. As a result, we are able to train efficient low bitwidth scene-parsing networks without losing much performance. The low bit-width networks are especially friendly to hardware implementations like FPGA as low bit-width multipliers usually require orders of magnitude less resources.\nAs future work, a better baseline model can be used and CRF as well as other techniques can be integrated into BFCN for even better performance. We also note that our methods of designing and training low bit-width network\ncan also be applied to other related tasks such as object detection and instance segmentation."}], "references": [{"title": "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling", "author": ["Handa Badrinarayanan", "V. Cipolla 2015] Badrinarayanan", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1505.07293", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen"], "venue": "arXiv preprint arXiv:1412.7062", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Chen"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Chen"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["Cordts"], "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Cordts,? \\Q2016\\E", "shortCiteRegEx": "Cordts", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Bengio Courbariaux", "M. David 2014] Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "arXiv preprint arXiv:1412.7024", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["Everingham"], "venue": null, "citeRegEx": "Everingham,? \\Q2015\\E", "shortCiteRegEx": "Everingham", "year": 2015}, {"title": "Cnp: An fpga-based processor for convolutional networks", "author": ["Farabet"], "venue": "In 2009 International Conference on Field Programmable Logic and Applications,", "citeRegEx": "Farabet,? \\Q2009\\E", "shortCiteRegEx": "Farabet", "year": 2009}, {"title": "Large-scale fpga-based convolutional networks", "author": ["Farabet"], "venue": "Machine Learning on Very Large Data Sets", "citeRegEx": "Farabet,? \\Q2011\\E", "shortCiteRegEx": "Farabet", "year": 2011}, {"title": "Laplacian reconstruction and refinement for semantic segmentation", "author": ["Ghiasi", "G. Fowlkes 2016] Ghiasi", "C.C. Fowlkes"], "venue": "CoRR abs/1605.02264", "citeRegEx": "Ghiasi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghiasi et al\\.", "year": 2016}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "Gong,? \\Q2014\\E", "shortCiteRegEx": "Gong", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta"], "venue": "arXiv preprint arXiv:1502.02551", "citeRegEx": "Gupta,? \\Q2015\\E", "shortCiteRegEx": "Gupta", "year": 2015}, {"title": "Learning both weights and connections for efficient", "author": ["Han"], "venue": null, "citeRegEx": "Han,? \\Q2015\\E", "shortCiteRegEx": "Han", "year": 2015}, {"title": "Semantic contours from inverse detectors", "author": ["Hariharan"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "Hariharan,? \\Q2011\\E", "shortCiteRegEx": "Hariharan", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["He"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}, {"title": "Densebox: Unifying landmark localization with end to end object detection", "author": ["Huang"], "venue": "arXiv preprint arXiv:1509.04874", "citeRegEx": "Huang,? \\Q2015\\E", "shortCiteRegEx": "Huang", "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Karpathy Johnson", "J. Fei-Fei 2015] Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Bitwise neural networks. arXiv preprint arXiv:1601.06071", "author": ["Kim", "M. Smaragdis 2016] Kim", "P. Smaragdis"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Pvanet: Deep but lightweight neural networks for real-time object detection", "author": ["Kim"], "venue": "arXiv preprint arXiv:1608.08021", "citeRegEx": "Kim,? \\Q2016\\E", "shortCiteRegEx": "Kim", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097\u20131105", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Shelhamer Long", "J. Darrell 2015] Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Enet: A deep neural network architecture for real-time semantic segmentation", "author": ["Paszke"], "venue": "arXiv preprint arXiv:1606.02147", "citeRegEx": "Paszke,? \\Q2016\\E", "shortCiteRegEx": "Paszke", "year": 2016}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Pham"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham,? \\Q2012\\E", "shortCiteRegEx": "Pham", "year": 2012}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks. arXiv preprint arXiv:1603.05279", "author": ["Rastegari"], "venue": null, "citeRegEx": "Rastegari,? \\Q2016\\E", "shortCiteRegEx": "Rastegari", "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Ren"], "venue": null, "citeRegEx": "Ren,? \\Q2015\\E", "shortCiteRegEx": "Ren", "year": 2015}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Simonyan", "K. Zisserman 2014] Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Improving the speed of neural", "author": ["Senior Vanhoucke", "V. Mao 2011] Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "High-performance semantic segmentation using very deep fully convolutional networks. arXiv preprint arXiv:1604.04339", "author": ["Shen Wu", "Z. Hengel 2016] Wu", "C. Shen", "A. v. d. Hengel"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Optimizing fpga-based accelerator design for deep convolutional neural networks", "author": ["Zhang"], "venue": "In Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2016\\E", "shortCiteRegEx": "Zhou", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Some methods (Paszke et al. 2016; Kim et al. 2016) are proposed to reduce demand of computation resources of FCN by simplifying or redesigning the architecture of network.", "startOffset": 13, "endOffset": 50}], "year": 2016, "abstractText": "Fully convolutional neural networks give accurate, per-pixel prediction for input images and have applications like semantic segmentation. However, a typical FCN usually requires lots of floating point computation and large run-time memory, which effectively limits its usability. We propose a method to train Bit Fully Convolution Network (BFCN), a fully convolutional neural network that has low bit-width weights and activations. Because most of its computation-intensive convolutions are accomplished between low bit-width numbers, a BFCN can be accelerated by an efficient bit-convolution implementation. On CPU, the dot product operation between two bit vectors can be reduced to bitwise operations and popcounts, which can offer much higher throughput than 32-bit multiplications and additions. To validate the effectiveness of BFCN, we conduct experiments on the PASCAL VOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights and 2-bit activations, which runs 7.8x faster on CPU or requires less than 1% resources on FPGA, can achieve comparable performance as the 32-bit counterpart. Introduction Deep convolutional neural networks (DCNN), with its recent progress, has considerably changed the landscape of computer vision (Krizhevsky, Sutskever, and Hinton 2012) and many other fields. To achieve close to state-of-the-art performance, a DCNN usually has a lot of parameters and high computational complexity, which may easily overwhelm resource capability of embedded devices. Substantial research efforts have been invested in speeding up DCNNs on both general-purpose (Vanhoucke, Senior, and Mao 2011; Gong et al. 2014; Han et al. 2015) and specialized computer hardware (Farabet et al. 2009; Farabet et al. 2011; Pham et al. 2012; Chen et al. 2014b; Chen et al. 2014c; Zhang et al. 2015a). Recent progress in using low bit-width networks has considerably reduced parameter storage size and computation burden by using 1-bit weight and low bit-width activations. In particular, in BNN (Kim and Smaragdis 2016) and XNOR-net (Rastegari et al. 2016), during the forward pass the most computationally expensive convolutions can Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. network VOC12 Cityscapes speedup 32-bit FCN 69.8% 62.1% 1x 2-bit BFCN 67.0% 60.3% 4.1x 1-2 BFCN 62.8% 57.4% 7.8x Table 1: Summary results of our BFCNs. Performance measure in mean IoU. be done by combining xnor and popcount operations, thanks to the following equivalence when x and y are bit vectors:", "creator": "LaTeX with hyperref package"}}}