{"id": "1612.00334", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples", "abstract": "recent literature has pointed out that machine learning classifiers, including deep neural networks ( dnn ), are vulnerable to adversarial samples that are maliciously created inputs that force a machine learning classifier to produce wrong output labels. multiple studies have tried to analyze and thus harden machine classifiers under such adversarial noise ( an ). however, they are mostly empirical and scientists provide little understanding of the underlying principles that enable evaluation of the robustness of a classier against an. this paper proposes a unified framework using two metric spaces to evaluate classifiers'robustness against an and provides further general guidance for hardening such classifiers. the central idea of our work is that for a certain classification task, the robustness of a classifier $ f _ 1 $ against an is explicitly decided by both $ f _ 1 $ and its oracle $! f _ 2 $ ( like human annotator of that specific task ). in particular : ( 1 ) by adding oracle $ f _ 2 $ into the framework, we provide a general definition of the immediate adversarial sample problem. ( 2 ) we theoretically formulate a definition that decides whether a classifier is always robust specifically against an ( strong - robustness ) ; ( 3 ) using two metric spaces ( $ x _ 1, d _ 1 $ ) and ( $ x _ 2, d _ 2 $ ) defined by $ f _ 1 $ and $ f _ 9 2 $ respectively, we prove that the topological equivalence between ( $ x _ 1, d _ 1 $ ) and ( $ x _ 2, d _ 2 $ ) is sufficient in deciding whether $ f _ 1 $ is is strong - robust at test time, or not ; ( 5 ) by training a dnn classifier using the siamese architecture, we propose a new defense strategy \" siamese training \" to intuitively approach topological equivalence between ( $ x _ 1, d _ 1 $ ) and ( $ x _ 2, d _ 2 $ ). experimental results could show that siamese training helps multiple dnn models achieve better accuracy compared to previous defense strategies in examining an adversarial setting. dnn models after siamese training exhibit better robustness than the state - of - the - art baselines.", "histories": [["v1", "Thu, 1 Dec 2016 16:20:39 GMT  (978kb,D)", "http://arxiv.org/abs/1612.00334v1", "20 pages , submitting to ICLR 2017"], ["v2", "Mon, 5 Dec 2016 17:07:35 GMT  (1311kb,D)", "http://arxiv.org/abs/1612.00334v2", "20 pages , submitting to ICLR 2017"], ["v3", "Tue, 17 Jan 2017 22:23:55 GMT  (2899kb,D)", "http://arxiv.org/abs/1612.00334v3", "30 pages , submitting to ICLR 2017"], ["v4", "Sat, 21 Jan 2017 16:37:24 GMT  (2908kb,D)", "http://arxiv.org/abs/1612.00334v4", "30 pages , submitting to ICLR 2017"], ["v5", "Thu, 26 Jan 2017 15:32:06 GMT  (2918kb,D)", "http://arxiv.org/abs/1612.00334v5", "30 pages , submitting to ICLR 2017"], ["v6", "Wed, 1 Feb 2017 17:30:50 GMT  (2922kb,D)", "http://arxiv.org/abs/1612.00334v6", "30 pages , submitting to ICLR 2017"], ["v7", "Thu, 2 Feb 2017 14:39:50 GMT  (2922kb,D)", "http://arxiv.org/abs/1612.00334v7", "30 pages , submitting to ICLR 2017"], ["v8", "Fri, 3 Feb 2017 16:06:39 GMT  (2924kb,D)", "http://arxiv.org/abs/1612.00334v8", "30 pages , submitting to ICLR 2017"], ["v9", "Mon, 27 Feb 2017 20:18:26 GMT  (3233kb,D)", "http://arxiv.org/abs/1612.00334v9", "35 pages , submitting to ICLR 2017"], ["v10", "Thu, 9 Mar 2017 22:00:56 GMT  (3218kb,D)", "http://arxiv.org/abs/1612.00334v10", "35 pages , submitting to ICLR 2017"], ["v11", "Thu, 27 Apr 2017 14:36:40 GMT  (3029kb,D)", "http://arxiv.org/abs/1612.00334v11", "38 pages , ICLR 2017 Workshop Track"], ["v12", "Wed, 27 Sep 2017 16:02:48 GMT  (3236kb,D)", "http://arxiv.org/abs/1612.00334v12", "38 pages , ICLR 2017 Workshop Track"]], "COMMENTS": "20 pages , submitting to ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.CV", "authors": ["beilun wang", "ji gao", "yanjun qi"], "accepted": false, "id": "1612.00334"}, "pdf": {"name": "1612.00334.pdf", "metadata": {"source": "CRF", "title": "(DEEP) CLASSIFIERS UNDER ADVERSARIAL NOISE", "authors": ["Beilun Wang", "Ji Gao", "Yanjun Qi"], "emails": ["bw4mw@virginia.edu", "jg6yd@virginia.edu", "yanjun@virginia.edu"], "sections": [{"heading": null, "text": "Adversarial samples are maliciously created inputs that force a machine learning classifier to produce wrong output labels. An adversarial sample is often generated by adding adversarial noise (AN) to a normal sample. Recent literature has pointed out that machine learning classifiers, including deep neural networks (DNN), are vulnerable to AN. Multiple studies have tried to analyze and thus harden machine classifiers under AN. However, they are mostly empirical and provide little understanding of the underlying principles that enable evaluation of the robustness of a classier against AN. This paper proposes a unified framework using two metric spaces to evaluate classifiers\u2019 robustness against AN and provides general guidance for hardening such classifiers. The central idea of our work is that for a certain classification task, the robustness of a classifier f1 against AN is decided by both f1 and its oracle f2 (like human annotator of that specific task). In particular: (1) By adding oracle f2 into the framework, we provide a general definition of the adversarial sample problem. (2) We theoretically formulate a definition that decides whether a classifier is always robust against AN (strongrobustness); (3) Using two metric spaces (X1, d1) and (X2, d2) defined by f1 and f2 respectively, we prove that the topological equivalence between (X1, d1) and (X2, d2) is sufficient in deciding whether f1 is strong-robust at test time, or not; (4) Then a novel measure referred to Adversarial Robustness of Classifier (ARC) is defined to quantify the robustness of a classifier against AN; (5) By training a DNN classifier using the Siamese architecture, we propose a new defense strategy \u201cSiamese training\u201d to intuitively approach topological equivalence between (X1, d1) and (X2, d2). Experimental results show that Siamese training helps multiple DNN models achieve better accuracy compared to previous defense strategies in an adversarial setting. Using the proposed measure ARC, DNN models after Siamese training exhibit better robustness than the state-of-the-art baselines."}, {"heading": "1 INTRODUCTION", "text": "Deep Neural Networks (DNNs) can efficiently learn highly accurate models from a large corpus of training samples. As a result, DNNs have been demonstrated to perform exceptionally well on multiple machine learning tasks (Krizhevsky et al., 2012; Hannun et al., 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al., 2013; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016b). Unlike machine learning used in other fields, security sensitive tasks involve intelligent and adaptive adversaries responding to the learning systems. Recent studies show that attackers can force many machine learning models, including DNNs, to produce an abnormal output behavior, such as a misclassification. A so-called \u201cadversarial sample\u201d (Goodfellow et al., 2014; Szegedy et al., 2013) is crafted by adding a carefully chosen adversarial perturbation (i.e., adversarial noise) to a correctly classified sample drawn from the data distribution. The goal of this paper is to analyze the robustness of machine learning classifiers and thus enable learning-based classification systems to achieve robust performance when being attacked by \u201cadversarial samples\u201d.\nar X\niv :1\n61 2.\n00 33\n4v 1\n[ cs\n.L G\n] 1\nD ec\n2 01\n6\nInvestigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015). Recent studies can be roughly categorized into three types: (1) Poisoning attacks in which specially crafted attack points are injected into the training data. A few recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models. (2) Evasion attacks are attacks in which the adversary\u2019s goal is to create inputs that are misclassified by a deployed target classifier. Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest. (3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems. Recent studies propose various strategies to preserve the privacy of data such as differential privacy. This paper focuses on investigating the evasion attacks. In computer security setting, evasion attacks are mostly referred to as attacking classifiers that try to distinguish malicious behaviors from benign behaviors. Here we extend it to a broader meaning \u2013 adversarial manipulation of test samples. Evasion attacks may be encountered in adversarial settings during system deployment of machine learning methods.\nA variety of recent studies in the deep learning field are about how to find adversarial noise to fool DNN classification and how to design corresponding strategies against such adversarial noise. (Szegedy et al., 2013) firstly points out that convolution NNs are vulnerable to small artificial noises. They use box-constrained L-BFGS to reliably create adversarial samples. Their study also finds that adversarial perturbation generated from one network can also force other networks to produce wrong output. Then (Goodfellow et al., 2014) tries to clarify that the primary cause of such vulnerability is the linear nature of DNNs. They propose an algorithm \u2013 \u201cfast gradient sign method\u201d for generating adversarial examples fast, which makes adversarial training practical. More subsequent papers (Fawzi et al., 2015; Papernot et al., 2015a; Sabour et al., 2015; Nguyen et al., 2015) have explored other ways of adversarial manipulations on DNN outputs or deep representations recently.\nResearchers also try to propose effective countermeasures for making DNN systems robust to noises, that at the worst-case are \u201cadversarial noise\u201d. For instance, denoising NN architectures (Vincent et al., 2008; Gu & Rigazio, 2014; Jin et al., 2015) can discover more robust features by using noise corrupted version of inputs as training samples. A modified distillation strategy (Papernot et al., 2015b) is proposed to improve the robustness of DNNs against adversarial samples, though is shown to be unsuccessful recently (Carlini & Wagner, 2016a). More recent techniques incorporate smoothness penalty (Miyato et al., 2016; Zheng et al., 2016) or layer-wise penalty (Carlini & Wagner, 2016b) as a regularization term in the loss function to promote the smoothness of the DNN model distributions. In the broader \u201csecure machine learning\u201d field, researchers also make attempts for hardening learning systems before. For instance: (1) Barreno et al. (Barreno et al., 2010) and Biggio et al. (Biggio et al., 2008) propose a method to introduce some randomness in the selection of classification boundaries; (2) A few recent studies (Xiao et al., 2015; Zhang et al., 2015) consider the impact of using reduced feature sets on classifiers under adversarial attacks. (Xiao et al., 2015) proposes an adversary-aware feature selection model that can improve classifier\u2019s robustness against adversarial attacks by incorporating specific assumptions about the adversary\u2019s data manipulation strategy. (3) Multiple studies have modeled adversarial scenarios with formal frameworks representing the interaction between the classifier and the adversary. Related efforts include perfect information assumption (Dalvi et al., 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst noise) (Dekel et al., 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning by (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).\nDespite some previous studies (Zheng et al., 2016; Carlini & Wagner, 2016b;a), there exists little theoretical understanding today of how to evaluate the robustness of a machine-learning classifier against an adversary and how to design and harden classifiers. This paper aims to allow a classifier\ndesigner to understand why the classification performance degrades under adversarial attack at test time, and thus, to design a better classifier. In summary, we make the following contributions:\n\u2022 Section 2 points out that previous definitions of adversarial examples have overlooked the importance of the oracle of the task of interest. This paper proposes a theoretical framework connecting the robustness of a classifier against adversarial noises to the consistency of the classifier with the oracle (Definition (2.1)). \u2022 Using two metric spaces corresponding to the classifier and the oracle, Section 3 proves that if these two metric spaces are topologically equivalent, the classifier is always robust to adversarial noises at test time. We use this framework to analyze the robustness of DNN in Section 3.6. \u2022 A novel measure, named as adversarial robustness of classifier (ARC), is proposed in Section 4.1 to quantify the robustness of a classifier by using the expectation of how difficult it is to search for adversarial samples. \u2022 In Section 4.2, we propose a new defense strategy, \u201cSiamese training\u201d, to intuitively approach topological equivalence through Siamese architecture and in consequence to help DNN models achieve better robustness against adversarial attacks. \u2022 Section 5 includes extensive experimental results, showing that Siamese training can help DNN models achieve better ARC comparing to previous defense strategies in an adversarial setting."}, {"heading": "2 ORACLE MATTERS FOR DEFINING ADVERSARIAL SAMPLES AT TEST TIME", "text": "This section provides a formal definition of adversarial attack at test time, by including the notion of the oracle. In particular, we first introduce several previous descriptions of adversary attacks and show that previous definitions have overlooked the importance of the oracle. To overcome this issue, a generalized formulation for \u201cadversarial noise problem\u201d1 is defined and relates to the consistency between classifier and its oracle. All previous definitions of \u201cadversarial examples\u201d are special cases of our formulation.\nTable 1 provides a list of important notations we use in the paper. For a specific classification task a learned classifier f1 : X \u2192 Y , where X = Rp represents the sample space and Y is the output space. As f1 is a classifier, the output space Y is a categorical set. Let x be a sample drawing from the sample space X ."}, {"heading": "2.1 BACKGROUND: PREVIOUS FORMULATIONS OF MACHINE LEARNING CLASSIFIER UNDER ADVERSARIAL NOISE AT TEST TIME", "text": "To attack such a prediction function f1 in the test phase, the basic idea proposed in the recent literature is to generate a misclassified sample x\u2032 by perturbing a correctly classified sample x, with an adversarial perturbation (noise) r, so that:\nf1(x) 6= f1(x\u2032) r = \u2206(x, x\u2032)\n(2.1)\n1 We use \u201cadversarial noise problem\u201d, \u201cadversarial examples problem\u201d, \u201cadversarial attacks at test\u201d, \u201cadversarial attacks at test time\u201d, \u201cadversarial sample\u201d interchangeably in the rest of this paper.\nIf x and x\u2032 is defined in a vector space, r = x \u2212 x\u2032. Otherwise the meaning of r depends on the specific data type that x and x\u2032 belong to 3. Naturally the attacker want to control the size of the perturbation r to ensure the perturbed sample x\u2032 still stays as close as possible to the original sample x. For example, in the image classification case, Eq. (2.1) uses gradient information to get such a r that makes human annotators still recognize x\u2032 as almost the same as x, though the classifier will predict x\u2032 into a different class. In another PDF malware (Xu et al., 2016) case, Eq. (2.1) found r by genetic programming. The modified malicious PDF files will be recognized as malicious by the oracle, but are classified as benign by the the state of art machine learning classifier PDFRate. To control the size of the perturbation, it is necessary to have a distance function to measure \u2206(x, x\u2032). Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).\nHowever, a distance function is defined on some space. To the best of our knowledge, all previous works didn\u2019t clearly specify that space. This is because they have not realized the importance of the oracle classifier for the same task on hand. The goal of machine learning is to train a predictor function f1, which approximates oracle classifier f2 for the same classification task. For example, in image classification tasks, the oracle f2 is often human annotators. Definition 2.1. \u201cOracle\u201d represents a decision process generating ground truth labels for the specific task under interest. Each oracle is task- specific and with finite knowledge.\nImplicitly by the definitions of adversarial examples, a samples pair (x, x\u2032) should be classified into the same class by the oracle classifier. Therefore, the distance function d should be defined in the feature space of the oracle. We use d2 to denote such a distance function defined by the oracle. It is worth to point out that r in Eq. (2.1) was not clearly defined by the previous studies. Instead, our formulation d2(x, x\u2032) directly measures the difference between x and x\u2032 that is more precise and provides better insights (see discussions in Section 3).\nMost previous definitions about attacking machine learning models (Biggio et al., 2013; Lowd & Meek, 2005) view generating an adversarial samples as a constrained optimization problem. Eq. (2.2) provides a general formulation of these studies using f1 and d2. Table 2 summarizes different choices of f1 and d2 used in the recent literature. For instance, the authors of (Biggio et al., 2013) assume that d2(x, x\u2032) = ||r||2. (Goodfellow et al., 2014) assumes that d2(x, x\u2032) = ||r||\u221e.\nargmin x\u2032\u2208X\n(d2(x, x \u2032))\nSubject to: f1(x) 6= f1(x\u2032) (2.2)"}, {"heading": "2.2 BACKGROUND: PREVIOUS STUDIES GENERATING \u201cADVERSARIAL SAMPLES\u201d FOR DNN", "text": "To fool classifiers at test time, several approaches have been implemented to generate \u201cadversarial perturbations\u201d by solving Eq. (2.2). We summarize three typical attacking studies here:\nGradient ascent method (Biggio et al., 2013) The easiest way to solve Eq. (2.2) is through gradient ascent. To minimize the size of the perturbation and maximize the adversarial effect, the noise should follow the gradient direction. Therefore, the perturbation r is calculated as:\nr = \u2207xLf1(x) (2.3) 3For example, in the case of strings, r represents the difference between two strings.\nHere Lf1(x) is a loss function measuring the performance of f1(x). By varying , this method can find a sample x\u2032 with regard to d2(x, x\u2032) such that f1(x) 6= f1(x\u2032).\nBox L-BFGS adversary (Szegedy et al., 2013) Box-constrained L-BFGS view the adversarial problem as a constraint optimization problem, i.e., find a minimum perturbation in the restricted sample space. The perturbation is defined by\nmin r\n(cd2(x, x+ r) + Lf1(x+ r)), x+ r \u2208 D (2.4)\nHere D is the restricted sample space (e.g., in the image classification case, value of each dimension ranges from 0 to 255).\nFast gradient sign method (Goodfellow et al., 2014) Fast gradient sign method view d2 as the `\u221e-norm. In this case, a natural choice is to make the attack strength at every feature dimension to be the same. To maximize the adversarial effect, direction of the adversarial perturbation should be the same as the gradient of loss function Lf1(x). Therefore the perturbation is calculated by: r = sgn(\u2207xLf1(x)) (2.5) Here Lf1(x) is the loss function of f1(x). A recent paper (Kurakin et al., 2016) shows that adversarial examples generated by fast gradient sign method works are misclassified even perceived through cameras."}, {"heading": "2.3 A NOVEL FORMULATION OF \u201cADVERSARIAL SAMPLES\u201d: ORACLE MATTERS", "text": "As we have pointed out, previous studies overlook the importance of the oracle function f2 and have the assumption of a distance function in Eq. (2.2). We think the distance function used in Eq. (2.2) is actually d2 defined in the feature space of the oracle f2. Rather than relying on d2 to indirectly consider f2, we propose the following unified definition for \u201cAdversarial test sample\u201d for a classification task using oracle f2 directly.\nIn this paper, the oracle is defined as a decision process that provide the ground truth for a specific classification task. Oracle classifier f2(\u00b7) is assumed to give correct answers every time. For example, in the image classification task the oracle could be a group of human annotators which provides the ground truth results. In malicious detection task, the oracle decides whether a file is malicious or benign. In addition, we separate f2 as f2 = g2 \u25e6 c2 where g2 : X \u2192 X2 represents the feature extraction and function c2 : X2 \u2192 Y performs the operation of classification. Here, X2 is the feature space of the oracle. In the image classification tasks, X2 is the combination of all informative pixels. X2 in the malicious PDF detection case is the set of all the useful structure features in the PDF file. It is important to point out that the oracle in our context is specific for each classification task. We also illustrate this process in Figure 1.\nDefinition 2.2. Adversarial test sample for a classification task: Suppose we have two functions f1 and f2. f1 : X \u2192 Y is the classification function of learned from a training set and f2 : X \u2192 Y is the classification function of the oracle for the same task. The adversarial examples at test time can be defined as:\nGiven a sample x, to find sample x\u2032, so that f1(x) 6= f1(x\u2032) and f2(x) = f2(x\u2032).\nwhere x \u2208 X . This definition can be formulated in a similar format as Eq. (2.2): Find x\u2032\ns.t. f1(x) 6= f1(x\u2032) f2(x) = f2(x \u2032)\n(2.6)\nMost previous studies, like three methods we\u2019ve mentioned above, tried to find an adversarial sample that are similar to a given x. We would like to point out that Eq. (2.6) may give optimal solutions of x\u2032 that might not be too close to x. For example, (Xu et al., 2016) used genetic programming to generate multiple possible malicious variants from a seed (one malicious PDF file) that can all fool PDFRate (a malicious PDF classifier) to classify them as benign. The distances of these variants to the seed are not necessarily small. For such cases, our general definition of \u201cadversarial examples\u201d by Eq. (2.2) still fits, while Eq. (2.2) does not.\nMost previous studies measure the difference of f2 using a distance function d2. Although it seems to be a natural choice, it makes an implicit underlying assumption: f2 is almost everywhere (a.e.) continuous. We briefly discuss the continuity assumption as follow:\n2.3.1 ASSUMPTION : ALMOST EVERYWHERE (A.E.) CONTINUITY OF f1 AND f2\nDefinition 2.3. fi is continuous a.e. if\n\u2200 > 0 and x \u2208 X a.e., \u2203\u03b4 > 0, so that when di(x, x\u2032) < \u03b4, |fi(x)\u2212 fi(x\u2032)| < , i = 1, 2.\nFor notation simplicity, we use the term \u201ccontinuous a.e.\u201d for continuous almost everywhere4 in the rest of the paper. The fact that f1 and f2 are continuous a.e. is a hidden assumption made by most previous studies(Biggio et al., 2013; Lowd & Meek, 2005).\nBesides, almost all popular machine learning classifiers satisfy the a.e. continuity assumption. For instance, a deep neural network is certainly continuous a.e.. Similarly to the results shown by (Szegedy et al., 2013), DNNs satisfy that |f1(x) \u2212 f1(x\u2032)| \u2264 W \u2016 x \u2212 x\u2032 \u20162 where W \u2264 \u220f Wi and Wi \u2264 ||(wi, bi)||\u221e. Therefore, \u2200 > 0 let \u03b4 = /W . Then |f1(x) \u2212 f1(x\u2032)| < when d1(x, x\n\u2032) =\u2016 x\u2212 x\u2032 \u20162< \u03b4. This shows that a deep neural network is almost everywhere continuous when d1(\u00b7) = || \u00b7 ||2. In appendix Section 7.5, we show that if f1 is not continuous a.e., it is not robust to any types of noise. Considering the generality assumption of machine learning, most machine learning classifiers should satisfy the continuity a.e. assumption."}, {"heading": "3 USING TWO METRIC SPACES TO UNDERSTAND STRONG-ROBUSTNESS OF MACHINE LEARNING CLASSIFIER AGAINST ADVERSARIAL NOISE", "text": ""}, {"heading": "3.1 METRIC SPACES AND TOPOLOGICAL EQUIVALENCE OF TWO METRIC SPACES", "text": "Now we briefly introduce the concept of metric space and topological equivalence. A metric on a set/space X is a function d : X \u00d7X \u2192 [0,\u221e] with four properties: (1) non-negativity, (2) identity of indiscernible, (3) symmetry and (4) triangle inequality. In machine learning, for example, the most widely used metric is Euclidean distance. Kernel based methods, such as SVM, kernel regression and Gaussian process, consider samples in a Reproducing kernel Hilbert space (RKHS). The metric in a RKHS is naturally defined as: d2(x, y) = K(x, x) + K(y, y) \u2212 2K(x, y), here K(\u00b7, \u00b7) is a kernel function.\nAs we have stated, the two metric spaces in our problem are (X1, d1) and (X2, d2). For a pair of samples (x, x\u2032) \u2208 X , the feature extraction of f1 and f2 transfer (x, x\u2032) to (x1, x\u20321) \u2208 X1 and (x2, x \u2032 2) \u2208 X2. Then the adversarial problem defined by Definition (2.2) is to find a pair of samples (x, x\u2032) that d2(x2, x\u20322) is small while d1(x1, x \u2032 1) is large.\nNow we present an important definition \u201ctopological equivalence\u201d, that can represent a special relationship between two metric spaces.\nDefinition 3.1. Topological Equivalence (Kelley, 1975)"}, {"heading": "A function or mapping h(\u00b7) from one topological space to another is continuous if the inverse image", "text": "of any open set is open. If this continuous function is one-to-one and onto, and the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function, in our case (X1, d1) is said to be homeomorphic to the output range, e.g., here (X2, d2). In another word, metric space (X1, d1) is topological equivalent to metric space (X2, d2).\nWe can also state this definition as the following equation: \u2203h : X1 \u2192 X2,\u2200x1, x\u20321 \u2208 X1, h(x1) = x2, h(x \u2032 1) = x \u2032 2\nd1(x1, x \u2032 1) < \u21d4 d2(x2, x\u20322) < \u03b4\n(3.1)\nHere and \u03b4 are two small constants. 4The measure (e.g., Lebesgue measure) of discontinuous set is 0.\nThis definition leads to a number of important theory in Section 3.2 and Section 3.3.\n3.2 USING f1 AND f2 TO DEFINE STRONG-ROBUSTNESS AGAINST ADVERSARIAL NOISE\nWe then apply reverse-thinking on Eq. (2.6) and propose the following definition of strongrobustness for classification models against adversarial noise: Definition 3.2. strong-robustness of a Classification model\nGiven a test sample x \u2208 X a.e., if \u2200x\u2032 \u2208 X and f2(x) = f2(x\u2032), we always have that f1(x) = f1(x \u2032). Then we call the learned predictor f1(\u00b7) is strong-robust to attack at testing time.\nThis definition of classifier\u2019s strong-robustness can be reformatted into the following equation: Give x \u2208 X \u2200x\u2032 \u2208 X\nIf f2(x) = f2(x\u2032) Then f1(x) = f1(x\u2032)\n(3.2)\nWe can extend definition above to more complex models, like LSTM (Hochreiter & Schmidhuber, 1997). The formal definitions are included in the Appendix:Section 7.\nMeanwhile, we want to emphasize that a classifier needs to be both strong-robust and accurate in an adversarial setting. For example, a trivial example for strong-robust models is f1(x) \u2261 1,\u2200x \u2208 X . However, it is a useless model since it doesn\u2019t have any prediction power.\nNext, we prove that the robustness of a certain classification model is determined by the structure of two metric spaces by f1 and f2.\n3.3 TOPOLOGICAL EQUIVALENCE OF TWO METRIC SPACES (X1, d1) AND (X2, d2) IS SUFFICIENT IN DETERMINING THE STRONG-ROBUSTNESS\nBy the continuity assumption and the Definition (7.2), we have the following theorem describing a sufficient condition that determines whether a classification model is strong-robust or not. Theorem 3.3. If (X1, d1) and (X2, d2) are topological equivalent, then the learned classifier f1(\u00b7) is strong-robust to adversarial attack at test time.\nBy the continuity assumption, it is easy to prove that Eq. (3.2) is equivalent to Eq. (3.3).\nBy Theorem (3.3), the topological equivalence between two spaces is sufficient in deciding the robustness of a classification model. This result also shows that parameters of a model have no impact on the model robustness (if they are not related to the metric spaces). Before applying our theorem to real problems, we propose two useful corollaries that are closely related to wrong machine learning setting. Corollary 3.4. If d1 and d2 are norms in Rn, then the learned predictor f1(\u00b7) is strong-robust to adversarial attack at test time.\nThe first corollary shows that if a learned classifier and oracle classifier share the same feature space (X1 = X2), the learned classifier is strong-robust when two metrics are both norm functions (even if not the same norm). We can call this corollary as \u201cnorm doesn\u2019t matter\u201d. Many interesting phenomena can be answered by the corollary. For instance, for a norm regularized classifier, an important question that whether a different norm function will influence the performance of adversarial attack or not. This corollary indicates that two highly different metric function may not change the robustness of the model under adversarial attacks."}, {"heading": "3.4 FINER TOPOLOGY TO DEFINE A SUFFICIENT AND NECESSARY CONDITION FOR THE STRONG-ROBUSTNESS", "text": "We have briefly reviewed the concept of metric space in Section 3.1 and proposed the related Theorem (3.3) in Section 3.3. Our previous proofs rely on feature spaces X1 and X2. However, we can also directly give a proof on the original space X . The crucial problem of the original sample space X is that it\u2019s hard to strictly define a metric on the original feature space. Now we extend about results using a more general concept Pseudometric Space.\nIf a distance function d : X \u00d7X \u2192 [0,\u221e] has three properties: (1) non-negativity, (2) symmetry and (3) triangle inequality, we call d is a pseudometric or generalized metric. The space (X, d) is a pseudometric space or generalized metric space. It is worth to point out that the generalized metric space is a special case of topological space and metric space is a special case of pseudometric space.\nWe can assume a common machine learning classifier f1 = g1 \u25e6 c1, where g1 : X \u2192 X1 represents the feature extraction5 and function c1 : X1 \u2192 Y performs the operation of classification. Let a pseudometric d\u20321(\u00b7, \u00b7) on X so that \u2200x, x\u2032 \u2208 X , d\u20321(x, x\u2032) = d1(g1(x), g1(x\u2032)). Since d1 is a metric in X1, d\u20321 fulfills the (1) non-negativity, (2) symmetry and (3) triangle inequality properties. However, d\u20321 may not fulfills the identity of indiscernible property. For example, suppose g1 only selects the first three features from X . Two samples x and x\u2032 have the same value in the first three features but different values in the rest features. Clearly, x 6= x\u2032, but d\u20321(x, x\u2032) = d1(g1(x), g1(x\u2032)) = 0. This shows that d\u20321(\u00b7, \u00b7) is a pseudometric but not a metric in X . Similarly, a pseudometric d\u20322 for the oracle can be defined.\nTo analyze the strong robustness problem in the original feature space X , we need to a generalized metric space (X, d\u20321) for f1 and a generalized metric space (X, d \u2032 2) for f2. Now f1 and f2 consider\n5Notice that g1 may also include implicit feature selection steps like `1 regularization.\nthe same feature spaceX but two different pseudometrics. This makes it possible to define a sufficient and necessary conditions for the strong robustness of f1 against adversarial noise. Before introducing this condition, we need to briefly introduce the definition of finer/coarser topology here: Definition 3.5. Suppose \u03c41 and \u03c42 are two topologies space X . If \u03c42 \u2286 \u03c41, the topology \u03c42 is called a coarser (weaker or smaller) topology than the topology \u03c41 and \u03c41 is called a finer (stronger or larger) topology than \u03c42.\nBased on Corollary (3.5) we have the following conclusion: Theorem 3.6. If the (X, d\u20321) is a finer topology than (X, d\u20322), then the machine learning classifier f1 is strong-robust against adversarial test samples.\nWe can also state this sufficient and necessary condition as the following equation: d\u20322(x, x\n\u2032) < \u03b4 \u21d2 d\u20321(x, x\u2032) < \u2200x, x\u2032 \u2208 X (3.3)\nHere and \u03b4 are two small scalars.\nThe proof of Corollary (3.6) is in the appendix. Based on Corollary (3.6), we have: Corollary 3.7. IfX1 = Rn1 , X2 = Rn2 , n1 > n2, X2 ( X1, d1 = d2 and d1, d2 are norm function, then the learned predictor f1(\u00b7) is not strong-robust to adversarial attack at test time.\nSimilar to the Corollary (3.4), this corollary shows that even a small difference between two feature spaces can make the learned predictor f1(\u00b7) vulnerable to adversarial samples. Therefore, the feature selection step is crucial in deciding whether a predictor is strong-robust or not in adversarial test setting. Regardless of the model itself, we need to carefully select the features before the operation of classification.\nSummarizing Theorem (3.3), Theorem (3.6), Corollary (3.4) and Corollary (3.7), the robustness of learned classifier is decided by two factors: (1) the difference between two feature space; (2) the difference between the metric functions. In addition, these two corollaries also show that the difference between the feature spaces is more important than the difference between two metric functions."}, {"heading": "3.5 A CASE STUDY: WHY THEOREM 3.4 IS IMPORTANT ?", "text": "By Corollary (3.7), if an unrelated feature is selected in the feature selection step, then no matter how hard the model is trained, it is not strong-robust. Actually, we want to show that a model trained this way is often prone to adversaries.\nHere\u2019s an example. Figure 2 shows a situation that the oracle only uses one feature to classify items. A machine classifier uses an extra unrelated feature and successfully classifies all the items. However, it\u2019s very easy to find an adversary by simply adding a perturbation on the extra feature. In the figure, the red circle is such an adversary, which changes the prediction but is very close to the original sample in the oracle space.\nIn the real cases, such attacks can be adding words with a very small font size in a spam E-mail, which is invisible to a human annotator. However, if machine classifier tries to utilize such extra words, surely it could be led to a wrong direction.\nAnother interesting thing is the relationship between robustness and accuracy. Clearly, the performance of a machine learning model is related to the feature space. Using the topological space idea, we can now have a better understanding of the feature space: If the model misses some related features, the accuracy will drop. If the model uses extra unrelated features, it will be not robust to\nadversarial attacks. We can achieve a robust and accurate model only if we can find a feature space which is topological equivalent with the feature space of the oracle model. If a trained model is robust and accurate, then it could be considered as a very good approximation of the oracle.\nGuided by this idea, make a model robust is very related to choose a better feature space, thus will improve the performance of the model."}, {"heading": "3.6 A CASE STUDY: ARE DEEP NEURAL NETS STRONGLY ROBUST ?", "text": "In this section, we apply our theory to deep neural networks(DNN) classifier. More specifically, we find that (i) DNNs are not robust against adversarial attacks (ii) this weakness is an important limitation and should be improved to get better models.\nResearchers have proposed different adversarial attacks on deep neural networks(e.g., (Szegedy et al., 2013; Nguyen et al., 2015; He et al., 2015; Papernot et al., 2016a; Moosavi-Dezfooli et al., 2015; Papernot et al., 2015b)). Here we focus on the image classification task with symbols defined as follows:\n\u2022 f1(\u00b7): f1(\u00b7) is a DNN classifier with multiple layers, including linear perceptron layers, activation layers and convolutional layers. \u2022 (X1, d1): X1 denotes the feature space of the DNN. It is extracted from the original image space (e.g., RGB representation) by the DNN (e.g., convolutional layers). \u2022 (X2, d2): X2 denotes the feature space of the human oracle. It is difficult to achieve a precise form of d1, but we can still observe some properties of d1 through experiment results. To show the previous properties, we do experiments on a state-of-art residual network. The model we uses is a 200-layer residual network(He et al., 2015) on Imagenet dataset(Deng et al., 2009). The model is trained by Facebook6.\n6https://github.com/facebook/fb.resnet.torch\nFrom Table 4, we can see the accuracy of the model in the adversarial setting is very bad. We also test the accuracy with random perturbed samples. 7. Clearly, performance of random perturbed data is much better than performance of maliciously perturbed data, which means d1 in a random direction is larger than d1 in the adversarial direction.\nThe phenomenon we observed could be explained by Figure 3. Comparing to human oracle, neural network classifier is often over-fitting to several samples (e.g., possible images), and therefore (X1, d1) have the form of a very thin high-dimensional elliptical sphere. When the attacker try to minimize the perturbation size using the distance d2, the thin part is exactly the adversarial direction. In the human oracle there are no such problem. If we can overcome the problem of adversarial attack, the model is supposed to be closer to the human oracle, and thus supposed to have better performance on the classification task.\nThis idea inspires us to design strategies to harden the deep neural network."}, {"heading": "4 QUANTIFYING AND IMPROVING WEAK-ROBUSTNESS OF DNN AGAINST ADVERSARIAL SAMPLES", "text": ""}, {"heading": "4.1 A NOVEL MEASURE: ADVERSARIAL ROBUSTNESS OF CLASSIFIERS (ARC) TO QUANTIFY MODEL ROBUSTNESS AGAINST ADVERSARIAL TEST SAMPLES", "text": "Based on the previous analysis, a strong-robust model is difficult to obtain. This is because strongrobust is a very strong condition, which requires complete understanding of the oracle. However, it is also important to measure the robustness of machine learning models when they are not strong-robust. In this section, we propose a quantitative measure to capture how robust a model is, based on the performance against adversarial attack. We show that this measure can also capture the strongrobustness of a certain machine learning classifier when it achieves the maximum (1 when rescaled to [0, 1]).\nDefinition 4.1. Adversarial Robustness for Classifiers (ARC)\nWe can measure the robustness of a machine learning model based on how difficult the attacker generates those adversarial samples. By adding f2(x) = f2(x\u2032) into Eq. (2.2), we define a measure for the robustness of machine learning classifiers against adversarial samples.\nEx\u2208X [d2(x, x\u2032)] x\u2032 = argmin\nt\u2208X d2(x, t)\nSubject to: f1(x) 6= f1(t) f2(x) = f2(t)\n(4.1)\nA previous study (Papernot et al., 2015b) proposes a similar measure but it assumes the distance function is a norm function. Also, it does not consider the importance of the oracle in this problem. More importantly, it doesn\u2019t provide any computable way to calculate or estimate that measure.\nWe design a computable criteria to estimate Eq. (4.1) for image classification tasks based on Definition (4.1). We choose d2 = || \u00b7 ||\u221e as an example. Based on Eq. (4.1), E[||x\u2212 x\u2032||\u221e] = E[||r||\u221e]. To make an estimation of E[||x\u2212 x\u2032||\u221e], we need have some assumption. Assume that there exists a threshold \u03c4 , that any perturbation larger than \u03c4 will change the classification result of the oracle f2. That is if ||x \u2212 x\u2032||\u221e > \u03c4 , f2(x) 6= f2(x\u2032). In image classification tasks, as the input space is discrete (every dimension from 0 to 255), ARC can be estimated by equation Eq. (4.2):\nARC\u221e(f1) = E[\u2016 x\u2212 x\u2032 \u2016\u221e] = \u03c4\u2211\ni=1\niP(\u2016 x\u2212 x\u2032 \u2016\u221e= i)\n+ (\u03c4 + 1)P(f1(x) = f1(t),\u2200 \u2016 x\u2212 t \u2016\u221e\u2264 \u03c4). x\u2032 = argmin\nt\u2208X d2(x, t)\nSubject to: f1(x) 6= f1(t) f2(x) = f2(t)\n(4.2)\nWe want to point out that f1 is strong-robust against adversarial test sample if and only if ARC\u221e(f1)/(\u03c4 + 1) = 1.\nA measure combining Adversarial Robustness of Classifiers with Accuracy (ARCA) Both accuracy and robustness are important properties to determine whether a model is good or not. We combine accuracy and robustness into the following unified criteria:\nARCA(f1) = Accuracy(f1)\u00d7 ARC\u221e(f1) \u03c4 + 1\n(4.3)\nRetraining model by the adversarial test samples: A trivial hardening solution is retraining the model by the adversarial test samples. Some previous study(Szegedy et al., 2013) use retraining\nas the hardening strategy. In fact, this approach is trying to solve the following equation: argmax\nw ARC(f(\u00b7;w)) (4.4)\nHowever, since ARC is not related to the accuracy measure, retraining by the adversarial test samples will reduce the accuracy. Instead, we propose a regularization based approach to compromise both accuracy and adversarial robustness."}, {"heading": "4.2 A NOVEL HARDENING APPROACH : TO IMPROVE DNN\u2019S ADVERSARIAL-ROBUSTNESS WITH \u201cSIAMESE TRAINING\u201d", "text": "In this section, we propose a method: Siamese Training. In this method, we first generate random perturbed samples. By penalizing the difference of middle layer outputs between the perturbed sample and original sample, we can push two feature spaces X1 and X2 to be similar and thus increase the robustness of the model. Experimental results show that our method achieves better performance comparing to baseline methods in adversarial settings.\nSiamese network (Bromley et al., 1993) is a traditional approach of non-linear embedding methods. This method has long been used in many fields, including face recognition (Krizhevsky et al., 2012) and dimension reduction (Hadsell et al., 2006).\nA sketch of the Siamese Architecture is shown in sub-figure (a) of Figure 4. Basically, a Siamese network contains two copies of a model sharing the same weights. The input of the Siamese network is a pair of the data samples. Generally, the loss function of the Siamese network can be any distance measure. In this paper, we choose Euclidean distance \u2016 \u00b7 \u20162 as an example. The idea of Siamese training is to feed a slightly perturbed input x\u2032 together with the original input x to the Siamese network. If the model is robust against adversarial attacks, according to Eq. (3.2), for a pair of inputs which are close to each other (i.e., d2 is small), the difference on the extracted features of the two should be very small (i.e., d1 is small). Therefore, by training the Siamese network, it pushes these two outputs similar. The adversarial robustness of the revised model (against adversarial attack at that sample point) will increase.\nThe process of Siamese Training is described in the Algorithm 1:\nAlgorithm 1 Siamese Training algorithm input Training set x, Random perturbation threshold p, NN classifier M .\n1: Initialize a Siamese network with weights in M . 2: for i = 1, Ntest do 3: Pick an example xk, generate a random perturbation ||\u2206x||2 \u2264 p, x\u2032k = xk + \u2206x. 4: Combine xk and x\u2032k as the input and forward it into the network. 5: Get the middle layer output of the Siamese network and pass it to the loss function. 6: Do backward propagation on the network to update the weights. 7: end for"}, {"heading": "4.2.1 USING \u201cSIAMESE TRAINING\u201d TO IMPROVE ADVERSARIAL-ROBUSTNESS AND ACCURACY OF THE DNN TOGETHER", "text": "If we want to achieve a good classifier which has high accuracy and robustness, we should maximize accuracy and robustness together. Namely, we want to train a DNN model to have better ARCA based on 4.5 in our case.\nargmax w\nARCA(f1(\u00b7;w))\n= argmax w\nlog(ARCA(f1(\u00b7;w)))\n= argmax w\nlog(Accuracy(f1(\u00b7;w))) + log(ARC\u221e(f1(\u00b7;w))) (4.5)\nInspired by this, we should minimize two loss functions L1 and L2 at the same time. L1 is the loss function used in training to control the accuracy, and a new L2 constrains the robustness. Based on Eq. (4.5), we have:\nargmin w\nL1(f(\u00b7;w)) + \u03b1L2(f(\u00b7;w)) (4.6)\nBy using the middle layer output of Siamese network as the regularization item L2, we have: L(x;w) = L1(x;w) + \u03b1D(f1(x;w), f1(x\n\u2032;w)) (4.7) D here is the output of the Siamese network, which is the Euclidean distance of middle layer outputs."}, {"heading": "5 EXPERIMENT", "text": "Dataset: CIFAR-10: CIFAR-10 is a image classification dataset released in (Krizhevsky & Hinton, 2009). The training set contains 50,000 32x32 color images in 10 classes, and the test set contains 10,000 32x32 color images.\nBaseline methods and experiment design: We compare our training strategy with two approaches: (1) original training without any extra robust strategy, and (2) KL training in (Zheng et al., 2016).\nWe choose a VGG model(Simonyan & Zisserman, 2014) as a baseline model. The VGG model in our experiment have 16 weight layers (55 layers in total).\nTraining method and hyperparameters: We implement two methods other than the baselines. (1) Siamese training (Section 4.2) (2) Siamese training used as a regularization item (Section 4.2.1).\nWe choose SGD as the optimization method, and Euclidean distance as the distance measure of the siamese network. According to the result of (Zagoruyko & Komodakis, 2016), on the CIFAR-10 dataset, we train all the models for 200 epochs. Initial learning rate is set to be 0.1, and let it drop with rate 0.2 at the 60th, 120th and 160th epochs. To make it fair, in all comparison different strategies we use the same hyperparameters.\nMetric:\n\u2022 Test accuracy: We use top-1 test accuracy as our experiment metric, which is defined as the number of successfully classified samples dividing the number of all test samples. Basically our baseline models have a very high accuracy without adversarial attacks. \u2022 ARC ( Eq. (4.2)) : We use ARC to measure the robustness of the model. \u03c4 is chosen to be 10 in our case. \u2022 ARCA: ( Eq. (4.3)) : We use ARCA to measure the total performance of the model.\nTo generate adversary, we generate adversarial samples using the fast gradient sign method. It is because fast gradient sign method is easy to implement and also adjustable, which means the power of the adversary attack can be easily controlled. By controlling the power of fast sign attack, we can have a better view of how the accuracy changes according to the attack power.\nIn the following analysis, the attack power is defined as: P = ||x\u2212 x\u2032||\u221e (5.1) For image classification tasks, we control the perturbated sample to be still in the valid input space, that every dimension of the perturbed samples is on the range of integers between 0 and 255.\nExperimental result: The result of VGG model on CIFAR-10 dataset is displayed in table 5. From the results we show that our model have greatly increase the accuracy against adversarial attack when the attacking power is large. We also show that our approach can improve both ARC and ARCA a lot.\nFrom the experimental results, it is clear that our model achieves same level of performance comparing to original model with no adversary, and even slightly better. Actually, even without the original network, in Table 5 only using Siamese network could still keep the test accuracy. As we discussed in the Section 3.6, the oracles of the classification tasks f2 should be continuous a.e. . By doing the Siamese training, the network is supposed to reduce overfitting. Therefore, it\u2019s not surprising that the performance could even slightly increase in robust training.\nAlso, experimental results show that our defense approach greatly decrease the effectiveness of the adversarial attack. Even with very strong attack, the accuracy of our model is still good comparing to baseline methods. According to the result of our measures, Siamese training increases ARC and ARCA by more than 55%. Using Siamese training as a regularization item improves ARC and ARCA by more than 60%. Since using Siamese training as the regularization item achieves a better result, we recommend to use this method.\nTherefore, our approach is supposed to greatly increase the robustness of the model."}, {"heading": "6 CONCLUSION", "text": "Machine learning techniques were not designed to withstand manipulations made by intelligent and adaptive adversaries. This paper focuses on providing a theoretical framework for understanding the robustness of learning-based classifiers, especially DNN in the face of such adversaries at test time. By investigating the topology between two metric spaces corresponding to predictor and oracle, we develop several theoretical conditions that can determine if a classifier is (strong) robust against adversarial samples. Then taking advantages of theoretical results, a novel measure named ARC and a novel hardening strategy called \"Siamese training\" are proposed to enhance the robustness of DNN models. Empirically results on two benchmark datasets across multiple DNN models have shown strong improvements of \"Siamese training\" against other state-of-the-art defense strategies."}, {"heading": "7 APPENDIX OF PROBLEM FORMULATION", "text": ""}, {"heading": "7.1 PREVIOUS DEFINITIONS OF ADVERSARIAL ATTACKS AT TESTING TIME", "text": "We discuss about previous definitions of adversarial attacks in this section.\n(Biggio et al., 2013) use this formula argmin\nx\u2032 (f1(x\n\u2032))\ns.t. d2(x, x\u2032) < dmax f1(x) > 0\n(7.1)\n, and (Lowd & Meek, 2005) use this formula\nargmin x\u2032\n(d2(x, x \u2032))\ns.t. f1(x\u2032) < 0 f1(x) > 0\n(7.2)\n.\nHere dmax is a constant. Clearly, these two formulas are not equivalent."}, {"heading": "7.2 A GENERAL DEFINITION OF ADVERSARIAL TEST SAMPLE PROBLEM", "text": "Most previous studies focus on classification tasks. However, other machine learning tasks also suffer from adversarial attacks. We can easily extend our definition to general machine learning situations.\nDefinition 7.1. Adversarial test sample problem:\nGiven an original sample x, find a sample x\u2032, such that \u22030 < \u03b4, < c < 1, |f1(x)\u2212 f1(x\u2032)| > \u03b4 and |f2(x)\u2212 f2(x\u2032)| < ."}, {"heading": "7.3 A GENERAL DEFINITION OF STRONG-ROBUSTNESS", "text": "Definition 7.2. Strong-robustness\nGiven a test sample x \u2208 Rna.e., if \u2200x\u2032 \u2208 Rn and 0 < < c < 1 and |f2(x) \u2212 f2(x\u2032)| < , there exists 0 < \u03b4 < c < 1 such that |f1(x)\u2212 f1(x\u2032)| < \u03b4. Then we call the learned predictor f1(\u00b7) to be strong-robust against adversarial attacks at test time."}, {"heading": "7.4 A COUNTER-EXAMPLE OF THE CONTINUITY ASSUMPTION", "text": "A counter-example can be as following: Suppose a predictor function f(x) = 1p(x,y=1)> 12 for a classification problem, where p(x, y = 1) is the probability density function. Assuming the p(x, y = 1) is a continuous a.e. function, we define a probability density function p0 as:\np0(x) =\n{ p(x), x \u2208 R \\Q\n0, x \u2208 Q (7.3) Notice that p0 is still the probability density function. However, f0(x) = 1p0(x,y=1)> 12 is nowhere continuous in the set {x|f(x) = 1}. This is an extreme case, but it shows that a learned probability function might not be continuous a.e.."}, {"heading": "7.5 MORE ABOUT A.E.CONTINUITY ASSUMPTION", "text": "Lemma 7.3. If the a.e. continuity assumption doesn\u2019t hold, there exists a non-zero measure set D, such that\n\u2200x \u2208 D,\u2203x\u2032\ns.t. f1(x) 6= f1(x\u2032) d1(x, x \u2032) < \u03b4\n(7.4)\nProof. Without it, for any test sample x, you can easily find a very similar sample x\u2032 (i.e. for any small \u03b4, d1(x, x\u2032) < \u03b4) such that |f1(x) \u2212 f1(x\u2032)| > . In classification problems, this means that f1(x) 6= f1(x\u2032)(i.e. exist very similar pair of two samples x and x\u2032 that have different labels for most x \u2208 X1).\nThe Lemma (7.3) shows that f1 is not robust to a random noise if we don\u2019t assume f1 is continuous."}, {"heading": "7.6 MOST CLASSIFIERS SATISFY THE CONTINUITY ASSUMPTION", "text": "Almost all popular machine learning classifiers satisfy the continuity assumption. For example,\n\u2022 logistic regression for text categorization with a bag of word representation. A classifier with a discrete feature representation is naturally continuous. Since {x\u2032|d1(x, x\u2032) < \u03b4, x 6= x\u2032} = \u2205 when \u03b4 is small and x, x\u2032 are discrete. Thus, logistic regression with a bag of word representation is a continuous predictor. \u2022 Support Vector Machine with continuous feature representation. Suppose we define the d21(x, x\n\u2032) = k(x, x) + k(x\u2032, x\u2032)\u2212 2k(x, x\u2032). The support vector machine is a linear classifier with d1. Thus, the SVM prediction function is continuous with d1."}, {"heading": "8 APPENDIX: TWO METRIC SPACES FOR MODEL ROBUSTNESS AGAINST ADVERSARIAL NOISE", "text": ""}, {"heading": "8.1 MORE ABOUT METRIC SPACE", "text": "It is not necessary to use the metric as a similarity measurement in machine learning and statistics methods in the original feature space Rn. For example, people sometimes implement a feature selection at the beginning of data analysis. This, while improves the accuracy and speeds up the method, projects the original sample into an embedded feature space Rn\u2032 . One problem is that the distance function d used in the learning is not a metric. This is because that \u2203x, y \u2208 Rn such that d(x, y) = 0 but x 6= y(ProjRn\u2032 (x) = ProjRn\u2032 (y)). So d doesn\u2019t fit the (2) identity of indiscernible in the space Rn. A distance function satisfying property (1), (3) and (4) but not (2) is defined as \u201cPseudometric\u201d. In summary, the distance used in machine learning is a pseudometric or, more specifically, a suitable metric in an embedded feature space. Therefore, we choose \u201cPseudometric\u201d to prove the sufficient and neccessary condition of strong-robustness in Section 3.4."}, {"heading": "8.2 PROOFS FOR THEOREMS", "text": "In this section, we provide the proofs for Theorem (3.3), Corollary (3.4), Theorem (3.6), and Corollary (3.7). We first prove Theorem (3.6) and Corollary (3.7). Since \u201ctopological equivalence\u201d is a stronger condition than \u201cfiner topology\u201d, Theorem (3.3) and Corollary (3.4) are straightforward.\nProof of Theorem (3.6)\nProof. Let S1 = {B1(x, )} and S2 = {B2(x, )}, where B1(x, ) = {y|d\u20321(x, y) < } and B2(x, ) = {y|d\u20322(x, y) < }. Then S1 \u2282 \u03c41 and S2 \u2282 \u03c42. In fact, \u03c41 and \u03c42 are generated by S1 and S2. S1 and S2 are bases of (X, d\u20321) and (X, d \u2032 2). Consider a pair of samples (x, x\u2032) and d\u20322(x, x \u2032) \u2264 . x, x\u2032 \u2208 B2(x, ). Suppose the (X, d\u20321) is a finer topology than (X, d\u20322). Then B2(x, ) \u2208 \u03c41. You can find B1(x0, \u03b4/2) \u2208 \u03c41 such that B2(x, ) \u2282 B1(x0, \u03b4/2). Therefore d\u20321(x, x\u2032) \u2264 \u03b4. Based on a.e. continuity assumption, since d\u20322(x, x\n\u2032) \u2264 , f2(x) = f2(x\u2032) and since d\u20321(x, x\u2032) \u2264 \u03b4, f1(x) = f1(x \u2032). This fits our definition of strong-robustness.\nProof of Corollary (3.7) :\nProof. Suppose n1 > n2 and X2 \u2282 X1. (X, d\u20322) is a finer topology than (X, d\u20321). Therefore (X, d\u20321) is not a finer topology than (X, d\u20322), which indicates that f1 is not strong-robust against adversarial samples.\nProof of Theorem (3.3) :\nProof. Since (X1, d1) and (X2, d2) are topological equivalent, (X, d\u20321) is a finer topology than (X, d\u20322) and (X, d \u2032 2) is a finer topology than (X, d \u2032 1). By Theorem (3.6), we have the conclusion.\nProof of Corollary (3.4) :\nProof. By (Kelley, 1975), we know that if d1 and d2 are norms in Rn, (Rn, d1) and (Rn, d2) are topological equivalent. Therefore, we have the conclusion."}, {"heading": "8.3 A CASE STUDY: PREVIOUS GRADIENT-DRIVEN ATTACKS ON THE DIFFERENTIABLE PREDICTOR FUNCTIONS", "text": "In this section we discuss gradient-driven attacks on the differentiable predictor functions. Recent studies(Biggio et al., 2013; Szegedy et al., 2013) chose a support Vector Machine model with a continuous feature representation or a deep neural network. They generate the evasive samples by solving the following two convex optimization problem:\nx\u2032 = argmin y f1(y)\nSubject to:d(y, x) \u2264 \u03b4 (8.1)\nx\u2032 = argmin y d(y, x)\nSubject to:f1(y) \u2264 0 (8.2)\nThese two formulations are the dual form of each other. Previous studies choose gradient descent to solve this problem. The major issue of this attacking strategy is that it relies on the gradient information. An efficient solution to prevent this kind of attacks is hiding gradient information. Although there are some studies using the approximated gradient(Xu et al., 2016), it is difficult to estimate the gradient when the only output of the prediction function is discrete values. It is still an unsolved question to approximate the gradient of a binary function.\nMoreover, previous studies don\u2019t carefully analyze the distance function d(\u00b7, \u00b7). Most papers assume d(\u00b7, \u00b7) = d1(\u00b7, \u00b7), or even more simple as d(\u00b7, \u00b7) = | \u00b7 |2. These two equations above also omit the differences between feature spaces. According to Corollary (3.7), the difference between features spaces is another fatal condition for model robustness.\nTherefore, previous attacking strategies don\u2019t catch some important factors of robustness, and thus are not very comprehensive."}], "references": [{"title": "Data poisoning attacks against autoregressive models", "author": ["Scott Alfeld", "Xiaojin Zhu", "Paul Barford"], "venue": null, "citeRegEx": "Alfeld et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alfeld et al\\.", "year": 2016}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D Joseph", "J Doug Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, computer and communications security,", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "The Security of Machine Learning", "author": ["Marco Barreno", "Blaine Nelson", "Anthony D Joseph", "JD Tygar"], "venue": "Machine Learning,", "citeRegEx": "Barreno et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2010}, {"title": "Adversarial pattern classification using multiple classifiers and randomisation", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "URL http://link.springer.com/chapter/10", "citeRegEx": "Biggio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2008}, {"title": "Evasion attacks against machine learning at test time", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Biggio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2013}, {"title": "Poisoning complete-linkage hierarchical clustering", "author": ["Battista Biggio", "Samuel Rota Bul\u00f2", "Ignazio Pillai", "Michele Mura", "Eyasu Zemene Mequanint", "Marcello Pelillo", "Fabio Roli"], "venue": null, "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Differentially-and non-differentially-private random decision trees", "author": ["Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Yann LeCun"], "venue": "arXiv preprint arXiv:1410.6973,", "citeRegEx": "Bojarski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2014}, {"title": "Signature verification using a \u201csiamese\u201d time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Stackelberg Games for Adversarial Prediction Problems", "author": ["Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Br\u00fcckner and Scheffer.,? \\Q2011\\E", "shortCiteRegEx": "Br\u00fcckner and Scheffer.", "year": 2011}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1607.04311,", "citeRegEx": "Carlini and Wagner.,? \\Q2016\\E", "shortCiteRegEx": "Carlini and Wagner.", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini and Wagner.,? \\Q2016\\E", "shortCiteRegEx": "Carlini and Wagner.", "year": 2016}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Fitting Time Series Models to Nonstationary Processes", "author": ["Rainer Dahlhaus"], "venue": "The Annals of Statistics,", "citeRegEx": "Dahlhaus.,? \\Q1997\\E", "shortCiteRegEx": "Dahlhaus.", "year": 1997}, {"title": "Adversarial classification", "author": ["Nilesh Dalvi", "Pedro Domingos", "Sumit Sanghai", "Deepak Verma"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Dalvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2004}, {"title": "Learning to Classify with Missing and Corrupted Features", "author": ["Ofer Dekel", "Ohad Shamir", "Lin Xiao"], "venue": "Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Privacy aware learning", "author": ["John C Duchi", "Michael I Jordan", "Martin J Wainwright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Duchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2014}, {"title": "Differential Privacy", "author": ["Cynthia Dwork"], "venue": "In Encyclopedia of Cryptography and Security,", "citeRegEx": "Dwork.,? \\Q2011\\E", "shortCiteRegEx": "Dwork.", "year": 2011}, {"title": "Fundamental limits on adversarial robustness", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "In Proceedings of ICML, Workshop on Deep Learning, number EPFL-CONF-214923,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Real analysis: modern techniques and their applications", "author": ["Gerald B Folland"], "venue": null, "citeRegEx": "Folland.,? \\Q2013\\E", "shortCiteRegEx": "Folland.", "year": 2013}, {"title": "Nightmare at Test Time: Robust Learning by Feature Deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In 23rd International Conference on Machine Learning,", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "Grosse et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2016}, {"title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "[cs],", "citeRegEx": "Gu and Rigazio.,? \\Q2014\\E", "shortCiteRegEx": "Gu and Rigazio.", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "DeepSpeech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "others"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adversarial machine learning", "author": ["Ling Huang", "Anthony D Joseph", "Blaine Nelson", "Benjamin IP Rubinstein", "JD Tygar"], "venue": "In 4th ACM Workshop on Security and Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Robust convolutional neural networks under adversarial noise", "author": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "venue": "arXiv preprint arXiv:1511.06306,", "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Evasion and hardening of tree ensemble classifiers", "author": ["Alex Kantchelian", "JD Tygar", "Anthony D Joseph"], "venue": "arXiv preprint arXiv:1509.07892,", "citeRegEx": "Kantchelian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kantchelian et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Differentially private distributed online learning", "author": ["Chencheng Li", "Pan Zhou"], "venue": "arXiv preprint arXiv:1505.06556,", "citeRegEx": "Li and Zhou.,? \\Q2015\\E", "shortCiteRegEx": "Li and Zhou.", "year": 2015}, {"title": "Mining adversarial patterns via regularized loss minimization", "author": ["Wei Liu", "Sanjay Chawla"], "venue": "Machine learning,", "citeRegEx": "Liu and Chawla.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Chawla.", "year": 2010}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Lowd and Meek.,? \\Q2005\\E", "shortCiteRegEx": "Lowd and Meek.", "year": 2005}, {"title": "The security of latent dirichlet allocation", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": null, "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Some submodular data-poisoning attacks on machine learners", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": null, "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Under review as a conference", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Koyama Masanori"], "venue": "ICLR\u2019 16,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "The limitations of deep learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "A differentially private stochastic gradient descent algorithm for multiparty classification", "author": ["Arun Rajkumar", "Shivani Agarwal"], "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "Rajkumar and Agarwal.,? \\Q2016\\E", "shortCiteRegEx": "Rajkumar and Agarwal.", "year": 2016}, {"title": "Practical Evasion of a Learning-Based Classifier: A Case Study", "author": ["N. Rndic", "P. Laskov"], "venue": "Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rndic and Laskov.,? \\Q2012\\E", "shortCiteRegEx": "Rndic and Laskov.", "year": 2012}, {"title": "Adversarial manipulation of deep", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": null, "citeRegEx": "Sabour et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sabour et al\\.", "year": 2014}, {"title": "Intriguing properties", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Adversarial reinforcement learning", "author": ["William Uther", "Manuela Veloso"], "venue": "neural networks. arXiv preprint arXiv:1312.6199,", "citeRegEx": "Uther and Veloso.,? \\Q2013\\E", "shortCiteRegEx": "Uther and Veloso.", "year": 2013}, {"title": "Extracting and composing robust features with denoising", "author": ["Unpublished", "1997. Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": null, "citeRegEx": "Unpublished et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Unpublished et al\\.", "year": 1997}, {"title": "Is feature selection secure against training data", "author": ["Huang Xiao", "Battista Biggio", "Gavin Brown", "Giorgio Fumera", "Claudia Eckert", "Fabio Roli"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Xiao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2008}, {"title": "Crypto-nets: Neural networks", "author": ["Pengtao Xie", "Misha Bilenko", "Tom Finley", "Ran Gilad-Bachrach", "Kristin Lauter", "Michael Naehrig"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15),", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "2016", "shortCiteRegEx": "2016", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "As a result, DNNs have been demonstrated to perform exceptionally well on multiple machine learning tasks (Krizhevsky et al., 2012; Hannun et al., 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al.", "startOffset": 106, "endOffset": 152}, {"referenceID": 26, "context": "As a result, DNNs have been demonstrated to perform exceptionally well on multiple machine learning tasks (Krizhevsky et al., 2012; Hannun et al., 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al.", "startOffset": 106, "endOffset": 152}, {"referenceID": 12, "context": ", 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al., 2013; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016b).", "startOffset": 59, "endOffset": 183}, {"referenceID": 22, "context": "A so-called \u201cadversarial sample\u201d (Goodfellow et al., 2014; Szegedy et al., 2013) is crafted by adding a carefully chosen adversarial perturbation (i.", "startOffset": 33, "endOffset": 80}, {"referenceID": 29, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 1, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 4, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 31, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 0, "context": "A few recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models.", "startOffset": 20, "endOffset": 104}, {"referenceID": 5, "context": "A few recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models.", "startOffset": 20, "endOffset": 104}, {"referenceID": 22, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 149}, {"referenceID": 31, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 149}, {"referenceID": 4, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 149}, {"referenceID": 17, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 194}, {"referenceID": 6, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 194}, {"referenceID": 18, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 194}, {"referenceID": 22, "context": "Then (Goodfellow et al., 2014) tries to clarify that the primary cause of such vulnerability is the linear nature of DNNs.", "startOffset": 5, "endOffset": 30}, {"referenceID": 19, "context": "More subsequent papers (Fawzi et al., 2015; Papernot et al., 2015a; Sabour et al., 2015; Nguyen et al., 2015) have explored other ways of adversarial manipulations on DNN outputs or deep representations recently.", "startOffset": 23, "endOffset": 109}, {"referenceID": 30, "context": "For instance, denoising NN architectures (Vincent et al., 2008; Gu & Rigazio, 2014; Jin et al., 2015) can discover more robust features by using noise corrupted version of inputs as training samples.", "startOffset": 41, "endOffset": 101}, {"referenceID": 40, "context": "More recent techniques incorporate smoothness penalty (Miyato et al., 2016; Zheng et al., 2016) or layer-wise penalty (Carlini & Wagner, 2016b) as a regularization term in the loss function to promote the smoothness of the DNN model distributions.", "startOffset": 54, "endOffset": 95}, {"referenceID": 2, "context": "(Barreno et al., 2010) and Biggio et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "(Biggio et al., 2008) propose a method to introduce some randomness in the selection of classification boundaries; (2) A few recent studies (Xiao et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Related efforts include perfect information assumption (Dalvi et al., 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst noise) (Dekel et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 15, "context": ", 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst noise) (Dekel et al., 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning by (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).", "startOffset": 273, "endOffset": 319}, {"referenceID": 13, "context": ", 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning by (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).", "startOffset": 83, "endOffset": 128}, {"referenceID": 20, "context": "almost everywhere (Folland, 2013)2", "startOffset": 18, "endOffset": 33}, {"referenceID": 22, "context": "Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).", "startOffset": 142, "endOffset": 236}, {"referenceID": 23, "context": "Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).", "startOffset": 142, "endOffset": 236}, {"referenceID": 31, "context": "Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).", "startOffset": 142, "endOffset": 236}, {"referenceID": 4, "context": "Most previous definitions about attacking machine learning models (Biggio et al., 2013; Lowd & Meek, 2005) view generating an adversarial samples as a constrained optimization problem.", "startOffset": 66, "endOffset": 106}, {"referenceID": 4, "context": "For instance, the authors of (Biggio et al., 2013) assume that d2(x, x\u2032) = ||r||2.", "startOffset": 29, "endOffset": 50}, {"referenceID": 22, "context": "(Goodfellow et al., 2014) assumes that d2(x, x\u2032) = ||r||\u221e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 22, "context": "f1 d2 (Goodfellow et al., 2014) Convolutional neural networks `\u221e (Szegedy et al.", "startOffset": 6, "endOffset": 31}, {"referenceID": 4, "context": ", 2013) Convolutional neural networks `2 (Biggio et al., 2013) Support vector machine `2 (Kantchelian et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 31, "context": ", 2013) Support vector machine `2 (Kantchelian et al., 2015) Decision tree and Random forest `2, `1, `\u221e (Grosse et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 23, "context": ", 2015) Decision tree and Random forest `2, `1, `\u221e (Grosse et al., 2016) Convolutional neural networks `0", "startOffset": 51, "endOffset": 72}, {"referenceID": 4, "context": "Gradient ascent method (Biggio et al., 2013) The easiest way to solve Eq.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": "Fast gradient sign method (Goodfellow et al., 2014) Fast gradient sign method view d2 as the `\u221e-norm.", "startOffset": 26, "endOffset": 51}, {"referenceID": 34, "context": "A recent paper (Kurakin et al., 2016) shows that adversarial examples generated by fast gradient sign method works are misclassified even perceived through cameras.", "startOffset": 15, "endOffset": 37}, {"referenceID": 4, "context": "is a hidden assumption made by most previous studies(Biggio et al., 2013; Lowd & Meek, 2005).", "startOffset": 52, "endOffset": 92}, {"referenceID": 27, "context": ", (Szegedy et al., 2013; Nguyen et al., 2015; He et al., 2015; Papernot et al., 2016a; Moosavi-Dezfooli et al., 2015; Papernot et al., 2015b)).", "startOffset": 2, "endOffset": 141}, {"referenceID": 27, "context": "The model we uses is a 200-layer residual network(He et al., 2015) on Imagenet dataset(Deng et al.", "startOffset": 49, "endOffset": 66}, {"referenceID": 16, "context": ", 2015) on Imagenet dataset(Deng et al., 2009).", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": "Siamese network (Bromley et al., 1993) is a traditional approach of non-linear embedding methods.", "startOffset": 16, "endOffset": 38}, {"referenceID": 33, "context": "This method has long been used in many fields, including face recognition (Krizhevsky et al., 2012) and dimension reduction (Hadsell et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 25, "context": ", 2012) and dimension reduction (Hadsell et al., 2006).", "startOffset": 32, "endOffset": 54}], "year": 2017, "abstractText": "Adversarial samples are maliciously created inputs that force a machine learning classifier to produce wrong output labels. An adversarial sample is often generated by adding adversarial noise (AN) to a normal sample. Recent literature has pointed out that machine learning classifiers, including deep neural networks (DNN), are vulnerable to AN. Multiple studies have tried to analyze and thus harden machine classifiers under AN. However, they are mostly empirical and provide little understanding of the underlying principles that enable evaluation of the robustness of a classier against AN. This paper proposes a unified framework using two metric spaces to evaluate classifiers\u2019 robustness against AN and provides general guidance for hardening such classifiers. The central idea of our work is that for a certain classification task, the robustness of a classifier f1 against AN is decided by both f1 and its oracle f2 (like human annotator of that specific task). In particular: (1) By adding oracle f2 into the framework, we provide a general definition of the adversarial sample problem. (2) We theoretically formulate a definition that decides whether a classifier is always robust against AN (strongrobustness); (3) Using two metric spaces (X1, d1) and (X2, d2) defined by f1 and f2 respectively, we prove that the topological equivalence between (X1, d1) and (X2, d2) is sufficient in deciding whether f1 is strong-robust at test time, or not; (4) Then a novel measure referred to Adversarial Robustness of Classifier (ARC) is defined to quantify the robustness of a classifier against AN; (5) By training a DNN classifier using the Siamese architecture, we propose a new defense strategy \u201cSiamese training\u201d to intuitively approach topological equivalence between (X1, d1) and (X2, d2). Experimental results show that Siamese training helps multiple DNN models achieve better accuracy compared to previous defense strategies in an adversarial setting. Using the proposed measure ARC, DNN models after Siamese training exhibit better robustness than the state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}