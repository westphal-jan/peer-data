{"id": "1612.00377", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Piecewise Latent Variables for Neural Variational Text Processing", "abstract": "recent advances in neural variational inference typically have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. however, otherwise these models usually assume simple, uni - modal priors - such as the multivariate gaussian distribution - yet many real - world data structures distributions are highly complex and multi - modal. examples of complex regions and multi - modal distributions range from specialized topics in newswire text to conversational dialogue responses. when such latent variable models broadly are applied to these domains, the restriction of the naive simple, uni - modal prior hinders much the overall expressivity of the uniquely learned model as it cannot possibly capture more socially complex aspects dependent of the individual data distribution. to overcome this critical restriction, then we propose a flexible, simple nash prior order distribution which can might be learned nearly efficiently and potentially capture an exponential number of modes of a target distribution. we develop the multi - modal optimal variational adaptive encoder - decoder framework and investigate the comparative effectiveness of the proposed prior in creating several natural language processing modeling tasks, including document modeling and dialogue modeling.", "histories": [["v1", "Thu, 1 Dec 2016 18:49:23 GMT  (192kb,D)", "http://arxiv.org/abs/1612.00377v1", "18 pages, 2 figures, 4 tables; under review at ICLR 2017"], ["v2", "Fri, 9 Dec 2016 03:18:54 GMT  (192kb,D)", "http://arxiv.org/abs/1612.00377v2", "18 pages, 2 figures, 4 tables; under review at ICLR 2017"], ["v3", "Thu, 13 Jul 2017 19:25:58 GMT  (1016kb,D)", "http://arxiv.org/abs/1612.00377v3", "19 pages, 2 figures, 8 tables; EMNLP 2017"], ["v4", "Sat, 23 Sep 2017 13:33:55 GMT  (1029kb,D)", "http://arxiv.org/abs/1612.00377v4", "19 pages, 2 figures, 8 tables; EMNLP 2017"]], "COMMENTS": "18 pages, 2 figures, 4 tables; under review at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alexander g ororbia ii", "joelle pineau", "aaron c courville"], "accepted": true, "id": "1612.00377"}, "pdf": {"name": "1612.00377.pdf", "metadata": {"source": "CRF", "title": "MULTI-MODAL VARIATIONAL ENCODER-DECODERS", "authors": ["Iulian V. Serban", "Alexander G. Ororbia II", "Joelle Pineau", "Aaron Courville"], "emails": [], "sections": [{"heading": null, "text": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, unimodal priors \u2014 such as the multivariate Gaussian distribution \u2014 yet many realworld data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling."}, {"heading": "1 INTRODUCTION", "text": "With the development of the variational autoencoding framework (Kingma & Welling, 2013; Rezende et al., 2014), a tremendous amount of progress has been made in learning large-scale, directed latent variable models. This approach has lead to improved performance in applications ranging from computer vision (Gregor et al., 2015; Larsen et al., 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al., 2015; Bowman et al., 2015; Serban et al., 2016b). Furthermore, these models naturally incorporate a Bayesian modeling perspective, by enabling the integration of problem-dependent knowledge in the form of a prior on the generating distribution.\nHowever, the majority of models proposed assume an extremely simple prior in the form of a multivariate Gaussian distribution in order to maintain mathematical and computational tractability. Although this assumption on the prior has lead to favorable results on several tasks, it is clearly a restrictive and often unrealistic assumption. First, it imposes a strong uni-modal structure on the latent variable space; latent samples from the generating model (prior distribution) all cluster around a single mean. Second, it encourages local smoothness on the latent variables; the similarity between two latent variables decreases exponentially as their distance increase. Thus, for complex, multi-modal distributions \u2014 such as the distribution over topics in a text corpus, or natural language responses in a dialogue system \u2014 the uni-modal Gaussian prior inhibits the model\u2019s ability to extract and represent important structure in the data. To learn more powerful and expressive models \u2014 in particular, models with multi-modal latent variable structures for natural language processing applications \u2014 we seek a suitable and flexible prior than can be automatically adapted to model multiple modes of a target distribution.\n\u2217First two authors contributed equally.\nar X\niv :1\n61 2.\n00 37\n7v 1\n[ cs\n.C L\n] 1\nD ec\n2 01\nIn this paper, we propose the multi-modal variational encoder-decoder framework, introducing an efficient, flexible prior distribution that is suitable for distributions such as those found in natural language text. We demonstrate the effectiveness of our multi-modal variational architectures in two representative tasks: document modeling and dialogue modeling. We find that our prior is able to capture elements of a target distribution that simpler priors \u2014 such as the uni-modal Gaussian \u2014 cannot model, thus allowing neural latent variable models to extract richer structure from data. In particular, we achieve state-of-the-art results on several document modeling tasks."}, {"heading": "2 RELATED WORK", "text": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference. NVIL allows for the joint training of an inference network and directed generative model, maximizing a variational lower-bound on the data log-likelihood and facilitating exact sampling of the variational posterior. Simultaneously with this work, the variational autoencoder framework was proposed by Kingma & Welling (2013) and Rezende et al. (2014). This framework is the motivation of this paper, and will be discussed in detail in the next section.\nWith respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999). Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014). In particular, Mnih & Gregor (2014) showed that using NVIL yields better generative models of documents than these previous approaches. The success of these discrete latent variable models \u2014 which are able to partition probability mass into separate regions \u2014 serve as the main motivation for investigating models with continuous multi-modal latent variables for document modeling. More recently, Miao et al. (2015) have proposed continuous latent variable representations for document modeling, which has achieved state-of-the-art results. This model will be described later.\nWith respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others. More recently, Zhai & Williams (2014) have proposed three models combining hidden Markov models and topic models. The success of these discrete latent variable models also motivates our investigation into dialogue models with multi-modal latent variables. Most related to our work is the Variational Hierarchical Recurrent Encoder-Decoder (VHRED) model by Serban et al. (2016b), which is a neural architecture with latent multivariate Gaussian variables. This model will be described later.\nThere has been some work exploring alternative distributions for the latent variables in the variational autoencoder framework, including multi-modal distributions. Rezende & Mohamed (2015) propose an approach called normalizing flows which computes a more complex, potentially multimodal distribution, by projecting standard Gaussian variables through a sequence of non-linear transformations. This approach is similar to the inverse auto-regressive flow proposed by Kingma et al. (2016). Unfortunately, both normalizing flows and auto-regressive flow are only applicable to the approximate posterior distribution; typically these approaches require fixing the prior distri-\nbution to a uni-modal multivariate Gaussian. Furthermore, to the best of our knowledge, neither of these approaches have been investigated in the context of larger scale text processing tasks, such as the document modeling and dialogue modeling tasks we evaluate on. A complementary approach is to combine variational inference with MCMC sampling (Salimans et al., 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks. Enriching the latent variable distributions has also been investigated by Maal\u00f8e et al. (2016)."}, {"heading": "2.1 APPROACHES FOR LEARNING MULTI-MODAL LATENT VARIABLES", "text": "Mixture of Gaussians Perhaps the most direct and naive approach to learning multi-modal latent variables is to parametrize the latent variable prior and approximate posterior distributions as a mixture of Gaussians. However, the KL divergence between two mixtures of Gaussian distributions cannot be computed in closed form (Durrieu et al., 2012). To train such a model, one would have to either resort to MCMC sampling, which may slow down and hurt the training process due to the high variance it incurs, or resort to approximations of the KL divergence, which may also hurt the training process.1\nDeep Directed Models An alternative to a mixture of Gaussians parametrization is to construct a deep directed graphical model composed of multiple layers of uni-modal latent variables (e.g. multivariate Gaussians) (Rezende et al., 2014). Such models have the potential to capture highly complex, multi-modal latent variable representations through the marginal distribution of the toplayer latent variables. However, this approaches has two major drawbacks. First, the variance of the gradient estimator grows with the number of layers. This makes it difficult to learn highly multimodal latent representations. Second, it is not clear how many modes such models can represent or how their inductive biases will affect their performance on tasks containing multi-modal latent structure. The piecewise constant latent variables we propose do not suffer from either of these two drawbacks; the piecewise constant variables incur low variance in the gradient estimator, and can, in principle, represent a number of modes exponential in the number of latent variables.\nDiscrete Latent Variables A third approach for learning multi-modal latent representations is to instead use discrete latent variables as discussed above. For example, the learning procedure proposed by Mnih & Gregor (2014) for discrete latent variables can easily be combined with the variational autoencoder framework to learn models with both discrete and continuous latent variables. However, the major drawback of discrete latent variables is the high variance in the gradient estimator. Without further approximations, it might be difficult to scale up models with discrete latent variables for real-world tasks."}, {"heading": "3 THE MULTI-MODAL VARIATIONAL ENCODER-DECODER FRAMEWORK", "text": "We start by describing the general neural variational learning framework. Then we present our proposed prior model aimed at enhancing the model\u2019s ability to learn multiple modes of data distributions. We focus on modeling discrete output variables in the context of natural language processing applications. However, the framework can easily be adapted to handle continuous output variables, such as images, video and audio."}, {"heading": "3.1 NEURAL VARIATIONAL LEARNING", "text": "Let w1, . . . , wN be a sequence of N words conditioned on a continuous latent variable z. In the general framework, the distribution over the variables follows the directed graphical model:\nP\u03b8(w1, . . . , wN , z) = \u222b N\u220f n=1 P\u03b8(wn|w<n, z)P\u03b8(z)dz, (1)\nwhere \u03b8 are the model parameters. The model first generates the higher-level, continuous latent variable z, and then, conditioned on this, generates the word sequence. The document modeling\n1Our lab has previously investigated incorporating mixture of Gaussian models into the autoencoder framework, but without any success. This work has not been published.\ntask further simplifies the model by assuming the words are independent of each other:\nP\u03b8(w1, . . . , wN , z) = \u222b N\u220f n=1 P\u03b8(wn|z)P\u03b8(z)dz. (2)\nFollowing the variational autoencoder (VAE) framework Kingma & Welling (2013), the parameters can be learned using the variational lower-bound: logP\u03b8(w1, . . . , wN , z) \u2265 Ez\u223cQ\u03c8(z|w1,...,wN )[logP\u03b8(wn|w<n, z)]\u2212 KL [Q\u03c8(z|w1, . . . , wN )||P\u03b8(z)] , (3) where Q\u03c8(z|w1, . . . , wN ) is the approximation to the posterior for z, called the encoder, or sometimes the recognition model or inference model, with parameters \u03c8. The distribution P\u03b8(z) is the prior model for z. The variational autoencoder model further makes use of the re-parametrization trick, which allows one to move the derivative of the lower-bound to inside the expectation. To accomplish this, we need to parametrize z as a transformation from a fixed (parameter-less) random distribution:\nz = f\u03b8( ), (4) where is drawn from a random distribution, e.g. a standard Gaussian distribution (with zero mean and unit standard deviation) or a uniform distribution in the interval [0, 1], and f is some transformation of this variable, also parametrized by \u03b8.\nThe majority of work on VAEs that uses the re-parametrization trick propose to parametrize z \u2014 both the prior and approximate posterior (encoder) \u2014 as a multivariate Gaussian variable. However, the multivariate Gaussian is a uni-modal distribution and can therefore only represent one mode in latent space. This means the mapping from latent variable to outputs \u2014 i.e. the conditional distribution P\u03b8(wn|z) \u2014 has to be highly non-linear in order to capture additional modes. However, in general, it is difficult to learn such non-linear mappings with existing stochastic optimization methods, such as mini-batch stochastic gradient descent and its variants. Learning such a non-linear mapping is particularly difficult using the variational bound in eq. (3), because it incurs additional variance from sampling the latent variable z. Consequently, such a model is very likely to converge on a solution which does not model multi-modality which then leads to a poor approximation of the output distribution."}, {"heading": "3.2 THE PIECEWISE-CONSTANT PRIOR FOR LATENT VARIABLES", "text": "In this work, we overcome the uni-modal restriction by parametrizing z using a piecewise constant probability density function (PDF). This parametrization will allow z to represent complex aspects of the data distribution in latent variable space, such as multiple modes and highly non-smooth regions of probability mass. From a manifold learning perspective, this extension translates into expanding the set of manifolds representable by the model parameters to include more non-linear manifolds \u2013 in particular, manifolds where there exists separate clusters of probability mass.\nLet n \u2208 N be the number of piecewise constant components. We assume z is drawn from the PDF:\nP (z) = 1\nK n\u2211 i=1 1( i\u2212 1 n \u2264z\u2264 i n\n)ai, (5) where 1(x) is the indicator function (which is one whenever x is true and otherwise zero), ai > 0 for i = 1, . . . , n are the distribution parameters (which will be learned during training), and K is the normalization constant:\nK = n\u2211 i=1 Ki, where K0 := 0,Ki := ai n for i = 1, . . . , n. (6)\nTo train the model using the re-parametrization trick, we need to generate z = f( ) where \u223c Uniform(0, 1). To do so, we employ inverse transform sampling (Devroye, 1986), which requires finding the inverse of the cumulative distribution function (CDF). We first derive the CDF of eq. (5):\n\u03c6(z) = 1\nK n\u2211 i=1 1( i n \u2264z )Ki + 1( i\u2212 1 n \u2264z\u2264 i n )(z \u2212 i\u2212 1 n ) ai. (7)\nNext, we derive its inverse:\n\u03c6\u22121( ) = n\u2211 i=1 1( 1 K \u2211i\u22121 j=0Kj\u2264 \u2264 1 K \u2211i j=0Kj\n)  i\u2212 1\nn + K\nai\n \u2212 1 K i\u22121\u2211 j=0 Kj  (8) Armed with the inverse CDF, we can now draw a sample z:\nz = \u03c6\u22121( ), where \u223c Uniform(0, 1). (9)\nIn addition to sampling, we need to compute the Kullback-Leibler (KL) divergence between the prior and approximate posterior distributions of the piecewise constant variables. We assume both the prior and the posterior are piecewise constant distributions. We use prior to denote prior parameters and post to denote posterior parameters (encoder model parameters). The KL divergence between the prior and posterior can be computed using a sum of integrals, where each integral inside the sum corresponds to one constant segment:\nKL [Q\u03c8(z|w1, . . . , wN )||P\u03b8(z)] = \u222b 1 0 Q\u03c8(z|w1, . . . , wN ) log ( Q\u03c8(z|w1, . . . , wN ) P\u03b8(z) ) dz (10)\n= n\u2211 i=1 \u222b 1/n 0 aposti Kpost log ( aposti /K post apriori /K prior ) dz (11)\n(12)\n= 1\nn n\u2211 i=1 aposti Kpost log\n( aposti /K post\napriori /K prior\n) (13)\n= 1\nn\n1\nKpost n\u2211 i=1 aposti ( log(aposti )\u2212 log(a prior i ) ) (14)\n+ log(Kprior)\u2212 log(Kpost)\nIn order to train the model, we take partial derivatives of the variational bound in eq. (3) w.r.t. each parameter in \u03b8 and \u03c8. These expressions involve derivatives of the indicator functions, which have derivatives zero everywhere except for the changing points where the derivative is undefined. However, the probability of sampling such that an indicator function is exactly at its changing point is effectively zero. Therefore, we fix their derivatives to zero.2 A similar approach is used for training neural networks with rectified linear units. Figure 1 illustrates how the piecewise constant latent variables can work with Gaussian latent variables in order to model multi-modality."}, {"heading": "4 LATENT VARIABLE PARAMETRIZATIONS", "text": "The latent variable parametrizations are crucial to modeling the data effectively. In this section, we will develop the parametrizations for both the Gaussian variable and our proposed piecewise latent variable.\nFor all parametrizations, let c be the conditioning information for the prior. In document modeling there is no conditioning information available to the prior, so c = \u2205. In dialogue modeling c is the vector representation of the dialogue context, namely all previous utterances until the current time step. Let x be the current output sequence (observation), which the model must generate (e.g. w1, . . . , wN for document modeling)."}, {"heading": "4.1 GAUSSIAN PARAMETRIZATION", "text": "Let \u00b5prior and \u03c32,prior be the prior mean and variance, and let \u00b5post and \u03c32,post be the posterior mean and variance. For Gaussian latent variables, the prior distribution mean and variances are encoded using linear transformations of a hidden state. In particular, the prior distribution covariance is\n2We thank Christian A. Naesseth for pointing out this assumption.\nencoded as a diagonal covariance matrix using a softplus function:\n\u00b5prior = Hprior\u00b5 Enc(c) + b prior \u00b5 , (15)\n\u03c32,prior = diag(log(1 + exp(Hprior\u03c3 Enc(c) + b prior \u03c3 ))), (16)\nwhere Enc(c) is an embedding/encoding of the context c (e.g. given by a bag-of-words encoder or an LSTM encoder applied to c), which is shared across all latent variable dimensions. The parameters Hprior\u00b5 , b prior \u00b5 , H prior \u03c3 , b prior \u03c3 are to be learned.\nFor the posterior distribution, our preliminary experiments have shown that it is much better to parametrize the posterior distribution by interpolating between the prior distribution mean and variance and a new estimate of the mean and variance. This interpolation is controlled by a gating mechanism, which makes it easy for the model to learn how to turn on/off latent dimensions:\n\u00b5post = (1\u2212 \u03b1\u00b5)\u00b5prior + \u03b1\u00b5 ( Hpost\u00b5 Enc(c, x) + b post \u00b5 ) , (17)\n\u03c32,post = (1\u2212 \u03b1\u03c3)\u03c32,prior + \u03b1\u03c3diag(log(1 + exp(Hpost\u03c3 Enc(c, x) + bpost\u03c3 ))), (18)\nwhere Enc(c, x) is an encoding/embedding of both c and x, and where the parameters are Hpost\u00b5 , b post \u00b5 , H post \u03c3 , b post \u03c3 , \u03b1\u00b5, \u03b1\u03c3 . The interpolation mechanism is controlled by \u03b1\u00b5 and \u03b1\u03c3 , which are initialized to zero (i.e. initialized such that the posterior is equal to the prior).3"}, {"heading": "4.2 PIECEWISE CONSTANT PARAMETRIZATION", "text": "Similar to the Gaussian variances, we propose to parametrize the piecewise constant prior parameters using an exponential function applied to a linear transformation of the context embedding/encoding:\napriori = exp(H prior a,i Enc(c) + b prior a,i ), i = 1, . . . , n, (19)\nwhere Hpriora and b prior a are the parameters to be learned.\nWe may also constrain the piecewise constant posterior parameters to be an interpolation between the prior parameters and a new estimated parameter:\naposti = (1\u2212 \u03b1a,i)a prior i + \u03b1a,i exp(H post a,i Enc(c, x) + b post a,i ), i = 1, . . . , n, (20)\n3We experimented with more sophisticated mechanisms for controlling the gating variables, including defining \u03b1\u00b5 and \u03b1\u03c3 to be a linear function of the encoder. However, we found that that simpler was often was better and thus do not report these results using more advanced mechanisms.\nwhereHposta , b post a , \u03b1a are the parameters. However, we found that this interpolation hurt performance and therefore fixed \u03b1a = 1.\nTo take advantage of the properties of both priors, the Gaussian and piecewise constant variables may be combined, as was suggested in Section 3.2. In this work, we simply experimented with their concatenation to create a hybrid model."}, {"heading": "5 VARIATIONAL TEXT MODELING", "text": "We now present two probabilistic models, the NVDM and the VHRED, which are extended to incorporate the latent variable parametrization and used for the document modeling and the dialogue modeling experiments described below."}, {"heading": "5.1 NEURAL VARIATIONAL DOCUMENT MODEL (NVDM)", "text": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x). Let V be the vocabulary. Let W represent a document matrix, where row wi is the 1-of-|V | binary encoding of the i\u2019th word in the document. Enc(W ) is trained to compress a document vector into a continuous distributed representation upon which the posterior model is built.\nThe NVDM parametrization requires only learning the parameters bpriora ,W post a , b post a for the piecewise variables, and learning the parameters bprior\u00b5 , b prior \u03c3 ,W post \u00b5 , b post \u00b5 ,W post \u03c3 , b post \u03c3 for the Gaussian variables. We initialize the bias parameters to zero, in order for the NVDM to start with a centered Gaussian prior. This prior will be adapted by the parametric encoder as learning progresses, while also learning to turn on/off latent dimensions controlled through the gating mechanism. It is important to note that our particular instantiation of the NVDM is different from that of Mnih & Gregor (2014) and Miao et al. (2015); we jointly learn the prior mean and variance whereas in previous work it has been assumed to be a standard Gaussian. Furthermore, our models learn to interpolate between the generated prior and posterior models to calculate a new posterior.\nBased on preliminary experiments, we choose the encoder to be a 2-hidden layer perceptron, defined by parameters {E0, b0, E1, b1}. The decoder is defined by parameters {R, c}. For example, in the case of the hybrid VAE we use eq. (15)\u2013(20) to generate the distribution parameters. In this case, to draw a sample from the Gaussian prior, we draw a standard Gaussian variable and then multiply it by the standard deviation and add the mean of the Gaussian prior. To draw a sample from the piecewise prior, we use eq. (8). As such, the complete architecture is:\n\u03c0(W ) = f0(E0W + b0),\nEnc(W ) = f1(E1\u03c0(W ) + b1),\nzGaussian = \u00b5 post + \u221a \u03c32,post \u2297 0,\nzPiecewise = \u03c6 \u22121,post( 1),\nz = \u3008zGaussian, zPiecewise\u3009, Dec(w, z) = g(\u2212wTRz),\nwhere\u2297 is the Hadamard product, \u3008\u25e6, \u25e6\u3009 is an operator that combines the Gaussian and the Piecewise variables 4 and Dec(w, z) is the decoder model. As a result of using the re-parametrization trick and choice of prior, we calculate the latent variable z through the two samples, 0 and 1. f(\u25e6) is a non-linear activation function. We choose it to be the softsign function, or f(v) = v/(1 + |v|). The decoder model Dec(z) outputs a probability distribution over words conditioned on z. In this case, we define g(\u25e6) as the softmax function (omitting the bias term c for clarity) computed as:\nDec(w, z) = P\u03b8(w|z) = exp (\u2212wTRz)\u2211 w\u2032 exp (\u2212wTRz) ,\n4Operations include vector concatenation, summation, or averaging.\nThe decoder\u2019s output is used to calculate the first term in the variational lower-bound: logP\u03b8(W |z). The prior and posterior distributions are used to compute the KL term in the variational lower-bound. The lower-bound defined becomes:\nL = EQ\u03c8(z|W ) [ N\u2211 i=1 logP\u03b8(wi|z) ] \u2212 KL [Q\u03c8(z|W )||P\u03b8(z)] ,\nwhere the KL term is the sum of the Gaussian and piecewise KL-divergence measures:\nKL [Q(z|W )||P (z)] = KLGaussian [Q(z|W )||P (z)] + KLPiecewise [Q(z|W )||P (z)] .\nThe KL-terms may be interpreted as regularizers of the parameter updates for the encoder model (Kingma & Welling, 2013). These terms encourage the posterior distributions to be similar to their corresponding prior distributions, by limiting the amount of information the encoder model transmits regarding the output. For example, it encourages the uni-modal Gaussian posterior to move its mean close to the mean of the Gaussian prior, which makes it difficult for the Gaussian posterior to represent different modes conditioned on the observation. Similarly, this encourages the piecewise constant posterior to be similar to the piecewise constant prior. However, since the piecewise constant posterior is multi-modal, it may be able to shift some of its probability mass towards the prior distribution while keeping other probability mass on one or several modes dependent upon the output observation (e.g. if the prior distribution is a uniform distribution and the true posterior concentrates all its probability mass in several small regions, then the approximate posterior could interpolate between the prior and the true posterior)."}, {"heading": "5.2 VARIATIONAL HIERARCHICAL RECURRENT ENCODER-DECODER (VHRED)", "text": "The VHRED model is an extension of the hierarchical recurrent encoder-decoder model (HRED) for dialogue (Serban et al., 2016b;a). The model decomposes dialogues using a two-level hierarchy: sequences of utterances (e.g. sentences), and sub-sequences of tokens (words). Let wn be the n\u2019th utterance in a dialogue with N utterances. Let wn,m be the m\u2019th word in the n\u2019th utterance from vocabulary V , and let Mn be the number of words in the n\u2019th utterance. In addition to this, VHRED has a latent multivariate continuous variable zn for each utterance n = 1, . . . , N . The probability distribution of the generative model factorizes as:\nP\u03b8(w1, . . . ,wN ) = N\u220f n=1 P\u03b8(wn|w<n, zn)P\u03b8(zn|w<n),\n= N\u220f n=1 Mn\u220f m=1 P\u03b8(wn,m|wn,<m,w<n, zn)P\u03b8(zn|w<n), (21)\nwhere \u03b8 are the model parameters. VHRED uses three RNN modules: an encoder RNN, a context RNN and a decoder RNN. First, each utterance is encoded into a vector by the encoder RNN:\nhencn,0 = 0, h enc n,m = f enc \u03b8 (h enc n,m\u22121, wn,m) \u2200m = 1, . . . ,Mn,\nwhere f enc\u03b8 is either a GRU or a bidirectional GRU function. The last hidden state of the encoder RNN is given as input to the context RNN. Then, the context RNN updates its internal hidden state to reflect all the information up until that utterance:\nhcon0 = 0, h con n = f con \u03b8 (h con n\u22121, h enc n,Mn),\nwhere f con\u03b8 is a GRU function taking as input two vectors. This state is used to compute the prior distribution over the latent variable zn:\nP\u03b8(zn | w<n) = f prior\u03b8 (h con n\u22121), (22)\nwhere f prior is a PDF parametrized by both \u03b8 and hconn . Next, a sample is drawn from this distribution: zn \u223c P\u03b8(zn|w<n). The sample and context state are given as input to the decoder RNN:\nhdecn,0 = 0, h dec n,m = f dec \u03b8 (h dec n,m\u22121, h con n\u22121, zn, wn,m)\n\u2200m = 1, . . . ,Mn,\nwhere f dec\u03b8 is the LSTM gating function taking as input four vectors. The output distribution is computed by passing hdecn,m through an MLP f mlp \u03b8 , an affine transformation and a softmax function:\nP\u03b8(wn,m+1|wn,\u2264m,w<n, zn) = e(Own,m+1) Tfmlp\u03b8 (h dec n,m)\u2211\nw\u2032 e (Ow\u2032)Tfmlp\u03b8 (h dec n,m)\n, (23)\nwhereO \u2208 R|V |\u00d7d is the word embedding matrix for the output distribution with embedding dimensionality d \u2208 N. The model is trained by maximizing the variational lower-bound, which factorizes into independent terms for each sub-sequence (utterance):\nlogP\u03b8(w1, . . . ,wN ) \u2265 N\u2211 n=1 \u2212 KL [Q\u03c8(zn | w1, . . . ,wn)||P\u03b8(zn | w<n)]\n+ EQ\u03c8(zn|w1,...,wn) [logP\u03b8(wn | zn,w<n)] , (24) where distribution Q\u03c8 is the approximate posterior distribution with parameters \u03c8, which is computed similar to the prior distribution but further conditioned on the future encoder RNN hidden state:\nQ\u03c8(zn | w\u2264n) = f post\u03c8 (h con n\u22121, h enc n,Mn), (25)\nwhere f post is a PDF. More details are given by Serban et al. (2016b).\nThe original VHRED model as described by Serban et al. (2016b) used only Gaussian latent variables. We will refer to this model as Gaussian-VHRED (G-VHRED). The VHRED model with both Gaussian and piecewise constant latent variables will be referred to as Hybrid-VHRED (HVHRED). In this case, we combine the Gaussian and piecewise latent variables by concatenating them into one vector.5"}, {"heading": "6 EXPERIMENTS", "text": "In order to validate the ability of our piecewise latent variables in capturing complex aspects of data distributions, we conduct experiments with both the NVDM and VHRED models.\nAll models are trained using back-propagation to obtain parameter gradients with respect to the variational lower-bound on the log-likelihood or the exact log-likelihood. We used a standard firstorder gradient-descent optimizer, Adam (Kingma & Ba, 2015), for both models, where only hyperparameter choices varied depending on the task. The specifics of the design of the encoder and decoder differed between the two tasks (as described in Sections 5.1 and 5.2). For all models that used piecewise latent variables, we chose to fix \u03b1ai = 1, meaning the piecewise prior and posterior models are kept separate (instead of having the posterior be an interpolation between another distribution and the prior), since we found this to perform better6"}, {"heading": "6.1 DOCUMENT MODELING", "text": "For our experiments in document modeling, we make use of the 20 News-Groups dataset. We follow the pre-processing and set-up of Hinton & Salakhutdinov (2009). In addition, we make use of the Reuters corpus (RCV1-V2), using a version that contained a selected 5,000 term vocabulary. 7 Note that the features are a log(1 + TF ) transform of the original frequency vectors. To test our document models on text from another language (in this case, Brazilian Portuguese), we made use of the CADE12 dataset (stop-word removed and stemmed) Cardoso-Cachopo (2007), where we further filtered terms that occurred less than 130 times to obtain a vocabulary of 3,736 terms (over 26,991 training and 13,486 test documents). For all datasets, we track the validation bound on a subset of 100 vectors randomly drawn from each training corpus.\n5Before concatenation, we transform the piecewise constant latent variables to lie within the interval [\u22121, 1]: z\u2032 = 2z\u2212 1. This ensures the input to the decoder RNN has mean zero at the beginning of training.\n6We believe that if \u03b1ai = 0 for a long period of time, then the posterior receives no gradient signal. Without a gradient signal, the estimated posterior becomes increasingly disconnected from the rest of the model and, thus, less effective. This might be due to the choice of non-linearities, which affect the piecewise latent variables moreso than the Gaussian latent variables.\n7We will make the code and scripts used to create the final document input vectors and vocabulary files publicly available upon publication.\nFor the Gaussian NVDM (G-NVDM), we constrained the interpolated posterior variance to lie in the range of [0.01, 10.0]. For the hybrid NVDMs (H-NVDM) 8, we varied the number of components used in the PDF, investigating the effect that 3 and 5 pieces had on the final quality of the model. Parameter updates for all models were estimated using mini-batches of 100 samples drawn randomly without replacement from the training data over 150 epochs. A learning rate of 0.002 was used. Model selection and early stopping (the only additional form of regularization employed for this set of experiments) were conducted using the validation lower-bound, estimated using five stochastic samples per validation example. We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al. (2015)).\nIn Table 1, we report the test document perplexity (under the Sampled column), calculated using the standard formula, exp(\u2212 1D \u2211 n 1 Ln\nlogP\u03b8(xn). Note that logP\u03b8(xn), or the log-probability of a particular document, was approximated with an estimate of the variational lower-bound using 10 samples, as was done in Mnih & Gregor (2014). The second score (or column SGD-Inf ), refers to the model\u2019s test-perplexity when the lower-bound is tightened using iterative inference to search for the optimal latent variable per document. In this paper, our iterative inference procedure consisted of simple stochastic gradient descent (no more than 100 steps), with a learning rate of 0.1 and the\n8We ultimately found that averaging the variables, as opposed to using concatenation, yielded best perplexity and thus report these results.\nsame gradient rescaling used in training, using early-stopping (for 20 News-Groups, the lookahead was 10, while on Reuters and CADE the lookahead was 5). The parameters of the model, as the well as the generated prior, are fixed, and the gradients of the variational lower bound with respect to generated posterior model parameters (i.e., the mean and variance of the Gaussian variables, and the piecewise components, ai) are used to update the posterior model for each document (using a freshly drawn sample each step).\nFirst and foremost, we note that the best baseline model (i.e., the NVDM) is more competitive when both the prior and posterior models are learnt together (i.e., the G-NVDM), as opposed to the fixed prior of Miao et al. (2015). However, we observe that integrating our proposed piecewise variables yields even better results in our document modeling experiments, substantially improving over the baselines. More importantly, in some cases, as in the 20 News-Groups and Reuters datasets, increasing the number of pieces from 3 to 5 can further reduce perplexity. Thus, we have achieved a new state-of-the-art perplexity on 20 News-Group task and \u2014 to the best of our knowledge \u2013 better perplexities on the CADE12 and RCV1 tasks compared to using a state-of-the-art model like the GNVDM. Furthermore, we observe iterative inference yields yet a further boost in performance since the bound estimated is tighter, however, this form of inference is expensive and requires additional meta-parameters (e.g., a step-size, an early-stopping criterion, etc.). We remark a simpler, and more accurate, approach to inference would be to use importance sampling.\nIn Table 2, we examine the top ten highest ranked words given a query term, using the decoder parameter matrix (since the decoder is directly affected by the latent variables in our document models). It is clear that the piecewise variables affect what is uncovered by the model with respect to the data, as each model returns different, but relevant results with respect to the query word. In our current examples, it appears that the H-NVDM with 5 pieces returns more general words. For example, in the case of \u201cgovernment\u201d, the baseline seems to value the plural form of the word (which is largely based on morphology) while the hybrid model actually pulls out meaningful terms such as \u201cfederal\u201d, \u201cpolicy\u201d, and \u201cadministration\u201d. The case of \u201cspace\u201d is interesting\u2013the hybrid with 5 pieces seems to value two senses of the word\u2013one related to \u201couter space\u201d (e.g., \u201csun\u201d, \u201cworld\u201d, etc.) and another related to the dimensions of depth, height, and width within which things may exist and move (e.g., \u201carea\u201d, \u201cform\u201d, \u201cscale\u201d, etc.)."}, {"heading": "6.2 DIALOGUE MODELING", "text": "We experiment with VHRED for dialogue modeling. This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a). Related systems for dialogue response generation have recently gained a significant amount of attention from industry, with high-profile projects such as Google\u2019s SmartReply system (Kannan et al., 2016) and Microsoft\u2019s chatbot Xiaolice (Markoff & Mozur, 2015). Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system (Farber, 2016).\nWe focus on non-goal-driven dialogue modeling and use the Twitter Dialogue Corpus (Ritter et al., 2011) based on public Twitter conversations. The dataset is split into training, validation, and test sets, containing respectively 749,060, 93,633 and 9,399 dialogues each. On average, each dialogue contains about 6 utterances (dialogue turns) and about 94 words. The dataset is the same as used by Serban et al. (2016b), but further pre-processed using byte-pair encoding (Sennrich et al., 2016) using a vocabulary consisting of 5000 sub-words.9 The dialogues are substantially longer than recent large-scale language modeling corpora, such as the 1 Billion Word Language Model Benchmark (Chelba et al., 2014), which usually focus on modeling single sentences.\nParameter optimization was conducted with a learning rate of 0.0002 and mini-batches of size 40 or 80.10 We use a variant of truncated back-propagation and apply gradient clipping (Pascanu et al., 2012). Model selection and early stopping \u2014 the only additional form of regularization employed for this set of experiments \u2014 are conducted using the validation lower-bound, estimated using one stochastic sample per validation example.\n9In addition to applying byte-pair encoding, we filtered out 601 test dialogues so that no test dialogue context overlapped with the training or validation sets.\n10We had to vary the mini-batch size to make the training fit on GPU architectures with low memory.\nSimilar to Serban et al. (2016b), we use a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden units. We experiment with context RNN encoders with 500 and 1000 hidden units, and find that that 1000 hidden units reach better performance w.r.t. the variational lower-bound on the validation set. The encoder and context RNNs use layer normalization (Ba et al., 2016). We experiment with decoder RNNs with 1000, 2000 and 4000 hidden units (LSTM cells), and find that 2000 hidden units reach better performance. For the G-VHRED model, we experiment with latent multivariate Gaussian variables with 100 and 300 dimensions, and find that 100 dimensions reach better performance. For the H-VHRED model, we experiment with latent multivariate Gaussian and piecewise constant variables each with 100 and 300 dimensions, and find that 100 dimensions reach better performance. We follow the training procedure of Serban et al. (2016b): we drop words in the decoder with a fixed drop rate of 25% and multiply the KL terms in the variational lower-bound by a scalar, which starts at zero and linearly increases to 1 over the first 60,000 training batches.\nWe also experiment with an LSTM baseline model and a HRED baseline model (Serban et al., 2016a). For the LSTM model, we experiment with number of hidden units (LSTM cells) equal to 1000, 2000 and 4000 and find that 4000 hidden units perform best w.r.t. validation perplextiy. For the HRED model, we use the same encoder and context RNN architectures as the G-VHRED and H-VHRED models described earlier. We set the encoder RNN to have 1000 hidden units. We experiment for the context RNN with 500 and 1000 hidden units, and find that 1000 hidden units reach better performance. For the decoder RNN, we experiment with 1000 and 2000 hidden units (LSTM cells) and find that 2000 hidden units perform better.\nApproximate Posterior Analysis Our hypothesis is that the piecewise constant latent variable are able to capture multi-modal aspects of the dialogue. Therefore, we evaluate the models by analyzing what information they have learned to represent in the latent variables. For each test dialogue with n utterances, we condition each model on the first n \u2212 1 utterances and compute the latent posterior distributions using all n utterances. We then compute the gradients of the KL terms of the multivariate Gaussian and piecewise constant latent variables w.r.t. each word in the dialogue. Since the words vectors are discrete, we compute the sum of the squared gradients w.r.t. each word embedding. The higher the sum of the squared gradients of a word is, the more influence it will have\non the posterior approximation (encoder model). For every test dialogue, we count the top 5 words with highest squared gradients separately for the multivariate Gaussian and piecewise constant latent variables.11\nThe results are shown in Table 3. The piecewise constant latent variables clearly capture different aspects of the dialogue compared to the Gaussian latent variables. The piecewise constant variable approximate posterior encodes words related to time (e.g. weekdays and times of day) and events (e.g. parties, concerts, Easter). On the other hand, the Gaussian variable approximate posterior encodes words related to sentiment (e.g. laughter and appreciation) and acronyms, punctuation marks and emoticons (i.e. smilies). We also conducted a similar analysis on the document models evaluated in Sub-section 6.1, the results of which may be found in the Appendix.\nResponse Evaluation Non-goal-driven dialogue models are typically evaluated by asking humans to rate the quality of different responses. We follow the approach by Liu et al. (2016) by conducting an Amazon Mechanical Turk experiment to compare the G-VHRED and H-VHRED models. For each test dialogue, we use TF-IDF to extract 100 candidate responses (Lowe et al., 2015). We then rank the responses according to the G-VHRED model and H-VHRED model using the variational lowerbound.12 We ask three human evaluators to rate model responses for 45 dialogues on a Likert-type scale 1 \u2212 5, with 1 representing an inappropriate response and 5 representing a highly appropriate response.13 For each dialogue, we show the human evaluators the top two responses ranked by the G-VHRED and H-VHRED models. We choose to evaluate the re-ranked responses for two reasons. First, it reduces variance in the output because it uses the approximate posterior model, compared to using beam search with samples from the high-entropy prior. Second, it decreases the number of generic responses, which are extremely common among generative models and which human evaluators tend to prefer despite not advancing the dialogue (Li et al., 2016).\nThe results are as follows. The G-VHRED model achieved scores 1.88 and 2.13 for the first and second ranked responses on average, and the H-VHRED model achieved scores 1, 93 and 2.04 on average. In other words, H-VHRED performs nominally better on the first ranked response while G-VHRED performs nominally better on the second ranked response. In conclusion, if there exists a difference between the two models, naive human evaluators cannot see it.\nAlthough naive human evaluators cannot distinguish between the model responses, based on our previous analysis we know that the two models encode different aspects of dialogue conversations. Therefore, we further investigate the probability of different responses to dialogue contexts related to time and events. Two examples are shown in Figure 2, where the dialogue contexts are \u201cwhen do you want to meet this weekend?\u201d and \u201cwhere are you going tomorrow?\u201d. H-VHRED assigns substantially more probability mass to relevant words compared to the G-VHRED as well as an LSTM baseline and HRED baseline. This confirms the ability of the piecewise constant latent variable to generate responses related to time and events.\nFinally, we also evaluated the diversity of the G-VHRED and H-VHRED model outputs w.r.t. the top ranked FF-IDF candidate responses. We measured the average word entropy (Serban et al., 2016b) as well as number of unique words for each response and unique words across all test responses, but did not find a significant difference between the two models. This indicates that the Gaussian latent variables alone are able to increase response diversity, while the piecewise constant latent variables instead help encode specific aspects of the dialogue such as time and events.\n11Our approach is equivalent to counting the top 5 words with the highest L2 gradient norms. We also did some experiments using L1 gradient norms, which showed similar patterns.\n12We use one stochastic sample. 13Human evaluators are only given a minimal description of the task, without any examples, before beginning\nthe evaluation."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we have proposed the multi-modal variational encoder-decoder framework. In order to capture complex aspects of unknown data distributions, we developed the piecewise constant prior, which can be efficiently and flexibly adjusted to capture distributions with many modes, such as those over topics. In experiments on document modeling and dialogue modeling, we have shown the effectiveness of our framework in building models capable of learning richer structure from data. In particular, we have demonstrated new state-of-the-art results on several document modeling tasks.\nFuture work should focus on exploring other natural language processing tasks, where multimodality plays an important role such as modeling technical help dialogues (Lowe et al., 2015) and online debates (Rosenthal & McKeown, 2015), and where additional information is available, such as in semi-supervised document categorization Ororbia II et al. (2015a)."}, {"heading": "APPENDIX A: ANALYSIS OF DOCUMENT MODEL PIECEWISE VARIABLES", "text": "We present an additional analysis of the learned 20 News-Groups document models in order to explore what each set of latent variables might be capturing. To calculate the gradient of the KL terms needed to formulate word scores, we follow the approach described in Sub-section 6.2, however, conditioning only on the (training) document bag-of-words to compute the latent posterior to then calculate the gradient of the KL-terms with respect to each word in the document.\nIn Table 4, we observe results similar to those of Sub-section 6.2\u2013the piecewise variables capture different aspects of the document data. It is worth noting, in this experiment, that the Gaussian variables alone were originally were sensitive to some of these words. However, in the hybrid model, nearly all of the temporal words that the Gaussian variables were once more sensitive to now more strongly affect the piecewise variables, which themselves also capture all of the words that were originally missed. This might indicate a shift in responsibility in which latent variables the document model decide are more suitable to capture certain aspects of the data. This effect appears to be even stronger in the case of certain nationality-based adjectives (e.g., \u201camerican\u201d, \u201cisraeli\u201d, etc.). While the G-NVDM could model multi-modality in the data to some degree, this work would be primarily done in the model\u2019s decoder. In the H-NVDM, the piecewise variables provide an explicit mechanism for capturing modes in the unknown target distribution, so it makes sense that the model would learn to use the piecewise variables instead, thus freeing up the Gaussian variables to capture other aspects of the data, as we found was the case with names (e.g., \u201cjesus\u201d, \u201ckent\u201d, etc.)."}], "references": [{"title": "Learning the structure of task-driven human\u2013human dialogs", "author": ["S. Bangalore", "G. Di Fabbrizio", "A. Stent"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Bangalore et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Reweighted wake-sleep", "author": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "venue": "In ICLR 2015,", "citeRegEx": "Bornschein and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bornschein and Bengio.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "In Conference on Computational Natural Language Learning,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Improving Methods for Single-label Text Categorization", "author": ["Ana Cardoso-Cachopo"], "venue": "PdD Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa,", "citeRegEx": "Cardoso.Cachopo.,? \\Q2007\\E", "shortCiteRegEx": "Cardoso.Cachopo.", "year": 2007}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "In INTERSPEECH,", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Unsupervised classification of dialogue acts using a dirichlet process mixture model", "author": ["N. Crook", "R. Granell", "S. Pulman"], "venue": "In Proceedings of the SIGDIAL", "citeRegEx": "Crook et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crook et al\\.", "year": 2009}, {"title": "Varieties of helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "Neural Networks,", "citeRegEx": "Dayan and Hinton.,? \\Q1996\\E", "shortCiteRegEx": "Dayan and Hinton.", "year": 1996}, {"title": "Sample-based non-uniform random variate generation", "author": ["Luc Devroye"], "venue": "In Proceedings of the 18th conference on Winter simulation,", "citeRegEx": "Devroye.,? \\Q1986\\E", "shortCiteRegEx": "Devroye.", "year": 1986}, {"title": "Lower and upper bounds for approximation of the kullback-leibler divergence between gaussian mixture models", "author": ["J-L Durrieu", "J-Ph Thiran", "Finnian Kelly"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Durrieu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Durrieu et al\\.", "year": 2012}, {"title": "Amazon\u2019s \u2019Alexa Prize", "author": ["M. Farber"], "venue": "Will Give College Students Up To $2.5M To Create A Socialbot. Fortune,", "citeRegEx": "Farber.,? \\Q2016\\E", "shortCiteRegEx": "Farber.", "year": 2016}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In ICLR,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E. Hinton", "Ruslan R Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2009}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["Geoffrey E. Hinton", "Richard S. Zemel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hinton and Zemel.,? \\Q1994\\E", "shortCiteRegEx": "Hinton and Zemel.", "year": 1994}, {"title": "The\" wake-sleep\" algorithm for unsupervised neural networks", "author": ["Geoffrey E Hinton", "Peter Dayan", "Brendan J Frey", "Radford M Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Smart Reply: Automated Response Suggestion for Email", "author": ["Anjuli Kannan", "Karol Kurach"], "venue": "In KDD,", "citeRegEx": "Kannan and Kurach,? \\Q2016\\E", "shortCiteRegEx": "Kannan and Kurach", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Stanislas Lauly"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle and Lauly.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Document neural autoregressive distribution estimation", "author": ["Stanislas Lauly", "Yin Zheng", "Alexandre Allauzen", "Hugo Larochelle"], "venue": "arXiv preprint arXiv:1603.05962,", "citeRegEx": "Lauly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["C.-W. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": "In Proceedings of the SIGDIAL 2015 Conference,", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Mozur. For Sympathetic Ear, More Chinese Turn to Smartphone Program", "author": ["P.J. Markoff"], "venue": "NY Times,", "citeRegEx": "Markoff,? \\Q2015\\E", "shortCiteRegEx": "Markoff", "year": 2015}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1511.06038,", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["Radford M Neal"], "venue": "Artificial intelligence,", "citeRegEx": "Neal.,? \\Q1992\\E", "shortCiteRegEx": "Neal.", "year": 1992}, {"title": "Learning a deep hybrid model for semisupervised text classification", "author": ["Alexander G Ororbia II", "C Lee Giles", "David Reitter"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders", "author": ["Alexander G Ororbia II", "C. Lee Giles", "David Reitter"], "venue": "arXiv preprint arXiv:1511.06964,", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende and Mohamed.,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed.", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "I couldn\u2019t agree more: The role of conversational structure in agreement and disagreement detection in online discussions", "author": ["Sara Rosenthal", "Kathleen McKeown"], "venue": "In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Rosenthal and McKeown.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal and McKeown.", "year": 2015}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "In AISTATs, pp", "citeRegEx": "Salakhutdinov and Larochelle.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Larochelle.", "year": 2010}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Tim Salimans", "Diederik P Kingma", "Max Welling"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "In AAAI,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Modeling documents with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1309.6865,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "A deep and tractable density estimator", "author": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "venue": "In ICML, pp", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Discovering latent structure in task-oriented dialogues", "author": ["K. Zhai", "J.D. Williams"], "venue": "In ACL, pp", "citeRegEx": "Zhai and Williams.,? \\Q2014\\E", "shortCiteRegEx": "Zhai and Williams.", "year": 2014}], "referenceMentions": [{"referenceID": 36, "context": "With the development of the variational autoencoding framework (Kingma & Welling, 2013; Rezende et al., 2014), a tremendous amount of progress has been made in learning large-scale, directed latent variable models.", "startOffset": 63, "endOffset": 109}, {"referenceID": 12, "context": "This approach has lead to improved performance in applications ranging from computer vision (Gregor et al., 2015; Larsen et al., 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al.", "startOffset": 92, "endOffset": 134}, {"referenceID": 23, "context": "This approach has lead to improved performance in applications ranging from computer vision (Gregor et al., 2015; Larsen et al., 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al.", "startOffset": 92, "endOffset": 134}, {"referenceID": 30, "context": ", 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al., 2015; Bowman et al., 2015; Serban et al., 2016b).", "startOffset": 39, "endOffset": 122}, {"referenceID": 3, "context": ", 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al., 2015; Bowman et al., 2015; Serban et al., 2016b).", "startOffset": 39, "endOffset": 122}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996).", "startOffset": 103, "endOffset": 168}, {"referenceID": 32, "context": "Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al.", "startOffset": 79, "endOffset": 91}, {"referenceID": 17, "context": "Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999).", "startOffset": 283, "endOffset": 304}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference.", "startOffset": 126, "endOffset": 987}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference. NVIL allows for the joint training of an inference network and directed generative model, maximizing a variational lower-bound on the data log-likelihood and facilitating exact sampling of the variational posterior. Simultaneously with this work, the variational autoencoder framework was proposed by Kingma & Welling (2013) and Rezende et al.", "startOffset": 126, "endOffset": 1464}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference. NVIL allows for the joint training of an inference network and directed generative model, maximizing a variational lower-bound on the data log-likelihood and facilitating exact sampling of the variational posterior. Simultaneously with this work, the variational autoencoder framework was proposed by Kingma & Welling (2013) and Rezende et al. (2014). This framework is the motivation of this paper, and will be discussed in detail in the next section.", "startOffset": 126, "endOffset": 1490}, {"referenceID": 1, "context": "With respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003).", "startOffset": 192, "endOffset": 211}, {"referenceID": 16, "context": "For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999).", "startOffset": 163, "endOffset": 178}, {"referenceID": 46, "context": "Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al.", "startOffset": 209, "endOffset": 234}, {"referenceID": 47, "context": ", 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 24, "context": ", 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 1, "context": "With respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999). Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014). In particular, Mnih & Gregor (2014) showed that using NVIL yields better generative models of documents than these previous approaches.", "startOffset": 193, "endOffset": 851}, {"referenceID": 1, "context": "With respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999). Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014). In particular, Mnih & Gregor (2014) showed that using NVIL yields better generative models of documents than these previous approaches. The success of these discrete latent variable models \u2014 which are able to partition probability mass into separate regions \u2014 serve as the main motivation for investigating models with continuous multi-modal latent variables for document modeling. More recently, Miao et al. (2015) have proposed continuous latent variable representations for document modeling, which has achieved state-of-the-art results.", "startOffset": 193, "endOffset": 1231}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others.", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others. More recently, Zhai & Williams (2014) have proposed three models combining hidden Markov models and topic models.", "startOffset": 79, "endOffset": 181}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others. More recently, Zhai & Williams (2014) have proposed three models combining hidden Markov models and topic models. The success of these discrete latent variable models also motivates our investigation into dialogue models with multi-modal latent variables. Most related to our work is the Variational Hierarchical Recurrent Encoder-Decoder (VHRED) model by Serban et al. (2016b), which is a neural architecture with latent multivariate Gaussian variables.", "startOffset": 79, "endOffset": 521}, {"referenceID": 21, "context": "This approach is similar to the inverse auto-regressive flow proposed by Kingma et al. (2016). Unfortunately, both normalizing flows and auto-regressive flow are only applicable to the approximate posterior distribution; typically these approaches require fixing the prior distri-", "startOffset": 73, "endOffset": 94}, {"referenceID": 41, "context": "A complementary approach is to combine variational inference with MCMC sampling (Salimans et al., 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks.", "startOffset": 80, "endOffset": 123}, {"referenceID": 4, "context": "A complementary approach is to combine variational inference with MCMC sampling (Salimans et al., 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks.", "startOffset": 80, "endOffset": 123}, {"referenceID": 4, "context": ", 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks. Enriching the latent variable distributions has also been investigated by Maal\u00f8e et al. (2016).", "startOffset": 8, "endOffset": 228}, {"referenceID": 10, "context": "However, the KL divergence between two mixtures of Gaussian distributions cannot be computed in closed form (Durrieu et al., 2012).", "startOffset": 108, "endOffset": 130}, {"referenceID": 36, "context": "multivariate Gaussians) (Rezende et al., 2014).", "startOffset": 24, "endOffset": 46}, {"referenceID": 10, "context": "However, the KL divergence between two mixtures of Gaussian distributions cannot be computed in closed form (Durrieu et al., 2012). To train such a model, one would have to either resort to MCMC sampling, which may slow down and hurt the training process due to the high variance it incurs, or resort to approximations of the KL divergence, which may also hurt the training process.1 Deep Directed Models An alternative to a mixture of Gaussians parametrization is to construct a deep directed graphical model composed of multiple layers of uni-modal latent variables (e.g. multivariate Gaussians) (Rezende et al., 2014). Such models have the potential to capture highly complex, multi-modal latent variable representations through the marginal distribution of the toplayer latent variables. However, this approaches has two major drawbacks. First, the variance of the gradient estimator grows with the number of layers. This makes it difficult to learn highly multimodal latent representations. Second, it is not clear how many modes such models can represent or how their inductive biases will affect their performance on tasks containing multi-modal latent structure. The piecewise constant latent variables we propose do not suffer from either of these two drawbacks; the piecewise constant variables incur low variance in the gradient estimator, and can, in principle, represent a number of modes exponential in the number of latent variables. Discrete Latent Variables A third approach for learning multi-modal latent representations is to instead use discrete latent variables as discussed above. For example, the learning procedure proposed by Mnih & Gregor (2014) for discrete latent variables can easily be combined with the variational autoencoder framework to learn models with both discrete and continuous latent variables.", "startOffset": 109, "endOffset": 1673}, {"referenceID": 9, "context": "To do so, we employ inverse transform sampling (Devroye, 1986), which requires finding the inverse of the cumulative distribution function (CDF).", "startOffset": 47, "endOffset": 62}, {"referenceID": 30, "context": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x).", "startOffset": 19, "endOffset": 59}, {"referenceID": 30, "context": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x). Let V be the vocabulary. Let W represent a document matrix, where row wi is the 1-of-|V | binary encoding of the i\u2019th word in the document. Enc(W ) is trained to compress a document vector into a continuous distributed representation upon which the posterior model is built. The NVDM parametrization requires only learning the parameters b a ,W post a , b post a for the piecewise variables, and learning the parameters b \u03bc , b prior \u03c3 ,W post \u03bc , b post \u03bc ,W post \u03c3 , b post \u03c3 for the Gaussian variables. We initialize the bias parameters to zero, in order for the NVDM to start with a centered Gaussian prior. This prior will be adapted by the parametric encoder as learning progresses, while also learning to turn on/off latent dimensions controlled through the gating mechanism. It is important to note that our particular instantiation of the NVDM is different from that of Mnih & Gregor (2014) and Miao et al.", "startOffset": 41, "endOffset": 1163}, {"referenceID": 30, "context": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x). Let V be the vocabulary. Let W represent a document matrix, where row wi is the 1-of-|V | binary encoding of the i\u2019th word in the document. Enc(W ) is trained to compress a document vector into a continuous distributed representation upon which the posterior model is built. The NVDM parametrization requires only learning the parameters b a ,W post a , b post a for the piecewise variables, and learning the parameters b \u03bc , b prior \u03c3 ,W post \u03bc , b post \u03bc ,W post \u03c3 , b post \u03c3 for the Gaussian variables. We initialize the bias parameters to zero, in order for the NVDM to start with a centered Gaussian prior. This prior will be adapted by the parametric encoder as learning progresses, while also learning to turn on/off latent dimensions controlled through the gating mechanism. It is important to note that our particular instantiation of the NVDM is different from that of Mnih & Gregor (2014) and Miao et al. (2015); we jointly learn the prior mean and variance whereas in previous work it has been assumed to be a standard Gaussian.", "startOffset": 41, "endOffset": 1186}, {"referenceID": 43, "context": "More details are given by Serban et al. (2016b).", "startOffset": 26, "endOffset": 48}, {"referenceID": 43, "context": "The original VHRED model as described by Serban et al. (2016b) used only Gaussian latent variables.", "startOffset": 41, "endOffset": 63}, {"referenceID": 5, "context": "To test our document models on text from another language (in this case, Brazilian Portuguese), we made use of the CADE12 dataset (stop-word removed and stemmed) Cardoso-Cachopo (2007), where we further filtered terms that occurred less than 130 times to obtain a vocabulary of 3,736 terms (over 26,991 training and 13,486 test documents).", "startOffset": 162, "endOffset": 185}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models).", "startOffset": 41, "endOffset": 63}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 405}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 491}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 552}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 636}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 690}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 759}, {"referenceID": 30, "context": "On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al. (2015)).", "startOffset": 590, "endOffset": 609}, {"referenceID": 30, "context": ", the G-NVDM), as opposed to the fixed prior of Miao et al. (2015). However, we observe that integrating our proposed piecewise variables yields even better results in our document modeling experiments, substantially improving over the baselines.", "startOffset": 48, "endOffset": 67}, {"referenceID": 38, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 27, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 45, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 25, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 11, "context": "Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system (Farber, 2016).", "startOffset": 160, "endOffset": 174}, {"referenceID": 38, "context": "We focus on non-goal-driven dialogue modeling and use the Twitter Dialogue Corpus (Ritter et al., 2011) based on public Twitter conversations.", "startOffset": 82, "endOffset": 103}, {"referenceID": 42, "context": "(2016b), but further pre-processed using byte-pair encoding (Sennrich et al., 2016) using a vocabulary consisting of 5000 sub-words.", "startOffset": 60, "endOffset": 83}, {"referenceID": 6, "context": "9 The dialogues are substantially longer than recent large-scale language modeling corpora, such as the 1 Billion Word Language Model Benchmark (Chelba et al., 2014), which usually focus on modeling single sentences.", "startOffset": 144, "endOffset": 165}, {"referenceID": 10, "context": "Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system (Farber, 2016). We focus on non-goal-driven dialogue modeling and use the Twitter Dialogue Corpus (Ritter et al., 2011) based on public Twitter conversations. The dataset is split into training, validation, and test sets, containing respectively 749,060, 93,633 and 9,399 dialogues each. On average, each dialogue contains about 6 utterances (dialogue turns) and about 94 words. The dataset is the same as used by Serban et al. (2016b), but further pre-processed using byte-pair encoding (Sennrich et al.", "startOffset": 161, "endOffset": 596}, {"referenceID": 35, "context": "10 We use a variant of truncated back-propagation and apply gradient clipping (Pascanu et al., 2012).", "startOffset": 78, "endOffset": 100}, {"referenceID": 43, "context": "Similar to Serban et al. (2016b), we use a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden units.", "startOffset": 11, "endOffset": 33}, {"referenceID": 43, "context": "Similar to Serban et al. (2016b), we use a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden units. We experiment with context RNN encoders with 500 and 1000 hidden units, and find that that 1000 hidden units reach better performance w.r.t. the variational lower-bound on the validation set. The encoder and context RNNs use layer normalization (Ba et al., 2016). We experiment with decoder RNNs with 1000, 2000 and 4000 hidden units (LSTM cells), and find that 2000 hidden units reach better performance. For the G-VHRED model, we experiment with latent multivariate Gaussian variables with 100 and 300 dimensions, and find that 100 dimensions reach better performance. For the H-VHRED model, we experiment with latent multivariate Gaussian and piecewise constant variables each with 100 and 300 dimensions, and find that 100 dimensions reach better performance. We follow the training procedure of Serban et al. (2016b): we drop words in the decoder with a fixed drop rate of 25% and multiply the KL terms in the variational lower-bound by a scalar, which starts at zero and linearly increases to 1 over the first 60,000 training batches.", "startOffset": 11, "endOffset": 961}, {"referenceID": 27, "context": "For each test dialogue, we use TF-IDF to extract 100 candidate responses (Lowe et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 25, "context": "Second, it decreases the number of generic responses, which are extremely common among generative models and which human evaluators tend to prefer despite not advancing the dialogue (Li et al., 2016).", "startOffset": 182, "endOffset": 199}, {"referenceID": 25, "context": "We follow the approach by Liu et al. (2016) by conducting an Amazon Mechanical Turk experiment to compare the G-VHRED and H-VHRED models.", "startOffset": 26, "endOffset": 44}, {"referenceID": 27, "context": "Future work should focus on exploring other natural language processing tasks, where multimodality plays an important role such as modeling technical help dialogues (Lowe et al., 2015) and online debates (Rosenthal & McKeown, 2015), and where additional information is available, such as in semi-supervised document categorization Ororbia II et al.", "startOffset": 165, "endOffset": 184}, {"referenceID": 27, "context": "Future work should focus on exploring other natural language processing tasks, where multimodality plays an important role such as modeling technical help dialogues (Lowe et al., 2015) and online debates (Rosenthal & McKeown, 2015), and where additional information is available, such as in semi-supervised document categorization Ororbia II et al. (2015a).", "startOffset": 166, "endOffset": 357}], "year": 2017, "abstractText": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, unimodal priors \u2014 such as the multivariate Gaussian distribution \u2014 yet many realworld data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling.", "creator": "LaTeX with hyperref package"}}}