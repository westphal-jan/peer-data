{"id": "1612.00554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Higher Order Mutual Information Approximation for Feature Selection", "abstract": "feature element selection is a process of choosing a subset of relevant features so that the quality of prediction models can be improved. an internally extensive body survey of work exists on information - theoretic feature selection, based on maximizing mutual information ( mi ) between subsets lists of features and class labels. the prior methods use a lower order approximation, by treating the joint distribution entropy as a summation of several single variable entropies. this leads to maximal locally optimal selections and misses multi - way feature combinations. simultaneously we suggest present a higher upper order mi based approximation technique called higher initial order feature selection ( hofs ). instead of producing a single list of features, our method produces a ranked collection of feature subsets that maximizes mi, giving better comprehension ( feature ranking ) as to which features work best together when selected, due to their underlying interdependent structure. our experiments demonstrate that the proposed method performs better than existing feature selection approaches while keeping similar running times and computational complexity.", "histories": [["v1", "Fri, 2 Dec 2016 03:34:44 GMT  (219kb,D)", "http://arxiv.org/abs/1612.00554v1", "14 page, 5 figures"]], "COMMENTS": "14 page, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jilin wu", "soumyajit gupta", "chandrajit bajaj"], "accepted": false, "id": "1612.00554"}, "pdf": {"name": "1612.00554.pdf", "metadata": {"source": "CRF", "title": "Higher Order Mutual Information Approximation for Feature Selection", "authors": ["Jialin Wu", "Soumyajit Gupta", "Chandrajit Bajaj"], "emails": ["wujl13@mails.tsinghua.edu.cn", "smjtgupta@utexas.edu", "bajaj@cs.utexas.edu"], "sections": [{"heading": "1. Introduction", "text": "Feature selection is a dimensionality reduction technique for a wide range of search problems. A common practice used in machine learning is to find subset of available features for a learning algorithm. The best subset contains the least number of dimensions that most contribute to accuracy [13]. Feature selection differs from standard dimensionality reduction. Both methods seek to reduce the number of attributes in the dataset, but a dimensionality reduction method does so by creating new combinations of attributes, where as feature selection methods includes and excludes attributes in the data without changing them.\nThe central premise when using a feature selection technique is that the data contains many features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information [2]. Redundant features refer to those having information which is already contained in other features, while irrelevant features are those with no useful information related to the class variable. Feature selection mostly acts as a filter, muting out features that are not useful in addition to the existing features , and thus avoid over-fitting, speed up computation, or improve the interpretability of the results.\nFeature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11]. Classifier dependent methods exploit the label information as a measure to guide the feature selection process. Examples are Backward Elimination and Forward selection. In forward selection, variables are progressively incorporated into larger subsets, whereas in backward elimination one starts with the set of all variables and progressively eliminates the least promising ones [9]. The inherent advantage of this class of methods are that multi-collinearity issues are automatically handled. However, no prior knowledge about the actual relationship between the variables can be inferred.\nClassifier independent methods, on the other hand, define a scoring function between features and labels in the selection process. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables and are particularly effective in computation time and robust to over-fitting. A very simple filter type feature selection algorithm is a basic correlation analysis [10], where only attributes which are correlated to the label are\n\u2217Authors contribute equally 0This work is supported by the NIH Grants #R41 GM116300, #R01 GM117594. 0Primary Contact: C. Bajaj, ; email: bajaj@cs.utexas.edu; 201 East 24th Street, POB 2.324A, 1 University Station, C0200, Austin, TX, 78712-0027.\nar X\niv :1\n61 2.\n00 55\n4v 1\n[ cs\n.L G\n] 2\nD ec\n2 01\nbe chosen in a predictive analytics model. Correlation analysis helps to identify these \u201cfeatures\u201d right at the outset and allows a better understanding of how the attribute affects the predicted variable. However correlation table needs to be generated and collinear variables must be eliminate \u201cby hand\u201d.\nThe Mutual Information MI (also called cross entropy or information gain) is a widely used information theoretic measurement for the stochastic dependency of discrete random variables. MI based feature selection works similar to a standard correlation analysis, except that it is more robust when the variables are noisy or non-linearly related to the predicted or target variable. Heuristic greedy algorithms are adopted to find out the most relevant and least redundant feature at each step. Brown et al. [3] show that the redundant term should refer to conditional mutual information and summarizes a uniform framework for information theoretic feature selection. However, due to the computational cost, the joint probability density functions (pdf) are often approximated as multiplication of single variable distribution functions. This is based on two inconsistent independence assumptions in Gao et al. [8], which can be seen as first order approximation of Information Gain, leading to the miss of multi-way feature combinations. This feature independence assumption produces singleton feature elements, thereby failing to capture he joint MI gain over a collection of correlated variables. A higher order approximation is thus needed to counter this scenario and get a better comprehension of the inter-feature dependence.\nThe ICA (Independent Component Analysis) algorithm, based on infomax or maximum likelihood estimation, tries to represent an input vector x = (x1, x2, ..., xM )T as a linear combination of independent signal vector s = (s1, s2, ..., sM )T e.g. X = AS, where each xi and si represents a random variable and A is called the mixing matrix [4]. The connection between ICA algorithm and information theory has been well known for many years [16]. However, to our knowledge, most of work concerns extracting signal vector s by finding out the optimal unmixing matrix (W \u223c A\u22121), that can minimize the mutual information(MI) between si, which leads to large running time.\nTo address all these issues, this paper presents a feature selection method, termed Higher Order Feature Selection (HOFS). This integrates ICA to approximate the MI in higher order dimension. Instead of producing a single list of features, we provide several subsets of features that maximizes MI, giving better comprehension as to which features work best when selected together due to their underlying interdependent structure. This addresses Feature Ranking, by scoring the feature subsets based on their combined predictive power, which is a significant advantage over lower order models. Our model also considers simplifying the computational cost of calculating multi-variable MI using ICA. We compare the results of our experiments against well known feature selection algorithms on publicly available datasets. Observations show that we perform at par if not better than them in classification error and global MI captured using the subset of selected features, while keeping similar running times and computational complexity.\nRest of the paper is organized as follows: Sec. 2 sets up the basic framework of MI based feature selection approach with notations, definitions and prior work. Sec. 3 discusses the lo-order and ICA issues which we are trying to address in this paper. In Sec. 4, we introduce the proposed Higher Order Feature Selection (HOFS) approach with its computation analysis and running times. All experiments on publicly available real datasets and their comparison results are illustrated in Sec. 5. Experiments performed on synthetic data (numerical graphical models and heterogeneous feature models) for model checking are tabulated in Sec. 6. We conclude our findings and discuss some of the future problems that can be addressed in Sec. 7."}, {"heading": "2. Feature Selection Background", "text": ""}, {"heading": "2.1. Notation", "text": "Let x denote a possible value of the discrete random variable (r.v.) X which is drawn from the alphabet set X . Also, let p(X) define the probability density function (pdf) of X , and P (X = x) denote the probability that r.v. X takes value x. When X is discrete, the probability can be estimated as a fraction of observations p\u0302(x) = #xN , taking on value x from the total N . An event x \u2208 X is indicated as a draw of a value from the set. Also let M be the total number of features in X , with Xi, (i \u2208 [1 : M ]) as the current feature which is considered for selection, y denote its label and \u2126 denote the sparse set of selected features with elements xft for t \u2208 [1, T ], where |\u2126| = T denotes its cardinality."}, {"heading": "2.2. Definitions", "text": "Entropy and mutual information are two well-known concepts in information theory, which are used to measure the information provided by random variables. Entropy H(X): It is a measure of uncertainty of the random variable X , or the average amount of information that we\nreceive with every event. The more certain X is, higher the value of H(X), H(X) = \u2212 \u2211 x\u2208X P (X = x) logP (X = x)\n= \u2212 \u2211 x\u2208X p(x) log p(x) (1)\nJoint entropy H(X,Y ): It is the measure of uncertainty of a joint variable, which consists of two random variables X and Y . The joint entropy of a set of variables is greater than or equal to all of the individual entropies of the variables in the set.\nH(X,Y ) = \u2212 \u2211\nx\u2208X ,y\u2208Y p(x, y) log(p(x, y)) (2)\nwhere p(x, y) = P (X = x, Y = y). Conditional EntropyH(X|Y ): It quantifies the amount of information needed to describe the outcome of a random variable, given that the value of another random variable.\nH(X|Y ) = \u2212 \u2211 x\u2208X p(x) \u2211 y\u2208Y p(y|x) log(p(y|x))\n= \u2212 \u2211\nx\u2208X ,y\u2208Y p(x, y) log(p(x|y)) (3)\nwhere p(x|y) = P (X = x|Y = y). Mutual Information (MI) I(X : Y ): It is a measure of shared information between two random variables. MI between two r.v. X and Y can be defined as in Eq. 4:\nI(X : Y ) = H(X)\u2212H(X|Y ) = H(X) +H(Y )\u2212H(X,Y ) (4)\nwhich can be regarded as the certainty gain of random variable X when conditioned of label Y . In this case, the higher value of I means the stronger relation between X and Y ."}, {"heading": "2.3. Prior Work", "text": "Lewis [15] and Duch [6] coined the term Mutual Information Maximization (MIM) as a feature scoring criteria, which measures the usefulness of a feature subset when used for classification. This heuristic considers a score for each feature independently of others, which is a limitation by itself, yet is known to be suboptimal in those cases.\nJMIM (xi) = I(xk : y)\nBattiti [1] defined the Mutual Information Feature Selection (MIFS) criteria which ensures feature relevance and forces a penalty to ensure low correlations within the set of selected features, in a sequential manner. Setting the penalty term \u03b2 to zero would result in MIM scores.\nJMIFS(xi) = I(xi : y)\u2212 \u03b2 \u2211 xj\u2208\u2126 I(xi : xj)\nYang and Moody [22] and Meyer et al. [18] proposed an alternate version of MIFS, called the Joint Mutual Information (JMI), which increases complementary information between features. This is able to capture the relevance-redundancy tradeoff with various heuristic terms.\nJJMI(xi) = \u2211 xj\u2208\u2126 I(xixj : y)\nPeng et al. [20] gave the Minimum-Redundancy Maximum-Relevance (MRMR) criteria which omits the conditional relevance term completely. The penalty term is inversely proportional to the size of current feature set. As the set grows the penalty term will diminish, hence all selected features will become strongly pairwise independent.\nJMRMR(xi) = I(xi : y)\u2212 1 |\u2126| \u2211 j\u2208\u2126 I(xi : xj)\nFleuret [7] introduced the Conditional Mutual Information Maximization (CMIM) criteria, which assumes that selected features are independent and class-conditionally independent given the unselected feature. This criterion ensures a good trade-off between independence and discrimination.\nJCMIM (xi) = I(xi : y)\u2212max xj\u2208\u2126 [I(xi : xj)\u2212 I(xi : xj |y)]\nNguyen et al. [19] proposed a Global MI-based feature selection via spectral relaxation (SPECCMI) approach. They have the ability to handle second-order feature dependency, by favoring features having large total pairwise conditional relevance. They also show that for large data, low rank approximation can be applied to gain computational advantage to our global algorithm over its greedy counterpart.\nJSPECCMI (xi) = I(xi : y) + \u2211 xj\u2208\u2126 I(xi : y|xj)\nGao et al. [8] produced Variational Information Maximization (VMI) which can be applied for feature selection over any general class of distributions and provide tractable lower bounds for mutual information. Their model is optimal if the data is generated according to tree graphical models. They also outperform previous methods in speed.\nJVMI(xi) = arg max i/\u2208\u2126,xj\u2208\u2126\nILB(xj \u222a xi : y)"}, {"heading": "3. MI based Feature Selection", "text": ""}, {"heading": "3.1. Forward Heuristic", "text": "Consider a supervised learning scenario where x = {x1, x2, ..., xM} is a M -dimensional input feature vector, and y is the output label. In filter methods, the mutual information-based feature selection task is to select T features x\u2126T\u2217 = {xf1 , xf2 , ..., xfT } such that the mutual information between x\u2126\u2217 and y is maximized. Formally,\n\u2126\u2217 = arg max \u2126 I(\u2126 : y) s.t. |\u2126| = T (5)\nwhere I(\u00b7) denotes the mutual information. Directly optimizing Eq. 5 is a NP-Hard combinatorial problem [? ]. Thus, most of MI-based features selection methods use a greedy search strategy, which selects features incrementally.\nft = arg max i {I(\u2126t\u22121 \u222a xi : y)}, i \u2208 [1,M ] (6)\nBecause we just focus on the next feature xi to be considered for selection, this becomes equivalent to solving Eq.7. Readers can refer to [8] for a detailed derivation.\nft = arg max{(I(xi : y) +H(x\u2126t\u22121 |xi)\u2212H(x\u2126t\u22121 |xi, y)} (7)"}, {"heading": "3.2. Lower Order Approximation", "text": "Approximating the higher order information-theoretic based MI measure is a difficult task. Therefore, most current methods propose a relevant term (features having information which is already contained in other features) and a redundant term (features with no useful information related to the class variable), where Feature Independence and Class-conditioned Independence are assumed (e.g. Eq. 8 and 9).\nH(x\u2126t\u22121 |xi) \u2248 t\u22121\u2211 k=1 H(xfk |xi) (8)\nH(x\u2126t\u22121 |xi, y) \u2248 t\u22121\u2211 k=1 H(xfk |xi, y) (9)\nGao et al. [8] discusses the inconsistency of these two assumptions. The independence assumption leads to the miss of multi-way feature combinations, because the interdependence between features is ignored.\nLet us look at a scenario where a lower order MI based feature selection algorithm would fail. For example, consider three random variables x1, x2, x3, where x1, x2 have the same pdf: p(x = 0) = 0.5 and p(x = 1) = 0.5, and p(x3 = 0) = 1. Let Y = x1 x2. In that case, I(x1 \u222a x2 : y) = 1, however, I(x1 : y) + I(x2 : y) = 0. Thus a lower order MI based algorithm should select x3, which is irrelevant to y and ignore the feature combination x1 \u222a x2, which fully defines y."}, {"heading": "3.3. Independent Component Analysis", "text": "The starting point for ICA is the very simple assumption that input variables xi can be represented as a linear mixture of N independent components s1, s2, . . . , sN .\nxi = T\u2211 j=1 \u03b1ijsj i = 1, 2, . . . , T\nWhen writing into a compact version, we have\nX = AS or S = WX, where W = A\u22121\nBased on maximum likelihood estimation, ICA algorithm constructs unmixing matrix W that gives the best fit of input data vectors xi given joint pdf of signals vectors.\nIn this context, both W and S are unknown, and we further assume that signal variables\u2019 cumulative distribution function (cdf) fit a sigmoid function.\ng(s) = 1\n1 + e\u2212s\nThus, the unmixing matrix W is defined as follow in Eq. 10, and the log-likelihood function can be optimized by stochastic gradient descend using Eq. 11.\nW = arg max{ M\u2211 i=1 ( T\u2211 j=1 log(g\u2032(wTj x (i)) + log(|W |))} (10)\nwhere each x(i) denotes a sample of T selected features.\nW := W + \u03b1(1\u2212 2g(Wxbatch)xTbatch + (WT )\u22121) (11)\nwhere xbatch = [x(1), x(2), . . . , x(m)] and \u03b1 is learning rate."}, {"heading": "4. Proposed Method: Higher Order Feature Selection", "text": "We are now considering finding out inter-dependent feature combinations, which will be missed by only implementing lower order MI approximation. This would jointly address the problem of feature selection and feature ranking 1."}, {"heading": "4.1. Motivation", "text": "The problem of approximating joint mutual information(MI) as the summation of single variables is limited by the interdependence between selected features. That motivates us to determine K independent feature subsets during the greedy search (Eq. 6) process X\u21261 , X\u21262 , . . . , X\u2126K , where features within a subset can be dependent on each other. Secondly, we determine independent representation S\u21261 , S\u21262 , . . . , S\u2126K for each of the independent feature subsets, then reconstruct H(X\u2126i) using H(S\u2126i). Potential problems of this framework lies in the accuracy of recovering H(X\u2126i) using H(S\u2126i) and the computational cost of ICA algorithm. We solve both of them by the following mutual balance and incremental ICA algorithm."}, {"heading": "4.2. Subset Independence", "text": "Instead of assuming single feature independence and class-conditioned independence, we assume that the optimal selected feature subset can be divided into several feature subsets, which are independent and Class-Conditioned Independent (Eq. 13 and 14).\nThe rationality of the subset assumption is that in most cases, label y can be better approximated as some function of X\u2126i , e.g.\ny = f(\u2126) = f(X\u21261 , X\u21262 , , X\u2126K ) (12)\n1Our code shall soon be made available online from https://cvcweb.ices.utexas.edu/cvcwp/software/\nwhere K indicates the number of selected subsets. Let \u2126 = K\u22c3 i=1 X\u2126i ,where each X\u2126i denote a feature subset and xt denotes the next feature to be selected.\nAssumption1. P (X) = K\u220f i=1 P (X\u2126i) (13)\nAssumption2. P (X|y) = K\u220f i=1 P (X\u2126i |y) (14)"}, {"heading": "4.3. Entropy relation between Inputs and Signals", "text": "Joint entropy of feature combinations X\u2126i is computed using Eq. 15\nH(X\u2126i) = \u2212 \u2211\nx\u2208X\u2126i\np(X\u2126i)log(p(X\u2126i)) (15)\nHowever, the joint pdf ofXi is difficult to estimate using finite samples as the size of subset increases. Directly computing that is also an a NP-Hard combinatorial problem which leads to inaccuracy because of insufficient sampling.\nTo solve that, we first transform the original feature vectors X\u2126i to S\u2126i using ICA. Note that each feature vector in S\u2126i is independent with each other. Thus H(X\u2126i) can be computed using H(S\u2126i), which is a summation of H(sj), where\nsj \u2208 S\u2126i , shown in Eq. 16.\nH(S\u2126i) = \u2212 \u2211\ns1,...,sTi\np(s1, ..., sTi)log(p(s1, ..., sTi))\n= \u2212 \u2211 s1 \u2211 s2 \u00b7 \u00b7 \u00b7 \u2211 sTi Ti\u220f j=1 p(sj)( Ti\u2211 j=1 log(p(si)))\n= \u2212 Ti\u2211 j=1 ( \u2211 s1 \u2211 s1 \u00b7 \u00b7 \u00b7 \u2211 sTi ( Ti\u220f j=1 p(sj))log(p(sj)))\n= \u2212 Ti\u2211 j=1 \u2211 sj p(si)log(p(si))\n= Ti\u2211 j=1 H(sj) (16)\nNoticing the relation of pdf between X and S in Eq. 17 and 18, H(X\u2126) can be computed using Eq. 19\nFX(x) = P (X \u2264 x) = P (AS \u2264 x) = P (S \u2264Wx) = P (S \u2264 s) = FS(s) (17)\nPX(x) = \u2207XFX(x) = \u2207SFS(s) \u2217 \u2207XS = PS(s)|W | (18)\nThus, H(X\u2126) can be formulated as: H(X\u2126) = \u2212 \u2211 x pX(x)log(pX(x))\n= \u2212 \u2211 x PX(x)log(PS(s)|W |)\n= \u2212 \u2211 x PX(x)log(PS(s))\u2212 log(|W |)\n= \u2212 \u2211 s PS(s)log(PS(s))\u2212 log(|W |)\n= H(S\u2126)\u2212 log(|W |)\n= M\u2211 i=1 H(si)\u2212 log(|W |) (19)"}, {"heading": "4.4. Forward Search", "text": "Based on the Assumption 1 and 2 (Eq. 13 and Eq. 14), the forward heuristic search function Eq. 7 becomes:\nft = arg max i/\u2208\u2126\nI(xi : y) +H(\u2126|xi)\u2212H(\u2126|xi, y)\n= arg max i/\u2208\u2126 I(xi : y) + K\u2211 j=1 (H(X\u2126j |xi)\u2212H(X\u2126j |xi, y)) (20)\nThe computation H(X\u2126t\u22121j |xi) and H(X\u2126t\u22121j |xi, y) is still a NP-Hard problem and finite discrete samples of input vectors X deteriorate the approximation of joint pdf. To solve this problem, we firstly transform them into non-conditioned terms as Eq. 21, 22. By assuming some general cumulative distribution function of signal vectors si, we calculate those two terms in linear computational complexity using Eq. 20, thus avoiding computation of the joint pdf.\nH(X\u2126t\u22121 |xi) = \u2212H(xi) +H(X\u2126t\u22121 \u222a xi) (21) H(X\u2126t\u22121 |xi, y) = \u2212H(xi, y) +H(X\u2126t\u22121 \u222a xi \u222a y) (22)"}, {"heading": "4.5. Inaccuracy Removal and Incremental ICA", "text": "The transformation from input vectors X\u2126i to independent signal vectors X\u2126i assumes continuous distribution. Finite discrete samples lead to inaccuracy.\nH(Xdiscrete) \u2248 H(Sdiscrete)\u2212 log(|W |) (23) Note that we try to find a mixture of independent signals si to represent xi. The inaccuracy comes from H(X\u2126t\u22121 |xi) \u2212 H(X\u2126t\u22121 |xi, y) in Eq. 20. Since forward greedy search concern a subtraction of two multi-variables entropies we are motivated to design a balanced algorithm to reduce noise. The computational cost for implement ICA algorithm is not tractable either. Hence, we adopt another assumption that the new input vector can be represented as a linear combination of our origin signal set SSj and a new signal vector st. That is, given X\u2126j = AS\u2126j , where we can obtain another independent signal vector st make the following equation hold true.(\nX\u2126j xi\n) = ( A\u2126j 0\naTi,1, ..., aTi,Ti\u22121 aTi,Ti )( S\u2126j st ) = ASjS\u2126t\nThus,\nW i\u2126j = (A i \u2126j ) \u22121\n=\n( A\u22121\u2126j 0\nwTi+1,1, . . . , wTi+1,Ti a \u22121 Ti+1,Ti+1 ) = ( W\u2126j 0\nwTi+1,1, . . . , wTi+1,Ti wTi+1,Ti+1 ) In this context, the ICA algorithm can be done incrementally and reduce the computational cost from computing the gradient of whole matrix A to only the last row of A. And because A is a lower triangular matrix, computational complexity drops from O(T 3) to O(T 2).\nGiven X\u2126j = A\u2126jS\u2126j , we imply incremental ICA for the next feature xi and its label y.( X\u2126j xi ) = ( A\u2126j 0 aiTj+1,1, ..., a i Tj+1,Tj aiTj+1,Tj+1 )( S\u2126j sxi ) = AxiS \u2032 \u2126j\nand X\u2126jxi y  = ( Axt 0 aiTj+2,1, ..., a i Tj+2,Tj+1 aiTj+2,Tj+2 )S\u2126jsxi siy  So, the subtraction of H(X\u2126j |xi)\u2212H(X\u2126j |xi, y) can be computed as Eq. 24.\nH(X\u2126j |xi)\u2212H(X\u2126j |xi, y) =\u2212H(xi) +H(xi, y)\u2212H(X\u2126j \u222a xi) +H(X\u2126j \u222a xi \u222a y) =\u2212H(xi) +H(xi, y)\n\u2212 ( Tj\u2211 i=1 H(si) +H(sxi) + log(|Axi |))\n+ ( Tj\u2211 i=1 H(si) +H(sxi) +H(s i y) + log(|Axi |aiTj+2,Tj+2))\n=\u2212H(xi) +H(xi, y) +H(siy) + log(aiTj+2,Tj+2) (24)\nLet I(X\u2126j : y|xi) denote H(X\u2126j |xi)\u2212H(X\u2126j |xi, y) The forward search function becomes,\nft = arg max i/\u2208\u2126t\u22121 {I(xi : y) + K\u2211 j=1 (I(X\u2126j : y|xi))} (25)"}, {"heading": "4.6. Time Complexity", "text": "A detailed stepwise description of the HOFS algorithm is given in Alg. ??. Because the forward heuristic search at each step will compute the information gain between each unselected node and selected node, the total computational complexity for entropy is O(MNT ), where N is the total number of samples, M is the total number of features and T is the number of features to be selected.\nThe complexity of ICA algorithm concerns the computation of gradients of matrix W\u2126i (Eq. 11). Because W\u2126i is lower triangular matrix and we only need the gradients of elements in the last row, Inverse computation can be done in O(MT 2\nK ).\nAlgorithm 1 Higher Order Feature Selection (HOFS)\n1: Data:(Xi, yi) \u2208 RN , i = 1, 2, ..,M 2: Input:T \u2190 number of features to be selected. 3: \u2126\u2190 {\u2205},K \u2190 1, t\u2190 1 4: Body: 5: while |\u2126| < T do 6: imply ICA for each unselected feature xi 7: select a new feature xft using Eq. 25 8: compute Argcov and Maxcov using Eq. 26 and Eq. 27 9: if Maxcov > C then\n10: K \u2190 arg maxMaxcov 11: else 12: K \u2190 |\u2126| 13: X\u2126K \u2190 X\u2126K \u222a xft 14: \u2126\u2190 \u2126 \u222a xft 15: Output: 16: selected feature subsets: \u2126"}, {"heading": "4.7. Feature Subset Determination", "text": "We aim to find the independent feature combinations, so it is of utmost importance to judge if a new subset should be created for incoming feature xt or put it into an existing subset. Specifically, we compute the maximum average correlation between xt and each subset X\u2126i .\nArgcovi = 1 |X\u2126i | \u2211\nu\u2208XSi\ncov(xt, u) (26)\nMaxcov = max i Argcov (27)\nIf Maxcov is over a predefined constant C, then we add xt to the relevant subset, else we create a new subset for xt."}, {"heading": "5. Experiments", "text": "We conduct experiments by fisrt analyzing the quality of the incremental ICA algorithm. We then compare our proposed approach HOFS, with other popular lower order MI based feature selection algorithm on some publicly available standard datasets."}, {"heading": "5.1. Incremental ICA Quality", "text": "We verify the feasibility of our incremental ICA algorithm by two measures (Table 1). Firstly, we compute the average Pearson\u2019s product-moment coefficient between the signal vectors after ICA, to show that by maximizing Eq. 10, we get uncorrelated vectors, which ensures that Eq. 16 holds true. Low values (\u223c 0.055) on average, show that the vectors in the ICA space are highly uncorrelated to each other, hence almost independent. Secondly, we compute average Rbalance ratio:\nRbalance = 1\nK K\u2211 i=1 H(X\u2126i)\u2212H(X\u2126i , y) H(siy) + log(a i Ti+2,Ti+2 )\nwhere X\u2126i , i = 1, 2, 3, ...K is the ith selected feature combination, siy is the incremental signal vector for y in subset X\u2126i . With Rbalance near to 1, we show that the error of approximating H(X) using H(S) is balanced out by the subtraction of H(X)\u2212H(X, y)."}, {"heading": "5.2. Real-World Data", "text": "We compare our algorithm with other popular information-theoretic feature selection methods, including VMI, mRMR, JMI, CMIM and SPECCMI. We use 9 well-known datasets commonly used in feature selection studies. They were chosen to have a wide variety of feature ratios and multi-class problems (Table 1). We use the average cross-validation error rate on the range of 10 to 100 (total feature number if it is less than 100) features to compare different algorithms under the same setting. 10-fold cross-validation is employed for datasets with number of samples N \u2265 100 and leave-one-out crossvalidation otherwise. The classifier is chosen to be Linear SVM. We pre-process data following the approach proposed in VMI [8].\nFig. 2 shows the cross validation error in dataset \u2018Semeion\u2019, where our method incurs the lowest validation error in terms of classification using the selected feature sets. Highlighted entries in each table shows the best results obtained during our experimentation. We show the average cross validation error results for the nine datasets [17] in Table 2. Our method performs best for most of the datasets and has very low relative error compared to others, when it is not the best.\nThe global Mutual Information captured by the various methods for top 11 features is shown in Table 3, where our method returns relatively high MI compared to others by choosing the best possible combination of features. This further goes to ground our assumption that certain set of features when selected together, rather than independently, have a higher impact on global MI.\nWe also consider the Average Relative Absolute Error (ARAE) of the classification process (Table 4). This parameter shows how a feature selection method could affect the classifiers not to predict wrongly or at least predict closer to the true labels. Considering RAEi as the relative absolute error for a specific classification algorithm, and Q as the number of such algorithms used in the experiment, ARAE is then defined as:\nARAE = \u2211Q i=1RAEi Q"}, {"heading": "6. Synthetic Model Experiment", "text": ""}, {"heading": "6.1. Inference from Graphical Model", "text": "We perform experiment on a synthetic model (Fig. 3) according to the tree structure. The root node Y in Fig. 3a is a binary variable indicating the class label, while other variables xi are continuous gaussian with unit variance and mean set to the value of its parent. We generate 100000 samples from the model and compare the results of HOFS with VMI in Table 5]. The covariance structure among the 9 features (Fig. 3b) confirms the relation generated by the tree graphical model. Although both the methods selects the same set of features, HOFS selects them in the correct order and maintains the proper subsets. \u2126VMI = {x1, x4, x5, x2, x6, x7, x3, x8, x9} and \u2126HOFS = {{x1, x4, x5}, {x2, x6, x7}, {x3, x8, x9}}. Therefore, HOFS goes an extra step in showing which features are correlated to each other, which is not evident from VMI. Fig. 3c shows the plot of Information gain according to the generated model and as predicted by HOFS. We also show the Information gain by adding a new feature to the current selected subset in Table 6."}, {"heading": "6.2. Handling Non-Numerical Features", "text": "To verify the effectiveness of the proposed HOFS for a non-/numerical feature selection, 1000 samples with 20 heterogeneous features and 5 classes were synthesized according to [21]. There were 4 groups of features and each group had 5 features of both numerical and non-numerical features, which are shown in Table 7. The class label can be described by the group of features. In Group I, F1, F2, F3 are non-numerical features which can explain class 1 - class 3. F4, F5 are numerical features which can explain class 4 and class 5. Here, F1 is the most prominent feature to classify the class label and there are redundancies within this group. Ideally, the selected feature subset should be {F1, F3, F5} or {F1, F4, F5}. If anyone of\nthese two subsets is selected, other features in this group are redundant. Group II contains 5 features which were generated by adding some bias to the related features in Group I. Group III contains 5 features which were generated by adding noise to the related features in Group I. Group IV contains 5 random attributes, which were uncorrelated with class label.\nVMI completely breaks down in this scenario and selects the features in linear order as \u2126VMI = {F1, F2, . . . , F20}. The subsets produced are \u2126HOFS = {{F1, F6, F2, F7}, {F9, F4, F14, F10, F5, F15}, {F3, F8}, {F11}, {F13}, {F12}, {F19}, {F20}, {F16}, {F17}, {F18}}. The first set {F1, F6, F2, F7} clearly separates classes 1 and 2 from the other classes, where F1, F6 separates class 1 from class 2. The next subset {F9, F4, F14, F10, F5, F15} segregates classes 3, 4 and 5 amongst themselves. Since F14, F15 were generated by randomly replacing 200 negative values with positive values in F4, F5 respectively, they group up in together side-by side in the selected subset. Hence HOFS preserves the underlying structure of the generated features in presence of heterogeneity and provides meaningful feature ranking. Fig. 4 shows the plot of Information gain according to the heterogeneous model and as predicted by HOFS."}, {"heading": "7. Conclusion", "text": "Mutual Information (MI) defines a measurement of how informative the features are. Feature selection based on MI has been developed a lot over the past decade. However,the computation of global MI is a NP-Hard problem. Thus, most of those algorithms are forced to do a lower order estimation. We introduce an improved method (HOFS) to estimate the global MI by integrating incremental ICA algorithm. Our method clearly performs better than most owing to its ability to select feature subsets that jointly maximize MI, while keeping similar running times and computational complexity as the current\napproaches. We would like to extend our work to provide minimal cardinality feature subsets for a wide range of datasets including geometric and hyperspectral."}], "references": [{"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on neural networks, 5(4):537\u2013550,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Application of high-dimensional feature selection: evaluation for genomic prediction in man", "author": ["M.L. Bermingham", "R. Pong-Wong", "A. Spiliopoulou", "C. Hayward", "I. Rudan", "H. Campbell", "A.F. Wright", "J.F. Wilson", "F. Agakov", "P. Navarro"], "venue": "Scientific reports,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "author": ["G. Brown", "A. Pocock", "M.-J. Zhao", "M. Luj\u00e1n"], "venue": "Journal of Machine Learning Research, 13(Jan):27\u201366,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Independent component analysis, a new concept", "author": ["P. Comon"], "venue": "Signal processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Minimum redundancy feature selection from microarray gene expression data", "author": ["C. Ding", "H. Peng"], "venue": "Journal of bioinformatics and computational biology, 3(02):185\u2013205,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature extraction: Foundations and applications", "author": ["W. Duch"], "venue": "Studies in Fuzziness and Soft Computing, chapter 3, pages 89\u2013117. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["F. Fleuret"], "venue": "Journal of Machine Learning Research, 5(Nov):1531\u20131555,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Variational information maximization for feature selection", "author": ["S. Gao", "G.V. Steeg", "A. Galstyan"], "venue": "arXiv preprint arXiv:1606.02827,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of machine learning research, 3(Mar):1157\u20131182,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Correlation-based feature selection of discrete and numeric class machine learning", "author": ["M.A. Hall"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L.A. Rendell"], "venue": "Proceedings of the ninth international workshop on Machine learning, pages 249\u2013256,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial intelligence, 97(1):273\u2013324,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Feature selection methods and algorithms", "author": ["L. Ladha", "T. Deepa"], "venue": "International journal on computer science and engineering, 1(3):1787\u20131797,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Embedded methods", "author": ["T.N. Lal", "O. Chapelle", "J. Weston", "A. Elisseeff"], "venue": "Feature extraction, pages 137\u2013165. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Feature selection and feature extraction for text categorization", "author": ["D.D. Lewis"], "venue": "Proceedings of the workshop on Speech and Natural Language, pages 212\u2013217. Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Information Technology and Applications: Proceedings of the 2014 International Conference on Information technology and Applications (ITA 2014), Xian, China, 8-9 August 2014", "author": ["X. Li"], "venue": "CRC Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "http://archive.ics.uci.edu/ml,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Information-theoretic feature selection in microarray data using variable complementarity", "author": ["P.E. Meyer", "C. Schretter", "G. Bontempi"], "venue": "IEEE Journal of Selected Topics in Signal Processing, 2(3):261\u2013274,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Effective global approaches for mutual information based feature selection", "author": ["X.V. Nguyen", "J. Chan", "S. Romano", "J. Bailey"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 512\u2013521. ACM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection based on mutual information criteria of max-dependency, maxrelevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Transactions on pattern analysis and machine intelligence, 27(8):1226\u20131238,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Heterogeneous feature subset selection using mutual information-based feature transformation", "author": ["M. Wei", "T.W. Chow", "R.H. Chan"], "venue": "Neurocomputing, 168:706\u2013718,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["H.H. Yang", "J.E. Moody"], "venue": "NIPS, volume 99, pages 687\u2013693. Citeseer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 12, "context": "The best subset contains the least number of dimensions that most contribute to accuracy [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "The central premise when using a feature selection technique is that the data contains many features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information [2].", "startOffset": 209, "endOffset": 212}, {"referenceID": 11, "context": "Feature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Feature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Feature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11].", "startOffset": 157, "endOffset": 161}, {"referenceID": 8, "context": "In forward selection, variables are progressively incorporated into larger subsets, whereas in backward elimination one starts with the set of all variables and progressively eliminates the least promising ones [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 9, "context": "A very simple filter type feature selection algorithm is a basic correlation analysis [10], where only attributes which are correlated to the label are \u2217Authors contribute equally 0This work is supported by the NIH Grants #R41 GM116300, #R01 GM117594.", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "[3] show that the redundant term should refer to conditional mutual information and summarizes a uniform framework for information theoretic feature selection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], which can be seen as first order approximation of Information Gain, leading to the miss of multi-way feature combinations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "X = AS, where each xi and si represents a random variable and A is called the mixing matrix [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "The connection between ICA algorithm and information theory has been well known for many years [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Prior Work Lewis [15] and Duch [6] coined the term Mutual Information Maximization (MIM) as a feature scoring criteria, which measures the usefulness of a feature subset when used for classification.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "Prior Work Lewis [15] and Duch [6] coined the term Mutual Information Maximization (MIM) as a feature scoring criteria, which measures the usefulness of a feature subset when used for classification.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "JMIM (xi) = I(xk : y) Battiti [1] defined the Mutual Information Feature Selection (MIFS) criteria which ensures feature relevance and forces a penalty to ensure low correlations within the set of selected features, in a sequential manner.", "startOffset": 30, "endOffset": 33}, {"referenceID": 21, "context": "Yang and Moody [22] and Meyer et al.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "[18] proposed an alternate version of MIFS, called the Joint Mutual Information (JMI), which increases complementary information between features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] gave the Minimum-Redundancy Maximum-Relevance (MRMR) criteria which omits the conditional relevance term completely.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Fleuret [7] introduced the Conditional Mutual Information Maximization (CMIM) criteria, which assumes that selected features are independent and class-conditionally independent given the unselected feature.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "[19] proposed a Global MI-based feature selection via spectral relaxation (SPECCMI) approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] produced Variational Information Maximization (VMI) which can be applied for feature selection over any general class of distributions and provide tractable lower bounds for mutual information.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Readers can refer to [8] for a detailed derivation.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "[8] discusses the inconsistency of these two assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Rbalance Lung [5] 56 32 3 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 16, "context": "1 Splice [17] 60 3175 3 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "07 Waveform [17] 40 5000 3 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "94 Semeion [17] 256 1593 10 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "94 Optdigits[17] 64 5620 10 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "89 Musk2 [17] 168 6598 2 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "98 Spambase[17] 57 4601 2 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "97 Promoter [17] 58 106 4 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "02 Madelon [9] 500 4400 2 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "We pre-process data following the approach proposed in VMI [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Figure 2: Average Cross Validation Error of the aforementioned methods and ours, on the SEMEION [17] dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "We show the average cross validation error results for the nine datasets [17] in Table 2.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "Handling Non-Numerical Features To verify the effectiveness of the proposed HOFS for a non-/numerical feature selection, 1000 samples with 20 heterogeneous features and 5 classes were synthesized according to [21].", "startOffset": 209, "endOffset": 213}], "year": 2016, "abstractText": "Feature selection is a process of choosing a subset of relevant features so that the quality of prediction models can be improved. An extensive body of work exists on information-theoretic feature selection, based on maximizing Mutual Information (MI) between subsets of features and class labels. The prior methods use a lower order approximation, by treating the joint entropy as a summation of several single variable entropies. This leads to locally optimal selections and misses multi-way feature combinations. We present a higher order MI based approximation technique called Higher Order Feature Selection (HOFS). Instead of producing a single list of features, our method produces a ranked collection of feature subsets that maximizes MI, giving better comprehension (feature ranking) as to which features work best together when selected, due to their underlying interdependent structure. Our experiments demonstrate that the proposed method performs better than existing feature selection approaches while keeping similar running times and computational complexity.", "creator": "LaTeX with hyperref package"}}}