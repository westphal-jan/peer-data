{"id": "1612.00584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Alleviating Overfitting for Polysemous Words for Word Representation Estimation Using Lexicons", "abstract": "much though there are some works on improving distributed word representations using lexicons, the improper distributed overfitting of the words that have multiple meanings is a remaining issue deteriorating the learning when appropriate lexicons are used, which needs to be solved. an alternative method is to allocate a vector per corpus sense instead a vector per word. also however, the word representations estimated in the former way are not as easy to use rapidly as the latter one. our previous experiment work uses a naive probabilistic method to alleviate the overfitting, but it is not simultaneously robust with small vocabulary corpus. in this paper, we propose mounting a complex new neural network to estimate distributed variables word representations using a lexicon and a corpus. simply we add a lexicon layer in continuous bag - of - words model, matching and adding a threshold node after the output of the lexicon layer. the threshold rejects the \" bad \" outputs of the lexicon layer that are less likely to be the same with their inputs. unfortunately in this way, it alleviates reduces the overfitting of the polysemous words. the proposed statistical neural network can be trained using negative sampling, which was maximizing the log probabilities of target vector words given the context words, by distinguishing the target words from random noises. we compare merely the proposed neural network with continuous bag - of - words model, the fewer other works improving like it, and the previous works estimating distributed word representations using both a lexicon and a corpus. the experimental results already show that balancing the proposed neural network is yet more efficient and balanced for both semantic tasks needs and syntactic related tasks than the initial previous works, and robust to the size of within the corpus.", "histories": [["v1", "Fri, 2 Dec 2016 07:45:40 GMT  (383kb)", "http://arxiv.org/abs/1612.00584v1", "7 pages, under review as a conference paper at IEEE IJCNN 2017"], ["v2", "Thu, 9 Mar 2017 12:36:26 GMT  (1977kb)", "http://arxiv.org/abs/1612.00584v2", "Accepted by IEEE IJCNN 2017. Copyright transferred to IEEE"]], "COMMENTS": "7 pages, under review as a conference paper at IEEE IJCNN 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuanzhi ke", "masafumi hagiwara"], "accepted": false, "id": "1612.00584"}, "pdf": {"name": "1612.00584.pdf", "metadata": {"source": "CRF", "title": "Alleviating Overfitting for Polysemous Words for Word Representation Estimation Using Lexicons", "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "emails": ["enshi@soft.ics.keio.ac.jp", "hagiwara@soft.ics.keio.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 58\n4v 1\n[ cs\n.C L\n] 2\nD ec\n2 01\nI. INTRODUCTION\nNatural language processing is still a challenging research area of artificial intelligence. Especially, it is a difficult issue to recognize and represent the implicit features of a piece of text properly.\nDistributed text representations estimated using a neural network are useful to be applied to conventional natural language processing algorithms [1]\u2013[11]. Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.\nHowever, some natural language processing tasks are still challenging. For example, the conventional algorithms fail to correctly predicate the number of starts of 40% amazon reviews [16]. It indicates the needs of higher quality text representation to improve the conventional algorithms.\nThe early approaches to estimate text representations use n-gram models [1], [4], [5]. Mikolov et al. propose continuous bag-of-words and skip-gram models [7], [8]. Their\nmethod outperforms the previous algorithms and costs less time. Pennington et al. [10] propose an algorithm using both local information and global information in the corpus and report a higher performance. Bojanowski et al. [11] extend the models of Mikolov et al. using the character-level information of words. Their reported experimental results outperform the original models in syntactic tasks, but fail to achieve an obvious improvement in semantic tasks.\nLexicons are useful for us humans to learn a language. We can use them to help machines to learn natural languages as well. Chen et al. [21] use the definitions in the lexicons to estimate representations for word senses and outperform the sense representations by Huang et al. [5]. Other researches take advantage of the defined synonyms or paraphrases in lexicons. Yu et al. [22] and Bollegala et al. [23] estimate the word representations by not only maximizing the probability of target word given a context, but also minimizing the distance of the paraphrases in a lexicon at the same time. Faruqui et al. [24] propose a method refining trained word representation vectors using lexicons. Xu et al. [25] estimate the word representations jointly by minimizing the distance of the tail word from the sum of the vectors of the head word and the relation for a triplet of words (head, relation, tail), and making words less similar to each other in a larger category. However, even though the previous methods using the paraphrases in lexicons reported improvements in syntactic analogical tasks, all of them failed to outperform the baselines in semantic analogical tasks.\nThere is a not well addressed issue of the previous researches on using paraphrases in lexicons to improve estimated distributed word representations: For polysemous words that have different synonyms in different contexts, if we use the paraphrases to represent them without disambiguation, they may be over-fitted to improper senses. However, disambiguation is another difficult issue. Besides, it is less easy to use if a word has several vectors for its different senses instead of one vector per word, because the usage of such word representations in the conventional systems requires additional word disambiguation. Ke et al. [26] propose a method considering the lexicon as a fuzzy set of paraphrases and using Bernoulli distribution subjected to the membership function of paraphrases to alleviate the problem. Although the method outperforms the previous works in a large corpus, the experimental results show that it is weak at small corpora.\nIn this paper, we propose a new method to improve the distributed word representations using paraphrases in a lexicon\nthat is able to alleviate the overfitting of the polysemous words without disambiguation. Our method is efficient and easy to be combined with the conventional algorithms estimating word representations using corpora. In the experiment, our method outperforms the previous methods using paraphrases and keeps efficient for corpora in different sizes."}, {"heading": "II. RELATED WORKS", "text": ""}, {"heading": "A. Continuous Bag-of-Words", "text": "Continuous Bag-of-Words (CBOW) with negative sampling [8] is an efficient algorithm to estimate distributed word representations. The objective of CBOW is to maximize the log probability of a target word given the vectors of the context words. Denote the size of the vocabulary as V , the size of the word vectors as W . The model is like Fig. 1.\nNegative Sampling is an efficient method to maximize the log probability. It is a simplified Noise Contrastive Estimation [27]. It trains the model by distinguishing target from randomly drawn noise. Denote the vector of target word as vwO\n, the vector of a context word as vwI , for each context word of the target word, the objective is to maximize:\nlog \u03c3(vwO TvwI )+\nn \u2211\ni=1\nEwi \u223c Pn(w)[log \u03c3(\u2212vwi TvwI )]. (1)\nHere, Pn(w) is the distribution of the noise. \u03c3 is a sigmoid function, \u03c3(x) = 1/(1 + e\u2212x)."}, {"heading": "B. Continuous Bag of Fuzzy Paraphrases", "text": "Continuous Bag of Fuzzy Paraphrases (CBOFP) [26] proposed by Ke et al. is a model based on CBOW to learn word representations using both a lexicon and a corpus. It is able to alleviate the overfitting of the word vectors for polysemous words. It outperforms the previous works using lexicons to estimate distributed word representations, but is not robust for small corpora. Fig. 2 shows the structure of the model.\nCBOFP adds a lexicon layer to CBOW. Unlike the previous work using lexicons to estimate distributed word representations, in CBOFP, every paraphrase is a fuzzy member of the paraphrase set of a word with a degree of truth. The outputs of the lexicon layer are dropped out randomly. The dropout is controlled by a function of the paraphrases\u2019 degrees of truth that returns 0 or 1 drawn from a Bernoulli distribution. Denote\nthe degree of truth as x, the control function f(x) is defined as the following:\nf(x) \u223c Bernoulli(x). (2)\nThe degree of truth is measured using the score provided by a paraphrase database called PPDB [28]\u2013[30]. Denote the score as S, the paraphrases set as L, the degree x is calculated as the following:\nx = S\nmax L\nS . (3)\nThe reported experimental results show that this method outperforms the previous works, especially in semantic tasks, but not robust with small corpora such as text81."}, {"heading": "III. THE PROPOSED METHOD", "text": ""}, {"heading": "A. Structure", "text": "Our previous work CBOFP is weak at small corpus because it involves a probabilistic method to alleviate the overfitting of polysemous words. Such a method requires enough amount of training data. Thus we consider another method not using a probabilistic way.\nInstead of dropping out some of the outputs of the lexicon layer randomly, we add a node after the lexicon layer as shown in Fig. 3. The inputs are the context word of the target word. They are both input into the hidden layer that contains the word vectors to learn, and input into the lexicon layer. The lexicon layer outputs the paraphrases of the inputs. The node after the lexicon layer takes the score of the paraphrase and holds a threshold. If the score of the paraphrase is higher than the threshold, it returns true. Otherwise, it returns false. The paraphrases whose outputs are false are not learned. True paraphrases of the context words are input into the hidden layer to learn together with the original context words. The output of the hidden layer is a vector of the vocabulary size. After softmax, the neural network outputs the target word. By maximizing the probability of outputting correct word, the\n1http://mattmahoney.net/dc/text8.zip\nhidden layer can be trained and used as the vectors represent the words in the vocabulary."}, {"heading": "B. The Lexicon Layer", "text": "We use a paraphrase database called PPDB2.0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26]. The paraphrases in it are extracted automatically from multilingual resources. It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].\nPPDB2.0 provides not only the paraphrases, but also the features, the alignment types and the entailment types. There are six types of entailment in PPDB2.0 as shown in Table I. We do not consider the \u201cexclusion\u201d type, \u201cother related\u201d type, \u201cindependent\u201d type, and only use the more \u201cnatural\u201d paraphrases of the \u201cequivalence\u201d type, \u201cforward entailment\u201d type, and \u201creverse entailment\u201d type.\nFor each word, we build a vector representing the set of its paraphrases in the lexicon layer. Let us denote each paraphrase in PPDB2.0 as (headword, tailword, relationship). For word A, B, C, D, E, if there are paraphrases (A, B, Equivalence), (A, C, Forward Entailment), (D, A, Reverse Entailment), (A, E, Exclusion), (A, F , Independent), and (A, G, OtherRelated), the paraphrase set vector for A is (B,C,D), and E, F , G are discarded.\nAs the score input to threshold node, we use the PPDB2.0 scores. They are estimated by a supervised scoring model on\nthe basis of human judgments for 26,455 paraphrase pairs and high correlation of the PPDB2.0 scores and human judgments are reported [29]."}, {"heading": "C. Learning the Word Representations", "text": "With the threshold, the objective to maximize becomes\nG \u2211\nwi\u2208G\n\u2211\nj\u2208C\n\nlog p(wi|wj) +\nLwj \u2211\nwk\u2208Lwj\nf(Sjk) log p(wi|wk)\n\n .\n(4)\nHere, G is the set of words in the corpus, C is the context of word wi, Lwj is the paraphrase set of the context word wj , Sjk is the score of paraphrase wk in Lwj . The function f(Sjk) is defined as the following:\nf(Sjk)\n{\n1 if Sjk > \u03b8 0 if Sjk < \u03b8.\n(5)\nHere, \u03b8 is the threshold of the threshold node.\nThe log probability is maximized in the learning phrase using negative sampling. Similarly to that for CBOW, we maximize the log probability in equation (4) by maximizing\nlog \u03c3(vwi Tvwj ) +\nN \u2211\nn=1\nEwi \u223c Pn(w)[log \u03c3(\u2212vwn Tvwj )],\nwn 6= wi, wn /\u2208 Lwj . (6)\nWe draw noise that does not equal the target word or is not in the paraphrase set of the target word from the noise distribution Pn(w). The target of the noise is labeled with zero, and the neural network is trained by maximizing the probability of the target word, given the input from the input layer and the lexicon layer, while minimizing the probability of the target word, given the noise, at the same time."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. The Corpus Used in the Experiments", "text": "We use text8 and a larger corpus called enwiki92 for the experiments because we are to evaluate if the proposed model is more robust for a smaller corpus than the previous work and keeps efficient for a larger one. Both text8 and enwiki9 are part of the English Wikipedia3 dump. Text8 contains 16,718,843 tokens while enwiki9 contains 123,353,508 tokens. The vocabulary size of text8 is 71,291, while that of enwiki9 is 218,317. We see that text8 is one tenth the size of enwiki9.\n2http://mattmahoney.net/dc/enwiki9.zip 3https://en.wikipedia.org/"}, {"heading": "B. The Task for Evaluation", "text": "The word analogical reasoning task introduced by Mikolov et al.[8] is used for evaluation in the experiments. For a quaternion of words (wA, wB , wC , wD) in which the relationship of wA and wB is similar to that of wC and wD, the objective is to predict wD on the basis of wA, wB and wC by searching the word whose vector is the closest to vB \u2212vA+vC . The dataset has a semantic part and a syntactic part. In the semantic part, (wA, wB) and (wC , wD) have a similar semantic relationship while they have a syntactic one in the syntactic part. Table II shows an example of the questions in the task.\nThere are 8,869 questions in the semantic part and 10,675 questions in the syntactic part."}, {"heading": "C. Tuning", "text": "To find the proper value of threshold \u03b8, we let the proposed neural network learn the word representations for text8 and enwiki9 and run the word analogical reasoning task with different threshold. The results for text8 is shown in Fig. 4. The results for enwiki9 is shown in Fig. 5.\nWe see that the correlation of the performance and the threshold is not linear. And it is not very same for different tasks or corpora. However, because all of the score of the paraphrases \u2014the PPDB2.0 scores are less than seven in the version we used, we can find the best threshold in the interval.\nFrom Fig. 4, we see that the best threshold for the semantic part and the total dataset is 3.8, using text8. The best threshold for the syntactic part is 5.7. From Fig. 5, we see that the best threshold for the semantic part and the total dataset is 1.5, using text9. The best threshold for the syntactic part is 5.2. It shows that lower threshold is better for a larger corpus and semantic tasks. The best accuracies are shown in Table III.\nThe other parameters are set as those used by the public word2vec demo4 that are already well tuned. The initial learning rate is set to 0.05. The number of negative samples drawn in negative sampling is set to 25. The context window is set to 8. The total iteration time is set to 25. And the size of the word vectors in the hidden layer is set to 200."}, {"heading": "D. Comparison and Results", "text": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al. [24], jointReps [23], RC-Net [25] and CBOFP [26]. For the first four, we got their public online available implements by the authors. We used the implements by the authors to learn word representations using text8 and enwiki9 and report the results. For jointReps and RC-Net, we\n4https://code.google.com/archive/p/word2vec/\nfailed to find an available implements that correctly run using text8 and enwiki9. Thus we use the results in their papers for jointReps and RC-Net. The reported results of RC-Net are achieved using enwiki9 as well, while those of jointReps are achieved using ukWaC5.\nIn Table IV, we compare our proposed neural network under text8 with CBOW, CBOW enriched with subword information, GloVe, the work of Faruqui et al. and CBOFP. We do not compare with jointReps and RC-Net here, because there are no reported results evaluated by the word analogical reasoning tasks of these methods with a similar corpus.\nWe can see that using text8, our proposed neural network achieves the best accuracy in the semantic, and outperforms the others except CBOW enriched with subword information in syntactic part. CBOW enriched with subword information is reported powerful for representing syntactic features, but ours is more balanced.\nIn Table V, we compare our proposed neural network with the previous works using enwiki9. The results of jointReps and RC-Net here are the reported results in their paper. The results of JointReps are not achieved using enwiki9 but using ukWaC.\nWe see that our proposed neural network outperforms the previous works for the semantic part and the whole dataset under enwiki9. The proposed neural network failed to outperform CBOW enriched with subword information in the syntactic part. However, our proposed neural network is better at the semantic part than the CBOW enriched with subword information and outperforms it in the overall accuracy.\nAll of the experimental results show that the proposed neural network is more balanced than the previous works, more powerful than the other works using lexicons to estimate or improve distributed word representations, benefiting from the threshold node alleviating the overfitting of polysemous words. Moreover, while the CBOFP failed to outperform most of the others under text8, the proposed neural network keeps outperform all the others in semantic parts and achieves the second best overall accuracy. It shows that the proposed neural network is more robust to the size of the corpus."}, {"heading": "V. CONCLUSIONS", "text": "To alleviate the overfitting of the polysemous words that is not well addressed in the previous works, we proposed a new neural network estimating distributed word representations in this paper. Additional to the conventional continuous bagof-words model, we added a lexicon layer, and a threshold node after the output of the lexicon layer. The threshold is manually tuned. The neural network can be trained using negative sampling. The experimental results show that the\n5http://wacky.sslmit.unibo.it\nproposed neural network is more powerful and balanced than the previous models using lexicons to estimate or improve distributed word representations. Besides, unlike our previous work CBOFP, the proposed neural network in this paper is robust to small corpora. Auto tunning of the threshold and the other parameters is a remaining issue. We are going to work on it in the future."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "journal of machine learning research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 641\u2013648.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2009, pp. 1081\u20131088.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u20132537, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 873\u2013882.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Ph.D. dissertation, Brno University of Technology, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "Proceedings of NAACL HLT, vol. 13, 2013, pp. 746\u2013751.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1532\u20131543.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Enriching word vectors with subword information", "author": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.04606, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics (ACL 2010). Association for Computational Linguistics, 2010, pp. 384\u2013394.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 2012, pp. 1201\u20131211.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations of sentences and documents.", "author": ["Q.V. Le", "T. Mikolov"], "venue": "in the 31st International Conference on Machine Learning (ICML 2014),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Bag of tricks for efficient text classification", "author": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.01759, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Gaussian lda for topic models with word embeddings", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative topic embedding: a continuous representation of documents", "author": ["S. Li", "T.-S. Chua", "J. Zhu", "C. Miao"], "venue": "the 54th annual meeting of the Association for Computational Linguistics (ACL 2016). Association for Computational Linguistics, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems 27 (NIPS 2014), 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified model for word sense representation and disambiguation.", "author": ["X. Chen", "Z. Liu", "M. Sun"], "venue": "in EMNLP. Citeseer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge.\u201d in the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)", "author": ["M. Yu", "M. Dredze"], "venue": "Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Joint word representation learning using a corpus and a semantic lexicon", "author": ["D. Bollegala", "A. Mohammed", "T. Maehara", "K.-I. Kawarabayashi"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI\u201916), 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "Proceedings of NAACL, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014, pp. 1219\u20131228.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Fuzzy paraphrases in learning word representations with a corpus and a lexicon", "author": ["Y. Ke", "M. Hagiwara"], "venue": "ArXiv e-prints, Nov. 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, vol. 13, no. Feb, pp. 307\u2013361, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "PPDB: The paraphrase database", "author": ["J. Ganitkevitch", "B. Van Durme", "C. Callison-Burch"], "venue": "Proceedings of NAACL-HLT. Atlanta, Georgia: Association for Computational Linguistics, June 2013, pp. 758\u2013764.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["E. Pavlick", "P. Rastogi", "J. Ganitkevich", "B.V. Durme", "C. Callison- Burch"], "venue": "Association for  Computational Linguistics. Beijing, China: Association for Computational Linguistics, July 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Adding semantics to data-driven paraphrasing", "author": ["E. Pavlick", "J. Bos", "M. Nissim", "C. Beller", "B.V. Durme", "C. Callison- Burch"], "venue": "Association for Computational Linguistics. Beijing, China: Association for Computational Linguistics, July 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Utexas: Natural language semantics using distributional semantics and probabilistic logic", "author": ["I. Beltagy", "S. Roller", "G. Boleda", "K. Erk", "R.J. Mooney"], "venue": "SemEval 2014, p. 796, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["J. Bjerva", "J. Bos", "R. Van der Goot", "M. Nissim"], "venue": "Proceedings of SemEval, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Umbc ebiquity-core: Semantic textual similarity systems", "author": ["L. Han", "A. Kashyap", "T. Finin", "J. Mayfield", "J. Weese"], "venue": "Proceedings of the Second Joint Conference on Lexical and Computational Semantics, vol. 1, 2013, pp. 44\u201352.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative improvements to distributional sentence similarity.", "author": ["Y. Ji", "J. Eisenstein"], "venue": "in EMNLP,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Dls@ cu: Sentence similarity from word alignment", "author": ["M.A. Sultan", "S. Bethard", "T. Sumner"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), 2014, pp. 241\u2013246.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence", "author": ["M.A. Sultan", "S. Bethard", "T. Sumner"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 219\u2013230, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-markov phrase-based monolingual alignment.", "author": ["X. Yao", "B. Van Durme", "C. Callison-Burch", "P. Clark"], "venue": "in EMNLP,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation", "author": ["J. Ganitkevitch", "C. Callison-Burch", "C. Napoles", "B. Van Durme"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011, pp. 1168\u20131179.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Distributed text representations estimated using a neural network are useful to be applied to conventional natural language processing algorithms [1]\u2013[11].", "startOffset": 146, "endOffset": 149}, {"referenceID": 10, "context": "Distributed text representations estimated using a neural network are useful to be applied to conventional natural language processing algorithms [1]\u2013[11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 161, "endOffset": 165}, {"referenceID": 15, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 189, "endOffset": 193}, {"referenceID": 17, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 231, "endOffset": 235}, {"referenceID": 15, "context": "For example, the conventional algorithms fail to correctly predicate the number of starts of 40% amazon reviews [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "The early approaches to estimate text representations use n-gram models [1], [4], [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "The early approaches to estimate text representations use n-gram models [1], [4], [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The early approaches to estimate text representations use n-gram models [1], [4], [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "propose continuous bag-of-words and skip-gram models [7], [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "propose continuous bag-of-words and skip-gram models [7], [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "[10] propose an algorithm using both local information and global information in the corpus and report a higher performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] extend the models of Mikolov et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] use the definitions in the lexicons to estimate representations for word senses and outperform the sense representations by Huang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] and Bollegala et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] estimate the word representations by not only maximizing the probability of target word given a context, but also minimizing the distance of the paraphrases in a lexicon at the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] propose a method refining trained word representation vectors using lexicons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] estimate the word representations jointly by minimizing the distance of the tail word from the sum of the vectors of the head word and the relation for a triplet of words (head, relation, tail), and making words less similar to each other in a larger category.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] propose a method considering the lexicon as a fuzzy set of paraphrases and using Bernoulli distribution subjected to the membership function of paraphrases to alleviate the problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Continuous Bag-of-Words (CBOW) with negative sampling [8] is an efficient algorithm to estimate distributed word representations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 26, "context": "It is a simplified Noise Contrastive Estimation [27].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Continuous Bag of Fuzzy Paraphrases (CBOFP) [26] proposed by Ke et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "The degree of truth is measured using the score provided by a paraphrase database called PPDB [28]\u2013[30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "The degree of truth is measured using the score provided by a paraphrase database called PPDB [28]\u2013[30].", "startOffset": 99, "endOffset": 103}, {"referenceID": 27, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 127, "endOffset": 131}, {"referenceID": 34, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 35, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 160, "endOffset": 164}, {"referenceID": 36, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 166, "endOffset": 170}, {"referenceID": 37, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 204, "endOffset": 208}, {"referenceID": 28, "context": "0 [29], [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "0 [29], [30].", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "0 scores and human judgments are reported [29].", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "[8] is used for evaluation in the experiments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "CBOW [8] 46.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "Enriched CBOW[11] 15.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "GloVe [10] 41.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Faruqui [24] 34.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "CBOFP [26] 46.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "CBOW [8] 72.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "Enriched CBOW[11] 33.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "GloVe [10] 66.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Faruqui [24] 53.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "JointReps [23] 61.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "RC-Net [25] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "CBOFP [26] 73.", "startOffset": 6, "endOffset": 10}], "year": 2016, "abstractText": "Though there are some works on improving distributed word representations using lexicons, the improper overfitting of the words that have multiple meanings is a remaining issue deteriorating the learning when lexicons are used, which needs to be solved. An alternative method is to allocate a vector per sense instead a vector per word. However, the word representations estimated in the former way are not as easy to use as the latter one. Our previous work uses a probabilistic method to alleviate the overfitting, but it is not robust with small corpus. In this paper, we propose a new neural network to estimate distributed word representations using a lexicon and a corpus. We add a lexicon layer in continuous bag-of-words model, and a threshold node after the output of the lexicon layer. The threshold rejects the \u201cbad\u201d outputs of the lexicon layer that are less likely to be the same with their inputs. In this way, it alleviates the overfitting of the polysemous words. The proposed neural network can be trained using negative sampling, which maximizing the log probabilities of target words given the context words, by distinguishing the target words from random noises. We compare the proposed neural network with continuous bagof-words model, the other works improving it, and the previous works estimating distributed word representations using both a lexicon and a corpus. The experimental results show that the proposed neural network is more efficient and balanced for both semantic tasks and syntactic tasks than the previous works, and robust to the size of the corpus.", "creator": "gnuplot 4.6 patchlevel 3"}}}