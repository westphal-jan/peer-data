{"id": "1612.00671", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Reliable Evaluation of Neural Network for Multiclass Classification of Real-world Data", "abstract": "this paper presents a systematic evaluation of neural network ( nn ) for classification of a real - world data. however in the field of machine learning, it is often seen that a single parameter that is'predict predictive accuracy'is being used for accurately evaluating the performance of a classifier model. however, this parameter might not be considered reliable given a dataset with very well high level of skewness. to demonstrate such behavior, seven different types of datasets have lately been used to evaluate a multilayer uniform perceptron ( mlp ) using twelve ( 12 ) different parameters which include micro - and macro - level estimation. in the present study, the most common problem of prediction called'multiclass'classification has been considered. screening the results that are obtained for different parameters for each of what the dataset could demonstrate interesting findings to support the usability of these set of performance modeling evaluation parameters.", "histories": [["v1", "Wed, 30 Nov 2016 19:58:44 GMT  (130kb)", "http://arxiv.org/abs/1612.00671v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["siddharth dinesh", "tirtharaj dash"], "accepted": false, "id": "1612.00671"}, "pdf": {"name": "1612.00671.pdf", "metadata": {"source": "CRF", "title": "Reliable Evaluation of Neural Network for Multiclass Classification of Real-world Data", "authors": ["Siddharth Dinesh", "Tirtharaj Dash"], "emails": ["f2012519@goa.bits-pilani.ac.in", "tirtharaj@goa.bits-pilani.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 67\n1v 1\n[ cs\n.N E\n] 3\n0 N\nov 2\nI. INTRODUCTION\nMachine Learning (ML) has been a well-explored domain of research in the present decade. It is basically a method of data analysis that automatically builds models from historical data. ML uses algorithms that iteratively learn from such historical data, which in turn finds hidden insights and patterns inside the data without being explicitly programmed for the it. ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]). For all such real world tasks, ML requires certain algorithms which are called \u2018learning algorithms\u2019 based on which the pattern inside the historical data could be explored. Based on definition of the problem and availability of data, a learning algorithm could be of two major types: supervised, or unsupervised. Supervised learning trains a model in the presence of a supervisor (technically an \u2018error\u2019 term) whereas the unsupervised algorithms do not require such an error term for training. Supervised learning methods could be applied to cases of prediction and classification. This paper is more focused in the area of supervised ML.\nSupervised ML allows access to the data labels during training and testing phases of the model. For example, there could be a problem of identifying how a student will perform in the present semester given his attendance, weekly performance, participation in the class, past records. Such a problem\nrequires huge historical student records and their performance. Supervised ML tries to understand such characteristic in the data and predicts the performance of the presently considered student. Consider a set of data records D1,D2, . . . ,Dn that have to be assigned to a set of predefined labels, or classes c1, c2, . . . , cm (m is usually less than n). The process of assigning a class label to a data record is termed as classification. Classification falls into two major categories such as binary classification, multiclass classification. Binary classification is one of the basic classification task where m is 2. For example, finding out whether the performance of the student would be \u2018good\u2019 or \u2018bad\u2019; here, \u2018good\u2019 and \u2018bad\u2019 are two categorical classes. However, classification of a data record where more than two classes are available (i.e. m > 2) is one of the challenging tasks and is called multiclass classification. For example, finding out whether the performance of the student would be \u2018excellent\u2019, \u2018good\u2019, \u2018average\u2019, \u2018below average\u2019, or \u2018poor\u2019. Evaluating a learning model for binary classification is easier as compared to evaluation of a learning model for multiclass classification because of the following reason. In the binary classification problem, a data record can be identified as true positive (tp) or true negative (tn) or false positive (fp) or false negative (fn). Generation of all these information during testing could be presented as a matrix called \u2018confusion matrix\u2019 (Table I). The performance measures which could be used to evaluate a learning model for binary classification are accuracy, sensitivity (recall), specificity, precision, F\u2212score. These parameters could be computed using Equations (1) through to Equation (5). For more information on these parameters, one can see the published work of Sokolova and Lapalme [5]. Accuracy gives an overall estimate of predictive power of a model. Pricision gives information about positive predictive value of the model. Sensitivity and Specificity estimate the true positive rate and true negative rate for the testing dataset. F \u2212 score is a balanced mean between Precision and Sensitivity.\nAccuracy = tp+ tn\ntp+ fn+ fp+ tn (1)\nSensitivity(recall) = tp\ntp+ fn (2)\nThe above-mentioned performance measures are helpful for evaluating classifiers in binary classification problems. However, researchers often use accuracy for evaluating the performance of classifiers for multiclass classification tasks. This is because of the fact that the evaluation of the above performance measures are quite difficult when more than two classes are there to be considered for the problem (e.g. multiclass classification of student performance in the semester). Moreover, accuracy measure for multiclass classification task could be non-reliable when the dataset (a set of data records) is skewed more towards a particular class. For such kind of problems, a reliable evaluation of a classifier would be very much crucial. In this work, we limit our discussion to popularly used supervised learning model (classifier) called Neural Network (NN)1. We focus our attention majorly on reliable performance evaluation of NN for real-world multiclass classification problems rather than reasoning about the obtained results. We estimate various performance measures which could be used for multiclass classification problems based on the information provided in [5].\nThe rest of the paper consists of following sections. Section II provides an explanation of the implemented NN and its training algorithm. Section III details on the tested benchmark datasets used in this work. Section IV expands on the results and discussion followed by conclusion in section V."}, {"heading": "II. NN AND ITS TRAINING WITH GRADIENT DESCENT", "text": "NN is a biologically inspired mathematical model which is used to approximate functions that depend on a set of inputs called \u2018features\u2019. Computational processing of NN closely follows information processing inside the human brain which has a complex network of neurons. The motivation behind evaluating NN in this work is that they have high adaptation power given a better learning algorithm and their rigorous applications in many different real-life problems. Moreover, there is a good amount of flexibility to tune a learning algorithm with different parameters to improve the performance of the NN. Generally, NN is a layered architecture where neurons (nodes) are arranged in layers. In this work, multilayered\n1Also known as Artificial Neural Network (ANN)\nNN, specifically, a multilayer perceptron (MLP), has been implemented. MLP architecture comprises of an input layer, a hidden layer, and an output layer. The input layer takes values of the features from the dataset and computes an output in the output layer. The hidden layer is responsible for transforming the input features into a set of features which could be processed by the output layer. It has been seen that the performance of the NN depends on the learning algorithm based on which it has been trained using the training dataset. The on-line learning has been made popular by researchers by the development of back-propagation algorithm that uses gradient descent strategy for minimization of error during training of the NN. It should be noted that the stochastic version of the gradient descent algorithm, called stochastic gradient descent (SGD), has been used in this work where the weights of the NN are updated for each random sample from the training dataset. An NN can be trained effectively by back-propagation if a sufficiently large dataset is used during training. The training dataset refers to a set of data records with known class (i.e. the class to which the data record belongs). Let a data instance be represented as a pair of vectors ( ~x,~t )\n, where ~x is the vector of input features, and ~t is the vector of target output values or classes. Let us denote ith record in the dataset as ~xi = (\nxi1 , xi2 , xi3 , . . . , xinin\n)\nand the class labels for this data record as a set {ti} (see footnote2). So, the ith training data instance in a training dataset can be represented as (xi1 , xi2 , xi3 , . . . , xinin , {ti}), where nin is the number of inputs to the neural net. Let the number of neurons in the hidden layer be represented as nhidden and number of neurons in the output layer as nout. Each neuron in one layer of the NN is connected to each neuron in its next layer with a weight value, which represents the strength of the connection. We denote these weight set as a vector \u2212\u2192 W = {wij}, where 1 \u2264 i, j \u2264 (nin + 1)nhidden + (nhidden + 1)nout. \u2212\u2192 W also includes a set of biases in the hidden layer and the output layer. The weights including the biases of an NN is called as the knowledge base (KB) of the NN. The KB of the NN is updated during training of the NN. The back-propagation based training algorithm [6] of the NN has been presently briefly in Algorithm 1 followed by a set of governing equations.\nInitialize the weights \u2212\u2192 W to small random numbers; while Stopping criteria not met do for each training pattern ( ~xi, ~ti )\ndo Process the input forward using Eq. (6); Propagate the error backward through the network using equations Eq. (9)\u2013Eq. (11);\nend end Algorithm 1: Training of NN with back-propagation\nDuring the forward processing, the output of NN can be\n2ti is a set because a class can be represented as a set of multiple classes e.g. If there are three classes of data, then a data belonging to class 1, class 2 and class 3 could be represented as {0,0,1}, {0,1,0} and {0,0,1} respectively.\nobtained by multiplying weights and the input pattern instance as shown in Eq. (6).\no = f ( \u2212\u2192 W.~x )\n(6)\nwhere, f is the activation function of the output unit and is usually a sigmoid function as given in Eq. (7).\n\u03c3(y) = 1\n1 + e\u2212c.y (7)\nwhere, c is a non-negative constant and is set to 1 in this work. The back-propagation training algorithm attempts to minimize an error term, \u03b4 (for supervised classification tasks) by changing or updating the weights of the NN. The error term is basically the squared error between the net output values and the target values for the corresponding input instance (Refer Eq. (8)). Please note that when ti is represented as a set of ones and zeros, Eq. (8) might not be suitable. In this work, as batch learning is used, it uses a modified version of the following equation. The details have been provided in the section IV.\nE( \u2212\u2192 W ) = 1\n2\n\u2211\ni\n\u2211\nk\u2208outputs\n(tki \u2212 o k i ) 2 (8)\nwhere, outputs is the set of output units in the NN; tki and o k i are the target and output values associated with the kth output unit for the ith input instance.\nFor each network output unit k, the error term \u03b4k can be computed using Eq. (9).\n\u03b4k = ok(1\u2212 ok)(tk \u2212 ok) (9)\nSimilarly, for each hidden unit h, the error term \u03b4h can be computed using Eq. (10).\n\u03b4h = oh(1\u2212 oh) \u2211\nk\u2208outputs\nwkh\u03b4k (10)\nThe weight update equation for the network are as given in Eq. (11) where, xji is the value of jth feature of the ith data record.\nwji = wji +\u2206wji (11)\nand,\n\u2206wji = \u03b7\u03b4jxji (12)\nIn the Eq. (12), the constant \u03b7 is the learning rate. It should be noted that, we also used a constant term in the weight update rule called \u2018momentum factor\u2019 (denoted as \u00b5) to the weight update rule, which makes the amount of weight update on the nth training iteration depend partially on the weight update that had occurred during the (n\u22121)th training iteration, which can be clearly understood from Eq. (13).\n\u2206wji(n) = \u03b7\u03b4jxji + \u00b5\u2206wji(n\u2212 1) (13)\nHere \u2206wji(n) is the weight update performed during the nth iteration; the constant \u00b5 is usually fixed in the range [0, 1) prior to training. In our work, the learning rate, \u03b7 and the momentum factor, \u00b5 are set to 0.3 and 0.1 respectively."}, {"heading": "III. DATASETS", "text": "To evaluate the performance of multiclass classification problem with NN, following seven different real-world benchmark datasets have been used in this work. All the datasets have been obtained from UCI Machine Learning repository [7]. However, we briefly explain all the datasets with regard to their dimension. More details about each dataset could be obtained from [7].\nAbalone dataset: The goal of using Abalone dataset is to predict abalone age through the number of rings on the shell given various descriptive attributes of the abalone. There are 4177 data instance each with 8 input features and a class label.\nBreast Cancer dataset: This dataset is one of the popular medical benchmarks in ML research. The patient records have been obtained from University of Wisconsin Hospital, Madison. There are total 699 records with 10 input features (one is an ID which is not used for computation) and a class label.\nE-coli dataset: This dataset contains protein localization sites of 336 proteins with 8 input features (one is a sequence number and not being used for computation) and class label.\nGlass dataset: This dataset is used to identify and predict the type of glass based on 9 different manufacturing features. It has 214 records containing such information.\nILPD dataset: ILPD refers to Indian Liver Patient Database contains, 583 liver patient records each has been marked as a liver patient or a non-liver patient as their type.\nIris dataset: Iris dataset is a very well known benchmark dataset which contains 4 input features and a class attribute. The dataset contains total 150 instances, 50 of each type of plants such as Iris Setosa, Iris Versicolour, and Iris Virginica.\nWine: It contains the chemical analysis results of wines grown in Italy. There are 13 predicting features and a class attribute. There are total 178 instances."}, {"heading": "IV. PERFORMANCE EVALUATION", "text": "All the simulations in this work are carried out in MATLAB R2015b using a personal computer system with Windows 10 operating system, quad-core processor with the equal clock rate of 1.70 GHz and main memory of 4 GB."}, {"heading": "A. Preparation of data for simulation", "text": "It is important to prepare data wisely before training of the NN. The real world data obtained from UCI ML repository are distributed non-uniformly and hence, they can not be used directly during training and testing of the NN. Therefore, the input features were initially normalized in the range [0,1]. The normalized dataset was then partitioned into training and an independent test set in the ratio 70:30. The process of training and testing have been repeated for 10 independent runs (simulations) to get the average performance of the NN and its performance deviation from the mean."}, {"heading": "B. Performance measures", "text": "The performance parameters which are evaluated for multiclass classification are described as follows. For a class ci, the classifier performance can be assessed with tpi, fni, tni and fpi and can be calculated from counts of testing instance belonging to ci. The quality of the overall classification performance can be assessed in two different ways such as micro and macro averaging. Macro averaging treats all the classes equally while micro averaging favors classes with more data instances. Computation of various performance measures suitable for evaluating NN for multiclass classification problem can be obtained as follows which is a generalization of the parameters presented in Table I for many classes ci [5]. For a class ci, tpi, fni, tni and fpi counts respectively. Micro- and macroaveraging indices are represented by \u00b5 and M respectively.\nAverage accuracy (Accuracy) could be used to evaluate average per-class effectiveness of the NN and can be computed as,\nAccuracy =\n\u2211m i=1 tpi+tni tpi+fni+fpi+tni\nm , (14)\nwhere, m is the number of classes. Other crucial measures could be obtained from Equation (15) through to Equation (22) [5].\nPrecision\u00b5 =\n\u2211m i=1 tpi\n\u2211m i=1 (tpi + fpi)\n(15)\nPrecisionM =\n\u2211m i=1 tpi tpi+fpi\nm (16)\nSpecificity\u00b5 =\n\u2211m\ni=1 tni \u2211m\ni=1 (fpi + tni) (17)\nSpecificityM =\n\u2211m\ni=1 tni\nfpi+tni\nm (18)\nSensitivity\u00b5(recall\u00b5) =\n\u2211m i=1 tpi\n\u2211m i=1 (tpi + fni)\n(19)\nSensitivityM(recallM ) =\n\u2211m i=1 tpi tpi+fni\nm (20)\nF \u2212 score\u00b5 = (\u03b22 + 1)Precision\u00b5Recall\u00b5\n\u03b22Precision\u00b5Recall\u00b5 (21)\nF \u2212 score\u00b5 = (\u03b22 + 1)PrecisionMRecallM\n\u03b22PrecisionMRecallM (22)\nApart from the above mentioned micro and macro measures, the training error (which is mean-squared error during training, MSEtrain), testing error (which is mean-squared error during training, MSEtest), and the training time (T imetrain) have also been noted in this work. However, it is wise to mention that the SGD does not require MSE during training rather it requires the error term (\u03b4) between a random sample and\nits prediction for updating the weights of the NN. MSE has been computed as parameter to observe the average error of convergence for the model during training and testing. MSE can be calculated using the following equation,\nMSE = 1\nn\nn \u2211\ni=1\n(ti \u2212 oi), (23)\nwhere, n is the number of data records considered during the process (either training or testing), ti is the hypothesis or the target for ith data instance, and oi is the output of the NN for the ith data instance."}, {"heading": "C. Results", "text": "The number of neurons in the hidden layer (nhidden) is one of the most important architectural parameters which directly influences performance of NN during training and capturing data for preparing a knowledge base. However, the setting of this parameter apriori has been an unsolved problem in ML research [3]. In this work, nhidden has been set to 60, 80 and 100 and the results have been noted for each of the datasets. All the obtained results have been summarized and depicted as tables for different nhidden values. Table II depicts results obtained for nhidden = 60. Similarly, Table III and Table IV present results obtained for nhidden = 80 and nhidden = 100 respectively. It should be noted that all the results shown in these three tables are averaged over ten(10) independent simulations for each of the datasets."}, {"heading": "D. Discussion", "text": "Discussion regarding the obtained results (as depicted in Table II, Table III and Table IV) in this work has been primarily based on various performance parameters rather than how the values are obtained. This work summarizes different performance parameters to use for evaluation of NN or similar classifier for multiclass classification of real-world data. It has been seen that the popularly known \u2018accuracy\u2019 parameter could not be as a reliable parameter for proper evaluation of a classifier. Hence, in this work, diversified datasets are being used for evaluation of the NN for the classification problem.\nThe training performances of the NN have been approximately equal for the Abalone dataset with different nhidden settings such as nhidden = 60, nhidden = 80 and nhidden = 100. Moreover, the standard deviation in the MSEtrain is very low for all the three cases. However, it is obvious that increasing the number of the hidden neurons increases the architectural complexity of the NN and hence the training time (T imetrain). Moreover, when the nhidden is set to as high as 100, there might be a high probability of over-fitting training data and could not achieve the better performance than performances observed for other considered settings such as nhidden = 60 or nhidden = 80. It is also evident from the obtained average classification accuracy. Moreover, given such a high accuracy of approximately 92%, the positive predictive value that is Precision does not seem to be satisfactory. Similarly, all other parameters such as Sensitivity, F\u2212score\nare not reliable. However, the true negative rate (Specificity) is quite good for all the tree different settings.\nThe performance results which have been obtained for the Breast Cancer dataset is quite better than that of Abalone. With the increase in the number of hidden neurons, the NN is able to predict the hypothesis for the test dataset with high accuracy. The error of convergence decreases with increase in nhidden which could be possible by capturing the features properly during the training of the NN. The performance parameters such as Accuracy, Precision\u00b5, PrecisionM , and other mentioned parameters seem to be following similar property. Hence, it could be assumed that predictive accuracy could be a good measure for this dataset. However, Accuracy parameter alone could not be taken as a reliable parameter while evaluating NN during multiclass classification of the Abalone dataset.\nThe evaluated performance of the NN for E-coli dataset closely follows the discussion about the performance for the Abalone dataset. Unlike the Abalone dataset, the predictive accuracy of the NN for the E-coli dataset dropped quite high with nhidden = 100. Similar cases could also be seen for other evaluated parameters for the same dataset. The performance of NN for the Glass dataset is comparatively improved with the increase in nhidden along with other performance parameters. However, although the predictive accuracy is good, the microand macro-sensitivity seems to be compromised even though the input feature is transformed into a higher dimensional feature set by the increase in nhidden. Therefore, for Glass dataset, predictive accuracy might not be a suitable parameter for evaluation of NN.\nEvaluation of the NN with regard to ILPD dataset is quite good as compared to recent literature (see [8]) considering the fact that the accuracy and other parameters still follow a particular limit of deviation unlike the results obtained for Abalone, E-coli and Glass datasets. This means that the micro and macro parameters could be considered reliable given such a low accuracy for the dataset. The performance of the NN for Iris classification is superior when nhidden = 80 as compared to the results obtained for other two settings. The positive predictive rate and negative predictive rates are also better as compared to those with nhidden = 60 and nhidden = 100. However, assuming that the predictive accuracy values are low for these two mentioned settings, it could be seen that the other performance evaluation parameters still reveal reliable performance for this dataset. However, not much deviation\nover the results obtained for the parameters could be seen for the Wine dataset where the performance for the Accuracy and other micro and macro parameters are improving with the increase in nhidden. Moreover, given the present setup of experimentation, one could also achieve slightly different results because of the fact that the initial weights and biases of the NN are fixed at random. If a proper weight set is fixed initially, one could possibly land up in obtaining a better results for the same settings. This argument could be supported by the fact that the gradient descent may not always guarantee a close-to-optimal weight set at the end of the NN training\nprocess."}, {"heading": "V. CONCLUSION", "text": "In this work, a detailed evaluation of NN classifier has been carried out for multiclass classification of real-world data. It has been seen that the use of predictive accuracy as a single parameter for evaluating an NN would not be wise given high skewness in the test data. Results obtained for different types of datasets clearly show that although the accuracy is very high, there could be fair chance that the positive or negative predictive rate falls far below any reliable range. In this work, this type of property has been seen in the performance of NN for a majority of tested datasets such as Abalone dataset, Ecoli dataset, Glass dataset. Hence, it would be wise to use many different parameters for such classification problems to accurately evaluate a classifier."}], "references": [{"title": "Machine learning: Trends, perspectives, and prospects", "author": ["M.I. Jordan", "T.M. Mitchell"], "venue": "Science, vol. 349, no. 6245, pp. 255260, Jul. 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning applications in genetics and genomics", "author": ["M.W. Libbrecht", "W.S. Noble"], "venue": "Nature Reviews Genetics, vol. 16, no. 6, pp. 321332, May 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A study on intrusion detection using neural networks trained with evolutionary algorithms", "author": ["T. Dash"], "venue": "Soft Computing, Dec. 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic navigation of wall following mobile robot using Adaptive resonance theory of type-1", "author": ["T. Dash"], "venue": "Biologically Inspired Cognitive Architectures, vol. 12, pp. 18, Apr. 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing & Management, vol. 45, no. 4, pp. 427437, Jul. 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks, vol. 1, p. 445, Jan. 1988.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1988}, {"title": "Hybrid gravitational search and particle swarm based fuzzy MLP for medical data classification", "author": ["T. Dash", "S.K. Nayak", "H.S. Behera"], "venue": "Computational Intelligence in Data Mining - Volume 1. Springer Science + Business Media, 2014, pp. 3543.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 295, "endOffset": 298}, {"referenceID": 1, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 300, "endOffset": 303}, {"referenceID": 2, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 305, "endOffset": 308}, {"referenceID": 3, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 310, "endOffset": 313}, {"referenceID": 4, "context": "For more information on these parameters, one can see the published work of Sokolova and Lapalme [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "We estimate various performance measures which could be used for multiclass classification problems based on the information provided in [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "The back-propagation based training algorithm [6] of the NN has been presently briefly in Algorithm 1 followed by a set of governing equations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "Therefore, the input features were initially normalized in the range [0,1].", "startOffset": 69, "endOffset": 74}, {"referenceID": 4, "context": "Computation of various performance measures suitable for evaluating NN for multiclass classification problem can be obtained as follows which is a generalization of the parameters presented in Table I for many classes ci [5].", "startOffset": 221, "endOffset": 224}, {"referenceID": 4, "context": "Other crucial measures could be obtained from Equation (15) through to Equation (22) [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "However, the setting of this parameter apriori has been an unsolved problem in ML research [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Evaluation of the NN with regard to ILPD dataset is quite good as compared to recent literature (see [8]) considering the fact that the accuracy and other parameters still follow a particular limit of deviation unlike the results obtained for Abalone, E-coli and Glass datasets.", "startOffset": 101, "endOffset": 104}], "year": 2016, "abstractText": "This paper presents a systematic evaluation of Neural Network (NN) for classification of real-world data. In the field of machine learning, it is often seen that a single parameter that is \u2018predictive accuracy\u2019 is being used for evaluating the performance of a classifier model. However, this parameter might not be considered reliable given a dataset with very high level of skewness. To demonstrate such behavior, seven different types of datasets have been used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different parameters which include microand macro-level estimation. In the present study, the most common problem of prediction called \u2018multiclass\u2019 classification has been considered. The results that are obtained for different parameters for each of the dataset could demonstrate interesting findings to support the usability of these set of performance evaluation parameters.", "creator": "LaTeX with hyperref package"}}}