{"id": "1612.00835", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color", "abstract": "recently, today there have been several promising competing methods to generate realistic imagery from robust deep convolutional networks. these methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from consistently large collections of photos ( e. g. faces or bedrooms ). however, unlike these methods themselves are of limited utility because it is difficult for a user to control what the network produces. in this paper, we propose a comprehensive deep adversarial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. additionally we together demonstrate a graph sketch based image synthesis system which allows users to'scribble'over the explicit sketch to indicate preferred color for objects. our network can then generate convincing images that satisfy both the color and the constraint sketch constraints required of user. the network computation is feed - forward which allows users to see repeatedly the effect of their edits individually in real time. we compare to recent work on sketch to image synthesis and additionally show that our approach can generate more realistic, more diverse, and more controllable 3d outputs. yet the architecture design is also effective at user - guided matrix colorization manipulation of grayscale images.", "histories": [["v1", "Fri, 2 Dec 2016 20:53:01 GMT  (8565kb,D)", "http://arxiv.org/abs/1612.00835v1", "13 pages, 14 figures"], ["v2", "Mon, 5 Dec 2016 20:06:57 GMT  (8565kb,D)", "http://arxiv.org/abs/1612.00835v2", "13 pages, 14 figures"]], "COMMENTS": "13 pages, 14 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["patsorn sangkloy", "jingwan lu", "chen fang", "fisher yu", "james hays"], "accepted": false, "id": "1612.00835"}, "pdf": {"name": "1612.00835.pdf", "metadata": {"source": "CRF", "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color", "authors": ["Patsorn Sangkloy", "Jingwan Lu", "Chen Fang", "Fisher Yu", "James Hays"], "emails": [], "sections": [{"heading": null, "text": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to scribble over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images."}, {"heading": "1. Introduction", "text": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13]. These methods can generate detailed and diverse (if not quite photorealistic) images in many domains. However, it is still unclear how to control these powerful new tools. How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes. Sketches are a compelling form of control because anyone can draw (potentially very badly) and because it is easy to edit sketches, e.g. to remove or add objects, whereas the equivalent operations in the image domain require artistic expertise. Color is a compelling form of control because many sketches or grayscale scenes are fundamentally ambiguous with respect to color [53], but it is easy for a user to intervene, e.g. to scribble that drapes should be blue and the valance should be red (Figure 1). Both forms of control are relatively sparse and require a deep network to synthesize image detail beyond what is contained in the input. The deep network must also implicitly learn a significant amount of high-level image understanding, e.g. what colors are allowable for particular objects, the boundaries\n1\nar X\niv :1\n61 2.\n00 83\n5v 1\nof objects such that color does not bleed beyond a single semantic region, and the appropriate high frequency textures for different scene elements.\nWe propose a deep adversarial (GAN) image synthesis architecture trained to generate realistic images from sparse and simple sketched boundaries and color strokes. We train our network on a diverse set of synthetic sketches optionally augmented with randomly sampled color strokes. The network learns to recover the color and detail lost to the sketching process and to extrapolate the sparse color indications to semantic scene elements. We show qualitative results of image synthesis in three domains \u2013 faces, cars, and bedrooms. We test on synthetic sketches as well as imperfect hand-drawn sketches.\nOur approach is similar to Sketch Inversion [14], which also generates images from sketches, although we show the benefit of adversarial training, introduce color control signals, demonstrate results on image domains beyond faces, and demonstrate that users can perform simple edits to sketches to control the synthesis. Our control signals are most similar to Zhu et al. [55] \u2013 they also demonstrate that GANs can be constrained by sketch and color strokes. However, our architecture is a feed-forward mapping from sketch and color to images while Zhu et al. perform an optimization to map user sketches into the latent GAN space in order to find the most similar image on the natural image manifold (as understood by the GAN). Their approach does not see user inputs at training time and thus cannot learn the complex mapping between user inputs and desired image outputs. Their method is also significantly slower because it is not a strictly feed-forward process and this hinders interactive image editing. The concurrent work of Isola et al. [18] significantly overlaps with our own. Both approaches use conditional GANs for the sketch to photo as well as grayscale to color synthesis tasks, although they do not focus on user control of the synthesis.\nThe contributions of this paper include:\n\u2022 First and foremost, we are the first to demonstrate an adversarial deep architecture that can learn to generate realistic images from imperfect sketches with sparse color \u2019scribbles\u2019. Our feed-forward architecture is fast and interactive.\n\u2022 We improve the quality of sketch-to-image synthesis compared to existing work [14]. We produce higher resolution, more diverse images spanning more image domains (bedrooms and cars in addition to faces).\n\u2022 We show that our method can generate realistic images from diverse sketch styles, including imperfect human sketches or edits of synthetic sketches. We achieve this generality by augmenting our training data with multiple sketch styles.\n\u2022 Finally, we demonstrate that our adversarial architecture is also promising for image colorization. We show encouraging results for grayscale to RGB conversion and introduce controllable colorization using the same sparse color strokes used with sketches."}, {"heading": "2. Related Work", "text": "Synthesizing images by learning from image collections is a long standing interest of the computer graphics and computer vision communities. Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].\nIn the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13]. While deep image synthesis methods cannot yet create realistic, high-resolution images they have an implicit ability to generalize that is difficult for data-driven non-parametric methods (e.g. the ability to hallucinate unseen viewpoints of particular chairs based on the appearance changes of other chairs [8]). Because our visual world is both enormously complex (with appearance depending on viewpoints, materials, attributes, object identity, lighting, etc.) and heavy-tailed, non-parametric methods are limited even in the \u201cbig data\u201d era. But deep image synthesis methods might implicitly factorize our visual world and thus generalize to situations beyond the training examples.\nA common approach to deep image synthesis is to learn a low dimensional latent representation that can later be used to reconstruct an image, e.g. with Variational Autoencoders (VAEs) [21] or Generative Adversarial Networks (GANs) [12]. In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .\nGenerative Adversarial Networks (GANs) Among the most promising deep image synthesis techniques are Generative Adversarial Networks (GANs) [12, 36] in which a generative network attempts to fool a simultaneously trained discriminator network that classifies images as real or synthetic. The discriminator discourages the network from producing obviously fake images. In particular, straightforward regression loss for image synthesis often leads to \u2018conservative\u2019 networks which produce blurry and desaturated outputs which are close to the mean of the data yet perceptually unrealistic. After training, the generator network is able to produce diverse images from a low dimensional latent input space. Although optimizing in this latent space can be used to \u2019walk\u2019 the natural image manifold (e.g. for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to\nsemantic attributes although mapping them to an intermediate structure image [46] can give us more insight.\nConditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50]. Conditional GANs have also been used to transform images into different domains such as a product images [51] or different artistic styles [26]. Conditional GANs can also condition the discriminator on particular inputs, e.g. Reed et al. [38] condition both the generator and discriminator on an embedding of input text. This effectively makes the discriminator more powerful. In this paper, only our generator is conditioned on input sketches and color strokes leaving the discriminator to discern real vs fake and not to evaluate the appropriateness of an output given the particular input.\nControlling deep image synthesis Several recent works share our motivation of adding user editable control to deep image generation. Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].\nThe artistic style transfer approach of Gatys et al. [11] could also be considered a mechanism to control deep image synthesis. Their method does not \u2018learn\u2019 transformations end-to-end but instead uses a pre-trained network and optimizes for output images which have deep network feature activations (content) similar to one input image and deep network feature correlations (style) similar to another input image. The approach does not perform well for transformations which requires the synthesis of realistic detail (e.g. trying to preserve the \u2018content\u2019 of a sketch and the \u2018style\u2019 of a photograph).\nThe most similar previous deep image synthesis approach in terms of control is Zhu et al. [55] which optimizes for an image that is similar to an input sketch (potentially with color strokes) that lies on a learned natural image manifold. However, identifying a matching image within this manifold that is similar content-wise to the sketch can be challenging when the sketch and the image are significantly different. For the \u2018sketch brush\u2019 in [55], they get around this by optimizing for image with the same edges as user sketch that also lies within a natural image manifold as approximated by a pre-trained GAN. However, image edges are not necessarily a good proxy for human sketched strokes [41] and their method has no capacity to learn the mapping between user inputs and desired outputs. In contrast, our method enables control via sketch and color strokes in a unified framework learned end-to-end. Sketch Inversion [14] is also closely related to our work although\nthey do not address color control. We compare our sketchto-photo results with Sketch Inversion.\nControllable Colorization Our color control strokes are inspired by Colorization Using Optimization [25] which interpolates sparse color strokes such that color changes tend to happen at intensity boundaries. The algorithm does not learn the association between objects and colors and thus can only interpolate user provided colors (e.g. a tree in the background of a scene will not be green if the user only marked foreground objects). The algorithm also does not learn the spatial extent of objects, and thus colors might \u2018snap\u2019 to spurious boundaries or bleed over weak intensity edges that are none-the-less salient boundaries. Our deep network learns object color tendencies and object extent and thus can cleanly color objects either with no color strokes or with color strokes on a subset of scene elements (Figure 1). Similar control strokes have been applied to sketch and manga imagery [35, 44], but the results remain nonphotorealistic and lack lighting and shading.\nWe are unaware of sparse scribbles being used as input constraints to deep generative networks, although ScribbleSup [27] uses sparse scribbles to supervise the output of semantic segmentation networks. The scribbles are training data and there is no user control at test time.\nConcurrent work Concurrent to our work, the \u2018pix2pix\u2019 method of Isola et al. [18] also uses conditional GANs for sketch to photo and grayscale to color synthesis. Additionally, they explore several other interesting image-to-image \u2018translation\u2019 tasks. Unlike our approach, they use a \u201cU-Net\u201d architecture [39] which allows later layers of the network to be conditioned on early layers where more spatial information is preserved. They condition both their generator and discriminator on the input whereas we condition only the generator. Their results are high quality and they are able to synthesize shoes and handbags from coarse sketches [10] even though their training data was simple image edges. In contrast, we take care to train on a diversity of synthetic sketch styles. The most significant difference between our works is that we introduce sparse color control strokes and demonstrate how to train a network so that it learns to intelligently interpolate such control signals, whereas Isola et al. [18] does not emphasize controllable synthesis."}, {"heading": "3. Overview", "text": "In this paper, we explore adding direct and fine-grained user controls to generative neural networks. We propose a generic feed-forward network that can be trained end-toend to directly transform users\u2019 control signals, for example a hand-drawn sketch and color strokes, to a high-res photo with realistic textural details.\nOur proposed network is essentially a deep generative model that is conditioned on control signals. The network\nlearns a transformation from control signal to the pixel domain. It learns to fill in missing details and colors in a realistic way. Section 3.1 discusses the network structure that is shared by all applications presented in the paper. Section 3.2 introduces the objective functions, in particular the combination of content loss and adversarial loss, which encourages the result to be photo-realistic while satisfying user\u2019s fine-grained control. Section 4 and 5 show how to enforce two different user controls in the proposed framework \u2013 using hand-drawn sketches to determine the gist or shape of the contents and using sparse color strokes to propagate colors to semantic regions. Section 6 applies the proposed framework in several interactive applications."}, {"heading": "32 64 128 256 128 64 32", "text": ""}, {"heading": "3.1. Network Architecture", "text": "We design a feed-forward neural network that takes an image as input and generates a photo of the same resolution as output. When generating an image conditioned on a high dimensional input in the same domain (i.e. from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42]. In a typical network structure, the input gets downsampled several times to a lower dimension, then goes through a sequence of non-linear transformations, and finally gets upsampled to the desired output size. Recently, He et al. [16] proposed the residual connection that uses skip layers allowing network blocks to learn only the residual component. The use of residual block eases the training of deeper networks which improves the capability of neural network for more complex tasks.\nWe employ an encoder-decoder architecture with residual connections. Starting from the network design in Sketch Inversion [14], we introduce several important modifications to improve the visual quality of output and accommodate higher resolution input and more challenging image categories, such as car and bedroom. In particular, we add one more up/downsampling layer and double the number of filters in all convolutional layers between the last downsampling layer and the first upsampling step. In ad-\ndition, we replace the deconvolutional layers with the bilinear upsampling step followed by two residual blocks, due to the recent finding that deconvolutional layers have the tendency to produce checkerboard artifacts commonly seen in deep generative models [32]. Overall, our architecture has around 7.8 millions learnable parameters, while the Sketch Inversion network we implemented has around 1.7 millions. See Figure 2 for a diagram of our architecture."}, {"heading": "3.2. Objective Function", "text": "Given pairs of training images (input, ground-truth), where the input image is derived from the ground-truth photo (synthetically generated sketches and color strokes in our case), the simplest and most common loss is the average per-pixel L2 difference between the generated image and the ground-truth, which we denote as Lp.\nPrevious work [14] showed that adding a feature loss to the objective function is beneficial for image generation tasks. Feature loss Lf is defined as the L2 difference in a feature space, where a feature is extracted from a certain layer of a pre-trained neural network representing highlevel information of images.\nWhile pixel and feature losses are widely used to explicitly correlate synthesized output with input, using them alone is often not sufficient to generate diverse, realistic images. More importantly, in our problem setup, conditioning on coarse user controls leaves us with a highly ill-posed problem where the potential solution space is multimodal. Therefore, with only pixel and feature losses, the network tends to average over all plausible solutions, due to the lack of a loss which pushes for realism and diversity.\nFor image categories like face, the generated results tend to have similar skin tones [14]. For more complicated categories like cars and bedrooms, where the foreground and background contents can have large variety of shapes and colors, the generated results might not be visually plausible, since neutral colors are chosen by the network to minimize MSE. The second and third rows in Figure 3 demonstrate the problems.\nTo encourage more variations and vividness in generated results, we experiment with adding an adversarial loss to the objective function. Generative adversarial networks (GAN), proposed by Goodfellow et al [12], have attracted considerable attention recently. A generative network G\u03b8 is jointly trained with a discriminative adversarial network D\u03c6, so that the discriminator tries to distinguish between the generated images and ground-truth images, while the generator tries to fool the discriminator into thinking the generated result is real. Dosovitskiy et al [7] showed that complimenting the feature loss with an adversarial loss leads to more realistic results. The adversarial loss Ladv is defined as:\nLadv = \u2212 \u2211 logD\u03c6(G\u03b8(xi)) (1)\nWe find that adversarial loss is also beneficial for our sketch-based image synthesis problem (Figure 3). With adversarial training, the network puts less emphasis on exactly reproducing ground-truth, but instead focuses on generating more realistic results with plausible color and shape deviation from ground-truth.\nAdversarial training tends to be unstable, especially at the start of training when the generator does not produce anything meaningful and the discriminator can easily distinguish between real and fake. We find that using a weak discriminator D\u03c6 helps stabilize the training. We also avoided conditioning the discriminator on the input image, as this tends to increase the instability [34]. In particular, we use a fully convolutional structure without fully connected layers and batch normalization. Section 7 introduces additional tricks for successful adversarial training.\nFinally, we also add a total variation loss Ltv to encourage smoothness in the output [19].\nOur final objective function becomes:\nL = wpLp +wfLf +wadvLadv +wtvLtv (2)"}, {"heading": "4. Sketch-based Photo Synthesis", "text": "In this section, we explore how to apply the proposed feed-forward network to hallucinate content, color and texture to reconstruct a photo based on an input sketch of arbitrary style. To train such a deep neural network, we need lots of training sketch-photo pairs. Though high quality hand-drawn sketches are readily available online, the corresponding photos based on which sketches are drawn are not. Therefore, we apply high-quality line drawing synthesis algorithms to generate synthetic sketches from photos. In order to handle real hand-drawn sketches at test time, we apply various data augmentations to the training data to improve the generality of the network. In this paper, we experiment with three image classes \u2013 faces [28], cars, and bedrooms [52]. We believe the proposed framework can generalize well to other categories given similar amounts of training data and training time."}, {"heading": "4.1. Generation of Training Sketches", "text": "For each image category \u2013 face, car, or bedroom \u2013 we apply the boundary detection filter XDoG [48] on 200k photos to generate the corresponding synthetic sketches. The input (and output) resolution to our network during the training phase is 128x128.\nTo make the network invariant to the exact locations of the objects, we randomly crop both the input and the ground-truth images. For the face and bedroom categories, we first resize the images to 256x256 before randomly cropping them to 128x128. For the car category, we scale the images to 170x170 before cropping, since most cars already occupy large image areas, enlarging them too much means losing the global spatial arrangement and the contexts around the cars.\nIn addition to randomly cropping an image and its corresponding sketch, we also randomly adjust the brightness level of the sketch to get different levels of details from the same sketch (i.e. some sketch lines will disappear with higher brightness level). Finally, we also randomly cut off some lines in the sketch, by overlaying a random number of white strokes (the background color of sketch input) on top of the sketch. We randomize the length, width and locations of the white strokes."}, {"heading": "4.2. Network Generalization", "text": "Real hand-drawn sketches exhibit a large variety of styles, from abstract pen-and-ink illustrations to elaborate pencil-like drawings with shading. The characteristics of the hand-drawn sketches might be very different from the synthetic sketches we generated algorithmically. Even with the various augmentations, random cropping, random brightness adjustment and random cut-off, the trained network might still overfit to that particular style of sketches. To improve the network generality, we further augment the training data by adding multiple styles of sketches.\nFor the face category, we obtain 20k additional images and for each image we randomly choose one of the following four algorithms to synthesize a corresponding sketch. See example sketches in Figure 4.\n\u2022 StyleNet [11] We apply neural network-based style transfer algorithm to transfer the texture style of a pencil drawing to the ground-truth photo.\n\u2022 Photoshop filters [2] Applying Photoshop\u2019s \u2019photocopy\u2019 effect to ground-truth images, we can generate two different versions of sketches with different levels of details and stroke darkness.\n\u2022 Gaussian blur on inverse image [1] Using Photoshop, we can also synthesize another sketch style by performing Gaussian blur on an inverse (grayscale)\nimage in Photoshop color dodge mode. This creates detailed line drawings with very little shading.\n\u2022 CUHK Finally, we add the CUHK dataset, which contains 188 hand-drawn portrait sketches and their corresponding photos [47]. To give higher weights to the high quality hand-drawn sketches, we apply mirroring and varying degrees of rotation to the sketches and end up with 1869 images in total.\nAt this point, we have 21848 images of 6 different sketch styles. Pre-trained on the 200k sketches of the XDoG style, the network is fine-tuned using the 20k multi-style sketches. We use the same parameter settings as before and train the network on these additional data for 5 epochs."}, {"heading": "4.3. Results and Discussions", "text": "For comparison purposes, we implemented the Sketch Inversion architecture as described in [14]. We trained both the Sketch Inversion network and our deeper network using the same training data and parameter settings. Figure 3 shows side-by-side comparisons of the results generated by Sketch Inversion (second row), our deeper network trained\nwithout (third row) and with adversarial loss (fourth row) on three different image categories. Compared to Sketch Inversion, our deeper network even without adversarial loss produces sharper results on complex bedroom scenes and performs better at hallucinating missing details (shapes of eyes\nand eyebrows) given simplified sketches with few lines. With adversarial loss, our network is encouraged to generate images with sharper edges, higher contrast and more realistic color and lighting. As discussed in Section 3.2, adversarial loss helps the network generate more diversified results, avoiding always producing similar skin tones and hair colors for portraits and dull and unrealistic colors for the bedrooms and cars. Figure 5 shows diverse hair colors and skin tones in the result.\nAmong the three image categories, bedroom is arguably most challenging, since each bedroom scene can contain multiple object categories. The fact that our current network handles it successfully with only 200K training data leads us to believe the possibility of training a general sketchto-photo network across several image categories using an even deeper network.\nAfter training with multiple sketch styles and various data augmentations (Section 4.2), our network generates much more realistic results given arbitrary hand-drawn sketches as input. Figure 5 shows reconstruction results based on sketches found by Google search. Note that the sketches are drawn with diverse styles, some detailed and realistic, some abstract and simplified. The results show that our network generalizes well to arbitrary hand-drawn sketches and is robust to the variations in head pose, background colors, and textures. Data augmentation such as random cropping and cutoff also helps our network hallucinate missing details. Figure 6 (left) shows that the network can fill in the missing eye to some extent. However, generating missing object parts is a challenge itself, therefore we consider it beyond the scope of this paper.\nThe network trained with adversarial loss has an interesting behavior. When applying it to cartoonish or unprofessional sketches with intentional or unintentional exaggeration of facial features, the network tends to \u2018realistify\u2019 or beautify the input sketch to generate result more photo-like at the cost of not strictly following the sketch constraints. For example, eyes that are inhumanly large will get reduced to a realistic size or faces with weird shapes will be smoothed and \u2018beautified\u2019 (see Figure 6). To produce realistic results, the network learns not to blindly trust the sketch input, but resorts to its understanding about the natural image manifold acquired during the adversarial training."}, {"heading": "5. User-guided Colorization", "text": "The previous section focuses on using a gray-scale sketch to guide the generation of a color photos. The lack of color information in the input causes the problem to be under-determined, since one sketch can correspond to photos colored in many different ways.\nAlthough the use of adversarial loss constrains the output to lie on an approximated manifold of natural images and therefore limits the color choices, it is still up to the\ngenerator (and the discriminator) to choose a specific color. In this section, we explore how to allow users to directly control the colors in the output. To do that, we need to modify the input to the network to include rough color information during training (Section 5.1). We investigated adding color controls in two applications, guided sketch colorization (Section 5.2) and guided image colorization (Section 5.3)."}, {"heading": "5.1. Generation of Training Color Strokes", "text": "One of the most intuitive ways to control the outcome of colorization is to \u2018scribble\u2019 some color strokes to indicate the preferred color in a region.\nTo train a network to recognize these control signals at test time, we need to synthesize color strokes for the training data. We generate synthetic strokes based on the colors in the ground-truth image.\nTo emulate arbitrary user behaviors, we blur the groundtruth image and sample a random number of color strokes of random length and thickness at random locations. We pick the ground-truth pixel color at the stroke starting point as the stroke color and continue to grow the stroke until the maximum length is reached.\nWhen growing a stroke, if the difference between the current pixel color and the stroke color exceeds a certain threshold, we restart the stroke with a new color sampled at the current pixel. By randomizing various stroke parameters, we are able to synthesize color strokes similar to what human would draw during test time."}, {"heading": "5.2. Guided Sketch Colorization", "text": "The goal here is to add color control to our sketch-based image synthesis pipeline. Our previous objective function still holds: we want the output to have the same content as the input (pixel and feature loss), and appear realistic (adversarial loss). Pixel loss is essential here as it forces the network to be more precise with color by paying more attention to the color strokes. We modify the training data by placing color strokes on top of the input sketches. We then train the network as before using a parameter setting that emphasizes content loss and de-emphasizes adversarial loss, so that the results better satisfy color constraints (Section 7.2).\nFigure 7 shows the results of reconstructing bedroom and car scenes based on an input sketch and color strokes. Note that the colors of the strokes deviate a lot from the colors in the ground-truth image, nevertheless, the network is able to propagate the input color to the relevant regions respecting object boundaries. In the bedroom scene (two rightmost columns), based on the crude outline of a picture frame and a yellow lamp, our network successfully generates plausible details in the results. See more results in the supplementary material."}, {"heading": "5.3. Guided Image Colorization", "text": "Recent work [17, 53] explores training deep neural network models for the image colorization tasks. However, the selection of colors in the output is entirely up to the network. In this section, we investigate using color strokes (Section 5.1) to guide the colorization process. We generate training data by extracting a one-channel grayscale image from the ground-truth photo and combining it with the three-channel image containing color strokes.\nFigure 8 shows various colorization results on a car image. Given a gray-scale image, our system synthesizes realistic looking cars based on strokes drawn with different colors at random locations. Note that most strokes are placed on the body of the car and therefore do not influence the colorization of the other regions. Due to adversarial training, the sky is colored blue and the trees are colored green, regardless of the colors of the foreground object. Internally, the network learns to recognize semantic contents and therefore can put right colors in the relevant regions while at the same time satisfying user constraints wherever possible.\nSee more results in the supplementary material."}, {"heading": "6. Applications", "text": "We believe being able to control the generated output using sketch and color allows for many useful applications, especially in the artistic domain."}, {"heading": "6.1. Interactive Image Generation Tools", "text": "Given an input image of resolution 256x256, our network takes about 20ms to transform it to a photo-like result. The real-time performance enables instant visual feedback after incremental edits in image generation and editing applications.\nUsing sketch and color strokes to enforce fine-grained control is useful for several design applications. For example, an interior designer can quickly sketch out rough shapes of the objects, specify colors in various regions and let our system fill in missing details and textures to generate a plausible bedroom scene. After seeing the result, the designer can interactively modify the shapes and colors of the objects and receive instant visual feedback. Figure 7 illustrates the potential design workflow. Similarly, a car designer can follow similar workflow to design new cars and test out the looks in different background settings.\nOur portrait synthesis system provides a tool for artists to design virtual characters (see Figure 9). Based on the initial design, one can change the shape of eyes and/or hairstyle, add glasses and/or head decorations, etc. In addition to design, portrait reconstruction technology is useful for forensic purposes, for example the law enforcement department can use it to help identify suspects [14]."}, {"heading": "6.2. Sketch and Color Guided Visual Search", "text": "Our image generation tools provide flexible ways to perform visual search. With a target scene in mind, one can sketch out the object boundaries and color constraints, based on which our network can reconstruct a plausible arrangement. The reconstructed image can then be used in a typical visual search tool to identify high-res images with similar contents (see Figure 10)."}, {"heading": "7. Network Training Details", "text": "With the unpredictable nature of adversarial training, we find it helpful to separate the training into two stages."}, {"heading": "7.1. Optimizing for Content Loss", "text": "In the first stage, we set the adversarial weight wadv from equation 2 to 0 and let the network focus on minimizing the content loss which is a combination of pixel and feature loss. To enforce a fine-grained control using the input sketch, we choose the ReLU2-2 layer of the VGG-19 net [43] to compute the feature loss, since higher level feature representations tend to encourage the network to ignore important details such as the exact locations of the pupils.\nWe set the weights of pixel loss and feature loss wp, wf to 1, and the weight of TV loss wtv to 1e-5. We train the network for around 3 epochs using a batch size of 32 before moving on to the second stage of the training."}, {"heading": "7.2. Adding Adversarial Loss", "text": "Given the network pretrained for content loss, we fine tune it with different loss settings for different applications. For photo reconstruction from gray-scale sketches (Section 4), we turn off the pixel loss, keep the feature loss and add the adversarial loss with the following weight setting, wf = 1,wp = 0,wtv = 0,wadv \u2248 1e8. For colorization applications (Section 5), we emphasize the feature and pixel loss and de-emphasize the adversarial loss, so that the output better follows the color controls, wf = 10,wp = 1,wtv = 0,wadv \u2248 1e5.\nWe train the adversarial discriminator alongside our generative network for three epochs using\na learning rate between 1e-5 and 1e-6."}, {"heading": "8. Conclusion and Future Work", "text": "In this paper, we propose a deep generative framework that enables two types of user controls to guide the result\ngeneration, using coarse sketch to guide high-level visual structure and using sparse color strokes to control object color pattern.\nDespite the promising results, our current system suffers from several limitations. First, we sometimes observe blurry boundaries between object parts or regions of different colors which diminish the overall realism of the results.\nFigure 7 shows the color leaking problem on the car results, where the color of the car\u2019s hood leaks into the background. Second, our system struggles between strictly following color/sketch controls and minimizing adversarial loss. In other words, adversarial loss prohibits the generated images from taking uncommon colors and shapes. If the user specifies a rare color, for example, purple for car, red for trees, our network will map it to a different color deemed more realistic by the adversarial loss. Third, the network sees objects of similar scale during training, and would expect to see the same scale at testing. As future work, we can add multi-scale support to the network by randomizing the ratio between the cropping size and the image size during training.\nGoing forward, we would like to investigate how to further improve the visual results by encouraging sharp color boundaries and finding systematic ways to deal with rare control signals."}, {"heading": "Acknowledgments", "text": "We thank Yijun Li for assistance with generation of synthetic training sketches from [11]. This work is supported by a Royal Thai Government Scholarship to Patsorn Sangkloy, NSF CAREER award 1149853 to James Hays, and NSF award 1561968."}], "references": [{"title": "Patchmatch: a randomized correspondence algorithm for structural image editing", "author": ["C. Barnes", "E. Shechtman", "A. Finkelstein", "D. Goldman"], "venue": "ACM Transactions on Graphics- TOG,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Neural Photo Editing with Introspective Adversarial Networks", "author": ["A. Brock", "T. Lim", "J.M. Ritchie", "N. Weston"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Semantic style transfer and turning two-bit doodles into fine artworks", "author": ["A.J. Champandard"], "venue": "arXiv preprint arXiv:1603.01768,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Sketch2photo: internet image montage", "author": ["T. Chen", "M.-M. Cheng", "P. Tan", "A. Shamir", "S.-M. Hu"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Tobias Springenberg", "T. Brox"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Texture synthesis by nonparametric sampling", "author": ["A.A. Efros", "T.K. Leung"], "venue": "In Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "How do humans sketch objects", "author": ["M. Eitz", "J. Hays", "M. Alexa"], "venue": "ACM Trans. Graph.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Image style transfer using convolutional neural networks", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems, pages 2672\u20132680,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Convolutional sketch inversion", "author": ["Y. G\u00fc\u00e7l\u00fct\u00fcrk", "U. G\u00fc\u00e7l\u00fc", "R. van Lier", "M.A. van Gerven"], "venue": "In Proceeding of the ECCV workshop on VISART Where Computer Vision Meets Art,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Scene completion using millions of photographs", "author": ["J. Hays", "A.A. Efros"], "venue": "In ACM SIGGRAPH 2007 Papers, SIGGRAPH", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification", "author": ["S. Iizuka", "E. Simo-Serra", "H. Ishikawa"], "venue": "ACM Transactions on Graphics (Proc. of SIGGRAPH 2016),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Image-to-image translation with conditional adversarial networks. arxiv, 2016", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "author": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Amortised MAP Inference for Image Superresolution", "author": ["C. Kaae S\u00f8nderby", "J. Caballero", "L. Theis", "W. Shi", "F. Husz\u00e1r"], "venue": "ArXiv e-prints,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In The International Conference on Learning Representations (ICLR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang", "W. Shi"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Colorization using optimization", "author": ["A. Levin", "D. Lischinski", "Y. Weiss"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Precomputed real-time texture synthesis with markovian generative adversarial networks", "author": ["C. Li", "M. Wand"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation", "author": ["D. Lin", "J. Dai", "J. Jia", "K. He", "J. Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Plenoptic modeling: An imagebased rendering system", "author": ["L. McMillan", "G. Bishop"], "venue": "In Proceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1995}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["A. Nguyen", "A. Dosovitskiy", "J. Yosinski", "T. Brox", "J. Clune"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["A. Nguyen", "J. Yosinski", "Y. Bengio", "A. Dosovitskiy", "J. Clune"], "venue": "In arXiv pre-print", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Deconvolution and checkerboard artifacts. http://distill.pub/2016/deconvcheckerboard/, 2016", "author": ["A. Odena", "V. Dumoulin", "C. Olah"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A. v. d. Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "In Proceedings of the 33th International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Manga colorization", "author": ["Y. Qu", "T.-T. Wong", "P.-A. Heng"], "venue": "ACM Transactions on Graphics (SIGGRAPH", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Learning what and where to draw", "author": ["S. Reed", "Z. Akata", "S. Mohan", "S. Tenka", "B. Schiele", "H. Lee"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Generative adversarial text-to-image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "U-Net: Convolutional Networks for Biomedical Image Segmentation, pages 234\u2013241", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "The sketchy database: Learning to retrieve badly drawn bunnies", "author": ["P. Sangkloy", "N. Burnell", "C. Ham", "J. Hays"], "venue": "ACM Transactions on Graphics (proceedings of SIG- GRAPH),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup", "author": ["E. Simo-Serra", "S. Iizuka", "K. Sasaki", "H. Ishikawa"], "venue": "ACM Transactions on Graphics (SIG- GRAPH),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Lazybrush: Flexible painting tool for hand-drawn cartoons", "author": ["D. S\u1ef3kora", "J. Dingliana", "S. Collins"], "venue": "In Computer Graphics Forum,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["X. Wang", "A. Gupta"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Face photo-sketch synthesis and recognition", "author": ["X. Wang", "X. Tang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Xdog: an extended difference-of-gaussians compendium including advanced image stylization", "author": ["H. Winnem\u00f6Ller", "J.E. Kyprianidis", "S.C. Olsen"], "venue": "Computers & Graphics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Semantic image inpainting with perceptual and contextual losses", "author": ["R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do"], "venue": "arXiv preprint arXiv:1607.07539,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Pixellevel domain transfer", "author": ["D. Yoo", "N. Kim", "S. Park", "A.S. Paek", "I.S. Kweon"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "LSUN: construction of a large-scale image dataset using deep learning with humans", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "in the loop. CoRR,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV, 2016", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "View synthesis by appearance flow", "author": ["T. Zhou", "S. Tulsiani", "W. Sun", "J. Malik", "A.A. Efros"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 21, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 9, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 33, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 18, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 10, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 5, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 46, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 51, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 11, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 52, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 15, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 42, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 50, "context": "Color is a compelling form of control because many sketches or grayscale scenes are fundamentally ambiguous with respect to color [53], but it is easy for a user to intervene, e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "Our approach is similar to Sketch Inversion [14], which also generates images from sketches, although we show the benefit of adversarial training, introduce color control signals, demonstrate results on image domains beyond faces, and demonstrate that users can perform simple edits to sketches to control the synthesis.", "startOffset": 44, "endOffset": 48}, {"referenceID": 52, "context": "[55] \u2013 they also demonstrate that GANs can be constrained by sketch and color strokes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] significantly overlaps with our own.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "\u2022 We improve the quality of sketch-to-image synthesis compared to existing work [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 6, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 12, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 3, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 0, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 9, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 5, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 33, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 18, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 10, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 5, "context": "the ability to hallucinate unseen viewpoints of particular chairs based on the appearance changes of other chairs [8]).", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "with Variational Autoencoders (VAEs) [21] or Generative Adversarial Networks (GANs) [12].", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "with Variational Autoencoders (VAEs) [21] or Generative Adversarial Networks (GANs) [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 42, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 72, "endOffset": 76}, {"referenceID": 46, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 147, "endOffset": 150}, {"referenceID": 51, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 180, "endOffset": 184}, {"referenceID": 50, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 205, "endOffset": 217}, {"referenceID": 14, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 205, "endOffset": 217}, {"referenceID": 19, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 205, "endOffset": 217}, {"referenceID": 9, "context": "Generative Adversarial Networks (GANs) Among the most promising deep image synthesis techniques are Generative Adversarial Networks (GANs) [12, 36] in which a generative network attempts to fool a simultaneously trained discriminator network that classifies images as real or synthetic.", "startOffset": 139, "endOffset": 147}, {"referenceID": 33, "context": "Generative Adversarial Networks (GANs) Among the most promising deep image synthesis techniques are Generative Adversarial Networks (GANs) [12, 36] in which a generative network attempts to fool a simultaneously trained discriminator network that classifies images as real or synthetic.", "startOffset": 139, "endOffset": 147}, {"referenceID": 1, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 18, "endOffset": 25}, {"referenceID": 52, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 18, "endOffset": 25}, {"referenceID": 27, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 51, "endOffset": 59}, {"referenceID": 28, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 51, "endOffset": 59}, {"referenceID": 43, "context": "semantic attributes although mapping them to an intermediate structure image [46] can give us more insight.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 182, "endOffset": 190}, {"referenceID": 34, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 182, "endOffset": 190}, {"referenceID": 20, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 232, "endOffset": 240}, {"referenceID": 17, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 232, "endOffset": 240}, {"referenceID": 31, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 276, "endOffset": 288}, {"referenceID": 30, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 276, "endOffset": 288}, {"referenceID": 47, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 276, "endOffset": 288}, {"referenceID": 48, "context": "Conditional GANs have also been used to transform images into different domains such as a product images [51] or different artistic styles [26].", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "Conditional GANs have also been used to transform images into different domains such as a product images [51] or different artistic styles [26].", "startOffset": 139, "endOffset": 143}, {"referenceID": 35, "context": "[38] condition both the generator and discriminator on an embedding of input text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 55, "endOffset": 58}, {"referenceID": 35, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 174, "endOffset": 178}, {"referenceID": 8, "context": "[11] could also be considered a mechanism to control deep image synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[55] which optimizes for an image that is similar to an input sketch (potentially with color strokes) that lies on a learned natural image manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "For the \u2018sketch brush\u2019 in [55], they get around this by optimizing for image with the same edges as user sketch that also lies within a natural image manifold as approximated by a pre-trained GAN.", "startOffset": 26, "endOffset": 30}, {"referenceID": 38, "context": "However, image edges are not necessarily a good proxy for human sketched strokes [41] and their method has no capacity to learn the mapping between user inputs and desired outputs.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "Sketch Inversion [14] is also closely related to our work although they do not address color control.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "Controllable Colorization Our color control strokes are inspired by Colorization Using Optimization [25] which interpolates sparse color strokes such that color changes tend to happen at intensity boundaries.", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "Similar control strokes have been applied to sketch and manga imagery [35, 44], but the results remain nonphotorealistic and lack lighting and shading.", "startOffset": 70, "endOffset": 78}, {"referenceID": 41, "context": "Similar control strokes have been applied to sketch and manga imagery [35, 44], but the results remain nonphotorealistic and lack lighting and shading.", "startOffset": 70, "endOffset": 78}, {"referenceID": 24, "context": "We are unaware of sparse scribbles being used as input constraints to deep generative networks, although ScribbleSup [27] uses sparse scribbles to supervise the output of semantic segmentation networks.", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "[18] also uses conditional GANs for sketch to photo and grayscale to color synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Unlike our approach, they use a \u201cU-Net\u201d architecture [39] which allows later layers of the network to be conditioned on early layers where more spatial information is preserved.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "Their results are high quality and they are able to synthesize shoes and handbags from coarse sketches [10] even though their training data was simple image edges.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "[18] does not emphasize controllable synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 124, "endOffset": 128}, {"referenceID": 50, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 149, "endOffset": 157}, {"referenceID": 14, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 149, "endOffset": 157}, {"referenceID": 39, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 185, "endOffset": 189}, {"referenceID": 13, "context": "[16] proposed the residual connection that uses skip layers allowing network blocks to learn only the residual component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Starting from the network design in Sketch Inversion [14], we introduce several important modifications to improve the visual quality of output and accommodate higher resolution input and more challenging image categories, such as car and bedroom.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "In addition, we replace the deconvolutional layers with the bilinear upsampling step followed by two residual blocks, due to the recent finding that deconvolutional layers have the tendency to produce checkerboard artifacts commonly seen in deep generative models [32].", "startOffset": 264, "endOffset": 268}, {"referenceID": 11, "context": "Previous work [14] showed that adding a feature loss to the objective function is beneficial for image generation tasks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "For image categories like face, the generated results tend to have similar skin tones [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Generative adversarial networks (GAN), proposed by Goodfellow et al [12], have attracted considerable attention recently.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "Dosovitskiy et al [7] showed that complimenting the feature loss with an adversarial loss leads to more realistic results.", "startOffset": 18, "endOffset": 21}, {"referenceID": 31, "context": "We also avoided conditioning the discriminator on the input image, as this tends to increase the instability [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "Finally, we also add a total variation loss Ltv to encourage smoothness in the output [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "In this paper, we experiment with three image classes \u2013 faces [28], cars, and bedrooms [52].", "startOffset": 62, "endOffset": 66}, {"referenceID": 49, "context": "In this paper, we experiment with three image classes \u2013 faces [28], cars, and bedrooms [52].", "startOffset": 87, "endOffset": 91}, {"referenceID": 45, "context": "For each image category \u2013 face, car, or bedroom \u2013 we apply the boundary detection filter XDoG [48] on 200k photos to generate the corresponding synthetic sketches.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "\u2022 StyleNet [11] We apply neural network-based style transfer algorithm to transfer the texture style of a pencil drawing to the ground-truth photo.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "\u2022 CUHK Finally, we add the CUHK dataset, which contains 188 hand-drawn portrait sketches and their corresponding photos [47].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "For comparison purposes, we implemented the Sketch Inversion architecture as described in [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 50, "context": "Guided Image Colorization: a) grayscale input, b) original color image, c) deep colorization result [53], d) First and third rows: color strokes overlaid on top of the grayscale input (zoom in to see the color strokes).", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Recent work [17, 53] explores training deep neural network models for the image colorization tasks.", "startOffset": 12, "endOffset": 20}, {"referenceID": 50, "context": "Recent work [17, 53] explores training deep neural network models for the image colorization tasks.", "startOffset": 12, "endOffset": 20}, {"referenceID": 11, "context": "In addition to design, portrait reconstruction technology is useful for forensic purposes, for example the law enforcement department can use it to help identify suspects [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 40, "context": "To enforce a fine-grained control using the input sketch, we choose the ReLU2-2 layer of the VGG-19 net [43] to compute the feature loss, since higher level feature representations tend to encourage the network to ignore important details such as the exact locations of the pupils.", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "We thank Yijun Li for assistance with generation of synthetic training sketches from [11].", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to scribble over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.", "creator": "LaTeX with hyperref package"}}}