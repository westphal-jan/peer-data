{"id": "1612.02295", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2016", "title": "Large-Margin Softmax Loss for Convolutional Neural Networks", "abstract": "cross - entropy loss together with softmax is arguably one way of the most common best used supervision components in convolutional neural networks ( cnns ). despite its simplicity, popularity and excellent performance, the component does not likewise explicitly encourage discriminative learning of features. in interpreting this further paper, we propose a generalized large - resource margin softmax ( l - softmax ) loss which explicitly encourages intra - class compactness and inter - class separability between learned features. moreover, l - softmax not only can adjust the desired margin but also can avoid incentive overfitting. we also show that the l - softmax characteristic loss can be optimized by typical stochastic gradient descent. extensive experiments on four benchmark datasets separately demonstrate that the deeply - learned features with l - softmax loss has become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.", "histories": [["v1", "Wed, 7 Dec 2016 15:36:11 GMT  (2423kb,D)", "http://arxiv.org/abs/1612.02295v1", "Published in ICML 2016. Revised some typos"], ["v2", "Tue, 17 Jan 2017 07:28:18 GMT  (2423kb,D)", "http://arxiv.org/abs/1612.02295v2", "Published in ICML 2016. Revised some typos"], ["v3", "Mon, 23 Jan 2017 08:32:08 GMT  (2423kb,D)", "http://arxiv.org/abs/1612.02295v3", "Fixed some typos"]], "COMMENTS": "Published in ICML 2016. Revised some typos", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["weiyang liu", "yandong wen", "zhiding yu", "meng yang"], "accepted": true, "id": "1612.02295"}, "pdf": {"name": "1612.02295.pdf", "metadata": {"source": "META", "title": "Large-Margin Softmax Loss for Convolutional Neural Networks", "authors": ["Weiyang Liu", "Yandong Wen", "Zhiding Yu", "Meng Yang"], "emails": ["WYLIU@PKU.EDU.CN", "WEN.YANDONG@MAIL.SCUT.EDU.CN", "YZHIDING@ANDREW.CMU.EDU", "YANG.MENG@SZU.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "Over the past several years, convolutional neural networks (CNNs) have significantly boosted the state-of-the-art performance in many visual classification tasks such as object recognition, (Krizhevsky et al., 2012; Sermanet et al., 2014; He et al., 2015b;a), face verification (Taigman et al., 2014; Sun et al., 2014; 2015) and hand-written digit recognition (Wan et al., 2013). The layered learning architecture, together with convolution and pooling which carefully extract features from local to global, renders the strong visual representation ability of CNNs as well as their current significant positions in large-scale visual recognition tasks.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nConvolution Units\nFully Connected Layer (Classifier)\nSoftmax Function\nCross-entropy Loss\nSoftmax Loss\nConvolutional Feature Learning Training\nData Fully Connected Layer (Feature Extraction)\nFigure 1. Standard CNNs can be viewed as convolutional feature learning machines that are supervised by the softmax loss.\nFacing the increasingly more complex data, CNNs have been continuously improved with deeper structures (Simonyan & Zisserman, 2014; Szegedy et al., 2015), smaller strides (Simonyan & Zisserman, 2014) and new non-linear activations (Goodfellow et al., 2013; Nair & Hinton, 2010; He et al., 2015b). While benefiting from the strong learning ability, CNNs also have to face the crucial issue of overfilling. Considerable effort such as large-scale training data (Russakovsky et al., 2014), dropout (Krizhevsky et al., 2012), data augmentation (Krizhevsky et al., 2012; Szegedy et al., 2015), regularization (Hinton et al., 2012; Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013) and stochastic pooling (Zeiler & Fergus, 2013) has been put to address the issue.\nA recent trend towards learning with even stronger features is to reinforce CNNs with more discriminative information. Intuitively, the learned features are good if their intra-class compactness and inter-class separability are simultaneously maximized. While this may not be easy due to the inherent large intra-class variations in many tasks, the strong representation ability of CNNs make it possible to learn invariant features towards this direction. Inspired by such idea, the contrastive loss (Hadsell et al., 2006) and triplet loss (Schroff et al., 2015) were proposed to enforce extra intra-class compactness and inter-class separa-\n\u2020Equal contribution. ?Corresponding author.\nar X\niv :1\n61 2.\n02 29\n5v 1\n[ st\nat .M\nL ]\n7 D\nec 2\nbility. A consequent problem, however, is that the number of training pairs and triplets can theoretically go up to O(N2) where N is the total number of training samples. Considering that CNNs often handle large-scale training sets, a subset of training samples need to be carefully selected for these losses. The softmax function is widely adopted by many CNNs (Krizhevsky et al., 2012; He et al., 2015a;b) due to its simplicity and probabilistic interpretation. Together with the cross-entropy loss, they form arguably one of the most commonly used components in CNN architectures. In this paper, we define the softmax loss as the combination of a cross-entropy loss, a softmax function and the last fully connected layer (see Fig. 1). Under such definition, many prevailing CNN models can be viewed as the combination of a convolutional feature learning component and a softmax loss component, as shown in Fig. 1. Despite its popularity, current softmax loss does not explicitly encourage intra-class compactness and inter-class-separability. Our key intuition is that the separability between sample and parameter can be factorized into amplitude ones and angular ones with cosine similarity: Wcx = \u2016Wc\u20162\u2016x\u20162 cos(\u03b8c), where c is the class index, and the corresponding parameters Wc of the last fully connected layer can be regarded as the linear classifier of class c. Under softmax loss, the label prediction decision rule is largely determined by the angular similarity to each class since softmax loss uses cosine distance as classification score. The purpose of this paper, therefore, is to generalize the softmax loss to a more general large-margin softmax (L-Softmax) loss in terms of angular similarity, leading to potentially larger angular separability between learned features. This is done by incorporating a\npreset constantmmultiplying with the angle between sample and the classifier of ground truth class. m determines the strength of getting closer to the ground truth class, producing an angular margin. One shall see, the conventional softmax loss becomes a special case of the L-Softmax loss under our proposed framework. Our idea is verified by Fig. 2 where the learned features by L-Softmax become much more compact and well separated.\nThe L-Softmax loss is a flexible learning objective with adjustable inter-class angular margin constraint. It presents a learning task of adjustable difficulty where the difficulty gradually increases as the required margin becomes larger. The L-Softmax loss has several desirable advantages. First, it encourages angular decision margin between classes, generating more discriminative features. Its geometric interpretation is very clear and intuitive, as elaborated in Section 3.2. Second, it partially avoids overfitting by defining a more difficult learning target, casting a different viewpoint to the overfitting problem. Third, L-Softmax not only benefits classification tasks but also benefits verification tasks. Taking face verification as an example, the task is to determine whether two given testing faces belong to the same person. Ideal features for verification requires the minimal inter-class distance to be greater than the maximal intra-class distance. While conventional softmax loss does not perform well, the proposed L-Softmax loss can significantly improve the performance on this tasks.\nOur experiments validate that L-Softmax can effectively boost the performance in both classification and verification tasks. More intuitively, the visualizations of the learned features in Fig. 2 and Fig. 5 show great discrimina-\ntiveness of the L-Softmax loss. Besides performance, the L-Softmax loss is also well motivated with clear geometric interpretation as elaborated in Section 3.3."}, {"heading": "2. Related Work and Preliminaries", "text": "Current widely used data loss functions in CNNs include Euclidean loss, (square) hinge loss, information gain loss, contrastive loss, triplet loss, Softmax loss, etc. To enhance the intra-class compactness and inter-class separability, (Sun et al., 2014) trains the CNN with the combination of softmax loss and contrastive loss. The contrastive loss inputs the CNNs with pairs of training samples. If the input pair belongs to the same class, the contrastive loss will require their features are as similar as possible. Otherwise, the contrastive loss will require their distance larger than a margin. (Schroff et al., 2015) uses the triplet loss to encourage a distance constraint similar to the contrastive loss. Differently, the triplet loss requires 3 (or a multiple of 3) training samples as input at a time. The triplet loss minimizes the distance between an anchor sample and a positive sample (of the same identity), and maximizes the distance between the anchor sample and a negative sample (of different identity). Both triplet loss and contrastive loss require a carefully designed pair selection procedure. Both (Sun et al., 2014) and (Schroff et al., 2015) suggest that enforcing such a distance constraint that encourages intraclass compactness and inter-class separability can greatly boost the feature discriminativeness, which motivates us to employ a margin constraint in the original softmax loss.\nUnlike any previous work, our work cast a novel view on generalizing the original softmax loss. We define the i-th input feature xi with the label yi. Then the original softmax loss can be written as\nL = 1\nN \u2211 i Li = 1 N \u2211 i\n\u2212 log (\nefyi\u2211 j e fj\n) (1)\nwhere fj denotes the j-th element (j \u2208 [1,K], K is the number of classes) of the vector of class scores f , and N is the number of training data. In the softmax loss, f is usually the activations of a fully connected layer W , so fyi can be written as fyi = W T yixi in which Wyi is the yi-th column of W . Note that, we omit the constant b in fj ,\u2200j here to simplify analysis, but our L-Softmax loss can still be easily modified to work with b (In fact, the performance is nearly of no difference, so we do not make it complicated here.). Because fj is the inner product between Wj and xi, it can be also formulated as fj = \u2016Wj\u2016\u2016xi\u2016 cos(\u03b8j) where \u03b8i (0 \u2264 \u03b8j \u2264 \u03c0) is the angle between the vector Wj and xi. Thus the loss becomes\nLi = \u2212 log (\ne\u2016Wyi\u2016\u2016xi\u2016 cos(\u03b8yi )\u2211 j e \u2016Wj\u2016\u2016xi\u2016 cos(\u03b8j)\n) (2)"}, {"heading": "3. Large-Margin Softmax Loss", "text": ""}, {"heading": "3.1. Intuition", "text": "We give a simple example to describe our intuition. Consider the binary classification and we have a sample x from class 1. The original softmax is to force W T1 x > W T 2 x (i.e. \u2016W1\u2016\u2016x\u2016 cos(\u03b81) > \u2016W2\u2016\u2016x\u2016 cos(\u03b82)) in order to classify x correctly. However, we want to make the classification more rigorous in order to produce a decision margin. So we instead require \u2016W1\u2016\u2016x\u2016 cos(m\u03b81) > \u2016W2\u2016\u2016x\u2016 cos(\u03b82) (0 \u2264 \u03b81 \u2264 \u03c0m ) where m is a positive integer. Because the following inequality holds:\n\u2016W1\u2016\u2016x\u2016 cos(\u03b81) \u2265 \u2016W1\u2016\u2016x\u2016 cos(m\u03b81) > \u2016W2\u2016\u2016x\u2016 cos(\u03b82).\n(3)\nTherefore, \u2016W1\u2016\u2016x\u2016 cos(\u03b81) > \u2016W2\u2016\u2016x\u2016 cos(\u03b82) has to hold. So the new classification criteria is a stronger requirement to correctly classify x, producing a more rigorous decision boundary for class 1."}, {"heading": "3.2. Definition", "text": "Following the notation in the preliminaries, the L-Softmax loss is defined as\nLi = \u2212 log (\ne\u2016Wyi\u2016\u2016xi\u2016\u03c8(\u03b8yi ) e\u2016Wyi\u2016\u2016xi\u2016\u03c8(\u03b8yi ) + \u2211 j 6=i e \u2016Wj\u2016\u2016xi\u2016 cos(\u03b8j) ) (4)\nin which we generally require\n\u03c8(\u03b8) =  cos(m\u03b8), 0 \u2264 \u03b8 \u2264 \u03c0 m D(\u03b8), \u03c0\nm < \u03b8 \u2264 \u03c0\n(5)\nwhere m is a integer that is closely related to the classification margin. With larger m, the classification margin becomes larger and the learning objective also becomes harder. Meanwhile, D(\u03b8) is required to be a monotonically decreasing function and D( \u03c0m ) should equal cos( \u03c0 m ).\nTo simplify the forward and backward propagation, we construct a specific \u03c8(\u03b8i) in this paper:\n\u03c8(\u03b8) = (\u22121)k cos(m\u03b8)\u2212 2k, \u03b8 \u2208 [k\u03c0 m , (k + 1)\u03c0 m ] (6)\nwhere k \u2208 [0,m \u2212 1] and k is an integer. Combining Eq. (1), Eq. (4) and Eq. (6), we have the L-Softmax loss that\nis used throughout the paper. For forward and backward propagation, we need to replace cos(\u03b8j) with WTj xi \u2016Wj\u2016\u2016xi\u2016 , and replace cos(m\u03b8yi) with\ncos(m\u03b8yi) = C 0 m cos m(\u03b8yi)\u2212 C 2 m cos m\u22122(\u03b8yi)(1\u2212 cos 2(\u03b8yi))\n+ C4m cos m\u22124(\u03b8yi)(1\u2212 cos 2(\u03b8yi)) 2 + \u00b7 \u00b7 \u00b7\n(\u22121)nC2nm cosm\u22122n(\u03b8yi)(1\u2212 cos 2(\u03b8yi)) n + \u00b7 \u00b7 \u00b7 (7)\nwhere n is an integer and 2n \u2264 m. After getting rid of \u03b8, we could perform derivation with respect to x and W . It is also trivial to perform derivation with mini-batch input."}, {"heading": "3.3. Geometric Interpretation", "text": "We aim to encourage aa angle margin between classes via the L-Softmax loss. To simplify the geometric interpretation, we analyze the binary classification case where there are only W1 and W2.\nFirst, we consider the \u2016W1\u2016 = \u2016W2\u2016 scenario as shown in Fig. 4. With \u2016W1\u2016 = \u2016W2\u2016, the classification result depends entirely on the angles between x and W1(W2). In the training stage, the original softmax loss requires \u03b81 < \u03b82 to classify the sample x as class 1, while the L-Softmax loss requires m\u03b81 < \u03b82 to make the same decision. We can see the L-Softmax loss is more rigor about the classification criteria, which leads to a classification margin between class 1 and class 2. If we assume both softmax loss and L-Softmax loss are optimized to the same value\nand all training features can be perfectly classified, then the angle margin between class 1 and class 2 is given by m\u22121 m+1\u03b81,2 where \u03b81,2 is the angle between classifier vector W1 and W2. The L-Softmax loss also makes the decision boundaries for class 1 and class 2 different as shown in Fig 4, while originally the decision boundaries are the same. From another viewpoint, we let \u03b8\u20321 = m\u03b81 and assume that both the original softmax loss and the L-Softmax loss can be optimized to the same value. Then we can know \u03b8\u20321 in the original softmax loss ism\u22121 times larger than \u03b81 in the L-Softmax loss. As a result, the angle between the learned feature and W1 will become smaller. For every class, the same conclusion holds. In essence, the L-Softmax loss narrows the feasible angle1 for every class and produces an angle margin between these classes.\nFor both the \u2016W1\u2016 > \u2016W2\u2016 and \u2016W1\u2016 < \u2016W2\u2016 scenarios, the geometric interpretation is a bit more complicated. Because the length of W1 and W2 is different, the feasible angles of class 1 and class 2 are also different (see the decision boundary of original softmax loss in Fig. 4). Normally, the larger Wj is, the larger the feasible angle of its corresponding class is. As a result, the L-Softmax loss also produces different feasible angles for different classes. Similar to the analysis of the \u2016W1\u2016 = \u2016W2\u2016 scenario, the proposed loss will also generate a decision margin between class 1 and class 2."}, {"heading": "3.4. Discussion", "text": "The L-Softmax loss utilizes a simple modification over the original softmax loss, achieving a classification angle margin between classes. By assigning different values for m, we define a flexible learning task with adjustable difficulty for CNNs. The L-Softmax loss is endowed with some nice properties such as\n\u2022 The L-Softmax loss has clear geometric interpretation. m controls the margin among classes. With bigger m (under the same training loss), the ideal margin between classes becomes larger and the learning difficulty is also increased. With m = 1, the L-Softmax loss becomes identical to the original softmax loss.\n\u2022 The L-Softmax loss defines a relatively difficult learning objective with adjustable margin (difficulty). A difficult learning objective can effectively avoid overfitting and take full advantage of the strong learning ability from deep and wide architectures.\n\u2022 The L-Softmax loss can be used as drop-in replacements for standard loss function, and also can be used in tandem with other performance-boosting ap-\n1Feasible angle of the i-th class refers to the possible angle between x and Wi that is learned via the L-Softmax loss.\nproaches such as learning activation functions, adding batch normalization, training with data augmentation, using other pooling functions, etc."}, {"heading": "4. Optimization", "text": "It is easy to compute the forward and backward propagation for the L-Softmax loss, so it is also trivial to optimize the L-Softmax loss using typical stochastic gradient descent. For Li, the only difference between the original softmax loss and the L-Softmax loss lies in fyi . Thus we only need to compute fyi in forward and backward propagation while fj , j 6= yi is the same as the original softmax loss. Putting in Eq. (6) and Eq. (7), fyi is written as\nfyi =(\u22121) k \u00b7 \u2016Wyi\u2016\u2016xi\u2016 cos(m\u03b8i)\u2212 2k \u00b7 \u2016Wyi\u2016\u2016xi\u2016 =(\u22121)k \u00b7 \u2016Wyi\u2016\u2016xi\u2016 ( C0m ( W Tyixi \u2016Wyi\u2016\u2016xi\u2016 )m\u2212 C2m ( W Tyixi \u2016Wyi\u2016\u2016xi\u2016 )m\u22122 (1\u2212 ( W Tyixi \u2016Wyi\u2016\u2016xi\u2016 )2 ) + \u00b7 \u00b7 \u00b7\n) \u2212 2k \u00b7 \u2016Wyi\u2016\u2016xi\u2016\n(8)\nwhere WTyi x\n\u2016Wyi\u2016\u2016x\u2016 \u2208 [cos( (k+1)\u03c0m ), cos( k\u03c0 m )] and k is an in-\nteger that belongs to [0,m\u22121]. For the backward propagation, we use the chain rule to compute the partial derivative: \u2202Li \u2202xi = \u2211 j \u2202Li \u2202fj \u2202fj \u2202xi and \u2202Li\u2202Wyi = \u2211 j \u2202Li \u2202fj \u2202fj \u2202Wyi\n. Because \u2202Li \u2202fj and \u2202fj\u2202xi , \u2202fj \u2202Wyi\n,\u2200j 6= yi are the same for both original softmax loss and L-Softmax loss, we leave it out for simplicity. \u2202fyi\u2202xi and \u2202fyi \u2202Wyi can be computed via\n\u2202fyi \u2202xi\n= (\u22121)k \u00b7 ( C0m m(W Tyixi) m\u22121Wyi\n(\u2016Wyi\u2016\u2016xi\u2016)m\u22121 \u2212\nC0m (m\u2212 1)(W Tyixi) mxi\n\u2016Wyi\u2016m\u22121\u2016xi\u2016m+1 \u2212 C2m\n(m\u2212 2)(W Tyixi) m\u22123Wyi\n(\u2016Wyi\u2016\u2016xi\u2016)m\u22123\n+ C2m (m\u2212 3)(W Tyixi) m\u22122xi\n\u2016Wyi\u2016m\u22123\u2016xi\u2016m\u22121 + C2m\nm(W Tyixi) m\u22121Wyi\n(\u2016Wyi\u2016\u2016xi\u2016)m\u22121\n\u2212 C2m (m\u2212 1)(W Tyixi) mxi\n\u2016Wyi\u2016m\u22121\u2016xi\u2016m+1 + \u00b7 \u00b7 \u00b7 ) \u2212 2k \u00b7 \u2016Wyi\u2016xi\u2016xi\u2016 ,\n(9) \u2202fyi \u2202Wyi\n= (\u22121)k \u00b7 ( C0m m(W Tyixi) m\u22121xi\n(\u2016Wyi\u2016\u2016xi\u2016)m\u22121 \u2212\nC0m (m\u2212 1)(W Tyixi) mWyi \u2016Wyi\u2016m+1\u2016xi\u2016m\u22121 \u2212 C2m (m\u2212 2)(W Tyixi) m\u22123xi (\u2016Wyi\u2016\u2016xi\u2016)m\u22123\n+ C2m (m\u2212 3)(W Tyixi) m\u22122Wyi \u2016Wyi\u2016m\u22121\u2016xi\u2016m\u22123 + C2m m(W Tyixi) m\u22121xi (\u2016Wyi\u2016\u2016xi\u2016)m\u22121 \u2212 C2m (m\u2212 1)(W Tyixi)\nmWyi \u2016Wyi\u2016m+1\u2016xi\u2016m\u22121 + \u00b7 \u00b7 \u00b7 ) \u2212 2k \u00b7 \u2016xi\u2016Wyi\u2016Wyi\u2016 .\n(10)\nIn implementation, k can be efficiently computed by constructing a look-up table for WTyi xi\n\u2016Wyi\u2016\u2016xi\u2016 (i.e. cos(\u03b8yi)). To\nbe specific, we give an example of the forward and back-\nward propagation when m = 2. Thus fi is written as\nfi = (\u22121)k 2(W Tyixi) 2 \u2016Wyi\u2016\u2016xi\u2016 \u2212 ( 2k + (\u22121)k ) \u2016Wyi\u2016\u2016xi\u2016 (11)\nwhere, k =  1, WTyi xi \u2016Wyi\u2016\u2016xi\u2016 \u2264 cos(\u03c0 2 )\n0, WTyi xi\n\u2016Wyi\u2016\u2016xi\u2016 > cos(\u03c0\n2 ) .\nIn backward propagation, \u2202fyi\u2202xi , \u2202fyi \u2202Wyi\ncan be computed with\n\u2202fyi \u2202xi =(\u22121)k ( 4W TyixiWyi \u2016Wyi\u2016\u2016xi\u2016 \u2212 2(W Tyixi) 2xi \u2016Wyi\u2016\u2016xi\u20163 ) \u2212 ( 2k + (\u22121)k\n)\u2016Wyi\u2016xi \u2016xi\u2016 ,\n(12)\n\u2202fyi \u2202Wyi\n=(\u22121)k ( 4W Tyixixi\n\u2016Wyi\u2016\u2016xi\u2016 \u2212\n2(W Tyixi) 2Wyi\n\u2016xi\u2016\u2016Wyi\u20163 ) \u2212 ( 2k + (\u22121)k\n)\u2016xi\u2016Wyi \u2016Wyi\u2016 .\n(13)\nWhilem \u2265 3, we can still use Eq. (8), Eq. (9) and Eq. (10) to compute the formula for forward and backward propagation."}, {"heading": "5. Experiments and Results", "text": ""}, {"heading": "5.1. Experimental Settings", "text": "We evaluate the generalized softmax loss in two typical vision applications: visual classification and face verification. In visual classification, we use three standard benchmark datasets: MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky, 2009), and CIFAR100 (Krizhevsky, 2009). In face verification, we evaluate our method on the widely used LFW dataset (Huang et al., 2007). We only use a single model in all baseline CNNs to compare our performance. For convenience, we use L-Softmax to denote the L-Softmax loss. Both Softmax and L-Softmax in the experiments use the same CNN shown in Table 1.\nGeneral Settings: We follow the design philosophy of VGG-net (Simonyan & Zisserman, 2014) in two aspects: (1) for convolution layers, the kernel size is 3\u00d73 and 1 padding (if not specified) to keep the feature map unchanged. (2) for pooling layers, if the feature map size is halved, the number of filters is doubled in order to preserve the time complexity per layer. Our CNN architectures are described in Table 1. In convolution layers, the stride is set to 1 if not specified. We implement the CNNs using the Caffe library (Jia et al., 2014) with our modifications. For all experiments, we adopt the PReLU (He et al., 2015b) as the activation functions, and the batch size is 256. We use a weight decay of 0.0005 and momentum of 0.9. The weight initialization in (He et al., 2015b)\nLayer MNIST (for Fig. 2) MNIST CIFAR10/CIFAR10+ CIFAR100 LFW Conv0.x N/A [3\u00d73, 64]\u00d71 [3\u00d73, 64]\u00d71 [3\u00d73, 96]\u00d71 [3\u00d73, 64]\u00d71, Stride 2 Conv1.x [5\u00d75, 32]\u00d72, Padding 2 [3\u00d73, 64]\u00d73 [3\u00d73, 64]\u00d74 [3\u00d73, 96]\u00d74 [3\u00d73, 64]\u00d74\nPool1 2\u00d72 Max, Stride 2 Conv2.x [5\u00d75, 64]\u00d72, Padding 2 [3\u00d73, 64]\u00d73 [3\u00d73, 96]\u00d74 [3\u00d73, 192]\u00d74 [3\u00d73, 256]\u00d74\nPool2 2\u00d72 Max, Stride 2 Conv3.x [5\u00d75, 128]\u00d72, Padding 2 [3\u00d73, 64]\u00d73 [3\u00d73, 128]\u00d74 [3\u00d73, 384]\u00d74 [3\u00d73, 256]\u00d74\nPool3 2\u00d72 Max, Stride 2 Conv4.x N/A N/A N/A N/A [3\u00d73, 256]\u00d74\nFully Connected 2 256 256 512 512\nCIFAR10 L-Softmax(m=4)\nCIFAR10+ L-Softmax(m=4)\nCIFAR100 L-Softmax(m=4)\nFigure 5. Confusion matrix on CIFAR10, CIFAR10+ and CIFAR100.\nand batch normalization (Ioffe & Szegedy, 2015) are used in our networks but with no dropout. Note that we only perform the mean substraction preprocessing for training and testing data. For optimization, normally the stochastic gradient descent will work well. However, when training data has too many subjects (such as CASIA-WebFace dataset), the convergence of L-Softmax will be more difficult than softmax loss. For those cases that L-Softmax has difficulty converging, we use a learning strategy by letting fyi = \u03bb\u2016Wyi\u2016\u2016xi\u2016 cos(\u03b8yi )+\u2016Wyi\u2016\u2016xi\u2016\u03c8(\u03b8yi ) 1+\u03bb and start the gradient descent with a very large \u03bb (it is similar to optimize the original softmax). Then we gradually reduce \u03bb during iteration. Ideally \u03bb can be gradually reduced to zero, but in practice, a small value will usually suffice.\nMNIST, CIFAR10, CIFAR100: We start with a learning rate of 0.1, divide it by 10 at 12k and 15k iterations, and eventually terminate training at 18k iterations, which is determined on a 45k/5k train/val split.\nFace Verification: The learning rate is set to 0.1, 0.01, 0.001 and is switched when the training loss plateaus. The total number of epochs is about is about 30 for our models.\nTesting: we use the softmax to classify the testing samples in MNIST, CIFAR10 and CIFAR100 dataset. In LFW dataset, we use the simple cosine distance and the nearest neighbor rule for face verification."}, {"heading": "5.2. Visual Classification", "text": "MNIST: Our network architecture is shown in Table 1. Table 2 shows the previous best results and those for our proposed L-Softmax loss. From the results, the L-Softmax loss not only outperforms the original softmax loss using the same network but also achieves the state-of-the-art performance compared to the other deep CNN architectures. In Fig. 2, we also visualize the learned features by the L-Softmax loss and compare them to the original softmax\nloss. Fig. 2 validates the effectiveness of the large margin constraint within L-Softmax loss. With larger m, we indeed obtain a larger angular decision margin.\nCIFAR10: We use two commonly used comparison protocols in CIFAR10 dataset. We first compare our L-Softmax loss under no data augmentation setup. For the data augmentation experiment, we follow the standard data augmentation in (Lee et al., 2015) for training: 4 pixels are padded on each side, and a 32\u00d732 crop is randomly sampled from the padded image or its horizontal flip. In testing, we only evaluate the single view of the original 32\u00d732 image. The results are shown in Table 3. One can observe that our L-Softmax loss greatly boosts the accuracy, achieving 1%-2% improvement over the original softmax loss and the other state-of-the-art CNNs.\nCIFAR100: We also evaluate the generalize softmax loss on the CIFAR100 dataset. The CNN architecture refers to Table 1. One can notice that the L-Softmax loss outperform the CNN with softmax loss and all the other competitive methods. The L-Softmax loss improves more than 2.5%\naccuracy over the CNN and more than 1% over the current state-of-the-art CNN.\nConfusion Matrix Visualization: We also give the confusion matrix comparison between the softmax baseline and the L-Softmax loss (m=4) in Fig. 5. Specifically we normalize the learned features and then calculate the cosine distance between these features. From Fig. 5, one can see that the intra-class compactness is greatly enhanced while the inter-class separability is also enlarged.\nError Rate vs. Iteration: Fig. 6 illustrates the relation between the error rate and the iteration number with different m in the L-Softmax loss. We use the same CNN (same as the CIFAR10 network) to optimize the L-Softmax loss with m = 1, 2, 3, 4, and then plot their training and testing error rate. One can observe that the original softmax suffers from severe overfitting problem (training loss is very low but testing loss is higher), while the L-Softmax loss can greatly avoid such problem. Fig. 7 shows the relation between the error rate and the iteration number with different number of filters in the L-Softmax loss (m=4). We use four different CNN architecture to optimize the L-Softmax loss with m = 4, and then plot their training and testing error rate. These four CNN architectures have the same structure and only differ in the number of filters\n(e.g. 32/32/64/128 denotes that there are 32, 32, 64 and 128 filters in every convolution layer of Conv0.x, Conv1.x Conv2.x and Conv3.x, respectively). On both the training set and testing set, the L-Softmax loss with larger number of filters performs better than those with smaller number of filters, indicating L-Softmax loss does not easily suffer from overfitting. The results also show that our L-Softmax loss can be optimized easily. Therefore, one can learn that the L-Softmax loss can make full use of the stronger learning ability of CNNs, since stronger learning ability leads to performance gain."}, {"heading": "5.3. Face Verification", "text": "To further evaluate the learned features, we conduct an experiment on the famous LFW dataset (Huang et al., 2007). The dataset collects 13,233 face images from 5749 persons from uncontrolled conditions. Following the unrestricted with labeled outside data protocol (Huang et al., 2007), we train on the publicly available CASIA-WebFace (Yi et al., 2014) outside dataset (490k labeled face images belonging to over 10,000 individuals) and test on the 6,000 face pairs on LFW. People overlapping between the outside training data and the LFW testing data are excluded. As preprocess-\ning, we use IntraFace (Asthana et al., 2014) to align the face images and then crop them based on 5 points. Then we train a single network for feature extraction, so we only compare the single model performance of current state-ofthe-art CNNs. Finally PCA is used to form a compact feature vector. The results are given in Table 5. The generalize softmax loss achieves the current best results while only trained with the CASIA-WebFace outside data, and is also comparable to the current state-of-the-art CNNs with private outside data. Experimental results well validate the conclusion that the L-Softmax loss encourages the intraclass compactness and inter-class separability."}, {"heading": "6. Concluding Remarks", "text": "We proposed the Large-Margin Softmax loss for the convolutional neural networks. The large-margin softmax loss defines a flexible learning task with adjustable margin. We can set the parameter m to control the margin. With larger m, the decision margin between classes also becomes larger. More appealingly, the Large-Margin Softmax loss has very clear in- tuition and geometric interpretation. The extensive experimental results on severak benchmark datasets show clear advantages over current state-of-\nthe-art CNNs and all the compared baselines."}], "references": [{"title": "Incremental face alignment in the wild", "author": ["Asthana", "Akshay", "Zafeiriou", "Stefanos", "Cheng", "Shiyang", "Pantic", "Maja"], "venue": "In CVPR,", "citeRegEx": "Asthana et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asthana et al\\.", "year": 2014}, {"title": "Robust face recognition via multimodal deep face representation", "author": ["Ding", "Changxing", "Tao", "Dacheng"], "venue": "IEEE TMM,", "citeRegEx": "Ding et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2015}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Hadsell", "Raia", "Chopra", "Sumit", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "LearnedMiller", "Erik"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical Report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "author": ["Lee", "Chen-Yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["Liang", "Ming", "Hu", "Xiaolin"], "venue": "In CVPR,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Deep face recognition", "author": ["Parkhi", "Omkar M", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In BMVC,", "citeRegEx": "Parkhi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parkhi et al\\.", "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Romero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Schroff", "Florian", "Kalenichenko", "Dmitry", "Philbin", "James"], "venue": "In CVPR,", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Striving for simplicity", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "In ICLR,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Stollenga", "Marijn F", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In NIPS,", "citeRegEx": "Stollenga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Deep learning face representation by joint identificationverification", "author": ["Sun", "Yi", "Chen", "Yuheng", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In NIPS,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Deeply learned face representations are sparse, selective, and robust", "author": ["Sun", "Yi", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In CVPR,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lars"], "venue": "In CVPR,", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Learning face representation from scratch", "author": ["Yi", "Dong", "Lei", "Zhen", "Liao", "Shengcai", "Li", "Stan Z"], "venue": "arXiv preprint arXiv:1411.7923,", "citeRegEx": "Yi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2014}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": ", 2015b;a), face verification (Taigman et al., 2014; Sun et al., 2014; 2015) and hand-written digit recognition (Wan et al.", "startOffset": 30, "endOffset": 76}, {"referenceID": 25, "context": ", 2015b;a), face verification (Taigman et al., 2014; Sun et al., 2014; 2015) and hand-written digit recognition (Wan et al.", "startOffset": 30, "endOffset": 76}, {"referenceID": 29, "context": ", 2014; 2015) and hand-written digit recognition (Wan et al., 2013).", "startOffset": 49, "endOffset": 67}, {"referenceID": 27, "context": "Facing the increasingly more complex data, CNNs have been continuously improved with deeper structures (Simonyan & Zisserman, 2014; Szegedy et al., 2015), smaller strides (Simonyan & Zisserman, 2014) and new non-linear activations (Goodfellow et al.", "startOffset": 103, "endOffset": 153}, {"referenceID": 18, "context": "Considerable effort such as large-scale training data (Russakovsky et al., 2014), dropout (Krizhevsky et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 11, "context": ", 2014), dropout (Krizhevsky et al., 2012), data augmentation (Krizhevsky et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 11, "context": ", 2012), data augmentation (Krizhevsky et al., 2012; Szegedy et al., 2015), regularization (Hinton et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 27, "context": ", 2012), data augmentation (Krizhevsky et al., 2012; Szegedy et al., 2015), regularization (Hinton et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 5, "context": ", 2015), regularization (Hinton et al., 2012; Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013) and stochastic pooling (Zeiler & Fergus, 2013) has been put to address the issue.", "startOffset": 24, "endOffset": 113}, {"referenceID": 29, "context": ", 2015), regularization (Hinton et al., 2012; Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013) and stochastic pooling (Zeiler & Fergus, 2013) has been put to address the issue.", "startOffset": 24, "endOffset": 113}, {"referenceID": 2, "context": "Inspired by such idea, the contrastive loss (Hadsell et al., 2006) and triplet loss (Schroff et al.", "startOffset": 44, "endOffset": 66}, {"referenceID": 19, "context": ", 2006) and triplet loss (Schroff et al., 2015) were proposed to enforce extra intra-class compactness and inter-class separa-", "startOffset": 25, "endOffset": 47}, {"referenceID": 25, "context": "To enhance the intra-class compactness and inter-class separability, (Sun et al., 2014) trains the CNN with the combination of softmax loss and contrastive loss.", "startOffset": 69, "endOffset": 87}, {"referenceID": 19, "context": "(Schroff et al., 2015) uses the triplet loss to encourage a distance constraint similar to the contrastive loss.", "startOffset": 0, "endOffset": 22}, {"referenceID": 25, "context": "Both (Sun et al., 2014) and (Schroff et al.", "startOffset": 5, "endOffset": 23}, {"referenceID": 19, "context": ", 2014) and (Schroff et al., 2015) suggest that enforcing such a distance constraint that encourages intraclass compactness and inter-class separability can greatly boost the feature discriminativeness, which motivates us to employ a margin constraint in the original softmax loss.", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "In visual classification, we use three standard benchmark datasets: MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky, 2009), and CIFAR100 (Krizhevsky, 2009).", "startOffset": 74, "endOffset": 94}, {"referenceID": 6, "context": "In face verification, we evaluate our method on the widely used LFW dataset (Huang et al., 2007).", "startOffset": 76, "endOffset": 96}, {"referenceID": 9, "context": "We implement the CNNs using the Caffe library (Jia et al., 2014) with our modifications.", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": "Method Error Rate CNN (Jarrett et al., 2009) 0.", "startOffset": 22, "endOffset": 44}, {"referenceID": 29, "context": "53 DropConnect (Wan et al., 2013) 0.", "startOffset": 15, "endOffset": 33}, {"referenceID": 17, "context": "57 FitNet (Romero et al., 2015) 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "31 GenPool (Lee et al., 2016) 0.", "startOffset": 11, "endOffset": 29}, {"referenceID": 29, "context": "Method CIFAR10 CIFAR10+ DropConnect (Wan et al., 2013) 9.", "startOffset": 36, "endOffset": 54}, {"referenceID": 17, "context": "32 FitNet (Romero et al., 2015) N/A 8.", "startOffset": 10, "endOffset": 31}, {"referenceID": 22, "context": "97 All-CNN (Springenberg et al., 2015) 9.", "startOffset": 11, "endOffset": 38}, {"referenceID": 13, "context": "43 GenPool (Lee et al., 2016) 7.", "startOffset": 11, "endOffset": 29}, {"referenceID": 17, "context": "5% Method Error Rate FitNet (Romero et al., 2015) 35.", "startOffset": 28, "endOffset": 49}, {"referenceID": 24, "context": "57 dasNet (Stollenga et al., 2014) 33.", "startOffset": 10, "endOffset": 34}, {"referenceID": 22, "context": "78 All-CNN (Springenberg et al., 2015) 33.", "startOffset": 11, "endOffset": 38}, {"referenceID": 13, "context": "75 GenPool (Lee et al., 2016) 32.", "startOffset": 11, "endOffset": 29}, {"referenceID": 19, "context": "Method Outside Data Accuracy FaceNet (Schroff et al., 2015) 200M* 99.", "startOffset": 37, "endOffset": 59}, {"referenceID": 16, "context": "65 Deep FR (Parkhi et al., 2015) 2.", "startOffset": 11, "endOffset": 32}, {"referenceID": 26, "context": "95 DeepID2+ (Sun et al., 2015) 300K* 98.", "startOffset": 12, "endOffset": 30}, {"referenceID": 30, "context": "7 (Yi et al., 2014) WebFace 97.", "startOffset": 2, "endOffset": 19}, {"referenceID": 6, "context": "To further evaluate the learned features, we conduct an experiment on the famous LFW dataset (Huang et al., 2007).", "startOffset": 93, "endOffset": 113}, {"referenceID": 6, "context": "Following the unrestricted with labeled outside data protocol (Huang et al., 2007), we train on the publicly available CASIA-WebFace (Yi et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 30, "context": ", 2007), we train on the publicly available CASIA-WebFace (Yi et al., 2014) outside dataset (490k labeled face images belonging to over 10,000 individuals) and test on the 6,000 face pairs on LFW.", "startOffset": 58, "endOffset": 75}, {"referenceID": 0, "context": "As preprocessing, we use IntraFace (Asthana et al., 2014) to align the face images and then crop them based on 5 points.", "startOffset": 35, "endOffset": 57}], "year": 2016, "abstractText": "Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.", "creator": "LaTeX with hyperref package"}}}