{"id": "1701.02870", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Context-aware Captions from Context-agnostic Supervision", "abstract": "broadly we introduce a technique to produce discriminative context - aware image captions ( captions consisting that describe differences between images or visual concepts ) using only generic context - agnostic training data ( captions that describe a concept or an acoustic image in isolation ). for example, given images and captions of \" siamese cat \" and \" tiger malay cat \", our system generates language that describes the \" siamese cat \" in a way that distinguishes it from \" tiger cat \". we firstly start with a theoretical generic language model that is context - agnostic and add a listener to discriminate recognition between closely - related concepts. our approach offers two key advantages over previous work : 1 ) our listener does not need separate training, and 2 ) allows joint inference to decode sentences that satisfy both the speaker and listener - - yielding an introspective interaction speaker. we first apply our introspective speaker to a justification task, i. e. to describe why an image contains a particular fine - grained category as opposed to another closely related category in the ieee cub - 200 - 2011 dataset. we then study discriminative image captioning to generate language that visually uniquely refers to one out of two semantically similar images in the coco dataset. evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker - listener approaches for discrimination.", "histories": [["v1", "Wed, 11 Jan 2017 07:42:58 GMT  (8500kb,D)", "http://arxiv.org/abs/1701.02870v1", "16 pages, 10 figures"], ["v2", "Tue, 20 Jun 2017 08:59:56 GMT  (8699kb,D)", "http://arxiv.org/abs/1701.02870v2", "Accepted to CVPR 2017"], ["v3", "Mon, 31 Jul 2017 23:29:36 GMT  (8699kb,D)", "http://arxiv.org/abs/1701.02870v3", "Accepted to CVPR 2017 (Spotlight)"]], "COMMENTS": "16 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ramakrishna vedantam", "samy bengio", "kevin murphy", "devi parikh", "gal chechik"], "accepted": false, "id": "1701.02870"}, "pdf": {"name": "1701.02870.pdf", "metadata": {"source": "CRF", "title": "Context-aware Captions from Context-agnostic Supervision", "authors": ["Ramakrishna Vedantam", "Virginia Tech", "Samy Bengio", "Kevin Murphy", "Gal Chechik"], "emails": ["vrama91@vt.edu", "bengio@google.com", "kpmurphy@google.com", "parikh@gatech.edu", "gal@google.com"], "sections": [{"heading": "1. Introduction", "text": "Language is the primary modality we use to communicate, and represent knowledge. To convey relevant information, we often use language in a way that takes into account context: instead of describing a situation in a \u201cliteral\u201d way, we might pragmatically emphasize selected aspects in order to be persuasive, impactful or effective. Consider describing the target image at the bottom left in Fig. 1. A\nliteral description \u201cAn airplane is flying in the sky\u201d conveys the semantics of the image, but would be unsatisfying if the goal is to disambiguate this image from the distractor image (red border). For this purpose, a more pragmatic description would be, \u201cA large passenger jet flying through a blue sky\u201d, which is aware of the context of the distractor image being a small propeller plane. Humans use such pragmatic considerations continuously, and effortlessly in teaching, conversation, discussion, etc.\nIn this vein, it seems prudent to create machines which are more aware of pragmatics. One approach for this would be to collect training data of how language is used in context, for example, discriminative ground truth utterances\nar X\niv :1\n70 1.\n02 87\n0v 1\n[ cs\n.C V\n] 1\n1 Ja\nn 20\nfrom humans describing images in the context of other images, or justifications explaining why an image contains a target class as opposed to a distractor class (Fig. 1). Unfortunately, collecting such data has a prohibitive cost, since the space of objects, and possible contexts is too large. Furthermore, in some cases the context in which we wish to be pragmatic may not even be known apriori. For example, a free-form conversation agent may have to respond in a context-aware or discriminative fashion depending upon the history of a conversation. Such scenarios may also arise in human-robot interaction, as in the case where, a robot may need to reason about which spoon a person is asking to hand over. Collecting context-aware data in such cases seems intractable. In this paper, we focus on deriving pragmatic (context-aware) behavior given access to generic (contextagnostic) ground truth.\nWe study two qualitatively different real-world vision tasks that require pragmatic reasoning. The first is justification, where the model needs to justify why an image corresponds to one fine-grained object category, as opposed to a closely related, yet undepcited category. Justification is a task that hobbyists, and domain experts would identify with: ornithologists and botanists often need to explain why an image is of a particular species as opposed to a closelyrelated species. Another potential application for justifications is \u201cmachine teaching\u201d, where an algorithm instructs non-expert humans about new concepts.\nThe second task we study is discriminative image captioning, where the goal is to generate a sentence that describes an image in the context of similar-looking images. This task is not only grounded in pragmatics, but is also interesting as a scene understanding task to check fine-grained image understanding, with potential applications to human robot interaction.\nAt a high level, we are motivated by similar considerations as recent work by Andreas, and Klein [2]. This work presents the first weakly supervised (using only context-free data) model for derived pragmatics without hand-tuned language models. Our approach addresses a similar problem, but develops a simple, unified inference procedure to induce pragmatic behavior. Our insight is that one need not assess discrimination by training a separate model, but may simply re-use the sampling distribution from the generative model. This allows us to develop an inference procedure which is more efficient at searching the exponentially large output space of discriminative utterances (Sec. 5). Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.\nIn summary, the key contributions of this paper are: \u2022 A novel inference procedure that models an introspec-\ntive speaker (IS), allowing a speaker (S) (say a generic\nimage captioning model) to reason about pragmatic behavior without additional training. \u2022 Two new tasks for studying discriminative behaviour,\nand pragmatics, grounded in vision: justification, and discriminative image captioning. \u2022 A new dataset (CUB-Justify) to evaluate justification\nsystems on fine-grained bird images with 5 captions for 3161 (image, target class, distractor class) triplets. \u2022 Our evaluations on CUB-Justify, and human evalua-\ntion on COCO show that our approach outperforms baseline approaches at inducing discrimination."}, {"heading": "2. Related Work", "text": "Pragmatics: The study of pragmatics \u2013 how context influences usage of language, stems from the foundational work of Grice [14] who analyzed how cooperative multiagent linguistic agents could model each others behavior to achieve a common objective. Consequently, a lot of pragmatics literature has studied higher-level behavior in agents including conversational implicature [6] and the Gricean maxims [37]. These works aim to derive pragmatic behavior given minimal assumptions on individual agents and typically use hand-tuned lexicons and rules. We are also interested in deriving pragmatic behavior, but our focus is on scaling context-sensitive behavior to computer vision tasks.\nOther works model ideas from pragmatics to learn language via games played online [39] or for human-robot collaboration [32]. In a similar spirit, here we are interested in applying ideas from pragmatics to build systems that can provide justifications (Sec. 4.1) and provide discriminative image captions (Sec. 4.2).\nMost relevant to our work is the recent work on deriving pragmatic behavior in abstract scenes made with clipart, by Andreas, and Klein [2]. Unlike their technique, our proposed approach does not require training a second listener model and supports more efficient inference (Sec. 3.3). More details are provided in Sec. 3.1. Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40]. Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41]. In a similar spirit, the two tasks we introduce here, justification, and discriminative image captioning, can be viewed as \u201cbeyond image captioning\u201d tasks. Sadovnik et al. [29] first studied a discriminative image description task, with the goal of distinguishing one image from a set of images. Their approach incorporates cues such as discriminability and saliency, and uses hand-designed rules for constructing sentences. In contrast, here we develop neural models for pragmatic inference to induce discriminative behavior. The reference game from [2] can also be seen as a discriminative\nimage captioning task on abstract scenes made from clipart, while we are interested in the domain of real images. The work on generating referring expressions by Mao et al. [24] generates discriminative captions which refer to particular objects in an image given context-aware supervision. Our work is different in the sense that we address an instance of pragmatic reasoning in the common case where contextdependent data is not available for training. Rationales: Several works have studied how machines can understand human rationales, including enriching classification by asking explanations from humans [9], and incorporating human rationales in active learning [7, 26]. In contrast, we focus on machines providing justifications to humans. This could potentially allow machines to teach new concepts to humans (machine teaching). Other recent work [15] looks at post-hoc explanations for classification decisions. Instead of explaining why a model thinks an image is a particular class, [15] describes why an image is of a class predicted by the classifier. Unlike this task, our justification task requires reasoning about explicit context from the distractor class. Further, we are not interested in providing rationalizations for classification decisions but explaining the differences in confusing concepts to humans. We show a comparison to [15] in the appendix, demonstrating the importance of context for justification. Beam Search with Modified Objectives: Beam search is an approximate, greedy technique for inference in sequential models. We perform beam search on a modified objective for our introspective speaker model to induce discrimination. This is similar in spirit to recent works on inducing diversity in beam search [35], and maximum mutual information inference for sequence-to-sequence models [20]."}, {"heading": "3. Approach", "text": "We now describe our approach for creating discriminative language that describes an image from a target class relative to its context for two tasks: justification, where the context consists of another class, and discriminative captioning, where the context consists of a semantically similar image. For simplicity, we first describe the formulation for justification, and then discuss a simple modification to apply it to discriminative captioning.\nIn the justification task (Fig. 1 top), we wish to produce a discriminative sentence s, comprised of a sequence of words {si}, based on a given image I of a target concept ct in the context of a distractor concept cd. Note that images of the distractor class are not provided to the algorithm. The produced discriminative sentence (justification) should capture aspects of the image that discriminate between the target, and the distractor concepts.\nWe first train a generic context-agnostic image captioning model (from here on referred to as speaker) using training data from Reed et al. [28] who collected captions de-\nscribing bird images on the CUB-200-2011 [38] dataset. We condition the model on ct in addition to the image. That is, we model p(s|I, ct). This not only helps produce better sentences (providing the model access to more information), but is also the cornerstone of our approach for discrimination (Sec. 3.2). Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc."}, {"heading": "3.1. Reasoning Speaker", "text": "To induce discrimination in the utterances from a language model, it is natural to consider using a generator, or speaker, which models p(s|I, ct) in conjunction with a listener function f(s, ct, cd) that scores how discriminative an utterance s is. The task of a pragmatic reasoning speaker RS, then, is to select utterances which are good sentences as per the generative model p, and are discriminative per f :\nRS(I, ct, cd)=arg max s \u03bbp(s|I, ct) + (1\u2212\u03bb)f(s, ct, cd) (1) where 0 \u2264 \u03bb \u2264 1 controls the tradeoff between linguistic adequacy of the sentence, and discriminability.\nA similar model of the reasoning speaker forms the core of the approach of [2], where p, and f are implemented using multi-layer perceptrons (MLPs), for contextual reasoning on abstract images. As noted in [2], selecting utterances from such a reasoning speaker poses several challenges. First, exact inference in this model over the exponentially large space of sentences is intractable. Second, in general one would not expect the discriminator function f to factorize across words, making joint optimization of the reasoning speaker objective difficult. Thus, Andreas, and Klein [2] adopt a sampling based strategy, where p is considered as the proposal distribution whose samples are ranked by a linear combination of p, and f (Eq. 1). Importantly, this distribution is over full sentences, hence the effectiveness of this formulation depends heavily on the distribution captured by p, since the search over the space of all strings is solely based on the speaker. This is inefficient, especially when there is a large mismatch in the statistics of the context-free (generative), and the unknown context-aware (discriminative) sentence distributions. We might need to draw a lot of samples from the proposal distribution p for us to find a good discriminative sentence in such cases."}, {"heading": "3.2. Introspective Speaker", "text": "We next explain our model for incorporating contextual behavior, based on a simple modification to the listener f (Eq. 1). Given the generator p, we construct a listener module that wants to discriminate between ct, and cd, using the following log-likelihood ratio:\nf(s, ct, cd) = log p(s|ct, I) p(s|cd, I) . (2)\nThis listener only depends on a generative model, p(s|c, I), for the two classes ct, and cd. We name it \u201cintrospector\u201d to emphasize that this step re-uses the generative model, and does not need to train an explicit listener model. Substituting the introspector into Eq. 1 induces the following introspective speaker model for discrimination:\n\u2206(I, ct, cd)\ufe38 \ufe37\ufe37 \ufe38 introspective speaker = arg max s \u03bb log p(s|ct, I)\ufe38 \ufe37\ufe37 \ufe38 speaker\n+ (1\u2212\u03bb) log p(s|ct, I) p(s|cd, I)\ufe38 \ufe37\ufe37 \ufe38\nintrospector\n, (3)\nwith \u03bb that trades-off the weight given to generation, and introspection (similar to Eq. 1). In general, we expect this approach to provide sensible results when ct, and cd are similar. That is, we expect humans to describe similar concepts in similar ways, hence p(s|ct, I) should not be too different from p(s|cd, I). Thus, the introspector is less likely to overpower the speaker in Eq. 3 in such cases (for a given \u03bb). Note that for sufficiently different concepts the speaker alone is likely to be sufficient for discrimination. That is, describing the concept in isolation is likely to be enough to discriminate against a different or unrelated concept.\nA careful inspection of the introspective speaker model reveals two desirable properties over previous work [2]. First, the introspector model does not need training, since it only depends on p, the original generative model. Thus, existing generative models can be readily re-used to produce discriminative outputs by conditioning on cd. We demonstrate empirical validation of this in Sec. 5. This would help scale this approach to scenarios where it is not known apriori which concepts need to be discriminated, in contrast to approaches which train a separate listener module. Second, it allows us to develop a unified, and efficient inference procedure for the introspective speaker (Eq. 3), which we describe next."}, {"heading": "3.3. Emitter-Suppressor (ES) Beam Search for RNNs", "text": "We now describe a search algorithm for implementing the maximization in Eq. 3, which we call emitter-suppressor (ES) beam search. We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].\nWe first factorize the posterior log-probability terms in the introspective speaker equation (Eq. 3) p(s|ct, I) =\u220fT \u03c4=1 p(s\u03c4 |s1:\u03c4\u22121, ct, I), denoting s1:T = {s\u03c4}T\u03c4=1 (s1:0 corresponds to a null string). T is the length of the sentence. We then combine terms from Eq. 3, yielding the following\nemitter-suppressor objective for the introspective speaker:\n\u2206(I, ct, cd) = arg max s T\u2211 \u03c4=1 log\nemitter\ufe37 \ufe38\ufe38 \ufe37 p(s\u03c4 |s1:\u03c4\u22121, ct, I)\np(s\u03c4 |s1:\u03c4\u22121, cd, I)1\u2212\u03bb\ufe38 \ufe37\ufe37 \ufe38 suppressor .\n(4) The emitter (numerator in Eq. 4) is the generative model conditioned on the target concept ct, deciding which token to select at a given timestep. The suppressor (the denominator in Eq. 4) is conditioned on the distractor concept cd, providing signals to the emitter on which tokens to avoid. This is intuitive: we want to emit words that match ct, but avoid emitting words that match cd. Overall, we expect this to result in a caption that matches ct, and not cd, and would thus be a discriminative caption of ct in context of cd.\nWe maximize the emitter-suppressor objective (Eq. 4) using beam search. Vanilla beam search, as typically used in language models, prunes the output space at every time-step keeping the top-B (usually incomplete) sentences with highest log-probabilities so far (speaker in Eq. 3). Instead, we run beam search to keep the top-B sentences with highest ES ratio in Eq. 4. Fig. 2 illustrates this ES beam search for a beam size of 1. Note that a suppressor with uniform beliefs yields outputs identical to vanilla beam search. Thus, ES beam search can be seen as a generalization of vanilla beam search.\nIt is important to consider how the trade-off parameter\n\u03bb affects the produced sentences. For \u03bb = 1, the model generates descriptions that ignore the context. At the other extreme, low \u03bb values are likely to make the produced sentences very different from any sentence in the training set (repeated words, ungrammatical sentences). As such, this approach is fundamentally different than training with sets of discriminative sentences provided by humans. It is not trivial to assume that there exist a wide enough range of \u03bb creating sentences that are both discriminative, and wellformed. However, our results (Sec. 5) indicate that such a range of \u03bb exists in practice."}, {"heading": "3.4. Discriminative Image Captioning", "text": "In the discriminative image captioning task, we are given two images It (target image), and Id (distractor image) between which we wish to discriminate, similar to the two classes for the justification task. We construct a speaker (or generator) for this task by training an image captioning model that generates a sentence conditioned on the image. Given this speaker, we construct an emitter-suppressor equation (as in Eq. 4):\n\u2206(It, Id) = arg max s T\u2211 \u03c4=1 log\nemitter\ufe37 \ufe38\ufe38 \ufe37 p(s\u03c4 |s1:\u03c4\u22121, It)\np(s\u03c4 |s1:\u03c4\u22121, Id)1\u2212\u03bb\ufe38 \ufe37\ufe37 \ufe38 suppressor\n. (5)\nWe re-use the mechanics of emitter-suppressor beam search from Sec. 3.3, conditioning the emitter on the target image It, and the suppressor on the distractor image Id."}, {"heading": "4. Experimental Setup", "text": "We first discuss the dataset, and speaker-training setup for the justification task. We then detail our new CUBJustify dataset for evaluation of the justification task. Finally, we discuss the experimental protocols for the discriminative captioning task."}, {"heading": "4.1. Justification", "text": "CUB dataset: The Caltech UCSD birds (CUB) dataset [38] contains 11788 images for 200 species of North American birds. Each image in the dataset has been annotated with 5 fine-grained captions by Reed et al. [28]. These captions mention various details about the bird (\u201cThis is a white spotted bird with a long pointed black beak.\u201d) without mentioning the name of the bird species. CUB-Justify dataset: We collect a new dataset (CUBJustify) with ground truth justifications for evaluating our model. We first select a subset of (approx.) 15 images from the test set of the CUB-200-2011 dataset [38] for each of the 200 classes to form a test set, and use the rest for speaker training. We then sample the target, and distractor classes from within a hyper-category created on the basis of the\nlast name in the folk names of the birds. For instance, a \u201crufous hummingbird\u201d, and a \u201cruby throated hummingbird\u201d both fall in the hyper-category \u201chummingbird\u201d. We induce 37 such hyper-categories on the CUB dataset. The largest single hypercategory is \u201cWarbler\u201d with 25 categories.\nWorkers were then shown an image of the \u201crufous hummingbird\u201d, for instance, and a set of 6 other images all belonging to the distractor class \u201cruby throated hummingbird\u201d from the curated test set (to help form the visual notion of the class). They were also shown a diagram of the morphology of birds indicating various specialized parts of the bird, such as tarsus, rump, wingbars etc. (similar to Reed et al. [28]). The instruction was to describe the target image such that it is not ambiguous with respect to the images from the distractor class. Some of the bird categories are best distinguished by their call, or their migration patterns, i.e., not all cues for distinction are visual. Thus, we drop the categories of birds from the original list of triplets which were labeled as too hard to distinguish by the workers. At the end of this process we are left with 3161 triplets with 5 captions each. We split this dataset into 1070 validation (for selecting the best value of \u03bb), and 2091 test examples respectively. More details on the interface can be found in the appendix. Speaker training: We implement a model similar to \u201cShow, Attend, and Tell\u201d from Xu et al. [40], modifying the original model to also provide the class as input, similar in spirit to [15]. Exact details of our model architecture are given in the appendix. We train the model on a modified training split of the CUB dataset described in the previous section. Recall that this just has context-agnostic captions from [28].\nTo evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28]. Our captioning model with both the image, and class as input reaches a validation score of 50.2 CIDEr-D, while the original image-only captioning model reaches an CIDEr-D of 49.1. The scores are in a simlar ballpark to [15], although our numbers are not exactly comparable, given we train on a different split of the CUB dataset. There is however a key difference: we condition the model on the ground truth class since we are eventually interested in discriminative justifications between concepts, while [15] conditions the model on the predicted class, since they are interested in explanation of a classification decision. Justification evaluation: We measure performance of the (context-aware) justification captions on the CUB-Justify test set using the CIDEr-D metric, similar to previous work on evaluating image captions of the CUB dataset [15]. CIDEr-D weighs n-grams by their inverse document frequencies (IDF), giving higher weights to sentences having \u201ccontent\u201d n-grams (\u201cred beak\u201d) than generic n-grams (\u201cthis\nbird\u201d). Further, CIDEr-D captures importance of an n-gram for an image. For instance, it emphasizes \u201cred beak\u201d over, say, \u201cblack belly\u201d if \u201dred beak\u201d is used more often in human justification. We also report the METEOR scores for completeness. More detailed discussion on metrics can be found in the appendix."}, {"heading": "4.2. Discriminative Image Captioning", "text": "Dataset: We are interested to test if inducing discrimination, and reasoning about context can help discriminate between pairs of very similar images from the COCO dataset. To construct a set of confusing image pairs, we follow two strategies. First, easy confusion: For each image in the validation (test) set, we find its nearest neighbor in the FC7 space of a pre-trained VGG-16 CNN [31], and repeat this process of neighbor finding for 1000 randomly chosen source images. Second, hard confusion: To further narrow down to a list of semantically similar confusing images, we then run the speaker model on the nearest neighbor images, and compute word-level overlap (intersection over union) of their generated sentences. Interestingly, the captions for the two images are the same for 539 out of the 5000 pairs of nearest neighbor images (determined through exact match). This reflects the issue of the output of image captioning models lacking diversity, and seeming templated [8, 36]. For the hard-confusion set, we pick the top 1000 image pairs with most similar sentences. Speaker Training and Evaluation: We train our generative speaker for use in emitter-suppressor beam search using the model from [36] implemented in the neuraltalk2 project (github.com/karpathy/neuraltalk2). We use the train/val/test splits from [18]. Our trained and finetuned speaker model achieves a performance of 91 CIDEr-D on the test set. As seen in Eq. 5, no category information is used for this task. We evaluate approaches for discriminative image captioning based on how often they help humans to select the correct image out of the pair of images."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Justification", "text": "Methods and Baselines: We evaluate the following models: 1. IS(\u03bb): Introspective speaker from Eq. 3; 2. IS(1): standard literal speaker, which generates a caption conditioned on the image and target class, but which ignores the distractor class; 3. semi-blind-IS(\u03bb): Introspective speaker in which the listener does not have access to the image, but the speaker does; 4. blind-IS(\u03bb): Introspective speaker without access to image, conditioned only on classes; 5. RS(\u03bb): Our implementation of the approach [2], but using our (more powerful) language model, and Eq. 3 with a listener that models p(s|ct)p(s|cd) (similar to semi-blind-IS(\u03bb)) for ranking samples (as opposed to a trained MLP [2], to keep things\ncomparable). All approaches use a beam of size 10 (or sample of size 10), unless mentioned otherwise, since we found it performs better than lower values and higher values are often slow in runtime.\nValidation performance: Fig. 3 shows the performance over the CUB-Justify validation set as a function of \u03bb, the hyperparameter controlling the tradeoff between the generator and the introspector in our models (Eq. 3). For the RS(\u03bb) baseline, \u03bb stands for the tradeoff between the log probability of the sentence and the score from the discriminator function for sample re-ranking. A few interesting observations emerge. First, both our IS(\u03bb) and semi-blind-IS(\u03bb) models outperform the baselines for the mid range of \u03bb values. IS(\u03bb) model does better overall, but semi-blind-IS(\u03bb) has a more stable performance over a wider range of \u03bb. This indicates that when conditioned on the image, the introspector has to be highly discriminative (low lambda values) to overcome the signals from the image, since discrimination is between classes. Second, as \u03bb is decreased from 1, most methods improve as the sentences become more discriminative, but then get worse again as \u03bb becomes too low. This is likely to happen because when \u03bb is too low, the model explores rare tokens and parts of the output space that have not been seen during training, leading to badly-formed sentences (Fig. 4). This effect is stronger for our models than for the RS(\u03bb) model, since the RS(\u03bb) model searches the output space over samples from the generator and only ranks using the joint reasoning speaker objective (Eq. 1). Interestingly, at the generative extreme of \u03bb = 1, the RS(\u03bb) approach, which samples from the generator, also performs better than other approaches, which use beam search to select high log-probability (context-agnostic) sentences. This indicates that in the absence of discriminative ground truth data, there might indeed be a discrepancy between optimizing for discrimination and optimizing for a high-likelihood\ncontext-agnostic sentence. We performed more comparisons with the RS(\u03bb) baseline, sweeping over {10, 50, 100} samples from the generator for listener reranking (Eq. 1). Our search found that using 100 samples, RS(\u03bb) gets comparable CIDEr-D scores (18.8) (but lower METEOR scores) than our semi-blindIS(\u03bb) approach with a beam size of 10. This suggests that our semi-blind-IS(\u03bb) approach is more computationally efficient at exploring the output space because our emittersuppressor beam search allows us to do joint greedy inference over speaker and introspector, leading to more meaningful local decisions.\nTest performance: Table. 1 details the performance of the above models on the test set of CUB-Justify, with each model using its best-performing \u03bb on the validation set (Fig. 3). Both introspective-speaker models strongly outperform the baselines, with semi-blind-IS(\u03bb) slightly outperforming the IS(\u03bb) model. This could potentially be due to the performance of semi-blind-IS(\u03bb) being less sensitive to the exact choice of \u03bb (from Fig. 3). Among the baselines, the best performing method is the blind-IS(\u03bb) model, presumably because this model does emitter-suppressor beam search, while the other two baseline approaches rely on sampling and regular beam search.\nQualitative Results: We next showcase some qualitative results that demonstrate aspects of pragmatics, and context dependence captured by our best-performing semi-blindIS(\u03bb) model.\nFirst, Fig. 4 demonstrates how sentences uttered by the introspective speaker change with \u03bb. At \u03bb = 1 the sentence describes the image well, but is oblivious of the context (distractor class). The sentence \u201cA small sized bird has a very long and pointed bill.\u201d is discriminative of hummingbirds against other birds, but not among hummingbirds (many of which tend to have long beaks/bills). At \u03bb = 0.7, and \u03bb = 0.5, the model captures discriminative features such as the \u201cred neck\u201d, \u201cwhite belly\u201d, and \u201cred throat\u201d. Interestingly, at \u03bb = 0.7 the model avoids saying \u201clong beak\u201d, a feature shared by both birds.\nFig. 5 demonstrates the second, and most important aspect of pragmatic reasoning, that selected utterances change\nbased on the context. A limitation of our approach is that, since the model never sees discriminative training data, and discrimination is induced at inference, in some cases, it produces word repetations (\u201cgreen green green\u201d) when encouraged be discriminative.\nFinally, Fig. 6 illustrates why visual reasoning is important for the justification task. Fine-grained species often have complicated intra-class variances which a blind approach to justification would ignore. Thus, a good justification approach needs to be grounded in the image signal to pick the discriminative cues appropriate for the given instance."}, {"heading": "5.2. Discriminative Image Captioning", "text": "As explained in Sec. 4.2 we create two sets of semantically similar target, and distractor images: easy confusion based on FC7 features alone, and hard confusion based on both FC7, and sentences generated from the speaker (image captioning model). We are interested in understanding if emitter-suppressor inference helps identify the target image better than the generative speaker baseline. Thus the two approaches are speaker (S) (baseline), and introspective speaker (IS) (our approach). We use \u03bb = 0.3 based on\nour results on the CUB dataset. We run both approaches at a beam size of 2 which is typically best for image captioning on COCO [17]. Human Studies: We setup a two annotation forced choice (2AFC) human study where we show a caption to raters with a prompt to \u201cpick an image that the sentence is more likely to be describing.\u201d. A discriminative image captioning method is considered better if it enables a human to identify the target image more often. Each target distractor image\npair is tested against both the speaker (S) as well as the introspective speaker (IS) captions. We evaluate by checking the fraction of times a method caused the target image to be picked by a human. Results of the study are summarized in Table. 2. We find that our approach outperforms the baseline speaker on the easy confusion as well as the hard confusion splits. However, the gains from our approach are larger on the hard confusion split, which is intuitive. Qualitative Results: The qualitative results from our COCO experiments are shown in Fig. 7. The target image, when successfully identified, is shown with a green border. We show examples where our model identifies the target image better in the first two rows, and some failure cases in the third row. Notice how the model is able to modify its utterances to account for context, and pragmatics, when going from \u03bb = 1 (speaker) to \u03bb = 0.3 (introspective speaker). Note that the sentences typically respect grammatical constructs despite being forced to be discriminative."}, {"heading": "6. Discussion", "text": "There are numerous interesting directions to explore in the direction of generating context-aware captions from context-agnostic supervision. These include describing the absence of concepts (\u201cThis is a dog, not a cat, because it lacks whiskers\u201d), and learning to induce comparative lan-\nguage (\u201cThe giraffe is taller than the horse.\u201d). Beyond pragmatics, the justification task also has interesting relations to human learning. Indeed, we all experience that we learn better when someone takes time out to justify or explain their point of view. One can imagine such justifications being helpful for \u201cmachine teaching\u201d, where a teacher (machine) can provide justifications to a human learner explaining the rationale for an image belonging to a particular finegrained category as opposed to a different, possibly mistaken, or confusing fine-grained category.\nOur approach for inducing context-aware captions from context-agnostic supervision does have some natural limitations. For instance, if two distinct concepts are very similar, human-generated context-free descriptions may be identical, and our model (as well as baselines) would not be able to extract any discriminative signal. Indeed, it seems hard to account for such situations without access to context-aware ground truth.\nFinally, our introspection approach has interesting analogies with generative adversarial networks. It might be possible to formulate other problems where we derive highorder reasoning (such as pragmatics) by reusing the sampling distribution from language models: that is, if we can write down the higher order reasoning as a function that has terms from the generative model, we can reuse similar ideas to this work. Indeed, our inference objective can also be formulated for training, to adjust the speaker\u2019s weights to account for the introspector. However, initial experiments on this did not yeild major performance improvements."}, {"heading": "7. Conclusion", "text": "We introduce a novel technique for deriving pragmatic language from recurrent neural network language models, namely, an image-captioning model that takes into account the context of a distractor class or a distractor image. Our technique can be used at inference time to better discriminate between concepts, without having seen discriminative training data. We study two tasks in the vision, and language domain which require pragmatic reasoning: justification \u2013 explaining why an image belongs to one category as opposed to another, and discriminative image captioning \u2013 describing an image so that one can distinguish it from a closely related image. Our experiments demonstrate the strength of our method over generative baselines, as well as\nadaptations of previous work to our setting. We will make the code, and datasets available online."}, {"heading": "Acknowledgements", "text": "We thank David Rolnick for lending his invaluable expertise \u2013 both as a bird-watcher, guiding various design choices for the justification task, as well as for brainstorming about the technical details of this work. We thank Tom Duerig for his support, and guidance in shaping this project. We thank Vahid Kazemi for his help with the interface for CUB-Justify data collection, Bharadwaja Ghali for helping collect CUB-Justify, and Ashwin Kalyan for sharing a trained checkpoint for the discriminative image captioning experiments. We also thank Stefan Lee, Andreas Veit, Chris Shallue, and numerous members of the Machine Perception team at Google Research, and CVMLP Labs, Virginia Tech for their valuable inputs at various stages of this work. This work was funded in part by an NSF CAREER award, ONR YIP award, Sloan Fellowship, ARO YIP award, Allen Distinguished Investigrator award from the Paul G. Allen Family Foundation, Google Faculty Research Award, Amazon Academic Research Award to DP.\nAppendix We organize the appendix as follows: \u2022 Sec. 1: Analysis of performance as we consider unre-\nlated images as distractors. \u2022 Sec. 2: Generating visual explanations [15] adapted to\nthe justification task. \u2022 Sec. 3: Architectural changes to the \u201cShow, Attend,\nand Tell\u201d image captioning model [40] for justification. \u2022 Sec. 4: Optimization details for justification speaker\nmodel. \u2022 Sec. 5: Choice of metrics for evaluating justification. \u2022 Sec. 6: CUB-Justify data collection details. \u2022 Sec. 7: Analysis of the RS(\u03bb) baseline in more detail."}, {"heading": "1. COCO Qualitative Results", "text": "COCO Qualitative Examples: Fig. 8 shows more qualitative results on discriminative image captioning on the hard confusion split of the COCO dataset. Notice how our introspective speaker captions (denoted by IS), which model the context (distractor image) explicitly are often more discriminative, helping identify the target image more clearly than the baseline speaker approach (denoted by S). For example in the second row, our IS model generates the caption \u201ca delta passenger jet flying through a clear blue sky\u201d, which is a more discriminative (and accurate) caption than the baseline caption \u201ca large passenger jet flying through a blue sky\u201d, which applies to both the target and distractor images.\nEffect of increasing distance: We illustrate how the quality of the discriminative captions from the introspective speaker (IS) approach varies as the distractor image becomes less relevant to the target image (Fig. 9). For the target image on the left, we show the 1-nearest neighbor (which has a very similar caption to the target image), the 10th-nearest neighbor and a randomly selected distractor image. When we pick a random image to be the distractor, the generated discriminatve captions become less comprehensible, losing relevance as well as grammatical structure. This is consistent with our understanding of the introspective speaker (IS) formulation from Sec. 3.2: modeling the context explicitly during inference helps discrimination when the context is relevant. When the context is not relevant, as with the randomly picked images, the original speaker model (S) is likely sufficient for discrimination."}, {"heading": "2. Comparison to previous work on Generating", "text": "Visual Explanations [15]\nHendricks et al. [15] propose a method to explain classification decisions to an end user by providing post-hoc rationalizations. Given a prediction from a classifier, this work generates a caption conditioned on the predicted class, and the original image. While Hendricks et al. aim to provide a rationale for a classification, we focus on a related but different problem of concept justification. Namely, we want to explain why an image contains a target class as opposed to a specific distractor class, while Hendircks et al. want to explain why a classifier thought an image contains a particular class. Thus, unlike the visual explanation task, it is intuitive that the justification task requires explicit reasoning about context. We verify this hypothesis, by first adapting the work of [15] to our justification task, using it as a speaker, and then augmenting the speaker with our approach to construct an intropsective speakerm which accounts for context. Interestingly, we find that our introspective speaker approach helps improve the performance of generating visual explanations [15] on justification.\nThe approach of Hendricks et al. [15] differs from our setup in two important ways. Firstly, uses a stronger CNN, namely the fine-grained compact-bilinear pooling CNN [12] which provides state-of-the-art performance on the CUB dataset. Secondly, to make the explanations more grounded in the class information, they also add a constraint to induce captions which are more specific to the class. This is achieved by using a policy gradient on a reward function that models p(c|s) for a given sentence s and class c. Thus, in some sense the approach encourages the model to produce sentences that are highly discriminative of a given class against all other classes, as opposed to a particular distractor class that we are interested in for justification. Finally, the policy gradient is used in conjunction with standard maximum likelihood training to train the explanation\nmodel. At inference, the explanation model is run by conditioning the caption generation on the predicted class.\nWe modify the inference setup of [15] slightly to condition the caption generation on the target class for justification, as opposed to the predicted class for explanation. We call this the vis-exp approach. We then apply the emittersuppressor beam search (at a beam size of 1, to be consistent with [15]) to account for context, giving us an introspective visual explanation model (vis-exp-IS). Given the stronger image features and a more complicated training procedure involving policy gradients (hard to implement and tune in practice), the vis-exp approach achieves a strong CIDEr-D score of 20.36 with a standard error of 0.16 on our CUBJustify test set. Note that this CUB-Justify test set is a strict subset of the test set from [15]. These results are better than those achieved with our semi-blind-IS(\u03bb) CUB model, which is based on regular image features from VGG-16 implemented in the \u201cShow, Attend and Tell\u201d framework and uses standard log-likelihood training (Table. 1).\nHowever, as mentioned before, the approach of [15], similar to a baseline speaker S, cannot explicitly model context from a specific distractor class at inference. That is, while the approach reasons (through its training procedure) that given an image of a hummingbird, one should talk about its long beak (a discriminating feature for a hummingbird against all other birds), it cannot reason about a specific distractor class presented at inference. If the distractor class is another hummingbird with a long beak, we would want to avoid talking about the long beak in our justification. On the other hand, if the distractor class were a hummingbird with a shorter beak and there do exist such hummingbirds, then the long beak would be an important feature to mention in a justification. Clearly, this is non-trivial to realize without explicitly modeling context. Hence, intuitively, one would expect that incorporating context from the distractor class should help the justification task.\nAs explained previously, we implement our emittersuppressor inference (Eq. 3), on top of the vis-exp approach, yielding an vis-exp-IS approach. We sweep over the values of \u03bb on validation and find that the best performance is achieved at \u03bb = 0.9. Plugging this value and evaluating on test, our vis-exp-IS approach achieves a CIDEr-D score of 21.52 with a standard error of 0.17 (Table. 3). This\nis an improvement of 1.16 CIDEr-D. Our gains over vis-exp are lower than the gains on the IS(1) approach (reported in Table. 1), presumably because the vis-exp approach already captures a lot of the context-independent discriminative signals (e.g., long beak for a hummingbird), due to policy gradient training. Overall though, these results provide further evidence that our emitter-suppressor inference scheme can be adapted to a variety of context-agnostic captioning models, to effectively induce context awareness during inference."}, {"heading": "3. Architectures for Show, Attend, and Tell", "text": "with Class Conditioning for CUB\nWe explain some minor modifications to the \u201cShow, Attend and Tell\u201d [40] image captioning model to condition it on the class label in addition to the image, for our experiments on CUB. Note that the explanation in this section is only for CUB \u2013 our COCO models are trained using the neuraltalk2 package1 which implements the \u201cShow and Tell\u201d captioning model from Vinyals et al. [36]. Our changes can be understood as three simple modifications aimed to use class information in the model. We first embed the class label (1 out of 200 classes for CUB) into a\n1https://github.com/karpathy/neuraltalk2\ncontinuous vector k \u2208 RD, D = 512. The three changes then, on top of the Show, Attend, and Tell model [40] are as follows: \u2022 Changes to initial LSTM state: The original Show,\nAttend, and Tell model uses image annotation vectors ai (i indexes spatial location), which are the outputs from a convolutional feature map to compute the initial cell and hidden states of the long-short term memory (LSTM) (c0, h0). The image annotation vector is averaged across spatial locations a\u0304 = 1L \u2211L i=1 ai and used to compute the initial state as follows:\nc0 = finit,c(a\u0304)\nh0 = finit,h(a\u0304)\nWe modify this to also use the class embedding k to predict the initial state of the LSTM, by concatenating it with the averaged anntoation vector (a\u0304):\nc0 = finit,c([a\u0304; k])\nh0 = finit,h([a\u0304; k])\n\u2022 Changes to the LSTM recurrence: \u201cShow, Attend and Tell\u201d computes a scalar attention \u03b1ti at each location of the feature map and uses it to compute a context\nvector at every timestep z\u0302t = \u03c6({\u03b1ti,ai}) by attending on the image annotation ai. It also embeds an input word yt using an embedding matrix E and uses the previous hidden state ht to compute the following LSTM recurrence at every timestep, producing outputs it (input gate), ft (forget gate), ot (output gate), gt (input) (Eqn. 1, 2, 3 from [40]):\nit ft ot gt\n =  \u03c3 \u03c3 \u03c3\ntanh\nT Eytht\u22121\nz\u0302t\n (6)\nct = ft ct\u22121 + it gt (7) ht = ot tanh(ct) (8)\nWe use the class embeddings k in addition to the context vector z\u0302t in Eqn. 1:\nit ft ot gt\n =  \u03c3 \u03c3 \u03c3\ntanh T \u2032  Eyt ht\u22121 z\u0302t k  (9) The remaining equations for the LSTM recurrence remain the same (Eqn. 2, 3). \u2022 Adding class information to the deep output layer:\n\u201cShow, Attend and Tell\u201d uses a deep output layer [27] to compute the output word distribution at every timestep, incorporating signals from the LSTM hidden state ht, context vector z\u0302t and the input word yt:\np(yt) \u221d exp(Lo(Eyt + Lhht + Lzzt))\nHere Lh, Lz are matrices used to project ht and zt to the dimensions of the word embeddings Eyt and Lo is the output layer which produces an output of the size of the vocabulary. Similar to the previous two adaptations, we use the class embedding k in addition to the context vector z\u0302t to predict the output at every timestep:\np(yt) \u221d exp(Lo(Eyt + Lhht + Lzzt + Lkk))\n\u2022 Blind models: For implementing our class-only blindIS(\u03bb) model, we need to train a model that only uses the class to produce a sentence. For this, we drop the attention component from the model, which is equivalent to setting z\u0302t and \u02c6\u0304a to zero for all our equations above and run the model using the class embedding k."}, {"heading": "4. Optimization Details", "text": "Our CUB captioning network is trained using Rmsprop [33] with a batch size of 32 and a learning rate of\n0.001. We decayed the learning rate on every 5 epochs of cycling through the training data. Our word embedding E embeds words into a 512 dimensional vector and we set LSTM hidden and cell state (h0, c0) sizes to 1800, similar to the \u201cShow, Attend, and Tell\u201d model on COCO. The rest of our design choices closely mirror the original work of [40], based on their implementation available at https:// github.com/kelvinxu/arctic-captions. We will make our Tensorflow implementation of \u201cShow, Attend, and Tell\u201d publicly available."}, {"heading": "5. Metrics for Justification", "text": "In this section, we expand more on our discussion on the choice of metrics for evaluating justification (Sec. 4.1). In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1]. The SPICE metric uses a dependency parser to extract a scene graph representation for the candidate and reference sentences and computes an F-measure between the scene graph representations. Given that the metric uses a dependency parser as an intermediate step, it is unclear how well it would scale to our justification task: some of the sentences from our model might be good justifications but may not be exactly grammatical. This is because our discriminative justifications emerge as a result of a tradeoff between high-likelihood sentences and discrimination (Eq. 3). Note that this tradeoff is inherent since we don\u2019t have ground truth (well-formed) discriminative training data. Thus SPICE can be a problematic metric to use in our context. However, for the sake of completeness, we report SPICE numbers on validation, giving each approach access to its best \u03bb value, in Table. 4.\nAlthough we outperform the baselines using the SPICE metric, in some corner cases we also found the SPICE metric scores to be slightly un-interpretable. For example, for the candidate sentence \u201cthis bird has a speckled belly and breast with a short pointy bill.\u201d, and reference sentences \u201cThis bird has a yellow eyebrow and grey auriculars\u201d, \u201cThis is a bird with yellow supercilium and white throat\u201d, the SPICE scores were higher than one would expect (0.30). For reference, an intuitively more related sentence \u201cthis is a grey and yellow bird with a yellow eyebrow.\u201d obtains a\nlower SPICE score of 0.28 for the same reference sentences. Further investigation revealed that the relation F-measure, which roughly measures if the two sentences encode the same relations, had a high score in these corner cases. We hypothesize that this inconcsistency in scores might be because SPICE uses soft similarity from WordNet for computing the F-measure, which might not be calibrated for this fine-grained domain, with specialized words such as supercilium, auriculars etc. As a result of these observations, we decided not to perform key evaluations with the SPICE metric."}, {"heading": "6. CUB-Justify Dataset Interface", "text": "We provide more details on the collection of the CUBJustify dataset (Sec. 4.1). We presented a target image from a selected target class to the workers along with a set of six distractor images, all belonging to one other distractor class. The distractor images were chosen at random from the validation, and test split of the CUB dataset we created for justification. Non-expert workers are unlikely to given have an explicit visual model of a given ditractor category, say Indigo Bunting. Thus the distractor images were shown to entail the concept of the distractor class for justification. As explained in Sec. 4.1 the choice of the distractor classes is made based on the hierarchy we induce using the folk names of the birds. Given the target class, and the distractor class images, workers were asked to describe the target image in a manner that the sentence is not confusing with respect to the distractor images. Further, the workers were instructed that someone who reads the sentence should be able to recognize the target image, distinguishing it from the set of distractor images. In order to get workers to pay attention to all the images (and the intra-class invariances), they were not told explicitly that the distractor images all belonged to one other, unique, distractor class. For helping identify minute difference between images of birds, as well as enabling workers to write more accurate captions, we also showed them a diagram of the morphology of a bird (Fig. 10). We also showed them a list of some other parts with examples not shown in the diagram, such as eyeline, rump, eyering, etc. The list of these words as well as examples, and the morphology diagram were picked based on consultation with an ornithology hobbyist. The workers were also explicitly instructed to describe only the target image, in an accurate manner, mentioning details that are present in the target image, as opposed to providing jusitifications that talk about features that are absent.\nThe initial rounds of data collection revealed some interesting corner cases that caused some ambiguity. For example, some workers were confused whether a part of the bird should be called gray or white, because it could appear gray either because the part was white, and in shadow, or the part was actually gray. After these initial rounds of feedback, we\nproceeded to collect the entire dataset."}, {"heading": "7. Reasoning Speaker Performance Analysis", "text": "In this section, we provide more details on how the performance of our adaptation of Andreas, and Klein [2], namely the RS(\u03bb) approach varies as we sweep over the number of samples we draw from the model for \u03bb = 0.3, \u03bb = 0.5, and \u03bb = 0.7. We note that for \u03bb = 0.5, the RS(\u03bb) approach approaches the best performance from our IS(\u03bb) approach as we draw 100 samples from the model (Fig. 11). Interestingly, our IS(\u03bb) model is only evaluated with a beam size of 10. Thus our model is able to perform more efficient search for discriminative sentences than a sampling, and reranking based approach like RS(\u03bb). It is easy to note that, in case we were willing to spend time to enumerate over all exponentially-many sentences, we would find the optimal solution in worst case exponential time \u2013 most approximate inference techniques in such a setting offer a time vs. optimality tradeoff. Our approach seems to fit this tradeoff better than the RS(\u03bb) approach based on this empirical evidence."}], "references": [{"title": "Spice: Semantic propositional image caption evaluation", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "CoRR, abs/1607.08822,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning about pragmatics with neural listeners and speakers", "author": ["J. Andreas", "D. Klein"], "venue": "EMNLP,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "pages 65\u201372,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "A computational account of comparative implicatures for a spoken dialogue agent", "author": ["L. Benotti", "D.R. Traum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Simultaneous active learning of classifiers & attributes via relative feedback", "author": ["A. Biswas", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R.B. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "CoRR, abs/1505.04467,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CoRR, abs/1411.4952,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Compact bilinear pooling", "author": ["Y. Gao", "O. Beijbom", "N. Zhang", "T. Darrell"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Logic and conversation", "author": ["H. Grice"], "venue": "Syntax and Semantics. Academic Press,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1975}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9:1735\u20131780,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "EMNLP,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "W.B. Dolan"], "venue": "HLT-NAACL,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S.J. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "The harpy speech understanding system", "author": ["B.T. Lowerre", "D.R. Reddy"], "venue": "Trends in Speech Recognition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1980}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Generation and comprehension of unambiguous object", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A.L. Yuille", "K. Murphy"], "venue": "descriptions. 2016", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Modeling context between objects for referring expression understanding", "author": ["V.K. Nagaraja", "V.I. Morariu", "L.S. Davis"], "venue": "ECCV,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Attributes for classifier feedback", "author": ["A. Parkash", "D. Parikh"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Learning deep representations of fine-grained visual descriptions", "author": ["S. Reed", "Z. Akata", "B. Schiele", "H. Lee"], "venue": "IEEE Computer Vision and Pattern Recognition,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Image description with a goal: Building efficient discriminating expressions for images", "author": ["A. Sadovnik", "Y.-I. Chiu", "N. Snavely", "S. Edelman", "T. Chen"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Not everybody\u2019s special: Using neighbors in referring expressions with uncertain attributes", "author": ["A. Sadovnik", "A.C. Gallagher", "T. Chen"], "venue": "CVPR Workshops,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R.A. Knepper", "A. Li", "D. Rus", "N. Roy"], "venue": "Robotics: Science and Systems,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Diverse beam search: Decoding diverse solutions from neural sequence models", "author": ["A.K. Vijayakumar", "M. Cogswell", "R.R. Selvaraju", "Q. Sun", "S. Lee", "D.J. Crandall", "D. Batra"], "venue": "CoRR, abs/1610.02424,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Emergence of gricean maxims from multi-agent decision theory", "author": ["A. Vogel", "M. Bodoia", "C. Potts", "D. Jurafsky"], "venue": "HLT- NAACL,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning language games through interaction", "author": ["S.I. Wang", "P. Liang", "C.D. Manning"], "venue": "CoRR, abs/1606.02447,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "J.R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "ICCV,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "At a high level, we are motivated by similar considerations as recent work by Andreas, and Klein [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 19, "endOffset": 22}, {"referenceID": 39, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 64, "endOffset": 68}, {"referenceID": 34, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 185, "endOffset": 189}, {"referenceID": 18, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "Pragmatics: The study of pragmatics \u2013 how context influences usage of language, stems from the foundational work of Grice [14] who analyzed how cooperative multiagent linguistic agents could model each others behavior to achieve a common objective.", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "Consequently, a lot of pragmatics literature has studied higher-level behavior in agents including conversational implicature [6] and the Gricean maxims [37].", "startOffset": 126, "endOffset": 129}, {"referenceID": 33, "context": "Consequently, a lot of pragmatics literature has studied higher-level behavior in agents including conversational implicature [6] and the Gricean maxims [37].", "startOffset": 153, "endOffset": 157}, {"referenceID": 35, "context": "Other works model ideas from pragmatics to learn language via games played online [39] or for human-robot collaboration [32].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Other works model ideas from pragmatics to learn language via games played online [39] or for human-robot collaboration [32].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Most relevant to our work is the recent work on deriving pragmatic behavior in abstract scenes made with clipart, by Andreas, and Klein [2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 9, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 32, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 36, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 2, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 11, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 20, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 38, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 16, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 21, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 22, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 27, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 37, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "[29] first studied a discriminative image description task, with the goal of distinguishing one image from a set of images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The reference game from [2] can also be seen as a discriminative", "startOffset": 24, "endOffset": 27}, {"referenceID": 21, "context": "[24] generates discriminative captions which refer to particular objects in an image given context-aware supervision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Rationales: Several works have studied how machines can understand human rationales, including enriching classification by asking explanations from humans [9], and incorporating human rationales in active learning [7, 26].", "startOffset": 214, "endOffset": 221}, {"referenceID": 23, "context": "Rationales: Several works have studied how machines can understand human rationales, including enriching classification by asking explanations from humans [9], and incorporating human rationales in active learning [7, 26].", "startOffset": 214, "endOffset": 221}, {"referenceID": 13, "context": "Other recent work [15] looks at post-hoc explanations for classification decisions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "Instead of explaining why a model thinks an image is a particular class, [15] describes why an image is of a class predicted by the classifier.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "We show a comparison to [15] in the appendix, demonstrating the importance of context for justification.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "This is similar in spirit to recent works on inducing diversity in beam search [35], and maximum mutual information inference for sequence-to-sequence models [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "This is similar in spirit to recent works on inducing diversity in beam search [35], and maximum mutual information inference for sequence-to-sequence models [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "[28] who collected captions describing bird images on the CUB-200-2011 [38] dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[28] who collected captions describing bird images on the CUB-200-2011 [38] dataset.", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc.", "startOffset": 163, "endOffset": 171}, {"referenceID": 36, "context": "Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc.", "startOffset": 163, "endOffset": 171}, {"referenceID": 3, "context": "Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc.", "startOffset": 193, "endOffset": 196}, {"referenceID": 1, "context": "A similar model of the reasoning speaker forms the core of the approach of [2], where p, and f are implemented using multi-layer perceptrons (MLPs), for contextual reasoning on abstract images.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "As noted in [2], selecting utterances from such a reasoning speaker poses several challenges.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "Thus, Andreas, and Klein [2] adopt a sampling based strategy, where p is considered as the proposal distribution whose samples are ranked by a linear combination of p, and f (Eq.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "A careful inspection of the introspective speaker model reveals two desirable properties over previous work [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].", "startOffset": 140, "endOffset": 148}, {"referenceID": 31, "context": "We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].", "startOffset": 140, "endOffset": 148}, {"referenceID": 34, "context": "CUB dataset: The Caltech UCSD birds (CUB) dataset [38] contains 11788 images for 200 species of North American birds.", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": ") 15 images from the test set of the CUB-200-2011 dataset [38] for each of the 200 classes to form a test set, and use the rest for speaker training.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "[28]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[40], modifying the original model to also provide the class as input, similar in spirit to [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[40], modifying the original model to also provide the class as input, similar in spirit to [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "Recall that this just has context-agnostic captions from [28].", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 134, "endOffset": 142}, {"referenceID": 32, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 134, "endOffset": 142}, {"referenceID": 25, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 190, "endOffset": 194}, {"referenceID": 13, "context": "The scores are in a simlar ballpark to [15], although our numbers are not exactly comparable, given we train on a different split of the CUB dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "There is however a key difference: we condition the model on the ground truth class since we are eventually interested in discriminative justifications between concepts, while [15] conditions the model on the predicted class, since they are interested in explanation of a classification decision.", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Justification evaluation: We measure performance of the (context-aware) justification captions on the CUB-Justify test set using the CIDEr-D metric, similar to previous work on evaluating image captions of the CUB dataset [15].", "startOffset": 222, "endOffset": 226}, {"referenceID": 28, "context": "First, easy confusion: For each image in the validation (test) set, we find its nearest neighbor in the FC7 space of a pre-trained VGG-16 CNN [31], and repeat this process of neighbor finding for 1000 randomly chosen source images.", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "This reflects the issue of the output of image captioning models lacking diversity, and seeming templated [8, 36].", "startOffset": 106, "endOffset": 113}, {"referenceID": 32, "context": "This reflects the issue of the output of image captioning models lacking diversity, and seeming templated [8, 36].", "startOffset": 106, "endOffset": 113}, {"referenceID": 32, "context": "Speaker Training and Evaluation: We train our generative speaker for use in emitter-suppressor beam search using the model from [36] implemented in the neuraltalk2 project (github.", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "We use the train/val/test splits from [18].", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "RS(\u03bb): Our implementation of the approach [2], but using our (more powerful) language model, and Eq.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "3 with a listener that models p(s|ct) p(s|cd) (similar to semi-blind-IS(\u03bb)) for ranking samples (as opposed to a trained MLP [2], to keep things 0 4 8 12 16 20", "startOffset": 125, "endOffset": 128}, {"referenceID": 13, "context": "2: Generating visual explanations [15] adapted to the justification task.", "startOffset": 34, "endOffset": 38}, {"referenceID": 36, "context": "3: Architectural changes to the \u201cShow, Attend, and Tell\u201d image captioning model [40] for justification.", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Comparison to previous work on Generating Visual Explanations [15]", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "[15] propose a method to explain classification decisions to an end user by providing post-hoc rationalizations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We verify this hypothesis, by first adapting the work of [15] to our justification task, using it as a speaker, and then augmenting the speaker with our approach to construct an intropsective speakerm which accounts for context.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Interestingly, we find that our introspective speaker approach helps improve the performance of generating visual explanations [15] on justification.", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "[15] differs from our setup in two important ways.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Firstly, uses a stronger CNN, namely the fine-grained compact-bilinear pooling CNN [12] which provides state-of-the-art performance on the CUB dataset.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "vis-exp [15] 20.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Table 3: CUB-Justify test results: We compare vis-exp [15] and our emitter-suppressor beam search implemented on top of vis-exp, namely vis-exp-IS.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "We modify the inference setup of [15] slightly to condition the caption generation on the target class for justification, as opposed to the predicted class for explanation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "We then apply the emittersuppressor beam search (at a beam size of 1, to be consistent with [15]) to account for context, giving us an introspective visual explanation model (vis-exp-IS).", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "Note that this CUB-Justify test set is a strict subset of the test set from [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "However, as mentioned before, the approach of [15], similar to a baseline speaker S, cannot explicitly model context from a specific distractor class at inference.", "startOffset": 46, "endOffset": 50}, {"referenceID": 36, "context": "We explain some minor modifications to the \u201cShow, Attend and Tell\u201d [40] image captioning model to condition it on the class label in addition to the image, for our experiments on CUB.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The three changes then, on top of the Show, Attend, and Tell model [40] are as follows:", "startOffset": 67, "endOffset": 71}, {"referenceID": 36, "context": "1, 2, 3 from [40]): \uf8ec\uf8ec\uf8ed it ft ot gt \uf8f7\uf8f7\uf8f8 = \uf8ec\uf8ec\uf8ed \u03c3 \u03c3", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "\u2022 Adding class information to the deep output layer: \u201cShow, Attend and Tell\u201d uses a deep output layer [27] to compute the output word distribution at every timestep, incorporating signals from the LSTM hidden state ht, context vector \u1e91t and the input word yt:", "startOffset": 102, "endOffset": 106}, {"referenceID": 36, "context": "The rest of our design choices closely mirror the original work of [40], based on their implementation available at https:// github.", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In this section, we provide more details on how the performance of our adaptation of Andreas, and Klein [2], namely the RS(\u03bb) approach varies as we sweep over the number of samples we draw from the model for \u03bb = 0.", "startOffset": 104, "endOffset": 107}], "year": 2017, "abstractText": "We introduce a technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of \u201csiamese cat\u201d and \u201ctiger cat\u201d, our system generates language that describes the \u201csiamese cat\u201d in a way that distinguishes it from \u201ctiger cat\u201d. We start with a generic language model that is context-agnostic and add a listener to discriminate between closely-related concepts. Our approach offers two key advantages over previous work: 1) our listener does not need separate training, and 2) allows joint inference to decode sentences that satisfy both the speaker and listener \u2013 yielding an introspective speaker. We first apply our introspective speaker to a justification task, i.e. to describe why an image contains a particular fine-grained category as opposed to another closely related category in the CUB-2002011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one out of two semantically similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination.", "creator": "LaTeX with hyperref package"}}}