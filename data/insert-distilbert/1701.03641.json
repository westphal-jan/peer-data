{"id": "1701.03641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Symbolic Regression Algorithms with Built-in Linear Regression", "abstract": "recently, several algorithms for symbolic regression ( sr ) emerged tools which normally employ a form of multiple linear regression ( as lr ) to produce generalized linear models. the use of lr allows in the algorithms to create models with relatively easy small error right from inside the beginning of the iteration search ; such algorithms are erroneously thus claimed to be ( sometimes adjusted by orders of magnitude ) faster than corresponding sr algorithms based on vanilla genetic programming. however, a systematic comparison of these algorithms on combining a common set of primary problems is still missing. in this paper we conceptually and experimentally compare several representatives of such traditional algorithms ( gptips, ffx, and efs ). they are applied as off - the - world shelf, ready - to - use techniques, mostly using their default reference settings. initially the methods are compared on several synthetic and real - world sr / benchmark problems. their performance is also related to the performance of three conventional machine learning algorithms - - - projective multiple regression, random forests and support vector regression.", "histories": [["v1", "Fri, 13 Jan 2017 12:23:10 GMT  (188kb)", "http://arxiv.org/abs/1701.03641v1", "Submitted to Journal of Heuristics"], ["v2", "Thu, 9 Mar 2017 10:24:16 GMT  (0kb,I)", "http://arxiv.org/abs/1701.03641v2", "Withdrawn due to wrong journal data"], ["v3", "Fri, 10 Mar 2017 11:14:17 GMT  (188kb)", "http://arxiv.org/abs/1701.03641v3", null]], "COMMENTS": "Submitted to Journal of Heuristics", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jan \\v{z}egklitz", "petr po\\v{s}\\'ik"], "accepted": false, "id": "1701.03641"}, "pdf": {"name": "1701.03641.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["zegkljan@fel.cvut.cz", "petr.posik@fel.cvut.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n03 64\n1v 1\n[ cs\n.L G\n] 1\n3 Ja\nn 20\nKeywords symbolic regression \u00b7 genetic programming \u00b7 linear regression \u00b7 comparative study"}, {"heading": "1 Introduction", "text": "Symbolic regression (SR) is an inductive learning task with the goal to find a model in the form of a (preferably simple) symbolic mathematical expression\nJ. Z\u030cegklitz Czech Technical University in Prague, Czech Institute of Informatics, Robotics and Cybernetics, Zikova street 1903/4, 166 36 Prague 6, Czech Republic E-mail: zegkljan@fel.cvut.cz\nP. Pos\u030c\u0301\u0131k Czech Technical University in Prague, Faculty of Electrical Engineering, Department of Cybernetics, Technicka\u0301 2, 166 27 Prague 6, Czech Republic E-mail: petr.posik@fel.cvut.cz\nthat fits the available training data. While the models produced by other wellknown machine learning (ML) techniques for regression (e.g. neural networks, support vector machines, or random forests) are often useful, they are essentially black boxes which are hard to analyze. On the other hand, SR aims to extract white box models, easy to analyze.\nSR is a landmark application of Genetic Programming (GP) [12]. GP is an evolutionary optimization technique that is inspired by biological evolution to evolve computer programs that perform well in a given task. GP is similar to Genetic Algorithms [9]: it uses a population of individuals (candidate solutions), a fitness function that evaluates the behavior of the solutions, a selection mechanism to promote better solutions over the worse ones, a crossover operator(s) that combines two (or more) individuals and a mutation operator(s) that (randomly) modifies individuals. The difference from GAs is that the evolved structure is not a fixed-sized array of binary or real numbers but a variable-sized data structure, typically a tree, that represents a program that solves (or is supposed to solve) a given class of problems. Such a program can also be a mathematical expression. For the rest of this article we will refer to the Koza\u2019s original GP [12] system as to \u2018vanilla GP\u2019.\nWhen vanilla GP is applied to a SR task, it usually needs a relatively long time to find an acceptable solution. While the conventional ML techniques usually fit models with a structure fixed in advance and only tune the parameters, GP searches a much broader class of possible models limited only by the user, usually by specifying the sets of function and terminal symbols, and maximal model complexity. In other words, GP searches also for a useful structure of the model. Such a system may reach impressive results [20,21] when given good data and enough time, sometimes even recovering the true equations describing the underlying phenomenon which generated the observed data.\nA novel, revealing view of the SR problem is provided by Geometric Semantic Genetic Programming (GSGP) [18]. The authors put emphasis on the difference between syntax (the actual trees and expressions) and the semantics (the output values of the candidate functions). The semantic space is n-dimensional euclidean space where n is the number of test cases. Each candidate function maps into this semantic space as a single point, with coordinates equal to the errors the function makes for individual test cases. From this point of view, the goal is to find a function that lies as close as possible to the origin of the semantic space.\nGSGP uses simple linear operators to search the semantic space. Crossover takes two trees from the population and creates an offspring by constructing a tree representing a (weighted) average of the parents. Mutation takes an individual and produces an offspring by linear combination of the parent and a randomly generated tree (which is itself generated as a difference of 2 random trees). From the point of view of these operators, the fitness landscape is unimodal, hence easy to search. GSGP is able to converge very quickly (compared to vanilla GP) and steadily. It is also resistant to overfitting thanks to the small steps it is taking towards the optimum. On the other hand, GSGP\u2019s major disadvantage is the fact that the size of a solution grows exponentially\nwith time, resulting in huge trees, that are (i) effectively black-box and (ii) slow to evaluate (even though this can be alleviated by a careful housekeeping).\nA combination of GSGP with Local Search [5] proposed recently uses only the mutation, but the offspring is constructed as the optimal linear combination with respect to the parent and a random tree via multiple regression. Using only this local search operator, the GSGP-LS converges much faster than GSGP on the training sets (though, it is also much more susceptible to overfitting). In the end, however, both the above mentioned versions of GSGP produce models which have the form of a linear combination of randomly generated trees.\nRecently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.e. to a linear combination of possibly non-linear basis functions. With the help of linear regression techniques applied to the basis functions, such models can be learned much faster. In [14], it is argued that (some of) these SR methods already have the status of a technology, i.e. that they are available to their prospective users as off-theshelf, ready-to-use tools that can be simply applied to available data, without modifying their internals or investing much effort to tune the method.\nThe first goal of this paper is to evaluate and compare several recent algorithms for SR which\u2014according to our opinion\u2014are close to being a technology. We chose 3 methods1: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models. All these methods were reported by their authors to be successful SR solvers creating simple and interpretable models. We do not expect that one of the above algorithms would produce better models in all reasonable circumstances (cf. No Free Lunch theorems for supervised learning [27]); we are more interested in the types of differences we can expect from these algorithms when applied to the same regression problems. To the best of our knowledge, such a comparison has not been done yet.\nIn this paper, all the above algorithms are used with their default parameter settings (or with minimal changes allowing a reasonable comparison). They are applied to 5 synthetic and 4 real-world SR problems of varying complexity. The synthetic problems contain internal constants2 which are hard to find for all these algorithms. The results on the real-world problems should show whether\n1 Another candidate for such a comparison would be system Eureqa [20,21]. However, we decided not to include it in the comparison because (1) it is currently a commercial software and we want to focus on open-source solutions freely available to anyone, and (2) the free academic licence does not contain any API for automating the system. We also exclude GSGP systems, since they tend to create too complex models, and the model creation process does not contain an explicit use of multiple linear regression on the global level.\n2 By an internal constant we mean a constant other than a coefficient of a top-level linear combination. Example: in 3x2 + 6 sin(1.3x), the ,,3\u201c and ,,6\u201c are not internal constants, because these are tuned by the top-level multiple regression, while the ,,1.3\u201c is internal constant (part of the nonlinear basis function).\nthe inability to find internal constants prevents the methods from finding a useful model.\nThe second goal of this article is to provide a meaningfull baselines for the comparison of the above SR methods. For the classical ML methods it is nowadays common to tune their hyperparameters; from our point of view this is also a ready-to-use technology. It is thus fair to include in the comparison a few baselines constituted by conventional ML methods (pure multiple regression, random forest, and support vector regression) with their hyperparameters tuned using a grid search (as opposed to comparing with ML methods with fixed, arbitrarily chosen hyperparameters as done in the original articles). This way we will compare SR methods which optimize the model expression structure within the given model complexity constraints (and with a very limited ability to tune the internal constants of the models) on the one hand, and on the other hand ML methods which use fixed-structure models with varying complexity (set by the grid search over hyperparameters), which are able to tune their internal constants very well.\nThe rest of the article is organized as follows: in Section 2, the compared algorithms are described in more detail. Section 3 then introduces the benchmark problems we use to compare the SR methods, and also describes the experimental methodology. Section 4 contains the results and their discussion. Section 5 concludes the paper and provides suggestions for future work."}, {"heading": "2 Compared Algorithms", "text": "This section briefly describes the selected algorithms and important aspects regarding the complexity of models produced by these algorithms."}, {"heading": "2.1 GPTIPS", "text": "GPTIPS [23,22] is an open-source SR toolbox for MATLAB. It is an implementation of Multi-Gene Genetic Programming (MGGP) [8] and thus has its roots in vanilla GP. Each solution is composed of multiple independent trees, called genes, and their outputs are linearly combined. The coefficients of this linear combination are computed optimally with respect to the mean squared error (MSE) of the resulting expression measured on the training data using ordinary least squares method.\nMGGP (and GPTIPS in particular) is based on classical Genetic Programming. This means that it works with a population of fixed size, subtree mutation, subtree crossover, tournament selection, standard initialization procedures, and is able to handle the internal constants of the model (to certain extent) using ephemeral random constants. The output of GPTIPS is the last population of models (not a pareto front); it is up to the user to choose the final one.\nTo limit the complexity of the candidate models and to prefer simpler ones, GPTIPS by default uses Lexicographic Parsimony Pressure [13] using Expressional Complexity [26] of the models (genes). The top-level linear combination of the models is not restricted (regularized) in any way.\nMGGP was shown to be faster and more accurate than vanilla GP [8] and also a comparable or better alternative to classical methods like Support Vector Regression and Artificial Neural Networks [7]."}, {"heading": "2.2 FFX", "text": "FFX, or Fast Function Extraction [14], is a deterministic algorithm for symbolic regression. It first exhaustively generates a massive set of basis functions, which are then linearly combined using Pathwise Regularized Learning [6,29] to produce sparse models. The algorithm produces a pareto-front of models with respect to their accuracy and complexity. Again, it is up to the user to choose the final model.\nThere are two kinds of bases that are generated: univariate bases and bivariate bases. Univariate bases are: a variable raised to a power (chosen from a fixed set of options) and (non-linear) functions applied to another univariate base. Bivariate bases are products of all pairs of univariate bases excluding the pairs where both the bases are of function-type; the author argues that such products are \u201cdeemed to be too complex.\u201d FFX also includes a trick that allows it to produce rational functions of the bases using the same learning procedure.\nThe original paper [14] reports FFX to be more accurate than many classical methods including vanilla GP, neural networks and SVM."}, {"heading": "2.3 EFS", "text": "EFS, or Evolutionary Feature Synthesis [2], is the most recent of the three algorithms. In EFS, the population does not consist of complete models but rather of features which, collectively, form a single model. In this respect EFS is similar to FFX: in FFX the individual features are relatively simple and are generated systematically and exhaustively, while in EFS, features may be more complex (depending on the complexity constraints) and are generated stochastically.\nThe initial population is formed by the original features of the dataset. Then, in each generation, a model is composed of the features in the current population by Pathwise Regularized Learning and is stored if it is the best. The next step in a generation is the composition of new features by applying unary and binary functions to the features already present in the current population. This way, more complex features are created from simpler ones. Also, the features are selected during this composition step according to the Pearson correlation coefficient with the feature\u2019s parents.\nEFS does not build the symbolic model explicitly \u2013 it works with the data of the features in a vectorial fashion and only stores the structure for logging purposes. This results in a very fast algorithm.\nThe original paper [2] reports EFS being comparable to neural networks and similar or better than Multiple Regression Genetic Programming which itself was reported to outperform vanilla GP, multiple regression and Scaled Symbolic Regression (introduced in [10]).\n2.4 Model Complexity Constraints\nEach algorithm described above handles the issue of resulting model complexity in a different way. GPTIPS has (user-defined) limits on the maximum number of nodes and/or maximum depth, and on the maximum number of bases. By default there is a depth limit of 4, and maximum number of bases (not counting the intercept) is also 4. EFS computes the maximum number of bases from the number of input features; maximum number of nodes in a base is hard-coded to 5. The FFX procedure results in a maximum model depth of 5."}, {"heading": "3 Benchmarks and Testing", "text": "For testing, we selected five artificial and four real-world benchmarks. The artificial benchmarks cover various types of complexities and features. An important feature of all the artificial benchmarks except Koza-1 is that they contain internal constants, which is challenging for all the algorithms. In case of the real-world benchmarks, the ground truth, i.e. the function that generated the data, is not known. The quality of the results is judged just by the testing error: we shall thus see whether the inability to learn the internal constants is a show-stopper for these algorithms.\n3.1 Artificial Benchmarks\nAll the datasets except the last one were picked based on [17]. Table 1 presents a summary of the used artificial benchmarks: their definitions, number of dimensions and their original source. Table 2 presents the training and testing sampling of those datasets. Using the notation from [17]:\n\u2013 the expression U [a, b, c] means c random samples uniformly distributed in the interval [a, b] for each variable; \u2013 the expression E[a, b, c] means a grid in the interval [a, b] with spacing of c for each variable.\nKoza-1 [12] is a classical, easy-to-solve SR benchmark. It shall test the ability of the algorithms to fit a very simple function.\nKorns-11 [11] is specific in the fact that the output depends on only one of the 5 input features and also by the presence of internal constant. The function is hard to fit because of the high frequency components.\nSalustowicz 1D (S1) [26] (called Vladislavleva-2 in [17]) is defined by a single, relatively complex term. It does not fit the generalized linear model structure well.\nSalustowicz 2D (S2) [26] (called Vladislavleva-3 in [17]) has similar features as S1, but in two dimensions.\nUnwrapped Ball 5D (UB) [26] is specific by the presence of a fraction and consists of 5 features which all influence the target value. Again, it does not fit the generalized linear model structure well.\nA note on training and testing sampling. Originally (i.e. in the referenced articles), some of the benchmarks had different sampling for training and testing data than we present here. There are two modifications we have made:\n\u2013 For Koza-1, originally there is no testing set, i.e. the same points are used both for training and testing. In order to make the results more descriptive, we decided to sample an independent testing set using the same procedure but producing more points (100). \u2013 For S1, originally the training sampling is E[0.05, 10, 0.1] and testing sampling is E[\u22120.5, 10.5, 0.05]. This means that the range of training data is smaller than the one of testing data. Because we want to focus on interpolation rather than extrapolation, we used the bigger of the two ranges, i.e.\n[\u22120.5, 10.5] both for training and testing. The grid spacing we left at the original values: 0.1 for training and 0.05 for testing.\n3.2 Real-World Benchmarks\nThe summary of the used real-world benchmarks is in Table 3. We used random 0.7/0.3 split for training/testing dataset.\nEnergy Efficiency (ENC, ENH) [25] are datasets regarding energy efficiency of cooling (ENC) and heating (ENH) of buildings, acquired from the UCI repository [4]. They were already used as benchmarks in [2], where the EFS method was introduced.\nConcrete Compressive Strength (CCS) [28] is a dataset representing a highly non-linear function of concrete age and ingredients, acquired from the UCI repository [4].\nAirfoil Self-Noise (ASN), acquired from the UCI repository [4], is a dataset regarding the sound pressure levels of airfoils based on measurements from a wind tunnel.\n3.3 Baseline Algorithms\nIn order to provide reasonable baselines for the results of the three SR algorithms, we also computed the results for three classical machine learning algorithms. The implementations of all three ML algorithms were grabbed from the Python machine learning package, scikit-learn [19,16].\nLinear Regression (LR) is an ordinary least-squares multiple linear regression, i.e. without any form of regularization. The model is built just from the original input features.\nRandom Forest (RF) is an ensemble regression model made of a number of regression trees, each fitted to a slightly perturbed version of the training data.3 Using the grid search, we tuned the following hyperparameters of the method:\n3 For details about the implementation and parameters see http://scikit-learn.org/0.17/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n\u2013 number of trees in the forest with possible values 5, 10, 50, 100, 200, and \u2013 number of features to consider when looking for the best split with possible\nvalues N and \u221a N , where N is the number of features of the dataset.\nThe grid search computes crossvalidation score for each grid point with 3- fold crossvalidation and selects the best settings4. The grid search is considered to be a part of the training.\nSupport Vector Machine for Regression (SVR)5 with RBF kernel, combined with grid search in in the following hyperparameters:\n\u2013 C, the penalty parameter of the error term, with possible values 10\u22123, 10\u22122, 10\u22121, 100, 101, 102, 103, and \u2013 \u03b3, the parameter of the RBF kernel, with possible values 0.01/N , 0.1/N , 1/N , 10/N , 100/N , 1000/N , where N is the number of features of the dataset.\nThe grid search works in the same way as in RF.\n3.4 Settings and Usage of the Algorithms\nThe goal is to perform a comparison of the chosen methods as ready-to-use tools. Therefore we didn\u2019t modify to the code of the algorithms6, and we left all of the settings at their default values. See more details below.\nAdditionally, because the default function set of GPTIPS is very limited, we added a second version of GPTIPS, which we refer to as mGPTIPS, with the function set as close as possible to that of EFS without coding new functions, i.e. using only functions already available (either in MATLAB or in the GPTIPS package). This is possible because GPTIPS is easily configurable via a config file without the need to modify the code (in contrast to the other methods). Summary of the function sets of all compared methods is in Table 4.\n4 For details about the implementation and parameters see http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html\n5 For details about the implementation and parameters see http://scikit-learn.org/0.17/modules/generated/sklearn.svm.SVR.html\n6 The only exception is EFS: we changed the round variable to false (which was originally hard-coded to true) according to the issue we opened on the algorithm\u2019s GitHub repository, see https://github.com/flexgp/efs/issues/1.\nParameter values. GPTIPS and mGPTIPS use identical default values of parameters, except the function set. Among the most interesting parameters: population size is 100, number of generations is 150, tournament size is 10, fraction of elites is 0.15, max. tree depth is 4, max. number of genes is 4, and the initialization procedure is Ramped half\u2019n\u2019half.\nEFS, except for the timeout, has no user-definable settings. The number of evolved features is determined automatically from the number of features in the data set. For details of the parameter settings, see the original paper [2].\nFFX has no user-definable settings. But it is worth to note that the possible exponents for a variable are -1, -0.5, 0.5 and 1; it is thus impossible for the algorithm to create e.g. a quartic term.\nFor EFS and FFX, which use regularized linear regression, we left the regularization settings at their default values.\nModel training and selection. From each run of each algorithm, we need to get a single model. EFS returns just a single model as a result, that best fits the training data. We decided to use the same strategy also for FFX and GPTIPS. In case of FFX, which produces as its output a set of nondominated models with respect to performance on the testing dataset and the number of bases, we provided the same data set as both the training and testing data, and selected the best model with respect to MSE. GPTIPS also returns a population of models, from which we chose the best one.\nChoosing the model with minimal training set error might not be considered a good practice because of possible overfitting to the training set. Yet, we decided to do so because of the following reasons:\n\u2013 In all three methods, overfitting is constrained by setting hard limits on the expressional complexity and/or by putting soft emphasis on simpler models (pathwise regularized learning, parsimony pressure). \u2013 Underfitting usually has more sever effects on performance than overfitting.\nTimeout. Both EFS and GPTIPS support a timeout after which the computation is terminated. We set it to 10 minutes for both methods. However, as will be seen in Table 9, all runs of all algorithms (including FFX which has no support for timeout) finished before this timeout.\n3.5 Testing Environment\nWe used GPTIPS version 2 retrieved from [24], FFX in version 1.3.4 retrieved from [15]. EFS was retrieved from [3].\nAll computations were performed on the same PC with Intel Core 2 Duo E6550 at 2.33 GHz, running 64-bit Ubuntu 15.04. The environments for the three algorithms were: MATLAB version R2014a (8.3.0.532) 64-bit for GPTIPS, Java version 1.8.0 60-b27 for EFS, Python version 2.7.9 (built with GCC 4.9.2) for FFX and Python version 3.4.3 (built with GCC 4.9.2) for the baseline algorithms.\n3.6 Testing Methodology\nEach artificial dataset with uniform random sampling (i.e. the U -type sampling) was independently sampled 100 times. Artificial datasets with deterministic sampling (i.e. the E-type sampling) are used only in the single instance. Each real-world dataset was randomly and independently split 100 times into training and testing sets using 70 % and 30 % of the datapoints respectively.\nEach algorithm was run once on each of the dataset instances producing a single model. The accuracy and complexity of the resulting models are then aggregated and statistically compared. The only exception is the FFX algorithm on S1 and S2 datasets: these datasets are sampled deterministically (so there is only one instance for both these datasets) and the FFX algorithm is also deterministic, hence a single run is sufficient for these cases."}, {"heading": "4 Results", "text": "In the following subsections, we discuss the results per dataset, some global trends we recognize in the results, the time demands of the methods, and the differences among SR and ML models.\nWe define the number of nodes as the sum of the numbers of nodes across all basis functions of the model. We count only the expression trees themselves, i.e. we do not count the additional coefficients and operators related to the toplevel linear combination produced by the linear regresssion approach used in the tested algorithms. These coefficients and operators are not counted because they are fully dependent on the bases themselves (their number) and counting them brings no interesting information.7 FFX\u2019s hinge functions, having a form of max(0, x\u2212 thr) or similar, count as 5 nodes.\nDifferences between individual methods in terms of the testing RMSE and the model complexity were statistically evaluated using one-sided MannWhitney U-test (MWUT) for each pair of algorithms with the Bonferroni correction with the significance level \u03b1 = 0.05.8\n4.1 Error and Complexity By Dataset\nIn this subsection we discuss the results from the point of view of the achieved RMSE and model complexity in terms of the number of nodes. Table 5 presents median RMSE for individual algorithms (SR and ML) and problems. The ranks of the algorithms w.r.t. the testing RMSE and the results of MWUT for errors are presented in Table 6. Table 7 presents median complexities (numbers of nodes) for individual algorithms and problems. The ranks of the algorithms w.r.t. the model complexity and the results of MWUT for model complexities are presented in Table 8. The model complexities are compared among the SR models only, since the \u201cnumber of nodes\u201d measure does not make sense for ML models.\n7 The number of nodes is used as a simple common measure of complexity accross all the algorithms only for reporting purposes. The individual algorithms use their own measures of complexity to find the best model.\n8 Nevertheless, the results are robust with respect to \u03b1: the same significance of the differences were obtained for alpha ranging from 0.001 to 0.1.\nKoza-1. As can be seen from Table 5 and Figure 1, GPTIPS was the only method that achieved zero error. With the default function set it found such model in all runs, although needing more nodes for that. Enriching the function set (mGPTIPS) enables the method to find simpler models also with optimal performance, but \u2013 due to a larger search space \u2013 it sometimes fails to find the optimum.\nFFX and EFS are worse, both reaching RMSE of the order of 10\u22122 with no significant difference between them (Table 6), partially due to the large range of RMSE values produced by EFS. The non-zero error is caused by the regularization used in these methods \u2013 EFS indeed found the optimal bases but their coefficients are not exactly 1.\nFFX and GPTIPS tend to construct significantly more complex models (Tables 7 and 8) than other methods \u2013 this is most likely caused by the fact\nthey are unable to effectively create the 4th power, and therefore need to compensate for it by creating a lot of bases.\nFor Koza-1, the SR models are better than or comparable to the tuned ML models.\nKorns-11. This dataset comes from a quickly changing function with a constant range of values. The datasets look very much like samples from a constant function with noise. As can be seen from Tables 5, 6 and Figure 2, all the methods (SR and ML) provide models of comparable performance. The best for this problem is mGPTIPS which is better than the others from the statis-\ntical point of view despite the outliers; the real importnace of the difference is, however, questionable.\nFFX and mGPTIPS produced significantly simpler models than GPTIPS and EFS (see Tables 7 and 8). Even though FFX is deterministic, the complexity of its models varies highly. The only possible cause are the differences in the individual datasets themselves. Somewhat unexpected is the fact that it influences FFX so much compared to the stoachastic EFS. Note, however, that despite the larger variance in complexitites, the overal complexity of FFX models is still significantly lower than that of EFS models.\nS1. As can be seen from Table 5 and Figure 3, the original GPTIPS with the most limited function set among the compared methods, produces complex models with relatively large errors. FFX produced a simpler model (10 nodes only) with comparable error. The complexity of EFS models is comparable to FFX, but EFS tends to produce more accurate models. The best trade-off\nis provided by mGPTIPS models which are significantly more accurate, with complexities slightly worse than those of EFS. Note that FFX was run only once since it is a deterministic algorithm and there is only a single instance of this dataset.\nThe performance of SR models on this benchmark is better than pure linear regression, but worse than RF and SVR.\nS2. For this problem, the only algorithm that produced models discernibly better than a constant function from a practical point of view was RF. Out of SR methods, only FFX was able to provide the constant model with only a single node, as can be seen in Table 7 and Figure 4. Default GPTIPS provides models with comparable performance (yet statistically better than FFX), but with much larger complexity. Some models of mGPTIPS are in fact able to reach better perfomance, but sometimes also much worse (by several orders of magnitude). EFS provides results similar to mGPTIPS, but more consistent.\nUB. Except LR, the default GPTIPS is the least accurate solver here, as can be seen in Table 5 and Figure 5, and also statistically confirmed in Table 6. Enlarging the function set allows mGPTIPS to find not only more accurate but also simpler models, but still not as good as those provided by the other two SR methods. The most accurate SR algorithms for this problem are EFS and FFX, with EFS generating models with lower number of nodes than FFX. Both EFS and FFX, however, produce more complex models than (m)GPTIPS.\nSimilarly to S1, SR methods are better than pure LR, but worse than SVR and RF.\nENC, ENH. As can be seen in Figures 6 and 7, the pattern of the results is similar for both datasets w.r.t. both the accuracy and complexity of the\nmodels, which can also be seen in Tables 5-8. The results of GPTIPS are dominated both in accuracy and simplicity by mGPTIPS, the results of FFX are dominated by EFS. EFS and mGPTIPS provide a good compromise with EFS producing more accurate models, while mGPTIPS producing simpler models.\nRF and SVR are comparable or better than the best of SR methods, EFS, in terms of accuracy.\nCCS. In this dataset, a similar pattern among SR algorithms as in ENC and ENH is also present, except that the accuracies of EFS and FFX are flipped, as displayed in Figure 8 and Tables 5 and 6.\nFrom the complexity point of view, however, the ENC/ENH pattern remains: mGPTIPS provides the simplest models, followed closely by GPTIPS. EFS produces just over a hundred nodes and, finally, FFX explodes with four\nto five hundreds of nodes. The high number of nodes is caused by the majority of bases being the hinge functions which carry high complexity\nRF models are only slightly, but significantly better than those of the best SR algorithms, FFX and EFS. All SR algorithms produce better models than pure linear regression. Note, however, the failure of SVR on this dataset \u2014 it is better than LR by only a small margin. Having the best training errors and much worse testing errors, SVR is suspect from overfitting here.\nASN. Figure 9 shows that all of the SR methods perform similarly in terms of RMSE. From the accuracy point of view (Table 6), EFS and FFX are best (not significantly different from each other), followed by mGPTIPS, and GPTIPS being the worst. However, EFS, as the only algorithm in this dataset, produced\na number of outliers (some actually worse than a pure linear model), and is thus less reliable.\nThe complexities, however, vary among the algorithms. The simplest models are produced by mGPTIPS, followed by FFX and GPTIPS which are statistically indifferent (Table 8), and EFS produces the largest models.\nRF again produced the most accurate models. LR models were in general worse than models of SR methods. SVR failed again, with both the training and testing errors larger than the errors of LR. The explanation may lie in the dataset which may be unsuitable for SVR modeling. Another reason may be the fact that SVR optimizes the hinge loss, and not RMSE.\n4.2 Global Trends\nAcross all datasets we can see that none of the compared SR algorithms was the best everywhere, both from the performance and complexity points of view. We can see that EFS and FFX perform quite well on real-world datasets and the UB artifical dataset, but not as well on the other artificial datasets. This suggests that for certain class of real-world problems the inability to work with internal constants is not crucial and can be compensated by a linear combination of sufficiently large number of features.\nAcross all datasets, EFS and FFX methods are very consistent, meaning that the clusters in complexity-performance space are compact and without too many outliers. This fact might be important in applications where consistency of the produced models is an issue. In contrast to (m)GPTIPS, this may be the results of the regularized learning employed in EFS and FFX.\n(m)GPTIPS tends to have a higher spread of either complexity or accuracy or both (except on Korns-11 where all the algorithms are similarly inconsistent). We argue that this is caused by the vanilla GP approach based on population of models, in contrast to the population of features of EFS and deterministic generation of features in FFX.\nThe comparison of SR methods with conventional ML approaches (with tuned hyperparameters) shows that SR is no silver bullet. In the majority of cases, the SR approaches were better than pure LR models, but were worse than RF or SVR models. For many datasets it can also be observed that the differences between training and testing errors were much larger for RF and SVR models, than for SR models. We thus hypothesize that with the default settings, the SR algorithms were too constrained and produced underfitted models, while the settings found by the grid search for RF and SVR may result in somewhat overfitted models. If we relaxed the model complexity constraints\nof the SR algorithms, they may find more accurate models, however the effects on the model interpretability and on the time requirements are not clear and deserve further study.\n4.3 Running Time\nThe running times of the methods are presented in Table 9. They are, however, influenced by the implementation language and running environment (FFX runs in Python 2.7, EFS in Java, GPTIPS in MATLAB). Because of this, the running times are only informative and do not necessarily represent the real complexity of the algorithms.\nBased on the wall-clock time, from the SR algorithms, (m)GPTIPS tend to run for the longest time (tens of seconds), with the exception of ENC and ENH datasets, where FFX was even worse. Runtime of EFS follows the number of features in the dataset: with Koza-1 and S1 (1D) requiring the least time, S2 (2D) requiring a bit more time, followed by Korns-11, UB, and ASN (5D), and finally ENC, ENH, and CCS (8D) requiring the most time.\nThe time demands of SR methods usually depend only linearly on the number of training examples (since they are used typically only to compute the value of evaluation function). Conventional ML methods may have much worse dependency on the number of training examples. This difference is pronounced in our study in case of the SVR algorithm (which needs to compute the kernel matrix) and the Korns-11 benchmark which has a large training set, where SVR is by far the slowest algorithm. In other cases, the time required to find a symbolic model was more or less comparable to tune and train a conventional ML model (with the exception of pure LR which is of course the fastest among the algorithms)."}, {"heading": "5 Conclusions and Future work", "text": "In this article we compared three recent methods for symbolic regression, EFS, FFX, and GPTIPS. All of them produce models from the class of Generalized Linear Models. Two of those methods, EFS and FFX, use Pathwise Regularized Learning, while GPTIPS uses classical (multiple) linear regression to determine the linear coefficients of the resulting model. EFS and GPTIPS are stochastic methods based on GP operators of mutation and crossover, while FFX is a completely deterministic method.\nWe used the methods as off-the-shelf tools, with their default settings and without modifications of their implementations. Since the default GPTIPS has a very limited function set, we added mGPTIPS with a function set closer to the one used by EFS.\nThe methods were compared on five artificial and four real world benchmarks. The results show that none of the algorithms is exceptionally worse or better than the others. We have shown some global trends such as the higher tendency of GPTIPS to larger spread in performance. EFS and FFX turned out to be consistent methods, though not always the best.\nThe comparison with tuned conventional ML algorithms shows, that in majority of cases these produced more accurate models, especially RF. However, the gap of the SR methods is not large and they produce models with symbolic representation which may be an important asset in circumstances when not only prediction accuracy is important, but also the understanding of the underlying phenomena is required.\n5.1 Future work\nThe comparison presented here provides a basic insight into the performance differences between the selected methods. In the future we plan to expand the set of benchmarks (with varying complexities, higher number of dimensions, and noise), and also expand the set of the compared algorithms, including GSGP. Another view on the algorithm comparison may be provided by unifying the sets of function symbols of all compared algorithms (which will however require generalizations of some of the presented algorithm implementations). The expanded set of benchmarks shall also allow us to tune the available parameters of the methods, and thus reduce the effects caused by possibly suboptimal parameter settings.\nAcknowledgements Jan Z\u030cegklitz was supported by the Czech Science Foundation project Nr. 15-22731S. Petr Pos\u030c\u0301\u0131k was supported by the Grant Agency of the Czech Technical University in Prague, grant No. SGS14/194/OHK3/3T/13."}], "references": [{"title": "Multiple regression genetic programming", "author": ["I. Arnaldo", "K. Krawiec", "U.M. O\u2019Reilly"], "venue": "Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201914, pp. 879\u2013886. ACM, New York, NY, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Building predictive models via feature synthesis", "author": ["I. Arnaldo", "U.M. O\u2019Reilly", "K. Veeramachaneni"], "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201915, pp. 983\u2013990. ACM, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Lichman, M.: UCI machine learning repository", "author": ["K. Bache"], "venue": "Http://archive.ics.uci.edu/ml", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Geometric semantic genetic programming with local search", "author": ["M. Castelli", "L. Trujillo", "L. Vanneschi", "S. Silva", "E. Z-Flores", "P. Legrand"], "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201915, pp. 999\u2013 1006. ACM, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software 33(1), 1\u201322", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A multi-gene genetic programming model for estimating stress-dependent soil water retention curves", "author": ["A. Garg", "A. Garg", "K. Tai"], "venue": "Computational Geosciences 18(1), 45\u201356", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Modelling chemical process systems using a multi-gene genetic programming algorithm", "author": ["M. Hinchliffe", "H. Hiden", "B. McKay", "M. Willis", "M. Tham", "G. Barton"], "venue": "Late Breaking Paper, GP\u201996, pp. 56\u201365. Stanford, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Adaptation in Natural and Artificial Systems", "author": ["J.H. Holland"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Scaled symbolic regression", "author": ["M. Keijzer"], "venue": "Genetic Programming and Evolvable Machines 5(3), 259\u2013269", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Genetic Programming Theory and Practice IX, chap", "author": ["M.F. Korns"], "venue": "Accuracy in Symbolic Regression, pp. 129\u2013151. Springer New York, New York, NY", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection", "author": ["J.R. Koza"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Lexicographic parsimony pressure", "author": ["S. Luke", "L. Panait"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201902, pp. 829\u2013 836. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Ffx: Fast, scalable, deterministic symbolic regression technology", "author": ["T. McConaghy"], "venue": "R. Riolo, E. Vladislavleva, J.H. Moore (eds.) Genetic Programming Theory and Practice IX, Genetic and Evolutionary Computation, pp. 235\u2013 260. Springer New York", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Genetic programming needs better benchmarks", "author": ["J. McDermott", "D.R. White", "S. Luke", "L. Manzoni", "M. Castelli", "L. Vanneschi", "W. Jaskowski", "K. Krawiec", "R. Harper", "K. De Jong", "U.M. O\u2019Reilly"], "venue": "Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation, GECCO \u201912, pp. 791\u2013 798. ACM, New York, NY, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric semantic genetic programming", "author": ["A. Moraglio", "K. Krawiec", "C. Johnson"], "venue": "C. Coello, V. Cutello, K. Deb, S. Forrest, G. Nicosia, M. Pavone (eds.) Parallel Problem Solving from Nature - PPSN XII, Lecture Notes in Computer Science, vol. 7491, pp. 21\u201331. Springer Berlin Heidelberg", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research 12, 2825\u20132830", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. Schmidt", "H. Lipson"], "venue": "Science 324(5923), 81\u201385", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Eureqa (Version 0.98 beta", "author": ["M. Schmidt", "H. Lipson"], "venue": "Www.nutonian.com", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "GPTIPS 2: An Open-Source Software Platform for Symbolic Data Mining, pp", "author": ["D.P. Searson"], "venue": "551\u2013573. Springer International Publishing, Cham", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "GPTIPS: an open source genetic programming toolbox for multigene symbolic regression", "author": ["D.P. Searson", "D.E. Leahy", "M.J. Willis"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists, vol. 1, pp. 77\u201380", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools", "author": ["A. Tsanas", "A. Xifara"], "venue": "Energy and Buildings 49, 560\u2013567", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming", "author": ["E. Vladislavleva", "G. Smits", "D. den Hertog"], "venue": "Evolutionary Computation, IEEE Transactions on 13(2), 333\u2013349", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "The supervised learning no-free-lunch theorems", "author": ["D.H. Wolpert"], "venue": "In Proc. 6th Online World Conference on Soft Computing in Industrial Applications, pp. 25\u201342", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling of strength of high-performance concrete using artificial neural networks", "author": ["I.C. Yeh"], "venue": "Cement and Concrete Research 28(12), 1797\u20131808", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(2), 301\u2013320", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "SR is a landmark application of Genetic Programming (GP) [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "GP is similar to Genetic Algorithms [9]: it uses a population of individuals (candidate solutions), a fitness function that evaluates the behavior of the solutions, a selection mechanism to promote better solutions over the worse ones, a crossover operator(s) that combines two (or more) individuals and a mutation operator(s) that (randomly) modifies individuals.", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "For the rest of this article we will refer to the Koza\u2019s original GP [12] system as to \u2018vanilla GP\u2019.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "Such a system may reach impressive results [20,21] when given good data and enough time, sometimes even recovering the true equations describing the underlying phenomenon which generated the observed data.", "startOffset": 43, "endOffset": 50}, {"referenceID": 17, "context": "Such a system may reach impressive results [20,21] when given good data and enough time, sometimes even recovering the true equations describing the underlying phenomenon which generated the observed data.", "startOffset": 43, "endOffset": 50}, {"referenceID": 14, "context": "A novel, revealing view of the SR problem is provided by Geometric Semantic Genetic Programming (GSGP) [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "A combination of GSGP with Local Search [5] proposed recently uses only the mutation, but the offspring is constructed as the optimal linear combination with respect to the parent and a random tree via multiple regression.", "startOffset": 40, "endOffset": 43}, {"referenceID": 19, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 18, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 12, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 1, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 0, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 12, "context": "In [14], it is argued that (some of) these SR methods already have the status of a technology, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "We chose 3 methods: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "We chose 3 methods: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models.", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "We chose 3 methods: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models.", "startOffset": 229, "endOffset": 232}, {"referenceID": 22, "context": "No Free Lunch theorems for supervised learning [27]); we are more interested in the types of differences we can expect from these algorithms when applied to the same regression problems.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "1 Another candidate for such a comparison would be system Eureqa [20,21].", "startOffset": 65, "endOffset": 72}, {"referenceID": 17, "context": "1 Another candidate for such a comparison would be system Eureqa [20,21].", "startOffset": 65, "endOffset": 72}, {"referenceID": 19, "context": "GPTIPS [23,22] is an open-source SR toolbox for MATLAB.", "startOffset": 7, "endOffset": 14}, {"referenceID": 18, "context": "GPTIPS [23,22] is an open-source SR toolbox for MATLAB.", "startOffset": 7, "endOffset": 14}, {"referenceID": 6, "context": "It is an implementation of Multi-Gene Genetic Programming (MGGP) [8] and thus has its roots in vanilla GP.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "To limit the complexity of the candidate models and to prefer simpler ones, GPTIPS by default uses Lexicographic Parsimony Pressure [13] using Expressional Complexity [26] of the models (genes).", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "To limit the complexity of the candidate models and to prefer simpler ones, GPTIPS by default uses Lexicographic Parsimony Pressure [13] using Expressional Complexity [26] of the models (genes).", "startOffset": 167, "endOffset": 171}, {"referenceID": 6, "context": "MGGP was shown to be faster and more accurate than vanilla GP [8] and also a comparable or better alternative to classical methods like Support Vector Regression and Artificial Neural Networks [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "MGGP was shown to be faster and more accurate than vanilla GP [8] and also a comparable or better alternative to classical methods like Support Vector Regression and Artificial Neural Networks [7].", "startOffset": 193, "endOffset": 196}, {"referenceID": 12, "context": "FFX, or Fast Function Extraction [14], is a deterministic algorithm for symbolic regression.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "It first exhaustively generates a massive set of basis functions, which are then linearly combined using Pathwise Regularized Learning [6,29] to produce sparse models.", "startOffset": 135, "endOffset": 141}, {"referenceID": 24, "context": "It first exhaustively generates a massive set of basis functions, which are then linearly combined using Pathwise Regularized Learning [6,29] to produce sparse models.", "startOffset": 135, "endOffset": 141}, {"referenceID": 12, "context": "The original paper [14] reports FFX to be more accurate than many classical methods including vanilla GP, neural networks and SVM.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "EFS, or Evolutionary Feature Synthesis [2], is the most recent of the three algorithms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "The original paper [2] reports EFS being comparable to neural networks and similar or better than Multiple Regression Genetic Programming which itself was reported to outperform vanilla GP, multiple regression and Scaled Symbolic Regression (introduced in [10]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "The original paper [2] reports EFS being comparable to neural networks and similar or better than Multiple Regression Genetic Programming which itself was reported to outperform vanilla GP, multiple regression and Scaled Symbolic Regression (introduced in [10]).", "startOffset": 256, "endOffset": 260}, {"referenceID": 13, "context": "All the datasets except the last one were picked based on [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "Using the notation from [17]:", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "Koza-1 [12] is a classical, easy-to-solve SR benchmark.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Koza-1 f1(x) = x4 + x3 + x2 + x 1 [12] Korns-11 f2(x, y, z, v, w) = 6.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "Korns-11 [11] is specific in the fact that the output depends on only one of the 5 input features and also by the presence of internal constant.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "Salustowicz 1D (S1) [26] (called Vladislavleva-2 in [17]) is defined by a single, relatively complex term.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Salustowicz 1D (S1) [26] (called Vladislavleva-2 in [17]) is defined by a single, relatively complex term.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Salustowicz 2D (S2) [26] (called Vladislavleva-3 in [17]) has similar features as S1, but in two dimensions.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Salustowicz 2D (S2) [26] (called Vladislavleva-3 in [17]) has similar features as S1, but in two dimensions.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Unwrapped Ball 5D (UB) [26] is specific by the presence of a fraction and consists of 5 features which all influence the target value.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 10, "endOffset": 16}, {"referenceID": 20, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 27, "endOffset": 33}, {"referenceID": 23, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 45, "endOffset": 51}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 45, "endOffset": 51}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "Energy Efficiency (ENC, ENH) [25] are datasets regarding energy efficiency of cooling (ENC) and heating (ENH) of buildings, acquired from the UCI repository [4].", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "Energy Efficiency (ENC, ENH) [25] are datasets regarding energy efficiency of cooling (ENC) and heating (ENH) of buildings, acquired from the UCI repository [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "They were already used as benchmarks in [2], where the EFS method was introduced.", "startOffset": 40, "endOffset": 43}, {"referenceID": 23, "context": "Concrete Compressive Strength (CCS) [28] is a dataset representing a highly non-linear function of concrete age and ingredients, acquired from the UCI repository [4].", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Concrete Compressive Strength (CCS) [28] is a dataset representing a highly non-linear function of concrete age and ingredients, acquired from the UCI repository [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Airfoil Self-Noise (ASN), acquired from the UCI repository [4], is a dataset regarding the sound pressure levels of airfoils based on measurements from a wind tunnel.", "startOffset": 59, "endOffset": 62}, {"referenceID": 15, "context": "The implementations of all three ML algorithms were grabbed from the Python machine learning package, scikit-learn [19,16].", "startOffset": 115, "endOffset": 122}, {"referenceID": 1, "context": "For details of the parameter settings, see the original paper [2].", "startOffset": 62, "endOffset": 65}], "year": 2017, "abstractText": "Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms \u2014 multiple regression, random forests and support vector regression.", "creator": "LaTeX with hyperref package"}}}