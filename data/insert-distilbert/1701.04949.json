{"id": "1701.04949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2017", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe", "abstract": "this paper thus presents the development of several models of a deep convolutional auto - encoder in the universal caffe filtering deep learning framework and their experimental evaluation on the example of mnist dataset. we have created five models properties of a convolutional auto - encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto - encoder's encoder and decoder parts. importantly our results show that the developed models provide very good results useful in dimensionality reduction and unsupervised clustering tasks, and small gap classification or errors when we used either the learned internal code as an input of employing a supervised linear output classifier and multi - domain layer perceptron. the best results were provided by a model where the encoder part originally contains fixed convolutional and pooling layers, followed by an analogous decoder sorting part with bounded deconvolution and unpooling layers without the corresponding use of intermediate switch variables in the decoder filter part. the simulation paper also discusses various practical details of the creation of a faster deep purple convolutional auto - encoder in the very popular caffe deep learning framework. we believe ultimately that our approach described and results presented in this paper could help other researchers to build efficient deep neural network architectures in the open future.", "histories": [["v1", "Wed, 18 Jan 2017 05:24:24 GMT  (3514kb)", "http://arxiv.org/abs/1701.04949v1", "21 pages, 11 figures, 5 tables, 62 references"]], "COMMENTS": "21 pages, 11 figures, 5 tables, 62 references", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["volodymyr turchenko", "eric chalmers", "artur luczak"], "accepted": false, "id": "1701.04949"}, "pdf": {"name": "1701.04949.pdf", "metadata": {"source": "CRF", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe", "authors": ["Volodymyr Turchenko", "Eric Chalmers", "Artur Luczak"], "emails": ["luczak}@uleth.ca"], "sections": [{"heading": null, "text": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4]. An AE is trained in unsupervised fashion which allows extracting generally useful features from unlabeled data. AEs and unsupervised learning methods have been widely used in many scientific and industrial applications, mainly solving tasks like network pre-training, feature extraction, dimensionality reduction, and clustering. A classic or shallow AE has only one hidden layer which is a lowerdimensional representation of the input. In the last decade, the revolutionary success of deep neural network (NN) architectures has shown that deep AEs with many hidden layers in the encoder and decoder parts are the state-of-the-art models in unsupervised learning. In comparison with a shallow AE, when the number of trainable parameters is the same, a deep AE can reproduce the input with lower reconstruction error [5]. A deep AE can extract hierarchical features by its hidden layers and, therefore, substantially improve the quality of solving specific task. One of the variations of a deep AE [5] is a deep convolutional auto-encoder (CAE) which, instead of fully-connected layers, contains convolutional layers in the encoder part and deconvolution layers in the decoder part. Deep CAEs may be better suited to image processing tasks because they fully utilize the properties of convolutional neural networks (CNNs), which have been proven to provide better results on noisy, shifted (translated) and corrupted image data [6].\nModern deep learning frameworks, i.e. ConvNet2 [7], Theano with lightweight extensions Lasagne and Keras [8- 10], Torch7 [11], Caffe [12], TensorFlow [13] and others, have become very popular tools in deep learning research since they provide fast deployment of state-of-the-art deep learning models along with state-of-the-art training algorithms (Stochastic Gradient Descent, AdaDelta, etc.) allowing rapid research progress and emerging commercial applications. Moreover, these frameworks implement many state-of-the-art approaches to network initialization, parametrization and regularization, as well as state-of-the-art example models. Besides many outstanding features, we have chosen the Caffe deep learning framework [12] mainly for two reasons: (i) a description of a deep NN is pretty straightforward, it is just a text file describing the layers and (ii) Caffe has a Matlab wrapper, which is very convenient and allows getting Caffe results directly into a Matlab workspace for their further processing (visualization, etc.) [12].\nThe goal of this paper is to present the practical implementation of several CAE models in the Caffe deep learning framework, as well as experimental results on solving an unsupervised clustering task using the MNIST dataset. This study is an extended version of our paper published in arXiv [14]. All developed Caffe .prototxt files to reproduce our models along with Matlab-based visualization scripts are included in supplementary materials. The paper is organized as follows: Section 2 describes relevant related work, Section 3 presents the developed models along with practical rules of thumb we have used to build the models, Section 4 presents the experimental results, Section 5 discusses the observations we have received in our experiments and Section 6 contains conclusions."}, {"heading": "2. Related Work", "text": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15]. There are many\nstudies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.\nThe work of Ranzato et al. [15] is one of the first studies which uses convolutional layers for unsupervised learning of sparse hierarchical features. Their model consists of two levels of CAE; each CAE has convolutional and maxpooling layers in the encoder part and upsampling and full convolutional layers in the decoder part. The model is trained independently using a greedy layer-wise approach, and the output of the first CAE level serves as an input to the second level. The authors suggested that if an AE may learn the identity function in a trivial way and produce uninteresting features, its hidden layer describes a code which can be overcomplete. Making this code sparse is a way to overcome this disadvantage. Lee et al. [24] and Norouzi et al. [25] have researched unsupervised learning of hierarchical features using a stack of convolutional Restricted Boltzmann Machines (RBM) and a greedy layer-wise training approach. The fully-connected operations were substituted by convolutional operations, and the probabilistic max-pooling was introduced in [24]. Deterministic max-pooling was used in [25]. Both Lee and Norouzi argue that convolutional RBMs have increased overcompleteness of the learned features and suggest adding sparsity to the hidden features. Masci et al. [26] have investigated shallow and deep (stacked) CAEs for hierarchical feature extraction, trained by a greedy layer-wise approach. Valid convolutional layers with and without max-pooling are used in the encoder part and full convolutional layers are used in the decoder part. The authors have stated that the use of max-pooling layers is an elegant way to provide the architecture with enough sparsity, and no additional regularization parameters are needed.\nThe closest work to our results is a recent paper by Zhao et al. [27] entitled \u201cStacked What-Where Auto-Encoders\u201d (SWWAE). Their architecture integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning. Within the unsupervised part, their SWWAE consists of several convolutional and max-pooling layers followed by one fully-connected layer in the encoder part and, inversely, one fully-connected, unpooling and deconvolution layers in the decoder part. Their SWWAE is symmetric in the encoder and decoder parts. The terms \u201cwhat\u201d and \u201cwhere\u201d correspond to pooling and appropriate unpooling operations which were proposed in [28-29]. The output of a max-pooling layer is the \u201cwhat\u201d variable (it is a max-pooled feature), which is fed to the next layer in the encoder part. The complementary \u201cwhere\u201d variables are the max-pooling \u201cswitch\u201d positions. The \u201cwhat\u201d variables inform the next layer about the content with incomplete information about position, while the \u201cwhere\u201d variables inform the corresponding feed-back unpooling layer in the decoder part about where these max-pooled (dominant) features should be reconstructed in the unpooled feature map. In addition to a standard L2 (Euclidean) reconstruction cost function at the input level, the authors have proposed to use an L2 middle reconstruction cost function between the corresponding hidden layers in the encoder and decoder parts to provide better model training. Similarly to some other solutions, the authors have used a dropout layer [30] added to the fully-connected layers and an L1 sparsity penalty on hidden layers as a regularization technique.\nThe high quality of the extracted latent hierarchical features in the existing solutions analyzed above has been confirmed by the state-of-the-art classification results obtained by different classifiers trained in a supervised way. The extracted features were used as inputs for these classifiers. However, [23] was the only paper to provide a visualization of the extracted features in a two-dimensional space. We believe that such visuals might be considered as an inherent addition to the results presented in all studies above.\nThe use of a pooling layer is still the most controversial question in the theory and practice of CNNs [31]. The state-of-the-art approach in building supervised convolutional models is to have a max-pooling layer which computes the maximum activation of the units in a small region of the previous convolutional layer. Encoding the result of convolution operation with max-pooling allows higher-layer representations to be invariant to small translations of the input and reduces computational cost [24]. Scherer et al. [32] have shown that a max-pooling operation is considerably better at capturing invariances in image data, compared to subsampling operation. In unsupervised convolutional models Masci et al. [26] have shown that a CAE without max-pooling layers learns trivial solutions and interesting and biology plausible filters only emerge once a CAE is trained with a max-pooling layer. Zhao et al. [27] have proven that their SWWAE with max-pooling \u2013 unpooling layers provides much better quality of image reconstruction than maxpooling \u2013 unsampling layers. Controversially, recent findings by Springenberg et al. [33] have proven that a maxpooling operation can simply be replaced by a convolutional operation with increased stride without decreasing accuracy on several image recognition benchmarks. In our previous paper we have presented a CAE without pooling \u2013 unpooling layers which provided acceptable quality of the dimensionality reduction and unsupervised clustering tasks [14]. Therefore we have included the pooling - unpooling layers in our study below aiming to find which model, with or without pooling \u2013 unpooling layers, will be better.\nTaking into account that a deep unsupervised model is a complex model from a training perspective, the question about a training approach is very important. We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27]. A top-down approach implies efficient training of all hidden layers of a model with respect to the input, while a greedy layer-wise training approach [15, 26] specifies that each layer receives its input from the latent representation of the layer below and trains independently. Zeiler et al. argued that the major drawback to a greedy layer-wise approach \u201cis that the image pixels are discarded after the first layer, thus higher layers of the model have an increasingly diluted connection to the input. This makes learning fragile and impractical for models beyond a few layers\u201d [29]. Moreover, Zeiler et al. have proven their conclusions by the comparison of top-down and greedy layer-\nwise approaches showing significantly better performance of the former on the Caltech-101 database. The advantage of a top-down training approach over a greedy layer-wise has been proven in the SWWAE study too [27].\nAlso there are several practical solutions/attempts to develop a CAE model on different platforms: shallow CAE [35] and convolutional RBM [36] in Matlab, deep CAE in Theano/Lasagne [37], Theano/Keras [38], Torch7 [39-40] and Neon [41]."}, {"heading": "3. Model description", "text": "In general, Caffe models are end-to-end machine learning systems. A typical network begins with a data layer which loads data from a disk and ends with one or several loss layers which specify a goal of learning (also known as an error, cost, objective or loss function). Our CAE models include convolutional, pooling, fully-connected, deconvolution, unpooling and loss layers. For brevity we do not include a description of the fully connected layer, which is pretty well-known.\nThe convolutional/deconvolution layer followed by an activation function is described by the expression [26] )(\u2211\n\u2208 +\u2297= Ll\nkklk bwxfh ,\nwhere kh is the latent representation of the k -th feature map of the current layer, f is an activation function (normally non-linear), lx is l -th feature map of the group of feature maps L of the previous layer or l -th-channel of input image with total L channels in a case of the first convolutional layer, \u2297 denotes the 2D convolution operation,\nkw and kb are the weights (filters) and the bias of the k -th feature map of the current layer respectively. If lx is an image or a feature map with size mm \u00d7 and the filters are nn \u00d7 , a convolutional layer performs \u2018valid convolution\u2019 and the size of the output feature map )1()1( +\u2212\u00d7+\u2212 nmnm is decreasing. A deconvolution layer performs \u2018full convolution\u2019 and the size of the output feature map )1()1( \u2212+\u00d7\u2212+ nmnm is increasing [26, 28]. This is how convolutional layers provide encoding of an input image by decreasing the output feature maps from layer to layer in the encoder part and, inversely, how deconvolution layers provide reconstruction of the input image by the increasing the output feature maps from layer to layer in the decoder part.\nA max-pooling layer pools features by taking the maximum activity within input feature maps (outputs of the previous convolutional layer) and produces (i) its output feature map with reduced size according to the size of pooling kernel and (ii) supplemental switch variables (switches) which describe the position of these max-pooled features [28- 29]. The unpooling layer restores the max-pooled feature (what) either (i) into the correct place, specified by the switches (where), or, (ii) into some specific place within the unpooled output feature map. Fig. 1 illustrates convolution \u2013 deconvolution and pooling \u2013 unpooling operations. We have used a Caffe implementation of unpooling layer provided by Noh et al. [42], which was successfully applied for semantic segmentation task [43].\nWe have used the following practical rules of thumb in creating our CAE models in Caffe: 1. The main issue with a deep AE is asymmetry [27]. Thus, the model should be symmetric in terms of the total\nsize of the feature maps and the number of neurons in all hidden layers in both the encoder and decoder parts. These sizes and numbers should decrease from layer to layer in the encoder part and increase in the same way in the decoder part providing the encoder-decoder paradigm, i.e. contractive AE [19]. These sizes and numbers should not be less than some minimal values, allowing handling the size of the input problem from the informational point of view; 2. As a cost function, it is better to use two loss layers, <Sigmoid_Cross_Entropy_Loss> and <Euclidean_Loss> [5, 19]. Preliminary experiments has shown that the use of only one of these loss layers separately does not provide good training convergence:\n\u2022 the cross-entropy (logistic) loss layer is defined by the expression\n[ ]\u2211 =\n\u2212\u2212+\u2212= N\nn nnnn yyyyN E 1\n)\u02c61log()1(\u02c6log1 , (1)\nwhere N is a number of samples, y are the targets }{ 1,0\u2208y , )(\u02c6 nn xwfy \u22c5\u2261 , where f is a logistic function, w is the vector of weights optimized through gradient descent and x is the input vector [44], and, \u2022 the Euclidean (L2) loss layer defined by expression\n\u2211 =\n\u2212= N\nn nn yyN E 1\n2\n2 \u02c6 2 1 ,\nwhere y\u0302 are the predictions }{ \u221e+\u221e\u2212\u2208 ,y\u0302 and y are the targets }{ \u221e+\u221e\u2212\u2208 ,y . 3. Visualization of the values (along with numerical representations) of trainable filters, feature maps and hidden\nunits from layer to layer plays a very important diagnostic role. It allows us to inspect the function of intermediate layers and, therefore, better understand how data are converted/processed inside a deep model [45]; 4. The main purpose of an activation function after each layer is non-linear data processing [15]. Since the nature of convolution/deconvolution operations is a multiplication, our visualization showed that the result of the convolution/deconvolution operations (the values of feature maps) increasing sharply from layer to layer, preventing the CAE model from converging during training. Thus, the use of sigmoid or hyperbolic tangent activation functions, which constrain the resulting values of feature maps to the interval [0,1] or [-1,1] respectively, sets appropriate limits on the values of feature maps at the end of the decoder part, and provides good convergence of the whole model; 5. The well-known fact is that good generalization properties of NNs depend on the ratio of trainable parameters, i.e. weights (|w|) and biases (|b|) to the size of the input data (|Data|). Therefore experimentation is required [5, 46] to find the AE architecture, (i.e. the size of trainable parameters and appropriate number of neurons in all hidden layers) with the best generalization properties. Then, a similar size in terms of the total size of feature maps and the number of neurons in all hidden layers could be considered as an appropriate starting point when designing a CAE with good generalization properties. Direct comparison of AE and CAE models is inappropriate, because a CNN of the same size as a given fully-connected network would have fewer trainable parameters [6]; 6. The created model should be stable. NN training may converge to different local optima in different runs depending on initial weights/biases. By \u201cstable model\u201d we mean that the same convergence results can be obtained in several consecutive runs (at least three).\nWe have created five deep CAE models in Caffe, denoted in Table 1, as follows: Model 1 (Fig. 2 (left)), notation (conv <-> deconv), contains two convolutional layers followed by two fully-connected\nlayers in the encoder part and, inversely, one fully-connected layer followed by two deconvolution layers in the decoder part. This is the model which we have investigated in our arXiv paper [14];\nModel 2 (Fig. 2 (right)), notation (conv, pool <-> deconv), contains two pairs of convolutional and pooling layers followed by two fully-connected layers in the encoder part and, inversely, one fully-connected layer followed by only two deconvolution layers in the decoder part. As we can see from Table 1, Model 2 is not symmetric. However, we have included it because we have seen a similar idea in the Neon deep learning framework [41] and wanted to study it; Model 3 (Fig. 3 (left)), notation (conv, pool, sw (switches) <-> deconv, sw, unpool), contains two pairs of convolutional and pooling layers followed by two fully-connected layers in the encoder part and, inversely, one fullyconnected layer followed by two pairs of deconvolution and unpooling layers WITH the use of switch variables in the decoder part. The unpooling layer works here as follows: the max-pooled feature (what) is restored into the correct place (where) within the unpooled output feature map, as specified by the switch variable. All other elements of the map are zeros. Model 3 is similar to the networks presented in [27, 29]; Model 4 (Fig. 3 (right)), notation (conv, pool <-> deconv, unpool), contains two pairs of convolutional and pooling layers followed by two fully-connected layers in the encoder part and, inversely, one fully-connected layer followed by two deconvolution and unpooling layers WITHOUT switch variables in the decoder part. The unpooling layer works here as follows: the max-pooled feature (what) is restored into a predetermined place (where) within the unpooled output feature map, all other elements of the map are zeros. This approach could be called \u201ccentralized\u201d unpooling, when the max-pooled feature is placed in the center of each pool [29, 47]. In the Noh implementation [42] we used, this specific position is a left and a top corner [0,0] of each pool. Some research results for this unpooling mode are presented in [29]; Model 5 (does not have a Figure), notation (conv, pool (tanh) <-> deconv, unpool (tanh)), the same model as the Model 4 except of using a hyperbolic tangent activation function in all layers. All previous Model 1 \u2013 Model 4 use a standard sigmoid activation function.\nMore technical details about our Models (with the reference to Table 1 and Figs. 2-3) along with the results of their experimental research are presented in the next Section."}, {"heading": "4. Experimental results", "text": "4.1. Size of the models We have used the MNIST dataset [48] for the experimental research. In all experiments we have used the standard 60000 examples as a training set and 10000 examples as a testing set. The size of the MNIST training dataset is 60000 examples x 784 elements, so our |Data| = 47040 K elements. We will be using this number to calculate the ratio of trainable parameters of our Models (weights |w| and biases |b|) to the size of the input data |Data/(w+b)| in our experiments below. We have created an HDF5 version of the MNIST dataset and have not applied any modification to input images except for normalization into the range [0,1]. We have used the MNIST labels only for visualization of clustering results.\nIn the Caffe examples there are two models which solve a dimensionality reduction task: a semi-supervised Siamese network [49], proposed by Hadsell et al. [50] and a deep AE [46], proposed by Hinton et al. [5]. We have included these two models in our results for comparison with our Models 1-5. A Siamese network consists of two coupled LeNet [6] architectures followed by a contrastive loss function. It trains in a semi-supervised fashion, since we form the training set by labelling a pair of input images (chosen randomly) as 1 if the images belong to the same class and 0 otherwise. We have placed the Siamese model in the first row of Table 1 to have a semi-supervised model first in the list and followed by all unsupervised models. In the Siamese network available in the examples [49] we have changed the number of planes (feature maps) in the convolutional layers and the number of neurons in the fullyconnected layers to be the same as the encoder part of our Models 2-5. We left only one ReLU (a Rectified Linear Unit activation function) layer before the fully connected layers as in the Caffe examples [49].\nAccording to our rule 5 in Section 3 above, we have researched a deep AE [5] in our previous paper [14]. We have shown that the exact deep AE architecture (1000-500-250-30) presented in [5] provides the best generalization properties out of 5 different models. This model is specified in the second row of Table 1. Rule 5 says that a CAE model with the same total size of feature maps and the same number of neurons in all hidden layers (we call this parameter \u201cModel size\u201d, see the fourth column, Table 1) could be considered as an appropriate starting point when designing the model with good generalization properties. However, the research results of CAE models without pooling \u2013 unpooling layers in [14] have actually shown that a CAE model which is twice as big (in terms of feature maps) gives better results. This CAE model is our Model 1 and it is specified in the third row of Table 1. Model 1 has about 297K training parameters and its |Data/(w+b)| ratio is 158. Therefore we have created our Models 2-5 trying to keep the same |Data/(w+b)| ratio which is 148 and 139 for the Model 2 and the Model 3 respectively. Note that our Models 1-5 are half the size (in terms of trainable parameters) of the SWWAE with 679K trainable parameters [27].\nTable 1 contains the architecture parameters for all researched models. Table 2 contains a detailed calculation of the number of trainable parameters for a Siamese network. Table 3 contains a detailed calculation of the number of trainable parameters for a deep AE and Models 1-3 only, because Models 4-5 parametrically are similar to Model 3. We trained each model in three fashions, with 2, 10 and 30 (30 is exactly as in Caffe examples) neurons in the last hidden layer of the encoder part <ip2encode>, which corresponds to 2-, 10-, 30-dimensional (we will be using abbreviations 2D, 10D and 30D) space of encoding respectively. Here in Tables 1-3 all calculations are provided for the 2D case. But graphically the layer <ip2encode> looks a bit different (Figs. 2-3): since it does a linear inner product operation only, it does not have a green rectangle with activation function; find a blue rectangle <ip2encode> somewhere in the center of each Model. The followed blob with the same title, see above the yellow octagon <ip2encode>, keeps the result of this operation. This is an actual place, i.e. 2, 10 and 30 neurons, where data are stored. Further we will be calling it as \u201cinternal code\u201d of our Models. Also it serves as an input for the decoder part. Therefore, graphically we see that the Models have two fully-connected layers <ip1encode> and <ip2encode> in the encoder part and only one fullyconnected layer <ip1decode> in the decoder part. We, therefore, use this rhetoric, that our Models have two fullyconnected layers in the encoder part and only one fully-connected layer in the decoder part. However, it provides a symmetric number of feature maps and the number of neurons in the hidden layers (see the third column of Table 1, bold).\nTo provide a better understanding of our notations in Table 1 as well as Figs. 2-3, let us explain the conv/deconv layers for Model 1 and Model 3. The encoder part of Model 1 has 8 planes (or feature maps) with kernel 9x9 in the conv1 layer and 4 planes with kernel 9x9 in the conv2 layer. The decoder part has 4 planes with kernel 12x12 in the deconv2 layer and 4 planes with kernel 17x17 in the deconv1 layer. Model 3 has 26 planes with kernel 5x5 in the conv1 layer followed by the max-pooling layer pool1 with kernel 2x2 and 36 planes with kernel 5x5 in the conv2 layer followed by the max-pooling layer pool2 with kernel 2x2 in the encoder part. In the decoder part Model 3 has 36 planes with kernel 4x4 in the deconv2 layer followed by the unpooling layer unpool2 with kernel 2x2 and 26 planes with kernel 5x5 in the deconv1 layer followed by the unpooling layer unpool1 with kernel 2x2. In the decoder part, the purpose of the deconvolution layer deconv1neur, which corresponds to the term (1*1w+1b) for Model 1 and the term (5*5w+1b) for Model 3 in the third column of Table 3, is to reshape all feature maps of the last decoder layer deconv1 or unpool1 into one reconstructed image with the same size of 28x28 pixels as the original MNIST image. There was an explanation in the Caffe user group how to do that [51].\nSince the deconvolution operation has the same nature as convolution [29], we have used the same approach to calculate the number of trainable parameters both in the encoder and decoder parts (Table 3). The accuracy of our calculations can be easily checked by calling Caffe from Matlab or Python using the command <caffe(\u2018weights\u2019);>. As we can see from Tables 1 and 3, our Models (except of the Model 2) are practically symmetric not only in terms of the\ntotal number of elements in feature maps and the number of neurons in the hidden layers, but also in terms of the number of trainable parameters in both the encoder and decoder parts.\n4.2. Dimensionality reduction results The experimental results of a Siamese network and a deep AE along withModels 1-5 are presented in Table 4. We did three runs for each model. For unsupervised models the values of both loss layers <Sigmoid_Cross_Entropy_Loss> and <Euclidean_Loss> are separated by semicolon. We presented a value of a <Contrastive_Loss> layer for a Siamese network. The values of loss layers are presented in red for the training set and in black for the test set. The learning parameters of the solver for all models were the same: 50000 training iterations were used, Stochastic Gradient Descent algorithm used one hundred patterns in a batch, base learning rate was 0.006, learning policy was \"fixed\", and weight decay was equal to 0.0005. A standard sigmoid activation function was used in the deep AE (it was in examples) and our Models 1-4. A standard hyperbolic tangent activation function was used in Model 5. We ran many experiments changing the architecture and learning parameters, but in many cases they were not stable architectures. For example, we tried different initializations of weights and biases (<weight_filler> and <bias_filler>). The presented results were obtained with the following initialization: <bias_filler {type: \"constant\"}> for all layers, <weight_filler {type: \"xavier\"}> for convolutional/deconvolution layers, <weight_filler {type: \"gaussian\" std: 1 sparse: 25}> for fullyconnected (InnerProduct) layers.\nIn order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29]. The goal of this experiment was to apply the classifiers with the same structure and learning parameters and estimate which Model provides the best internal code as determined by classification results. We have used two simple classification models, linear model (function <classify()>) and a multi-layer perceptron (MLP) implemented in the Matlab Statistics and Machine Learning Toolbox and Neural Network Toolbox respectively. We created three separate MLP models for 2D, 10D and 30D input code keeping the ratio |Data/(w+b)| = 100 (where |Data| is, for example, 20K for the 2D internal code, given the MNIST test set contains the 10000 instances). Thus we have used three models, 2-14-10, 10-42-10 and 30-66-10 for these three cases. Standard Matlab functions were used: <patternnet()> - to create a MLP, <train()> - to train it and <net()> - to provide classification results. We used sigmoid and softmax activation functions for hidden and output layers of the MLP respectively in all cases. The following learning parameters were used: the training accuracy was 10-10, the minimum training gradient was 10-10, training epochs were 6000 and the maximum training validations checks were 6000. A scaled conjugate gradient backpropagation algorithm \u2018trainscg\u2019 was used for the training. We used 10-fold cross-validation and calculated an average per-class classification error [52]. The classification results are specified in the seventh and eighth columns of Table 4. The last column of Table 4 contains the sparsity estimation of the internal code for all researched models (see more details in Section 5.1).\nObviously, a MLP has shown better classification results than a linear model. A semi-supervised Siamese network showed the best classification results (1.39% by 10D Siamese network) in comparison with unsupervised models. Model 1 (2.65% by 30D Model 1) and Model 2 (2.54% by 30D Model 2) have provided slightly better results than a deep AE (2.85% by 30D deep AE). Model 3 with pooling-unpooling layers and WITH switches obtained the worst classification results (8.75% and 3.75% by 10D and 30D Model 3). Model 4, with pooling-unpooling layers and\nWITHOUT switches, has shown the best classification results (2.19% by 30D Model 4). The results of Model 5 (3.36% by 10D Model 5) show that the use of a hyperbolic tangent activation function provides worse results than the use of a sigmoid activation function. For comparison we also run the same classification experiment using an MLP with the same number of trainable parameters, the same learning parameters and the same 10-fold cross validation on original 784-dimensional MNIST images. The MLP 784-88-10 (which has same ratio |Data/(w+b)| = 100) obtained worse 2.85% average per-class classification error. Therefore we can conclude, that the developed Models provide very good quality feature extraction and dimensionality reduction on the MNIST dataset, because the obtained classification errors are less than 12% in a case of a linear model and less than 2.85% in a case of a MLP.\nWe also collected the training times for unsupervised models only (for 50000 training iterations) both in CPU and GPU modes (Table 5) on the three computational systems located in the Canadian Centre for Behavioural Neuroscience (CCBN), University of Lethbridge, Canada:\n\u2022 The workstation WS1241 operated under Ubuntu 14.04.2. It is equipped with a 4-core (total of 8 CPU threads visible in Linux) Inter(R) Xeon(R) E5620@2.40 GHz processor, 51 Gb of RAM and an NVidia(R) GeForce GTS 450 GPU. The GPU has Fermi architecture, 192 CUDA cores, 1 Gb of RAM and compute capability 2.1;\n\u2022 The workstation Polaris1 operated under Ubuntu 16.04.2. It is equipped with two 8-core (total of 32 CPU threads visible in Linux) Inter(R) Xeon(R) E5-2630 v3 @2.40 GHz processors, 256 Gb of RAM and an NVidia(R) GeForce GTX TITAN X GPU. The GPU has Maxwell architecture, 3072 CUDA cores, 12 Gb of RAM and compute capability 5.2;\n\u2022 The cluster Hodgkin operated under Red Hat Enterprise Linux. The cluster consists of 64 blade servers. Each server is equipped with two 6-core Intel(R) Xeon(R) CPU E5520@2.27 GHz processors and 48 GB of RAM. The cluster has a total of 768 CPU cores and 3072 GB of RAM. We have used Hodgkin in a CPU mode only. Since it has a lot of CPU cores we run many models (3 times per model) in parallel and, therefore, we have received most of our results on this computational platform.\nInterestingly, as we can see from Table 5, the relation between training times is straightforward in CPU computational mode, where the bigger Models 3-5 obviously train longer. But the training times in GPU mode are smaller (practically 1.5 times) for the bigger Models 3-5, which have larger feature maps in conv/deconv layers, and more trainable parameters (see Tables 1-2) in comparison with Models 1-2. This is because a GPU-implementation of conv/deconv layers in Caffe is much better optimized (i.e. bigger-sized layers are computed much faster) than appropriate CPU implementation thanks to NVidia CUDA technology [53] and the cuDNN library [54].\n4.3. Clustering and visualization results We have used the t-SNE technique [55] to visualize 10D and 30D internal code, produced by all models. The t-SNE technique is an advanced method for dimensionality reduction and visualization. It is based on Stochastic Neighbor Embedding (SNE) [56] which converts the high-dimensional Euclidean distances between datapoints into conditional probabilities that represent similarities. SNE minimizes the sum of Kullback-Leibler divergences over all datapoints using a gradient descent method. t-SNE differs from SNE in two ways: (1) it uses a symmetrized version of the SNE cost function with simpler gradients and (2) it uses a Student-t distribution rather than a Gaussian to compute the similarity between two points in the low-dimensional space. This allows t-SNE to have a cost function that is easier to optimize and produces significantly better visualizations [55]. This advanced technology has been used for visualization in many state-of-the-art problems, for example, to visualize the last hidden-layer representations of a deep Q-network playing Atari 2600 games [57].\nOur clustering task is to encode the 10,000 instances of the MNIST test set in a 2D space. A Siamese network does it in a semi-supervised fashion, and a deep AE and our Models 1-5 do it in an unsupervised fashion. The visualization results from our previous paper [14] have shown that there is no big difference between visualized 10D and 30D internal codes. Therefore we are presenting here the 10D visualizations only. The visualizations of how a Siamese network, a deep AE, and our Models 1-5 solve the clustering task are depicted in Figs. 4-10 respectively. Table 6 shows how a deep AE and our Models 1-5 provide the reconstruction of several example MNIST images.\nThe clustering results in Figs. 4-10 and reconstruction results in Table 6 confirm the numerical results (which characterize the quality of the internal codes) in Table 1. Model 4, which provided the smallest classification errors, has (not surprisingly) shown the best clustering results (Fig. 9) among the unsupervised models. The clusters produced by Model 4 are more dense (Fig. 9 (right)) and look qualitatively similar to the result of the semi-supervised Siamese model (Fig. 4 (right)) comparing to the other models. Model 4 has also shown the second-best reconstruction quality of MNIST images among the unsupervised models, after Model 3 (Table 6). Model 3 has shown contradictory results. It provided the worst classification errors in Table 1 and the worst (even unacceptable) clustering results (Fig. 8). However, it has shown the smallest values of our cost function (Table 1) and practically ideal reconstruction quality of MNIST images (Table 6). We discuss these results further in Section 5.1.\nAs mentioned above in rule 3, the visualization of a deep network plays very important diagnostic role and allows us to easily inspect the function of intermediate layers. Similarly to Zeiler and Fergus [45], who stated that visualization allowed them to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark, our visualization allowed us basically to create our Models. The implementation of such visualization is simple and straightforward thanks to the Matlab wrapper implemented in Caffe. In order to visualize a model, it is necessary to create the number of <.prototxt> files corresponding to the number of its layers. After training, and having an appropriate <.caffemodel> file with saved weights and biases, it is necessary to call Caffe from Matlab using each of those <.prototxt> files as an argument. The received values produced by each layer should be visualized. The visualization example of how Model 4 encodes and decodes the digit \u201c5\u201d is depicted in Fig. 11. The visualizations for other Models look similar. Fig. 11 should be read together with the graphical representation of the Model 4 in Fig. 3 (right). Processing results of intermediate layers are stored in blobs. Blobs are just matrixes, depicted by yellow octagons in Fig. 3 (right) and we visualized their content in Fig. 11. Each panel in Fig. 11 corresponds to the appropriate blob in Fig. 3 (right). The title of each panel in Fig. 11 starts with the same name as the corresponding blob in Fig. 3 (right). As seen in Fig. 3 (right) some blobs have two inputs. For example, the blob \u201cconv1\u201d has two inputs: from the convolutional layer conv1 and from the layer sig1en which uses this blob \u201cconv1\u201d as an input, performs on that input a transformation by a sigmoid activation function and then stores the result back to the same blob \u201cconv1\u201d. Therefore in the titles of the appropriate panels in Fig. 11, the title conv1|conv1 means that this panel visualizes the content of the blob \u201cconv1\u201d stored by convolutional layer conv1 and the title conv1|sig1en means that this panel visualizes the content of the blob \u201cconv1\u201d stored by sigmoid layer sig1en. The same approach is used for all other panels in Fig. 11. The next element of the title of each panel is the size of appropriate blob: for example, 24x24x26 in the panel conv1|conv1 means that the blob \u201cconv1\u201d contains 26 feature maps, each map has size 24x24 elements; 1x1x250 in the panel ip1encode|sig3en means that the blob \u201cip1encode\u201d contains 250 neurons with size 1x1 elements; and so on. The titles of all panels (except for fully-connected layers) have square brackets where we indicated the minimum and maximum values of the appropriate blob. The panels which visualize the blobs \u201cip1encode\u201d, \u201cip2encode\u201d and \u201cip1decode\u201d belonging to fully-connected layers, contain their numerical representation on the ordinate axis. Thus, the title of every single panel in Fig. 11 clearly describes what processing results are provided by every intermediate layer of a deep network. Such visualizations, and especially a numerical representation of the blobs allowed us to understand that in the failed experiments, the outputs of deconv2 and deconv1 layers were negatively or positively saturated, and therefore the pixels of the final reconstructed image in the layer deconv1neursig had the value 0 or 1 (obtained by a sigmoid function as a preparation step to calculate a cross-entropy loss, see expression (1)) and the values of loss layers were NaN (Not A Number) during the training.\nAll developed Caffe .prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59]. Note, that the developed Models work in Caffe version used/contributed by Noh et al. [42] (the date of the files in this version is Jun 16, 2015). We ran our models in the latest version we have (the date of files in this version is Apr 05, 2016). It seems, in the newer versions (after Jun 16, 2015) the Caffe developers have changed: (i) the syntax of layer descriptions \u2013 from \u201clayers\u201d to \u201clayer\u201d, and layers\u2019 types from \u201cCONVOLUTION\u201d to \u201cConvolution\u201d, etc. and (ii) the internal representation of fully-connected (InnerProduct) layers: it is a 2D array now, not a 4D, as it was in the previous version(s). To deal with these issues it is necessary to change the syntax in the .prototxt files accordingly and to change the dimensionality of the blob of the last fully-connected layer ip1decode before the first deconvolution layer decode2 in the decoder part using the reshape layer as follows: <layer {name: \"reshape\" type: \"Reshape\" bottom: \"ip1decode\" top: \"ip1decodesh\" reshape_param { shape { dim: 0 dim: 0 dim: 1 dim: 1 }}}> and use the variable \"ip1decodesh\" as an input to the following layer decode2.\nEach panel visualizes the output of appropriate layer of Model 4. The title of each panel describes a type of the layer (e.g. conv1, pool1, conv2, pool2, and so on), a size of the layer (e.g. 24x24x26 means 26 feature maps with 24x24 elements each) and a range of the layer\u2019s output [min..max]. See more explanation in the next to last paragraph of Section 4.3."}, {"heading": "5. Discussion", "text": ""}, {"heading": "5.1. Why does Model 3 WITH switches provide the worst dimensionality reduction and unsupervised clustering?", "text": "Model 3 featured pooling-unpooling layers WITH switches. It reached a much smaller value of our cost function (Table 4) and provided practically an ideal reconstruction of the input images (Table 6). However, it fulfilled the dimensionality reduction (8.75% and 3.75% by 10D and 30D Model 3) and unsupervised clustering tasks (Fig. 8) in the worst way. In this section we discuss this interesting situation.\nOne explanation is mentioned by Zhao et al. in their paper [27]: the MNIST dataset is too simple a problem for such a model as Model 3. Our Model 3 is similar to their SWWAE and we have presented the same perfect reconstruction of MNIST images as in their Figure 2. Zhao et al. stated that \u201cWe reason that SWWAE not working so well \u2026 is due to the fact that reconstructing MNIST digits is overly easy for SWWAE. \u2026 Since MNIST is a roughly binary dataset (0/1) and thus within unpooling stage, decoding does not necessarily demand the information from \u2018what\u2019 for reconstruction; i.e., it could get perfect reconstruction by pinning 1 on the positions indicated by \u2018where\u2019. Therefore, we believe that reconstructing MNIST dataset renders insufficient regularization on the encoding pathway\u201d [27]. In addition to the statement above our results show that a CAE model with pooling-unpooling layers and WITHOUT switches (our Model 4) in its decoder part does not necessarily demand the \u201cwhere\u201d information for successful reconstruction, having the \u201cwhat\u201d information. Model 4 restores the max-pooled features in the predefined positions [0, 0] (so-called \u201ccentralized\u201d unpooling [29, 47]) of the unpooled feature maps and, therefore, prevents too much self-adapting. Such \u201ccentralized\u201d unpooling plays a role of a peculiar penalty which penalizes sensitivity of a model to the input and encourages a robustness of the learned representation [19]. Model 4 does not reconstruct an input image perfectly; however, it showed the best quality of dimensionality reduction and unsupervised clustering among all considered unsupervised models.\nWe present an alternate explanation for Model 3\u2019s poor dimensionality reduction and unsupervised clustering performance. The principal difference between Model 3 and other considered unsupervised models is that the use of switch variables considerably increases (inflates) the dimensionality of internal representation of Model 3. Model 3 has the low-dimensional internal code and the switch variables used for reconstruction [15], but all other unsupervised models have the low-dimensional internal code only. Thus Model 3 is not a contractive AE anymore. The dimension of its internal representation is much bigger than the dimension of the input MNIST image (see simple calculations below). Model 3 simply learns the identity function in a trivial way and produces useless, uninteresting features; its internal code is overcomplete [15]. It corresponds to the description of the overcompletness of features by Norouzi et al. [25] as the following: \u201cWhat happens is that after a few iterations of parameter update, sampled images become very close to the original ones [which means perfect reconstruction], and the learning signal disappears\u201d. We see confirmation that the features in Model 3 are overcomplete in Fig. 8 (right). The last hidden 10D internal code of our Model 3 is so overcomplete, that even the state-of-the-art t-SNE method [55] cannot find any appropriate similarity between the datapoints; it can barely distinguish only two clusters: digit \u201c1\u201d (label \u201c2\u201d, orange) and digit \u201c6\u201d (label \u201c7\u201d, blue).\nAs stated in [15, 24-25, 27] the sparsity of an overcomplete code is low. Specifically, Zhao et al. [27] stated in the quotation above that the regularization of their model is insufficient, which means the sparsity is low. Similarly to Zeiler et al. [28], who calculated relative sparsity of feature maps, we have calculated sparsity of the hidden internal code for all our Models (see the last column of Table 4). We have used the sparsity measure [60]\n},{#0 \u03b5\u03b5 \u2264= jcj , (2) where # is a number which describes the sparsity of a vector ],,[ 21 Ncccc 2  = , N is a dimensionality of the analyzed vector c , \u03b5 is a threshold. Thus, we calculate the number of elements jc in vector c \nthat are smaller than a threshold \u03b5 , which means bigger value specifies bigger level of sparsity. We did three experiments for three values of the threshold \u03b5 : 10%, 20% and 30% of the maximum value in each analyzed internal code for all models presented in Table 4. We have received similar results for all thresholds \u03b5 and we have specified the average value of sparsity for 30% threshold for all three runs in Table 4. Rather than specify the sparsity 0\u03b5 as a number (which actually will be hard to interpret and to compare between all 2D, 10D and 30D models), we specified it as a percentage of the elements of the internal code which are smaller than a threshold \u03b5 (30%) for the appropriate internal code. For example, the sparsity 70.9% for 2D Model 4 (Table 4) means that 70.9% of total elements of that code within all 10000 test patterns (i.e. 2D * 10000 = 20000 total elements) are less than 30% of the maximum value found in these 20000 total elements. Such a relative measure of 0\u03b5 allows us to compare all models. Note, that 1% here means 200 elements for 2D internal code, 1000 elements for 10D internal code and 3000 elements for 30D internal code. The analysis of Table 4 has shown that the sparsity of the internal code of the Model 3 is less than all other models: in the 2D case it is 30.8% against the interval of a minimum 59.3% in a Siamese network and a maximum 79.7% in Model 2, in the 10D case it is 67.4% against the interval of a minimum 68.9% in Model 5 and a maximum 75.8% in Model 2, and in the 30D case it is 74.8% against the interval of a minimum 77.6% in Model 5 and a maximum 86.9% in Model 4. Our results show no clear\nrelation between better sparsity of the internal code and better average per-class classification error: we just state that obviously the sparsity of the internal code of the Model 3 is less than other successfully-working models.\nThe well-known regularization techniques, for example dropout [30] and weight decay [61], aim to improve the sparsity of deep models and prevent trainable parameters from growing too large. Therefore we have estimated the sparsity of the trainable parameters of all researched models using expression (2) and the approach described above. Again, for the 30% threshold \u03b5 , the successfully-working models have shown bigger sparsity (12.3-12.4%), while Model 3 has shown a lower value of 11.8%.\nThe overcompleteness of Model 3\u2019s internal representation makes reconstruction \u201coverly easy\u201d [27], explaining its good reconstruction but poor clustering performance. A complementary explanation arises when we realize that the essential function of an AE is compression: finding a low-dimensional code to represent a high-dimensional input [5]. Such a low-dimensional but information-rich code represents a useful clustering of the data and can facilitate effective classification. We propose that switch variables undermine the autoencoder\u2019s principal role as a compressor. To illustrate this, consider the compression ratio achieved by Model 3. One simple way to approximate this compression ratio is to assume that switch variables and other values are uniformly distributed over their ranges. Under this simple assumption, a MNIST image of 28 x 28 pixels in the range [0,255] requires 784 bytes to encode, while the 2D, 10D, and 30D single-precision codes require 8, 40, and 120 bytes respectively. Each of the 3744 switch variables stored by Model 3\u2019s first pooling layer is an integer address in the range [0,575], and thus requires at least 9 bits to encode. The 576 switch variables in the second pooling layer take values in the range [0,63], and require 6 bits to encode. Thus Model 3\u2019s switch variables and internal code together total between 4652 and 4764 bytes of information \u2013 significantly more information than was stored in the input image itself, and giving a negative compression ratio of -490%. The use of switch variables causes Model 3 to inflate rather than compress the input data. In contrast, all other unsupervised models, which produce only the 2D, 10D, or 30D internal codes, achieve compression ratios of 99%, 95%, and 85% respectively. A more thorough analysis of compression ratio could measure self-information or entropy, but we suppose that this simple calculation is sufficiently illustrative.\nModel 3\u2019s inflation \u2013 rather than compression \u2013 of the input data short-circuits the AE training process, which normally guides the AE to achieve minimal reconstruction error through effective compression. The extra information cached in switch variables allow Model 3 to achieve good reconstruction during training without learning an effective compression scheme, which explains why the resulting 2D, 10D, and 30D internal codes have low clustering and classification value.\n5.2. Experiments with ReLU activation function A ReLU activation function has been proven better for supervised deep machine learning tasks in a lot of research studies because it provides better sparsification of a deep NN model, and models with sparser coding provide better classification results. This useful property of a ReLU function should serve for unsupervised models too. The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.\nWhen we have used a ReLU function in our Models they, surprisingly, did not train. When we ran the experiments using the same initialization parameters for weights and biases as it was described in Section 4.2 above, i.e. <bias_filler {type: \"constant\"}> for all layers, <weight_filler {type: \"xavier\"}> for convl/deconv layers, <weight_filler {type: \"gaussian\" std: 1 sparse: 25}> for fully-connected layers, the Models contained the value NaN (Not A Number) in all feature maps and the values of our cost function during training were NaN too. When we have initialized the weights and biases within the smaller interval, i.e. we have used <weight_filler {type: \"gaussian\" std: 0.01}>, <bias_filler {type: \"constant\" value: 0}> for all layers, the Models calculated the values of our cost function in the first training iteration only. After the first iteration both values were set to 307.24; 37.88 and further they changed only slightly; staying moreless the same until training stopped. In our opinion, it may be necessary to find some correct initialization of a CAE model with a ReLU activation function or use some ideas described in the PyLearn user group [62]. Then, hopefully, due to the better sparsification properties, a ReLU activation function could be able to provide better results than our Models with sigmoid and hyperbolic tangent activation functions. This will be one of the future lines of our research."}, {"heading": "6. Conclusions", "text": "The development of several deep convolutional auto-encoder models in the Caffe deep learning framework and their experimental evaluation on the MNIST dataset are presented in this paper. In contrast to the deep fully-connected auto-encoder proposed by Hinton et al. [5], convolutional auto-encoders allow using the desirable properties of convolutional neural networks for image processing tasks while working within an unsupervised learning paradigm.\nWe have created and researched five convolutional auto-encoder models in Caffe: (i) Model 1 which contains two convolutional layers followed by two fully-connected layers in the encoder part and, inversely, one fully-connected layer followed by two deconvolution layers in the decoder part, (ii) Model 2 which contains two pairs of convolutional and pooling layers followed by two fully-connected layers in the encoder part and, inversely, one fully-connected layer followed by only two deconvolution layers in the decoder part, (iii) Model 3 which contains two pairs of convolutional and pooling layers followed by two fully-connected layers in the encoder part and, inversely, one fully-connected layer followed by two pairs of deconvolution and unpooling layers WITH the use of switch variables in the decoder part, (iv)\nModel 4 which is the same as Model 3 but WITHOUT the use of switch variables in the decoder part and (v) Model 5 which is the same as Model 4 except of using a hyperbolic tangent activation function in all layers (instead of sigmoid in Models 1-4). The hidden low-dimensional internal code learned by these Models in an unsupervised way was used as an input for a linear classifier and a standard one-hidden-layer perceptron and the classification errors for each Model were estimated.\nOur results show that the developed Models 1-2 and 4-5 provide very good results of the dimensionality reduction and unsupervised clustering tasks and small classification errors. Specifically, Model 1 and Model 2 without pooling \u2013 unpooling layers provide slightly better results (2.65% by 30D Model 1 and 2.54% by 30D Model 2) than a deep fullyconnected auto-encoder (2.85% by 30D deep auto-encoder). Model 4 with pooling-unpooling layers and WITHOUT switches shows the best result (2.19% by 30D Model 4). Model 5 shows that the use of a hyperbolic tangent activation function instead sigmoid provides worse results (3.36% by 10D Model 5). Model 3 with pooling-unpooling layers and WITH switches shows a practically an ideal reconstruction of the input images, but the worst fulfilment of the dimensionality reduction and unsupervised clustering tasks and therefore large classification errors (8.75% and 3.75% by 10D and 30D Model 3). We think this is because Model 3\u2019s use of switch variables considerably inflates the dimensionality of its internal representation. Model 3 most likely learns only the trivial identity mapping.\nDuring the creation of Models of a convolutional auto-encoder we have followed several rules of thumb, mentioned in Section 3 above, which are used by many machine learning researchers every day. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. All developed Caffe .prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59]. We believe that our approach and results, presented in this paper, could help other researchers to build efficient deep neural network architectures in future.\nThe application of the developed Models of a deep convolutional auto-encoder for the analysis of input images in the neuroscience field is the direction of our future research."}, {"heading": "Acknowledgements", "text": "We would like to thank the Caffe developers (the Berkeley Vision and Learning Center, UC Berkeley) for creating such a powerful framework for deep machine learning research. We thank Karim Ali (CCBN) for help with Caffe installation on Hodgkin and Polaris1, Hyeonwoo Noh (POSTECH, Korea) for using his Caffe implementation of the unpooling layer and discussions on some results presented in this paper, and Dr. Robert Sutherland (CCBN) for help with financial support."}], "references": [{"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature. 323 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Modeles connexionistes de l\u2019apprentissage", "author": ["Y. LeCun"], "venue": "Ph.D. thesis, Universite de Paris VI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1987}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourland", "Y. Kamp"], "venue": "Biological Cybernetics. 59 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks. 2 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science. 313 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. of the IEEE. 86 (11) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "in: J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, K.Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 24, NIPS Foundation Inc., Granada", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv:1408.5093", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Creation of a deep convolutional auto-encoder in Caffe", "author": ["V. Turchenko", "A. Luczak"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "in: 2007 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Minneapolis, MN", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "in: 2008 25th International Conference on Machine Learning (ICML), International Machine Learning Society, Helsinki", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press, http://www.deeplearningbook.org, 2016 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "Lecture Notes in Computer Sci. 6791 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "in: 28th International Conference on Machine Learning (ICML), International Machine Learning Society, Bellevue, WA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "K-Sparse autoencoders", "author": ["A. Makhzani", "B. Frey"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1312.5663, Banff", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1312.6114, Banff", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "arXiv:1509.00519", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial autoencoders", "author": ["A. Makhzani", "J. Shlens", "N. Jaitly", "I. Goodfellow", "B. Frey"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1511.05644, San Juan", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "in: 26th Annual International Conference on Machine Learning (ICML), ACM, New York, NY", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning", "author": ["M. Norouzi", "M. Ranjbar", "G. Mori"], "venue": "in: 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Miami, FL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Ciresan", "J. Schmidhuber"], "venue": "Lecture Notes in Computer Sci. 6791 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked what-where auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. LeCun"], "venue": "in: International Conference on Learning Representations (ICLR), town, arXiv:1506.02351, San Juan", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "in: 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, San Francisco, CA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "in: 2011 IEEE International Conference on Computer Vision (ICCV), IEEE, Barcelona", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Machine Learning Res. 15 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. Muller", "S. Behnke"], "venue": "Lecture Notes in Computer Sci. 6354 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Striving for simplicity: the all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1412.6806, San Diego", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks", "author": ["A. Dosovitskiy", "P. Fischer", "J. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence. 38 (9) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional autoencoders in python/theano/lasagne", "author": ["M. Swarbrick Jones"], "venue": "https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/, 2015 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Training autoencoders on ImageNet using Torch 7", "author": ["S. Khallaghi"], "venue": "http://siavashk.github.io/2016/02/22/autoencoder-imagenet/, 2016 ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Modified version of Caffe which support DeconvNet and DecoupledNet", "author": ["H. Noh"], "venue": "https://github.com/HyeonwooNoh/caffe, 2015 ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "in: 2015 IEEE International Conference on Computer Vision (ICCV), IEEE, Santiago", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A tutorial on the cross-entropy method", "author": ["P.T. De Boer", "D.P. Kroese", "S. Mannor", "R.Y. Rubinstein"], "venue": "Annals of Operations Res. 134 (1) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Lecture Notes in Computer Sci. 8689 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning with hierarchical convolutional factor analysis", "author": ["B. Chen", "G. Polatkan", "G. Sapiro", "D. Blei", "D. Dunson", "L. Carin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "http://yann.lecun.com/exdb/mnist/, 1998 ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "in: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New York", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Inf. Processing and Management. 45 (4) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G. Hinton"], "venue": "J. Machine Learning Res", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Stochastic neighbor embedding", "author": ["G.E. Hinton", "S.T. Roweis"], "venue": "in: Advances in Neural Information Processing Systems (NIPS), the MIT Press, Cambridge, MA", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2002}, {"title": "D", "author": ["V. Mnih", "K. Kavukcuoglu"], "venue": "Silver et al, Human-level control through deep reinforcement learning, Nature. 518 ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional Auto-Encoder in Caffe", "author": ["V. Turchenko"], "venue": "but still without pooling-unpooling layers. https://groups.google.com/forum/#!topic/caffe-users/GhrCtONcRxY, 2015 ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Lethbridge Brain Dynamics", "author": ["A. Luczak"], "venue": "http://lethbridgebraindynamics.com/artur-luczak/, 2009 ", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Comparing measures of sparsity", "author": ["N. Hurley", "S. Rickard"], "venue": "IEEE Transactions on Inf. Theory. 55 (10) ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural networks and the bias/variance dilemma", "author": ["S. Geman", "E. Bienenstock", "R. Doursat"], "venue": "Neural Computation. 4 ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1992}, {"title": "Rectified linear units in autoencoder", "author": ["Bios Volodymyr"], "venue": "Artur Luczak received a M.Sc. in Biomedical Engineering", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 1, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 2, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 3, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 4, "context": "In comparison with a shallow AE, when the number of trainable parameters is the same, a deep AE can reproduce the input with lower reconstruction error [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "One of the variations of a deep AE [5] is a deep convolutional auto-encoder (CAE) which, instead of fully-connected layers, contains convolutional layers in the encoder part and deconvolution layers in the decoder part.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "Deep CAEs may be better suited to image processing tasks because they fully utilize the properties of convolutional neural networks (CNNs), which have been proven to provide better results on noisy, shifted (translated) and corrupted image data [6].", "startOffset": 245, "endOffset": 248}, {"referenceID": 6, "context": "ConvNet2 [7], Theano with lightweight extensions Lasagne and Keras [810], Torch7 [11], Caffe [12], TensorFlow [13] and others, have become very popular tools in deep learning research since they provide fast deployment of state-of-the-art deep learning models along with state-of-the-art training algorithms (Stochastic Gradient Descent, AdaDelta, etc.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "ConvNet2 [7], Theano with lightweight extensions Lasagne and Keras [810], Torch7 [11], Caffe [12], TensorFlow [13] and others, have become very popular tools in deep learning research since they provide fast deployment of state-of-the-art deep learning models along with state-of-the-art training algorithms (Stochastic Gradient Descent, AdaDelta, etc.", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "Besides many outstanding features, we have chosen the Caffe deep learning framework [12] mainly for two reasons: (i) a description of a deep NN is pretty straightforward, it is just a text file describing the layers and (ii) Caffe has a Matlab wrapper, which is very convenient and allows getting Caffe results directly into a Matlab workspace for their further processing (visualization, etc.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": ") [12].", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "This study is an extended version of our paper published in arXiv [14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 1, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 2, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 3, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 4, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 206, "endOffset": 213}, {"referenceID": 9, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 206, "endOffset": 213}, {"referenceID": 4, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 132, "endOffset": 140}, {"referenceID": 11, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 173, "endOffset": 181}, {"referenceID": 13, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 173, "endOffset": 181}, {"referenceID": 14, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 236, "endOffset": 240}, {"referenceID": 17, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 257, "endOffset": 261}, {"referenceID": 9, "context": "[15] is one of the first studies which uses convolutional layers for unsupervised learning of sparse hierarchical features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24] and Norouzi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[25] have researched unsupervised learning of hierarchical features using a stack of convolutional Restricted Boltzmann Machines (RBM) and a greedy layer-wise training approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The fully-connected operations were substituted by convolutional operations, and the probabilistic max-pooling was introduced in [24].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "Deterministic max-pooling was used in [25].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "[26] have investigated shallow and deep (stacked) CAEs for hierarchical feature extraction, trained by a greedy layer-wise approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[27] entitled \u201cStacked What-Where Auto-Encoders\u201d (SWWAE).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The terms \u201cwhat\u201d and \u201cwhere\u201d correspond to pooling and appropriate unpooling operations which were proposed in [28-29].", "startOffset": 111, "endOffset": 118}, {"referenceID": 23, "context": "The terms \u201cwhat\u201d and \u201cwhere\u201d correspond to pooling and appropriate unpooling operations which were proposed in [28-29].", "startOffset": 111, "endOffset": 118}, {"referenceID": 24, "context": "Similarly to some other solutions, the authors have used a dropout layer [30] added to the fully-connected layers and an L1 sparsity penalty on hidden layers as a regularization technique.", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "However, [23] was the only paper to provide a visualization of the extracted features in a two-dimensional space.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "Encoding the result of convolution operation with max-pooling allows higher-layer representations to be invariant to small translations of the input and reduces computational cost [24].", "startOffset": 180, "endOffset": 184}, {"referenceID": 25, "context": "[32] have shown that a max-pooling operation is considerably better at capturing invariances in image data, compared to subsampling operation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[26] have shown that a CAE without max-pooling layers learns trivial solutions and interesting and biology plausible filters only emerge once a CAE is trained with a max-pooling layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[27] have proven that their SWWAE with max-pooling \u2013 unpooling layers provides much better quality of image reconstruction than maxpooling \u2013 unsampling layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[33] have proven that a maxpooling operation can simply be replaced by a convolutional operation with increased stride without decreasing accuracy on several image recognition benchmarks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In our previous paper we have presented a CAE without pooling \u2013 unpooling layers which provided acceptable quality of the dimensionality reduction and unsupervised clustering tasks [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 23, "context": "We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27].", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "A top-down approach implies efficient training of all hidden layers of a model with respect to the input, while a greedy layer-wise training approach [15, 26] specifies that each layer receives its input from the latent representation of the layer below and trains independently.", "startOffset": 150, "endOffset": 158}, {"referenceID": 20, "context": "A top-down approach implies efficient training of all hidden layers of a model with respect to the input, while a greedy layer-wise training approach [15, 26] specifies that each layer receives its input from the latent representation of the layer below and trains independently.", "startOffset": 150, "endOffset": 158}, {"referenceID": 23, "context": "This makes learning fragile and impractical for models beyond a few layers\u201d [29].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "The advantage of a top-down training approach over a greedy layer-wise has been proven in the SWWAE study too [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Also there are several practical solutions/attempts to develop a CAE model on different platforms: shallow CAE [35] and convolutional RBM [36] in Matlab, deep CAE in Theano/Lasagne [37], Theano/Keras [38], Torch7 [39-40] and Neon [41].", "startOffset": 181, "endOffset": 185}, {"referenceID": 29, "context": "Also there are several practical solutions/attempts to develop a CAE model on different platforms: shallow CAE [35] and convolutional RBM [36] in Matlab, deep CAE in Theano/Lasagne [37], Theano/Keras [38], Torch7 [39-40] and Neon [41].", "startOffset": 213, "endOffset": 220}, {"referenceID": 20, "context": "The convolutional/deconvolution layer followed by an activation function is described by the expression [26]", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "convolution\u2019 and the size of the output feature map ) 1 ( ) 1 ( \u2212 + \u00d7 \u2212 + n m n m is increasing [26, 28].", "startOffset": 96, "endOffset": 104}, {"referenceID": 22, "context": "convolution\u2019 and the size of the output feature map ) 1 ( ) 1 ( \u2212 + \u00d7 \u2212 + n m n m is increasing [26, 28].", "startOffset": 96, "endOffset": 104}, {"referenceID": 30, "context": "[42], which was successfully applied for semantic segmentation task [43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[42], which was successfully applied for semantic segmentation task [43].", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "Illustration of deconvolution and unpooling layers [43]", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "The main issue with a deep AE is asymmetry [27].", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "contractive AE [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "As a cost function, it is better to use two loss layers, <Sigmoid_Cross_Entropy_Loss> and <Euclidean_Loss> [5, 19].", "startOffset": 107, "endOffset": 114}, {"referenceID": 13, "context": "As a cost function, it is better to use two loss layers, <Sigmoid_Cross_Entropy_Loss> and <Euclidean_Loss> [5, 19].", "startOffset": 107, "endOffset": 114}, {"referenceID": 32, "context": "function, w is the vector of weights optimized through gradient descent and x is the input vector [44], and,", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "It allows us to inspect the function of intermediate layers and, therefore, better understand how data are converted/processed inside a deep model [45]; 4.", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "The main purpose of an activation function after each layer is non-linear data processing [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Thus, the use of sigmoid or hyperbolic tangent activation functions, which constrain the resulting values of feature maps to the interval [0,1] or [-1,1] respectively, sets appropriate limits on the values of feature maps at the end of the decoder part, and provides good convergence of the whole model; 5.", "startOffset": 138, "endOffset": 143}, {"referenceID": 0, "context": "Thus, the use of sigmoid or hyperbolic tangent activation functions, which constrain the resulting values of feature maps to the interval [0,1] or [-1,1] respectively, sets appropriate limits on the values of feature maps at the end of the decoder part, and provides good convergence of the whole model; 5.", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "Therefore experimentation is required [5, 46] to find the AE architecture, (i.", "startOffset": 38, "endOffset": 45}, {"referenceID": 5, "context": "Direct comparison of AE and CAE models is inappropriate, because a CNN of the same size as a given fully-connected network would have fewer trainable parameters [6]; 6.", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "This is the model which we have investigated in our arXiv paper [14]; Model 2 (Fig.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Model 3 is similar to the networks presented in [27, 29]; Model 4 (Fig.", "startOffset": 48, "endOffset": 56}, {"referenceID": 23, "context": "Model 3 is similar to the networks presented in [27, 29]; Model 4 (Fig.", "startOffset": 48, "endOffset": 56}, {"referenceID": 23, "context": "This approach could be called \u201ccentralized\u201d unpooling, when the max-pooled feature is placed in the center of each pool [29, 47].", "startOffset": 120, "endOffset": 128}, {"referenceID": 34, "context": "This approach could be called \u201ccentralized\u201d unpooling, when the max-pooled feature is placed in the center of each pool [29, 47].", "startOffset": 120, "endOffset": 128}, {"referenceID": 30, "context": "In the Noh implementation [42] we used, this specific position is a left and a top corner [0,0] of each pool.", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Some research results for this unpooling mode are presented in [29]; Model 5 (does not have a Figure), notation (conv, pool (tanh) <-> deconv, unpool (tanh)), the same model as the Model 4 except of using a hyperbolic tangent activation function in all layers.", "startOffset": 63, "endOffset": 67}, {"referenceID": 35, "context": "Size of the models We have used the MNIST dataset [48] for the experimental research.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We have created an HDF5 version of the MNIST dataset and have not applied any modification to input images except for normalization into the range [0,1].", "startOffset": 147, "endOffset": 152}, {"referenceID": 36, "context": "[50] and a deep AE [46], proposed by Hinton et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "A Siamese network consists of two coupled LeNet [6] architectures followed by a contrastive loss function.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "According to our rule 5 in Section 3 above, we have researched a deep AE [5] in our previous paper [14].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "According to our rule 5 in Section 3 above, we have researched a deep AE [5] in our previous paper [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "We have shown that the exact deep AE architecture (1000-500-250-30) presented in [5] provides the best generalization properties out of 5 different models.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "However, the research results of CAE models without pooling \u2013 unpooling layers in [14] have actually shown that a CAE model which is twice as big (in terms of feature maps) gives better results.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Note that our Models 1-5 are half the size (in terms of trainable parameters) of the SWWAE with 679K trainable parameters [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "Since the deconvolution operation has the same nature as convolution [29], we have used the same approach to calculate the number of trainable parameters both in the encoder and decoder parts (Table 3).", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 18, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 19, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 20, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 21, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 22, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 23, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 37, "context": "We used 10-fold cross-validation and calculated an average per-class classification error [52].", "startOffset": 90, "endOffset": 94}, {"referenceID": 38, "context": "Clustering and visualization results We have used the t-SNE technique [55] to visualize 10D and 30D internal code, produced by all models.", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "It is based on Stochastic Neighbor Embedding (SNE) [56] which converts the high-dimensional Euclidean distances between datapoints into conditional probabilities that represent similarities.", "startOffset": 51, "endOffset": 55}, {"referenceID": 38, "context": "This allows t-SNE to have a cost function that is easier to optimize and produces significantly better visualizations [55].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "This advanced technology has been used for visualization in many state-of-the-art problems, for example, to visualize the last hidden-layer representations of a deep Q-network playing Atari 2600 games [57].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "The visualization results from our previous paper [14] have shown that there is no big difference between visualized 10D and 30D internal codes.", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "Similarly to Zeiler and Fergus [45], who stated that visualization allowed them to find model architectures that outperform Krizhevsky et al.", "startOffset": 31, "endOffset": 35}, {"referenceID": 41, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 157, "endOffset": 161}, {"referenceID": 42, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 186, "endOffset": 190}, {"referenceID": 30, "context": "[42] (the date of the files in this version is Jun 16, 2015).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "in their paper [27]: the MNIST dataset is too simple a problem for such a model as Model 3.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "Therefore, we believe that reconstructing MNIST dataset renders insufficient regularization on the encoding pathway\u201d [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "Model 4 restores the max-pooled features in the predefined positions [0, 0] (so-called \u201ccentralized\u201d unpooling [29, 47]) of the unpooled feature maps and, therefore, prevents too much self-adapting.", "startOffset": 111, "endOffset": 119}, {"referenceID": 34, "context": "Model 4 restores the max-pooled features in the predefined positions [0, 0] (so-called \u201ccentralized\u201d unpooling [29, 47]) of the unpooled feature maps and, therefore, prevents too much self-adapting.", "startOffset": 111, "endOffset": 119}, {"referenceID": 13, "context": "Such \u201ccentralized\u201d unpooling plays a role of a peculiar penalty which penalizes sensitivity of a model to the input and encourages a robustness of the learned representation [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "Model 3 has the low-dimensional internal code and the switch variables used for reconstruction [15], but all other unsupervised models have the low-dimensional internal code only.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Model 3 simply learns the identity function in a trivial way and produces useless, uninteresting features; its internal code is overcomplete [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "[25] as the following: \u201cWhat happens is that after a few iterations of parameter update, sampled images become very close to the original ones [which means perfect reconstruction], and the learning signal disappears\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "The last hidden 10D internal code of our Model 3 is so overcomplete, that even the state-of-the-art t-SNE method [55] cannot find any appropriate similarity between the datapoints; it can barely distinguish only two clusters: digit \u201c1\u201d (label \u201c2\u201d, orange) and digit \u201c6\u201d (label \u201c7\u201d, blue).", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 18, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 19, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 21, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 21, "context": "[27] stated in the quotation above that the regularization of their model is insufficient, which means the sparsity is low.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[28], who calculated relative sparsity of feature maps, we have calculated sparsity of the hidden internal code for all our Models (see the last column of Table 4).", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "We have used the sparsity measure [60]", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "The well-known regularization techniques, for example dropout [30] and weight decay [61], aim to improve the sparsity of deep models and prevent trainable parameters from growing too large.", "startOffset": 62, "endOffset": 66}, {"referenceID": 44, "context": "The well-known regularization techniques, for example dropout [30] and weight decay [61], aim to improve the sparsity of deep models and prevent trainable parameters from growing too large.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "The overcompleteness of Model 3\u2019s internal representation makes reconstruction \u201coverly easy\u201d [27], explaining its good reconstruction but poor clustering performance.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "A complementary explanation arises when we realize that the essential function of an AE is compression: finding a low-dimensional code to represent a high-dimensional input [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 109, "endOffset": 124}, {"referenceID": 18, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 109, "endOffset": 124}, {"referenceID": 19, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 109, "endOffset": 124}, {"referenceID": 9, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 232, "endOffset": 240}, {"referenceID": 14, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 17, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 21, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 29, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 45, "context": "In our opinion, it may be necessary to find some correct initialization of a CAE model with a ReLU activation function or use some ideas described in the PyLearn user group [62].", "startOffset": 173, "endOffset": 177}, {"referenceID": 4, "context": "[5], convolutional auto-encoders allow using the desirable properties of convolutional neural networks for image processing tasks while working within an unsupervised learning paradigm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 41, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 157, "endOffset": 161}, {"referenceID": 42, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 186, "endOffset": 190}], "year": 2016, "abstractText": "This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. We have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder\u2019s encoder and decoder parts. Our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks, and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron. The best results were provided by a model where the encoder part contains convolutional and pooling layers, followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. We believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future.", "creator": "Acrobat PDFMaker 11 \u0434\u043b\u044f Word"}}}