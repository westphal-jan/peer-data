{"id": "1701.05053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2017", "title": "Highly Efficient Hierarchical Online Nonlinear Regression Using Second Order Methods", "abstract": "we introduce highly less efficient online nonlinear regression algorithms that are suitable for real life applications. we process the data in a truly online oriented manner such that absolutely no storage is needed, i. to e., after the data is discarded after being used. for nonlinear modeling we use a hierarchical piecewise linear database approach based firstly on the notion of decision trees where the space of the regressor vectors available is adaptively partitioned electronically based on the performance. as perhaps the recommended first time in learning the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly weighted effective second order methods, employing i. e., classic newton - raphson methods. hence, we avoid applying the well known problem over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region automatically are trained using the second order methods, we achieve substantial performance compared to the state of the art. we thoroughly demonstrate our gains over the well known benchmark digital data sets and provide performance results both in an individual sequence manner guaranteed to hold consistent without any statistical assumptions. hence, performing the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic reasoning sense.", "histories": [["v1", "Wed, 18 Jan 2017 13:23:21 GMT  (2163kb,D)", "http://arxiv.org/abs/1701.05053v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["burak c civek", "ibrahim delibalta", "suleyman s kozat"], "accepted": false, "id": "1701.05053"}, "pdf": {"name": "1701.05053.pdf", "metadata": {"source": "CRF", "title": "Highly Efficient Hierarchical Online Nonlinear Regression Using Second Order Methods", "authors": ["Burak C. Civek", "Ibrahim Delibalta", "Suleyman S. Kozat"], "emails": [], "sections": [{"heading": null, "text": "We introduce highly efficient online nonlinear regression algorithms that are suitable for real life applications. We process the data in a truly online manner such that no storage is needed, i.e., the data is discarded after being used. For nonlinear modeling we use a hierarchical piecewise linear approach based on the notion of decision trees where the space of the regressor vectors is adaptively partitioned based on the performance. As the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region are trained using the second order methods, we achieve substantial performance compared to the state of the art. We demonstrate our gains over the well known benchmark data sets and provide performance results in an individual sequence manner guaranteed to hold without any statistical assumptions. Hence, the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic sense.\nKey words: Hierarchical tree, nonlinear regression, online learning, piecewise linear regression, Newton method.\nPreprint submitted to Signal Processing 19 January 2017\nar X\niv :1\n70 1.\n05 05\n3v 1\n[ cs\n.L G\n] 1\n8 Ja"}, {"heading": "1 Introduction", "text": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138]. Today, many sources of information from shares on social networks to blogs, from intelligent device activities to security camera recordings are easily accessible. Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311]. In this paper, we investigate the nonlinear regression problem that is one of the most important topics in the machine learning and signal processing literatures. This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17]. However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].\nIn this context, to accommodate these problems, we introduce online regression algorithms that process the data in an online manner, i.e., instantly, without any storage, and then discard the data after using and learning [18, 19]. Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321]. From a unified point of view, in such problems, we sequentially\n\u2217 Corresponding author. Email addresses: civek@ee.bilkent.edu.tr (Burak C. Civek), ibrahim.delibalta@turktelekom.com.tr (Ibrahim Delibalta), kozat@ee.bilkent.edu.tr (Suleyman S. Kozat).\nobserve a real valued sequence vector sequence x1,x2, . . . and produce a decision (or an action) dt at each time t based on the past x1,x2, . . . ,xt. After the desired output dt is revealed, we suffer a loss and our goal is to minimize the accumulated (and possibly weighted) loss as much as possible while using a limited amount of information from the past.\nTo this end, for nonlinear regression, we use a hierarchical piecewise linear model based on the notion of decision trees, where the space of the regressor vectors, x1,x2, . . ., is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23]. We note that the piecewise linear models are extensively used in the signal processing literature to mitigate the overtraining issues that arise because of using nonlinear models [13]. However their performance in real life applications are less than adequate since their successful application highly depends on the accurate selection of the piecewise regions that correctly model the underlying data [24]. Clearly, such a goal is impossible in an online setting since either the best partition is not known, i.e., the data arrives sequentially, or in real life applications the statistics of the data and the best selection of the regions change in time. To this end, as the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods [25]. Hence, we avoid the well known over fitting issues by using piecewise linear models, moreover, since both the region boundaries as well as the linear models in each region are trained using the second order methods we achieve substantial performance compared to the state of the art [25]. We demonstrate our gains over the well known benchmark data sets extensively used in the machine learning literature. We also provide theoretical performance results in an individual sequence manner that are guaranteed to hold without any statistical assumptions [18]. In this sense, the introduced algorithms address computational complexity issues widely encountered in real life applications\nwhile providing superior guaranteed performance in a strong deterministic sense.\nIn adaptive signal processing literature, there exist methods which develop an approach based on weighted averaging of all possible models of a tree based partitioning instead of solely relying on a particular piecewise linear model [23, 24]. These methods use the entire partitions of the regressor space and implement a full binary tree to form an online piecewise linear regressor. Such approaches are confirmed to lessen the bias variance trade off in a deterministic framework [23, 24]. However, these methods do not update the corresponding partitioning of the regressor space based on the upcoming data. One such example is that the recursive dyadic partitioning, which partitions the regressor space using separation functions that are required to be parallel to the axes [26]. Moreover, these methods usually do not provide a theoretical justification for the weighting of the models, even if there exist inspirations from information theoretic deliberations [27]. For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds). Specifically, these methods are constructed in such a way that there is a significant correlation between the weighting coefficients, algorithmic parameters and their performance, i.e., one should adjust these parameters to the specific application for successful process [24]. Besides these approaches, there exists an algorithm providing adaptive tree structure for the partitions, e.g., the Decision Adaptive Tree (DAT) [30]. The DAT produces the final estimate using the weighted average of the outcomes of all possible subtrees, which results in a computational complexity of O(m4d), where m is the data dimension and d represents the depth. However, this would affect the computational efficiency adversely for the cases involving highly nonlinear structures. In this work, we propose a different approach that avoids combining the prediction of each sub-\ntrees and offers a computational complexity of O(m22d). Hence, we achieve an algorithm that is more efficient and effective for the cases involving higher nonlinearities, whereas the DAT is more feasible when the data dimension is quite high. Moreover, we illustrate in our experiments that our algorithm requires less number of data samples to capture the underlying data structure. Overall, the proposed methods are completely generic such that they are capable of incorporating all Recursive Dyadic, Random Projection (RP) and k-d trees in their framework, e.g., we initialize the partitioning process by using the RP trees and adaptively learn the complete structure of the tree based on the data progress to minimize the final error.\nIn Section 2, we first present the main framework for nonlinear regression and piecewise linear modeling. In Section 3, we propose three algorithms with regressor space partitioning and present guaranteed upper bounds on the performances. These algorithms adaptively learn the partitioning structure, region boundaries and region regressors to minimize the final regression error. We then demonstrate the performance of our algorithms through widely used benchmark data sets in Section 4. We then finalize our paper with concluding remarks."}, {"heading": "2 Problem Description", "text": "In this paper, all vectors are column vectors and represented by lower case boldface letters. For matrices, we use upper case boldface letters. The `2- norm of a vector x is given by \u2016x\u2016= \u221a xTx where xT denotes the ordinary transpose. The identity matrix with n\u00d7 n dimension is represented by In.\nWe work in an online setting, where we estimate a data sequence yt \u2208 R at time t \u2265 1 using the corresponding observed feature vector xt \u2208 Rm and then discard xt without any storage. Our goal is to sequentially estimate yt using\nxt as\ny\u0302t = ft(xt)\nwhere ft(\u00b7) is a function of past observations. In this work, we use nonlinear functions to model yt, since in most real life applications, linear regressors are inadequate to successively model the intrinsic relation between the feature vector xt and the desired data yt [31]. Different from linear regressors, nonlinear functions are quite powerful and usually overfit in most real life cases [32]. To this end, we choose piecewise linear functions due to their capability of approximating most nonlinear models [33]. In order to construct a piecewise linear model, we partition the space of regressor vectors into K\ndistinct m-dimensional regions Smk , where \u22c3K k=1 S m k = R m and Smi \u2229 Smj = \u2205 when i 6= j. In each region, we use a linear regressor, i.e., y\u0302t,i = wTt,ixt + ct,i, where wt,i is the linear regression vector, ct,i is the offset and y\u0302t,i is the estimate corresponding to the ith region. We represent y\u0302t,i in a more compact form as y\u0302t,i = w T t,ixt, by including a bias term into each weight vector wt,i and increasing the dimension of the space by 1, where the last entry of xt is always set to 1.\nTo clarify the framework, in Fig. 1, we present a one dimensional regression problem, where we generate the data sequence using the nonlinear model\nyt = exp ( xt sin(4\u03c0xt) ) + \u03bdt,\nwhere xt is a sample function from an i.i.d. standard uniform random process and \u03bdt has normal distribution with zero mean and 0.1 variance. Here, we demonstrate two different cases to emphasize the difficulties in piecewise linear modeling. For the case given in the upper plot, we partition the regression space into three regions and fit linear regressors to each partition. However, this construction does not approximate the given nonlinear model well enough since the underlying partition does not match exactly to the data. In order to better model the generated data, we use the second model as shown in the\nlower plot, where we have eight regions particularly selected according to the distribution of the data points. As the two cases signified in Fig. 1 imply, there are two major problems when using piecewise linear models. The first one is to determine the piecewise regions properly. Randomly selecting the partitions causes inadequately approximating models as indicated in the underfitting case on the top of Fig. 1 [22]. The second problem is to find out the linear model that best fits the data in each distinct region in a sequential manner [24]. In this paper, we solve both of these problems using highly effective and completely adaptive second order piecewise linear regressors.\nIn order to have a measure on how well the determined piecewise linear model fits the data, we use instantaneous squared loss, i.e., e2t = (yt\u2212 y\u0302t)2 as our cost function. Our goal is to specify the partitions and the corresponding linear\nregressors at each iteration such that the total regression error is minimized. Suppose w\u2217n represents the optimal fixed weight for a particular region after n iteration, i.e.,\nw\u2217n = arg min w n\u2211 t=1 e2t (w).\nHence, we would achieve the minimum possible regression error, if we have been considering w\u2217n as the fixed linear regressor weight up to the current iteration, n. However, we do not process batch data sets, since the framework is online, and thus, cannot know the optimal weight beforehand [18]. This lack of information motivates us to implement an algorithm such that we achieve an error rate as close as the possible minimum after n iteration. At this point, we define the regret of an algorithm to measure how much the total error diverges from the possible minimum achieved by w\u2217n, i.e.,\nRegret(A) = n\u2211 t=1 e2t (wt)\u2212 n\u2211 t=1 e2t (w \u2217 n),\nwhere A denotes the algorithm to adjust wt at each iteration. Eventually, we consider the regret criterion to measure the modeling performance of the designated piecewise linear model and aim to attain a low regret [18].\nIn the following section, we propose three different algorithms to sufficiently model the intrinsic relation between the data sequence yt and the linear regressor vectors. In each algorithm, we use piecewise linear models, where we partition the space of regressor vectors by using linear separation functions and assign a linear regressor to each partition. At this point, we also need to emphasize that we propose generic algorithms for nonlinear modeling. Even though we employ linear models in each partition, it is also possible to use, for example, spline modeling within the presented settings. This selection would cause additional update operations with minor changes for the higher order terms. Therefore, the proposed approaches can be implemented by using any other function that is differentiable without a significant difference in the algorithm, hence, they are universal in terms of the possible selection of functions.\nOverall, the presented algorithms ensure highly efficient and effective learning performance, since we perform second order update methods, e.g. Online Newton Step [34], for training of the region boundaries and the linear models."}, {"heading": "3 Highly Efficient Tree Based Sequential Piecewise Linear Predic-", "text": "tors\nIn this section, we introduce three highly effective algorithms constructed by piecewise linear models. The presented algorithms provide efficient learning even for highly nonlinear data models. Moreover, continuous updating based on the upcoming data ensures our algorithms to achieve outstanding performance for online frameworks. Furthermore, we also provide a regret analysis for the introduced algorithms demonstrating strong guaranteed performance.\nThere exist two essential problems of piecewise linear modeling. The first significant issue is to determine how to partition the regressor space. We carry out the partitioning process using linear separation functions. We specify the separation functions as hyperplanes, which are (m\u22121)-dimensional subspaces of m-dimensional regression space and identified by their normal vectors as shown in Fig. 2. To get a highly versatile and data adaptive partitioning, we also train the region boundaries by updating corresponding normal vectors. We denote the separation functions as pt,k and the normal vectors as nt,k where k is the region label as we demonstrate in Fig. 2. In order to adaptively\npt,k = 1\n1 + e\u2212x T t nt,k\n(1)\nwhere the offset ct,k is included in the norm vector nt,k as a bias term. In Fig. 3, logistic regression functions for 1-dimensional case are shown for different parameters. Following the partitioning process, the second essential problem is to find out the linear models in each region. We assign a linear regressor specific to each distinct region and generate a corresponding estimate y\u0302t,r, given by\ny\u0302t,r = w T t,rxt (2)\nwhere wt,r is the regression vector particular to region r. In the following subsections, we present different methods to partition the regressor space to construct our algorithms."}, {"heading": "3.1 Partitioning Methods", "text": "We introduce two different partitioning methods: Type 1, which is a straightforward partitioning and Type 2, which is an efficient tree structured partitioning."}, {"heading": "3.1.1 Type 1 Partitioning", "text": "In this method, we allow each hyperplane to divide the whole space into two subspaces as shown in Fig. 2. In order to clarify the technique, we work on the 2-dimensional space, i.e., the coordinate plane. Suppose, the observed feature vectors xt = [xt,1, xt,2] T come from a bounded set {\u2126} such that \u2212A \u2264 xt,1, xt,2 \u2264 A for some A > 0, as shown in Fig. 2. We define 1-dimensional hyperplanes, whose normal vector representation is given by nt,k \u2208 R2 where k denotes the corresponding region identity. At first, we have the whole space as a single set {\u2126}. Then we use a single separation function, which is a line in this case, to partition this space into subspaces {0} and {1} such that {0} \u222a {1} = {\u2126}. When we add another hyperplane separating the set \u2126, we get four distinct subspaces {00}, {01}, {10} and {11} where their union forms the initial regression space. The number of separated regions increases by O(k2). Note that if we use k different separation functions, then we can obtain up to k 2+k+2\n2 distinct regions forming a complete space."}, {"heading": "3.1.2 Type 2 Partitioning", "text": "In the second method, we use the tree notion to partition the regression space, which is a more systematic way to determine the regions [13,22]. We illustrate this method in Fig. 4 for 2-dimensional case. First step is the same as previously mentioned approach, i.e., we partition the whole regression space into two distinct regions using one separation function. In the following steps, the partition technique is quite different. Since we have two distinct subspaces after the first step, we work on them separately, i.e., the partition process continues recursively in each subspace independent of the others. Therefore, adding one more hyperplane has an effect on just a single region, not on the whole space. The number of distinct regions in total increases by 1, when we apply one more separation function. Thus, in order to represent p+ 1 distinct regions, we specify p separation functions. For the tree case, we use another\nidentifier called the depth, which determines how deep the partition is, e.g. depth of the model shown in Fig. 4 is 2. In particular, the number of different regions generated by the depth-d models are given by 2d. Hence, the number of distinct regions increases in the order of O(2d). For the tree based partitioning, we use the finest model of a depth-d tree. The finest partition consists of the regions that are generated at the deepest level, e.g. regions {00}, {01}, {10} and {11} as shown in Fig. 4.\nBoth Type 1 and Type 2 partitioning have their own advantages, i.e., Type 2 partitioning achieves a better steady state error performance since the models generated by Type 1 partitioning are the subclasses of Type 2, however, Type 1 might perform better in the transient region since it uses less parameters."}, {"heading": "3.2 Algorithm for Type 1 Partitioning", "text": "In this part, we introduce our first algorithm, which is based on the Type 1 partitioning. Following the model given in Fig. 2, say, we have two different separator functions, pt,0, pt,1 \u2208 R, which are defined by nt,0,nt,1 \u2208 R2\nrespectively. For the region {00}, the corresponding estimate is given by\ny\u0302t,00 = w T t,00xt,\nwhere wt,00 \u2208 R2 is the regression vector of the region {00}. Since we have the estimates of all regions, the final estimate is given by\ny\u0302t = pt,0pt,1y\u0302t,00 + pt,0(1\u2212 pt,1)y\u0302t,01\n+ (1\u2212 pt,0)pt,1y\u0302t,10 + (1\u2212 pt,0)(1\u2212 pt,1)y\u0302t,11 (3)\nwhen we observe the feature vector xt. This result can be easily extended to the cases where we have more then 2 separator functions.\nWe adaptively update the weights associated with each partition based on the overall performance. Boundaries of the regions are also updated to reach the best partitioning. We use the second order algorithms, e.g. Online Newton Step [34], to update both separator functions and region weights. To accomplish this, the weight vector assigned to the region {00} is updated as\nwt+1,00 = wt,00 \u2212 1\n\u03b2 A\u22121t \u2207e2t\n= wt,00 + 2\n\u03b2 etpt,0pt,1A\n\u22121 t xt,\n(4)\nwhere \u03b2 is the step size, \u2207 is the gradient operator w.r.t. wt,00 and At is an m\u00d7m matrix defined as\nAt = t\u2211 i=1 \u2207i\u2207Ti + Im, (5)\nwhere \u2207t , \u2207e2t and > 0 is used to ensure that At is positive definite, i.e., At > 0, and invertible. Here, the matrix At is related to the Hessian of the error function, implying that the update rule uses the second order information [34].\nRegion boundaries are also updated in the same manner. For example, the\ndirection vector specifying the separation function pt,0 in Fig. 2, is updated as\nnt+1,0 = nt,0 \u2212 1\n\u03b7 A\u22121t \u2207e2t\n= nt,0 + 2\n\u03b7 et[pt,1y\u0302t,00 + (1\u2212 pt,1)y\u0302t,01\n\u2212 pt,1y\u0302t,10 \u2212 (1\u2212 pt,1)y\u0302t,11]A\u22121t \u2202pt,0 \u2202nt,0 ,\n(6)\nwhere \u03b7 is the step size to be determined, \u2207 is the gradient operator w.r.t. nt,0 and At is given in (5). Partial derivative of the separation function pt,0 w.r.t. nt,0 is given by\n\u2202pt,0 \u2202nt,0\n= xte \u2212xTt nt,0\n(1 + e\u2212x T t nt,0)2\n. (7)\nAll separation functions are updated in the same manner. In general, we derive the final estimate in a compact form as\ny\u0302t = \u2211 r\u2208R \u03c8\u0302t,r, (8)\nwhere \u03c8\u0302t,r is the weighted estimate of region r and R represents the set of all region labels, e.g. R = {00, 01, 10, 11} for the case given in Fig. 2. Weighted estimate of each region is determined by\n\u03c8\u0302t,r = y\u0302t,r K\u220f i=1 p\u0302t,P (i), (9)\nwhere K is the number of separation functions, P represents the set of all separation function labels and P (i) is the ith element of set P , e.g. P = {0, 1}, P (1) = 0, and p\u0302t,P (i) is defined as\np\u0302t,P (i) =  pt,P (i) , r(i) = 0\n1\u2212 pt,P (i) , r(i) = 1 , (10)\nwhere r(i) denotes the ith binary character of label r, e.g. r = 10 and r(1) = 1. We reformulate the update rules defined in (4) and (6) and present generic expressions for both regression weights and region boundaries. The derivations of the generic update rules are calculated after some basic algebra. Hence, the\nAlgorithm 1 Straight Partitioning\n1: A\u221210 = 1 Im 2: for t\u2190 1, n do 3: y\u0302t \u2190 0 4: for all r \u2208 R do 5: y\u0302t,r \u2190 wTt,rxt 6: \u03c8\u0302t,r \u2190 y\u0302t,r 7: \u2207t,r \u2190 xt 8: for i\u2190 1,K do 9: if r(i) := 0 then\n10: p\u0302t,P (i) \u2190 pt,P (i) 11: else 12: p\u0302t,P (i) \u2190 1\u2212 pt,P (i) 13: end if 14: \u03c8\u0302t,r \u2190 \u03c8\u0302t,r p\u0302t,P (i) 15: \u2207t,r \u2190 \u2207t,r p\u0302t,P (i) 16: end for 17: for i\u2190 1,K do 18: \u03b1t,P (i) \u2190 (\u22121)r(i)(\u03c8\u0302t,r/p\u0302t,P (i))\n19: end for 20: y\u0302t \u2190 y\u0302t + \u03c8\u0302t,r 21: end for 22: et \u2190 yt \u2212 y\u0302t 23: for all r \u2208 R do 24: \u2207t,r \u2190 \u22122et\u2207t,r 25: A\u22121t,r \u2190 A \u22121 t\u22121,r\u2212 A\u22121t\u22121,r\u2207t,r\u2207 T t,rA \u22121 t\u22121,r\n1 +\u2207Tt,rA \u22121 t\u22121,r\u2207t,r\n26: wt+1,r \u2190 wt,r \u2212 1 \u03b2 A\u22121t,r\u2207t,r 27: end for 28: for i\u2190 1,K do 29: k \u2190 P (i) 30: \u2207t,k \u2190 \u22122et\u03b1t,kpt,k(1\u2212 pt,k)xt 31: A\u22121 t,k \u2190 A\u22121 t\u22121,k\u2212 A\u22121 t\u22121,k\u2207t,k\u2207 T t,kA \u22121 t\u22121,k\n1 +\u2207T t,k A\u22121 t\u22121,k\u2207t,k\n32: nt+1,k \u2190 nt,k \u2212 1 \u03b7 A\u22121 t,k \u2207t,k 33: end for 34: end for\nregression weights are updated as\nwt+1,r = wt,r + 2\n\u03b2 etA\n\u22121 t xt K\u220f i=1 p\u0302t,P (i) (11)\nand the region boundaries are updated as\nnt+1,k = nt,k + 2\n\u03b7 etA\n\u22121 t [\u2211 r\u2208R y\u0302t,r(\u22121)r(i) K\u220f j=1 j 6=i p\u0302t,P (j) ] xte \u2212xTt nt,k (1 + e\u2212x T t nt,k)2 , (12)\nwhere we assign k = P (i), i.e., separation function with label-k is the ith entry of set P . Partial derivative of the logistic regression function pt,k w.r.t. nt,k is also inserted in (12). In order to avoid taking the inverse of an m\u00d7m matrix, At, at each iteration in (11) and (12), we generate a recursive formula using matrix inversion lemma for A\u22121t given as [9]\nA\u22121t = A \u22121 t\u22121 \u2212 A\u22121t\u22121\u2207t\u2207Tt A\u22121t\u22121 1 +\u2207Tt A\u22121t\u22121\u2207t , (13)\nwhere \u2207t , \u2207e2t w.r.t. the corresponding variable. The complete algorithm for Type 1 partitioning is given in Algorithm 1 with all updates and initializations."}, {"heading": "3.3 Algorithm for Type 2 Partitioning", "text": "In this algorithm, we use another approach to estimate the desired data. The partition of the regressor space will be based on the finest model of a tree structure [13, 23]. We follow the case given in Fig. 4. Here, we have three separation functions, pt,\u03b5, pt,0 and pt,1, partitioning the whole space into four subspaces. The corresponding direction vectors are given by nt,\u03b5,nt,0 and nt,1 respectively. Using the individual estimates of all four regions, we find the final estimate by\ny\u0302t = pt,\u03b5pt,0y\u0302t,00 + pt,\u03b5(1\u2212 pt,0)y\u0302t,01\n+ (1\u2212 pt,\u03b5)pt,1y\u0302t,10 + (1\u2212 pt,\u03b5)(1\u2212 pt,1)y\u0302t,11 (14)\nwhich can be extended to depth-d models with d > 2.\nRegressors of each region is updated similar to the first algorithm. We demonstrate a systematic way of labeling for partitions in Fig. 5. The final estimate of this algorithm is given by the following generic formula\ny\u0302t = 2d\u2211 j=1 \u03c8\u0302t,Rd(j) (15)\nwhere Rd is the set of all region labels with length d in the increasing order for, i.e., R1 = {0, 1} or R2 = {00, 01, 10, 11} and Rd(j) represents the jth entry of set Rd. Weighted estimate of each region is found as\n\u03c8\u0302t,r = y\u0302t,r d\u220f i=1 p\u0302t,ri (16)\nwhere ri denotes the first i \u2212 1 character of label r as a string, i.e., r = {0101}, r3 = {01} and r1 = { }, which is the empty string { }. Here, p\u0302t,ri is defined as\np\u0302t,ri =  pt,ri , r(i) = 0\n1\u2212 pt,ri , r(i) = 1 . (17)\nUpdate rules for the region weights and the boundaries are given as a generic form and the derivations of these updates are obtained after some basic algebra. Regressor vectors are updated as\nwt+1,r = wt,r + 2\n\u03b2 etAtxt d\u220f i=1 p\u0302t,ri (18)\nand the separator function updates are given by\nnt+1,k = nt,k + 2\n\u03b7 etA\n\u22121 t [ 2d\u2212`(k)\u2211 j=1 y\u0302t,r(\u22121)r(`(k)+1) d\u220f i=1 ri 6=k p\u0302t,ri ] \u2202pt,k \u2202nt,k\n(19)\nwhere r is the label string generated by concatenating separation function id k and the label kept in jth entry of the set R(d\u2212`(k)), i.e., r = [k;R(d\u2212`(k))(j)] and `(k) represents the length of binary string k, e.g. `(01) = 2. The partial derivative of pt,k w.r.t. nt,k is the same expression given in (14). The complete algorithm for Type 2 partitioning is given in Algorithm 2 with all updates and initializations."}, {"heading": "3.4 Algorithm for Combining All Possible Models of Tree", "text": "In this algorithm, we combine the estimates generated by all possible models of a tree based partition, instead of considering only the finest model. The main\nAlgorithm 2 Finest Model Partitioning\n1: A\u221210 \u2190 1 Im 2: for t\u2190 1, n do 3: y\u0302t \u2190 0 4: for j \u2190 1, 2d do 5: r \u2190 Rd(j) 6: y\u0302t,r \u2190 wTt,rxt 7: \u03c8\u0302t,r \u2190 y\u0302t,r 8: \u03b3t,r \u2190 1 9: for i\u2190 1, d do\n10: if r(i)\u2190 0 then 11: p\u0302t,ri \u2190 pt,ri 12: else 13: p\u0302t,ri \u2190 1\u2212 pt,ri 14: end if 15: \u03c8\u0302t,r \u2190 \u03c8\u0302t,r p\u0302t,ri 16: \u03b3t,r \u2190 \u03b3t,r p\u0302t,ri 17: end for 18: y\u0302t \u2190 y\u0302t + \u03c8\u0302t,r 19: end for 20: for i\u2190 1, 2d \u2212 1 do 21: k \u2190 P (i)\n22: for j \u2190 1, 2d\u2212`(k) do 23: r \u2190 concat[k : Rd\u2212`(k)(j)] 24: \u03b1t,k \u2190 (\u22121)r(`(k)+1)(\u03c8\u0302t,r/p\u0302t,k) 25: end for 26: end for 27: et \u2190 yt \u2212 y\u0302t 28: for j \u2190 1, 2d do 29: r \u2190 Rd(j) 30: \u2207t,r \u2190 \u22122et\u03b3t,rxt 31: A\u22121t,r \u2190 A \u22121 t\u22121,r\u2212 A\u22121t\u22121\u2207t,r\u2207 T t,rA \u22121 t\u22121,r\n1 +\u2207Tt,rA \u22121 t\u22121,r\u2207t,r\n32: wt+1,r \u2190 wt,r \u2212 1 \u03b2 A\u22121t,r\u2207t,r 33: end for 34: for i\u2190 1, 2d \u2212 1 do 35: k \u2190 P (i) 36: \u2207t,k \u2190 \u22122et\u03b1t,kpt,k(1\u2212 pt,k)xt 37: A\u22121 t,k \u2190 A\u22121 t\u22121,k\u2212 A\u22121 t\u22121,k\u2207t,k\u2207 T t,kA \u22121 t\u22121,k\n1 +\u2207T t,k A\u22121 t\u22121,k\u2207t,k\n38: nt+1,k \u2190 nt,k \u2212 1 \u03b7 A\u22121 t,k \u2207t,k 39: end for 40: end for\ngoal of this algorithm is to illustrate that using only the finest model of a depthd tree provides a better performance. For example, we represent the possible models corresponding to a depth-2 tree in Fig. 6. We emphasize that the last partition is the finest model we use in the previous algorithm. Following the case in Fig. 6, we generate five distinct piecewise linear models and estimates of these models. The final estimate is then constructed by linearly combining the outputs of each piecewise linear model, represented by \u03c6\u0302t,\u03bb, where \u03bb represents the model identity. Hence, y\u0302t is given by\ny\u0302t = \u03c5 T t \u03c6\u0302t (20)\nwhere \u03c6\u0302t = [\u03c6\u0302t,1, \u03c6\u0302t,2, ..., \u03c6\u0302t,M ] T , \u03c5t \u2208 RM is the weight vector and M represents the number of possible distinct models generated by a depth-d tree, e.g. M = 5 for depth-2 case. In general, we have M \u2248 (1.5)2d . Model estimates, \u03c6\u0302t,\u03bb, are calculated in the same way as in Section 3.3. Linear combination weights, vt, are also adaptively updated using the second order methods as\nperformed in the previous sections."}, {"heading": "3.5 Computational Complexities", "text": "In this section, we determine the computational complexities of the proposed algorithms. In the algorithm for Type 1 partitioning, the regressor space is partitioned into at most k 2+k+2\n2 regions by using k distinct separator function.\nThus, this algorithm requires O(k2) weight update at each iteration. In the algorithm for Type 2 partitioning, the regressor space is partitioned into 2d regions for the depth-d tree model. Hence, we perform O(2d) weight update at each iteration. The last algorithm combines all possible models of depthd tree and calculates the final estimate in an efficient way requiring O(4d) weight updates [30]. Suppose that the regressor space is m-dimensional, i.e., xt \u2208 Rm. For each update, all three algorithms require O(m2) multiplication and addition resulting form a matrix-vector product, since we apply second order update methods. Therefore, the corresponding complexities are O(m2k2), O(m22d) and O(m24d) for the Algorithm 1, the Algorithm 2 and the Algorithm 3 respectively. In Table 1, we represent the computational complexities of the existing algorithms. \u201dFMP\u201d and \u201dSP\u201d represents Finest Model Partitioning and Straight Partitioning algorithms respectively. \u201dDFT\u201d stands for Decision Fixed Tree and \u201dDAT\u201d represents Decision Adaptive Tree [30]. \u201dS-DAT\u201d denotes the Decision Adaptive Tree with second order update rules. \u201dCTW\u201d is used for Context Tree Weighting [24], \u201dGKR\u201d represents Gaussian-Kernel re-\ngressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively."}, {"heading": "3.6 Logarithmic Regret Bound", "text": "In this subsection, we provide regret results for the introduced algorithms. All three algorithms uses the second order update rule, Online Newton Step [34], and achieves a logarithmic regret when the normal vectors of the region boundaries are fixed and the cost function is convex in the sense of individual region weights. In order to construct the upper bounds, we first let w\u2217n be the best predictor in hindsight, i.e.,\nw\u2217n = arg min w n\u2211 t=1 e2t (w) (21)\nand express the following inequality\ne2t (wt)\u2212 e2t (w\u2217n) \u2264 \u2207Tt (wt \u2212w\u2217n)\u2212 \u03b2\n2 (wt \u2212w\u2217n)T\u2207t\u2207Tt (wt \u2212w\u2217n) (22)\nusing the Lemma 3 of [34], since our cost function is \u03b1-exp-concave, i.e., exp(\u2212\u03b1e2t (wt)) is concave for \u03b1 > 0 and has an upper bound G on its gradient, i.e., \u2016\u2207t\u2016 \u2264 G. We give the update rule for regressor weights as\nwt+1 = wt \u2212 1\n\u03b2 A\u22121t \u2207t. (23)\nWhen we subtract the optimal weight from both sides, we get\nwt+1 \u2212w\u2217n = wt \u2212w\u2217n \u2212 1\n\u03b2 A\u22121t \u2207t (24)\nAt(wt+1 \u2212w\u2217n) = At(wt \u2212w\u2217n)\u2212 1\n\u03b2 \u2207t (25)\nand multiply second equation with the transpose of the first equation to get\n\u2207t(wt \u2212w\u2217n) = 1\n2\u03b2 \u2207Tt A\u22121t \u2207t +\n\u03b2 2 (wt \u2212w\u2217n)TAt(wt \u2212w\u2217n)\n\u2212 \u03b2 2\n(wt+1 \u2212w\u2217n)TAt(wt+1 \u2212w\u2217n). (26)\nBy following a similar discussion [34], except that we have equality in (26) and in the proceeding parts, we achieve the inequality\nn\u2211 t=1 St \u2264 1 2\u03b2 n\u2211 t=1 \u2207Tt A\u22121t \u2207t + \u03b2 2 (w1 \u2212w\u2217n)TA0(w1 \u2212w\u2217n), (27)\nwhere St is defined as\nSt , \u2207Tt (wt \u2212w\u2217n)\u2212 \u03b2\n2 (wt \u2212w\u2217n)T\u2207t\u2207Tt (wt \u2212w\u2217n). (28)\nSince we define A0 = Im and have a finite space of regression vectors, i.e., \u2016wt \u2212w\u2217n\u20162 \u2264 A2, we get\nn\u2211 t=1 e2t (wt)\u2212 n\u2211 t=1 e2t (w \u2217 n) \u2264 1 2\u03b2 n\u2211 t=1 \u2207Tt A\u22121t \u2207t + \u03b2 2 \u03b42\n\u2264 1 2\u03b2 n\u2211 t=1 \u2207Tt A\u22121t \u2207t + 1 2\u03b2 ,\n(29)\nwhere we choose = 1 \u03b22A2 and use the inequalities (10) and (17). Now, we specify an upper bound for the first term in LHS of the inequality (19). We make use of Lemma 11 given in [34], to get the following bound\n1\n2\u03b2 n\u2211 t=1 \u2207Tt A\u22121t \u2207t \u2264 m 2\u03b2 log\n( G2n\n+ 1 ) = m\n2\u03b2 log(G2n\u03b22A2 + 1) \u2264 m 2\u03b2 log(n),\n(30)\nwhere in the last inequality, we use the choice of \u03b2, i.e., \u03b2 = 1 2 min{ 1 4GA , \u03b1}, which implies that 1 \u03b2 \u2264 8(GA+ 1 \u03b1 ). Therefore, we present the final logarithmic\nregret bound as\nn\u2211 t=1 e2t (wt)\u2212 n\u2211 t=1 e2t (w \u2217 n) \u2264 5\n( GA+ 1\n\u03b1\n) m log(n). (31)"}, {"heading": "4 Simulations", "text": "In this section, we evaluate the performance of the proposed algorithms under different scenarios. In the first set of simulations, we aim to provide a better understanding of our algorithms. To this end, we first consider the regression of a signal that is generated by a piecewise linear model whose partitions match the initial partitioning of our algorithms. Then we examine the case of mismatched initial partitions to illustrate the learning process of the presented algorithms. As the second set of simulation, we mainly assess the merits of our algorithms by using the well known real and synthetic benchmark datasets that are extensively used in the signal processing and the machine learning literatures, e.g., California Housing [38], Kinematics [38] and Elevators [38]. We then perform two more experiments with two chaotic processes, e.g., the Gauss map and the Lorenz attractor, to demonstrate the merits of our algorithms. All data sequences used in the simulations are scaled to the range [\u22121, 1] and the learning rates are selected to obtain the best steady state performance of each algorithm."}, {"heading": "4.1 Matched Partition", "text": "In this subsection, we consider the regression of a signal generated using a piecewise linear model whose partitions match with the initial partitioning of the proposed algorithms. The main goal of this experiment is to provide an insight on the working principles of the proposed algorithms. Hence, this experiment is not designated to assess the performance of our algorithms with\nrespect to the ones that are not based on piecewise linear modeling. This is only an illustration of how it is possible to achieve a performance gain when the data sequence is generated by a nonlinear system.\nWe use the following piecewise linear model to generate the data sequence,\ny\u0302t =  wT1 xt + \u03c5t ,x T t n0 \u2265 0 and xTt n1 \u2265 0 wT2 xt + \u03c5t ,x T t n0 \u2265 0 and xTt n1 < 0 wT2 xt + \u03c5t ,x T t n0 < 0 and x T t n1 \u2265 0\nwT1 xt + \u03c5t ,x T t n0 < 0 and x T t n1 < 0\n(32)\nwhere w1 = [1, 1] T , w2 = [\u22121,\u22121]T , n0 = [1, 0]T and n1 = [0, 1]T . The feature vector xt = [xt,1, xt,2] T is composed of two jointly Gaussian processes with [0, 0]T mean and I2 variance. \u03c5t is a sample taken from a Gaussian process with zero mean and 0.1 variance. The generated data sequence is represented by y\u0302t. In this scenario, we set the learning rates to 0.125 for the FMP, 0.0625 for the SP, 0.005 for the S-DAT, 0.01 for the DAT, 0.5 for the GKR, 0.004 for the CTW, 0.025 for the VF and the EMFNF, 0.005 for the FNF.\nIn Fig. 7, we represent the deterministic error performance of the specified algorithms. The algorithms VF, EMFNF, GKR and FNF cannot capture the characteristic of the data model, since these algorithms are constructed to achieve satisfactory results for smooth nonlinear models, but we examine a highly nonlinear and discontinuous model. On the other hand, the algorithms FMP, SP, S-DAT, CTW and DAT attain successive performance due to their capability of handling highly nonlinear models. As seen in Fig. 7, our algorithms, the FMP and the SP, significantly outperform their competitors and achieve almost the same performance result, since the data distribution is completely captured by both algorithms. Although the S-DAT algorithm does not perform as well as the FMP and the SP algorithms, still obtains a better convergence rate compared to the DAT and the CTW algorithms."}, {"heading": "4.2 Mismatched Partition", "text": "In this subsection, we consider the case where the desired data is generated by a piecewise linear model whose partitions do not match with the initial partitioning of the proposed algorithms. This experiment mainly focuses on to demonstrate how the proposed algorithms learn the underlying data structure. We also aim to emphasize the importance of adaptive structure.\nWe use the following piecewise linear model to generate the data sequence,\ny\u0302t =  wT1 xt + \u03c5t ,x T t n0 \u2265 0.5 and xTt n1 \u2265 \u22120.5 wT2 xt + \u03c5t ,x T t n0 \u2265 0.5 and xTt n1 < \u22120.5 wT2 xt + \u03c5t ,x T t n0 < 0.5 and x T t n2 \u2265 \u22120.5\nwT1 xt + \u03c5t ,x T t n0 < 0.5 and x T t n2 < \u22120.5\n(33)\nwhere w1 = [1, 1] T , w2 = [1,\u22121]T , n0 = [2,\u22121]T , n1 = [\u22121, 1]T and n2 = [2, 1]T . The feature vector xt = [xt,1, xt,2] T is composed of two jointly Gaussian processes with [0, 0]T mean and I2 variance. \u03c5t is a sample taken from a Gaus-\nsian process with zero mean and 0.1 variance. The generated data sequence is represented by y\u0302t. The learning rates are set to 0.04 for the FMP, 0.025 for the SP, 0.005 for the S-DAT, the CTW and the FNF, 0.025 for the EMFNF and the VF, 0.5 for the GKR.\nIn Fig. 8, we demonstrate the normalized time accumulated error performance of the proposed algorithms. Different from the matched partition scenario, we emphasize that the CTW algorithm performs even worse than the VF, the FNF and the EMFNF algorithms, which are not based on piecewise linear modeling. The reason is that the CTW algorithm has fixed regions that are mismatched with the underlying partitions. Besides, the adaptive algorithms, FMP, SP, S-DAT and DAT achieve considerably better performance, since these algorithms update their partitions in accordance with the data distribution. Comparing these four algorithms, Fig. 8 exhibits that the FMP notably outperforms its competitors, since this algorithm exactly matches its partitioning to the partitions of the piecewise linear model given in (33).\nWe illustrate how the FMP and the DAT algorithms update their region\nboundaries in Fig. 9. Both algorithms initially partition the regression space into 4 equal quadrant, i.e., the cases shown in t = 0. We emphasize that when the number of iterations reaches 10000, i.e., t = 10000, the FMP algorithm trains its region boundaries such that its partitions substantially match the partitioning of the piecewise linear model. However, the DAT algorithm cannot capture the data distribution yet, when t = 10000. Therefore, the FMP algorithm, which uses the second order methods for training, has a faster convergence rate compared to the DAT algorithm, which updates its region boundaries using first order methods."}, {"heading": "4.3 Real and Synthetic Data Sets", "text": "In this subsection, we mainly focus on assessing the merits of our algorithms. We first consider the regression of a benchmark real-life problem that can be found in many data set repositories such as: California Housing, which is an m = 8 dimensional database consisting of the estimations of median house prices in the California area [38]. There exist more than 20000 data samples for this dataset. For this experiment, we set the learning rates to 0.004 for\nFMP and SP, 0.01 for the S-DAT and the DAT, 0.02 for the CTW, 0.05 for the VF, 0.005 for the FNF and the EMFNF. Fig. 10 illustrates the normalized time accumulated error rates of the stated algorithms. We emphasize that the FMP and the SP significantly outperforms the state of the art.\nWe also consider two more real and synthetic data sets. The first one is Kinematics, which is an m = 8 dimensional dataset where a realistic simulation of an 8 link robot arm is performed [38]. The task is to predict the distance of the end-effector from a target. There exist more than 50000 data samples. The second one is Elevators, which has an m = 16 dimensional data sequence obtained from the task of controlling an F16 aircraft [38]. This dataset provides more than 50000 samples. In Fig. 11, we present the steady state error performances of the proposed algorithms. We emphasize that our algorithms achieve considerably better performance compared to the others for both datasets.\nSpecial to this subsection, we perform an additional experiment using the Kinematics dataset to illustrate the effect of using second order methods for the adaptation. Usually, algorithms like CTW, FNF, EMFNF, VF and DAT\nuse the gradient based first order methods for the adaptation algorithm due to their low computational demand. Here, we modified the adaptation part of these algorithms and use the second order Newton-Raphson methods instead. In Fig. 12, we illustrate a comparison that involves the final error rates of both the modified and the original algorithms. We also keep our algorithms in their original settings to demonstrate the effect of using piecewise linear functions when the same adaptation algorithm is used. In Fig. 12, the CTW-2, the EMFNF-2, the FNF-2 and the VF-2 state for the algorithms using the second order methods for the adaptation. The presented S-DAT algorithm already corresponds to the DAT algorithm with the second order adaptation methods. Even though this modification decreases the final error of all algorithms, our algorithms still outperform their competitors. Additionally, in terms of the computational complexity, the algorithms EMFNF-2, FNF-2 and VF-2 become more costly compared to the proposed algorithms since they now use the second order methods for the adaptation. There exist only one algorithm,\ni.e., CTW-2, that is more efficient, but it does not achieve a significant gain on the error performance."}, {"heading": "4.4 Chaotic Signals", "text": "Finally, we examine the error performance of our algorithms when the desired data sequence is generated using chaotic processes, e.g. the Gauss map and the Lorenz attractor. We first consider the case where the data is generated using the Gauss map, i.e.,\nyt = exp ( \u2212 \u03b1x2t ) + \u03b2 (34)\nwhich exhibits a chaotic behavior for \u03b1 = 4 and \u03b2 = 0.5. The desired data sequence is represented by yt and xt \u2208 R corresponds to yt\u22121. x0 is a sample from a Gaussian process with zero-mean and unit variance. The learning rates are set to 0.004 for the FMP, 0.04 for the SP, 0.05 for the S-DAT and the\nDAT, 0.025 for the VF, the FNF, the EMFNF and the CTW.\nAs the second experiment, we consider a scenario where we use a chaotic signal that is generated from the Lorenz attractor, which is a set of chaotic solutions for the Lorenz system. Hence, the desired signal yt is modeled by\nyt = yt\u22121 + (\u03c3(ut\u22121 \u2212 yt\u22121))dt (35) ut = ut\u22121 + (yt\u22121(\u03c1\u2212 vt\u22121)\u2212 ut\u22121)dt (36) vt = vt\u22121 + (yt\u22121ut\u22121 \u2212 \u03b2vt\u22121)dt, (37)\nwhere \u03b2 = 8/3, \u03c3 = 10, \u03c1 = 28 and dt = 0.01. Here, ut and vt are used to represent the two dimensional regression space, i.e., the data vector is formed as xt = [ut, vt] T . We set the learning rates to 0.005 for the FMP, 0.006 for the SP, 0.0125 for the S-DAT, 0.01 for the DAT, the VF, the FNF, the EMFNF and the CTW.\nIn Fig. 13 and 14, we represent the error performance of the proposed algorithms for the Gauss map and the Lorenz attractor cases respectively. In both cases, the proposed algorithms attain substantially faster convergence rate and better steady state error performance compared to the state of the\nart. Even for the Lorenz attractor case, where the desired signal has a dependence on more than one past output samples, our algorithms outperform the competitors."}, {"heading": "5 Concluding Remarks", "text": "In this paper, we introduce three different highly efficient and effective nonlinear regression algorithms for online learning problems suitable for real life applications. We process only the currently available data for regression and then discard it, i.e., there is no need for storage. For nonlinear modeling, we use piecewise linear models, where we partition the regressor space using linear separators and fit linear regressors to each partition. We construct our algorithms based on two different approaches for the partitioning of the space of the regressors. As the first time in the literature, we adaptively update both the region boundaries and the linear regressors in each region using the second order methods, i.e., Newton-Raphson Methods. We illustrate that the proposed algorithms attain outstanding performance compared to the state of\nart even for the highly nonlinear data models. We also provide the individual sequence results demonstrating the guaranteed regret performance of the introduced algorithms without any statistical assumptions."}, {"heading": "Acknowledgment", "text": "This work is supported in part by Turkish Academy of Sciences Outstanding Researcher Programme, TUBITAK Contract No. 113E517, and Turk Telekom Communications Services Incorporated."}], "references": [{"title": "Slope estimation in noisy piecewise linear functions", "author": ["A. Ingle", "J. Bucklew", "W. Sethares", "T. Varghese"], "venue": "Signal Processing 108 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear spline adaptive filtering", "author": ["M. Scarpiniti", "D. Comminiello", "R. Parisi", "A. Uncini"], "venue": "Signal Processing 93 (4) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A tree-weighting approach to sequential decision problems with multiplicative loss", "author": ["S.S. Kozat", "A.C. Singer", "A.J. Bean"], "venue": "Signal Processing 91 (4) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential distributed detection in energy-constrained wireless sensor networks", "author": ["Y. Yilmaz", "X. Wang"], "venue": "IEEE Transactions on Signal Processing 17 (4) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["N. Asadi", "J. Lin"], "venue": "de Vries, Runtime optimizations for tree-based machine learning models, IEEE Transactions on Knowledge and Data Engineering 26 (9) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal processing in large systems", "author": ["R. Couillet", "M. Debbah"], "venue": "IEEE Signal Processing Magazine 24 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Online learning for very large data sets", "author": ["L. Bottou", "Y.L. Cun"], "venue": "Applied Stochastic Models in Business and Industry 21 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "in: Advances in Neural Information Processing (NISP)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": "John Wiley & Sons, NJ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.-Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering 26 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Universal FIR MMSE filtering", "author": ["T. Moon", "T. Weissman"], "venue": "IEEE Transactions on Signal Processing 57 (3) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear autoregressive modeling and estimation in the presence of noise", "author": ["A.C. Singer", "G.W. Wornell", "A.V. Oppenheim"], "venue": "Digital Signal Processing 4 (4) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Tree-structured nonlinear signal modeling and prediction", "author": ["O.J.J. Michel", "A.O. Hero", "A.-E. Badel"], "venue": "IEEE Transactions on Signal Processing 47 (11) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Coupled market behavior based financial crisis detection", "author": ["W. Cao", "L. Cao", "Y. Song"], "venue": "in: The 2013 International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Long-term trend in non-stationary time series with nonlinear analysis techniques", "author": ["L. Deng"], "venue": "in: 2013 6th International Congress on Image and Signal Processing (CISP), Vol. 2", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised non-linear dimensionality reduction techniques for classification in intrusion detection", "author": ["K. mei Zheng", "X. Qian", "N. An"], "venue": "in: 2010 International Conference on Artificial Intelligence and Computational Intelligence (AICI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Nlmf: Nonlinear matrix factorization methods for top-n recommender systems", "author": ["S. Kabbur", "G. Karypis"], "venue": "in: 2014 IEEE International Conference on Data Mining Workshop (ICDMW)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Prediction", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Learning, and Games, Cambridge University Press, Cambridge", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Universal linear least squares prediction: upper and lower bounds", "author": ["A.C. Singer", "S.S. Kozat", "M. Feder"], "venue": "IEEE Transactions on Information Theory 48 (8) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Steady state MSE performance analysis of mixture approaches to adaptive filtering", "author": ["S.S. Kozat", "A.T. Erdogan", "A.C. Singer", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 58 (8) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Competitive randomized nonlinear prediction under additive noise", "author": ["Y. Yilmaz", "S. Kozat"], "venue": "Signal Processing Letters, IEEE 17 (4) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Random projection trees for vector quantization", "author": ["S. Dasgupta", "Y. Freund"], "venue": "IEEE Transactions on Information Theory 55 (7) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["D.P. Helmbold", "R.E. Schapire"], "venue": "Machine Learning 27 (1) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Universal piecewise linear prediction via context trees", "author": ["S.S. Kozat", "A.C. Singer", "G.C. Zeitler"], "venue": "IEEE Transactions on Signal Processing 55 (7) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Introduction to linear optimization", "author": ["D. Bertsimas", "J.N. Tsitsiklis"], "venue": "Athena scientific series in optimization and neural computation, Athena Scientific, Belmont (Mass.)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiscale generalised linear models for nonparametric function estimation", "author": ["E.D. Kolaczyk", "R.D. Nowak"], "venue": "Biometrika 92 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "The context-tree weighting method: basic properties", "author": ["F.M.J. Willems", "Y.M. Shtarkov", "T.J. Tjalkens"], "venue": "IEEE Transactions on Information Theory 41 (3) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "Universal linear prediction by model order weighting", "author": ["A.C. Singer", "M. Feder"], "venue": "IEEE Transactions on Signal Processing 47 (10) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient adaptive algorithms and minimax bounds for zero-delay lossy source coding", "author": ["A. Gyorgy", "T. Linder", "G. Lugosi"], "venue": "IEEE Transactions on Signal Processing 52 (8) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "A comprehensive approach to universal piecewise nonlinear regression based on trees", "author": ["N. Vanli", "S. Kozat"], "venue": "IEEE Transactions on Signal Processing 62 (20) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Advances in Nonlinear Modeling for Speech Processing, Adaptive computation and machine learning", "author": ["M.S.D. Raghunath S. Holambe"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Machine learning : A probabilistic perspective", "author": ["K.P. Murphy"], "venue": "Adaptive computation and machine learning series, MIT Press, Cambridge (Mass.)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "A new approach to piecewise linear modeling of time series", "author": ["M. Mattavelli", "J. Vesin", "E. Amaldi", "R. Gruter"], "venue": "in: IEEE Digital Signal Processing Workshop Proceedings", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning 69 (2-3) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel partial least squares regression in reproducing kernel Hilbert space", "author": ["R. Rosipal", "L.J. Trejo"], "venue": "J. Mach. Learn. Res. 2 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "The Volterra and Wiener Theories of Nonlinear Systems", "author": ["M. Schetzen"], "venue": "John Wiley & Sons, NJ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1980}, {"title": "Fourier nonlinear filters", "author": ["A. Carini", "G.L. Sicuranza"], "venue": "Signal Processing 94 (0) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 1, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 2, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 3, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 4, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 5, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 6, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 7, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 8, "context": "Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311].", "startOffset": 148, "endOffset": 154}, {"referenceID": 9, "context": "Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311].", "startOffset": 148, "endOffset": 154}, {"referenceID": 10, "context": "Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311].", "startOffset": 148, "endOffset": 154}, {"referenceID": 11, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 78, "endOffset": 85}, {"referenceID": 12, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 78, "endOffset": 85}, {"referenceID": 13, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 289, "endOffset": 297}, {"referenceID": 7, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 289, "endOffset": 297}, {"referenceID": 17, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 289, "endOffset": 297}, {"referenceID": 6, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 456, "endOffset": 461}, {"referenceID": 7, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 456, "endOffset": 461}, {"referenceID": 17, "context": ", instantly, without any storage, and then discard the data after using and learning [18, 19].", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": ", instantly, without any storage, and then discard the data after using and learning [18, 19].", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321].", "startOffset": 158, "endOffset": 165}, {"referenceID": 19, "context": "Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321].", "startOffset": 158, "endOffset": 165}, {"referenceID": 20, "context": "Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321].", "startOffset": 158, "endOffset": 165}, {"referenceID": 12, "context": ", is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23].", "startOffset": 91, "endOffset": 101}, {"referenceID": 21, "context": ", is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23].", "startOffset": 91, "endOffset": 101}, {"referenceID": 22, "context": ", is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23].", "startOffset": 91, "endOffset": 101}, {"referenceID": 12, "context": "We note that the piecewise linear models are extensively used in the signal processing literature to mitigate the overtraining issues that arise because of using nonlinear models [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "However their performance in real life applications are less than adequate since their successful application highly depends on the accurate selection of the piecewise regions that correctly model the underlying data [24].", "startOffset": 217, "endOffset": 221}, {"referenceID": 24, "context": ", Newton-Raphson Methods [25].", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Hence, we avoid the well known over fitting issues by using piecewise linear models, moreover, since both the region boundaries as well as the linear models in each region are trained using the second order methods we achieve substantial performance compared to the state of the art [25].", "startOffset": 283, "endOffset": 287}, {"referenceID": 17, "context": "We also provide theoretical performance results in an individual sequence manner that are guaranteed to hold without any statistical assumptions [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 22, "context": "In adaptive signal processing literature, there exist methods which develop an approach based on weighted averaging of all possible models of a tree based partitioning instead of solely relying on a particular piecewise linear model [23, 24].", "startOffset": 233, "endOffset": 241}, {"referenceID": 23, "context": "In adaptive signal processing literature, there exist methods which develop an approach based on weighted averaging of all possible models of a tree based partitioning instead of solely relying on a particular piecewise linear model [23, 24].", "startOffset": 233, "endOffset": 241}, {"referenceID": 22, "context": "Such approaches are confirmed to lessen the bias variance trade off in a deterministic framework [23, 24].", "startOffset": 97, "endOffset": 105}, {"referenceID": 23, "context": "Such approaches are confirmed to lessen the bias variance trade off in a deterministic framework [23, 24].", "startOffset": 97, "endOffset": 105}, {"referenceID": 25, "context": "One such example is that the recursive dyadic partitioning, which partitions the regressor space using separation functions that are required to be parallel to the axes [26].", "startOffset": 169, "endOffset": 173}, {"referenceID": 26, "context": "Moreover, these methods usually do not provide a theoretical justification for the weighting of the models, even if there exist inspirations from information theoretic deliberations [27].", "startOffset": 182, "endOffset": 186}, {"referenceID": 18, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 23, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 27, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 28, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 23, "context": ", one should adjust these parameters to the specific application for successful process [24].", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": ", the Decision Adaptive Tree (DAT) [30].", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "In this work, we use nonlinear functions to model yt, since in most real life applications, linear regressors are inadequate to successively model the intrinsic relation between the feature vector xt and the desired data yt [31].", "startOffset": 224, "endOffset": 228}, {"referenceID": 31, "context": "Different from linear regressors, nonlinear functions are quite powerful and usually overfit in most real life cases [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "To this end, we choose piecewise linear functions due to their capability of approximating most nonlinear models [33].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "1 [22].", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "The second problem is to find out the linear model that best fits the data in each distinct region in a sequential manner [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "However, we do not process batch data sets, since the framework is online, and thus, cannot know the optimal weight beforehand [18].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Eventually, we consider the regret criterion to measure the modeling performance of the designated piecewise linear model and aim to attain a low regret [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "Online Newton Step [34], for training of the region boundaries and the linear models.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "In the second method, we use the tree notion to partition the regression space, which is a more systematic way to determine the regions [13,22].", "startOffset": 136, "endOffset": 143}, {"referenceID": 21, "context": "In the second method, we use the tree notion to partition the regression space, which is a more systematic way to determine the regions [13,22].", "startOffset": 136, "endOffset": 143}, {"referenceID": 33, "context": "Online Newton Step [34], to update both separator functions and region weights.", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "Here, the matrix At is related to the Hessian of the error function, implying that the update rule uses the second order information [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 8, "context": "In order to avoid taking the inverse of an m\u00d7m matrix, At, at each iteration in (11) and (12), we generate a recursive formula using matrix inversion lemma for A\u22121 t given as [9] A\u22121 t = A \u22121 t\u22121 \u2212 A\u22121 t\u22121\u2207t\u2207t A\u22121 t\u22121 1 +\u2207t A\u22121 t\u22121\u2207t , (13)", "startOffset": 175, "endOffset": 178}, {"referenceID": 12, "context": "The partition of the regressor space will be based on the finest model of a tree structure [13, 23].", "startOffset": 91, "endOffset": 99}, {"referenceID": 22, "context": "The partition of the regressor space will be based on the finest model of a tree structure [13, 23].", "startOffset": 91, "endOffset": 99}, {"referenceID": 29, "context": "The last algorithm combines all possible models of depthd tree and calculates the final estimate in an efficient way requiring O(4) weight updates [30].", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "\u201dDFT\u201d stands for Decision Fixed Tree and \u201dDAT\u201d represents Decision Adaptive Tree [30].", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "\u201dCTW\u201d is used for Context Tree Weighting [24], \u201dGKR\u201d represents Gaussian-Kernel re-", "startOffset": 41, "endOffset": 45}, {"referenceID": 34, "context": "gressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively.", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "gressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively.", "startOffset": 46, "endOffset": 50}, {"referenceID": 36, "context": "gressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively.", "startOffset": 133, "endOffset": 137}, {"referenceID": 33, "context": "All three algorithms uses the second order update rule, Online Newton Step [34], and achieves a logarithmic regret when the normal vectors of the region boundaries are fixed and the cost function is convex in the sense of individual region weights.", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "using the Lemma 3 of [34], since our cost function is \u03b1-exp-concave, i.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "By following a similar discussion [34], except that we have equality in (26) and in the proceeding parts, we achieve the inequality", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "We make use of Lemma 11 given in [34], to get the following bound 1 2\u03b2 n \u2211", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 1, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 72, "endOffset": 78}], "year": 2017, "abstractText": "We introduce highly efficient online nonlinear regression algorithms that are suitable for real life applications. We process the data in a truly online manner such that no storage is needed, i.e., the data is discarded after being used. For nonlinear modeling we use a hierarchical piecewise linear approach based on the notion of decision trees where the space of the regressor vectors is adaptively partitioned based on the performance. As the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region are trained using the second order methods, we achieve substantial performance compared to the state of the art. We demonstrate our gains over the well known benchmark data sets and provide performance results in an individual sequence manner guaranteed to hold without any statistical assumptions. Hence, the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic sense.", "creator": "LaTeX with hyperref package"}}}