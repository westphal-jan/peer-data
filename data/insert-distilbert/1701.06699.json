{"id": "1701.06699", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Imitating Driver Behavior with Generative Adversarial Networks", "abstract": "the surprising ability to accurately predict and simulate human driving behavior is nevertheless critical for the development of intelligent transportation systems. traditional modeling methods have employed simple parametric models and behavioral cloning. this paper adopts a method for overcoming the problem of cascading errors inherent in prior approaches, resulting in realistic behavior that is robust to trajectory perturbations. we extend generative adversarial imitation learning to the training of recurrent policies, and for we demonstrate that our model outperforms rule - based controllers inputs and maximum likelihood models in realistic highway simulations. with our model currently both reproduces relatively emergent behavior of human drivers, such as lane change rate, because while maintaining realistic nash control costs over long time horizons.", "histories": [["v1", "Tue, 24 Jan 2017 00:59:42 GMT  (291kb,D)", "http://arxiv.org/abs/1701.06699v1", "8 pages, 6 figures"]], "COMMENTS": "8 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alex kuefler", "jeremy morton", "tim wheeler", "mykel kochenderfer"], "accepted": false, "id": "1701.06699"}, "pdf": {"name": "1701.06699.pdf", "metadata": {"source": "CRF", "title": "Imitating Driver Behavior with Generative Adversarial Networks", "authors": ["Alex Kuefler", "Jeremy Morton", "Tim Wheeler", "Mykel Kochenderfer"], "emails": ["akuefler@stanford.edu", "mykel}@stanford.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAccurate human driver models are critical for realistic simulation of driving scenarios, and have the potential to significantly advance research in automotive safety. Traditionally, human driver modeling has been the subject of both rule-based and data-driven approaches. Early rule-based attempts include parametric models of car following behavior, with strong built-in assumptions about road conditions [1] or driver behavior [2]. The Intelligent Driver Model (IDM) [3] extended this work by capturing asymmetries between acceleration and deceleration, preferred free road and bumper-tobumper headways, and realistic braking behavior. Such carfollowing models were later extended to multilane conditions with controllers like MOBIL [4], which maintains a utility function and \u201cpoliteness parameter\u201d to capture intelligent driver behavior in both acceleration and turning. These controllers are all largely characterized by smooth, collisionfree driving, but rely on assumptions about driver behavior and a small set of parameters that may not generalize well to diverse driving scenarios.\nIn contrast, imitation learning (IL) approaches rely on data typically provided through human demonstration in order to learn a policy that behaves similarly to an expert. These policies can be represented with expressive models, such as neural networks, with less interpretable parameters than those used by rule-based methods. Prior human behavior models for highway driving have relied on behavioral cloning (BC), which treats IL as a supervised learning problem, fitting a model to a fixed dataset of expert state-action pairs [5\u20138]. ALVINN [9], an early BC approach, trained a neural network to map raw images and rangefinder inputs to discrete\n1Alex Kuefler is in the Symbolic Systems Program at Stanford University, Stanford, CA 94305, USA akuefler@stanford.edu\n2Are in the department of Aeronautics and Astronautics at Stanford University, Stanford, CA 94305, USA {jmorton2, wheelert, mykel}@stanford.edu\nturning actions. Recent advances in computing and deep learning have allowed this approach to scale to realistic scenarios, such as parking lot, highway, and markerless road conditions [10]. These BC approaches are conceptually sound [11], but tend to fail in practice as small inaccuracies compound during simulation. Inaccuracies lead the policy to states that are underrepresented in the training data (e.g., an ego-vehicle edging towards the side of the road), which leads to yet poorer predictions, and ultimately to invalid or unseen situations (e.g., off-road driving). This problem of cascading errors [12] is well-known in the sequential decision making literature and has motivated work on alternative IL methods, such as inverse reinforcement learning (IRL) [14].\nInverse reinforcement learning assumes that the expert follows an optimal policy with respect to an unknown reward function. If the reward function is recovered, one can simply use RL to find a policy that behaves identically to the expert. This imitation extends to unseen states; in highway driving a vehicle that is perturbed toward the lane boundaries should know to return toward the lane center. IRL thus generalizes much more effectively and does not suffer from many of the problems of BC. Because of these benefits, some recent efforts in human driver modeling emphasize IRL [15, 16]. However, IRL approaches are typically computationally expensive in their recovery of an expert cost function. Instead, recent work has attempted to imitate expert behavior through direct policy optimization, without first learning a cost function [13, 17]. Generative Adversarial Imitation Learning (GAIL) [17] in particular has performed well on a number of benchmark tasks, leveraging the insight that expert behavior can be imitated by training a policy to produce actions that a binary classier mistakes for those of an expert.\nIn this work, we apply GAIL to the task of modeling human highway driving behavior. Our major contributions are twofold. First, we extend GAIL to the optimization of recurrent neural networks, showing that such policies perform with greater fidelity to expert behavior than feedforward counterparts. Second, we apply our models to a realistic highway simulator, where expert demonstrations are given by real-world driver trajectories included in the NGSIM dataset [18, 19]. We demonstrate that policy networks optimized by GAIL capture many desirable properties of earlier IL models, such as reproducing emergent driver behavior and assigning high likelihood to expert actions, while simultaneously reducing collision and off-road rates necessary for long horizon highway simulations. Unlike past work, our model learns to map raw LIDAR readings and simple, hand-picked road features to continuous actions, adjusting only turn-rate and acceleration each time step. ar X\niv :1\n70 1.\n06 69\n9v 1\n[ cs\n.A I]\n2 4\nJa n\n20 17"}, {"heading": "II. PROBLEM FORMULATION", "text": "We regard highway driving as a sequential decision making task in which the driver obeys a stochastic policy \u03c0(a | s) mapping observed road conditions s to a distribution over driving actions a. Given a class of policies \u03c0\u03b8 parameterized by \u03b8, we seek to find the policy that best mimics human driving behavior. We adopt an IL approach to infer this policy from a dataset consisting of a sequence of state-action tuples (st, at). IL can be performed using BC or reinforcement learning."}, {"heading": "A. Behavioral Cloning", "text": "Behavioral cloning solves a regression problem in which the policy parameterization is obtained by maximizing the likelihood of the actions taken in the training data. BC works well for states adequately covered by the training data. It is forced to generalize when predicting actions for states with little or no data coverage, which can lead to poor behavior. Unfortunately, even if simulations are initialized in common states, the stochastic nature of the policies allow small errors in action predictions to compound over time, eventually leading to states that human drivers infrequently visit and are not adequately covered by the training data. Poorer predictions can cause a feedback cycle known as cascading errors [20].\nIn a highway driving context, cascading errors can lead to off-road driving and collisions. Datasets rarely contain information about how human drivers behave in these situations, which can lead BC policies to act erratically when they encounter such states.\nBehavioral cloning has been successfully used to produce driving policies for simple behaviors such as car-following on freeways, in which the state and action space can be adequately covered by the training set. When applied to learning general driving models with nuanced behavior and the potential to drive out of lane, BC only produces accurate predictions up to a few seconds [5, 6]."}, {"heading": "B. Reinforcement Learning", "text": "Reinforcement learning (RL) instead assumes that drivers in the real world follow an expert policy \u03c0E whose actions maximize the expected, global return\nR(\u03c0, r) = E\u03c0 [ T\u2211 t=0 \u03b3tr(st, at) ] (1)\nweighted by a discount factor \u03b3 \u2208 [0, 1). The local reward function r(st, at) may be unknown, but fully characterizes expert behavior such that any policy optimizing R(\u03c0, r) will perform indistinguishably from \u03c0E .\nLearning with respect to R(\u03c0, r) has several advantages over maximum likelihood BC in the context of sequential decision making [21]. First, r(st, at) is defined for all stateaction pairs, allowing an agent to receive a learning signal even from unusual states. In contrast, BC only receives a learning signal for those states represented in a labeled, finite dataset. Second, unlike labels, rewards allow a learner to\nestablish preferences between mildly undesirable behavior (e.g., hard braking) and extremely undesirable behavioral (e.g., collisions). And finally, RL maximizes the global, expected return on a trajectory, rather than local instructions for each observation. Once preferences are learned, a policy may take mildly undesirable actions now in order to avoid awful situations later. As such, reinforcement learning algorithms provide robustness against cascading errors."}, {"heading": "III. POLICY REPRESENTATION", "text": "Our learned policy must be able to capture human driving behavior, which involves: \u2022 Non-linearity in the desired mapping from states to\nactions (e.g., large corrections in steering to avoid collisions caused by small changes in the current state). \u2022 High-dimensionality of the state representation, which must describe properties of the ego-vehicle, in addition to surrounding cars and road conditions. \u2022 Stochasticity because humans may take different actions each time they encounter a given traffic scene.\nTo address the first and second points, we represent all learned policies \u03c0\u03b8 using neural networks. To address the third point, we interpret the network\u2019s real-valued outputs given input st as the mean \u00b5t and logarithm of the diagonal covariance log \u03bdt of a Gaussian distribution. Actions are chosen by sampling at \u223c \u03c0\u03b8(at | st). An example feedforward model is shown in Fig. 2. We evaluate both feedforward and recurrent network architectures.\nFeedforward neural networks directly map inputs to outputs. The most common architecture, multilayer perceptrons (MLPs), consist of alternating layers of tunable weights and element-wise nonlinearities. Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].\nThe feedforward MLP is limited in its ability to adequately address partially observable environments. In real world driving, sensor error and occlusions may prevent the driver from seeing all relevant parts of the driving state. By maintaining sufficient statistics of past observations in memory, recurrent policies [30, 31] disambiguate perceptually similar states by acting with respect to histories of, rather than individual, observations. In this work, we represent recurrent policies using Gated Recurrent Unit (GRU) networks due to their comparable performance with fewer parameters than other architectures [32].\nWe use similar architectures for the feedforward and recurrent policies. The recurrent policies consist of five feedforward layers that decrease in size from 256 to 32 neurons, with an additional GRU layer consisting of 32 neurons. Exponential linear units (ELU) were used throughout the network, which have been shown to combat the vanishing gradient problem while supporting a zero-centered distribution of activation vectors [33]. The MLP policies have the same architecture, except the GRU layer is replaced\nwith an additional feedforward layer. For each network architecture, one policy is trained through BC and one policy is trained through GAIL. In all, we trained four neural network policies: GAIL GRU, GAIL MLP, BC GRU, and BC MLP."}, {"heading": "IV. POLICY OPTIMIZATION", "text": "Contrary to BC, which is trained with traditional regression techniques, reinforcement learning policies do not have training labels for individual actions. Controller performance is instead evaluated by expected return. This approach is problematic in modeling human drivers, as the reward function r(st, at) is unknown. We first discuss a method for training a policy with a known reward function and then provide a method for learning the reward function."}, {"heading": "A. Trust Region Policy Optimization", "text": "Policy gradient algorithms are a particularly effective class of reinforcement learning techniques for optimizing differentiable policies, including neural networks. As with standard backpropagation, network parameters are optimized using gradient-based updates, but the gradient can only be approximated using simulated rollouts of the policy interacting with the environment.\nThis empirical gradient estimate typically exhibits a high amount of variance. In practice, this variance can cause parameter updates that do not improve or even reduce performance. In this work, we use Trust Region Policy Optimization (TRPO) to learn our human driving policies [34]. TRPO updates policy parameters through a constrained optimization procedure that enforces that a policy cannot change too much in a single update, and hence limits the damage that can be caused by noisy gradient estimates.\nAlthough the true reward function that governs the behavior of any particular human driver is unknown, domain knowledge can be used to craft a surrogate reward function such that a policy maximizing this quantity will realize a similar stochastic state-action mapping as \u03c0E . Drivers avoid collisions and going off road, while also favoring smooth driving and minimizing lane-offset. If such features can be combined into a reward function that closely approximates the true reward function for human driving r(st, at), then modeling driver behavior reduces to RL. However, handcrafting an accurate reward function is often difficult, which motivates the use of Generative Adversarial Imitation Learning."}, {"heading": "B. Generative Adversarial Imitation Learning", "text": "Although r(st, at) is unknown, a surrogate reward r\u0303(st, at) may be learned directly from data, without making use of domain knowledge. GAIL [17] trains a policy to perform expert-like behavior by rewarding it for \u201cdeceiving\u201d a classifier trained to discriminate between policy and expert state-action pairs. Consider a set of simulated state-action pairs X\u03b8 = {(s1, a1), (s2, a2), ..., (sT , aT )} sampled from \u03c0\u03b8 and a set of expert pairs XE sampled from \u03c0E . For a\nneural network D\u03c8 parameterized by \u03c8, the GAIL objective is given by:\nmax \u03c8 min \u03b8 V (\u03b8, \u03c8) = E (s,a)\u223cXE [logD\u03c8(s, a)]+\nE (s,a)\u223cX\u03b8\n[log(1\u2212D\u03c8(s, a))]. (2)\nWhen fitting \u03c8, Equation (2) can simply be interpreted as a sigmoid cross entropy objective, maximized by minibatch gradient ascent. Positive examples are sampled from XE and negative examples are sampled from rollouts generated by interactions of \u03c0\u03b8 with the simulation environment. However, V (\u03b8, \u03c8) is non-differentiable with respect to \u03b8, requiring optimization via RL.\nIn order to fit \u03c0\u03b8, a surrogate reward function can be formulated from Eq. (2) as:\nr\u0303(st, at;\u03c8) = \u2212 log(1\u2212D\u03c8(st, at)), (3)\nwhich approaches infinity as tuples (st, at) drawn from X\u03b8 become indistinguishable from elements of XE based on the predictions of D\u03c8 . After performing rollouts with a given set of policy parameters \u03b8, surrogate rewards r\u0303(st, at;\u03c8) are calculated and TRPO is used to perform a policy update. Although r\u0303(st, at;\u03c8) may be quite different from the true reward function optimized by experts, it can be used to drive \u03c0\u03b8 into regions of the state-action space similar to those explored by \u03c0E ."}, {"heading": "V. DATASET", "text": "We use the public Next-Generation Simulation (NGSIM) datasets for US Highway 101 [19] and Interstate 80 [18]. NGSIM provides 45 minutes of driving at 10 Hz for each roadway. The US Highway 101 dataset covers an area in Los Angeles approximately 640 m in length with five mainline lanes and a sixth auxiliary lane for highway entrance and exit. The Interstate 80 dataset covers an area in the San Francisco Bay Area approximately 500 m in length with six mainline lanes, including a high-occupancy vehicle lane and an onramp.\nTraffic density in both datasets transitions from uncongested to full congestion and exhibits a high degree of vehicle interaction as vehicles merge on and off the highway and must navigate in congested flow. The diversity of driving conditions and the forced interaction of traffic participants makes these sources particularly useful for behavioral studies. The trajectories were smoothed using an extended Kalman filter [35] on a bicycle model and projected to lanes using centerlines extracted from the NGSIM CAD files. Cars, trucks, buses, and motorcycles are in the dataset, but only car trajectories were used for model training."}, {"heading": "VI. EXPERIMENTS", "text": "In this work, we use GAIL and BC to learn policies for two-dimensional highway driving. The performance of these policies is subsequently evaluated relative to baseline models."}, {"heading": "A. Environment", "text": "All experiments were conducted with the rllab reinforcement learning framework [36]. The simulation environment is a driving simulation on the NGSIM 80 and 101 road networks. Simulations are initialized to match frames from the NGSIM data, and the ego vehicle is randomly chosen from among the traffic participants in the frame. Simulations are run for 100 steps at 10 Hz and are ended prematurely if the ego vehicle is involved in a collision, drives off road, or drives in reverse.\nThe ego vehicle is driven according to a bicycle model with acceleration and turn-rate sampled from the policy network. All other traffic participants are replayed directly from the NGSIM data, but are augmented with emergency braking in the event of an imminent rear-end collision with the ego vehicle. Specifically, if the acceleration predicted by the Intelligent Driver Model (IDM) [3] is less than an activation threshold of \u22122 m/s2, the vehicle then accelerates according to the IDM while tracking the closest lane centerline. The IDM is parameterized with a desired speed equal to the vehicle\u2019s speed at transition, a minimum spacing of 1 m, a desired time headway of 0.5 s, a nominal acceleration of 3 m/s2, and a comfortable braking deceleration of 2.5 m/s2."}, {"heading": "B. Features", "text": "All experiments use the same set of features. These features can be decomposed into three sets. The first set, the core features, are eight scalar values that provide basic information about the vehicle\u2019s odometry, dimensions, and the lane-relative ego state. These are listed in Table I.\nThe core features alone are insufficient to describe the local context. Information about neighboring vehicles and the local road structure must be incorporated as well. Several approaches exist for encoding such information in handselected features relevant to the driving task [37]. Rather than\nrestrict the model to a subset of vehicle relationships, we introduce a more general and flexible feature representation.\nIn addition to the core features, a set of LIDAR-like beams emanating from the vehicle are used to gather information about its surroundings. These beams measure both the distance and range rate of the first vehicle struck by them, up to a maximum range. Our work used a maximum range of 100 m, with 20 range and range rate beams, each spaced uniformly in complete 360\u00b0 coverage around the ego vehicle\u2019s center, as shown in Fig. 3.\nFinally, a set of three indicator features are used to identify when the ego vehicle encounters undesirable states. These features take on a value of one whenever the ego vehicle is involved in a collision, drives off road, or travels in reverse, and are zero otherwise. All features were concatenated into a single 51-element vector and fed into each model.\nThe previous action taken by the ego vehicle is not included in the set of features provided to the policies. We found that policies can develop an over-reliance on previous actions at the expense of relying on the other features con-\ntained in their input. To counteract this problem, we studied the effect of replacing the previous actions with random noise early in the training process. However, it was found that even with these mitigations the inclusion of previous actions had a detrimental effect on policy performance."}, {"heading": "C. Baseline Models", "text": "The first baseline that we used to compare against our deep policies is a static Gaussian (SG) model, which is an unchanging Gaussian distribution \u03c0(a | s) = N (a | \u00b5,\u03a3) fit using maximum likelihood.\nThe second baseline model is a BC approach using mixture regression (MR) [6]. The model has been used for modelpredictive control and has been shown to work well in simulation and in real-world drive tests. Our MR model is a Gaussian mixture over the joint space of the actions and features, trained using expectation maximization [38]. The stochastic policy is formed from the weighted combination of the Gaussian components conditioned on the features. Greedy feature selection is used during training to select a subset of predictors up to a maximum feature count threshold while minimizing the Bayesian information criterion [39].\nThe final baseline model uses a rule-based controller to govern the lateral and longitudinal motion of the ego vehicle. The longitudinal motion is controlled by the Intelligent Driver Model with the same parameters as the emergency braking controller used in the simulation environment. For the lateral motion, MOBIL [4] is used to select the desired lane, with a proportional controller used to track the lane centerline. A small amount of noise is added to both the lateral and longitudinal accelerations to make the controller nondeterministic.\nD. Validation\nTo evaluate the relative performance of each model, a systematic validation procedure was performed. For each model, 1,000 ten-second scenes were simulated 20 times each in an environment identical to the one used to train the GAIL policies. As these rollouts were performed, several metrics were extracted to quantify the ability of each model to simulate human driver behavior.\n1) Root-Weighted Square Error: The root-weighted square error (RWSE) captures the deviation of a model\u2019s probability mass from real-world trajectories [40]. For predicted variable v over m trajectories, we estimate the RWSE\nby sampling n = 20 simulated traces per recorded trajectory:\nRWSEH = \u221a\u221a\u221a\u221a 1 mn m\u2211 i=1 n\u2211 j=1 ( v (i) H \u2212 v\u0302 (i,j) H )2 , (4)\nwhere v(i)H is the true value in the ith trajectory at time horizon H and v\u0302(i,j)H is the simulated variable under sample j for the ith trajectory at time horizon H . We extract the RWSE in predictions of global position, centerline offset, and speed over time horizons up to 5 s.\n2) Kullback-Leibler Divergence: Driver models should produce distributions over emergent quantities that match those observed in real-world data. For each model, empirical distributions were computed over speed, acceleration, turnrate, jerk, and inverse time-to-collision (iTTC) over simulated trajectories. The closeness between the simulated and real-world distributions was quantified using the KullbackLeibler (KL) divergence. Piecewise uniform distributions with 100 evenly spaced bins were used.\n3) Emergent Behavior: We also extracted a set of emergent metrics that indicate model imitation performance in relation to the NGSIM dataset. These additional metrics are the lane change rate, the offroad duration, the collision rate, and the hard brake rate.\nThe lane change rate is the average number of times a vehicle makes a lane change within a 10-second trajectory. Offroad duration is the average number of time steps per trajectory that a vehicle spends more than 1 m outside the closest outer road marker. The collision rate is the fraction of trajectories where the ego vehicle intersects with another traffic participant. The hard brake rate captures the frequency at which a model chooses to brake harder than \u22123 m/s2.\nThe environment in which validation occurs is not entirely realistic, as the non-ego vehicles have pre-recorded trajectories and do not always properly respond to deviations of the ego vehicle from its original trajectory, leading to an artificially high number of collisions. Hence, we also extract the hard brake rate to help quantify how often dangerous driving situations occur."}, {"heading": "VII. RESULTS", "text": "Validation results for root-weighted square error are given in Fig. 4. The root-weighted square error results show that the feedforward BC model has the best short-horizon performance, but then begins to accumulate error for longer time horizons. GAIL produces more stable trajectories and it short term predictions perform well. One can clearly see the controller adhere to the lane-centerline, so its lane offset error is close to a constant 0.5, which demonstrates that human drivers do not always closely track the nearest lanecenterline.\nKL divergence scores are given in Fig. 5. The KL divergence results show very good tracking for SG in everything but jerk. SG cannot overfit, and always takes the average action. Its performance in other metrics is poor. GAIL GRU performs well on the iTTC, speed, and acceleration metrics. It does poorly with respect to turn-rate and jerk.\nThis poor performance is likely due to the fact that, on average, the GAIL GRU policy takes similar actions to humans, but oscillates between actions more than humans. For instance, rather than outputting a turn-rate of zero on straight road stretches, it will alternate between outputting small positive and negative turn-rates. In comparison with the GAIL policies, the BC policies are worse with iTTC. The GRU version has the largest KL divergence in acceleration,\nmostly due to its accelerations being generally small in magnitude, but does reasonably well with turn-rate and jerk.\nValidation results for emergent variables are given in Fig. 6. The emergent values show that GAIL policies outperform the BC policies. The GAIL GRU policy has the closest match to the data everywhere except for hard brakes (it rarely takes extreme actions). Mixture regression largely performs better than SG and is on par with the BC policies, but is still susceptible to cascading errors. Offroad duration is perhaps the most striking statistic; only GAIL (and of course IDM + MOBIL) are able to stay on the road for extended stretches. SG never brakes hard because it only drives straight, and it has a high collision rate as a consequence. It is interesting that the collision rate for IDM + MOBIL is roughly the same as the collision rate for GAIL GRU, despite the fact that IDM + MOBIL should not collide. The inability of other vehicles within the simulation environment to fully react to the egovehicle may explain this phenomenon.\nThe results demonstrate that GAIL-based models capture many desirable properties of both rule-based and machine learning methods, while avoiding common pitfalls. With the exception of the hand-coded controller, GAIL policies achieve the lowest collision and off-road driving rates, considerably outperforming baseline and similarly structured BC models. However, GAIL also achieves a lane change rate closer to real human driving than any other method against which it is compared.\nFurthermore, extending GAIL to recurrent policies leads to improved performance. This result is an interesting contrast with the BC policies, where the addition of recurrence largely does not lead to better results. Thus, we find that recurrence\nby itself is insufficient for addressing the detrimental effects that cascading errors can have on BC policies."}, {"heading": "VIII. CONCLUSIONS", "text": "This paper demonstrates the effectiveness of deep imitation learning as a means of training driver models that perform realistically over long time horizons, while simultaneously capturing microscopic, human-like behavior. Our contributions have been to (1) extend Generative Adversarial Imitation Learning to the optimization of recurrent policies, and to (2) apply this technique to the creation of a new, intelligent model of highway driving that outperforms the state of the art on several metrics. Although behavioral cloning still outperforms Generative Adversarial Imitation Learning on short (\u223c2 s) horizons, its greedy behavior prevents it from achieving realistic driving over an extended period. The use of policy optimization by Generative Adversarial Imitation Learning enables us to overcome this problem of cascading errors to produce long-term, stable trajectories. Furthermore, the use of policy representation by deep, recurrent neural networks enables us to learn directly from general sensor inputs (i.e., LIDAR distance and range rate) that can capture arbitrary traffic states and simulate partial observability.\nWe have argued that reinforcement learning schemes incorporating surrogate reward functions overcome problems arising from supervised learning in highway driver modeling. As such, future work may wish to explore ways of combining other reward signals with our own. Whereas Generative Adversarial Imitation Learning captures human-like behavior present in the dataset, simulators may also wish to enforce certain behaviors (e.g., explicitly modeling driver style) by combining the learned, surrogate reward with a reward function crafted from hand-picked features. An engineered reward function could also be used to penalize the oscillations in acceleration and turn-rate produced by the GAIL GRU. Finally, we offer our model of human driving behavior as an important element for simulating realistic highway conditions. Future work will apply our model to decision making and safety validation. The code associated with this paper can be found at https://github.com/sisl/gail-driver."}, {"heading": "ACKNOWLEDGMENT", "text": "This material is based upon work supported by the Ford Motor Company, Robert Bosch LLC, and the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-114747."}], "references": [{"title": "A program for simulating the dispersion of platoons of road traffic", "author": ["P.A. Seddon"], "venue": "Simulation, vol. 18, no. 3, pp. 81\u201390", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1972}, {"title": "A behavioural car-following model for computer simulation", "author": ["P.G. Gipps"], "venue": "Transportation Research Part B: Methodological, vol. 15, no. 2, pp. 105\u2013111", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1981}, {"title": "Congested traffic states in empirical observations and microscopic simulations", "author": ["M. Treiber", "A. Hennecke", "D. Helbing"], "venue": "Physical Review E, vol. 62, no. 2, p. 1805", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "General lane-changing model mobil for car-following models", "author": ["A. Kesting", "M. Treiber", "D. Helbing"], "venue": "Journal of the Transportation Research Board", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis of microscopic behavior models for probabilistic modeling of driver behavior", "author": ["T.A. Wheeler", "P. Robbel", "M. Kochenderfer"], "venue": "IEEE International Conference on Intelligent Transportation Systems (ITSC)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Comparison of parametric and non-parametric approaches for vehicle speed prediction", "author": ["S. Lef\u00e8vre", "C. Sun", "R. Bajcsy", "C. Laugier"], "venue": "American Control Conference (ACC), pp. 3494\u20133499", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning context sensitive behavior models from observations for predicting traffic situations", "author": ["T. Gindele", "S. Brechtel", "R. Dillmann"], "venue": "IEEE International Conference on Intelligent Transportation Systems (ITSC)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimation of multivehicle dynamics by considering contextual information", "author": ["G. Agamennoni", "J. Nieto", "E. Nebot"], "venue": "IEEE Transactions on Robotics, vol. 28, no. 4, pp. 855\u2013870", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "ALVINN: an autonomous land vehicle in a neural network", "author": ["D.A. Pomerleau"], "venue": "DTIC Document, Tech. Rep. AIP-77", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L. Jackel", "M. Monfort", "U. Muller", "J. Zhang", "X. Zhang", "J. Zhao"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["U. Syed", "R.E. Schapire"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "J. Bagnell"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Model-free imitation learning with policy optimization", "author": ["J. Ho", "J.K. Gupta", "S. Ermon"], "venue": "arXiv preprint arXiv:1605.08478", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Highspeed highway scene prediction based on driver models learned from demonstrations", "author": ["D.S. Gonz\u00e1lez", "J. Dibangoye", "C. Laugier"], "venue": "IEEE International Conference on Intelligent Transportation Systems (ITSC)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Planning for autonomous cars that leverages effects on human actions", "author": ["D. Sadigh", "S. Sastry", "S.A. Seshia", "A.D. Dragan"], "venue": "Robotics: Science and Systems", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "arXiv preprint arXiv:1606.03476", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "US highway 80 dataset", "author": ["J. Colyar", "J. Halkias"], "venue": "Federal Highway Administration (FHWA), Tech. Rep. FHWA-HRT-06-137", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "US highway 101 dataset", "author": ["J. Colyar", "J. Halkias"], "venue": "Federal Highway Administration (FHWA), Tech. Rep. FHWA-HRT-07-030", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "An invitation to imitation", "author": ["J.A. Bagnell"], "venue": "DTIC Document, Tech. Rep. CMU-RI-TR-15-08", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "1. MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Develop a car-following model using data collected by \u201dfivewheel system\u201d", "author": ["J. Hongfei", "J. Zhicai", "N. Anning"], "venue": "IEEE International Conference on Intelligent Transportation Systems (ITSC), vol. 1", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Neural agent car-following models", "author": ["S. Panwai", "H. Dia"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 8, no. 1, pp. 60\u201370", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "A modified car-following model based on a neural network model of the human driver effects", "author": ["A. Khodayari", "A. Ghaffari", "R. Kazemi", "R. Braunstingl"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 42, no. 6, pp. 1440\u20131449", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of recurrent neural networks for probabilistic modeling of driver behavior", "author": ["J. Morton", "T.A. Wheeler", "M.J. Kochenderfer"], "venue": "IEEE Transactions on Intelligent Transportation Systems", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Vehicle lateral position prediction: a small step towards a comprehensive risk assessment system", "author": ["Q. Liu", "B. Lathrop", "V. Butakov"], "venue": "IEEE International Conference on Intelligent Transportation Systems (ITSC)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal modelling and hidden Markov models for driving manoeuvre recognition and driver fault diagnosis in an urban road scenario", "author": ["P. Boyraz", "M. Acar", "D. Kerr"], "venue": "IEEE Intelligent Vehicles Symposium", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Recurrent policy gradients", "author": ["D. Wierstra", "A. F\u00f6rster", "J. Peters", "J. Schmidhuber"], "venue": "Logic Journal of IGPL, vol. 18, no. 5, pp. 620\u2013634", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Memory-based control with recurrent neural networks", "author": ["N. Heess", "J.J. Hunt", "T.P. Lillicrap", "D. Silver"], "venue": "arXiv preprint arXiv:1512.04455", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "A new approach to linear filtering and prediction problems", "author": ["R.E. Kalman"], "venue": "Journal of Basic Engineering, vol. 82, no. 1, pp. 35\u201345", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1960}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep- Driving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "The Elements of Statistical Learning", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, vol. 6, no. 2, pp. 461\u2013464", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1978}, {"title": "Probabilistic airport acceptance rate prediction", "author": ["J. Cox", "M.J. Kochenderfer"], "venue": "AIAA Modeling and Simulation Conference", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Early rule-based attempts include parametric models of car following behavior, with strong built-in assumptions about road conditions [1] or driver behavior [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Early rule-based attempts include parametric models of car following behavior, with strong built-in assumptions about road conditions [1] or driver behavior [2].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "The Intelligent Driver Model (IDM) [3] extended this work by capturing asymmetries between acceleration and deceleration, preferred free road and bumper-tobumper headways, and realistic braking behavior.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Such carfollowing models were later extended to multilane conditions with controllers like MOBIL [4], which maintains a utility function and \u201cpoliteness parameter\u201d to capture intelligent driver behavior in both acceleration and turning.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "(BC), which treats IL as a supervised learning problem, fitting a model to a fixed dataset of expert state-action pairs [5\u20138].", "startOffset": 120, "endOffset": 125}, {"referenceID": 5, "context": "(BC), which treats IL as a supervised learning problem, fitting a model to a fixed dataset of expert state-action pairs [5\u20138].", "startOffset": 120, "endOffset": 125}, {"referenceID": 6, "context": "(BC), which treats IL as a supervised learning problem, fitting a model to a fixed dataset of expert state-action pairs [5\u20138].", "startOffset": 120, "endOffset": 125}, {"referenceID": 7, "context": "(BC), which treats IL as a supervised learning problem, fitting a model to a fixed dataset of expert state-action pairs [5\u20138].", "startOffset": 120, "endOffset": 125}, {"referenceID": 8, "context": "ALVINN [9], an early BC approach, trained a neural network to map raw images and rangefinder inputs to discrete", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Recent advances in computing and deep learning have allowed this approach to scale to realistic scenarios, such as parking lot, highway, and markerless road conditions [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "These BC approaches are conceptually sound [11], but tend to fail in practice as small inaccuracies compound during simulation.", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "This problem of cascading errors [12] is well-known in the sequential decision making literature and has motivated work on alternative IL methods, such as inverse reinforcement learning (IRL) [14].", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "This problem of cascading errors [12] is well-known in the sequential decision making literature and has motivated work on alternative IL methods, such as inverse reinforcement learning (IRL) [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 14, "context": "Because of these benefits, some recent efforts in human driver modeling emphasize IRL [15, 16].", "startOffset": 86, "endOffset": 94}, {"referenceID": 15, "context": "Because of these benefits, some recent efforts in human driver modeling emphasize IRL [15, 16].", "startOffset": 86, "endOffset": 94}, {"referenceID": 12, "context": "Instead, recent work has attempted to imitate expert behavior through direct policy optimization, without first learning a cost function [13, 17].", "startOffset": 137, "endOffset": 145}, {"referenceID": 16, "context": "Instead, recent work has attempted to imitate expert behavior through direct policy optimization, without first learning a cost function [13, 17].", "startOffset": 137, "endOffset": 145}, {"referenceID": 16, "context": "Generative Adversarial Imitation Learning (GAIL) [17] in particular has performed well on a number of benchmark tasks, leveraging the insight that expert behavior can be imitated by training a policy to produce actions that a binary classier mistakes for those of an expert.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "realistic highway simulator, where expert demonstrations are given by real-world driver trajectories included in the NGSIM dataset [18, 19].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "realistic highway simulator, where expert demonstrations are given by real-world driver trajectories included in the NGSIM dataset [18, 19].", "startOffset": 131, "endOffset": 139}, {"referenceID": 19, "context": "Poorer predictions can cause a feedback cycle known as cascading errors [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "When applied to learning general driving models with nuanced behavior and the potential to drive out of lane, BC only produces accurate predictions up to a few seconds [5, 6].", "startOffset": 168, "endOffset": 174}, {"referenceID": 5, "context": "When applied to learning general driving models with nuanced behavior and the potential to drive out of lane, BC only produces accurate predictions up to a few seconds [5, 6].", "startOffset": 168, "endOffset": 174}, {"referenceID": 20, "context": "Learning with respect to R(\u03c0, r) has several advantages over maximum likelihood BC in the context of sequential decision making [21].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 133, "endOffset": 141}, {"referenceID": 22, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 133, "endOffset": 141}, {"referenceID": 5, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 244, "endOffset": 254}, {"referenceID": 23, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 244, "endOffset": 254}, {"referenceID": 24, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 244, "endOffset": 254}, {"referenceID": 25, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 244, "endOffset": 254}, {"referenceID": 26, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 244, "endOffset": 254}, {"referenceID": 27, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 284, "endOffset": 288}, {"referenceID": 28, "context": "Neural networks have gained widespread popularity due to their ability to learn robust hierarchical features from complicated inputs [22, 23], and have been used in automotive behavioral modeling for action prediction in car-following contexts [6, 24\u201327], lateral position prediction [28], and maneuver classification [29].", "startOffset": 318, "endOffset": 322}, {"referenceID": 29, "context": "By maintaining sufficient statistics of past observations in memory, recurrent policies [30, 31] disambiguate perceptually similar states by acting with respect to histories of, rather than individual, observations.", "startOffset": 88, "endOffset": 96}, {"referenceID": 30, "context": "By maintaining sufficient statistics of past observations in memory, recurrent policies [30, 31] disambiguate perceptually similar states by acting with respect to histories of, rather than individual, observations.", "startOffset": 88, "endOffset": 96}, {"referenceID": 31, "context": "using Gated Recurrent Unit (GRU) networks due to their comparable performance with fewer parameters than other architectures [32].", "startOffset": 125, "endOffset": 129}, {"referenceID": 32, "context": "Exponential linear units (ELU) were used throughout the network, which have been shown to combat the vanishing gradient problem while supporting a zero-centered distribution of activation vectors [33].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "In this work, we use Trust Region Policy Optimization (TRPO) to learn our human driving policies [34].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "GAIL [17] trains a policy to perform expert-like behavior by rewarding it for \u201cdeceiving\u201d a classifier trained to discriminate between policy and expert", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "We use the public Next-Generation Simulation (NGSIM) datasets for US Highway 101 [19] and Interstate 80 [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "We use the public Next-Generation Simulation (NGSIM) datasets for US Highway 101 [19] and Interstate 80 [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 34, "context": "The trajectories were smoothed using an extended Kalman filter [35] on a bicycle model and projected to lanes using centerlines extracted from the NGSIM CAD files.", "startOffset": 63, "endOffset": 67}, {"referenceID": 35, "context": "All experiments were conducted with the rllab reinforcement learning framework [36].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Specifically, if the acceleration predicted by the Intelligent Driver Model (IDM) [3] is less than an activation threshold of \u22122 m/s2, the vehicle then accelerates according", "startOffset": 82, "endOffset": 85}, {"referenceID": 36, "context": "Several approaches exist for encoding such information in handselected features relevant to the driving task [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "The second baseline model is a BC approach using mixture regression (MR) [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 37, "context": "Our MR model is a Gaussian mixture over the joint space of the actions and features, trained using expectation maximization [38].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "Greedy feature selection is used during training to select a subset of predictors up to a maximum feature count threshold while minimizing the Bayesian information criterion [39].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "For the lateral motion, MOBIL [4] is used to select the desired lane, with a proportional controller used to track the lane centerline.", "startOffset": 30, "endOffset": 33}, {"referenceID": 39, "context": "square error (RWSE) captures the deviation of a model\u2019s probability mass from real-world trajectories [40].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "The ability to accurately predict and simulate human driving behavior is critical for the development of intelligent transportation systems. Traditional modeling methods have employed simple parametric models and behavioral cloning. This paper adopts a method for overcoming the problem of cascading errors inherent in prior approaches, resulting in realistic behavior that is robust to trajectory perturbations. We extend Generative Adversarial Imitation Learning to the training of recurrent policies, and we demonstrate that our model outperforms rule-based controllers and maximum likelihood models in realistic highway simulations. Our model both reproduces emergent behavior of human drivers, such as lane change rate, while maintaining realistic control over long time horizons.", "creator": "LaTeX with hyperref package"}}}