{"id": "1701.06972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Deep Network Guided Proof Search", "abstract": "deep learning techniques lie apart at precisely the heart of establishing several already significant ai advances in recent years including object recognition and detection, body image captioning, machine translation, speech recognition and synthesis, encoding and playing the game of go. automated first - order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program - analysis, theory reasoning, security, interpolation, and system verification. here we suggest deep learning based guidance in the proof search of accelerating the theorem prover e. h we train and compare several deep neural network models on the traces of existing atp prepared proofs of mizar written statements and use them to select processed algebraic clauses during ongoing proof search. thus we give experimental evidence that with a complex hybrid, two - click phase approach, deep intel learning based guidance can significantly reduce the average number of proof search matching steps while significantly increasing the number of theorems proved. using a few proof guidance strategies that leverage deep neural networks, we have found first - order proofs solutions of mere 7. 36 % of the first - order logic translations of the mizar mathematical library theorems that did well not previously have atp \u2013 generated proofs. this increases the ratio of statements in the corpus with atp generated proofs increasing from 56 % to 59 %.", "histories": [["v1", "Tue, 24 Jan 2017 16:39:05 GMT  (263kb,D)", "http://arxiv.org/abs/1701.06972v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["sarah loos", "geoffrey irving", "christian szegedy", "cezary kaliszyk"], "accepted": false, "id": "1701.06972"}, "pdf": {"name": "1701.06972.pdf", "metadata": {"source": "CRF", "title": "Deep Network Guided Proof Search", "authors": ["Sarah Loos", "Geoffrey Irving", "Christian Szegedy", "Cezary Kaliszyk"], "emails": ["smloos@google.com", "geoffreyi@google.com", "szegedy@google.com", "cezary.kaliszyk@uibk.ac.at"], "sections": [{"heading": null, "text": "Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification.\nHere we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved.\nUsing a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56% to 59%."}, {"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Motivation", "text": "In the past twenty years, various large corpora of computer-understandable reasoning knowledge have been developed (Harrison et al., 2014). Apart from axioms, definitions, and conjectures, such corpora include proofs derived in the selected logical foundation with sufficient detail to be machine-checkable. This is either given in the form of premises-conclusion pairs (Sutcliffe, 2009) or as procedures and intermediate steps (Wenzel, 1999). The development of many of these formal proofs required dozens of person-years, their sizes are measured in tens of thousands of human-named theorems and the complete proofs contain billions of low-level inference steps.\nThese formal proof libraries are also interesting for AI-based methods, with tasks such as concept matching, theory exploration, and structure formation (Autexier & Hutter, 2015). Furthermore, the AI methods can be augmented by automated reasoning: progress in the development of efficient first-order automated theorem provers (ATPs) (Kov\u00e1cs & Voronkov, 2013) allows applying them not only as tools that redo the formal proofs, but also to find the missing steps (Urban, 2006). Together with proof translations from the richer logics of the interactive systems to the simpler logics of the ATPs this becomes a commonly used tool in certain interactive provers (Blanchette et al., 2016). Many significant proof developments covering both mathematics and computer science have been created using such technologies. Examples include the formal proof of the Kepler conjecture (Hales et al., 2015), or the proof of correctness of the seL4 operating system kernel (Klein et al., 2010).\nar X\niv :1\n70 1.\n06 97\n2v 1\n[ cs\n.A I]\n2 4\nJa n\n20 17\nDespite the completeness of the employed proof calculi, modern ATPs perform poorly in the presence of large fact libraries. For this reason AI-based heuristic and learning techniques are used to preselect lemmas externally (K\u00fchlwein et al., 2012). Even with external selection Alama et al. (2012) show that ATPs are only able to find proofs of up to twenty human steps among the proofs in the Mizar Mathematical Library (Grabowski et al., 2015), whereas the library contains a number of proofs with hundreds of steps. In contrast to external lemma selection, guiding an ATP internally has much better potential as the complete proof state is known. For the tableaux calculus the Machine Learning Connection Prover (MaLeCoP) (Urban et al., 2011) shows that using machine learning to choose the next step can reduce the number of inferences in the proofs on average by a factor of 20. As the most competitive ATPs today are not based on tableau, but instead rely on the superposition calculus, in this paper we investigate guiding the state-of-the-art automated prover E (Schulz, 2013)1 using deep neural networks.\nDeep convolutional neural networks (LeCun et al., 1998) lie at the heart of several recent AI breakthroughs in the past few years. Deep convolutional networks have been instrumental in speech recognition (Hinton et al., 2012a) and natural language processing (Kim, 2014). Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks. DeepMind\u2019s AlphaGo (Silver et al., 2016) demonstrated superhuman performance in playing the game of Go by utilizing deep convolution neural networks that evaluate board positions. Deep hierarchical convolutional networks have been responsible for vast improvements in speech and sound generation (van den Oord et al., 2016).\nThe wide applicability of deep neural architectures suggest their potential usefulness for guiding the combinatorial search for proofs of mathematical theorems as well."}, {"heading": "1.2 Contributions", "text": "For the first time, we evaluate deep network models as a proof guidance method inside an automated theorem prover. We describe the technicalities that need to be addressed for a successful integration of relatively slow neural network models in a system that relies on very fast exploration.\nExperimental results are given on a large corpus of mathematical statements translated from the Mizar Mathematical Library (Grabowski et al., 2010) to first-order logic (Urban, 2006) which covers significant parts of basic mathematics ranging from discrete mathematics and mathematical logic to calculus and algebra.\nApplying deep network models inside the proof search process is challenging due to the relatively expensive nature of neural network inference. During the time one clause is evaluated by a neural network (on CPU), several hundreds or thousands of superposition steps may be performed by the prover. This means that proof guidance via neural networks has to provide very good quality suggestions to have a positive effect on the proof process. In this paper we describe two important technical details that were critical for getting improved results with neural network guidance.\nFirst, interleaving network guidance and hard-coded heuristics is still helpful for improving the performance over using the neural network alone to pick the next processed clause.\nSecond, we use a two-phase approach where a network-guided phase is followed by a phase using only fast heuristics. Without this second phase, the network-guided approach often processes only 1% of the clauses of a search with only fast heuristics. While the network-guided phase is slow, its high quality sets up the rest of the search process at a better starting point.\n1Version 1.9.1pre014\nIn essence, if we spend half of the time guiding the search and spend the rest finishing from that starting point, we get significant improvements over lower-quality clause selection that has a higher chance of losing its way in the search process and omitting the selection of crucial clauses that are critical for finishing the proof."}, {"heading": "2 Related Work", "text": "Suttner & Ertel (1990) proposed a multilayer-perceptron based proof search guiding algorithm on top of hand engineered features. Denzinger et al. (1999) surveyed learning and knowledge base based approaches to guide automated theorem provers. Schulz (2000) proposed a k-NN based learning methodology with hand-crafted features on a normalized form of the clauses for the selecting the next clause to be processed. Recently, for higher-order logic, na\u00efve Bayes and particle swarm based approaches were proposed for the internal guidance of the givenclause algorithm in Satallax (F\u00e4rber & Brown, 2016). FEMaLeCoP uses na\u00efve Bayes to guide its tableaux proof search (Kaliszyk & Urban, 2015b). For the theorem prover E, a handengineered similarity function with a few learned weights was proposed for better selection of processed clauses. All these approaches rely on carefully engineered features, whereas our deep learning method learns end-to-end from the textual representation or the syntax tree of the proof clauses.\nRecently, there have been several attempts to apply deep learning to theorem proving and reasoning in general. Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007). A fully differential theorem prover was tested on a few toy problems by Rockt\u00e4schel & Riedel (2016). Entailment analysis via deep learning was proposed by Rockt\u00e4schel et al. (2015)."}, {"heading": "3 Theorem Proving Preliminaries", "text": ""}, {"heading": "3.1 The First-Order Logic Prover E", "text": "We target the saturation-based first-order logic theorem prover E (Schulz, 2013). E preprocesses all inputs into clausal normal form (CNF) and conducts the proof search in this form. A firstorder formula is in CNF if it is a conjunction of clauses where each clause is a disjunction of literals, where each literal is a (possibly negated) application of an k-ary predicate symbol to k terms, and where the terms are recursively either variables or functions applied to terms.\nE\u2019s proof search is parametrized by variety of arguments, the most important of which is the clause selection heuristic which we focus on in this work. In saturation-based theorem proving two sets of clauses are manipulated: a set of unprocessed clauses initialized to the clausal form of the problem and an initially empty set of processed clauses. At each step the heuristic chooses one of the unprocessed clauses, generates all its consequences (adding them to the set of unprocessed clauses), then moves the clause into the processed clauses set. The order in which the heuristic processes the clauses greatly impacts how long it will take before a proof is found. E\u2019s auto mode inspects a problem and selects a clause search heuristic and other parameters likely to perform well.\nThe most successful heuristics in E are hybrid heuristics that select the next clause by targeting different criteria in a round-robin fashion. For example, a simplistic hybrid heuristic\nmight alternate between selecting clauses in a FIFO ordering and selecting the shortest clause from the set of unprocessed clauses. These two heuristics in isolation may miss selecting important clauses, but improve significantly when used in combination. The E prover implements hybrid heuristics by maintaining a separate ranking of all unprocessed clauses for each of the selection criteria. It then processes the top clause from each ranking, allowing for an arbitrary interleaving of selection criteria.\nIn this paper, we demonstrate that, given fixed time and memory limits, we can prove more statements when we add machine learning based classification to the clause selection heuristic."}, {"heading": "3.2 Mizar First-Order Problems", "text": "The Mizar Mathematical Library (MML) (Grabowski et al., 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning. The most extensive evaluation of AI-ATP methods on the library has been performed by Kaliszyk & Urban (2015a) on MML version 4.181.1147. The set of 57,882 Mizar theorems translated to first-order logic has been chronologically ordered together with about 90,000 other formulas (mostly to express Mizar\u2019s type system).\nThe first-order problems used to guide E prover in this paper are all distinct proofs found in Kaliszyk & Urban (2015a). This includes both proofs based on the human dependencies that the ATPs can redo and proofs found by running the ATPs on the predicted dependencies. Of the 57,882 Mizar theorems in FOL, 32,521 (about 56%) of them have at least one ATP proof with a total of 91,877 distinct ATP proofs.\nWe use these 91,877 proofs to train and evaluate our neural networks. We randomly assign them by conjecture into a training and a validation set using a 90%-10% split. By splitting the proofs by conjecture, we avoid contamination of our evaluation set. If one conjecture has multiple proofs, then all of those proofs are assigned to the training set, or all are assigned to the evaluation set. The training set contains 82,718 unique proofs of 29,325 conjectures. The remaining 3,196 conjectures and their 9,159 proofs are assigned to the evaluation set. When we evaluate clause selection heuristics over this evaluation set, we refer to them as the easy statements, since they have previously been proved by some ATP method. We call the 25,361 conjectures for which no ATP proof has been found the hard statements. These theorems taken together comprise the 57,882 Mizar theorems in FOL."}, {"heading": "4 Deep Networks", "text": "Here we describe the data collection, network architectures, and training methodology of our deep learning models to be incorporated into the proof search procedure of the automated theorem prover E by Schulz (2013).\nWe have reused several architectural and training choices that proved successful in the earlier related premise selection work by Alemi et al. (2016). Our models have two inputs: a negated conjecture to be proved and an unprocessed clause, both in CNF. Each is reduced to a fixed size vector using an embedding network, and a combiner network combines the two embedding vectors into a score for use as a clause selection heuristic. Ideally, the score would depend on the current set of processed clauses, but we have restricted the inputs to negated conjecture\nand clause for simplicity and speed. Thus, the overall architecture is\nvc = femb(clause, wc) vnc = femb(negated-conjecture, wnc)\np(useful|c, nc) = \u03c3(gcomb(vc, vnc, wcomb))\nwhere femb and gcomb are the embedding and combiner networks, wc, wnc, and wcomb are the weights to be learned, and \u03c3(z) = 1/(1 + e\u2212z) is sigmoid (Figure 1, left). Note that we use the same architecture to embed both negated conjecture and clause, but with different learned weights.\nAt training time, the output probabilities are trained using logistic loss towards p = 1 if the clause was used in the proof, and towards p = 0 otherwise. At proof time, the unprocessed clause with maximum probability is chosen to be processed.\nFor the embedding network femb, we have tried three architectures:\n1. A simple convolutional network with a few convolutional layers followed by max-pooling.\n2. A WaveNet (van den Oord et al., 2016) style network that can effectively model long range dependencies.\n3. Recursive neural networks (Goller & Kuchler, 1996) whose topology depends on the syntax tree of the formula to be evaluated.\nIn sections 4.3, 4.4, and 4.5, we give a detailed description of the models evaluated and compare their predictive performance after describing the training and evaluation data in section 4.1. The end-to-end system was only evaluated using the fastest models and those with very good prediction performance. Later, we use the best models to generate new ATP proofs for the \u201chard subset\u201d of the Mizar corpus where current ATP methods have failed."}, {"heading": "4.1 Data Collection", "text": "As described in Section 3.2, we have split the 32,521 theorems of Mizar that have at least one proof into a training and validation set using a 90%-10% split. The proofs in our dataset originate from various first-order ATPs. Furthermore, even for proof traces originating from\nE, the configuration of the preprocessing (skolemization and clausification) has a big effect on the the symbols that appear in the clauses and their shape. Therefore, we have replicated the proofs using a single, consistent configuration of the preprocessing procedure of E in order to avoid mismatch between our training set and the clauses seen at test time when we guide the search with the trained model. Specifically, we train on proofs generated using the Auto208 configuration detailed in the appendix."}, {"heading": "4.2 Architectures", "text": "We consider three architectures for the embedding networks femb: simple convolutional models, WaveNet (van den Oord et al., 2016) style models, and recursive networks (Goller & Kuchler, 1996). All experiments use the same combiner network gcomb with one 1024-unit hidden layer:\ngcomb(vc, vnc) =W2 relu(W1 concat(vc, vnc) + b1) + b2\nwhere W1 \u2208 R1024\u00d72 dim v, W2 \u2208 R1\u00d71024, b1 \u2208 R1024, and b2 \u2208 R. At data collection time, we collect all symbols (constants, function names, variable names, logical operations, and parentheses) into a vocabulary and the input layer converts each into an embedding vector of dimension 1024 using table lookup. The embeddings are initialized randomly and learned during the training process. We shared the same embedding vectors between clause and negated conjecture embedding. The convolutional and WaveNet models receive clause and negated conjecture as sequences of these embeddings; the recursive networks use embeddings only for names at leaves of the CNF trees. Unlike Alemi et al. (2016), we avoid character-level embeddings since proof search can involve very large clauses with large computational cost.\nAll models were trained using TensorFlow (Abadi et al., 2015) and the Adam (Kingma & Ba, 2014) optimizer."}, {"heading": "4.3 Simple Convolutional Model", "text": "Following the premise selection models of Alemi et al. (2016), our convolutional models (\u201cCNNs\u201d for \u201cconvolutional neural networks\u201d) have relatively shallow depth as they give good results on that related task. They consist of a stack of three one-dimensional convolutional layers, each layer with patch size 5 (the number of inputs each output is connected to), stride 1, and a rectified linear activation. We have tried several models with varying feature dimensions."}, {"heading": "4.4 WaveNet Model", "text": "WaveNet (van den Oord et al., 2016) is a special type of hierarchical convolutional network that employs dilated convolutions and residual connections. The dilated convolutions allow long range feature dependencies with only moderate overhead, as higher layers have geometrically increasing dilation factors (see Figure 2). While van den Oord et al. (2016) use causal dilated convolutions, we use symmetric convolutions as our task is discrimination, not generation. Our WaveNet embedding network consists of 3 WaveNet blocks, where each block consists of 7 convolutional layers dilated by d = 1, 2, 4, . . . , 64. We use the gated activation tanh(x)\u03c3(x\u2032) of van den Oord et al. (2016), and residual connections for both layers and blocks (He et al., 2015). For regularization, some experiments use 20% token-wise dropout at the input layer and\n20% feature-wise dropout at the input to each block (Hinton et al., 2012b). Overall,\nfemb(x) = (B \u25e6B \u25e6B)(Dt(x, p)) B(x) = x+ (L64 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 L2 \u25e6 L1)(Df (x, p)) Ld(x) = x+ tanh(Cd(x))\u03c3(C \u2032 d(x))\nCd(x)i = b+ s\u2211 j=1 wjxi\u2212d(j\u2212ds/2e)\nwhere Dt(x, p) sets each token embedding to zero with probability p, Df (x, p) sets each individual feature to zero with probability p, Cd and C \u2032d are convolutions with distinct weights and dilation factor d, and s = 3 is the patch size of the convolution. Here x,B,Ld, Cd are all sequences of vectors, or 2-D overall. Our experiments use vectors of dimension 256 and 640."}, {"heading": "4.5 Recursive Neural Network", "text": "Our third model class is recursive neural networks (Socher et al., 2011) that are constructed by the parse tree of the first-order logic formulas. The neural network topology reflects the parse tree and is wired on the fly using gather operations in TensorFlow.\nTo simplify the tree, all function applications are curried, giving the following types of layers:\n\u2022 apply nodes that apply a function with exactly two children. \u2022 or nodes that compute the embedding for the disjunction of exactly two children. \u2022 and nodes that compute the embedding for the conjunction of exactly two children. This\nis used only for embedding the negated conjecture, since the proof clauses do not contain conjunctions.\n\u2022 not nodes that compute the embedding for the negation of a single child node. The weights of each type of layer is shared across the same tree. This means that the layer weights are learned jointly for the same formula, as they tend to contain multiple instances of the node of the same type.\nAt the leaves of the tree, we have the constants that can represent functions of various arity. These symbols have their associated embeddings which are initialized randomly and learned together with the rest of the network.\nEach of these internal nodes are represented by either a fully connected layer with ReLU activation or tree LSTM network (Tai et al., 2015). They aggregate the fixed size children and output a vector that has the length of the pre-set output embedding size (or two vectors for tree\nLSTMs). Our tree LSTMs use separate forget gains per input and include off-diagonal forget gate terms (see Figure 3 and Tai et al. (2015) for details).\nThe network learns independent weight matrices for these three (four for negated conjectures) transformations. Although the same method is used for computing the embedding for the proof-step clause and for the embedding, their weights are not shared: we end up with seven sets of weights to learn in total. We tested the common embedding sizes of 256 and 512. Greater embedding size gave rise to more accurate models."}, {"heading": "5 Experimental Results", "text": "After training the deep neural networks as described in Section 4, we evaluate them using three measures, each more conclusive and computationally expensive than the last.\nThe first metric, presented in Section 5.1, checks that a trained model can accurately predict whether a clause was used in the final proof. This accuracy test is done on a holdout set of proof tasks from the training data.\nNext, in Section 5.2, we run E prover over the same holdout set of 9,159 FOL proof tasks from Kaliszyk & Urban (2015a) using the trained networks to help guide the clause selection. These theorems all have at least one ATP proof using classical search heuristics, and 96.6% of these theorems can be proved using one of E prover\u2019s built-in automated heuristics, so there is little room for improvement on this dataset. However, it allows us to perform a sanity check that the neural nets aren\u2019t doing more harm than good when added to the auto heuristic.\nThe final test is the most interesting but computationally expensive: do these deep network guided selection heuristics allow us to prove Mizar statements that do not yet have any ATP proof? For this, we use a corpus of 25,361 theorems from Mizar for which no proof was found in Kaliszyk & Urban (2015a). Because this is the most computationally expensive task, we only run it using the networks that performed best on the previous tasks. In Section 5.3, we present the models that, in aggregate, allow us to find proofs for 7.36% of these \u201chard statements\u201d."}, {"heading": "5.1 Accuracy Evaluations", "text": "Table 1 shows the accuracy on a 50-50% split of used and unused clauses of the holdout set of the conjectures. Here CNN-N\u00d7L is a convolutional net with L layers of N dimensions, and Tree-Type-N\u00d7L is a tree RNN or LSTM with L layers of N dimensions at each node of the input tree. WaveNet-N\u00d7B\u00d7L has B blocks, with L layers, of dimension N . We include (D%) to indicate that a dropout of D% is used as a regularizer during training.\nThe WaveNet 640 with dropout has the best accuracy at 81.5% accuracy, but many of the CNN and WaveNet models noticeably outperform the others. Note that the time required for evaluating the models for a set of examples varies widely. Given the fact that we limit the overall running time instead of the number of evaluations performed, higher quality but slower models might perform better in the system than slightly worse but much faster models. However with specialized hardware for neural network evaluation, we can expect that the prediction quality of the models will gain in importance compared to their running time."}, {"heading": "5.2 Experiments on Statements with Existing ATP Proofs", "text": "The clause selection heuristic is one of the most important parts of E prover, as it is the primary driver of the proof search. With a good heuristic, a proof may be found in a relatively small number of search steps. In this section, we use the models from Section 5.1 with the highest accuracy as the clause selection heuristic inside of E.2 These models, which are trained to select clauses that are most likely to contribute to a proof, now assign a score to each unprocessed clause, namely the trained value of p(useful|c,nc). In other words, the probability that a clause\n2Due to its huge memory footprint, we were unable to experiment with WaveNet-1024 in E prover.\nc, is used in the final proof, given the set of negated conjectures nc. E prover uses this score to rank the set of unprocessed clauses, then at each step processes the clause that has the best ranking.3 Since many new unprocessed clauses are generated when each clause is processed, this selection order is crucial. A good clause selection heuristic can be the difference between finding a proof after a small number of search steps, and finding a proof only after years of computation time.\nWhile our ultimate goal is to develop a heuristic that is powerful enough to prove the challenging statements that don\u2019t yet have an ATP proof, experimenting on this dataset is computationally very expensive. Instead, we kept a holdout set of 9,159 statements which we did not use as training data. These statements already have existing ATP proofs and tend to be computationally less intensive, so we can use this holdout set to run more experiments and directly compare with proofs generated by an existing Auto strategy, which we selected as the best-performing strategy on this dataset as discussed in the appendix.\nIn this section, we find that deep neural nets, which are computationally very expensive even when used for inference, are most effective when used in combination with existing heuristics. We also use the holdout set to investigate which of the model architectures presented in Section 4 are most effective."}, {"heading": "5.2.1 Guidance Design", "text": "In our experiments, we found that our computation heavy neural networks did much better when they were used in concert with existing, fast heuristics. In this section, we present different approaches to using neural networks to guide existing heuristics.\n1. The Auto heuristic is standard in E (see appendix). This approach is already a hybrid of several weighting functions and tuned parameters, but does not include any inference from a deep neural network.\n2. A pure neural network heuristic scores the clauses using only the trained neural network. This approach is slow, as it must evaluate all clauses with the expensive neural network. It also can not take advantage of ranking based on different heuristics, as the Auto function does, and we observe that it does not perform well on its own.\n3. A hybrid approach alternates between deep network based guidance and the fast Auto heuristic. This augments the standard clause selection methods with additional neural network suggested clauses. While this process still can do relatively few proof search steps due to the slowness of the neural network evaluations, which must still be done on all unprocessed clauses to obtain a rank ordering.\n4. A switched approach, which uses deep network guidance in the first phase of the computation (either pure or hybrid). As time resources begin to run out, we switch to Auto, a traditional complete search phase, which can process a lot of clauses without the overhead of the deep network guidance.\nFigure 4 left shows a direct comparison of these approaches to proof guidance using a simple CNN in all cases. Not surprisingly, the pure CNN does not perform well on its own; however, when we allow E prover to alternate between the CNN heuristic and the Auto heuristic, the hybrid outperforms both the Auto and CNN heuristics for lower limits on the processed clauses. Still, because the hybrid approach has all the overhead of the deep network evaluation, we see this hybrid method bottoms out after around 7,500 processed clauses, due to a lack of resources.\n3E prover uses a lowest-is-best ordering, so in implementation we use \u2212p(useful|c, nc).\nThe best of the guidance approaches we explored was the switched approach, which uses the Auto heuristic to avoid running out of resources at a relatively small number of processed clauses. The intuition behind this is that we expect the standard heuristics to be too weak to avoid combinatorial explosion while neural network guidance is still too slow on current hardware. If we run a guided proof search alone for awhile, it could end up selecting all the essential clauses for the proof, but it might fail to close to proof due to its slowness. Our \u201cswitched\u201d approach gives a cure to this shortfall of the network guided approach.\nThroughout the remainder of this paper, all the proof guidance methods employ the twophase \u201cswitched\u201d approach that first runs a hybrid network-guided phases for 20 minutes, followed by a standard heuristics-based phase for 10 minutes."}, {"heading": "5.2.2 Comparison of Model Performance on Easy Statements", "text": "In this section we present the performance of four WaveNet and two CNN models used to guide clause selection in E prover. These are the models with the highest accuracy from Table 1, which are still small enough to fit in memory. We also include all three models trained on a data set that includes unprocessed clauses as negative examples, these models are indicated with a (*). All experiments use the E theorem prover, version 1.9.1pre014 (Schulz, 2013), with some modifications to enable integration with the trained deep neural networks as clause selection heuristics.\nIn Figure 4 right and Table 2, we show the performance of each of these models with various processed clause limits. All of the heuristics that benefit from deep neural network guidance outperform the Auto baseline, regardless of the number of clauses processed before a proof is found. Both CNNs (embedding size 256) do very well, but we notice that the CNN* trained with unprocessed clauses as negative examples begins to do better with higher processed clause limits, which may better represent the clauses that are evaluated after several thousand clauses are already processed. Somewhat surprisingly, the WaveNet models (embedding size 256 and 640), which were more accurate at predicting whether a clause would be used in the final proof, did not perform as well at the proof guidance task. This is likely because the WaveNet models\nare much more resource heavy, and therefore could not evaluate as many clauses as the CNNs given the same resources.\nWhile these results indicate that the that guidance from deep neural networks can provide some boost to traditional search heuristics, the Auto heuristic alone already performs very well on this dataset, so the improvements are minor. The real test is on the set of \u201chard statements,\u201d which did not previously have ATP generated proofs."}, {"heading": "5.3 Experiments on Hard Statements", "text": "Here we describe our results on a corpus of 25,361 theorems from the Mizar corpus for which no proof was found by any of the provers, strategies, and premise selection methods considered by Kaliszyk & Urban (2015a). We will call these \u201chard statements\u201d in the rest of the paper.\nAlthough all the hard statements have human proofs, this subset could neither be proved by theorem prover E (Schulz, 2013) nor by Vampire (Kov\u00e1cs & Voronkov, 2013) with a timeout limit of 15 minutes and default settings (auto and casc heuristics respectively) using the premises derived from the human given proofs. Also note that the premises passed to the theorem prover form a very rough over-approximation of the set of necessary premises. Often around 10 to 20 dependencies are enough to prove the theorems, but the number of dependencies in the backward envelope can be in the thousands.\nThe main bottleneck for network guided proof search is the evaluation time for the ranking of clauses to be processed next. Given that we do not use special hardware for this purpose, our proof search is completely dominated by the deep network evaluation.\nIf the number of premises is very high in the beginning, there is a much bigger hit on the deep network guided proof search due to the larger number of clauses evaluations per step. This motivates the use of additional premise selection as an important additional component.\nStill, we first present a baseline without premise selection and show that the relatively fast premise selection phase is critical for good performance on the hard theorems with or without deep network guidance. In the second subsection, we turn on premise selection and compare various proof guidance models against the baseline auto heuristic."}, {"heading": "5.3.1 The Importance of Premise Selection", "text": "Here we start with a comparison of four different approaches on the hard statements. These include guided and unguided proof search with and without premise selection.\nFor premise selection, we use the character level model from the DeepMath paper (Alemi et al., 2016). The model is a very similar model to our convolutional proof guidance model, but was trained on premise-conclusion pairs from a training set that was randomly chosen from the 56% ATP-proved statements. One main difference is that our proof guidance convolutional network is word level with an embedding learned for each token, whereas the premise selection network takes character sequences as input. This limits quality, as the DeepMath paper suggests that using a word level model with definitional embeddings gives the best results. We have opted for the lower quality character level premise selection model for simplicity.\nAfter premise selection we run four proof attempts: first the premises are ranked by the model scores and the top 32, 64, 128, and 256 different premises selected (as long as the original number of premises is not exceeded). We run E prover using the selected set of premises as long as it does not find a proof. We stop searching for a proof when a proof is found for any subset of the premises.\nThe experimental results are given in Table 3. We give experimental evidence that the switched approach outperforms the unguided proof search. We have tested two different premise selection models to get a feel for the variability and complementarity of the results. It turns out that different premise selection models even with the same architecture can introduce a lot of variations that can lead to different number of theorems proved, but can result in choices that complement each other well and help the proof search process by increasing the diversity of starting set of premises."}, {"heading": "5.3.2 Comparison of Model Performance on Hard Statements", "text": "All the proof guidance methods employed the two-phase approach that first runs a hybrid network-guided phases for 20 minutes, followed by a standard heuristics based phase for 10 minutes.\nHere we tried two different premise selection models \u201cDeepMath 1\u201d and \u201cDeepMath 2\u201d using the same character level convolutional architecture as the best character level model described in (Alemi et al., 2016). This way, we can evaluate the stability of result with respect to the premise selection strategy. One can see that although the two models are trained the same way and have comparable quality, they result in significantly different sets of theorems proved, but\nthe number of theorems proved using both models is very similar and the \u201cswitch\u201d strategy using the simpler CNN performed best for both premise selection and proof guidance by a significant margin.\nThe experimental results are given in Table 4. The CNN models use a simple three layer convolutional networks to rank the unprocessed clauses while the WaveNet models use the WaveNet architecture which is significantly slower to evaluate, but produced better proxy metrics on the holdout set. Note that the simple convolutional network based approach proves 86% more theorems than the unguided approach (4.34% versus 2.33% of the hard statements). The overall number the statements proved by any means in this paper (including the use of non-switched neural guidance) is 1,866 which is 7.36% of all the hard statements."}, {"heading": "6 Conclusion", "text": "We have demonstrated the feasibility of guiding first-order logic proof search by augmenting the given clause selection method with ranking by deep neural networks. With a properly engineered mixture of neural guidance and hand-crafted search strategies, we get significant improvements in first-order logic prover performance, especially for theorems that are harder and require deeper search.\nDue to the slowness of neural network evaluation, our method leads to an increased number of successful proofs only if we utilize it in a two-phase approach where a deep network guided phase is followed by a faster combinatorial search with given clauses selected quickly using the existing hand-crafted clause selection strategies.\nAdditionally, we established that both the prediction accuracy of the deep network and its speed are important for increased proving power. For example, our WaveNet model yields higher accuracy on the holdout set than a simple convolutional network, but due to its much higher computational cost, when used as a clause scorer in E prover, it still proves fewer theorems under the same time constraints than the cheaper but lower quality convolutional network.\nCurrently our approach is computationally expensive, and we allow 30 minutes per proof compared to 15 minutes for previous work. However, this extra time does not significantly help existing heuristics unless combined with neural proof guidance (see appendix). This suggests\nthat the neural guidance represents a substantial improvement in constraining the search space. Moreover, we expect that the inroads of specialized hardware for deep learning will mitigate much of the computational overhead. This would allow higher performing models to play their strength with dramatic efficiency and quality gains in the future.\nOur approach is just the first step of applying deep learning to guiding combinatorial search processes and it is arguable that working with a purely syntactic input format the derived set of features is not strong enough to create a semantically relevant representation of mathematical content. Besides improving theorem proving, our approach has the exciting potential to generate higher quality training data for systems that study the behavior of formulas under a set of logical transformations. This could enable learning representation of formulas in ways that consider the semantics not just the syntactic properties of mathematical content and can make decisions based on their behavior during proof search."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "on heterogeneous systems,", "citeRegEx": "Talwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Talwar et al\\.", "year": 2015}, {"title": "Automated and Human Proofs in General Mathematics: An Initial Comparison", "author": ["Jesse Alama", "Daniel K\u00fchlwein", "Josef Urban"], "venue": "LPAR, volume 7180 of LNCS,", "citeRegEx": "Alama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2012}, {"title": "Deepmath-deep sequence models for premise selection", "author": ["Alexander A Alemi", "Francois Chollet", "Geoffrey Irving", "Niklas Een", "Christian Szegedy", "Josef Urban"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Alemi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemi et al\\.", "year": 2016}, {"title": "Structure formation in large theories", "author": ["Serge Autexier", "Dieter Hutter"], "venue": "Intelligent Computer Mathematics - International Conference,", "citeRegEx": "Autexier and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Autexier and Hutter.", "year": 2015}, {"title": "Mizar: State-of-the-art and beyond", "author": ["Grzegorz Bancerek", "Czes\u0142aw Byli\u0144ski", "Adam Grabowski", "Artur Korni\u0142owicz", "Roman Matuszewski", "Adam Naumowicz", "Karol P\u0105k", "Josef Urban"], "venue": "Intelligent Computer Mathematics - International Conference,", "citeRegEx": "Bancerek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bancerek et al\\.", "year": 2015}, {"title": "Hammering towards QED", "author": ["Jasmin Christian Blanchette", "Cezary Kaliszyk", "Lawrence C. Paulson", "Josef Urban"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Understanding LSTM networks, 2015. URL https://colah.github.io/posts/ 2015-08-Understanding-LSTMs", "author": ["Chris Olah"], "venue": null, "citeRegEx": "Olah.,? \\Q2015\\E", "shortCiteRegEx": "Olah.", "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Learning from Previous Proof Experience", "author": ["J\u00f6rg Denzinger", "Matthias Fuchs", "Christoph Goller", "Stephan Schulz"], "venue": "Technical Report AR99-4,", "citeRegEx": "Denzinger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Denzinger et al\\.", "year": 1999}, {"title": "Internal guidance for Satallax", "author": ["Michael F\u00e4rber", "Chad E. Brown"], "venue": "International Joint Conference on Automated Reasoning (IJCAR 2016),", "citeRegEx": "F\u00e4rber and Brown.,? \\Q2016\\E", "shortCiteRegEx": "F\u00e4rber and Brown.", "year": 2016}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "Goller and Kuchler.,? \\Q1996\\E", "shortCiteRegEx": "Goller and Kuchler.", "year": 1996}, {"title": "Mizar in a nutshell", "author": ["Adam Grabowski", "Artur Korni\u0142owicz", "Adam Naumowicz"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "Four decades of Mizar - foreword", "author": ["Adam Grabowski", "Artur Korni\u0142owicz", "Adam Naumowicz"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2015}, {"title": "A formal proof of the Kepler conjecture", "author": ["Thomas C. Hales", "Mark Adams", "Gertrud Bauer", "Dat Tat Dang", "John Harrison", "Truong Le Hoang", "Cezary Kaliszyk", "Victor Magron", "Sean McLaughlin", "Thang Tat Nguyen", "Truong Quang Nguyen", "Tobias Nipkow", "Steven Obua", "Joseph Pleso", "Jason Rute", "Alexey Solovyev", "An Hoai Thi Ta", "Trung Nam Tran", "Diep Thi Trieu", "Josef Urban", "Ky Khac Vu", "Roland Zumkeller"], "venue": "CoRR, abs/1501.02155,", "citeRegEx": "Hales et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hales et al\\.", "year": 2015}, {"title": "History of interactive theorem proving", "author": ["John Harrison", "Josef Urban", "Freek Wiedijk"], "venue": "URL http://www.sciencedirect.com/science/article/ pii/B9780444516244500046", "citeRegEx": "Harrison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Harrison et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "MizAR 40 for Mizar 40", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "FEMaLeCoP: Fairly efficient machine learning connection prover", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "20th International Conference,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "seL4: formal verification of an operating-system kernel", "author": ["Gerwin Klein", "June Andronick", "Kevin Elphinstone", "Gernot Heiser", "David Cock", "Philip Derrin", "Dhammika Elkaduwe", "Kai Engelhardt", "Rafal Kolanski", "Michael Norrish", "Thomas Sewell", "Harvey Tuch", "Simon Winwood"], "venue": "Commun. ACM,", "citeRegEx": "Klein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2010}, {"title": "First-order theorem proving and Vampire", "author": ["Laura Kov\u00e1cs", "Andrei Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Overview and evaluation of premise selection techniques for large theory mathematics", "author": ["Daniel K\u00fchlwein", "Twan van Laarhoven", "Evgeni Tsivtsivadze", "Josef Urban", "Tom Heskes"], "venue": "IJCAR, volume 7364 of LNCS,", "citeRegEx": "K\u00fchlwein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "K\u00fchlwein et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Metamath: A Computer Language for Pure Mathematics", "author": ["Norman Megill"], "venue": "URL http://us.metamath.org/ downloads/metamath.pdf", "citeRegEx": "Megill.,? \\Q2007\\E", "shortCiteRegEx": "Megill.", "year": 2007}, {"title": "Wavenet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Learning knowledge base inference with neural theorem provers", "author": ["Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "5th Workshop on Automated Knowledge Base Construction (AKBC)", "citeRegEx": "Rockt\u00e4schel and Riedel.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel and Riedel.", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Learning search control knowledge for equational deduction, volume 230 of DISKI", "author": ["Stephan Schulz"], "venue": "Infix Akademische Verlagsgesellschaft,", "citeRegEx": "Schulz.,? \\Q2000\\E", "shortCiteRegEx": "Schulz.", "year": 2000}, {"title": "Logic for Programming, Artificial Intelligence, and Reasoning - 19th International Conference, LPAR-19, volume 8312 of LNCS, pp. 735\u2013743", "author": ["Stephan Schulz"], "venue": null, "citeRegEx": "Schulz.,? \\Q2013\\E", "shortCiteRegEx": "Schulz.", "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "The TPTP problem library and associated infrastructure", "author": ["Geoff Sutcliffe"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Sutcliffe.,? \\Q2009\\E", "shortCiteRegEx": "Sutcliffe.", "year": 2009}, {"title": "Automatic acquisition of search guiding heuristics", "author": ["Christian Suttner", "Wolfgang Ertel"], "venue": "In International Conference on Automated Deduction,", "citeRegEx": "Suttner and Ertel.,? \\Q1990\\E", "shortCiteRegEx": "Suttner and Ertel.", "year": 1990}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher DManning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "MPTP 0.2: Design, implementation, and initial experiments", "author": ["Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Urban.,? \\Q2006\\E", "shortCiteRegEx": "Urban.", "year": 2006}, {"title": "MaLeCoP: Machine learning connection prover", "author": ["Josef Urban", "Ji\u0159\u00ed Vysko\u010dil", "Petr \u0160t\u011bp\u00e1nek"], "venue": "TABLEAUX, volume 6793 of LNCS,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}, {"title": "Isar - A generic interpretative approach to readable formal proof documents", "author": ["Markus Wenzel"], "venue": "Theorem Proving in Higher Order Logics, 12th International Conference, TPHOLs\u201999,", "citeRegEx": "Wenzel.,? \\Q1999\\E", "shortCiteRegEx": "Wenzel.", "year": 1999}, {"title": "Holophrasm: a neural automated theorem prover for higher-order logic", "author": ["Daniel Whalen"], "venue": "arXiv preprint arXiv:1608.02644,", "citeRegEx": "Whalen.,? \\Q2016\\E", "shortCiteRegEx": "Whalen.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "In the past twenty years, various large corpora of computer-understandable reasoning knowledge have been developed (Harrison et al., 2014).", "startOffset": 115, "endOffset": 138}, {"referenceID": 35, "context": "This is either given in the form of premises-conclusion pairs (Sutcliffe, 2009) or as procedures and intermediate steps (Wenzel, 1999).", "startOffset": 62, "endOffset": 79}, {"referenceID": 41, "context": "This is either given in the form of premises-conclusion pairs (Sutcliffe, 2009) or as procedures and intermediate steps (Wenzel, 1999).", "startOffset": 120, "endOffset": 134}, {"referenceID": 39, "context": "Furthermore, the AI methods can be augmented by automated reasoning: progress in the development of efficient first-order automated theorem provers (ATPs) (Kov\u00e1cs & Voronkov, 2013) allows applying them not only as tools that redo the formal proofs, but also to find the missing steps (Urban, 2006).", "startOffset": 284, "endOffset": 297}, {"referenceID": 5, "context": "Together with proof translations from the richer logics of the interactive systems to the simpler logics of the ATPs this becomes a commonly used tool in certain interactive provers (Blanchette et al., 2016).", "startOffset": 182, "endOffset": 207}, {"referenceID": 13, "context": "Examples include the formal proof of the Kepler conjecture (Hales et al., 2015), or the proof of correctness of the seL4 operating system kernel (Klein et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 22, "context": ", 2015), or the proof of correctness of the seL4 operating system kernel (Klein et al., 2010).", "startOffset": 73, "endOffset": 93}, {"referenceID": 25, "context": "For this reason AI-based heuristic and learning techniques are used to preselect lemmas externally (K\u00fchlwein et al., 2012).", "startOffset": 99, "endOffset": 122}, {"referenceID": 12, "context": "(2012) show that ATPs are only able to find proofs of up to twenty human steps among the proofs in the Mizar Mathematical Library (Grabowski et al., 2015), whereas the library contains a number of proofs with hundreds of steps.", "startOffset": 130, "endOffset": 154}, {"referenceID": 40, "context": "For the tableaux calculus the Machine Learning Connection Prover (MaLeCoP) (Urban et al., 2011) shows that using machine learning to choose the next step can reduce the number of inferences in the proofs on average by a factor of 20.", "startOffset": 75, "endOffset": 95}, {"referenceID": 32, "context": "As the most competitive ATPs today are not based on tableau, but instead rely on the superposition calculus, in this paper we investigate guiding the state-of-the-art automated prover E (Schulz, 2013)1 using deep neural networks.", "startOffset": 186, "endOffset": 200}, {"referenceID": 26, "context": "Deep convolutional neural networks (LeCun et al., 1998) lie at the heart of several recent AI breakthroughs in the past few years.", "startOffset": 35, "endOffset": 55}, {"referenceID": 20, "context": ", 2012a) and natural language processing (Kim, 2014).", "startOffset": 41, "endOffset": 52}, {"referenceID": 24, "context": "Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks.", "startOffset": 75, "endOffset": 139}, {"referenceID": 37, "context": "Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks.", "startOffset": 75, "endOffset": 139}, {"referenceID": 15, "context": "Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks.", "startOffset": 75, "endOffset": 139}, {"referenceID": 33, "context": "DeepMind\u2019s AlphaGo (Silver et al., 2016) demonstrated superhuman performance in playing the game of Go by utilizing deep convolution neural networks that evaluate board positions.", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "Even with external selection Alama et al. (2012) show that ATPs are only able to find proofs of up to twenty human steps among the proofs in the Mizar Mathematical Library (Grabowski et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 11, "context": "Experimental results are given on a large corpus of mathematical statements translated from the Mizar Mathematical Library (Grabowski et al., 2010) to first-order logic (Urban, 2006) which covers significant parts of basic mathematics ranging from discrete mathematics and mathematical logic to calculus and algebra.", "startOffset": 123, "endOffset": 147}, {"referenceID": 39, "context": ", 2010) to first-order logic (Urban, 2006) which covers significant parts of basic mathematics ranging from discrete mathematics and mathematical logic to calculus and algebra.", "startOffset": 29, "endOffset": 42}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here.", "startOffset": 84, "endOffset": 104}, {"referenceID": 7, "context": "Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007).", "startOffset": 98, "endOffset": 118}, {"referenceID": 27, "context": ", 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007).", "startOffset": 100, "endOffset": 114}, {"referenceID": 6, "context": "Denzinger et al. (1999) surveyed learning and knowledge base based approaches to guide automated theorem provers.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Denzinger et al. (1999) surveyed learning and knowledge base based approaches to guide automated theorem provers. Schulz (2000) proposed a k-NN based learning methodology with hand-crafted features on a normalized form of the clauses for the selecting the next clause to be processed.", "startOffset": 0, "endOffset": 128}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al.", "startOffset": 85, "endOffset": 171}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007). A fully differential theorem prover was tested on a few toy problems by Rockt\u00e4schel & Riedel (2016). Entailment analysis via deep learning was proposed by Rockt\u00e4schel et al.", "startOffset": 85, "endOffset": 484}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007). A fully differential theorem prover was tested on a few toy problems by Rockt\u00e4schel & Riedel (2016). Entailment analysis via deep learning was proposed by Rockt\u00e4schel et al. (2015).", "startOffset": 85, "endOffset": 565}, {"referenceID": 32, "context": "We target the saturation-based first-order logic theorem prover E (Schulz, 2013).", "startOffset": 66, "endOffset": 80}, {"referenceID": 12, "context": "The Mizar Mathematical Library (MML) (Grabowski et al., 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015).", "startOffset": 80, "endOffset": 103}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning.", "startOffset": 81, "endOffset": 333}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning. The most extensive evaluation of AI-ATP methods on the library has been performed by Kaliszyk & Urban (2015a) on MML version 4.", "startOffset": 81, "endOffset": 540}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning. The most extensive evaluation of AI-ATP methods on the library has been performed by Kaliszyk & Urban (2015a) on MML version 4.181.1147. The set of 57,882 Mizar theorems translated to first-order logic has been chronologically ordered together with about 90,000 other formulas (mostly to express Mizar\u2019s type system). The first-order problems used to guide E prover in this paper are all distinct proofs found in Kaliszyk & Urban (2015a). This includes both proofs based on the human dependencies that the ATPs can redo and proofs found by running the ATPs on the predicted dependencies.", "startOffset": 81, "endOffset": 868}, {"referenceID": 30, "context": "Here we describe the data collection, network architectures, and training methodology of our deep learning models to be incorporated into the proof search procedure of the automated theorem prover E by Schulz (2013). We have reused several architectural and training choices that proved successful in the earlier related premise selection work by Alemi et al.", "startOffset": 202, "endOffset": 216}, {"referenceID": 2, "context": "We have reused several architectural and training choices that proved successful in the earlier related premise selection work by Alemi et al. (2016). Our models have two inputs: a negated conjecture to be proved and an unprocessed clause, both in CNF.", "startOffset": 130, "endOffset": 150}, {"referenceID": 2, "context": "Unlike Alemi et al. (2016), we avoid character-level embeddings since proof search can involve very large clauses with large computational cost.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": "Following the premise selection models of Alemi et al. (2016), our convolutional models (\u201cCNNs\u201d for \u201cconvolutional neural networks\u201d) have relatively shallow depth as they give good results on that related task.", "startOffset": 42, "endOffset": 62}, {"referenceID": 15, "context": "(2016), and residual connections for both layers and blocks (He et al., 2015).", "startOffset": 60, "endOffset": 77}, {"referenceID": 27, "context": "WaveNet (van den Oord et al., 2016) is a special type of hierarchical convolutional network that employs dilated convolutions and residual connections. The dilated convolutions allow long range feature dependencies with only moderate overhead, as higher layers have geometrically increasing dilation factors (see Figure 2). While van den Oord et al. (2016) use causal dilated convolutions, we use symmetric convolutions as our task is discrimination, not generation.", "startOffset": 17, "endOffset": 357}, {"referenceID": 27, "context": "WaveNet (van den Oord et al., 2016) is a special type of hierarchical convolutional network that employs dilated convolutions and residual connections. The dilated convolutions allow long range feature dependencies with only moderate overhead, as higher layers have geometrically increasing dilation factors (see Figure 2). While van den Oord et al. (2016) use causal dilated convolutions, we use symmetric convolutions as our task is discrimination, not generation. Our WaveNet embedding network consists of 3 WaveNet blocks, where each block consists of 7 convolutional layers dilated by d = 1, 2, 4, . . . , 64. We use the gated activation tanh(x)\u03c3(x\u2032) of van den Oord et al. (2016), and residual connections for both layers and blocks (He et al.", "startOffset": 17, "endOffset": 686}, {"referenceID": 34, "context": "Our third model class is recursive neural networks (Socher et al., 2011) that are constructed by the parse tree of the first-order logic formulas.", "startOffset": 51, "endOffset": 72}, {"referenceID": 38, "context": "Each of these internal nodes are represented by either a fully connected layer with ReLU activation or tree LSTM network (Tai et al., 2015).", "startOffset": 121, "endOffset": 139}, {"referenceID": 6, "context": "Figures courtesy of Chris Olah (2015).", "startOffset": 26, "endOffset": 38}, {"referenceID": 38, "context": "Our tree LSTMs use separate forget gains per input and include off-diagonal forget gate terms (see Figure 3 and Tai et al. (2015) for details).", "startOffset": 112, "endOffset": 130}, {"referenceID": 39, "context": "2, we run E prover over the same holdout set of 9,159 FOL proof tasks from Kaliszyk & Urban (2015a) using the trained networks to help guide the clause selection.", "startOffset": 86, "endOffset": 100}, {"referenceID": 39, "context": "2, we run E prover over the same holdout set of 9,159 FOL proof tasks from Kaliszyk & Urban (2015a) using the trained networks to help guide the clause selection. These theorems all have at least one ATP proof using classical search heuristics, and 96.6% of these theorems can be proved using one of E prover\u2019s built-in automated heuristics, so there is little room for improvement on this dataset. However, it allows us to perform a sanity check that the neural nets aren\u2019t doing more harm than good when added to the auto heuristic. The final test is the most interesting but computationally expensive: do these deep network guided selection heuristics allow us to prove Mizar statements that do not yet have any ATP proof? For this, we use a corpus of 25,361 theorems from Mizar for which no proof was found in Kaliszyk & Urban (2015a). Because this is the most computationally expensive task, we only run it using the networks that performed best on the previous tasks.", "startOffset": 86, "endOffset": 839}, {"referenceID": 32, "context": "1pre014 (Schulz, 2013), with some modifications to enable integration with the trained deep neural networks as clause selection heuristics.", "startOffset": 8, "endOffset": 22}, {"referenceID": 32, "context": "Although all the hard statements have human proofs, this subset could neither be proved by theorem prover E (Schulz, 2013) nor by Vampire (Kov\u00e1cs & Voronkov, 2013) with a timeout limit of 15 minutes and default settings (auto and casc heuristics respectively) using the premises derived from the human given proofs.", "startOffset": 108, "endOffset": 122}, {"referenceID": 37, "context": "Here we describe our results on a corpus of 25,361 theorems from the Mizar corpus for which no proof was found by any of the provers, strategies, and premise selection methods considered by Kaliszyk & Urban (2015a). We will call these \u201chard statements\u201d in the rest of the paper.", "startOffset": 201, "endOffset": 215}, {"referenceID": 2, "context": "For premise selection, we use the character level model from the DeepMath paper (Alemi et al., 2016).", "startOffset": 80, "endOffset": 100}, {"referenceID": 2, "context": "Here we tried two different premise selection models \u201cDeepMath 1\u201d and \u201cDeepMath 2\u201d using the same character level convolutional architecture as the best character level model described in (Alemi et al., 2016).", "startOffset": 188, "endOffset": 208}], "year": 2017, "abstractText": "Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56% to 59%.", "creator": "easychair.cls-3.4"}}}