{"id": "1701.07398", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Learning an attention model in an artificial visual system", "abstract": "the human visual perception of the world is of a large fixed image that is highly detailed and sharp. however, receptor density in the retina is not uniform : a small central region called the fovea is very dense dense and exhibits high resolution, whereas a peripheral region around each it has much lower spatial resolution. thus, contrary to our perception, we are likewise only able to observe a very small region around the line of sight with high resolution. the perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene. the eyes move between these targets in quick, unconscious movements, processes known formally as \" saccades \". once a target is centered at the fovea, the eyes subsequently fixate for a fraction of a second while the visual system extracts the necessary functional information. an explicitly artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol, and learned to attend to regions closely of interest while then solving a classification task. the model is consistent with several experimentally observed phenomena, and suggests novel predictions.", "histories": [["v1", "Tue, 24 Jan 2017 09:07:59 GMT  (3394kb,D)", "http://arxiv.org/abs/1701.07398v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["alon hazan", "yuval harel", "ron meir"], "accepted": false, "id": "1701.07398"}, "pdf": {"name": "1701.07398.pdf", "metadata": {"source": "CRF", "title": "Learning an Attention Model in an Artificial Visual System", "authors": ["Alon Hazan", "Yuval Harel", "Ron Meir"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nNeuroscientists and cognitive scientists have many tools at their disposal to study the brain and neural networks in general, including Electroencephalography (EEG), Single-Photon Emission Computed Tomography (SPECT), functional Magnetic Resonance Imaging (fMRI) and Microelectrode Arrays (MEA), to name a few. However, the amount of information and level of control afforded by these tools do not remotely resemble what is available to an engineer working on an artificial neural network. The engineer can manipulate any neuron at any time, force certain excitations, intervene in ongoing processes, and collect as much data about the network as needed, at any level of detail. This wealth of information has enabled reverse engineering research on artificial neural networks, leading to insights into the inner workings of trained artificial neural networks. This suggests an indirect approach to studying the brain: training a biologically plausible neural network model to exhibit complex behavior observed in real brains, and reverse engineering the result. In line with this approach, we designed an artificial visual system based on a fully recurrent unlayered neural network that learns to perform saccadic eye movements. Saccadic eye movements are quick, unconscious, task-dependent [1] motions following the demand of attention [2], that direct the eye to new targets that require the high resolution of the fovea. These targets are usually detected within the peripheral visual system [3]. Once\na target is centered at the fovea, the eye fixates for a fraction of a second while the visual system extracts the necessary information. Most eye movements are proactive rather than reactive, predict actions in advance and do not merely respond to visual stimuli [4].\nThere is good evidence that much of the active vision in humans results from Reinforcement Learning (RL) [5], as part of and organism\u2019s attempt to maximize its performance while interacting with the environment [6]. Accordingly, we train the artificial visual system within the RL paradigm. The network was not explicitly engineered to perform a certain task, and does not contain an explicit memory component \u2014 rather it has memory only by virtue of its recurrent topology. Learning takes place in a model-free setting using policy gradient techniques.\nWe find that the network displays attributes of human learning such as: (a) decision making and gradual confidence increase along with accumulated evidence, (b) skill transfer, namely the ability to use a pre-learned skill in a certain task in order to improve learning on a related but more difficult task, (c) selectively attending information relevant for the task at hand, while ignoring irrelevant objects in the field of view."}, {"heading": "II. THE ARTIFICIAL VISUAL SYSTEM", "text": "We designed an Artificial Visual System (AVS) with the task of learning an attention model to control saccadic eye movements, and subsequent classification of digits. We refer to this task as the attention-classification task. The AVS is similar in many ways to that presented in [7]. It is a simplified model of the human visual system, consisting of a small region in the center with high resolution, analogous to the human fovea, and two larger concentric regions which are sub-sampled to lower resolution and are analogous to the peripheral visual system in humans. The AVS was trained and tested on the classification of handwritten digits from the MNIST data set [8] . Only a small part of the image is visible to the AVS at any one time. Specifically, full resolution is only available at the fovea, which is 9-by-9 pixels, as in [7], or 5-by-5 pixels (about 69% smaller). The first peripheral region is double the size of the fovea, but sub-sampled with period 2 to match the size of the fovea in pixels. Similarly, the second peripheral region is quadruple the size of the fovea but sub-sampled with period 4. For comparison, a typical digit in the MNIST database\n978-1-5090-2152-9/16/$31.00 c\u00a92016 IEEE\nar X\niv :1\n70 1.\n07 39\n8v 1\n[ cs\n.C V\n] 2\n4 Ja\nn 20\n17\noccupies about 20-by-20 pixels of the image. The location of the observation within the image is not available to the AVS (unlike [7]), and movements of the observation location are not constrained to image boundaries. Instead, locations outside the image boundaries are observed as black pixels.\nThe AVS consists of inputs (observations) projected upon the network via input weights Win, a neural network consisting of N neurons connected by the recurrent weights W , and two outputs: a classifier yID, responsible for identifying the digit after the AVS has explored the image, and the attention model output yAtt, responsible for directing the eye to new locations based on the information represented in the network state (see Figure 1). The output yID consists of one neuron for each possible digit. At the end of the trial, the identity of the highest valued neuron is interpreted as the network\u2019s classification. The progression of a single trial follows these principal stages:\n1) A random digit is selected from the MNIST training database. 2) A location across the image is randomly selected. 3) The observation (called \u2018glimpse\u2019 [7]) from the current\nlocation is projected upon the network through Win, along with any pre-existing information within the network state through the recurrent weights W . 4) The attention model output yAtt is fed back as a saccade of the eye, i.e. as the size of movement from the current location in the horizontal and vertical axes. 5) If a predefined number of glimpses has passed (or by network decision), compare the classifier output yID to the true label, otherwise return to stage 3. 6) Reward the AVS if the classification was correct, and continue to the next trial.\nThe AVS is implemented by a fully recurrent neural network. Its network topology is similar the Echo State Network (ESN) [10] in that the recurrent neural connections are drawn randomly and are not constrained to a particular topology such as in layered feedforward networks, or long short-term memory networks.\nThe network state evolves according to\nsn+1 = (1\u2212 \u03b1) sn + \u03b1 (W tanh (sn) +Winon+1 + \u03b6n) , yn =Wout tanh (sn) + \u03ben, (1)\nwhere \u2022 sn \u2208 RN is the state of network at time step n, each\nelement representing the state of a single neuron,\n\u2022 \u03b1 \u2208 (0, 1] is the leak rate, \u2022 W \u2208 RN\u00d7N is the internal connections weight matrix, \u2022 Win \u2208 RNin\u00d7N is the input weight matrix, \u2022 o \u2208 RNin is the observation (network input), \u2022 \u03b6n \u2208 RN and \u03ben = ( \u03beIDn ; \u03be Att n ) \u2208 RM are indepen-\ndent discrete-time Gaussian white noise processes with independent components, each having variance \u03c32\u03b6 , \u03c3 2 \u03be\nrespectively, \u2022 yn = ( yIDn ; y Att n ) \u2208 RM is the state of the M output\nneurons, \u2022 Wout \u2208 RM\u00d7N is the output weight matrix (consisting\nof blocks WID,WAtt for the corresponding output components).\nThe gradient of the expected reward J is estimated as in [11],\n\u2207\u0302J = \u3008\u2207 log p (\u03c4) (r (\u03c4)\u2212 b)\u3009 ,\nwhere \u03c4 = (s0, w1, (on, sn, yAttn , y ID n ) Ng n=1) is a random trajectory of Ng glimpses, p (\u03c4) is the probability of trajectory \u03c4 , r (\u03c4) the observed (usually binary) reward, b a fixed baseline computed as in [11], w1 is the random location of the first glimpse, and \u3008\u00b7\u3009 indicates averaging over trajectories. Viewed as a partially observable Markov decision process (POMDP), we can write the distributions describing the agent:\np (sn+1|sn, on+1) = \u03b1\u22121p\u03b6 ( \u03b1\u22121 (sn+1 \u2212 (1\u2212 \u03b1) sn)\n\u2212W tanh (sn)\u2212Winon+1 )\np (yn|sn) = p\u03be (yn \u2212Wout tanh (sn)) , (2)\nwhere p\u03b6 , p\u03be are the probability density functions of \u03b6t, \u03bet respectively. The POMDP dynamics are deterministic: the glimpse position wn evolves as wn+1 = wn + yAttn .\nFor the AVS (1) the probability density of a trajectory \u03c4 is\np (\u03c4) = p (s0) p (w1) Ng\u220f n=1 p (sn+1|sn, on+1) p (yn+1|sn+1) ,\nHere only the output probabilities p (yn+1|sn+1) depend on WOut, and, using (2) and the Gaussian distribution of the noise, we find that the log likelihood gradient with respect to WOut takes the form\n\u2207WOut log p (\u03c4) = Ng\u2211 n=1 \u03c3\u22121\u03be \u03ben tanh (sn) T ,\nwhere Ng is the number of glimpses. Stochastic gradient ascent is performed only for the output weights WOut. Recurrent weights are randomly selected, with spectral radius 1, and remain fixed throughout training. The log likelihood with respect to the internal weight matrix W takes a similar form. However, the recurrent connections were not learned in our simulations."}, {"heading": "III. RESULTS", "text": ""}, {"heading": "A. Use of memory", "text": "Since information accumulated by the neural network over time is mixed into the state of the network, it is not obvious that the potential to extract useful historic information can be exploited within the attention model solution. Training uses gradient ascent to the local maxima of the estimated expected reward and therefore may converge to sub-optimal maxima that do not make use of the full potential of the system. In order to test the use of memory by the trained network, two similar AVS were trained on the attention-classification task. In the first AVS, recurrent weights were random, whereas the second AVS was set to \u2018forget\u2019 historic information, by setting the recurrent weights matrix to zero.\nUse of memory was found to depend on the size of the fovea. Fig. 2 shows the performance of the system across training epochs, for the case of a large (9\u00d79 pixels) fovea. Initially, the AVS with memory has the advantage as the attention model is still poor at this stage, leading to relatively uninformative glimpses, so the use of information from several glimpses results in better classification. However, as the attention mechanism improves the last glimpse becomes highly informative, so the memoryless network, where information from the last glimpse is not corrupted by memory of previous glimpses, has the advantage. In fact, we found that information from a well-placed glimpse suffices to classify the digit with over 90% success rate in this case, driving the network to a solution of finding a single good glimpse location across the digit, and classification based on that glimpse, without regard to the rest of the trajectory.\nThe situation is different with a smaller fovea (5\u00d75 pixels), where classification from a single glimpse becomes harder. As seen in Figure 3, the AVS with memory outperforms the one without memory in the small fovea case."}, {"heading": "B. Gathering Information", "text": "The human visual system acts to maximize the information relevant to the task [12]. In order to assess whether our AVS behaves similarly, we have to characterize the relevant information in the context of our task. Since the network\nclassification yID depends linearly on the network state in the last time step, we quantify the task-relevant information as the best linear separation of the network state, between each class and the other classes. Accordingly, we use Linear Discriminant Analysis (LDA) [13], which acts to find the projection that minimizes the distance between samples of the same cluster Sw while at the same time maximizes the distance between clusters Sb. The distance within each class is measured by the variance of samples belonging to that class, and Sw is taken to be the mean of these distances across all classes. The Distance between classes Sb is defined as the variance of the set of class centers.\nWe trained an AVS on the attention-classification task with 5 glimpses per digit. After the AVS was trained, it was tested in two cases. In the first case, the system was run as usual and the network state vector was recorded after the last glimpse of each digit in the test set. In the second case, the location of the last glimpse was chosen randomly rather than following the learned attention model. The results are illustrated in Figure 4, where the state of the network is projected on the first two eigenvectors of S\u22121w Sb. Separation is significantly better with the full attention model compared to the one with the random last glimpse. We conclude that, at the very least, the attention model acts to maximize task-relevant information in the last glimpse better than a random walk."}, {"heading": "C. Transfer learning", "text": "Biological learning often displays the ability to use a skill learned on a simple task in order to improve learning of a harder yet related task, e.g., proficiency at tennis is beneficial when learning racquetball and even seemingly unrelated tasks such as skiing for example [14]. To test whether transfer learning is possible in the AVS, we trained it to learn the attention model and classification of 3 digits (out of 10 in the MNIST database). The resulting solution served as an initial condition for learning the full task of classifying all 10 digits. As seen in Fig. 5, not only did the AVS with pre-learned attention learn much faster, but it also achieved a better result at the end of training.\nD. Ignoring distractions\nThe eyes are not directed to the most visually salient points in the field of view, but rather to the ones that are most relevant for the task at hand [6]. Accordingly, we introduced an highly salient object into the training images. The object is a square, approximately the size of a digit but with maximum brightness, whereas the digits are handwritten and displayed in grayscale. The object is inserted at a fixed position relative to the digit, always on the right hand side of the digit. The trained network successfully avoids unnecessary fixations on the salient object. In cases where the first glimpse falls upon an area where both the digit and the object are within the peripheral visual region, the object seems to be completely ignored. Perhaps more interesting is the case where only the object is visible in the first glimpse, within the peripheral view. In such a case, the AVS learned to exploit the fact that the digit is always located on the left of the object and consistently performs saccades to the left. Thus, not only was the presence of a distracting object not harmful to performance, but it was actually beneficial.\nIn Fig. 6, the colored squares represent the foveal view of 5-by-5 pixels at each time step going from blue to red. The left and middle images show cases where the first glimpse only observes the distracting object within the peripheral view. The left image shows a case where the first glimpse observes both the distracting object and the digit within the peripheral view.\nNext, we test the network with a distracting object in a random position around the digit. The observed behavior was similar when the first glimpse happened to fall on a location where both the object and digit are within the peripheral view: the AVS ignored the distraction and directed itself towards the digit. However, in the case where the first glimpse falls on a location where only the distracting object is visible in the peripheral view, the AVS failed to locate the digit.\nAn example is seen in Fig. 7. The lines are trajectories of the AVS each starting from a different point on a test grid and followed until the last glimpse (blue/magenta dot). Green lines are trajectories that led to a correct classification while red lines are trajectories that led to a false classification. When the AVS happen to fall at a location where the digit is not seen, it directs its gaze towards the square, which it then chooses to classify as \u201c1\u201d thus earning 10% expected reward which is better than nothing."}, {"heading": "E. Learning aided by demonstration (guidance)", "text": "Learning by demonstration (or learning with guidance) was implemented in the AVS. Demonstration differs from supervision in two key ways. First, demonstration is not continuous, and is applied sparsely in time in order to suggest new trajectories to the system. Second, demonstration is not required to provide the best solution to the system, because the system maintains its freedom to explore and even improve upon it. Demonstration was achieved by providing the network with a sparse and naive suggestion for the attention model. For example, on 10% of trajectories, the system was directed to the center of the digit on the last glimpse. Such partial direction resulted in a significant improvement of both speed of learning and the final success rate, as can be observed in figure 8.\nDemonstration in the AVS system was made possible by manipulating the exploration noise. The exploration noise is a Gaussian white noise and as such has probability greater than zero to accept any value. Since the output of the system at any given time is a function of that noise, we can force the output to a specific value by setting the exploration noise in that particular time step to be \u03b6\u0303n = yattn \u2212WAtt tanh (sn) where yattn is now the demonstrated output of the attention model and \u03b6\u0303n is the determined exploration noise that will bring the system to that desired output. As long as the demonstration is kept sparse enough, it would in practice not break the assumption that the noise is a Gaussian white noise. The noise in the system is an essential part of the log likelihood gradient and therefore the system would not only arrive to the desired output at that particular time step, but also learn from that experience."}, {"heading": "IV. CONCLUSION", "text": "We have shown that a simple artificial visual system, implemented through a recurrent neural network using policy gradient reinforcement learning, can be trained to perform classification of objects that are much larger than its central region of high visual acuity. While receiving only classification based reward, the system develops an active vision solution, which directs attention towards relevant parts of the image in a task-dependent way. Importantly, the internal network memory plays an essential role in maintaining information across saccades, so that the final classification is achieved by combing information from the current visual input and from previous inputs represented in the network state. Within a generic active vision system, without any specifically crafted features, we have been able to explain several features characteristic of biological vision: (i) Good classification performance using reinforcement learning based on highly limited central vision and low resolution peripheral vision, (ii) Gathering task-relevant information through active search, (iii) Transfer learning, (iv) Ignoring task-irrelevant distractors, (v) Learning through guidance. Beyond providing a model for biological vision, our results suggest possible avenues for cost-effective image recognition in artificial vision systems.\nThe Matlab code will be made available at: https://alonhazan.wordpress.com/"}], "references": [{"title": "How people look at pictures: a study of the psychology and perception in art", "author": ["G T Buswell"], "venue": "Chicago University of Chicago Press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1935}, {"title": "Portable eyetracking: a study of natural eye movements", "author": ["Jeff B. Pelz", "Roxanne L. Canosa", "Diane Kucharczyk", "Jason Babcock", "Amy Silver", "Daisei Konno"], "venue": "Proceedings of SPIE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "The knowledge base of the oculomotor system", "author": ["M F Land", "S Furneaux"], "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Multiple reward signals in the brain", "author": ["W Schultz"], "venue": "Nature reviews. Neuroscience,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Eye movements in natural behavior", "author": ["Mary Hayhoe", "Dana Ballard"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Recurrent Models of Visual Attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "What\u2019 and \u2019where\u2019 in the human brain", "author": ["L Ungerleider"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "The \u201cecho state\u201d approach to analysing and training recur- rent neural networks. GMD-Forschungszentrum Informationstechnik", "author": ["H Jaeger"], "venue": "Report 148,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Human gaze control during real-world scene perception", "author": ["John M. Henderson"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Introduction to Statistical Pattern Recognition", "author": ["Keinosuke Fukunaga"], "venue": "Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Neural correlates of motor learning, transfer of learning, and learning to learn", "author": ["Rachael D Seidler"], "venue": "Exercise and sport sciences reviews,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Saccadic eye movements are quick, unconscious, task-dependent [1] motions following the demand of attention [2], that direct the eye to new targets that require the high resolution of the fovea.", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "These targets are usually detected within the peripheral visual system [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Most eye movements are proactive rather than reactive, predict actions in advance and do not merely respond to visual stimuli [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "There is good evidence that much of the active vision in humans results from Reinforcement Learning (RL) [5], as part of and organism\u2019s attempt to maximize its performance while interacting with the environment [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "There is good evidence that much of the active vision in humans results from Reinforcement Learning (RL) [5], as part of and organism\u2019s attempt to maximize its performance while interacting with the environment [6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "The AVS is similar in many ways to that presented in [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "The AVS was trained and tested on the classification of handwritten digits from the MNIST data set [8] .", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "Specifically, full resolution is only available at the fovea, which is 9-by-9 pixels, as in [7], or 5-by-5 pixels (about 69% smaller).", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "(unlike [7]), and movements of the observation location are not constrained to image boundaries.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "3) The observation (called \u2018glimpse\u2019 [7]) from the current location is projected upon the network through Win, along with any pre-existing information within the network state through the recurrent weights W .", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "Its network topology is similar the Echo State Network (ESN) [10] in that the recurrent neural connections are drawn randomly and are not constrained to a particular topology such as in layered feedforward networks, or long short-term memory networks.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "The gradient of the expected reward J is estimated as in [11],", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "where \u03c4 = (s0, w1, (on, sn, y n , y ID n ) Ng n=1) is a random trajectory of Ng glimpses, p (\u03c4) is the probability of trajectory \u03c4 , r (\u03c4) the observed (usually binary) reward, b a fixed baseline computed as in [11], w1 is the random location of the first glimpse, and \u3008\u00b7\u3009 indicates averaging over trajectories.", "startOffset": 211, "endOffset": 215}, {"referenceID": 10, "context": "The human visual system acts to maximize the information relevant to the task [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Accordingly, we use Linear Discriminant Analysis (LDA) [13], which acts to find the projection that minimizes the distance between samples of the same cluster Sw while at the same time maximizes the distance between clusters Sb.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": ", proficiency at tennis is beneficial when learning racquetball and even seemingly unrelated tasks such as skiing for example [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "The eyes are not directed to the most visually salient points in the field of view, but rather to the ones that are most relevant for the task at hand [6].", "startOffset": 151, "endOffset": 154}], "year": 2017, "abstractText": "The Human visual perception of the world is of a large fixed image that is highly detailed and sharp. However, receptor density in the retina is not uniform: a small central region called the fovea is very dense and exhibits high resolution, whereas a peripheral region around it has much lower spatial resolution. Thus, contrary to our perception, we are only able to observe a very small region around the line of sight with high resolution. The perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene. The eyes move between these targets in quick, unconscious movements, known as \u201csaccades\u201d. Once a target is centered at the fovea, the eyes fixate for a fraction of a second while the visual system extracts the necessary information. An artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol, and learned to attend to regions of interest while solving a classification task. The model is consistent with several experimentally observed phenomena, and suggests novel predictions.", "creator": "LaTeX with hyperref package"}}}