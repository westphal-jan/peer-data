{"id": "1701.08191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Incremental Maintenance Of Association Rules Under Support Threshold Change", "abstract": "maintenance of association rules is an interesting problem. several incremental maintenance parameter algorithms were proposed since the work of ( cheung et. al, 1996 ). the precious majority of successfully these algorithms maintain rule bases assuming rules that support threshold doesn't change. in this paper, soon we present incremental maintenance algorithm under support threshold change. this solution basically allows user to maintain its rule base under any support threshold.", "histories": [["v1", "Fri, 27 Jan 2017 21:02:52 GMT  (569kb)", "http://arxiv.org/abs/1701.08191v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["mohamed anis bach tobji", "mohamed salah gouider"], "accepted": false, "id": "1701.08191"}, "pdf": {"name": "1701.08191.pdf", "metadata": {"source": "CRF", "title": "INCREMENTAL MAINTENANCE OF ASSOCIATION RULES UNDER SUPPORT THRESHOLD CHANGE", "authors": ["Mohamed Anis BACH TOBJI", "Mohamed Salah GOUIDER"], "emails": ["anis.bach@isg.rnu.tn", "ms.gouider@isg.rnu.tn"], "sections": [{"heading": null, "text": "Maintenance of association rules is an interesting problem. Several incremental maintenance algorithms were proposed since the work of (Cheung et al, 1996). The majority of these algorithms maintain rule bases assuming that support threshold doesn't change. In this paper, we present incremental maintenance algorithm under support threshold change. This solution allows user to maintain its rule base under any support threshold."}, {"heading": "KEYWORDS", "text": "Association rules, Incremental maintenance."}, {"heading": "1. INTRODUCTION", "text": "The problem of association rules mining (ARM) (Agrawal et al, 1993) receives a lot of attention. This attention is motivated by several application domains such as market basket analysis, telecommunications analysis, Web logs analysis etc. Several ARM algorithms have been constructed to solve this problem (Agrawal and Srikant, 1994), (Savasere et al, 1995), (Zaki et al, 1997), (Pasquier et al, 1999), (Han et al, 2000). They compute association rules that describe a data set. Nevertheless, data sources are frequently updated. It follows that extracted rules describe a data set at a moment t, the mining moment. This change of data implies invalidation of rules and necessity to maintain them.\nSeveral algorithms of incremental maintenance have been proposed (Cheung et al, 1996), (Tsai et al, 1999), (Zhang et al, 1997); the first algorithm called FUP (Cheung et al, 1996) was achieved in 1996. These algorithms target maintenance of rule bases under the same support threshold. In this paper, we present IMSC, an algorithm of incremental maintenance after several updates (insertion of transactions). IMSC allows the user to change support threshold. This algorithm is based on Apriori (Agrawal and Srikant, 1994) that is oriented sparse databases such as transactional databases.\nGenerally, maintenance of association rules is achieved according to one of the two following approaches; incremental approach and non-incremental approach. Non-incremental maintenance consists in executing again an ARM algorithm on the whole updated data. The advantage of this method is the free choice of a new support threshold. Its drawback is ignoring old association rules making the operation more expensive. Incremental maintenance is the second approach. This method uses old association rules to compute new association rules. It\u2019s faster in comparison with non-incremental maintenance. Nevertheless, its drawback is that maintenance is realized on the same initial extraction support threshold. The purpose of this\nwork is to develop an incremental maintenance algorithm under support threshold change. Our algorithm combines the advantages of the two maintenance approaches.\nThe remaining of the paper is organized as follows: in section 2, we define the problem of association rule mining and maintenance, and in section 3 we present our algorithm. The performance of the algorithm is studied in section 4, and finally, conclusion and perspectives are given in section 5."}, {"heading": "2. PROBLEM DESCRIPTION", "text": "Let I be a set of n items, I= {i1, i2, i3,\u2026., in}. Let BD be a database of D transactions with schema <tid,items>. Each transaction is included in I. An association rule is XY with X,Y I, X and X\u2229Y=\u00f8. Rule support is the occurrence number of XY in BD, and its confidence is support(XY)/support(X). Given a minimum support threshold s% and a minimum confidence threshold c%, ARM problem (Agrawal et al, 1993) is to find out all association rules which support exceeds s% and confidence exceeds c%. An itemset is a set of items. It said to be frequent in BD if its support in BD (X.supportBD) exceeds s% (X.supportBD\u2265s\u00d7D), we say that is s-frequent in BD. The ARM problem may be reduced to the computation of frequent itemsets, because once we have frequent itemsets set, association rules generation will be straightforward.\nLet be F the set of the frequent itemsets in BD and s the support threshold under which F is extracted. After several updates of BD, an increment bd of d transactions is added to BD. The problem of incremental maintenance under support threshold change is to compute F\u2019, the set of the frequent itemsets in BD\u2019=BDbd according to a support threshold s\u2019."}, {"heading": "3. THE IMSC APPROACH", "text": ""}, {"heading": "3.1 Winner, loser and persistent itemsets", "text": "To solve this problem, we propose a new approach we call IMSC (Incremental Maintenance of association rules under Support threshold Change) that is based on FUP (Fast UPdate) algorithm (Cheung et al, 1996). FUP maintains a rule base incrementally under the same support threshold. When s=s\u2019, IMSC and FUP are practically identical.\nLet be BD'=BDbd. BD\u2019 contains four types of itemsets:\n Winner itemsets that are not s-frequent in BD (F) and that are s\u2019-frequent in BD\u2019 (F\u2019).\n Persistent itemsets that are s-frequent in BD (F) and that are s\u2019-frequent in BD\u2019 (F\u2019).\n Loser itemsets that are s-frequent in BD (F) and that are not s\u2019-frequent in BD\u2019 (F\u2019).\n The itemsets that are not s-frequent in BD (F) and that are not s\u2019-frequent in BD\u2019 (F\u2019).\nWe are interested in winner, persistent and loser itemsets.\nDefinition 1 (persistent itemset)\nAn itemset X is persistent in BD\u2019 if XF and XF\u2019  X.supportBD\u2265s\u00d7D AND X.supportBD\u2019\u2265s\u2019\u00d7(D+d).\nDefinition 2 (winner itemset)\nAn itemset X is winner in BD\u2019 if XF and XF\u2019  X.supportBD<s\u00d7D AND X.supportBD\u2019\u2265s\u2019\u00d7(D+d).\nDefinition 3 (loser itemset)\nAn itemset X is loser in BD\u2019 if XF and XF\u2019  X.supportBD\u2265s\u00d7D AND X.supportBD\u2019 <s\u2019\u00d7(D+d)."}, {"heading": "3.2 Description of IMSC", "text": "The IMSC algorithm browses search space (itemsets lattice) in breadth-first way; it computes frequent itemsets level by level. In other words, it computes frequent itemsets of size i before computing itemsets of size i+1.\nFor each lattice level, frequent itemsets computation is done in two steps. Let\u2019s suppose we want to\ngenerate the set F\u2019i (set of s\u2019-frequent itemsets of size i), IMSC proceeds as follows:"}, {"heading": "I \u2013 IMSC scans bd :", "text": "1- Generation of Ci, set of candidate itemsets of size i. Ci=Apriori_gen(Fi-1)/Fi. Apriori_gen being\nApriori candidate generation function defined in (Agrawal and Srikant, 1994). We eliminate the set Fi because it will be treated in next phase (II).\n2- While scanning bd, we update itemsets supports of Fi and Ci.\n3- We insert in F\u2019i persistent itemsets which are generated from Fi (X.supportBD\u2019\u2265s\u2019\u00d7(D+d)).\n4- The set Ci contains possible winners. A candidate itemset is a possible winner if its support in bd exceeds the Candidate Pruning Threshold noted CPT (see lemma 1).\nLemma 1 (Candidate Pruning Threshold)\nA candidate itemset X is a possible winner in BD\u2019 if and only if its support in bd exceeds the threshold CPT, CPT = s\u2019\u00d7d + (s\u2019-s)\u00d7D +1.\nProof"}, {"heading": "X is a possible winner if :", "text": ""}, {"heading": "X.supportBD\u2019 \u2265 s\u2019\u00d7(d+D)  X.supportbd+X.supportBD \u2265 s\u2019\u00d7(d+D)  X.supportbd \u2265 s\u2019\u00d7(d+D) - X.supportBD", "text": "We know XF, so X.supportBD < s\u00d7D, the maximum support value of X in BD is (s\u00d7D -1).\n X.supportbd \u2265 s\u2019\u00d7(d+D) \u2013 s\u00d7D+1  X.supportbd \u2265 s\u2019\u00d7d + D\u00d7(s\u2019-s)+1.\nII\u2013 IMSC scans BD:\n1- Updating candidate itemsets supports. At the end we have their supports in BD\u2019.\n2- Insertion of winner itemsets in F\u2019i.\nIMSC iterates n+1 times to compute F\u2019, where n is the size of the longest s\u2019-frequent itemset. Such as FUP, the return to BD is achieved to generate the winner itemsets from candidate itemsets set. The pruning of this set is made in step I.4 according to the threshold CPT. But CPT can be negative (in this case, no candidate itemset is discarded), or superior to d (in this case, all itemsets candidates are discarded). We distinguish three possible scenarios: CPT\u22640, CPT>d, 0 <CPT\u2264d.\n CPT \u2264 0\nIn this case, each candidate itemset X satisfies the X.supportbd\u2265CPT condition, we do not eliminate any candidate. The return to BD will be heavy, because it\u2019s about computing all candidate itemset supports. We also know that the s-frequent itemsets in BD (the set F) are s\u2019-frequent in BD\u2019 (belong to F') thanks to the following lemma.\nLemma 2\nIn the case CPT\u22640, there is no possible losers, all s-frequent itemsets in BD are s\u2019-frequent in BD\u2019.\nProof\nLet X be an itemset that is s-frequent in BD and CPT \u2264 0.\nCPT \u2264 0  s\u2019\u00d7 d + (s\u2019-s)\u00d7D+1 \u2264 0  s\u2019\u00d7(d+D) \u2013 s\u00d7D+1\u2264 0\n s\u00d7D-1 \u2265 s\u2019\u00d7(d+D) ; we know that X.supportBD \u2265 s\u00d7D\n X.supportBD \u2265 s\u2019\u00d7(d+D) ; we know that X.supportBD\u2019 \u2265 X.supportBD  X.supportBD\u2019 \u2265 s\u2019\u00d7(d+D)\n X is s\u2019-frequent in BD\u2019.\n CPT > d\nIn this case, it is useless to generate candidate itemsets set since no itemset can win (it needs more than d occurrences in bd ; it\u2019s impossible). There is no return to BD (step II) and the scan of bd is sufficient to calculate F\u2019, this case is the best one.\n 0 < CPT \u2264 d In this case, candidate itemsets can win, the pruning is made according to CPT that is a moderate value."}, {"heading": "3.3 IMSC algorithm", "text": "In this section, we present the IMSC algorithm which is composed of three procedures; each procedure\ncorresponds to one scenario. Algorithm description is given in pseudo code commentaries:"}, {"heading": "IMSC Algorithm :", "text": "Input :\nBD : Initial database.\nD : Cardinal of BD.\nbd : data increment.\nd : Cardinal of bd.\nF=  Fk (Union of sets Fk).\ns : support under which F is computed.\ns\u2019 : support under which we want maintain F.\nOutput:"}, {"heading": "F\u2019= F\u2019k (Union of sets F\u2019k).", "text": "Algorithm:\nBegin\n01 cpt=s\u2019\u00d7d/100+D\u00d7(s\u2019-s)/100+1 /*CPT\ncomputation from algorithm parameters and execution\nof the convenient procedure*/\n02 If cpt \u2264 d and cpt > 0 then\n03 IMSC1(BD, D, bd, d, F)\n04 ElseIf cpt < 1 Then\n05 IMSC2(BD, D, bd, d, F)\n06 Else\n07 IMSC3(BD, D, bd, d, F)\n08 EndIf\nEnd"}, {"heading": "IMSC1 Procedure", "text": "Same input and output of IMSC."}, {"heading": "Begin", "text": "01 F[1].CopyIn(Fk) /*Fk contains size one s-\nfrequent itemsets in BD*/\n02 Scanbd (Fk, Ck) /*While scanning bd, supports\nof F1 are updated and C1 is created*/ 03 min_supp=s\u2019\u00d7(d+D)/100\n04 Fk.Prune (min_supp) /*Items of F1 that are not s\u2019-frequents in BD\u2019 are discarded*/\n05 cpt=s\u2019\u00d7d/100 + D\u00d7(s\u2019-s)/100\n06 Ck.Prune(cpt) /*Items of C1 which supports doesn\u2019t exceed cpt are discarded*/\n07 ScanBD(Ck) /* While scaning BD, supports of C1 are updated*/\n08 Ck.Prune (min_supp) /*Only items that are s\u2019-\nfrequent in BD\u2019 are kept in C1*/ 09 F\u2019k=Union(Fk,Ck) /*F\u20191 is the union of F1 and C1*/"}, {"heading": "10 While (F\u2019k.Empty = false) Do /*F\u2019", "text": "Computation is done level by level*/\n11 F\u2019.Insert(F\u2019k) /*F\u2019k is inserted in F\u2019*/\n12 k=k+1 /*Going next level*/\n13 F[k].CopyIn(Fk)\n14 Apriori_gen(Ck,F\u2019k) /*Making Ck that is\nAprioi_gen of F\u2019k*/\n15 Fk=Fk.Intersect(Ck) /*Only Itemsets\nbelonging to FkCk can be persistent, they need only\nto a bd scan because we know their support in BD*/\n16 Ck=Ck.Minus(Fk) /*Other itemsets of Ck need a\nBD\u2019 scan because we don\u2019t know their support in BD*/\n17 Scanbd(Fk, Ck) /*Computing itemsets support\nin bd for Fk and Ck*/\n18 Fk.Prune(min_supp) /*We discard infrequent\nitemsets in Fk, we obtain persistent ones*/\n19 Ck.Prune(cpt) /*We discard itemsets in Ck\nthat cannot win*/\n20 ScanBD(Ck) /*Computation of the remaining\nitemsets support of Ck in BD*/\n21 Ck.Prune(min_supp) /*Computation of\nwinner itemsets*/\n22 F\u2019k=Union(Fk,Ck) /*F\u2019k contains persistent\nand winner itemsets*/\n23 EndWhile\nEnd"}, {"heading": "IMSC2 Procedure", "text": "Same input and output of IMSC.\nBegin\n01 F[1].CopyIn(Fk)\n02 Scanbd(Fk, Ck)\n03 min_supp=s\u2019\u00d7(d+D) / 100\n04 Fk.Prune(min_supp)\n05 ScanBD(Ck,F[1]) /* Items not belonging to C1\nand F[1] are inserted in C1 ; they can be s\u2019-frequent in\nBD\u2019 */\n06 Ck.Prune(min_supp)\n07 F\u2019k=Union(Fk,Ck)\n08 While (F\u2019k.Empty = false) Do\n09 F\u2019.Insert(F\u2019k)\n10 k=k+1\n11 F[k].CopyIn(Fk)\n12 Apriori_gen(Ck,F\u2019k)\n13 Ck=Ck.Minus(Fk)\n14 Scanbd(Fk, Ck)\n15 ScanBD(Ck)\n16 Ck.Prune(min_supp)\n17 F\u2019k=Union(Fk,Ck)\n18 EndWhile\nEnd"}, {"heading": "IMSC3 Procedure", "text": "Same input and output of IMSC.\nBegin\n01 F[1].CopyIn(Fk)\n02 Scanbd(Fk)\n03 min_supp=s\u2019\u00d7(d+D) / 100\n04 Fk.Prune(min_supp)\n05 F\u2019k=Union(Fk,Ck)\n06 While (F\u2019k.Empty = false) Do\n07 F\u2019.Insert(F\u2019k)\n08 k=k+1\n09 F[k].CopyIn(Fk)\n10 Scanbd(Fk)\n11 Fk.Prune(min_supp)\n12 F\u2019k=Union(Fk,Ck)\n13 EndWhile\nEnd"}, {"heading": "3.4 IMSC example", "text": "The IMSC algorithm process three possible scenarios to maintain the set of frequent patterns under support threshold change. Let be an initial database BD (see figure 1), an increment bd and different values of s and s\u2019. In the following, we execute IMSC to maintain F, i.e., to compute F\u2019:\n First scenario : 0 < CPT \u2264 d\nLet s=30%, then F is {A7, B5, C6, D4, AB4, AC4, BC4, CD3, ABC3}.\nLet bd be an increment of data containing the following transactions (each transaction is a doublet (Tid,\nItems)): {(11,ABD),(12,BD),(13,BCD)}, we have d=3.\nLet s\u2019 =35%, candidate pruning threshold (cpt) is 2,55, it is the first scenario. We compute C1 and F1. F1={A8, B8, C7, D7} and C1=\u00d8. All items of F1 are inserted in F\u20191 because their supports in BD\u2019 exceeds 0,35\u00d713=4,55. C2 is calculated from F\u20191 via Apriori_gen function, we remove elements of F2 from C2: C2={AD,BD}. After a scan on bd, we get the supports in bd of candidate itemsets of size 2 ; C2={AD1,BD3} and the supports of frequent itemsets in BD\u2019 ; F2={AB5, AC4, BC5, CD4}. We prune C2 and F2 according respectively to the thresholds 2,55 (cpt) and 4,55 (min_supp). C2={BD3}, F2={AB5, BC5}. Elements of F2 are added to F\u20192. Scan on BD is made to update the support of the itemset \u201cBD\u201d. Its support is 5, the itemset \u201cBD\u201d is inserted in F\u20192. F\u20192 = {AB5, BC5,BD5}. We go to level 3, C3=\u00d8, F3={ABC3}. While scanning bd, support of itemset \u201cABC\u201d doesn't change, it\u2019s rejected, F\u20193 will be empty and the algorithm stops.\n Second scenario : CPT \u2264 0\nLet s=50%, then F is {A7, B5, C6}.\nLet bd be a data increment of size d=3 containing the following transactions {(11,AB),(12,BC),(13,C)}.\nLet be s\u2019 =25%, CPT is equal to -0,75. In this case, we are certain there is no loser itemsets; F is included in F\u2019 (see lemma 2). After a scan of bd, we get C1=\u00d8, F1={A8, B7, C8}. Elements of F1 are inserted in F\u20191.Then a scan is achieved on BD. During this scan the items that are not in F1 and not in C1 are inserted in C1 because a non frequent item in BD according to s could be frequent in BD\u2019 according to s\u2019. C1={D4}. After pruning C1 according to support threshold 0,25\u00d713=3,25, the only item of C1 is added to F\u20191, F\u20191={A8, B7, C8, D4}. C2 is generated from F\u20191; C2=Apriori_gen(F\u20191)-F2={AB, AC, AD, BC, BD, CD}. F2 being empty, the scan on bd is made to compute the supports of the elements of C2, then a scan on BD is achieved to update the supports of all itemsets of C2. C2={AB5, AC4, AD1, BC5, BD2, CD3}. After pruning C2 according to s\u2019\u00d7(d+D)=3,25, we get C2={AB5, AC4, BC5}, the elements of C2 are inserted in F\u20192. Then, C3 is generated from F\u20192, we obtain C3={ABC}. After scanning bd and BD, the support of \u201cABC\u201d in BD\u2019 is 3, F\u20193=\u00d8 and the algorithm stops.\n Third scenario : CPT > d\nLet s=30%, F is then {A7, B5, C6, D4, AB4, AC4, BC4, CD3, ABC3}.\nLet bd be a data increment containing the following transactions {(11,AB),(12,BC),(13,C)}, we have d=3.\nLet s\u2019=40%, CPT is equal to 3,2 which is superior to d=3. In this case, there are no candidate itemsets. We must update the supports of elements of F, and to filter them. F\u20191={A8, B7, C8}. We go to second level since F\u20191 is not empty. F2={AB4, AC4, BC4, CD3} is updated while scanning bd, we have F2={AB5, AC4, BC5, CD3}. The pruning of F2 according to support threshold (=5,2) eliminates all its itemsets. F\u20192 being empty, the algorithm stops."}, {"heading": "4. PERFORMANCE STUDY", "text": "In this section, we present experimentations achieved to test IMSC efficiency. We implemented IMSC and Apriori and we tested them on synthetic dataset T5.I2.D100k. Generation of synthetic dataset follows the same technique as (Agrawal et al, 1993).\nIn the first experimentation, we divided database in two partitions: BD that is constituted of 70K transactions and bd that is constituted of 30K transactions. The initial extraction is done on BD according to a support threshold of 30%. The set of the resulting frequent itemsets of this extraction is stored in F. Having BD, bd, s% and F, we executed IMSC to compute F\u2019 under several maintenance support threshold going from 1% to 60%. In the same way, we executed Apriori on BD\u2019 (= BDbd) under the same support\nthreshold interval and we placed the two algorithms curves on the same graphic. The result is shown in the following figure.\nIt is obvious that IMSC is faster than Apriori. Two reasons explain this fact. First, IMSC has in memory the initially frequent itemsets in BD and their supports. It needs to scan only bd to compute their supports in BD\u2019. In contrast, Apriori scans all BD\u2019 to compute their supports. Secondly, Apriori generates the candidates set Ck from F\u2019k (Apriori_gen), whereas IMSC calculates Ck in the same way except that it eliminates the former frequent itemsets of it (Ck=Ck-Fk) and that it prunes it (Ck) according to CPT (Candidate Pruning threshold). This double pruning of Ck is done just after scaning the increment bd that is generally smaller than initial database BD.\nWe also tested IMSC performance for different increment sizes. We executed IMSC for two increments: D=70k, d=30k and s=30%, and D=70k, d=10k and s=30%. The following figure shows experimentation result. It is obvious that smaller increment is, faster IMSC is.\nThe two vertical lines of every curve delimit the three possible scenarios of our algorithm. For each curve, supports between the two lines make that CPT is between 0 and d. The supports on the left make CPT superior to d, and on the right those that make CPT lower to 0. We notice that when CPT is superior to d, the execution time is nearly null. When CPT is between 0 and d, more support increases, more the curve accelerates (the execution time accelerates), it is due to the return to the initial database. But this acceleration accentuates more in the last case, because of the heavier return to initial database.\nWe implemented our IMSC algorithm and we tested it against an ARM algorithm (Apriori). The results we obtained were expected; the incremental maintenance achieved by IMSC is faster than a new frequent itemsets extraction by an ARM algorithm, and IMSC curves have the same shape in which we distinguish the three maintenance scenarios."}, {"heading": "5. CONCLUSION AND PERSPECTIVES", "text": "In this paper, we propose an incremental maintenance approach for the maintenance of association rule base after data source updating under support threshold change. This approach (IMSC) is Apriori like. Apriori is efficient for mining sparse databases (such as transactional databases). Many other ARM algorithms are more efficient when it is about correlated database (Pasquier et al, 1999), (Bastide et al, 2002). It\u2019s interesting to develop methods for incremental maintenance under support threshold change based on these algorithms for correlated databases.\nWe also propose to integrate the incremental maintenance idea in inductive databases (Imielinski and Mannila, 1995), i.e., incremental pattern maintenance under several types of constraints and not only the antimonotone constraints such as for IMSC.\nMany algorithms use database vertical representation (Eclat (Zaki et al, 1997), Partition (Savasere et al, 1995) etc.); Support computation of candidate itemsets is done via tidlists intersection. These algorithms are deficient when databases are very large because of their memory need. However, in the maintenance problem, increment size is small. We think that using this strategy for itemset support computation should improve IMSC."}], "references": [{"title": "Mining Association Rules Between Sets of Items in Large Databases", "author": ["R. Agrawal", "T. Imielinski", "A Swami"], "venue": "In Proceedings ACM SIGMOD International Conference on Management of Data, Washington DC,", "citeRegEx": "Agrawal et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1993}, {"title": "Fast Algorithms for Mining Association Rules", "author": ["R. Agrawal", "R Srikant"], "venue": "In Proceedings of the 20th International Conference on Very Large Data Bases (VLDB\u201994),", "citeRegEx": "Agrawal and Srikant,? \\Q1994\\E", "shortCiteRegEx": "Agrawal and Srikant", "year": 1994}, {"title": "Pascal : un algorithme d'extraction des motifs fr\u00e9quents", "author": ["Y. Bastide", "R. Taouil", "N. Pasquier", "G. Stumme", "L Lakhal"], "venue": "Techniques et Science Informatiques, Herme\u0300s Science,", "citeRegEx": "Bastide et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bastide et al\\.", "year": 2002}, {"title": "Maintenance of Discovered Association Rules in Large Databases : An Incremental Update technique", "author": ["Cheung", "D-W", "J. Han", "Ng", "V-T", "Wang", "C-Y"], "venue": "In Proceedings of The Twelf International Conference on Data Engineering,", "citeRegEx": "Cheung et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 1996}, {"title": "Mining Frequent Patterns without Candidate Generation", "author": ["J. Han", "J. Pei", "Y Yin"], "venue": "In Proceedings of the 2000 ACMSIGMOD Int\u2019l Conf. On Management of Data. Dallas, Texas,", "citeRegEx": "Han et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Han et al\\.", "year": 2000}, {"title": "A database perspective on knowledge discovery", "author": ["T. Imielinski", "H Mannila"], "venue": "Communications of the ACM,", "citeRegEx": "Imielinski and Mannila,? \\Q1995\\E", "shortCiteRegEx": "Imielinski and Mannila", "year": 1995}, {"title": "Efficient Mining of Association Rules Using Closed Itemset Lattices", "author": ["N. Pasquier", "Y. Bastide", "T. Taouil", "L Lakhal"], "venue": "Information Systems,", "citeRegEx": "Pasquier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Pasquier et al\\.", "year": 1999}, {"title": "An Efficient Algorithm for Mining Association Rules in Large Databases", "author": ["A. Savasere", "E. Omiecinski", "S Navathe"], "venue": "In Proceedings of the 21th conference on VLDB (VLDB\u201995),", "citeRegEx": "Savasere et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Savasere et al\\.", "year": 1995}, {"title": "An Efficient Approach for Incremental Association Rule Mining", "author": ["P Tsai", "S.M", "Lee", "C-C", "A Chen", "L.P"], "venue": "In Proceedings of the Third Pacific-Asia KDD\u201999,", "citeRegEx": "Tsai et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Tsai et al\\.", "year": 1999}, {"title": "New Algorithms for fast discovery of Association Rules", "author": ["M.J. Zaki", "S. Parthasarathy", "M. Ogihara", "W Li"], "venue": "In Proceedings of the 3rd Int\u2019l Conference on KDD and data mining (KDD\u201997),", "citeRegEx": "Zaki et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zaki et al\\.", "year": 1997}, {"title": "Post-Mining : Maintenance of Association Rules by Weighting", "author": ["S. Zhang", "C. Zhang", "X Yan"], "venue": "Information Systems", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "Several ARM algorithms have been constructed to solve this problem (Agrawal and Srikant, 1994), (Savasere et al, 1995), (Zaki et al, 1997), (Pasquier et al, 1999), (Han et al, 2000).", "startOffset": 67, "endOffset": 94}, {"referenceID": 1, "context": "This algorithm is based on Apriori (Agrawal and Srikant, 1994) that is oriented sparse databases such as transactional databases.", "startOffset": 35, "endOffset": 62}, {"referenceID": 1, "context": "Apriori_gen being Apriori candidate generation function defined in (Agrawal and Srikant, 1994).", "startOffset": 67, "endOffset": 94}, {"referenceID": 5, "context": "We also propose to integrate the incremental maintenance idea in inductive databases (Imielinski and Mannila, 1995), i.", "startOffset": 85, "endOffset": 115}], "year": 2017, "abstractText": "Maintenance of association rules is an interesting problem. Several incremental maintenance algorithms were proposed since the work of (Cheung et al, 1996). The majority of these algorithms maintain rule bases assuming that support threshold doesn't change. In this paper, we present incremental maintenance algorithm under support threshold change. This solution allows user to maintain its rule base under any support threshold.", "creator": "Microsoft\u00ae Word 2010"}}}