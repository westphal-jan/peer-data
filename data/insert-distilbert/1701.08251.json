{"id": "1701.08251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2017", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "abstract": "the popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. in this joint paper, we present a novel task, image - grounded conversations ( previously igc ), in contexts which natural - sounding conversations are generated about shared photographic images. we investigate this task using training data specifically derived from image - grounded conversations on normal social media and introduce a new meaningful dataset of crowd - sourced conversations for measured benchmarking progress. experiments using deep neural network models trained on social media data show that the correct combination of visual and simulated textual context can enhance the quality assurance of generated conversational turns. gaps in human evaluation, a gap between human academic performance and that of both neural and retrieval architectures suggests that igc knowledge presents an nonetheless interesting challenge for vision and language research.", "histories": [["v1", "Sat, 28 Jan 2017 05:06:11 GMT  (244kb,D)", "http://arxiv.org/abs/1701.08251v1", null], ["v2", "Thu, 20 Apr 2017 00:36:35 GMT  (1132kb,D)", "http://arxiv.org/abs/1701.08251v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["nasrin mostafazadeh", "chris brockett", "bill dolan", "michel galley", "jianfeng gao", "georgios p spithourakis", "lucy vanderwende"], "accepted": false, "id": "1701.08251"}, "pdf": {"name": "1701.08251.pdf", "metadata": {"source": "CRF", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "authors": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P. Spithourakis", "Lucy Vanderwende"], "emails": ["nasrinm@cs.rochester.edu,", "chrisbkt@microsoft.com"], "sections": [{"heading": null, "text": "The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, ImageGrounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research."}, {"heading": "1 Introduction", "text": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to answering questions about images (Antol et al., 2015; Malinowski and Fritz, 2014), to storytelling around series of photos (Huang et al., 2016). Much of the focus has been on understanding images in terms of either describing (captioning)\n* This work was conducted at Microsoft.\nthe image or answering questions about their content (Visual Question Answering (VQA)). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by someone unable to see the image. Understanding an image, however, involves more than captioning what is explicitly visible. Figure 1 illustrates a conversation between two users on social media. The conversation is grounded not only in visible objects (e.g., the boys, the bike) but more importantly, in events and actions (e.g., the race, winning) implied by the image. To humans viewing the images, these may be the most interesting and meaningful aspects. Visual Question Generation (VQG) (Mostafazadeh et al., 2016a) attempts to address the challenge of how to generate questions that involve such commonsense understanding of image content.\nWe extend VQG by introducing multimodal conversational context when formulating questions around images and training on naturally occurring social media data. To this end, we introduce the task of Image-Grounded Conversation (IGC), which requires a system to generate questions and responses in a natural-sounding conversation around a given image. IGC thus falls on the continuum between\nar X\niv :1\n70 1.\n08 25\n1v 1\n[ cs\n.C L\n] 2\n8 Ja\nchit-chat models and goal-directed conversation designed to accomplish a task. Visual grounding in an image constrains the topic while providing objects or inferrable events of interest so that a system can proactively drive the conversation forward. In this paper we focus principally on generating questions as conversation drivers.\nThis work thus draws together two threads of investigation that have hitherto remained largely unrelated: vision & language and data-driven conversation models. The contributions of this paper are threefold: (1) We extend VQG by introducing multimodal conversational context when formulating questions around images. To support this, we introduce the task of Image-Grounded Conversation (IGC) via a crowd-sourced dataset of 4,222 6- turn image-grounded conversations that will be publicly released, and compare IGC with other vision & language tasks by analyzing the characteristics of our IGC datasets and the effect of multimodal context (Section 4). (2) We investigate the application of deep neural generation and retrieval approaches for question and response generation tasks (Section 5), using models trained on 250K 3-turn image- grounded conversations found on Twitter and evaluated on our crowdsourced dataset. (3) Our experiments suggest that the combination of visual and textual context improves the quality of generated conversational turns and that visual context is more important than textual (Section 8). It is our hope that this work will furnish useful baselines to others working on multimodal conversation generation."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Vision and Language", "text": "When trained on large datasets, such as the COCO dataset (Lin et al., 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system\nperformance is improved by treating questions as a series in a dialog rather than separate QA pairs. This form of dialog is best considered a simple one-sided QA exchange, in which only humans can ask questions and the system can only provide answers. (Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers.\nMostafazadeh et al. (2016a) introduce the task of visual question generation (VQG), in which the system itself outputs questions about the image. Questions are required to be \u2018natural and engaging\u2019, i.e. a person would find them interesting to answer, and may not be answerable from the image alone. In this work, we build on Mostafazadeh et al. (2016a) by introducing multimodal context when formulating questions and responses."}, {"heading": "2.2 Data-Driven Conversational Modeling", "text": "This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed the response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with the addition of multimodal features to build models that are capable of asking questions on topics of interests to a human, and that might allow a conversational agent to proactively drive the conversation forward."}, {"heading": "3 Image-Grounded Conversations", "text": "For present purposes, we define the scope of IGC as the following two consecutive conversational steps: \u2022 Question Generation: Given a visual context I and a textual context T (e.g., the first statement in Figure 1), generate a coherent, natural question Q about the image as the second utterance in the conversation. As seen in Figure 1, the question may not\nbe directly answerable from the image. \u2022 Response Generation: Given a visual context I, a textual context T, and a question Q, generate a coherent, natural, response R to the question as the third utterance in the conversation. The response may be an answer, as expected in the VQA or Visual Dialog tasks, or it may be a comment, deflection, or other kind of response. The present work does not attempt time to generate responses from generated questions, a task that we leave to future work.\n4 Data Collection\n4.1 IGCTwitter Previous work in neural conversation modeling (Ritter et al., 2010; Sordoni et al., 2015) has successfully used Twitter as the source of millions of natural conversations. In recent years, uploading a photo along with an accompanying tweet has become increasingly popular: multimedia tweets have risen 15% per year (as of June 2015, 28% total), with 42% of retweets containing non-verbal context (Morris et al., 2016). For training data, we sampled 250K quadruples of {visual context, textual context, question, response} tweet threads from a larger dataset of 1.4 million, extracted from the Twitter Firehose over a 3-year period beginning in May 2015 and filtered to select just those conversations in which the initial turn was associated with an image and the second turn was a question. Regular expressions were used to detect questions. To improve the likelihood that the authors are experienced Twitter conversationalists, we further limited extraction to those exchanges where users had actively engaged in at least 30 conversational exchanges during a 3- month period. Twitter data is notoriously noisy; we performed simple normalizations, and filtered out tweets that contained mid-tweet hashtags, were longer than 80 characters1 and contained URLs not linking to the image. Table 1 presents example conversations from this dataset. Although the filters result in significantly higher quality of the extracted conversations, issues remain. A random sample of tweets suggests that about 46% of the Twitter conversations is affected by prior history between users, making response generation particularly difficult. In addition, the abundance of screen shots and non-\n1Pilot studies showed that 80 character limit more effectively retains one-sentence utterances that are to the point.\nphotograph graphics is potentially a major source of noise in extracting features for neural generation.\nWe use the IGCTwitter dataset as primary training data. For validation and test sets during model building, we held out a set of Twitter conversations, in which images and conversations had been vetted by crowd workers to be contentful and free of the kinds of image-related noise noted above."}, {"heading": "4.2 IGCCrowd", "text": "To permit benchmarking of progress in the ICG task, we constructed test and validation datasets with more controlled parameters on the basis of the VQG dataset (Mostafazadeh et al., 2016a). We designed a crowdsourcing platform based on Turkserver (Mao et al., 2012), which enables synchronous and realtime interactions between crowd workers on Amazon Mechanical Turk (Mturk). Multiple workers wait in a virtual lobby to be paired with another worker who will be their conversation partner. After being paired, one of the users selects an image from a large photo gallery, after which the two users enter a chat window in which they have a short conversation about the selected image.\nImages were sampled from the VQG dataset by querying a search engine using event-centric query terms that aggregated \u2018event\u2019 and \u2018process\u2019 hyponyms in WordNet (Miller, 1995) and using fre-\nquent TimeBank events (Pustejovsky et al., 2003). Table 3 shows three full conversations found in the IGCCrowd dataset. As the examples show, eventful images lead to conversations which are semantically very rich and would seem to require commonsense reasoning. Although the present work utilizes only three conversational turns, we collected up to six utterances per image for use in future work. To enable multi-reference evaluation (Section 6), we crowdsourced four additional questions and responses for the best IGCCrowd contexts and initial questions, as ranked by human annotators. The IGCCrowd dataset will be publicly released to the research community."}, {"heading": "4.3 Dataset Characteristics", "text": "Table 2 summarizes basic dataset statistics. Figure 2 compares IGC questions with VQG and VQA questions in terms of vocabulary size, percentage of\nabstract terms, and inter-annotation textual similarity. The COCO image captioning dataset is also included as a point of reference. The IGCTwitter dataset has by far the largest vocabulary size, making it a more challenging dataset for training purposes. The IGCCrowd and IGCTwitter, in order, have the highest ratio of abstract to concrete terms. Broadly, abstract terms refer to intangibles, such as concepts, qualities, and feelings, whereas concrete terms refer to things that can be experienced with the five senses. It appears that conversational content may often involve abstract concepts than either captions or questions targeting visible image content.\nIt has been shown that humans achieve greater consensus on what a natural question to ask given an image (the task of VQG) than on captioning or asking a visually verifiable question (VQA) (Mostafazadeh et al., 2016b). The right-most plot in Figure 2 compares the inter-annotation textual similarity of our IGCCrowd questions using a smoothed BLEU metric (Lin and Och, 2004). IGCTwitter is excluded from this analysis as the data is not multireference. Contextually grounded questions of IGCCrowd are competitive with VQG in inter-\nannotation similarity. Figure 3 shows the distribution of the number of tokens per sentence. On average, the IGCTwitter dataset has longer sentences. Figure 4 visualizes the n-gram distribution (with n=6) of questions across datasets. IGCTwitter is the most diverse set, with the lighter-colored part of the circle indicating sequences with less than 0.1% representation in the dataset.\nThe Effectiveness of Multimodal Context: The task of IGC emphasizes modeling of not only visual but also textual context. We presented human judges with a random sample of 600 triplets of image, textual context, and question (I, T,Q) and asked them to rate the effectiveness of the image and the textual context, i.e., the degree to which the image or text is required in order for the sample question to sound natural. As Figure 5 shows, overall, both visual and textual contexts are indeed highly effective, and understanding both would be required for the question that was asked. We note that the crowd dataset more often requires understanding of the textual context than the Twitter set does.\nFrame Semantic Analysis: The grounded conversations with questions in our datasets are full of\nstereotypical commonsense knowledge. To get a better sense of the richness of our IGCCrowd dataset, we manually annotated a random sample of 330 (I, T,Q) triplets in terms of Minsky\u2019s Frames:2 We annotated the FrameNet (Baker et al., 1998) frame evoked by the image (IFN ), and then textual context (TFN ). Then, for the asked question, we annotated the frame slot3 (QFN\u2212slot) associated with a context frame (QFN ). These annotations can be accessed through https://goo.gl/MVyGzP. As the example in Table 4 shows, the image in isolation often does not evoke any uniquely contentful frame, whereas the textual context frequently does. In only 14% of cases does IFN=TFN , which further supports the complementary effect of our multimodal contexts. Moreover, QFN=IFN for 32% our annotations, whereas QFN=TFN for 47% of the triplets, again showing the effectiveness of textual context in determining the question to be asked."}, {"heading": "5 Models", "text": "We use the VGGNet architecture (Simonyan and Zisserman, 2014) for computing deep convolutional image features. We primarily use the 4096- dimensional output of the last fully connected layer (fc7) as the input to all the models sensitive to visual context.\n2Minsky defines \u2018frame\u2019 as follows: \u201cWhen one encounters a new situation, one selects from memory a structure called a Frame\u201d (Minsky, 1974). According to Minsky, a frame is a commonsense knowledge representation data-structure for representing stereotypical situations, such as a wedding ceremony. Minsky further connects frames to the nature of questions: \u201c[AFrame] is a collection of questions to be asked about a situation\u201d. These questions can ask about the cause, intention, or side-effects of a presented situation.\n3For 17% of cases we could not find a corresponding QFN\u2212slot in FrameNet."}, {"heading": "5.1 Generation Models", "text": "Figure 6 overviews our three generation models. The conversation shown is based on the first conversation in Table 3.\nVisual Context Sensitive Model (V-Gen). Similar to Recurrent Neural Network (RNN) models for image captioning (Devlin et al., 2015; Vinyals et al., 2015), (V-Gen) transforms the image feature vector to a 500-dimensional vector that serves as the initial recurrent state to a 500-dimensional one-layer Gated Recurrent Unit (GRU) which is the decoder module. The output sentence is generated one word at a time until the <EOS> (end-of-sentence) token is generated. We set the vocabulary size to 6000. Unknown words are mapped to an <UNK> token during training, which is not allowed to be generated at decoding time.\nTextual Context Sensitive Model (T-Gen). This is a neural Machine Translation-like model that maps an input sequence to an output sequence (Seq2Seq model (Cho et al., 2014; Sutskever et al., 2014)) using an encoder and a decoder RNN. The decoder module is like the model described above, in this case the initial recurrent state being the 500- dimensional encoding of the textual context. For consistency, we use the same vocab size and number of layers as in the (V-Gen) model.\nVisual & Textual Context Sensitive Model (V&T-Gen). This model fully leverages both textual and visual contexts. The vision feature is transformed to a 500-dimensional vector, and the textual context is likewise encoded into a 500-dimensional vector. The textual feature vector can be obtained using either a bag-of-words (V&T.BOW-Gen) representation, or an RNN (V&T.RNN-Gen), as depicted in Figure 7. The textual feature vector is then concatenated to the vision vector and fed into a fully connected (FC) feed forward neural network. As\na result, we obtain a single 500-dimensional vector encoding both visual and textual context, which then serves as the initial recurrent state of the decoder RNN.\nIn order to generate the response (the third utterance in the conversation), we need to represent the conversational turns in the textual context input. There are various ways to represent conversational history, including a bag of words model, or a concatenation of all textual utterances into one sentence (Sordoni et al., 2015). For response generation, we implement a more complex treatment in which utterances are fed into an RNN one word at a time (Figure 7) following their temporal order in the conversation. An <UTT> marker designates the end of one utterance and the beginning of the next.\nDecoding and Reranking. For all generation models, at decoding time we generate the N-best lists using left-to-right beam search with beam-size 25. We set the maximum number of tokens to 13 for the generated partial hypotheses. Any partial hypothesis that reaches <EOS> token becomes a viable full hypothesis for reranking. The first few hypotheses on top of the N-best lists generated by Seq2Seq models tend to be very generic,4 disregarding the input context. In order to address this issue we rerank\n4An example generic question is where is this? and a generic response is I don\u2019t know.\nthe N-best list using the following score function:\nlog p(h|C) + \u03bb idf(h,D) + \u00b5|h|+ \u03ba V (h) (1)\nwhere p(h|C) is the probability of the generated hypothesis h given the context C. The function V counts the number of verb POS in the hypothesis and |h| denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, simply computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N-best list. We define idf(h, D) = log |D||{d\u2208D:h\u2208d}| , where we set N=10 to cut short each N-best list. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011)."}, {"heading": "5.2 Retrieval Models", "text": "In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).\nVisual Context Sensitive Model (V-Ret). This model only uses the given image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the fc7 vision feature vectors. Then we retrieve thoseK annotations as our pool ofK candidates. Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och, 2004) similarity score, then emit the sentence with the highest similarity to the rest of the pool.\nVisual & Textual Context Sensitive Model (V&T-Ret). This model uses both visual and textual contexts to retrieve a question or a response. A linear combination of fc7 and word2vec feature vectors is utilized for retrieving similar training instances."}, {"heading": "6 Evaluation Setup", "text": "We provide both human and automatic evaluations for our question and response generation tasks. We crowdsource our human evaluation on an AMT-like crowdsourcing system, asking seven crowd workers to each rate the quality of candidate questions or responses on a three-point Likert-like scale, ranging from 1 to 3 (the highest). In order to ensure a calibrated rating, we show the human judges all system hypotheses for a particular test case at the same time. System outputs were randomly ordered to prevent judges from guessing which systems were which on the basis of position. After collecting judgments, we averaged the scores throughout the test set for each model. We discarded as spammers all annotators whose ratings varied from the mean by more than 2 standard deviations.\nAlthough human evaluation is to be preferred, and currently essential, in open-domain generation tasks involving intrinsically diverse outputs, it is useful to have an automatic metric for day-to-day evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and references. Results reported below employ BLEU with equal weights up to 4-grams."}, {"heading": "7 Experimental Results", "text": "We experiment with all the models presented in Section 5. For question generation, we use a visual & textual sensitive model that uses bag-ofwords (V&T.BOW-Gen) to represent the textual context, which achieved better results. Earlier vision & language work such as VQA (Antol et al., 2015) has shown that a bag-of-words baseline outperforms LSTM-based models for representing textual input when visual features are available (Zhou et al., 2015). In response generation, which needs to account for textual input consisting of two turns, we use the V&T.RNN-Gen model as the visual &\ntextual-sensitive model in the R. rows of tables 6 and 7. Since generating a response solely from the visual context is unlikely to be successful, we do not use the V-Gen model in response generation.\nTable 5 presents a few example generations by our best performing systems. Tables 6 and 7 provide the human and automatic evaluation results for all of our models. All models are trained on IGCTwitter (training set), except for the model labeled VQG, which is the same (V-Gen) model, but trained on 7,500 questions from the VQG dataset (Mostafazadeh et al., 2016b). All systems have been tuned/tested on the corresponding IGC dataset. As a point of reference, we include the gold standard human reference in the human evaluations.\nIn human evaluation, the model that encodes both visual and textual context outperforms others, except for the question generation on Twitter in which the visual model wins. It appears the visual context is more effective in Twitter dataset, as we have shown in Section 4. We note that human judges preferred the top generation in the n-best list over the reranked best, likely due to tradeoff between a safe and generic utterance and a riskier but contentful one. As shown in Table 6, our best performing\ngeneration system scores higher than human on the Twitter test set for question generation, but otherwise the human gold references in our Crowd set are consistently favored throughout the table. We take this as evidence that IGCCrowd provides a robust and challenging test set for benchmarking the progress on the task.\nBLEU scores are low, as is characteristic for language tasks with intrinsically diverse outputs (Li et al., 2016b; Li et al., 2016a). Automatic evaluation in Table 7 provides confirmation that the IGCCrowd test set is more challenging to models that have been trained on IGCTwitter. On BLEU, the multimodal V&T model outperforms all the other models across test sets, except for the multireference test set (Crowdm in Table 7), in which the VQG model does significantly better. We attribute this to differences in the Twitter and our Crowd datasets, as discussed in Section 4.\nOverall, in both automatic and human evaluation, our question generation models are more successful than response generation. More sophisticated models and larger training data sets may overcome this disparity in the future."}, {"heading": "8 Conclusions", "text": "We have introduced a new task of multimodal image-grounded conversation, in which, when given an image and a natural language text, the system must generate meaningful conversation turns. To support this task, we are releasing to the research community a crowdsourced dataset of 4,222 highquality conversations about eventful images with up to 6 turns each, and multiple references.\nOur experiments provide evidence that capturing multimodal context improves the quality of generation. The gap between the performances of our best\nmodels and humans opens opportunities further research in the continuum from casual conversation to more task- and topic-oriented vision and language dialog. We expect also that addition of other kinds of grounding may further improve performance of systems."}], "references": [{"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "Proceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING \u201998, pages 86\u201390, Stroudsburg, PA, USA. Association for Com-", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["Jianfu Chen", "Polina Kuznetsova", "David Warren", "Yejin Choi."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Visual dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra."], "venue": "CoRR, abs/1611.08669.", "citeRegEx": "Das et al\\.,? 2016", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Com-", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "CoRR, abs/1411.4389.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "CoRR,", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "Proceedings of the 11th European Conference on Computer Vision:", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "J. Artif. Int. Res., 47(1):853\u2013899, May.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352\u20131362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "Visual storytelling", "author": ["Mitchell."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1233\u20131239, San Diego, California, June. Association for Computa-", "citeRegEx": "Mitchell.,? 2016", "shortCiteRegEx": "Mitchell.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Strouds-", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "Microsoft COCO: Common Objects in Context, pages 740\u2013755", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems 27, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Turkserver: Enabling synchronous and longitudinal online experiments", "author": ["Andrew Mao", "David Parkes", "Yiling Chen", "Ariel D. Procaccia", "Krzysztof Z. Gajos", "Haoqi Zhang."], "venue": "Workshop on Human Computation (HCOMP).", "citeRegEx": "Mao et al\\.,? 2012", "shortCiteRegEx": "Mao et al\\.", "year": 2012}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Commun. ACM, 38(11):39\u201341, November.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A framework for representing knowledge", "author": ["Marvin Minsky."], "venue": "Technical report, Cambridge, MA, USA.", "citeRegEx": "Minsky.,? 1974", "shortCiteRegEx": "Minsky.", "year": 1974}, {"title": "with most of it being pictures now, I rarely use it\u201d: Understanding twitter\u2019s evolving accessibility to blind users", "author": ["Meredith Ringel Morris", "Annuska Zolyomi", "Catherine Yao", "Sina Bahram", "Jeffrey P. Bigham", "Shaun K. Kane."], "venue": "Jofish Kaye, Allison Druin,", "citeRegEx": "Morris et al\\.,? 2016", "shortCiteRegEx": "Morris et al\\.", "year": 2016}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende."], "venue": "Proceedings of the Annual Meeting on Association for Computational Linguistics, ACL \u201916. Association", "citeRegEx": "Mostafazadeh et al\\.,? 2016a", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Mostafazadeh et al\\.,? 2016b", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg."], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The TIMEBANK corpus", "author": ["J. Pustejovsky", "P. Hanks", "R. Sauri", "A. See", "R. Gaizauskas", "A. Setzer", "D. Radev", "B. Sundheim", "D. Day", "L. Ferro", "M. Lazo."], "venue": "Proceedings of Corpus Linguistics 2003, pages 647\u2013 656, Lancaster, March.", "citeRegEx": "Pustejovsky et al\\.,? 2003", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Question relevance in VQA: identifying non-visual and false-premise questions", "author": ["Arijit Ray", "Gordon Christie", "Mohit Bansal", "Dhruv Batra", "Devi Parikh."], "venue": "Jian Su, Xavier Carreras, and Kevin Duh, editors, Proceedings of the 2016 Conference on Empirical Meth-", "citeRegEx": "Ray et al\\.,? 2016", "shortCiteRegEx": "Ray et al\\.", "year": 2016}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "In HLT-NAACL.", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["Marcus Rohrbach", "Sikandar Amin", "Mykhaylo Andriluka", "Bernt Schiele."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE, June.", "citeRegEx": "Rohrbach et al\\.,? 2012", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2012}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Saenko."], "venue": "Proceedings the 2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT 2015),", "citeRegEx": "Saenko.,? 2015", "shortCiteRegEx": "Saenko.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Deep Learning Workshop, ICML.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus."], "venue": "CoRR, abs/1512.02167.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 7, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 6, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 2, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 29, "context": ", 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to answering questions about images (Antol et al.", "startOffset": 102, "endOffset": 151}, {"referenceID": 0, "context": ", 2015), to answering questions about images (Antol et al., 2015; Malinowski and Fritz, 2014), to storytelling around series of photos (Huang et al.", "startOffset": 45, "endOffset": 93}, {"referenceID": 16, "context": ", 2015), to answering questions about images (Antol et al., 2015; Malinowski and Fritz, 2014), to storytelling around series of photos (Huang et al.", "startOffset": 45, "endOffset": 93}, {"referenceID": 21, "context": "Visual Question Generation (VQG) (Mostafazadeh et al., 2016a) attempts to address the challenge of how to generate questions that involve such commonsense understanding of image content.", "startOffset": 33, "endOffset": 61}, {"referenceID": 15, "context": "When trained on large datasets, such as the COCO dataset (Lin et al., 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 5, "context": ", 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al.", "startOffset": 110, "endOffset": 172}, {"referenceID": 7, "context": ", 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al.", "startOffset": 110, "endOffset": 172}, {"referenceID": 6, "context": ", 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al.", "startOffset": 110, "endOffset": 172}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014).", "startOffset": 47, "endOffset": 67}, {"referenceID": 16, "context": ", 2015) and (Malinowski and Fritz, 2014).", "startOffset": 12, "endOffset": 40}, {"referenceID": 26, "context": "(Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system performance is improved by treating questions as a series in a dialog rather than separate QA pairs.", "startOffset": 48, "endOffset": 251}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system performance is improved by treating questions as a series in a dialog rather than separate QA pairs. This form of dialog is best considered a simple one-sided QA exchange, in which only humans can ask questions and the system can only provide answers. (Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers. Mostafazadeh et al. (2016a) introduce the task of visual question generation (VQG), in which the system itself outputs questions about the image.", "startOffset": 48, "endOffset": 935}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system performance is improved by treating questions as a series in a dialog rather than separate QA pairs. This form of dialog is best considered a simple one-sided QA exchange, in which only humans can ask questions and the system can only provide answers. (Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers. Mostafazadeh et al. (2016a) introduce the task of visual question generation (VQG), in which the system itself outputs questions about the image. Questions are required to be \u2018natural and engaging\u2019, i.e. a person would find them interesting to answer, and may not be answerable from the image alone. In this work, we build on Mostafazadeh et al. (2016a) by introducing multimodal context when formulating questions and responses.", "startOffset": 48, "endOffset": 1261}, {"referenceID": 33, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 31, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 30, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 36, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 12, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 13, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 25, "context": "Ritter et al. (2011) posed the response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": ", 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.", "startOffset": 30, "endOffset": 89}, {"referenceID": 27, "context": "Previous work in neural conversation modeling (Ritter et al., 2010; Sordoni et al., 2015) has successfully used Twitter as the source of millions of natural conversations.", "startOffset": 46, "endOffset": 89}, {"referenceID": 33, "context": "Previous work in neural conversation modeling (Ritter et al., 2010; Sordoni et al., 2015) has successfully used Twitter as the source of millions of natural conversations.", "startOffset": 46, "endOffset": 89}, {"referenceID": 20, "context": "In recent years, uploading a photo along with an accompanying tweet has become increasingly popular: multimedia tweets have risen 15% per year (as of June 2015, 28% total), with 42% of retweets containing non-verbal context (Morris et al., 2016).", "startOffset": 224, "endOffset": 245}, {"referenceID": 21, "context": "To permit benchmarking of progress in the ICG task, we constructed test and validation datasets with more controlled parameters on the basis of the VQG dataset (Mostafazadeh et al., 2016a).", "startOffset": 160, "endOffset": 188}, {"referenceID": 17, "context": "We designed a crowdsourcing platform based on Turkserver (Mao et al., 2012), which enables synchronous and realtime interactions between crowd workers on Amazon Mechanical Turk (Mturk).", "startOffset": 57, "endOffset": 75}, {"referenceID": 18, "context": "Images were sampled from the VQG dataset by querying a search engine using event-centric query terms that aggregated \u2018event\u2019 and \u2018process\u2019 hyponyms in WordNet (Miller, 1995) and using fre-", "startOffset": 159, "endOffset": 173}, {"referenceID": 25, "context": "quent TimeBank events (Pustejovsky et al., 2003).", "startOffset": 22, "endOffset": 48}, {"referenceID": 22, "context": "It has been shown that humans achieve greater consensus on what a natural question to ask given an image (the task of VQG) than on captioning or asking a visually verifiable question (VQA) (Mostafazadeh et al., 2016b).", "startOffset": 189, "endOffset": 217}, {"referenceID": 14, "context": "The right-most plot in Figure 2 compares the inter-annotation textual similarity of our IGCCrowd questions using a smoothed BLEU metric (Lin and Och, 2004).", "startOffset": 136, "endOffset": 155}, {"referenceID": 1, "context": "annotated the FrameNet (Baker et al., 1998) frame evoked by the image (IFN ), and then textual context (TFN ).", "startOffset": 23, "endOffset": 43}, {"referenceID": 32, "context": "We use the VGGNet architecture (Simonyan and Zisserman, 2014) for computing deep convolutional image features.", "startOffset": 31, "endOffset": 61}, {"referenceID": 19, "context": "Minsky defines \u2018frame\u2019 as follows: \u201cWhen one encounters a new situation, one selects from memory a structure called a Frame\u201d (Minsky, 1974).", "startOffset": 125, "endOffset": 139}, {"referenceID": 5, "context": "Similar to Recurrent Neural Network (RNN) models for image captioning (Devlin et al., 2015; Vinyals et al., 2015), (V-Gen) transforms the image feature vector to a 500-dimensional vector that serves as the initial recurrent state to a 500-dimensional one-layer Gated Recurrent Unit (GRU) which is the decoder module.", "startOffset": 70, "endOffset": 113}, {"referenceID": 37, "context": "Similar to Recurrent Neural Network (RNN) models for image captioning (Devlin et al., 2015; Vinyals et al., 2015), (V-Gen) transforms the image feature vector to a 500-dimensional vector that serves as the initial recurrent state to a 500-dimensional one-layer Gated Recurrent Unit (GRU) which is the decoder module.", "startOffset": 70, "endOffset": 113}, {"referenceID": 3, "context": "This is a neural Machine Translation-like model that maps an input sequence to an output sequence (Seq2Seq model (Cho et al., 2014; Sutskever et al., 2014)) using an encoder and a decoder RNN.", "startOffset": 113, "endOffset": 155}, {"referenceID": 34, "context": "This is a neural Machine Translation-like model that maps an input sequence to an output sequence (Seq2Seq model (Cho et al., 2014; Sutskever et al., 2014)) using an encoder and a decoder RNN.", "startOffset": 113, "endOffset": 155}, {"referenceID": 33, "context": "There are various ways to represent conversational history, including a bag of words model, or a concatenation of all textual utterances into one sentence (Sordoni et al., 2015).", "startOffset": 155, "endOffset": 177}, {"referenceID": 14, "context": "We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011).", "startOffset": 98, "endOffset": 117}, {"referenceID": 10, "context": "We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011).", "startOffset": 168, "endOffset": 191}, {"referenceID": 21, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 5, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 9, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 23, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 8, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 14, "context": "Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och, 2004) similarity score, then emit the sentence with the highest similarity to the rest of the pool.", "startOffset": 104, "endOffset": 123}, {"referenceID": 24, "context": "For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and references.", "startOffset": 80, "endOffset": 103}, {"referenceID": 0, "context": "Earlier vision & language work such as VQA (Antol et al., 2015) has shown that a bag-of-words baseline outperforms LSTM-based models for representing textual input when visual features are available (Zhou et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 38, "context": ", 2015) has shown that a bag-of-words baseline outperforms LSTM-based models for representing textual input when visual features are available (Zhou et al., 2015).", "startOffset": 143, "endOffset": 162}], "year": 2017, "abstractText": "The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, ImageGrounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research.", "creator": "LaTeX with hyperref package"}}}