{"id": "1701.08702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model", "abstract": "obviously in this paper, we describe a research method that generates bangla word clusters on the basis of relating to meaning in vernacular language and contextual similarity. the importance of such word clustering is in parts of speech ( pos ) tagging, functional word sense disambiguation, text classification, attribute recommender system, spell master checker, grammar checker, microsoft knowledge store discover and for many others natural language processing ( nlp ) applications. in the history of word clustering, academic english and some other languages have already implemented some methods on word clustering efficiently. but due to lack of the resources, word clustering in bangla context has not systematically been still implemented efficiently. comparatively presently, its implementation is in the beginning stage. in some research of word clustering in early english based on preceding and next five words of marking a key word they found an efficient result. now, we are trying to certainly implement the tri - gram, 4 - gram and 5 - ki gram model of word sample clustering for indian bangla to observe which one is asking the best noun among them. we have started commencing our research with quite a large corpus of approximate 1 lakh bangla words. we undoubtedly are using a machine learning technique in this research. we will generate sampled word clusters and analyze the clusters by testing some different attribute threshold values.", "histories": [["v1", "Fri, 27 Jan 2017 18:43:31 GMT  (281kb)", "http://arxiv.org/abs/1701.08702v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dipaloke saha", "md saddam hossain", "md saiful islam", "sabir ismail"], "accepted": false, "id": "1701.08702"}, "pdf": {"name": "1701.08702.pdf", "metadata": {"source": "CRF", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model", "authors": ["Dipaloke Saha", "Md Saddam Hossain", "Saiful Islam", "Sabir Ismail"], "emails": ["dipsustcse12@gmail.com,", "mshossaincse@gmail.com", "saiful-cse@sust.edu", "sabir.ismail01@gmail.com", "dipsustcse12@gmail.com"], "sections": [{"heading": null, "text": "Keywords:\n Word Cluster,\nNatural\nLanguage\nProcessing,\nMachine\nLearning,\nN-gram Model,\nTerm\nFrequency (tf).\n SUST, ICERIE."}, {"heading": "1. INTRODUCTION", "text": "Though Bangla is a widely spoken language, it has lack of resources in its research field. Recently, a new research dimension in Bangla is added called word clustering. In this paper, the research of word clustering for Bangla language is trying to be extended. For this, a large Bangla corpus containing 97,971 individual words is compiled to generate the word clusters. In this paper, an unsupervised machine learning technique and a method are proposed to cluster Bangla words on the basis of similarity in semantics and contexts.\nIn language processing word cluster has a wide range of applications. POS tag is one of them. Same clustered words usually contain the same POS tag. Word clustering can produce suggestions for an inaccurately typed word which is very much helpful for spell checker. Word sense disambiguation, sentence structure with grammatical mistakes can also be solvable using clustered words. In the case of recommender system if related products of the same category are clustered in the same group, more feasible suggestion can be produced. This type of work is also useful for Bangla search engine to find the appropriate content. So, there is a huge importance of word clustering in the field of natural language processing.\n* dipsustcse12@gmail.com"}, {"heading": "2. RELATED WORK", "text": "In Bangla the implementation of word clustering is in the neophyte stage. A previous work on Bangla word clustering exists in which an unsupervised machine learning technique is used to implement the bigram model by Sabir Ismail and M. Shahidur Rahman. In many other languages different types of techniques are used for word clustering. Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster. However, with the use of delete interpolation method by Mori, Nishimura and Itoh (1998) they got the better result than the Brown, Desouza\u2019s method. This was done for Japanese and English language. Besides these, there exists quite a good number of researches of word clustering for some other languages like Russian, Arabic, Chinese etc."}, {"heading": "3. PROBLEM DEFINITION", "text": "Clustering is an unsupervised machine learning technique that does not require any type of rules or predefined conditions. Items which are much similar either in semantically or contextually are grouped in the same cluster and which are dissimilar are in different clusters. The introduced method in this problem is concentrating on two types of similarity such as semantics and contextual similarity.\nConsider the following four sentences:\nand are similar in semantic meaning in sentence 1 and 2 and there is similarity in and\nin sentence 3 and 4. Here, the theory of N-gram model is implemented . Probability distribution is used\nhere to define n-th item in a sequence form previous or next (n-1) items. Tri-gram, 4 th and 5 th gram model is defined as size of 3, 4 and 5 of N-gram respectively. In this research, word clusters will be generated by implementing tri, 4 th and 5 th\ngram model. After finding the word clusters the most efficient model will be found out based on those clustering words."}, {"heading": "4. METHODOLOGY", "text": "Firstly, quite a large corpus of 97,971 individual words Wi is used in this research. Next, a list of previous three words of a specific word for tri-gram, four words for 4-gram, five words for 5-gram are prepared. Similarly, a list of next three words of a specific word for tri-gram, four words for 4-gram, five words for 5- gram are prepared. Next, similarity between a pair of words to be included in the same cluster based on preceding three words, four words and five words are determined as follows:\nIn tri-gram for every pair of words Wi, Wj the number of matched preceding words from list\nlist(Wi-3, Wi-2 , Wi-1) and list(Wj-3, Wj-2, Wj-1)\nP(Wi,Wj)=(Count(match(list(Wi-3,Wi-2,Wi-1),list(Wj-3,Wj-2,Wj-1)))/((Count(list(Wi-3,Wi-2,Wi1))+Count(list(Wj-3,Wj-2,Wj-1 )))\nSimilarly, calculation for the 4-gram model is:\nP(Wi,Wj) = (Count(match(list(Wi-4,Wi-3,Wi-2 ,Wi-1),list(Wj-4,Wj-3,Wj-2,Wj-1)))/((Count(list(Wi-4,Wi-3,Wi-2, Wi-1))+Count(list(Wj-4,Wj-3,Wj-2,Wj-1)))\n3 | Dipaloke et. al., I C E R I E 2 0 1 7\nand for 5-gram model is:\nP(Wi,Wj)=(Count(match(list(Wi-5,Wi-4,Wi-3,Wi-2,Wi-1),list(Wj-5,Wj-4,Wj-3,Wj-2,Wj-1)))/((Count(list(Wi5,Wi-4,Wi-3,Wi-2, Wi-1))+Count(list(Wj-5,Wj-4,Wj-3,Wj-2,Wj-1)))\nAgain similarly, between a pair of words to be included in the same cluster based on following three, four and five words are determined as follows, For tri-gram,\nP(Wi,Wj)=(Count(match(list(Wi+3,Wi+2,Wi+1),list(Wj+3,Wj+2,Wj+1)))/((Count(list(Wi+3,Wi+2,Wi+1)) +Count(list(Wj+3, Wj+2, Wj+1)))\nSimilarly, calculation for the 4-gram model is:\nP(Wi,Wj)=(Count(match(list(Wi+4,Wi+3,Wi+2,Wi+1),list(Wj+4,Wj+3,Wj+2,Wj+1)))/((Count(list(Wi+4,Wi+3 ,Wi+2, Wi+1))+Count(list(Wj+4,Wj+3,Wj+2,Wj+1)))\nand for 5-gram model is:\nP(Wi,Wj)=(Count(match(list(Wi+5,Wi+4,Wi+3,Wi+2,Wi+1),list(Wj+5,Wj+4,Wj+3,Wj+2,Wj+1)))/((Count(list (Wi+5,Wi+4,Wi+3,Wi+2,Wi+1))+Count(list(Wj+5,Wj+4,Wj+3,Wj+2,Wj+1)))\nIf the above equations of a particular model yield values greater than a predefined threshold value they\nare grouped into the same cluster for that model. For example, to implement the tri-gram model some of the following phrases are :\nFor word preceding three words list:\nlist(Wi-3, Wi-2, Wi-1) =\nCount (list(Wi-3, Wi-2, Wi-1)) = 3\nFor word Following three words list :\nlist(Wi+3, Wi+2, Wi+1) = Count (list(Wi+3, Wi+2, Wi+1)) = 3\nFor word preceding three words list:\nlist(Wj-3, Wj-2, Wj-1) = Count (list(Wj-3, Wj-2, Wj-1)) = 3\nFor word following three words list:\nlist(Wj+3, Wj+2, Wj+1) = Count(list(Wj+3, Wj+2, Wj+1)) = 3\nNumber of matched words for word with based on preceding three words :\nCount(match(list(Wi-3, Wi-2 , Wi-1),list(Wj-3, Wj-2 , Wj-1))) = 2 Count (list(Wi-3, Wi-2, Wi-1)) + Count (list(Wj-3, Wj-2, Wj-1)) = 6\nSimilarity between words and based on preceding three words:\nP(Wi, Wj) = 2/6 = 0.33\nNumber of matched words for word and based on following three words:\nCount(match(list(Wi+3, Wi+2 , Wi+1),list(Wj+3, Wj+2 , Wj+1))) = 2 Count(list(Wi+3, Wi+2, Wi+1)) + Count(list(Wj+3, Wj+2, Wj+1)) = 6\nSimilarity between words and based on following three words:\nP(Wi, Wj) = 2/6 = 0.33\nSimilarly, 4 th and 5 th gram model can be implemented in the same way.\nThe value of similarity between words with when considering preceding three words is 0.33 and considering following three words it is also 0.33. Different types of threshold values are experimented and best result is earned with 0.20. Both words are grouped in the same cluster when all the probability scores are greater than this threshold value."}, {"heading": "5. RESULT ANALYSIS", "text": "In the tri, 4 th and 5 th gram model we derive 2215, 3327 and 5730 word clusters in total respectively. Some\nclusters randomly from each of the model are represented here in the following tables:\n5 | Dipaloke et. al., I C E R I E 2 0 1 7\nAfter analyzing the word clusters of all the three models we find poor similarity in some word clusters such as 266 for tri-gram, 300 for 4 th gram and 360 for 5 th gram. So, we find 1949, 3027 and 5370 clusters in strong similarity for the tri, 4 th and 5 th gram model respectively. So, the accuracy for strong similarity in\nTri-gram :- 88%\n4 th gram :- 91%\n5 th gram :- 93%\nSo, it is observed that 4 th gram is is better than tri-gram and 5 th gram is the best in all of them."}, {"heading": "6. CONCLUSION", "text": "Word clustering is important for various types of purpose for any language. For this reason in Bangla, trigram, 4 th gram and 5 th gram model is implemented here to proceed the previous work on word clustering. The analysis and result presented above on quite a large Bangla corpus has helped us to find the efficiency among the three mentioned models for word clustering. On the basis of the observation, it can be said that better efficiency is in the higher orders than the preceding orders of the N-gram model."}, {"heading": "H A S\u00e1nchez, A P Porrata and R B Llavori. \u201cWord sense disambiguation based on word sense clustering\u201d.", "text": "Advances in Artificial Intelligence,Springer Berlin Heidelberg, 2006. P: 472-481.\nSabir Ismail, M. Shahidur Rahman. https://www.researchgate.net/publication/261551758_Bangla_Word_Cl ustering_Based_on_N-gram_Language_Model , in press."}, {"heading": "S Finch and N Chater. \u201cAutomatic methods for finding linguisticcategories\u201d. In Igor Alexander and John", "text": "Taylor, editors,ArtificialNeural Networks, Volume 2. Elsevier Science Publishers, 1992.\nP F Brown, P V Desouza, R L Mercer, V J D Pietra, V J Della. and J CLai. \u201cClass-based N-gram Models of Natural Language\u201d.Computationalinguistics, 18 No: 4, 1992, P: 467-479.\nEEKorkmaz. \u201cA method for improving automatic wordcategorization\u201d. Doctoral dissertation, Middle East Technical University, 1997, in press."}, {"heading": "S Mori, M Nishimura and N Itoh. \u201cWord clustering for a word bi - gramModel\u201d. International Conference on", "text": "Spoken Language Processing, 1998, in press.\nClustering \u2013 Introduction, http://home.deib.polimi.it/ matteucc/Clustering/tutorial_html.\nClustering \u2013 Introduction, \u201chttp://www.stanford.edu/class/cs345a/slides/12-clustering.pdf \u201c.Stanford UniversityClustering.\nSimilarity in semantics and contexts, http://www.ilc.cnr.it/EAGLES96/rep2/node37.html"}], "references": [{"title": "Word sense disambiguation based on word sense clustering", "author": ["H A S\u00e1nchez", "A P Porrata", "R B Llavori"], "venue": "Advances in Artificial Intelligence,Springer Berlin Heidelberg,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2006}, {"title": "Automatic methods for finding linguisticcategories", "author": ["S Finch", "N Chater"], "venue": "Elsevier Science Publishers,", "citeRegEx": "Finch and Chater.,? \\Q1992\\E", "shortCiteRegEx": "Finch and Chater.", "year": 1992}, {"title": "Word clustering for a word bi - gramModel", "author": ["S Mori", "M Nishimura", "N Itoh"], "venue": "Technical University,", "citeRegEx": "Mori et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mori et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster.", "startOffset": 0, "endOffset": 225}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster.", "startOffset": 0, "endOffset": 292}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster. However, with the use of delete interpolation method by Mori, Nishimura and Itoh (1998) they got the better result than the Brown, Desouza\u2019s method.", "startOffset": 0, "endOffset": 478}], "year": 2016, "abstractText": "\uf0b7 SUST, ICERIE. Abstract: \u2014 In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, it\u2019s implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values.", "creator": "Microsoft\u00ae Office Word 2007"}}}