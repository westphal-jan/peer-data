{"id": "1702.00500", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2017", "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar", "abstract": "this paper addresses the task of amr - to - text generation by leveraging synchronous node replacement grammar. during reinforcement training, graph - to - string rules are learned using only a heuristic extraction model algorithm. at test time, using a mechanical graph transducer is applied to collapse input amrs signals and generate output sentences. extensively evaluated on semeval - 2016 task 8, our method clearly gives a bleu optimal score of 25. 62, which is the best reported so far.", "histories": [["v1", "Wed, 1 Feb 2017 23:36:33 GMT  (42kb,D)", "http://arxiv.org/abs/1702.00500v1", null], ["v2", "Tue, 7 Feb 2017 18:22:24 GMT  (0kb,I)", "http://arxiv.org/abs/1702.00500v2", "Also there is a crucial error in section 2.2, 2.3 and 4.1"], ["v3", "Mon, 3 Apr 2017 02:39:14 GMT  (42kb,D)", "http://arxiv.org/abs/1702.00500v3", "Accepted by ACL 2017"], ["v4", "Fri, 28 Apr 2017 13:37:00 GMT  (78kb,D)", "http://arxiv.org/abs/1702.00500v4", "camera-ready version of ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "xiaochang peng", "yue zhang", "zhiguo wang", "daniel gildea"], "accepted": true, "id": "1702.00500"}, "pdf": {"name": "1702.00500.pdf", "metadata": {"source": "CRF", "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar", "authors": ["Linfeng Song", "Xiaochang Peng", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea"], "emails": [], "sections": [{"heading": null, "text": "AMR-to-text Generation with Synchronous Node Replacement Grammar\nLinfeng Song1, Xiaochang Peng1, Yue Zhang3, Zhiguo Wang2 and Daniel Gildea1 1Department of Computer Science, University of Rochester, Rochester, NY 14627\n2IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 3Singapore University of Technology and Design\nAbstract\nThis paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far."}, {"heading": "1 Introduction", "text": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as \u201cboy\u201d, \u201cwant-01\u201d) represent concepts, and edges (such as \u201cARG0\u201d, \u201cARG1\u201d) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).\nAMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).\nFlanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it\nto a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. On the other hand, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graph-fragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings.\nWe propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of \u3008AMR, sentence\u3009 pairs. At test time, we apply a graph transducer to collapse input AMR graphs and generate output strings according to the\nar X\niv :1\n70 2.\n00 50\n0v 1\n[ cs\n.C L\n] 1\nF eb\n2 01\n7\nlearned grammar. Our system makes use of a loglinear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on the SemEval2016 Task 8 benchmark, which is the best result reported so far."}, {"heading": "2 Synchronous Node Replacement Grammar", "text": ""}, {"heading": "2.1 Grammar Definition", "text": "A synchronous node replacement grammar (NRG) is a rewriting formalism: G = \u3008N,\u03a3,\u2206, P, S\u3009, where N is a finite set of nonterminals, \u03a3 and \u2206 are finite sets of terminal symbols for the source and target sides, respectively. S \u2208 N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi \u2192 (\u3008F,E\u3009,\u223c), where Xi \u2208 N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over \u03a3 and node labels over N \u222a \u03a3, E is a corresponding target string over N \u222a\u2206 and \u223c denotes the alignment of nonterminal symbols between F and E. A classic NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism defining how F is connected to the rest of the graph when replacing Xi with F on the graph. Here we omit defining C and allow arbitrary connections1. Following Chiang\n1This may over generate, but does not affect our case, as in our bottom-up decoding procedure (section 3) when F is\nData: training corpus C Result: rule instances R\n1 R\u2190 []; 2 for (S,A) in C do 3 Rcur \u2190 FRAGMENTEXTRACT(S,A); 4 for rj in Rcur do 5 R.APPEND(ri) ; 6 for ri in Rcur/{ri} do 7 if ri.CONTAINS(rj) then 8 rij \u2190 ri.COLLAPSE(rj); 9 R.APPEND(rij) ;\n10 end 11 end 12 end 13 end\nAlgorithm 1: Rule extraction\n(2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances.\nFigure 2 shows an example derivation process for the sentence \u201cthe boy wants to go\u201d given the rule set in Table 1. Given the start symbol S, which is first replaced with X1, rule (c) is applied to generate \u201cX2 to go\u201d and its AMR counterpart. Then rule (b) is used to generate \u201cX3 wants\u201d and its AMR counterpart from X2. Finally, rule (a) is used to generate \u201cthe boy\u201d and its AMR counterpart from X3. Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006)."}, {"heading": "2.2 Induced Rules", "text": "There are three types of rules in our system, namely induced rules, concept rules and graph glue rules. Here we first introduce induced rules, which are obtained by a two-step procedure on a training corpus. Shown in Algorithm 1, the first\nreplaced with Xi, nodes previously connected to F are reconnected to Xi\nstep is to extract a set of initial rules from training \u3008sentence, AMR\u3009 pairs (Line 2) using the phraseto-graph fragment extraction algorithm of Peng et al. (2015) (Line 3). Here an initial rule contains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj according to Cai and Knight (2013), and generate rij by collapsing ri with rj , if ri contains rj\n2 (Line 6-8). When collapsing ri with rj , we replace the corresponding subgraph in ri.F with a new non-terminal node, and the sub-phrase in ri.E with a non-terminal. For example, we obtain rule (b) from rules (d) and (a) in Table 1. All initial and generated rules are stored in a rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set."}, {"heading": "2.3 Concept Rules and Glue Rules", "text": "In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of the node concept. A concept rule is used in case no induced rule can cover the node. We refer to the verbalization list3 and AMR guidelines4 for creating more complex concept rules. For example, one concept rule created from the verbalization list is \u201c(k / keep-01 :ARG1 (p / peace)) ||| peacekeeping\u201d.\nInspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied. Three glue rules are defined for each type of edge label. Taking the edge label \u201cARG0\u201d as an example, we create the following glue rules: r1 (X1 / #X1# :ARG0 (X2 / #X2#)) ||| #X1# #X2# r2 (X1 / #X1# :ARG0 (X2 / #X2#)) ||| #X2# #X1# r3 (X1 / #X1# :ARG0 X1) ||| #X1# where for both r1 and r2, F contains two nonterminal nodes with a directed edge connecting them, and E is the concatenation the two nonterminals in either the monotonic or the inverse order. For r3, F contains one non-terminal node with a self-pointing edge, and E is the nonterminal. With concept rules and glue rules in our final rule set, it is easily guaranteed that there are legal derivations for any input AMR graph.\n2more formally, rj .F is a subgraph of ri.F and ri.E contains rj .E\n3http://amr.isi.edu/download/lists/verbalization-listv1.06.txt\n4https://github.com/amrisi/amrguidelines/blob/master/amr.md"}, {"heading": "3 Model", "text": "We adopt a log-linear model for scoring search hypotheses. Given an input AMR graph, we find the highest scored derivation t\u2217 from all possible derivations t:\nt\u2217 = argmax t exp( \u2211 i wifi(g, t)), (1)\nwhere g denotes the input AMR, fi(\u00b7, \u00b7) and wi represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).\nWe perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam."}, {"heading": "3.1 Translation Probabilities", "text": "Production rules serve as a basis for scoring hypotheses. We associate each synchronous NRG rule n \u2192 (\u3008F,E\u3009,\u223c) with a set of probabilities. First, phrase-to-fragment translation probabilities are defined based on maximum likelihood estimation (MLE), as shown in Equation 2, where c\u3008F,E\u3009 is the fractional count of \u3008F,E\u3009.\np(F |E) = c\u3008F,E\u3009\u2211 F \u2032 c\u3008F \u2032,E\u3009\n(2)\nIn addition, lexicalized translation probabilities are defined as:\npw(F |E) = \u220f l\u2208F \u2211 w\u2208E p(l|w) (3)\nHere l is a label (including both edge labels such as \u201cARG0\u201d and concept labels such as \u201cwant-01\u201d) in the AMR fragment F , and w is a word in the phrase E. Equation 3 can be regarded as a \u201csoft\u201d version of the lexicalized translation probabilities adopted by SMT, which picks the alignment yielding the maximum lexicalized probability for each translation rule. In addition to p(F |E)\nand pw(F |E), we use features in the reverse direction, namely p(E|F ) and pw(E|F ), the definitions of which are omitted as they are consistent with Equations 2 and 3, respectively. The probabilities associated with concept rules and glue rules are manually set to 0.0001."}, {"heading": "3.2 Reordering Model", "text": "Although the word order is defined for induced rules, it is not the case for glue rules. We learn a reordering model that helps to decide whether the translations of the nodes should be monotonic or inverse given the directed connecting edge label. The probabilistic model using smoothed counts is defined as:\np(M |h, l, t) = 1.0 + \u2211 h \u2211 t c(h, l, t,M)\n2.0 + \u2211 o\u2208{M,I} \u2211 h \u2211 t c(h, l, t, o) (4)\nc(h, l, t,M) is the count of monotonic translations of head h and tail t, connected by edge l."}, {"heading": "3.3 Moving Distance", "text": "The moving distance feature captures the distances between the subgraph roots of two consecutive rule matches in the decoding process, which controls a bias towards collapsing nearby subgraphs consecutively."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "We use the dataset of SemEval-2016 Task8, which contains 16833 training, 1368 dev and 1371 test instances. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 20.\nWe investigate the effectiveness of rules and features by ablation tests: \u201cNoInducedRule\u201d does not adopt induced rules, \u201cNoConceptRule\u201d does not adopt concept rules, \u201cNoMovingDistance\u201d does not adopt the moving distance feature, and \u201cNoReorderModel\u201d disables the reordering model. Given an AMR graph, if NoConceptRule\ncannot produce a legal derivation, we concatenate existing translation fragments into a final translation, and if a subgraph can not be translated, the empty string is used as the output. We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al., 2016), on the same dataset."}, {"heading": "4.2 Results", "text": "The results are shown in Table 2. First, All outperforms all baselines. NoInducedRule leads to the greatest performance drop compared with All, demonstrating that induced rules play a very important role in our system. On the other hand, NoConceptRule does not lead to much performance drop. This observation is consistent with the observation of Song et al. (2016) for their TSP-based system. NoMovingDistance leads to a significant performance drop, empirically verifying the fact that the translations of nearby subgraphs are also close. Finally, NoReorderingModel does not affect the performance significantly, which can be because the most important reordering patterns are already covered by the hierarchical induced rules. Compared with TSP-gen and JAMR-gen, our final model All improves the BLEU from 22.44 and 23.00 to 25.62, showing the advantage of our model. To our knowledge, this is the best result reported so far on the task."}, {"heading": "5 Conclusion", "text": "We showed that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG in the training time, and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time. Our method outperforms the previous state-of-the-art, empirically proving the advantages of our graph-to-string rules."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Artzi et al.2015] Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Artzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Cai", "Knight2013] Shu Cai", "Kevin Knight"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Node replacement graph grammars", "author": ["Engelfriet", "Rozenberg1997] J. Engelfriet", "G. Rozenberg"], "venue": "Handbook of Graph Grammars and Computing by Graph Transformation,", "citeRegEx": "Engelfriet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Engelfriet et al\\.", "year": 1997}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Flanigan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Generation from abstract meaning representation using tree transducers", "author": ["Chris Dyer", "Noah A. Smith", "Jaime Carbonell"], "venue": "In Proceedings of the 2016 Meeting of the North American chapter of the Association", "citeRegEx": "Flanigan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "Loosely tree-based alignment for machine translation", "author": ["Daniel Gildea"], "venue": "In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics", "citeRegEx": "Gildea.,? \\Q2003\\E", "shortCiteRegEx": "Gildea.", "year": 2003}, {"title": "Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing", "author": ["Andreas Vlachos", "Jason Naradowsky"], "venue": "In Proceedings of the 54th Annual Meeting of the Associa-", "citeRegEx": "Goodman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2016}, {"title": "Graph parsing with s-graph grammars", "author": ["Alexander Koller", "Christoph Teichmann"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Groschwitz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Groschwitz et al\\.", "year": 2015}, {"title": "Statistical syntax-directed translation with extended domain of locality", "author": ["Huang et al.2006] Liang Huang", "Kevin Knight", "Aravind Joshi"], "venue": "In Proceedings of Association for Machine Translation in the Americas", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Semantics-based machine translation with hyperedge replacement grammars", "author": ["Jones et al.2012] Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight"], "venue": "In Proceedings of the International Conference on Computational", "citeRegEx": "Jones et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Improving event detection with abstract meaning representation", "author": ["Li et al.2015] Xiang Li", "Thien Huu Nguyen", "Kai Cao", "Ralph Grishman"], "venue": "In Proceedings of the First Workshop on Computing News Storylines,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Liu et al.2006] Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning", "author": ["Mitra", "Baral2015] Arindam Mitra", "Chitta Baral"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI-", "citeRegEx": "Mitra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "author": ["Peng et al.2015] Xiaochang Peng", "Linfeng Song", "Daniel Gildea"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural Language Learning", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Generating english from abstract meaning representations", "author": ["Kevin Knight", "Ulf Hermjakob"], "venue": "In International Conference on Natural Language Generation", "citeRegEx": "Pourdamghani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pourdamghani et al\\.", "year": 2016}, {"title": "Parsing English into abstract meaning representation using syntax-based machine translation", "author": ["Pust et al.2015] Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May"], "venue": "In Conference on Empirical Methods in Natural Language", "citeRegEx": "Pust et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Amr-to-text generation as a traveling salesman problem", "author": ["Song et al.2016] Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Neural headline generation on abstract meaning representation", "author": ["Takase et al.2016] Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Takase et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takase et al\\.", "year": 2016}, {"title": "A discriminative model for semantics-to-string translation", "author": ["Chris Quirk", "Michel Galley"], "venue": "In Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation", "citeRegEx": "Tamchyna et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tamchyna et al\\.", "year": 2015}, {"title": "An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus", "author": ["Arul Menezes", "Chris Quirk"], "venue": "In Proceedings of the 2015 Meeting of the North American chapter", "citeRegEx": "Vanderwende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2015}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Wang et al.2015] Chuan Wang", "Nianwen Xue", "Sameer Pradhan"], "venue": "In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["Dekai Wu"], "venue": "Computational linguistics,", "citeRegEx": "Wu.,? \\Q1997\\E", "shortCiteRegEx": "Wu.", "year": 1997}, {"title": "A decoder for syntax-based statistical mt", "author": ["Yamada", "Knight2002] Kenji Yamada", "Kevin Knight"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yamada et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2002}, {"title": "Amr parsing with an incremental joint model", "author": ["Zhou et al.2016] Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", "startOffset": 38, "endOffset": 62}, {"referenceID": 11, "context": "Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al.", "startOffset": 113, "endOffset": 156}, {"referenceID": 23, "context": "Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al.", "startOffset": 113, "endOffset": 156}, {"referenceID": 22, "context": ", 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 13, "context": ", 2016) and event detection (Li et al., 2015).", "startOffset": 28, "endOffset": 45}, {"referenceID": 5, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 25, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 18, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 24, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 20, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 0, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 9, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 8, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 28, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 6, "context": ", 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", "startOffset": 62, "endOffset": 131}, {"referenceID": 21, "context": ", 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", "startOffset": 62, "endOffset": 131}, {"referenceID": 19, "context": ", 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", "startOffset": 62, "endOffset": 131}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it #X1#", "startOffset": 8, "endOffset": 242}, {"referenceID": 21, "context": "Song et al. (2016) directly generate sentences using graph-fragment-to-string rules.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "Our system makes use of a loglinear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding.", "startOffset": 86, "endOffset": 97}, {"referenceID": 26, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 7, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 3, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 10, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 14, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 18, "context": "step is to extract a set of initial rules from training \u3008sentence, AMR\u3009 pairs (Line 2) using the phraseto-graph fragment extraction algorithm of Peng et al. (2015) (Line 3).", "startOffset": 145, "endOffset": 164}, {"referenceID": 18, "context": "step is to extract a set of initial rules from training \u3008sentence, AMR\u3009 pairs (Line 2) using the phraseto-graph fragment extraction algorithm of Peng et al. (2015) (Line 3). Here an initial rule contains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj according to Cai and Knight (2013), and generate rij by collapsing ri with rj , if ri contains rj 2 (Line 6-8).", "startOffset": 145, "endOffset": 344}, {"referenceID": 21, "context": "In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 3, "context": "Inspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied.", "startOffset": 12, "endOffset": 26}, {"referenceID": 12, "context": "The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).", "startOffset": 84, "endOffset": 118}, {"referenceID": 3, "context": "The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).", "startOffset": 84, "endOffset": 118}, {"referenceID": 17, "context": "We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.", "startOffset": 77, "endOffset": 100}, {"referenceID": 16, "context": "MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 20.", "startOffset": 13, "endOffset": 24}, {"referenceID": 6, "context": "We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 21, "context": ", 2016) and TSP-gen (Song et al., 2016), on the same dataset.", "startOffset": 20, "endOffset": 39}, {"referenceID": 21, "context": "This observation is consistent with the observation of Song et al. (2016) for their TSP-based system.", "startOffset": 55, "endOffset": 74}], "year": 2017, "abstractText": "This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.", "creator": "LaTeX with hyperref package"}}}