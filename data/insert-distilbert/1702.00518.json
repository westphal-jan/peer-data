{"id": "1702.00518", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Recovering True Classifier Performance in Positive-Unlabeled Learning", "abstract": "a common smoothing approach in positive - unlabeled learning is to train a classification model between labeled and unlabeled data. this strategy is in fact known to give an optimal classifier under mild control conditions ; however, it results in biased empirical reconstruction estimates of the classifier performance. in this work, we show that twice the typically used performance measures such as making the receiver linear operating characteristic curve, or the precision - recall curve obtained on such data and can be corrected with the knowledge of class priors ; i. e., the proportions of the positive and negative examples in the unlabeled data. we extend the results to a noisy setting where some of then the examples labeled positive occurrences are in fact negative and show that the correction also requires the knowledge of the proportion of noisy examples in the labeled positives. using state - of - the - art algorithms to estimate the positive class prior and the proportion of noise, we experimentally evaluate two correction approaches and demonstrate their efficacy greatly on real - life data.", "histories": [["v1", "Thu, 2 Feb 2017 01:22:18 GMT  (602kb,D)", "http://arxiv.org/abs/1702.00518v1", "Full paper with supplement"]], "COMMENTS": "Full paper with supplement", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shantanu jain", "martha white", "predrag radivojac"], "accepted": true, "id": "1702.00518"}, "pdf": {"name": "1702.00518.pdf", "metadata": {"source": "CRF", "title": "Recovering True Classifier Performance in Positive-Unlabeled Learning", "authors": ["Shantanu Jain", "Martha White", "Predrag Radivojac"], "emails": ["predrag}@indiana.edu"], "sections": [{"heading": "Introduction", "text": "Performance estimation in binary classification is tightly related to the nature of the classification task. As a result, different performance measures may be directly optimized during training. When (mis)classification costs are available, the classifier is ideally trained and evaluated in a costsensitive mode to minimize the expected cost (Whalen 1971; Elkan 2001). More often, however, classification costs are unknown and the overall performance is assessed by averaging the performance over a range of classification modes. The most extensively studied and widely used performance evaluation in binary classification involves estimating the Receiver Operating Characteristic (ROC) curve that plots the true positive rate of a classifier as a function of its false positive rate (Fawcett 2006). The ROC curve provides insight into trade-offs between the classifier\u2019s accuracies on positive versus negative examples over a range of decision thresholds. Furthermore, the area under the ROC curve (AUC) has a meaningful probabilistic interpretation that correlates with the ability of the classifier to separate classes and is often used to rank classifiers (Hanley and McNeil 1982). Another important performance criterion generally used in information retrieval relies on the precision-recall\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n(pr-rc) curve, a plot of precision as a function of recall. The precision-recall evaluation, including summary statistics derived from the pr-rc curve, may be preferred to ROC curves when classes are heavily skewed (Davis and Goadrich 2006).\nAlthough model learning and performance evaluation in a supervised setting are well understood (Hastie et al. 2001), the availability of unlabeled data gives additional options and also presents new challenges. A typical semi-supervised scenario involves the availability of positive, negative and (large quantities of) unlabeled data. Here, the unlabeled data can be used to improve training (Blum and Mitchell 1998) or unbias the labeled data (Cortes et al. 2008); e.g., to estimate class proportions that are necessary to calibrate the model and accurately estimate precision when class balances (but not class-conditional distributions) in labeled data are not representative (Saerens et al. 2002). This is often the case when it is more expensive or difficult to label examples of one class than the examples of the other. A special case of the semi-supervised setting arises when the examples of only one class are labeled. It includes open-world domains such as molecular biology where, for example, wet lab experiments determining a protein\u2019s activity are generally conclusive; however, the absence of evidence about a protein\u2019s function cannot be interpreted as the evidence of absence. This is because, even when the labeling is attempted, a functional assay may not lead to the desired activity for a number of experimental reasons. In other domains, such as social networks, only positive examples can be collected (such as \u2018liking\u2019 a particular product) because, by design, the negative labeling is not allowed. The development of classification models in this setting is often referred to as positiveunlabeled learning (Denis et al. 2005).\nState-of-the-art techniques in positive-unlabeled learning tackle this problem by treating the unlabeled sample as negatives and training a classifier to distinguish between labeled (positive) and unlabeled examples. Following Elkan and Noto (2008), we refer to the classifiers trained on a labeled sample from the true distribution of inputs, containing both positive and negative examples, as traditional classifiers. Similarly, we refer to the classifiers trained on the labeled versus unlabeled data as non-traditional classifiers. In theory, the true performance of both traditional and non-traditional classifiers can be evaluated on a labeled sam-\nar X\niv :1\n70 2.\n00 51\n8v 1\n[ st\nat .M\nL ]\n2 F\neb 2\n01 7\nple from the true distribution (traditional evaluation). However, this is infeasible for non-traditional learners because such a sample is not available in positive-unlabeled learning. As a result, the non-traditional classifiers are evaluated by using the unlabeled sample as substitute for labeled negatives (non-traditional evaluation). Surprisingly, for a variety of performance criteria, non-traditional classifiers achieve similar performance under traditional evaluation as optimal traditional classifiers (Blanchard et al. 2010; Menon et al. 2015). The intuition for these results comes from the fact that in many practical situations, the posterior distributions in traditional and non-traditional setting provide the same optimal ranking of data points on a given test sample (Jain et al. 2016; Jain, White, and Radivojac 2016). Furthermore, the widely-accepted evaluation approaches using ROC or pr-rc curves are insensitive to the variation of raw prediction scores unless they affect the ranking.\nThough the efficacy of non-traditional classifiers has been thoroughly studied (Peng et al. 2003; Elkan and Noto 2008; Ward et al. 2009; Menon et al. 2015), estimating their true performance has been much less explored. Such performance estimation often involves computing the fraction(s) of correctly and incorrectly classified examples from both classes; however, in absence of labeled negatives, the fractions computed under the non-traditional evaluation are incorrect, resulting in biased estimates. Figure 1 illustrates the effect of this bias by showing the traditional and nontraditional ROC curves on a handmade data set. Because some of the unlabeled examples in the training set are in fact positive, the area under the ROC curve estimated when the unlabeled examples were considered negative (nontraditional setting) underestimates the true performance for positive versus negative classification (traditional setting).\nThis paper formalizes and evaluates performance estimation of a non-traditional classifier in the traditional setting when the only available training data are (possibly noisy) positive examples and unlabeled data. We show that the true (traditional) performance of such a classifier can be recovered with the knowledge of class priors and the fraction of mislabeled examples in the positive set. We derive formulas for converting the ROC and pr-rc curves from the nontraditional to the traditional setting. Using these recovery formulas, we present methods to estimate true classification performance. Our experiments provide evidence that the methods for the recovery of a classifier\u2019s performance are sound and effective."}, {"heading": "Problem formulation", "text": "Consider a binary classification problem from input x \u2208 X to output y \u2208 Y = {0, 1} in a positive-unlabeled setting. Let f be the true distribution over the input space X from which the unlabeled sample is drawn and let f1 and f0 be the distributions of the positive and negative examples, respectively. It follows that f can be expressed as a two-component mixture containing f1 and f0 as\nf(x) = \u03b1f1(x) + (1\u2212 \u03b1)f0(x), for all x \u2208 X where \u03b1 \u2208 [0, 1) is the mixing proportion (positive class prior) giving the proportion of positives in f .\nB. Positive vs. unlabeled"}, {"heading": "A. Data set: prediction scores and class labels", "text": "C. Positive vs. negative\n0.986 yes, as 1 0.943 no 0.863 yes, as 1 0.789 no\n0.009 no\n0.699 yes, as 1 0.473 no 0.211 no\n1 1 1 0 1 0 0 0\nPrediction Labeled True class label\n*\nLet now g be the distribution over X from which the labeled sample is drawn. We similarly express g as a twocomponent mixture containing f1 and f0 as\ng(x) = \u03b2f1(x) + (1\u2212 \u03b2)f0(x), for all x \u2208 X where \u03b2 \u2208 (\u03b1, 1] gives the proportion of positives in labeled data. All labeled examples are labeled as positives; thus, when \u03b2 = 1 we say that the labeled data is clean. When \u03b2 < 1, the labeled data contains a fraction (1\u2212\u03b2) of negatives that are in this case mislabeled. We will refer to the latter scenario as the noisy positive setting.\nLet X1 be the (positively) labeled sample drawn according to g(x) and X be the unlabeled sample drawn according to f(x). The learning objective is to train a classifier that discriminates between positive and negative data and estimate its performance. However, we can only train a nontraditional classifier h : X \u2192 Y between labeled and unlabeled data and estimate its performance by considering that all labeled data are positive and all unlabeled data are negative. We refer to the performance of h(x) directly estimated from samples X1 and X as perf pu. Given a non-traditional classifier h(x) and its performance perf pu, the main goal\nof this work is to estimate (recover) its performance in the traditional setting; i.e., its performance as a discriminator between positive and negative data."}, {"heading": "Methods", "text": "We consider a family of binary classifiers that map X into Y . To simplify the presentation, we can think of the entire family as generated from a single model that mapsX into R, where each individual classifier corresponds to a decision threshold picked from R. The classifier gives the positive class \u20181\u2019 when the model\u2019s output is above the threshold and the negative class \u20180\u2019 otherwise.\nThe true positive rate (sensitivity, recall) of each classifier is defined as the probability of correctly predicting a positive example; the true negative rate (specificity) is defined as the probability of correctly predicting a negative example; the false positive rate is defined as 1 \u2212 specificity, and the false negative rate is defined as 1 \u2212 sensitivity. Finally, the precision is defined as the probability that a positive prediction is correct; conversely, the false discovery rate is defined as 1 \u2212 precision (Hastie et al. 2001). Given a test set, each of the quantities above is estimated using relative frequencies. In this setup, each classifier corresponds to a single confusion matrix, whereas the entire family of classifiers corresponds to a particular ROC curve and a particular pr-rc curve (Fawcett 2006). The two main performance criteria considered in this work are the area under the ROC curve and the area under the pr-rc curve.\nThe case of clean positive data We first consider the setting of clean positive data, where the labeled data does not incorrectly contain negatives (\u03b2 = 1), to provide intuition before moving to the more general noisy-positive setting. For a classifier h : X \u2192 Y , the true positive rate, \u03b3, and false positive rate, \u03b7, can be defined as\n\u03b3 = Ef1 [h(x)] \u03b7 = Ef0 [h(x)],\nwhere Ef denotes expectation with respect to a distribution f . The goal is to estimate these values, despite the fact that we only have access to positive labels.\nThe true positive rate can be estimated as the empirical mean of h(x) over the positively labeled sample X1\n\u03b3\u0302 = 1 |X1| \u2211 x\u2208X1 h(x)\nbecause X1 was sampled from f1. The false positive rate, however, cannot be so simply estimated, because we do not have access to a sample from f0. Further, this prevents the estimation of the ROC curve and the area under this curve (AUC). Typically, ROC curves and AUCs are reported based only on the performance of the non-traditional positive-unlabeled classifier, h, on discriminating between positives and unlabeled data. The ROC curve for the positive versus unlabeled classification, ROCpu, can be estimated by plotting \u03b3\u0302 against \u03b7\u0302pu across different classifiers, where \u03b7\u0302pu, an estimate of \u03b7pu = Ef [h(x)], can be estimated using the\nunlabeled sample (which corresponds to the negative sample for the non-traditional positive-unlabeled classifier h):\n\u03b7\u0302pu = 1 |X| \u2211 x\u2208X h(x).\nThis curve, however, does not represent the true performance of h for positive versus negative classification. Similar difficulties exist in estimating the precision\n\u03c1 = \u03b1Ef1 [h(x)] Ef [h(x)]\nthat requires the positive class prior \u03b1, though recall, which is equal to \u03b3, can be directly estimated.\nOf key interest, therefore, is a correction approach that provides an estimate of the true performance. We provide just such a result in Theorem 1 below for the more general setting of noisy positives (see next Section). Using this theorem for \u03b2 = 1, for example, we can express the false positive rate \u03b7 in terms of the positive-unlabeled false positive rate, \u03b7pu as1\n\u03b7 = \u03b7pu \u2212 \u03b1\u03b3 1\u2212 \u03b1 ,\nand the AUC of the classifier on the positive-negative classification problem in terms of the AUC of the classifier on the positive-unlabeled classification problem2:\nAUC = AUCpu \u2212 \u03b12\n1\u2212 \u03b1 .\nTherefore, given estimates of \u03b1, \u03b3, \u03b7pu and AUCpu, we can obtain estimates of AUC and the precision. In the next Section, we present this key result that enables this conversion and also shows that the estimated AUC is better than AUCpu.\nThe case of (possibly) noisy positive data In this section we consider a more general case where the labeled sample of positives is allowed to be noisy; i.e., some positives may actually be negatives. Since this setting is a strict generalization of the previous discussion, we will overload terminology and use \u03b7pu again as the positive-unlabeled false positive rate.\nIn addition to previous difficulties, we now also cannot estimate the true positive rate \u03b3, because we do not have access to an unbiased sample from f1; rather, we only have access to a sample contaminated with negatives. Nonetheless, we can express all of the desired rates in terms of only rates for the non-traditional classifier. Theorem 1. For a given classifier h : X \u2192 Y , the true positive rate \u03b3 and the false positive rate \u03b7 can be expressed in terms of the positive-unlabeled \u03b3pu and \u03b7pu\n\u03b3 = (1\u2212 \u03b1)\u03b3pu \u2212 (1\u2212 \u03b2)\u03b7pu\n\u03b2 \u2212 \u03b1 (1)\n\u03b7 = \u03b2\u03b7pu \u2212 \u03b1\u03b3pu\n\u03b2 \u2212 \u03b1 . (2)\n1Iakoucheva et al. (2004) also provide this result for uncorrupted positive data.\n2Menon et al. (2015) provide an equivalent formula for the AUC. In Theorem 1, we give a full derivation from the probabilistic definition of the AUC and conversion formulas for other measures.\nThe precision \u03c1 can either be converted from a positiveunlabeled precision \u03c1pu, with c = |X1|/(|X|+|X1|), as\n\u03c1 = \u03b1(1\u2212 \u03b1) \u03b2 \u2212 \u03b1 ( 1\u2212 c c ( \u03c1pu 1\u2212 \u03c1pu ) \u2212 1\u2212 \u03b2 1\u2212 \u03b1 ) or computed directly as\n\u03c1 = \u03b1\u03b3\n\u03b7pu . (3)\nFurther, consider a family of classifiers F = {h\u03b7} indexed by \u03b7 \u2208 [0, 1] where \u03b7 is the false positive rate of h\u03b7 . Then for the ROC curve obtained from varying \u03b7, the AUC can be expressed in terms of the positive-unlabeled AUCpu as\nAUC = AUCpu \u2212 1\u2212(\u03b2\u2212\u03b1)2\n\u03b2 \u2212 \u03b1 . (4)\nMoreover, AUC > AUCpu, if and only if AUCpu > 1/2 and \u03b2 \u2212 \u03b1 < 1."}, {"heading": "Proof.", "text": "\u03b7pu = Ef [h(x)] = \u03b1Ef1 [h(x)] + (1\u2212 \u03b1)Ef0 [h(x)] = \u03b1\u03b3 + (1\u2212 \u03b1)\u03b7.\nSimilarly, we can obtain the true positive rate\n\u03b3pu = Eg[h(x)] = \u03b2Ef1 [h(x)] + (1\u2212 \u03b2)Ef0 [h(x)] = \u03b2\u03b3 + (1\u2212 \u03b2)\u03b7.\nWe can then solve for \u03b7 and \u03b3 to get the result. Next, we consider the precision. We can directly reexpress the precision as\n\u03c1 = \u03b1Ef1 [h(x)] Ef [h(x)] = \u03b1\u03b3 \u03b7pu .\nTo obtain a conversion from \u03c1pu, first consider\n\u03c1pu = cEg[h(x)]\nEcg+(1\u2212c)f [h(x)]\n= cEg[h(x)]\ncEg[h(x)] + (1\u2212 c)Ef [h(x)]\n= 1\n1 + 1\u2212cc Ef [h(x)] Eg [h(x)]\nWe can express a component of this as\nEg[h(x)] Ef [h(x)] = \u03b2Ef1 [h(x)] + (1\u2212 \u03b2)Ef0 [h(x)] Ef [h(x)]\n= \u03b2\n\u03b1 \u03b1Ef1 [h(x)] Ef [h(x)] + 1\u2212 \u03b2 1\u2212 \u03b1 (1\u2212 \u03b1)Ef0 [h(x)] Ef [h(x)]\n= \u03b2\n\u03b1 \u03c1+ 1\u2212 \u03b2 1\u2212 \u03b1 (1\u2212 \u03c1)\n= \u03b2 \u2212 \u03b1 \u03b1(1\u2212 \u03b1) \u03c1+ 1\u2212 \u03b2 1\u2212 \u03b1\nwhere rearranging gives the result. Next, we derive an equation that allows estimation of the AUC directly from the AUCpu, \u03b1 and \u03b2. Consider a family of classifiers F = {h\u03b7} indexed by \u03b7 \u2208 [0, 1] where \u03b7 is the false positive rate of h\u03b7 . We can express the \u03b3, \u03b7pu, \u03b3pu of h\u03b7 as a function of \u03b7 as follows:\n\u03b3(\u03b7) = Ef1 [h\u03b7(x)], \u03b7pu(\u03b7) = Ef [h\u03b7(x)]\n= \u03b1\u03b3(\u03b7) + (1\u2212 \u03b1)\u03b7, \u03b3pu(\u03b7) = Eg[h\u03b7(x)]\n= \u03b2\u03b3(\u03b7) + (1\u2212 \u03b2)\u03b7. By definition, the expression for AUCpu is\nAUCpu= \u222b 1 0 \u03b3pu(\u03b7) d\u03b7pu(\u03b7) d\u03b7 d\u03b7\n= \u222b 1 0 (\u03b2\u03b3(\u03b7) + (1\u2212 \u03b2)\u03b7) ( \u03b1 d\u03b3(\u03b7) d\u03b7 + (1\u2212 \u03b1) ) d\u03b7\n= \u03b1\u03b2 \u222b 1 0 \u03b3(\u03b7) d\u03b3(\u03b7) d\u03b7 d\u03b7 + (1\u2212 \u03b1)\u03b2 \u222b 1 0 \u03b3(\u03b7)d\u03b7\n+ \u03b1(1\u2212 \u03b2) \u222b 1 0 \u03b7 d\u03b3(\u03b7) d\u03b7 d\u03b7 + (1\u2212 \u03b1)(1\u2212 \u03b2) \u222b 1 0 \u03b7d\u03b7\nNow solving for each integral, we obtain\nAUCpu = \u03b1\u03b2\n2 [\u03b32(1)\u2212 \u03b32(0)] + (1\u2212 \u03b1)\u03b2AUC + \u03b1(1\u2212 \u03b2) [ [\u03b7\u03b3(\u03b7)]\n1 0 \u2212 \u222b 1 0 \u03b3(\u03b7)d\u03b7 ] +\n(1\u2212 \u03b1)(1\u2212 \u03b2) 2 [12 \u2212 02]\n= \u03b1\u03b2 + 2\u03b1(1\u2212 \u03b2) + (1\u2212 \u03b1)(1\u2212 \u03b2)\n2 + [(1\u2212 \u03b1)\u03b2 \u2212 \u03b1(1\u2212 \u03b2)]AUC\n= 1\u2212 (\u03b2 \u2212 \u03b1)\n2 + (\u03b2 \u2212 \u03b1)AUC\nRearranging the terms gives the desired result. Finally, from Equation 4, we see that\nAUC\u2212 AUCpu = 1\u2212 (\u03b2 \u2212 \u03b1) \u03b2 \u2212 \u03b1\n( AUCpu \u2212 1\n2 ) proving AUC > AUCpu, if and only if AUCpu > 1/2 and \u03b2 \u2212 \u03b1 < 1."}, {"heading": "Experiments and results", "text": "Data sets and classification models Our estimators were evaluated using twelve real-life data sets from the UCI Machine Learning Repository (Lichman 2013). All data sets were appropriately modified for binary classification; e.g., regression problems were converted into classification problems based on the mean of the target variable, whereas multiclass classification problems were converted into binary problems by combining classes. When\nneeded, categorical features were converted into numerical features based on the sparse binary representation.\nClassifiers were constructed as ensembles of 100 feedforward neural networks (Breiman 1996). Each network had five hidden neurons and was trained using resilient propagation (Riedmiller and Braun 1993). A validation set containing 25% of the training data was used to terminate training. For simplicity, no training parameters were varied. Accuracies were estimated using the out-of-bag approach."}, {"heading": "Experimental protocols", "text": "To evaluate the quality of performance estimation we first established the ground truth performance of a model by estimating accuracy in a standard supervised setting. All positive examples in all data sets were considered positive and all negative examples were considered negative. A model was then constructed and evaluated for its performance.\nWe next simulated the positive-unlabeled setting where we randomly included 1,000 examples (or 100 for smaller data sets) in the positive data set X1. The number of actual positive examples in each labeled set was a function of parameter \u03b2 \u2208 {1, 0.95, 0.75}. For example, when \u03b2 = 1, all positively labeled examples were positive, and when \u03b2 < 1, an appropriate fraction of the (positively) labeled data setX1 was filled with negatives. The remaining examples (positive and negative) were declared unlabeled (data setX). The size of the unlabeled data was limited to 10,000 (where relevant) and the fraction of positives in the unlabeled data was used as true \u03b1. Using all positively labeled examples as positives and all unlabeled examples as negatives, we then estimated the performance of the model in the positive-unlabeled setting. All experiments were repeated fifty times by randomly selecting positives and negatives for the labeled data.\nWe used our methodology from the previous Section to recover the true accuracy of a model. To recover the area under the ROC curve, we used the direct conversion (D) from Equation 4 as well as indirect conversion (I) where traditional true positive and false positive rates were recovered using Equations 1-2 for every threshold and then used to reconstruct the ROC curve. In the case of recovering the pr-rc curve, only the indirect conversion was used (using Equations 1 and 3) as no direct conversion formula is known to us. The full algorithm for the indirect recovery is given in the arXiv supplement of this paper.\nAll experiments were carried out (i) by assuming that the class prior \u03b1 and noise fraction \u03b2 were known (R), and (ii) by estimating \u03b1 and \u03b2 from positive and unlabeled data (E). These experiments were carried out to quantify the performance loss due to the inability to perfectly estimate (\u03b1, \u03b2). Class priors and noise fraction were estimated using the AlphaMax algorithm (Jain et al. 2016; Jain, White, and Radivojac 2016). Several recent studies have determined good performance of AlphaMax (Jain et al. 2016; Jain, White, and Radivojac 2016; Ramaswamy et al. 2016), in both clean and noisy setting.\nThe direct recovery methods using real and estimated (\u03b1, \u03b2) are hereafter referred to as DR and DE methods, respectively, whereas the indirect recovery methods are similarly referred to as IR and IE methods. All four approaches\nwere used to evaluate the estimated AUCs and only IR and IE methods were used to evaluate the estimated area under the pr-rc curve (AUC-PR)."}, {"heading": "Results", "text": "Figure 2 shows the general trends in estimating AUC and AUC-PR over all data sets. Detailed dataset-specific evaluations over all summary statistics are given in Tables 1-2, while the error between the true and recovered performance is further characterized in Figures 3-4. Tables 1-2 and Figures 3-4 are shown in the arXiv supplement of this paper.\nFigure 2(a) shows that, as expected, AUCpu consistently underestimates the true performance. Moreover, it deteriorates with increase in noise. On the other hand, using the correct values for \u03b1 and \u03b2 (IR and DR, corresponding to the yellow and green boxes) leads to excellent performance over all values of \u03b2. Replacing the true (\u03b1, \u03b2) by their estimates obtained from AlphaMax did not lead to significantly different performance estimates (IE and DE, corresponding to the blue and purple boxes). Since class prior estimation guarantees identifiability of only the upper bounds of (\u03b1, \u03b2), the observed differences are reasonable. Although the aggregate performance of direct and indirect estimation is similar, a detailed comparison between these methods (DR vs. IR and DE vs. IE) provides evidence that the indirect method was superior in both cases (P = 6.5 \u00b7 10\u22126 for real \u03b1 and \u03b2 and P = 5.7 \u00b7 10\u22123 for estimated \u03b1 and \u03b2; one-sided binomial test). Full details of these comparisons are shown in the arXiv supplement.\nFigure 2(b) shows that the performance breaks down with increase in the absolute error of estimates of \u03b2 \u2212 \u03b1. We selected this criterion because the term \u03b2 \u2212 \u03b1 appears in the denominator of Equation 4 and thus could significantly influence the quality of performance. The increase in error more strongly affects the estimators with approximate (\u03b1, \u03b2). Interestingly, the estimators IR and DR both underestimate, and IE and DE both overestimate. We note that in some cases the data sets obtained from UCI Machine Learning Repository may not be perfectly labeled in the first place. This suggests that our ground truth performance might be slightly biased for some data sets which would lead to a situation that the estimated performance is in fact more accurate than observed.\nFigures 2(c) and 2(d) show the equivalent plots for AUCPR from which similar conclusions can be drawn. However, errors in the uncorrected AUC-PR estimates (red boxes) are much higher in comparison. Estimating AUC-PR is therefore not particularly meaningful in the non-traditional setting because precision is sensitive to the proportion of labeled positives in the data set; i.e., |X1|/(|X|+|X1|), whereas \u03b3 and \u03b7 are not."}, {"heading": "Related work", "text": ""}, {"heading": "Evaluation metrics", "text": "Two-dimensional performance characterization such as ROC or pr-rc curves and the summary statistics based on them have become mainstream in empirical evaluation of classification performance (Flach 2003; Fawcett 2006;\nDavis and Goadrich 2006; Boyd et al. 2012; Clark and Radivojac 2013; Flach and Kull 2015). Of particular interest to our work is the well-explored relationship between these performance metrics and class priors. For example, Herna\u0301ndez-Orallo et al. (2012) use class priors and area under the ROC curve to compute the expected classification accuracy, whereas Boyd et al. (2012) relate class priors to the size of the unachievable region in pr-rc space. In the domain of positive-unlabeled learning, Menon et al. (2015) give the relationship between traditionally and non-traditionally evaluated balanced error rates and AUCs of a given classifier. They use this relationship to demonstrate that constructing a non-traditional classifier by optimizing non-traditional AUC results in an optimal traditional AUC. Claesen et al. (2015) similarly argue the importance of class priors and show how to compute bounds on the true ROC or pr-rc curves. In contrast, our approach directly estimates the unknown statistics and derives a closed-form conversion formula for recovering the area under the ROC curve from the first principles. Another similar work, although in the area of structured-output learning, is by Jiang et al. (2014) who studied the impact of sequential completion of the (structured) target variable; however, their work makes fewer assumptions on the data distributions and does not lead to the recovery of true performance.\nClass prior and noise estimation Though class prior (\u03b1) estimation in positive-unlabeled learning is nontrivial, several algorithms have recently emerged in the literature. Elkan and Noto (2008) estimate the priors from the probability obtained by calibrating the scores of a non-traditional classifier under strong assumptions that the class-conditional distributions do not overlap. The same assumptions are used by (du Plessis and Sugiyama 2014) who estimate the class prior as the minimizer of the Pearson divergence. du Plessis et al. (2015) improve the method by using penalized f -divergence to allow overlap. Blanchard et al. (2010) and Jain et al. (2016) showed that class prior estimation, in general, is an ill-posed problem and introduce an \u201cirreducibility\u201c constraint on the distribution of the negatives that makes the problem well defined. Blanchard et al. (2010) estimate the class prior as the slope of the right endpoint of the empirical ROC curve from non-\ntraditional classifiers while Sanderson and Scott (2014) use a fitted curve instead of the actual ROC curve to smooth large noise at endpoints. Loosely speaking, the ROC approach is based on the fact that the class prior under the irreducibility assumption is the minimum value attained by the ratio of the unlabeled and positive sample densities (Jain et al. 2016). Jain et al. (2016) also give an algorithm, AlphaMax, a nonparametric maximum likelihood based approach suitable for high-dimensional data. Ramaswamy et al. (2016) give an algorithm based on embedding distributions into a reproducing kernel Hilbert spaces.\nIn the case of noisy positives, Scott et al. (2013) and Jain, White, and Radivojac (2016) impose a \u201cmutual irreducibility\u201d constraint on the distribution of positives and negatives, to make the class prior and the noise proportion estimation well defined. Jain, White, and Radivojac (2016) estimate \u03b1, \u03b2 by combining the outputs of two executions of AlphaMax, one of which flips the role of positive and unlabeled samples."}, {"heading": "Conclusions", "text": "In this paper we propose simple methods for correcting the estimated performance of classifiers trained in the positiveunlabeled setting. We prove a fundamental result about the relationship between widely-used performance measures and their positive-unlabeled counterparts. The resulting estimators were evaluated over a diverse group of data sets to show that it is feasible and practical to obtain accurate estimates of a classifier\u2019s performance in the task of discriminating positive and negative examples.\nThe corrected performance measures were uniformly more accurate than the positive-unlabeled estimates, which typically underestimated the performance. Furthermore, we showed that the indirect method for performance recovery outperformed the direct method. This notwithstanding, we do not recommend stopping the established practice of reporting perf pu; rather we propose that the corrected performance measures should also be provided. In domains where \u03b1 and \u03b2 are unknown, such estimates will contribute to a better understanding of a classifier\u2019s performance and a deeper understanding of the domain itself."}, {"heading": "Acknowledgements", "text": "We thank Prof. Michael W. Trosset, Kymerleigh A. Pagel and Vikas Pejaver for helpful comments. Grant support: NSF DBI-1458477, NIH R01MH105524, NIH R01GM103725, and the Indiana University Precision Health Initiative."}, {"heading": "Appendix", "text": "This appendix describes the indirect method for recovering the area under the ROC curve (AUC) and the area under the precision-recall curve (AUC-PR). Additional characterization of the quality of recovered AUCs is then provided over the entire range of estimated \u03b1 and \u03b2. Finally, full datasetspecific results are summarized, including statistical tests for comparing direct and indirect recovery methods."}, {"heading": "Indirect estimators", "text": "As mentioned before, we estimate \u03b3pu and \u03b7pu as empirical means of h(x) over X1 and X , respectively; i.e.,\n\u03b3\u0302pu = 1 |X1| \u2211 x\u2208X1 h(x),\n\u03b7\u0302pu = 1 |X| \u2211 x\u2208X h(x).\nWe then estimate (recover) \u03b7 and \u03b3 by replacing \u03b1, \u03b2, \u03b7pu and \u03b3pu with their estimates in Equations 1 and 2 as\n\u03b3\u0302 = (1\u2212 \u03b1\u0302)\u03b3\u0302pu \u2212 (1\u2212 \u03b2\u0302)\u03b7\u0302pu\n\u03b2\u0302 \u2212 \u03b1\u0302\n\u03b7\u0302 = \u03b2\u0302\u03b7\u0302pu \u2212 \u03b1\u0302\u03b3\u0302pu\n\u03b2\u0302 \u2212 \u03b1\u0302 .\n(5)\nTo recover the ROC curve we estimate \u03b7 and \u03b3 as in Equation 5 across different classifiers. However, one needs to be careful because \u03b7\u0302 and \u03b3\u0302 can take values inconsistent with theory; i.e., outside of the [0, 1] range. For example \u03b7\u0302 and \u03b3\u0302 can be negative; moreover, there is no guarantee on the monotonicity between \u03b7\u0302 and \u03b3\u0302. We provide an algorithm to correct \u03b3\u0302 versus \u03b7\u0302 curve in Algorithm 1.\nAlgorithm 1 Algorithm for recovering the ROC curve\nRequire: \u03b1\u0302 and \u03b2\u0302 and vectors ~\u03b7pu, ~\u03b3pu, where ith entry contains the estimate of \u03b7pu, \u03b3pu pair coming from the same classifier hi. Ensure: vectors ~\u03b7,~\u03b3, where ith entry contains the \u03b7, \u03b3 estimates for classifier hi. The curve ~\u03b7 versus ~\u03b3 satisfies the properties of an ROC curve; i.e., monotonicity and restriction to the region between (0,0) and (1,1) // Apply Equation 5 to get initial \u03b7, \u03b3 estimates.\n~\u03b7 \u2190 \u03b2\u0302~\u03b7 pu \u2212 \u03b1\u0302~\u03b3pu \u03b2\u0302 \u2212 \u03b1\u0302 , ~\u03b3 \u2190 (1\u2212 \u03b1\u0302)~\u03b3 pu \u2212 (1\u2212 \u03b2\u0302)~\u03b7pu \u03b2\u0302 \u2212 \u03b1\u0302 .\n// Remove indices for which \u03b7\u0302 or \u03b3\u0302 are outside [0, 1] // Sort ~\u03b7 in ascending order and reorder the entries in ~\u03b3 accordingly. // Make the curve non-decreasing by replacing the nonincreasing values of \u03b3\u0302 by the largest value to its left.\nFinally, we can consider two approaches to estimating (recovering) precision. Equation 3 suggests estimating the precision as\n\u03c1\u0302 = \u03b1\u0302\u03b3\u0302\n\u03b7\u0302pu .\nAlternatively, the equation can be expressed in terms of \u03c1pu as follows\n\u03c1\u0302 = \u03b1\u0302(1\u2212 \u03b1\u0302) \u03b2\u0302 \u2212 \u03b1\u0302 ( 1\u2212 c c ( \u03c1\u0302pu 1\u2212 \u03c1\u0302pu ) \u2212 1\u2212 \u03b2\u0302 1\u2212 \u03b1\u0302 ) ,\nwhere\n\u03c1\u0302pu = |X1|\u03b3\u0302pu\n|X1|\u03b3\u0302pu + |X|\u03b7\u0302pu .\nThese estimates are equivalent. The estimate in terms of \u03c1\u0302pu can be useful if the positive-unlabeled precision is already computed. In general, however, in the absence of this estimate, the more direct computation in Equation 3 is more desirable. Computing precision directly leads to an equivalent indirect algorithm for recovering the area under the pr-rc curve (AUC-PR)."}, {"heading": "Visualizing errors of AUC estimates", "text": "Figure 3 shows the absolute difference between the true and recovered areas under the ROC curve as a function of \u03b1\u0302 and \u03b2\u0302 for several combinations of (\u03b1, \u03b2,AUC). Specifically, we first selected the true values (\u03b1, \u03b2,AUC), from which we calculated AUCpu using Equation 4. For this value of AUCpu, we then varied the estimates \u03b1\u0302 and \u03b2\u0302 in [0, 1] and used Equation 4 again to compute the recovered area under the ROC curve, AUCest.\nThe plots suggest that the feasible region for (\u03b1\u0302, \u03b2\u0302) pairs contains the upper left-hand triangle and lower right-hand triangle. However, since the values in the lower right region lead to the AUCest \u2264 0.5, this part of the (\u03b1\u0302, \u03b2\u0302) space is not of interest. The middle region corresponds to the estimated AUC values above 1 or below 0 and, therefore, is referred to as infeasible region (Figure 4). In our experiments, the estimated AUCs in this region are simply set to 1 or 0, but also suggest problems in the analysis; e.g., that the assumptions may not hold. The size of the infeasible region varies with the true values of (\u03b1, \u03b2,AUC), with generally larger values of AUC leading to larger infeasible regions.\nFigure 4 summarizes all panels from Figure 3, where each line of interest is characterized as a function of true values of \u03b1 and \u03b2. When \u03b1\u0302 = 0 and \u03b2\u0302 = 1 (upper left-hand corner), the estimated value of AUC equals AUCpu. The AUC estimate is correct whenever \u03b2 \u2212 \u03b1 is accurately estimated; i.e., anywhere on the 45\u25e6 line\n\u03b2\u0302 = \u03b1\u0302+ \u03b2 \u2212 \u03b1.\nThe remaining regions of interest are shown in Figure 4."}, {"heading": "Dataset-specific results", "text": "The full results on individual UCI data sets are provided in Table 1 and Table 2.\nThe AUC comparisons between the direct and indirect method was conducted using the counting tests. Each combination (data set, \u03b1, \u03b2) was considered to be an independent experiment and the number of wins vs. losses were counted for each algorithm; in case of ties, the wins were distributed in an alternating manner, starting with the direct method, then indirect, and so on. Finally, statistical significance was\ntested by using a one-sided binomial test where the null hypothesis (H0) was that the two algorithms have equal performance and the alternative hypothesis (H1) was that the indirect method is more accurate than the direct method. The P-value was calculated as\nP = n\u2211 i=k ( n i ) pi(1\u2212 p)n\u2212i\nwhere n = 36 is the total number of experiments, k is the number of times the indirect method outperformed the direct method, and p = 1/2 is the probability of a win for either method under H0.\nIn the case of real values of \u03b1 and \u03b2 (Table 1, columns DR vs. IR), we observed 1 win for the direct method, 28 wins for the indirect method and 7 ties (k = 31). This resulted in P = 6.5 \u00b7 10\u22126. On the other hand, in the case of the\nestimated values of \u03b1 and \u03b2 (Table 1, columns DE vs. IE), we observed 9 wins for the direct method, 25 wins for the indirect method and 2 ties (k = 26). This resulted in P = 5.7 \u00b7 10\u22123.\nA possible reason for this outcome may be the sensitivity of the one-step direct conversion from Equation 4 to errors in estimating AUCpu and \u03b2 \u2212 \u03b1, which can frequently land AUCest in the infeasible region. The indirect method, on the other hand, re-estimates the true positive and false positive rates for each decision threshold to first recover the ROC curve. Although this method seems more sensitive to the errors in estimating \u03b2\u2212\u03b1, it allows for removal of problematic points from the ROC curve and, thus, leads to an increased accuracy of estimation. Additional experiments are necessary to further characterize both direct and indirect methods; e.g., the sensitivity of the indirect method to the number of (\u03b7, \u03b3) points used to construct an ROC curve."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "A common approach in positive-unlabeled learning is to train a classification model between labeled and unlabeled data. This strategy is in fact known to give an optimal classifier under mild conditions; however, it results in biased empirical estimates of the classifier performance. In this work, we show that the typically used performance measures such as the receiver operating characteristic curve, or the precisionrecall curve obtained on such data can be corrected with the knowledge of class priors; i.e., the proportions of the positive and negative examples in the unlabeled data. We extend the results to a noisy setting where some of the examples labeled positive are in fact negative and show that the correction also requires the knowledge of the proportion of noisy examples in the labeled positives. Using state-of-the-art algorithms to estimate the positive class prior and the proportion of noise, we experimentally evaluate two correction approaches and demonstrate their efficacy on real-life data. Introduction Performance estimation in binary classification is tightly related to the nature of the classification task. As a result, different performance measures may be directly optimized during training. When (mis)classification costs are available, the classifier is ideally trained and evaluated in a costsensitive mode to minimize the expected cost (Whalen 1971; Elkan 2001). More often, however, classification costs are unknown and the overall performance is assessed by averaging the performance over a range of classification modes. The most extensively studied and widely used performance evaluation in binary classification involves estimating the Receiver Operating Characteristic (ROC) curve that plots the true positive rate of a classifier as a function of its false positive rate (Fawcett 2006). The ROC curve provides insight into trade-offs between the classifier\u2019s accuracies on positive versus negative examples over a range of decision thresholds. Furthermore, the area under the ROC curve (AUC) has a meaningful probabilistic interpretation that correlates with the ability of the classifier to separate classes and is often used to rank classifiers (Hanley and McNeil 1982). Another important performance criterion generally used in information retrieval relies on the precision-recall Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (pr-rc) curve, a plot of precision as a function of recall. The precision-recall evaluation, including summary statistics derived from the pr-rc curve, may be preferred to ROC curves when classes are heavily skewed (Davis and Goadrich 2006). Although model learning and performance evaluation in a supervised setting are well understood (Hastie et al. 2001), the availability of unlabeled data gives additional options and also presents new challenges. A typical semi-supervised scenario involves the availability of positive, negative and (large quantities of) unlabeled data. Here, the unlabeled data can be used to improve training (Blum and Mitchell 1998) or unbias the labeled data (Cortes et al. 2008); e.g., to estimate class proportions that are necessary to calibrate the model and accurately estimate precision when class balances (but not class-conditional distributions) in labeled data are not representative (Saerens et al. 2002). This is often the case when it is more expensive or difficult to label examples of one class than the examples of the other. A special case of the semi-supervised setting arises when the examples of only one class are labeled. It includes open-world domains such as molecular biology where, for example, wet lab experiments determining a protein\u2019s activity are generally conclusive; however, the absence of evidence about a protein\u2019s function cannot be interpreted as the evidence of absence. This is because, even when the labeling is attempted, a functional assay may not lead to the desired activity for a number of experimental reasons. In other domains, such as social networks, only positive examples can be collected (such as \u2018liking\u2019 a particular product) because, by design, the negative labeling is not allowed. The development of classification models in this setting is often referred to as positiveunlabeled learning (Denis et al. 2005). State-of-the-art techniques in positive-unlabeled learning tackle this problem by treating the unlabeled sample as negatives and training a classifier to distinguish between labeled (positive) and unlabeled examples. Following Elkan and Noto (2008), we refer to the classifiers trained on a labeled sample from the true distribution of inputs, containing both positive and negative examples, as traditional classifiers. Similarly, we refer to the classifiers trained on the labeled versus unlabeled data as non-traditional classifiers. In theory, the true performance of both traditional and non-traditional classifiers can be evaluated on a labeled samar X iv :1 70 2. 00 51 8v 1 [ st at .M L ] 2 F eb 2 01 7 ple from the true distribution (traditional evaluation). However, this is infeasible for non-traditional learners because such a sample is not available in positive-unlabeled learning. As a result, the non-traditional classifiers are evaluated by using the unlabeled sample as substitute for labeled negatives (non-traditional evaluation). Surprisingly, for a variety of performance criteria, non-traditional classifiers achieve similar performance under traditional evaluation as optimal traditional classifiers (Blanchard et al. 2010; Menon et al. 2015). The intuition for these results comes from the fact that in many practical situations, the posterior distributions in traditional and non-traditional setting provide the same optimal ranking of data points on a given test sample (Jain et al. 2016; Jain, White, and Radivojac 2016). Furthermore, the widely-accepted evaluation approaches using ROC or pr-rc curves are insensitive to the variation of raw prediction scores unless they affect the ranking. Though the efficacy of non-traditional classifiers has been thoroughly studied (Peng et al. 2003; Elkan and Noto 2008; Ward et al. 2009; Menon et al. 2015), estimating their true performance has been much less explored. Such performance estimation often involves computing the fraction(s) of correctly and incorrectly classified examples from both classes; however, in absence of labeled negatives, the fractions computed under the non-traditional evaluation are incorrect, resulting in biased estimates. Figure 1 illustrates the effect of this bias by showing the traditional and nontraditional ROC curves on a handmade data set. Because some of the unlabeled examples in the training set are in fact positive, the area under the ROC curve estimated when the unlabeled examples were considered negative (nontraditional setting) underestimates the true performance for positive versus negative classification (traditional setting). This paper formalizes and evaluates performance estimation of a non-traditional classifier in the traditional setting when the only available training data are (possibly noisy) positive examples and unlabeled data. We show that the true (traditional) performance of such a classifier can be recovered with the knowledge of class priors and the fraction of mislabeled examples in the positive set. We derive formulas for converting the ROC and pr-rc curves from the nontraditional to the traditional setting. Using these recovery formulas, we present methods to estimate true classification performance. Our experiments provide evidence that the methods for the recovery of a classifier\u2019s performance are sound and effective. Problem formulation Consider a binary classification problem from input x \u2208 X to output y \u2208 Y = {0, 1} in a positive-unlabeled setting. Let f be the true distribution over the input space X from which the unlabeled sample is drawn and let f1 and f0 be the distributions of the positive and negative examples, respectively. It follows that f can be expressed as a two-component mixture containing f1 and f0 as f(x) = \u03b1f1(x) + (1\u2212 \u03b1)f0(x), for all x \u2208 X where \u03b1 \u2208 [0, 1) is the mixing proportion (positive class prior) giving the proportion of positives in f . \u0301 \u0301 \u00b0 \u00b0 AUC = 0.8000 AUC = 0.9375 B. Positive vs. unlabeled A. Data set: prediction scores and class labels C. Positive vs. negative 0.986 yes, as 1 0.943 no 0.863 yes, as 1 0.789 no 0.009 no 0.699 yes, as 1 0.473 no 0.211 no 1 1 1 0 1 0 0 0 Prediction Labeled True class label", "creator": "LaTeX with hyperref package"}}}