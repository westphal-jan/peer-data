{"id": "1702.00953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Deep Learning with Low Precision by Half-wave Gaussian Quantization", "abstract": "proving the computational problem of quantizing the activations of a deep neural network is seldom considered. an examination of the popular binary quantization approach shows that this knowledge consists of calculations approximating a classical non - linearity, the hyperbolic tangent, by two functions : a piecewise constant sign error function, which is used in feedforward network computations, and via a piecewise linear hard disk tanh function, rarely used in the backpropagation step during network learning. the problem idea of approximating the relu non - linearity, widely used in the recent deep digital learning literature, is then considered. an half - wave gaussian quantizer ( hwgq ) is proposed for forward cross approximation and shown to rapidly have efficient implementation, by exploiting the statistics functions of of network activations and batch normalization operations commonly used in evaluating the literature. to overcome the problem of gradient mismatch, due to the use of substantially different forward and exit backward approximations, several piece - wise backward approximators are instead then adequately investigated. the implementation of the resulting quantized network, denoted as hwgq - net, is shown to achieve much closer arithmetic performance to full medium precision networks, such as alexnet, resnet, googlenet and vgg - net, bigger than previously traditionally available low - precision networks, with reduced 1 - bit binary weights overhead and 2 - bit quantized activations.", "histories": [["v1", "Fri, 3 Feb 2017 10:11:40 GMT  (454kb)", "http://arxiv.org/abs/1702.00953v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhaowei cai", "xiaodong he", "jian sun", "nuno vasconcelos"], "accepted": false, "id": "1702.00953"}, "pdf": {"name": "1702.00953.pdf", "metadata": {"source": "CRF", "title": "Deep Learning with Low Precision by Half-wave Gaussian Quantization", "authors": ["Zhaowei Cai", "Xiaodong He", "Jian Sun"], "emails": ["zwcai@ucsd.edu", "xiaohe@microsoft.com", "sunjian@megvii.com", "nuno@ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n00 95\n3v 1\n[ cs\n.C V\n] 3\nF eb\n2 01"}, {"heading": "1. Introduction", "text": "Deep neural networks have achieved state-of-the-art performance on computer vision problems, such as classification [22, 36, 37, 12, 13], detection [7, 33, 1], etc. However, their complexity is an impediment to widespread deployment in many applications of real world interest, where either memory or computational resource is limited. This is due to two main issues: large model sizes (50MB for GoogLeNet [37], 200M for ResNet-101 [13], 250MB for AlexNet [22], or 500M for VGG-Net [36]) and large computational cost, typically requiring GPU-based implementations. This generated interest in compressed models with smaller memory footprints and computation.\nSeveral works have addressed the reduction of model\nsize, through the use of quantization [3, 28, 26], low-rank matrix factorization [19, 6], pruning [11, 10], architecture design [27, 17], etc. Recently, it has been shown that weight compression by quantization can achieve very large savings in memory, reducing each weight to as little as 1 bit, with a marginal cost in classification accuracy [3, 28]. However, it is less effective along the computational dimension, because the core network operation, implemented by each of its units, is the dot-product between a weight and an activation vector. On the other hand, complementing binary or quantized weights with quantized activations allows the replacement of expensive dot-products by logical and bitcounting operations. Hence, substantial speed ups are possible if, in addition to the weights, the inputs of each unit are binarized or quantized to low-bit.\nIt appears, however, that the quantization of activations is more difficult than that of weights. For example, [4, 32] have shown that, while it is possible to binarize weights with a marginal cost in model accuracy, additional quantization of activations incurs nontrivial losses for large-scale classification tasks, such as object recognition on ImageNet [35]. The difficulty is that binarization or quantization of activations requires their processing with non-differentiable operators. This creates problems for the gradient descent procedure, the backpropagation algorithm, commonly used to learn deep networks. This algorithm iterates between a feedforward step that computes network outputs and a backpropagation step that computes the gradients required for learning. The difficulty is that binarization or quantization operators have step-wise responses that produce very weak gradient signals during backpropagation, compromising learning efficiency. So far, the problem has been addressed by using continuous approximations of the operator used in the feedforward step to implement the backpropagation step. This, however, creates a mismatch between the model that implements the forward computations and the derivatives used to learn it. In result, the model learned by the backpropagation procedure tends to be sub-optimal.\nIn this work, we view the quantization operator, used in the feedforward step, and the continuous approximation, used in the backpropagation step, as two functions\n1\nthat approximate the activation function of each network unit. We refer to these as the forward and backward approximation of the activation function. We start by considering the binary \u00b11 quantizer, used in [4, 32], for which these two functions can be seen as a discrete and a continuous approximation of a non-linear activation function, the hyperbolic tangent, frequently used in classical neural networks. This activation is, however, not commonly used in recent deep learning literature, where the ReLU nonlinearity [30, 39, 12] has achieved much greater preponderance. This is exactly because it produces much stronger gradient magnitudes. While the hyperbolic tangent or sigmoid nonlinearities are squashing non-linearities and mostly flat, the ReLU is an half-wave rectifier, of linear response to positive inputs. Hence, while the derivatives of the hyperbolic tangent are close to zero almost everywhere, the ReLU has unit derivative along the entire positive range of the axis.\nTo improve the learning efficiency of quantized networks, we consider the design of forward and backward approximation functions for the ReLU. To discretize its linear component, we propose to use an optimal quantizer. By exploiting the statistics of network activations and batch normalization operations that are commonly used in the literature, we show that this can be done with an half-wave Gaussian quantizer (HWGQ) that requires no learning and is very efficient to compute. While some recent works have attempted similar ideas [4, 32], their design of a quantizer is not sufficient to guarantee good deep learning performance. We address this problem by complementing this design with a study of suitable backward approximation functions that account for the mismatch between the forward model and the back propagated derivatives. This study suggests operations such as linearization, gradient clipping or gradient suppression for the implementation of the backward approximation. We show that a combination of the forward HWGQ with these backward operations produces very efficient low-precision networks, denoted as HWGQ-Net, with much closer performance to continuous models, such as AlexNet [22], ResNet [13], GoogLeNet [37] and VGG-Net [36], than other available low-precision networks in the literature. To the best of our knowledge, this is the first time that a single low-precision algorithm could achieve successes for so many popular networks. According to [32], theoretically HWGQ-Net (1-bit weights and 2-bit activations) has \u223c32\u00d7 memory and \u223c32\u00d7 convolutional computation savings. These suggest that the HWGQ-Net can be very useful for the deployment of state-of-the-art neural networks in real world applications."}, {"heading": "2. Related Work", "text": "The reduction of model size is a popular goal in the deep learning literature, due to its importance for the deployment of high performance neural networks in real word applica-\ntions. One strategy is to exploit the widely known redundancy of neural network weights [5]. For example, [19, 6] proposed low-rank matrix factorization as a way to decompose a large weight matrix into several separable small matrices. This approach has been shown most successful for fully connected layers. An alternative procedure, known as connection pruning [11, 10], consists of removing unimportant connections of a pre-trained model and retraining. This has been shown to reduce the number of model parameters by an order of magnitude without considerable loss in classification accuracy. Another model compression strategy is to constrain the model architecture itself, e.g. by removing fully connected layers, using convolutional filters of small size, etc. Many state-of-the-art deep networks, such as NIN [27], GoogLeNet [37] and ResNet [13], rely on such design choices. For example, SqueezeNet [17] has been shown to achieve a parameter reduction of \u223c50 times, for accuracy comparable to that of AlexNet. Moreover, hash functions have also been used to compress model size [2].\nAnother branch of approaches for model compression is weight binarization [3, 32, 4] or quantization [28, 26, 8]. [38] used a fixed-point representation to quantize weights of pre-trained neural networks, so as to speed up testing on CPUs. [8] explored several alternative quantization methods to decrease model size, showing that procedures such as vector quantization, with k-means, enable 4\u223c8 times compression with minimal accuracy loss. [26] proposed a method for fixed-point quantization based on the identification of optimal bit-width allocations across network layers. [24, 28] have shown that ternary weight quantization into levels {\u22121, 0, 1} can achieve 16\u00d7 or 32\u00d7 model compression with slight accuracy loss, even on large-scale classification tasks. Finally, [3] has shown that filter weights can be quantized to \u00b11 without noticeable loss of classification accuracy on datasets such as CIFAR-10 [21].\nBeyond weight binarization, the quantization of activations has two additional benefits: 1) further speed-ups by replacement of the expensive inner-products at the core of all network computations with logical and bit-counting operations; 2) training memory savings, by avoiding the large amounts of memory required to cache full-precision activations. Due to these, activation quantization has attracted some attention recently [38, 26, 4, 32, 40, 25, 28]. In [38], activations were quantized with 8 bits, to achieve speedups on CPUs. By performing the quantization after network training, this work avoided the issues of nondifferentiable optimization. [26] developed an optimal algorithm for bit-width allocation across layers, but did not propose a learning procedure for quantized neural networks. Recently, [4, 32, 40] tried to tackle the optimization of networks with nondifferentiable quantization units, by using a continuous approximation to the quantizer function in the backpropagation step. [25] proposed several potential so-\nlutions to the problem of gradient mismatch and [28, 40] showed that gradients can be quantized with a small number of bits during the backpropagation step. While some of these methods have produced good results on datasets such as CIFAR-10, none has produced low precision networks competitive with full-precision models on large-scale classification tasks, such as ImageNet [35]."}, {"heading": "3. Binary Networks", "text": "We start with a brief review of the issues involved in the binarization of a deep network."}, {"heading": "3.1. Goals", "text": "Deep neural networks are composed by layers of processing units that roughly model the computations of the neurons found in the mammalian brain. Each unit computes an activation function of the form\nz = g(wT x), (1)\nwhere w \u2208 Rc\u00b7w\u00b7h is a weight vector, x \u2208 Rc\u00b7w\u00b7h an input vector, and g(\u00b7) a non-linear function. A convolutional network implements layers of these units, where weights are usually represented as a tensor W \u2208 Rc\u00d7w\u00d7h. The dimensions c, w and h are defined by the number of filter channels, width and height, respectively. Since this basic computation is repeated throughout the network and modern networks have very large numbers of units, the structure of (1) is the main factor in the complexity of the overall model. This complexity can be a problem for applications along two dimensions. The first is the large memory footprint required to store weights w. The second is the computational complexity required to compute large numbers of dot-products wT x. Both difficulties are compounded by the requirement of floating point storage of weights and floating point arithmetic to compute dot-products, which are not practical for many applications. This has motivated interest in low-precision networks [4, 32, 40]."}, {"heading": "3.2. Weight Binarization", "text": "An effective strategy to binarize the weights W of convolutional filters, which we adopt in this work, has been proposed by [32]. This consists of approximating the full precision weight matrix W, used to compute the activations of (1) for all the units, by the product of a binary matrix B \u2208 {+1,\u22121}c\u00d7w\u00d7h and a scaling factor \u03b1 \u2208 R+, such that W \u2248 \u03b1B. A convolutional operation on input I can then be approximated by\nI \u2217 W \u2248 \u03b1(I \u2295 B), (2)\nwhere \u2295 denotes a multiplication-free convolution. [32] has shown that an optimal approximation can be achieved with\nB\u2217 = sign(W) and \u03b1\u2217 = 1 cwh \u2016W\u20161. More details can be found in [32].\nWhile binary weights tremendously reduce the memory footprint of the model, they do not fully solve the problem of computational complexity. Since I consists of either the activations of a previous layer or some transformation of the image to classify, it is usually represented with full precision. Hence, (2) requires floating point arithmetic and produces a floating point result. Substantial further reductions of complexity can be obtained by the binarization of I, which enables the implementation of dot products with logical and bit-counting operations [4, 32]."}, {"heading": "3.3. Binary Activation Quantization", "text": "This problem has been studied in the literature, where the use of binary activations has recently become popular [32, 4, 40]. This is usually implemented by replacing g(x) in (1) with the sign non-linearity\nz = sign(x) = { +1, if x \u2265 0, \u22121, otherwise\n(3)\nshown in Figure 1. [32] has also considered rescaling the binarized outputs z by a factor \u03b2, but found this to be unnecessary.\nWhile the adoption of (3) greatly simplifies the feedforward computations of the deep model, it severely increases the difficulty of learning. This follows from the fact that the derivative of the sign function is zero almost everywhere. Neural networks are learned by minimizing a cost C with respect to the weights w. This is done with the backpropagation algorithm, which decomposes these derivatives into a series of simple computations. Consider the unit of (1). The derivative of C with respect to w is\n\u2202C \u2202w = \u2202C \u2202z g\u2032(wTx)x. (4)\nThese derivatives are computed for all units during the backpropagation step. When g(x) is replaced by (3), the\nderivative g\u2032(wTx) is zero almost everywhere and the gradient magnitudes tend to be very small. In result, the gradient descent algorithm does not converge to minima of the cost. To overcome this problem, [4] proposed to use an alternative function, hard tanh, which we denote by s\u0303ign, in the backpropagation step. This function is shown in Figure 1, and has derivative\ns\u0303ign \u2032 (x) = { 1, if |x| \u2264 1 0, otherwise.\n(5)\nIn this work, we denote (3) as the forward and (5) as the backward approximations of the activation non-linearity g(x) of (1). These approximations have two main problems. The first is that they approximate the hyperbolic tangent\ng(x) = tanh(x) = ex \u2212 e\u2212x\nex + e\u2212x .\nThis (and the closely related sigmoid function) is a squashing non-linearity that replicates the saturating behavior of neural firing rates. For this reason, it was widely used in the classical neural net literature. However, squashing non-linearities have been close to abandoned in recent deep learning literature, because the saturating behavior emphasizes the problem of vanishing derivatives, compromising the effectiveness of backpropagation. The second problem is that the discrepancy between the approximation of g(x) by the forward sign and by the backward s\u0303ign creates a mismatch between the feedforward model and the derivatives used to learn it. In result, backpropagation can be highly suboptimal. This is called the \u201cgradient mismatch\u201d problem [25]."}, {"heading": "4. Half-wave Gaussian Quantization", "text": "In this section, we propose an alternative quantization strategy, which is based on the approximation of the ReLU non-linearity."}, {"heading": "4.1. ReLU", "text": "The ReLU is the half-wave rectifier defined by [30]\ng(x) = max(0, x). (6)\nIt is now well known that, when compared to squashing non-linearities, its use in (1) significantly improves the efficiency of the backpropagation algorithm. It thus seems\nmore sensible to rely on ReLU approximations for network quantization than those of the previous section. We propose a quantizer Q(x) to approximate (6) in the feedforward step and a suitable piecewise linear approximation Q\u0303(x) for the backpropagation step."}, {"heading": "4.2. Forward Approximation", "text": "A quantizer is a piecewise constant function\nQ(x) = qi, if x \u2208 (ti, ti+1], (7)\nthat maps all values of x within quantization interval (ti, ti+1] into a quantization level qi \u2208 R, for i = 1, \u00b7 \u00b7 \u00b7 ,m. In general, t1 = \u2212\u221e and tm+1 = \u221e. This generalizes the sign function, which can be seen as a 1-bit quantizer. A quantizer is denoted uniform if\nqi+1 \u2212 qi = \u2206, \u2200i, (8)\nwhere \u2206 is a constant quantization step. The quantization levels qi act as the reconstruction values for x, under the constraint of reduced precision. Since, for any x, it suffices to store the quantization index i of (7) to recover the quantization level qi, non-uniform quantizer requires log2 m bits of storage per activation x. However, it usually requires more than log2 m bits to represent x during arithmetic operations, in which it is qi to be used instead of index i. For a uniform quantizer, where \u2206 is a universal scaling factor that can be placed in evidence, it is intuitive to store any x by log2 m bits without storing the indexes. The same holds for arithmetic computation.\nOptimal quantizers are usually defined in the meansquared error sense, i.e.\nQ\u2217(x) = argmin Q Ex[(Q(x) \u2212 x) 2] (9)\n= argmin Q\n\u222b p(x)(Q(x) \u2212 x)2dx\nwhere p(x) is the probability density function of x. Hence, the optimal quantizer of the dot-products of (1) depends on their statistics. While the optimal solution Q\u2217(x) of (9) is usually non-uniform, a uniform solution Q\u2217(x) is available by adding the uniform constraint of (8) to (9). Given dot product samples, the optimal solution of (9) can be obtained by Lloyd\u2019s algorithm [29], which is similar to k-means algorithm. This, however, is an iterative algorithm. Since a\ndifferent quantizer must be designed per network unit, and this quantizer changes with the backpropagation iteration, the straightforward application of this procedure is computationally intractable.\nThis difficulty can be avoided by exploiting the statistical structure of the activations of deep networks. For example, [16, 18] have noted that the dot-products of (1) tend to have a symmetric, non-sparse distribution, that is close to Gaussian. Taking into account the fact that ReLU is a half-wave rectifier, this suggests the use of the half-wave Gaussian quantizer (HWGQ),\nQ(x) = { qi, if x \u2208 (ti, ti+1], 0, x \u2264 0,\n(10)\nwhere qi \u2208 R+ for i = 1, \u00b7 \u00b7 \u00b7 ,m and ti \u2208 R+ for i = 1, \u00b7 \u00b7 \u00b7 ,m + 1 (t1 = 0 and tm+1 = \u221e) are the optimal quantization parameters for the Gaussian distribution. The adoption of the HWGQ guarantees that these parameters only depend on the mean and variance of the dot-product distribution. However, because these can vary across units, it does not eliminate the need for the repeated application of Lloyd\u2019s algorithm across the network.\nThis problem can be alleviated by resorting to batch normalization [18]. This is a widely used normalization technique, which forces the responses of each network layer to have zero mean and unit variance. We apply this normalization to the dot-products, with the result illustrated in Figure 2, for a number of AlexNet units of different layers. Although the distributions are not perfectly Gaussian and there are minor differences between them, they are all close to Gaussian with zero mean and unit variance. It follows that the optimal quantization parameters q\u2217i and t \u2217\ni are approximately identical across units, layers and backpropagation iterations. Hence, Lloyd\u2019s algorithm can be applied once, with data from the entire network. In fact, because all distributions are approximately Gaussian of zero mean and unit variance, the quantizer can even be designed from samples of this distribution. In our implementation, we drew 106 samples from a standard Gaussian distribution of zero mean and unit variance, and obtained the optimal quantization parameters by Lloyd\u2019s algorithm. The resulting parameters t\u2217i and q\u2217i were used to parametrize a single HWGQ that was used in all layers, after batch normalization of dot-products."}, {"heading": "4.3. Backward Approximation", "text": "Since the HWGQ is a step-wise constant function, it has zero derivative almost everywhere. Hence, the approximation of g(x) by Q(x) in (4) leads to the problem of vanishing derivatives. As in Section 3, a piecewise linear function Q\u0303(x) can be used during the backpropagation step to avoid weak convergence. In summary, we seek a piece-wise function that provides a good approximation to the ReLU and to the HWGQ. We next consider three possibilities."}, {"heading": "4.3.1 Vanilla ReLU", "text": "Since the ReLU of (6) is already a piece-wise linear function, it seems sensible to use the ReLU itself, denoted the vanilla ReLU, as the backward approximation function. This corresponds to using the derivative\nQ\u0303\u2032(x) = { 1, if x > 0, 0, otherwise\n(11)\nin (4). The forward and backward approximations Q(x) and Q\u0303(x) of the ReLU are shown in Figure 1. Note that, while the backward approximation is exact, it is not equal to the forward approximation. Hence, there is a gradient mismatch. This mismatch is particularly large for large values of x. Note that, for x > 0, the approximation of Q(x) by the ReLU has error |Q(x) \u2212 x|. This is usually upper bounded by (ti+1 \u2212 qi) for x \u2208 (ti, ti+1] but unbounded when x \u2208 (tm,\u221e). Since these are the values on the tail of the distribution of x, the ReLU is said to have a large mismatch with Q(x) \u201con the tail.\u201d Due to this, when the ReLU is used to approximate g\u2032(x) in (4), it can originate very inaccurate gradients for dot-products on the tail of the dot-product distribution. In our experience, these inaccurate gradients can make the learning algorithm unstable.\nThis is a classical problem in the robust estimation literature, where outliers can unduly influence the performance of a learning algorithm [15]. For quantization, where Q(x) assumes that values of x beyond qm have very low probability, large dot-products are effectively outliers. The classical solution to mitigate the influence of these outliers is to limit the growth rate of the error function. Since this is |Q(x) \u2212 x|, the problem is the monotonicity of the ReLU beyond x = qm. To address it, we investigated alternative backwards approximation functions of slower growth rate."}, {"heading": "4.3.2 Clipped ReLU", "text": "The first approximation, denoted the clipped ReLU, is identical to the vanilla ReLU in (\u2212\u221e, qm] but constant beyond x = qm,\nQ\u0303c(x) =    qm, x > qm, x, x \u2208 (0, qm], 0, otherwise.\n(12)\nIts use to approximate g\u2032(wTx) in (4) guarantees that there is no mismatch on the tail. Gradients are non-zero only for dot-products in the interval (0, qm]. We refer to this as ReLU clipping. As illustrated in Figure 3, the clipped ReLU is a better match for the HWGQ than the vanilla ReLU. This idea is similar to the use of gradient clipping in [31]: clipping gradients to enable stable optimization, especially for very deep networks, e.g. recurrent neural network. In our experiments, ReLU clipping is also proved very useful to guarantee stable neural network optimization."}, {"heading": "4.3.3 Log-tailed ReLU", "text": "Ideally, a network with quantized activations should approach the performance of a full-precision network as the number of quantization levels m increases. The sensitivity of the vanilla ReLU approximation to outliers limits the performance of low precision networks (low m). While the clipped ReLU alleviates this problem, it can impair network performance due to the loss of information in the clipped interval (qm,\u221e). An intermediate solution is to use, in this interval, a function whose growth rate is in between that of the clipped ReLU (zero derivative) and the ReLU (unit derivative). One possibility is to enforce logarithmic growth on the tail, according to\nQ\u0303l(x) =    qm + log(x\u2212 \u03c4), x > qm, x, x \u2208 (0, qm], 0, x \u2264 0,\n(13)\nwhere \u03c4 = qm\u22121. This is denoted the log-tailed ReLU and is compared to the ReLU and clipped ReLU in Figure 3. It has derivative\nQ\u0303 \u2032\nl(x) =    1/(x\u2212 \u03c4), x > qm, 1, x \u2208 (0, qm], 0, x \u2264 0.\n(14)\nWhen used to approximate g\u2032(x) in (4), the log-tailed ReLU is identical to the vanilla ReLU for dot products of amplitude smaller than qm, but gives decreasing weight to amplitudes larger than this. It behaves like the vanilla ReLU (unit derivative) for x \u2248 qm but converges to the clipped ReLU (zero derivative) as x grows to infinity."}, {"heading": "5. Experimental Results", "text": "The proposed HWGQ-Net was evaluated on ImageNet (ILSVRC2012) [35], which has \u223c1.2M training images from 1K categories and 50K validation images. The evaluation metrics were top-1 and top-5 classification accuracy. Several popular networks were tested: AlexNet [22], ResNet [13], a variant of VGG-Net [36, 12], and GoogLeNet [37]. Our implementation is based on Caffe [20], and the source code is available at https://github.com/zhaoweicai/hwgq."}, {"heading": "5.1. Implementation Details", "text": "In all ImageNet experiments, training images were resized to 256\u00d7256, and a 224\u00d7224 (227\u00d7227 for AlexNet) crop was randomly sampled from an image or its horizontal flip. Batch normalization [18] was applied before each quantization layer, as discussed in Section 4.2. Since weight binarization provides regularization constraints, the ratio of dropout [14] was set as 0.1 for networks with binary weights and full activations, but no dropout was used for networks with quantized activations regardless of weight precision. All networks were learned from scratch. No data augmentation was used other than standard random image flipping and cropping. SGD was used for parameter learning. No bias term was used for binarized weights. Similarly to [32], networks with quantized activations used maxpooling before batch normalization, which is denoted \u201clayer re-ordering\u201d. As in [32, 40], the first and last network layers had full precision. Evaluation was based solely on central 224\u00d7224 crop.\nOn AlexNet [22] experiments, the mini-batch size was 256, weight decay 0.0005, and learning rate started at 0.01. For ResNet, the parameters were the same as in [13]. For the variant of VGG-Net, denoted VGG-Variant, a smaller version of model-A in [12], only 3 convolutional layers were used for input size of 56, 28 and 14, and the \u201cspp\u201d layer was removed. The mini-batch size was 128, and learning rate started at 0.01. For GoogLeNet [37], the branches for side losses were removed, in the inception layers, maxpooling was removed and the channel number of the \u201creduce\u201d 1\u00d71 convolutional layers was increased to that of their following 3\u00d73 and 5\u00d75 convolutional layers. Weight decay was 0.0002 and the learning strategy was similar to ResNet [13]. For all networks tested, momentum was 0.9, batch normalization was used, and when mini-batch size was 256 (128), the learning rate was divided by 10 after every 50K (100K) iterations, 160K (320K) in total. Only AlexNet, ResNet-18 and VGG-Variant were explored in the following ablation studies. In all tables and figures, \u201cFW\u201d indicates full-precision weights, \u201cBW\u201d binary weights, and \u201cFull\u201d full-precision weights and activations."}, {"heading": "5.2. Full-precision Activation Comparison", "text": "Before considering the performance of the forward quantized activation functions sign(x) and Q(x), we compared the performance of the continuous s\u0303ign(x) (hard tanh) and Q\u0303(x) (ReLU) as activation function. In this case, there is no activation quantization nor forward/backward gradient mismatch. AlexNet results are presented in Table 1, using iden-\ntical settings for s\u0303ign(x) and Q\u0303(x), for fair comparison. As expected from the discussion of Sections 3 and 4, Q\u0303(x) achieved substantially better performance than s\u0303ign(x), for both FW and BW networks. The fact that these results upper bound the performance achievable when quantization is included suggests that sign(x) is not a good choice for quantization function. Q(x), on the other hand, has a fairly reasonable upper bound."}, {"heading": "5.3. Low-bit Activation Quantization Results", "text": "We next compared the performance achieved by adding the sign and HWGQ Q(x) (backward vanilla ReLU) quantizers to the set-up of the previous section. The results of AlexNet, ResNet-18 and VGG-Variant are summarized in Table 2. At first, notice that BW is worse than BW+Q of AlexNet in Table 1 due to the impact of the layer reordering [32] introduced in Section 5.1. Next, comparing BW to FW+Q, where the former binarizes weights only and the latter quantizes activations only, we observed that weight binarization causes a minor degradation of accuracy. This is consistent with the findings of [32, 4]. On the other hand, activation quantization leads to a nontrivial loss. This suggests that the latter is a more difficult problem than the former. This observation is not surprising since, unlike weight binarization, the learning of an activation-quantized networks needs to propagate gradients through every nondifferentiable quantization layer.\nWhen weight binarization and activation quantization\nwere combined, recognition performance dropped even further. For AlexNet, the drop was much more drastic for BW+sign (backward hard tanh) than for BW+Q (backward vanilla ReLU). These results support the hypotheses of Section 3 and 4, as well as the findings of Table 1. The training errors of BW+sign and BW+Q of AlexNet are shown in Figure 4. Note the much lower training error of Q(x), suggesting that it enables a much better approximation of the full precision activations than sign(x). Nevertheless, the gradient mismatch due to the use of Q(x) as forward and the vanilla ReLU as backward approximators made the optimization somewhat instable. For example, the error curve of BW+Q is bumpy during training. This problem becomes more serious for deeper networks. In fact, for the ResNet-18 and VGG-Variant, BW+Q performed worse than BW+sign. This can be explained by the fact that the sign has a smaller gradient mismatch problem than the vanilla ReLU. As will be shown in the following section, substantial improvements are possible by correcting the mismatch between the forward quantizer Q(x) and its backward approximator."}, {"heading": "5.4. Backward Approximations Comparison", "text": "We next considered the impact of the backward approximators of Section 4.3. Table 3 shows the performance achieved by the three networks under the different approximations. In all cases, weights were binarized and the HWGQ was used as forward approximator (quantizer). \u201cnoopt\u201d refers to the quantization of activations of pre-trained BW networks. This requires no nondifferentiable approximation, but fails to account for the quantization error. We attempted to minimize the impact of cumulative errors across the network by recomputing the means and variances of all batch normalization layers. Even after this, \u201cno-opt\u201d had significantly lower accuracy than the full-precision activation networks.\nSubstantial gains were obtained by training the activation quantized networks from scratch. Although the vanilla ReLU had reasonable performance as backwards approximator for AlexNet, much better results were achieved with the clipped ReLU of (12) and the log-tailed ReLU of (13). Figure 5 shows that the larger gradient mismatch of the vanilla ReLU created instabilities in the optimization, for all networks. However, these instabilities were more serious for the deeper networks, such as ResNet-18 and VGG-\nVariant. This explains the sharper drop in performance of the vanilla ReLU for these networks, in Table 3. Note, in Figure 5, that the clipped ReLU and the log-tailed ReLU enabled more stable learning and reached a much better optimum for all networks. Among them, the log-tailed ReLU performed slightly better than the clipped ReLU on AlexNet, but slightly worse on ResNet-18 and VGGVariant. To be consistent, \u201cclipped ReLU\u201d was used in the following sections."}, {"heading": "5.5. Bit-width Impact", "text": "The next set of experiments studied the bit-width impact of the activation quantization. In all cases, weights were binarized. Table 4 summarizes the performance of AlexNet and ResNet-18 as a function of the number of quantization levels. While the former improved with the latter, there was a saturation effect. The default HWGQ configuration, also used in all previous experiments, consisted of two nonuniform positive quantization levels plus a \u201c0\u201d. This is denoted as \u201c2\u201d in the table. For AlexNet, this very low-bit quantization sufficed to achieve recognition rates close to those of the full-precision activations. For this network, quantization with seven non-uniform levels was sufficient to reproduce the performance of full-precision activations. For ResNet-18, however, there was a more noticeable gap between low-bit and full-precision activations. For example, \u201c3\u201d outperformed \u201c2\u201d by 3.1 points for ResNet-18, but only 2.1 points for AlexNet. These results suggest that increasing the number of quantization levels is more beneficial for ResNet-18 than for AlexNet.\nSo far we have used non-uniform quantization. As discussed in Section 4.2, this requires more bits than uniform quantization during arithmetic computation. Table 4 also\nshows the results obtained with uniform quantization, with superscript \u201c\u2217\u201d. Interestingly, for the same number of quantization levels, the performance of the uniform quantizer was only slightly worse than that of its non-uniform counterpart. But for the same bit width, the uniform quantizer was noticeably superior than the non-uniform quantizer, by comparing \u201c2\u201d and \u201c3\u2217\u201d (both of them need 2-bit representation during arithmetic computation)."}, {"heading": "5.6. Comparison with the state-of-the-art", "text": "Table 51 presents a comparison between the full precision and the low-precision HWGQ-Net of many popular network architectures. For completeness, we consider the GoogLeNet, ResNet-34 and ResNet-50 in this section. In all cases, the HWGQ-Net used 1-bit binary weights, a 2-bit uniform HWGQ as forward approximator, and the clipped\n1The reference performance of AlexNet and GoogLeNet is at https://github.com/BVLC/caffe, and of ResNet is at https://github.com/facebook/fb.resnet.torch. Our worse ResNet implementations are probably due to fewer training iterations and no further data augmentation.\nReLU as backwards approximator. Comparing to the previous ablation experiments, the numbers of training iterations were doubled and polynomial learning rate annealing (power of 1) was used for HWGQ-Net, where it gave a slight improvement over step-wise annealing. Table 5 shows that the HWGQ-Net approximates well all popular networks, independently of their complexity or depth. The top-1 accuracy drops from full- to low-precision are similar for all networks (5\u223c9 points), suggesting that low-precision HWGQ-Net will achieve improved performance as better full-precision networks become available.\nTraining a network with binary weights and lowprecision activations from scratch is a new and challenging problem, only addressed by a few previous works [4, 32, 40]. Table 6 compares the HWGQ-Net with the recent XNOR-Net [32] and DOREFA-Net [40], on the ImageNet classification task. The DOREFA-Net result is for a model of binary weights, 2-bit activation, full precision gradient and no pre-training. For AlexNet, the HWGQ-Net outperformed the XNOR-Net and the DOREFA-Net by a large margin. Similar improvements over the XNOR-Net were observed for the ResNet-18, where DOREFA-Net results are not available. It is worth noting that the gaps between the full-precision networks and the HWGQ-Net (- 5.8 for AlexNet and -7.7 for ResNet-18) are much smaller than those of the XNOR-Net (-12.4 for AlexNet and -18.1 for ResNet-18) and the DOREFA-Net (-8.2 for AlexNet). This is strong evidence that the HWGQ is a better activation quantizer. Note that, in contrast to the experimentation with one or two networks by [4, 32, 40], the HWGQ-Net is shown to perform well for various network architectures. To the best of our knowledge, this is the first time that a single low-precision network is shown to successfully approximate many popular networks."}, {"heading": "5.7. Results on CIFAR-10", "text": "In addition, we conducted some experiments on the CIFAR-10 dataset [21]. The network structure used, denoted VGG-Small, was similar to that of [4] but relied on a cross-entropy loss and without two fully connected layers. The learning strategy was that used in the VGG-Variant of Section 5.1, but the mini-batch size was 100 and the learning rate was divided by 10 after every 40K iterations (100K in total). The data augmentation strategy of [13] was used. As in Section 5.6, polynomial learning rate decay was used for low-precision VGG-Small. The HWGQ-Net results are compared with the state-of-the-art in Table 7. It can be seen that the HWGQ-Net achieved better performance than various popular full-precision networks, e.g. Maxout [9], NIN [27], DSN [23], FitNet [34], and than various low precision networks. The low-precision VGG-Small drops 0.67 points when compared to its full-precision counterpart. These findings are consistent with those on ImageNet."}, {"heading": "6. Conclusion", "text": "In this work, we considered the problem of training high performance deep networks with low-precision. This was achieved by designing two approximators for the ReLU non-linearity. The first is a half-wave Gaussian quantizer, applicable in the feedforward network computations. The second is a piece-wise continuous function, to be used in the backpropagation step during learning. This design overcomes the learning inefficiency of the popular binary quantization procedure, which produces a similar approximation for the less effective hyperbolic tangent nonlinearity. To minimize the problem of gradient mismatch, we have studied several backwards approximation functions. It was shown that the mismatch is most affected by activation outliers. Insights from the robust estimation literature were then used to propose the clipped ReLU and log tailed ReLU approximators. The network that results from the combination of these with the HWGQ, denoted HWGQ-Net was shown to significantly outperform previous efforts at deep learning with low precision, substantially reducing the gap between the low-precision and full-precision various stateof-the-art networks. These promising experimental results suggest that the HWGQ-Net can be very useful for the deployment of state-of-the-art neural networks in real world applications."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An halfwave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.", "creator": "LaTeX with hyperref package"}}}