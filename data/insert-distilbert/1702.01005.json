{"id": "1702.01005", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning", "abstract": "principal component analysis ( pca ) is a fundamental method for estimating a constrained linear subspace approximation to high - dimensional data. many algorithms exist however in literature able to achieve a statistically robust version of pca called rpca. in this paper, we present a geometric framework for computing the principal linear subspaces in both situations that directly amounts to computing the intrinsic average on the space of all subspaces ( the grassmann schmidt manifold ). points on this manifold are defined as locating the subspaces spanned by $ k $ - tuples of observations. we show that the intrinsic grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a gaussian distribution. similar results are also shown to hold techniques for locating the rpca. further, we propose introducing an efficient online algorithm to do subspace averaging which is devoid of linear complexity in terms of number of samples and has a linear convergence rate. when the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance ( accuracy and computation time ) gain over a recently published rpca methods with publicly accessible code. we have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. experimental results depicting stability of our proposed method are also presented. furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online rpca algorithm. in terms of reconstruction error and time required, both our algorithms outperform the competition.", "histories": [["v1", "Fri, 3 Feb 2017 13:44:44 GMT  (536kb,D)", "http://arxiv.org/abs/1702.01005v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["rudrasis chakraborty", "s{\\o}ren hauberg", "baba c vemuri"], "accepted": false, "id": "1702.01005"}, "pdf": {"name": "1702.01005.pdf", "metadata": {"source": "CRF", "title": "Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning", "authors": ["Rudrasis Chakraborty", "S\u00f8ren Hauberg", "Baba C. Vemuri"], "emails": ["baba.vemuri}@gmail.com", "2{sohau}@dtu.dk"], "sections": [{"heading": "1 Introduction", "text": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix. All these derivations lead to the same algorithm. Recently, Hauberg et al. [18] noted that the average of all one-dimensional subspaces spanned by normally distributed data coincides with the leading principal component. Here the average is computed over the Grassmann manifold of one-dimensional subspaces (cf. Sec. 2). This average can be computed very efficiently, but unfortunately\nar X\niv :1\n70 2.\n01 00\n5v 1\n[ cs\n.L G\n] 3\nF eb\ntheir formulation does not generalize to higher-dimensional subspaces. In this paper, we provide a formulation for estimating the average K-dimensional subspace spanned by the observed data, and present a very simple, parameter-free online algorithm for computing this average. When the data is normally distributed, we show that this average subspace coincides with that spanned by the leading K principal components. We further show that our online algorithm has a linear convergence rate. Moreover, since our algorithm is online, it has a linear complexity in terms of the number of samples. Furthermore, we propose an online robust subspace averaging algorithm which can be used to get the leading K robust principal components. Analogous to its non-robust counterpart, it has a linear time complexity in terms of the number of samples."}, {"heading": "1.1 Related Work", "text": "In this paper we consider a simple linear dimensionality reduction algorithm that works in an online setting, i.e. only uses each data point once. There are several existing approaches in literature that tackle the online PCA and the online Robust PCA problems and we discuss some of these approaches here:\nOja\u2019s rule [28] is a classic online estimator for the leading principal components of a dataset. Given a basis Vt\u22121 \u2208 RD\u00d7K this is updated recursively via Vt = Vt\u22121 + \u03b3tXt(X T t Vt\u22121) upon receiving the observation Xt. Here \u03b3t is the step-size (learning rate) parameter that must be set manually; small values gives slow-but-sure convergence, while larger values may give fast-but-unstable convergence.\nEM-PCA [33] is usually derived for probabilistic PCA, but is easily be adapted to the online setting [9]. Here, the E- and M-steps are given by:\n(E-step) Yt =(V Tt\u22121Vt\u22121) \u22121(V Tt\u22121Xt) (1)\n(M-step) V\u0303t =(XtY Tt )(YtY T t ) \u22121. (2)\nThe basis is updated recursively via the recursion, Vt = (1\u2212 \u03b3t)Vt\u22121 + \u03b3tV\u0303t, where \u03b3t is a step-size.\nGROUSE and GRASTA [4, 19] are online PCA and matrix completion algorithms. GRASTA can be applied to estimate principal subspaces incrementally on subsampled data. Both of these methods are online and use rank-one updatation of the principal subspace at each iteration. We have compared our online subspace estimation algorithm with GROUSE. GRASTA is an online robust subspace tracking algorithm and can be applied on subsampled data and specifically matrix completion problems. The authors proposed an `1-norm based fidelity term that measures the error between the subspace estimate and the outlier corrupted observations. The robustness of GRASTA is attributed to this `1-norm based cost. Their formulation of the subspace estimation involves the minimization of a non-convex function in an augmented Lagrangian framework. This optimization is carried out in an alternating fashion using the well known ADMM [7] for estimating a set of parameters involving the weights, the sparse outlier vector and the dual vector in the augmented Lagrangian framework. For fixed estimated values of these parameters, they employ an incremental gradient descent to solve for the low dimensional subspace. Note that the solution obtained is not the optimum of the combined non-convex function of GRASTA. In the experimental results section, we will present comparisons between GRASTA and our recursive robust PCA algorithm.\nRecursive covariance estimation [6] is straight-forward, and the principal components can be extracted via standard eigen-decompositions. Boutsidis et al. [6] consider\nefficient variants of this idea, and provide elegant performance bounds. The approach does not however scale to high-dimensional data as the covariance cannot practically be stored in memory for situations involving very large data sets as those considered in our work.\nIn [8], Candes et al. formulated Robust PCA (RPCA) as separating a matrix into a low rank (L) and a sparse matrix (S), i.e., data matrix X \u2248 L + S. They proposed Principal Component Pursuit (PCP) method to robustly find the principal subspace by decomposing into L and S. They showed that both L and S can be computed by optimizing an objective function which is a linear combination of nuclear norm on L and `1 norm on S. Recently, Lois and Vaswani [25] proposed an online RPCA problem to solve two interrelated problems, matrix completion and online robust subspace estimation. The authors have some assumptions including a good estimate of the initial subspace and that the basis of the subspace is dense. Though the authors have shown correctness of their algorithm under these assumptions, these assumptions are often not practical. In another recent work, Ha and Barber [17] proposed an online RPCA algorithm when X = (L+ S)C where C is a data compression matrix. They proposed an algorithm to extract L and S when the data X are compress sensed. This problem is quite interesting in its own right but not something pursued in our work presented here. Feng et al. [13] solved RPCA using a stochastic optimization approach. The authors have shown that if each observation is bounded, then their solution converges to the batch mode RPCA solution, i.e., their sequence of robust subspaces converges to the \u201ctrue\u201d subspace. Hence, they claimed that as the \u201ctrue subspace\u201d (subspace recovered by RPCA) is robust, so is their online estimate. Though their algorithm is online, the optimization steps ( Algorithm 1 in [13]) are expensive for high-dimensional data. In an earlier paper, Feng et al. [12] proposed a deterministic approach to solve RPCA (dubbed DHR-PCA) for high-dimensional data. They also showed that they can achieve maximal robustness, i.e., a breakdown point of 50%. They proposed a robust computation of the variance matrix and then performed PCA on this matrix to get robust PCs. This algorithm is suitable for very high dimensional data. As most of our real applications in this paper are in very high dimensions, we find DHR-PCA to be well suited to carry out comparisons with. Finally, we would like to refer the readers to an excellent source of references on RPCA in a recent MS thesis [36].\nMotivation for our work: Our work is motivated by the work presented by Hauberg et al. [18], who recently showed that for a data set drawn from a zero-mean multivariate Gaussian distribution, the average subspace spanned by the data coincides with the leading principal component. This idea is sketched in Fig. 1. Given, {xi}Ni=1 \u2282 RD, the 1-dimensional subspace spanned by each xi is a point on the Grassmann manifold (Sec. 2). Hauberg et al. then compute the average of these subspaces on the Grassmannian using an \u201cextrinsic\u201d metric, i.e. the Euclidean, distance. Besides the theoretical insight, this formulation gave rise to highly efficient algorithms. Unfortunately, the extrinsic approach is limited to one-dimensional subspaces, and Hauberg et al. resort to deflation methods to estimate higher dimensional subspaces. We overcome this limitation by using an intrinsic metric, extend the theoretical analysis of Hauberg et al., and provide an efficient online algorithm for subspace estimation. We further propose an online robust subspace averaging algorithm akin to online RPCA and proved that in the limit our proposed method returns the first K robust principal components. Moreover, we provide a proof of statistical robustness of our recursive PC estimator."}, {"heading": "2 An Online Linear Subspace Learning Algorithm", "text": "In this section, we propose an efficient online linear subspace learning algorithm for finding the principal components of a data set. We first briefly discuss the geometry of the Riemannian manifold of K-dimensional linear subspaces in RD. Then, we will present an online algorithm to get the firstK principal components of theD-dimensional data vectors."}, {"heading": "2.1 The Geometry of Subspaces", "text": "The Grassmann manifold (or the Grassmannian) is defined as the set of allK-dimensional linear subspaces in RD and is denoted Gr(K,D), where D \u2265 K. A special case of the Grassmannian is when K = 1, i.e., the space of one-dimensional subspaces of RD, which is known as the real projective space (denoted by RPD). A point X \u2208 Gr(K,D) can be specified by a basis, X , i.e., a set of K linearly independent vectors in RD (the columns of X) that spans X . We say X = Col(X) if X is a basis of X , where Col(.) is the column span operator. We have included a brief note on the geometry of the Grassmannian in the appendix section. As the Grassmannian is geodesically complete, one can extend the geodesics on the Grassmannian indefinitely [2, 11]. Given X ,Y \u2208 Gr(K,D), with their respective orthonormal basis X and Y , the unique geodesic \u0393YX : [0, 1]\u2192 Gr(K,D) between X and Y is given by:\n\u0393YX (t) = span ( XV\u0302 cos(\u0398t) + U\u0302 sin(\u0398t) ) (3)\nwith \u0393YX (0) = X and \u0393 Y X (1) = Y , where, U\u0302 \u03a3\u0302V\u0302 T = (I \u2212 XXT )Y (XTY )\u22121 is the \u201cthin\u201d Singular value decomposition (SVD), and \u0398 = arctan \u03a3\u0302. The length of the geodesic constitutes the geodesic distance on Gr(K,D), d : Gr(K,D) \u00d7 Gr(K,D) \u2192 R+ \u222a {0} which is as follows: Given X ,Y with respective orthonormal bases X and Y ,\nd2(X ,Y) , \u221a\u221a\u221a\u221a K\u2211 i=1 (arccos(\u03c3i)) 2, (4)\nwhere U\u0304\u03a3V\u0304 T = XTY be the SVD of XTY , and, [\u03c31, . . . , \u03c3K ] = diag(\u03a3). Here arccos(\u03c3i) is known as the ith principal angle between subspace X and Y ."}, {"heading": "2.2 The Intrinsic Grassmann Average (IGA)", "text": "We now consider intrinsic averages1 (IGA) on the Grassmannian. For the existence and uniqueness of IGA, we need to define an open ball on the Grassmannian. Using the geodesic distance (14) we define an open ball of radius r centered at X \u2208 Gr(K,D) as B(X , r) = {Y \u2208 Gr(K,D)|d(X ,Y) < r}. Let \u03ba be the maximum of the sectional curvature in the ball. Then, we call this ball \u201cregular\u201d [24] if 2r \u221a \u03ba < \u03c0. Using the results in [35], we know that, for RPD withD \u2265 2, \u03ba = 1, while for generalGr(K,D) with min(K,D) \u2265 2, 0 \u2264 \u03ba \u2264 2. So, on Gr(K,D) the radius of a \u201cregular geodesic ball\u201d is < \u03c0/2\u221a2, for min(K,D) \u2265 2 and on RPD, D \u2265 2, the radius is < \u03c0/2.\nLet X1, . . . ,XN be independent samples on Gr(K,D) drawn from a distribution P (X ), then we can define an intrinsic averageM\u2217 as:\nM\u2217 = argmin M\u2208Gr(K,D) N\u2211 i=1 d2 ( M,Xi ) (5)\nOn Gr(K,D), IGA exists and is unique if the support of P (X ) is within a \u201cregular geodesic ball\u201d of radius < \u03c0/2\u221a2 [3]. Note that for RPD, we can choose this bound to be \u03c0/2. In the rest of the paper, we have assumed that data points on Gr(K,D) are within a \u201cregular geodesic ball\u201d of radius < \u03c0/2\u221a2 unless otherwise specified. With this assumption, the IGA is unique. Note that this assumption is needed for proving the theorem presented below.\nThe IGA may be computed using a Riemannian steepest descent, but this is computationally expensive and requires selecting a suitable step-size [30]. Recently Chakraborty et al. [10] proposed a simple and efficient inductive (intrinsic) mean estimator:\nM1 = X1 , (\u2200k \u2265 1) ( Mk+1 = \u0393 Xk+1 Mk ( 1 k + 1 )) (6)\nThis approach only needs a single pass over the data set to estimate the IGA. Consequently, Eq. 6 has linear complexity in the number of observations. Furthermore, it is a truly online algorithm as each iteration only needs one new observation.\nEquation 6 merely performs repeated geodesic interpolation, which is analogous to standard recursive estimators of Euclidean averages: Consider observations xk \u2208 RD, k = 1, . . . , N . Then the Euclidean average can be computed recursively by moving an appropriate distance away from the kth estimator mk towards xk+1 on the straight line joining xk+1 and mk. The inductive algorithm (6) for computing the IGA works in the same way and is entirely based on traversing geodesics in Gr(K,D) and without requiring any optimization.\nTheorem 1. (Weak Consistency [10]) Let X1, . . . ,XN be samples on Gr(K,D) drawn from a distribution P (X ). Then MN (6) converges to the IGA of {Xi}Ni=1 in probability as N \u2192\u221e.\nTheorem 2. (Convergence rate) Let X1, . . . ,XN be samples on Gr(K,D) drawn from a distribution P (X ). Then Eq. 6 has a linear convergence rate.\nProof. See the appendix section.\n1These are also known as Fr\u00e9chet means [23, 15]."}, {"heading": "2.3 Principal Components as Grassmann Averages", "text": "Following Hauberg et al. [18] we pose the linear dimensionality reduction as an averaging problem on the Grassmannian. We consider an intrinsic Grassmann average (IGA), i.e. an average using the geodesic distance, which allow us to consider K > 1 dimensional subspaces. We then propose an online linear subspace learning and show that for the zero-mean Gaussian data, the expected IGA on Gr(K,D), i.e., expected K-dimensional linear subspace, coincides with the first K principal components.\nGiven {xi}Ni=1, the algorithm to compute the IGA to get the leading K-dimensional principal subspace is sketched in Algorithm 1.\nAlgorithm 1: The IGA algorithm to compute PCs Input: {xi}Ni=1 \u2282 RD , K > 0 Output: {v1, . . . ,vK} \u2282 RD 1 Partition the data {xj}Nj=1 into blocks of size D \u00d7K ; 2 Let the ith block be denoted by, Xi = [xi1, . . . ,xiK ] ; 3 Orthogonalize each block and let the orthogonalized block be denoted by Xi ; 4 Let the subspace spanned by each Xi be denoted by Xi \u2208 Gr(K,D) ; 5 Compute IGA,M\u2217, of {Xi} ; 6 Return the K columns of an orthogonal basis ofM\u2217; these span the principal K-subspace.\nLet {Xi} be the set of K-dimensional subspaces as constructed by IGA in Algorithm 1. Moreover, assume that the maximum principal angle between Xi and Xj is < \u03c0/2 \u221a 2, for all i 6= j. This condition is needed to ensure that the IGA exists and is unique on Gr(K,D). The condition can be ensured if the angle between xl and xk is < \u03c0/2\u221a2, for all xl,xk belonging to different blocks. For xl,xk in the same block, the angle must be < \u03c0/2. Note that, this assumption is needed to prove Theorem 3. In practice, even if IGA is not unique, we find a local minimizer of Eq. 5 [23], which serves as the principal subspace.\nTheorem 3. (Relation between IGA and PCA) Let us assume that xi \u223cN (0,\u03a3), for all i. Using the same notations as above, the jth column of M converges to the jth principal vector of {xi}Ni=1, j = 1, . . . ,K as N \u2192\u221e, i.e., in the limit, M spans the principal K-subspace,M\u2217, whereM\u2217 is defined as in Eq. 5.\nProof. Let Xi be the corresponding orthonormal basis of Xi, i.e., Xi spans Xi,for all i. The IGA, M\u2217 can be computed using Eq. 5. Let, Xi = [xi1 . . .xiK ] where and let xij be samples drawn from N(0,\u03a3). Let, M = [M1 . . .MK ] be an orthonormal basis of M\u2217. The distance between Xi and M\u2217 is defined as d2(Xi,M\u2217) =\u2211K j=1(arccos((Si)jj)) 2, where U\u0304iSiV\u0304 Ti = M TXi be the SVD, and (Si)jj \u2265 0 (we use (A)lmto denote (l,m)th entry of matrix A). As arccos is a decreasing function and a bijection on [0, 1], we can write an alternative form of Eq. 5 as follows:\nM\u2217 = argmax M N\u2211 i=1 K\u2211 j=1 ((Si)jj) 2 (7)\nIn fact the above alternative form can also be derived using a Taylor expansion of the RHS of Eq. 5. Note that, in the above equation Si is a function of M . It is easy to see that (MTXi)lm \u223c N (0, \u03c32Ml), l = 1, . . . ,K, m = 1, . . . ,K. Also, (MTXiV\u0304i)lm \u223c N (0, \u03c32Ml), l = 1, . . . ,K,m = 1, . . . ,K as V\u0304i is orthogonal. Thus, (Si)ll = (U\u0304 T i M TXiV\u0304i)ll \u223c N (0, \u03c32U\u0304ilMl). So, \u2211K j=1(Si) 2 jj \u223c \u0393( 12 \u2211K j=1 \u03c3 2 U\u0304ijMj , 2)\nand E[ \u2211K j=1(Si) 2 jj ] = \u2211K j=1 \u03c3 2 U\u0304ijMj\n. Now, as N \u2192 \u221e, RHS of Eq. 7 becomes E[ \u2211K j=1(Si) 2 jj ]. In order to maximize E[ \u2211K j=1(Si) 2 jj ] = \u2211K j=1 \u03c3 2 U\u0304ijMj , U\u0304ij should be the left singular vectors of MTXi, and Mj should be the jth eigenvector of \u03a3, for all j = 1, . . . ,K. Hence, M spans the principal subspace,M\u2217.\nNow, using Theorem 1 and Theorem 3, we replace the line 5 of the IGA Algorithm 1 by Eq. 6 to get an online subspace learning algorithm that we call, Recursive IGA (RIGA), to compute leading K principal components, K \u2265 1."}, {"heading": "3 A Robust Online Linear Subspace Learning Algorithm", "text": "Let {X1,X2, \u00b7 \u00b7 \u00b7 ,XN} \u2282 Gr(K,D),K < D be inside a regular geodesic ball of radius < \u03c0/2 \u221a 2 s.t., the Fr\u00e9chet Medain (FMe) exists and is unique. Let X1, X2, \u00b7 \u00b7 \u00b7 , XN be the corresponding orthonormal bases, i.e., Xi spans Xi, for all i. The FMe can be computed via the following minimization:\nM\u2217 = arg min M N\u2211 i=1 d(Xi,M) (8)\nWith a slight abuse of notation, we use the notationM\u2217 (M ) to denote both the FM and the FMe (and their orthonormal basis). The FMe is robust as was shown in [14], hence we call our estimator Robust IGA (RoIGA). In the following theorem, we will prove that RoIGA leads to the robust PCA in the limit as the number of the data samples goes to infinity. An algorithm to compute RoIGA is obtained by simply replacing Step 5 of Algorithm 1 by computation of RoIGA via minimization of Eq. 8 instead of Eq. 5. This minimization can be achieved using the Riemannian steepest descent, but instead, here we use the stochastic gradient descent of batch size 5 to compute RoIGA. As at each iteration, we need to store only 5 samples, the algorithm is online. The update step for each iteration of the online algorithm to compute RoIGA (we refer to our online RoIGA algorithm as Recursive RoIGA (RRIGA)) is as follows:\nM1 = X1 , Mk+1 = ExpMk ( Exp\u22121Mk(Xk+1)\n(k + 1)d(Mk,Xk+1)\n) (9)\nwhere, k \u2265 1, Exp and Exp\u22121 are Riemannian exponential and inverse exponential functions (see supplementary section for the definition of these maps). We refer the readers to [5] for the consistency proof of the estimator.\nTheorem 4. (Robustness of RoIGA) Assuming the above hypotheses and notations, as N \u2192\u221e, the columns of M converge to the robust principal vectors of the {xi}Ni=1, where M is the orthonormal basis ofM\u2217 as defined in Eq. 8.\nProof. Let, Xi = [xi1 \u00b7 \u00b7 \u00b7xiK ] and xij be i.i.d. samples drawn from N(0,\u03a3). Let, M = [M1 \u00b7 \u00b7 \u00b7MK ] be an orthonormal basis ofM. Define the distance between Xi and M by d(Xi,M) = \u221a\u2211K j=1(arccos((Si)jj)) 2, where U\u0304iSiV Ti = M TXi be the SVD, and (Si)jj \u2265 0. Since arccos is a decreasing function and is a bijection on [0, 1], we can rewrite Eq. 8 alternatively as follows:\nM\u2217 = arg max M N\u2211 i=1 \u221a\u221a\u221a\u221a K\u2211 j=1 ((Si)jj)2 (10)\nIn fact the above alternative form can also be derived using a Taylor expansion of the RHS of Eq. 8.\nFrom the proof of Theorem 3, we know that \u2211K j=1((Si)jj) 2 \u223c \u0393( 12 \u2211K j=1 \u03c3 2 U\u0304ijMj , 2).\nSo, \u221a\u2211K\nj=1((Si)jj) 2 \u223c Ng( 12 \u2211K j=1 \u03c3 2 U\u0304ijMj , \u2211K j=1 \u03c3 2 U\u0304ijMj ), where Ng is the Nak-\nagami distribution [27]. Now, lim N\u2192\u221e N\u2211 i=1 \u221a\u221a\u221a\u221a K\u2211 j=1 ((Si)jj)2 = E[ \u221a\u221a\u221a\u221a K\u2211 j=1 ((Si)jj)2].\nE[ \u221a\u2211K\nj=1((Si)jj) 2] = \u221a 2\u0393( \u2211K j=1 \u03c3 2 U\u0304ijMj + 0.5)/\u0393( \u2211K j=1 \u03c3 2 U\u0304ijMj\n), where \u0393 is the well known gamma function. It is easy to see that as \u0393 is an increasing function,\nE[ \u221a\u2211K\nj=1((Si)jj) 2] is maximized iff \u2211K j=1 \u03c3 2 U\u0304ijMj\nis maximized, i.e., when M spans the principal K-subspace.\nNow, if we contrast with the objective function of RIGA in Eq. 7, there we had to maximize E[ \u2211K j=1((Si)jj) 2] = \u2211K j=1 \u03c3 2 U\u0304ijMj . Thus, E[ \u221a\u2211K j=1((Si)jj) 2] = \u03c1(m) , \u221a 2\u0393(m + 0.5)/\u0393(m), where m = \u2211K j=1 \u03c3 2 U\u0304ijMj . Hence, the influence func-\ntion [21] of \u03c1 is proportional to \u03c8(m) , \u2202E[\n\u221a\u2211K j=1((Si)jj) 2]\n\u2202m and if we can show that limm\u2192\u221e \u03c8(m) = 0, then we can claim that our objective function in Eq. 10 is robust [21].\nNow, \u03c8(m) = \u0393(m)\u0393(m+ 0.5)\u03c6(m+0.5)\u2212\u03c6(m)\u0393(m)2 , where \u03c6 is the polygamma function [1] of order 0. After some simple calculations, we get,\nlim m\u2192\u221e (\u03c6(m + 0.5)\u2212 \u03c6(m)) = lim m\u2192\u221e log(1 + 1/(2m))\n+ lim m\u2192\u221e \u221e\u2211 k=1 ( Bk ( 1 kmk \u2212 1 k(m + 0.5)k )) = lim\nm\u2192\u221e log(1 + 1/(2m)) + 0 = 0\nHere, {Bk} are the Bernoulli numbers of the second kind [32]. So, limm\u2192\u221e \u03c8(m) = 0.\nWe would like to point out that the outlier corrupted data can be modeled using a mixture of independent random variables, Y1, Y2, where Y1 \u223c N (0,\u03a31) (to model non-outlier data samples) and Y2 \u223c N (\u00b5,\u03a32) (to model outliers), i.e., (\u2200i), xi = w1Y1 + (1 \u2212 w1)Y2, w1 > 0 is generally large, so that the probability of drawing outliers is low. Then as the mixture components are independent, (\u2200i), xi \u223c N ((1 \u2212 w1)\u00b5, w 2 1\u03a31 + (1\u2212 w1)2\u03a32). A basic assumption in any online PCA algorithm is that data is centered. So, in case the data is not centered (similar to the model of xi), the first step of PCA would be to centralize the data. But then the algorithm can not be made online, hence our above assumption that xi \u223c N (0,\u03a3) is a valid assumption in an online scenario. But, in a general case, after centralizing the data as the first step of PCA, the above theorem is valid."}, {"heading": "4 Experimental Results", "text": "In this section, we present an experimental evaluation of the proposed estimators on both real and synthetic data. Our overall experimental findings are that the RIGA and RRIGA estimators are more accurate and faster than other online linear and robust linear\nsubspace estimators. We believe that the higher accuracy in RIGA and RRIGA can be attributed to the use of intrinsic geometry of the Grassmannian in our geometric formulation. Specifically, finding the full set of PCs is cast as an intrinsic averaging problem on the Grassmannian achieved using a recursive estimator in both cases. From a computational perspective, in the online PCA case, we attribute the efficiency observed in the experiments to RIGA being an optimization and parameter free method. In the case of RRIGA, the reasons for accuracy and efficiency are much more complicated. At this juncture, we speculate the reason to be that our geometric formulation leads to directly finding the subspaces using a recursive scheme as opposed to methods that incrementally update basis of the subspace in an alternating fashion with no convergence guarantees. In the following, we consider RIGA and RRIGA separately."}, {"heading": "4.1 Online Linear Subspace Estimation", "text": "Baselines: Here, we present a comparison with Oja\u2019s rule and the online version of EM-PCA (Sec. 1.1). For Oja\u2019s rule we follow common guidelines and consider stepsizes \u03b3t = \u03b1/D \u221a t with \u03b1-values between 0.005 and 0.2. For EM-PCA we follow the recommendations from Capp\u00e9 [9] and use step-sizes \u03b3t = 1/t\u03b1 with \u03b1-values between 0.6 and 0.9 along with Polyak-Ruppert averaging. For GROUSE, we have chosen the stepsize to be 0.1.\n(Synthetic) Gaussian Data: Theorem 3 states that the RIGA estimates coincide in expectation with the leading principal subspace when the data are drawn from a zeromean Gaussian distribution. We empirically verify this for an increasing number of observations drawn from randomly generated zero-mean Gaussians. We measure the expressed variance which is the variance captured by the estimated subspace divided by the variance captured by the true principal subspace:\nExpressed Variance = \u2211K k=1 \u2211N n=1(x T nv (est) k )\n2\u2211K k=1 \u2211N n=1(x T nv (true) k ) 2 \u2208 [0, 1]. (11)\nAn expressed variance of 1 implies that the estimated subspace captures as much variance as the principal subspace. The top panel of Fig. 2 shows the mean (\u00b1 one standard deviation) expressed variance of RIGA over 150 trials. It is evident that for the Gaussian data, the RIGA estimator does indeed converge to the true principal subspace.\nA key aspect of any online estimator is that it should be stable and converge fast to a good estimate. Here, we compare RIGA to the above-mentioned baselines. Both Oja\u2019s rule and EM-PCA require a step-size to be specified, so we consider a larger selection of such step-sizes. The middle panel of Fig. 2 shows the expressed variance as a function of number of observations for different estimators and step-sizes. EM-PCA was found to be quite stable with respect to the choice of step-size, though it does not seem to converge to a good estimate. Oja\u2019s rule, on the other hand, seems to converge to a good estimate, but its practical performance is critically dependent on the step-size. GROUSE is seen to oscillate for small data size however, with a large number of samples, it yields a good estimate. On the other hand, RIGA is parameter-free and is observed to have good convergence properties.\nIn the bottom panel of Fig. 2, we perform a stability analysis of GROUSE and RIGA. Here, for a fixed value ofN , we generate a data matrix and perform 200 independent runs on the data matrix and report the mean (\u00b1 one standard deviation) expressed variance. As can be seen from the figure, RIGA is very stable in comparison to GROUSE.\nHuman Body Shape: Online algorithms are generally well-suited for solving largescale problems as by construction, they should have linear time-complexity in the number of observations. As an example, we consider a large collection of threedimensional scans of human body shape [31]. This dataset contains N = 21862 meshes which each consist of 6890 vertices in R3. Each mesh is, thus, viewed as a D = 6890 \u00d7 3 = 20670 vector. We estimate a K = 10 dimensional principal subspace using Oja\u2019s rule, EM-PCA, GROUSE and RIGA respectively. The average reconstruction error (squared distance between the original data and its estimate) over all meshes are 16.8 mm for Oja\u2019s rule, 1.9 mm for EM-PCA, 1.4 mm for GROUSE, and 1.0 mm for RIGA. Note that both Oja\u2019s rule and EM-PCA explicitly minimize the reconstruction error, while RIGA does not but yet outperforms the baseline methods. We speculate that this is due to RIGA\u2019s excellent convergence properties and it being a parameter free algorithm is not bogged down by the hard problem of step-size tuning confronted in the baseline algorithms used here.\nSanta Claus Conquers the Martians: We now consider an even larger scale experiment and consider all frames of the motion picture Santa Claus Conquers the Martians (1964)2. This consist of N = 145, 550 RGB frames of size 320\u00d7 240, corresponding to an image dimension of D = 230, 400. We estimate a K = 10 dimensional subspace using Oja\u2019s rule, EM-PCA, GROUSE and RIGA respectively. Again, we measure the accuracy of the different estimators via the reconstruction error. Pixel intensities are scaled to be between 0 and 1. Oja\u2019s rule gives an average reconstruction error of 0.054, EM-PCA gives 0.025, while RIGA and GROUSE give 0.023. Here RIGA and EM-PCA give roughly equally good results, with a slight advantage to RIGA. GROUSE gives same reconstruction error as RIGA. Oja\u2019s rule does not fare as well. As with the shape data, it is interesting to note that RIGA outperforms some of the other baseline methods on the error measure that they optimize even though RIGA optimizes a different measure."}, {"heading": "4.2 Robust Subspace Estimation", "text": "We now present the comparitive experimental evaluation of robust extension (RRIGA). Here we use DHR-PCA and GRASTA as baseline and measure performance using the reconstruction error (RE). We have used UCSD anomaly detection database [26] and Extended YaleB database [16].\nUCSD anomaly detection database: This data contains images of pedestrian movement on walkways captured by a stationary mounted camera. The crowd density on the walkway varies from sparse to very crowded. The anomaly includes bikers, skaters, carts, people in wheelchair etc.. This database is divided in two sets: \u201cPeds1\u201d (people are walking towards the camera) and \u201cPeds2\u201d (people are walking parallel to the camera plane). In \u201cPeds1\u201d there are 36 training and 34 testing videos where each video contains 180 frames of dimension 158\u00d7 238 (D = 37604). In \u201cPeds2\u201d there are 12 training and 16 testing videos containing varying samples of dimension 240\u00d7360 (D = 86400). The test frames do not have anomalous activities. Some sample frames (with and without outliers) are shown in Fig. 3. We first extract K principal components on the training data (including anomalies) and then compute reconstruction error on the test frames (without anomalies) using the computed principal components. It is expected that if the\n2https://archive.org/details/SantaClausConquerstheMartians1964\nPC computation technique is robust, the reconstruction error will be good as PCs should not be affected by the anomalies in training samples. In Fig. 4, we compare performance of RRIGA with GRASTA and DHR-PCA in terms of RE and time required by varying K from 1 to 100. In terms of time it is evident that RRIGA is very fast compared to both GRASTA and DHR-PCA. RRIGA also outperforms both DHR-PCA and GRASTA in terms of RE. Moreover, it is evident that RRIGA scales very well both in terms of RE and computation time unlike it\u2019s competitors.\nYale ExtendedB database: This data contains 2414 face images of 38 subjects. We crop each image to make a 32 \u00d7 32 images (D = 1024). Due to varying lighting condition, some of the face images are shaded/ dark and appeared as outliers (this experimental setup is similar to the one in [22]). In Fig. 5 some sample face images (outlier and non-outlier) are shown. One can see that due to poor lighting condition, though the middle face in top row is a face image, it looks completely dark and an outlier. For testing, we have used 142 non-outlier face images of 38 subjects and the rest we used to extract PCs. We report RE (with varying K) and time required for both RRIGA, GRASTA and DHR-PCA in Fig. 6. From the figure it is evident that for small number of PCs (i.e., small K) RRIGA performs similar to DHR-PCA, while for larger K values, RRIGA outperforms DHR-PCA and GRASTA. In terms of time required,\nRRIGA is faster than both DHR-PCA and GRASTA."}, {"heading": "5 Conclusions", "text": "In this paper, we present a new geometric framework for estimating the full set of principal components from given data. We present two online algorithms, one each for estimating PCA and RPCA respectively. Since they are inherently online, they are naturally scalable to very large data sets as demonstrated in the experimental results section. The key idea in the geometric framework involves computing an intrinsic Grassmann average as a proxy for the principal linear subspace. We show that the if the samples are drawn from a Gaussian distribution, the intrinsic Grassmann average coincides with the principal subspace in expectation. Further, for our online recursive RPCA algorithm, we proved that the estimated principal components are statistically robust. Our algorithms have a linear time complexity and linear convergence rate. Unlike most other online algorithms there are not step-sizes or other parameters to tune; a most useful property in practical settings. Our future work will focus on application of our geometric approach to the matrix completion problem."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Riemannian Geometry of the Grassmannian", "text": "The Grassmann manifold (or the Grassmannian) is defined as the set of allK-dimensional linear subspaces in RD and is denoted Gr(K,D), where K \u2208 Z+, D \u2208 Z+, D \u2265 K. A special case of the Grassmannian is when K = 1, i.e., the space of one-dimensional subspaces of RD, which is known as the real projective space (denoted by RPD). A\npoint X \u2208 Gr(K,D) can be specified by a basis, X , i.e., a set of K linearly independent vectors in RD (the columns of X) that spans X . We say X = Col(X) if X is a basis of X , where Col(.) is the column span operator. The set of all D \u00d7 K matrices of rank K, K < D is defined as the Stiefel manifold St(K,D). Now, consider a Riemannian metric g\u0303 on St(K,D) defined as follows: Given U\u0303 , V\u0303 \u2208 TSt(K,D), g\u0303X(U\u0303X , V\u0303X) = trace((XTX)\u22121U\u0303TX V\u0303X). It is easy to see that the general Lie group GL(k) acts isometrically, freely and properly on St(K,D). Moreover, Gr(K,D) can be identified with the quotient space St(K,D)/GL(K). Hence, the projection map \u03a0 : St(K,D) \u2192 Gr(K,D) is a Riemannian submersion, where \u03a0(X) , Col(X). Moreover, the triplet (St(K,D),\u03a0, Gr(K,D) is a principal fiber bundle.\nAt every point X \u2208 St(K,D), we can define vertical space, VX \u2282 TXSt(K,D) to be Ker(\u03a0\u2217X). Further, given g\u0303, we define the horizontal space, HX to be the g\u0303orthogonal complement of VX . Now, from the theory of principal bundles, for every vector field U on Gr(K,D), we define the horizontal lift of U to be the unique vector field U\u0303 on St(K,D) for which U\u0303X \u2208 HX and \u03a0\u2217X U\u0303X = U\u03a0(X), \u2200X \u2208 St(K,D). As, \u03a0 is a Riemannian submersion, the isomorphism \u03a0\u2217X |HX : HX \u2192 T\u03a0(X)Gr(K,D) is an isometry from (HX , g\u0303X) to (T\u03a0(X)Gr(K,D), g\u03a0(X)), where g is the Riemannian metric on Gr(K,D) defined as:\ng\u03a0(X)(U\u03a0(X), V\u03a0(X)) = g\u0303X(U\u0303X , V\u0303X) = trace((XTX)\u22121U\u0303TX V\u0303X) (12)\nwhere, U, V \u2208 T\u03a0(X)Gr(k, n) and \u03a0\u2217X U\u0303X = U\u03a0(X), \u03a0\u2217X V\u0303X = V\u03a0(X), U\u0303X \u2208 HX and V\u0303X \u2208 HX . Given X ,Y \u2208 Gr(K,D), with their respective orthonormal basis X and Y , the unique geodesic \u0393YX : [0, 1]\u2192 Gr(K,D) between X and Y is given by:\n\u0393YX (t) = span ( XV\u0302 cos(\u0398t) + U\u0302 sin(\u0398t) ) (13)\nwith \u0393YX (0) = X and \u0393 Y X (1) = Y , where, U\u0302 \u03a3\u0302V\u0302 T = (I \u2212 XXT )Y (XTY )\u22121 is the \u201cthin\u201d Singular value decomposition (SVD), (i.e., U\u0302 is D \u00d7K and V\u0302 is K \u00d7K column orthonormal matrix, and \u03a3\u0302 is K \u00d7K diagonal matrix), and \u0398 = arctan \u03a3\u0302. The Inverse Exponential map Exp\u22121X (Y) = \u03a0\u2217X(U\u0302\u0398V\u0302 ). The Exponential map ExpX (U) = span(XV cos(\u03a8)VT + U sin(\u03a8)VT ), where U = U\u03a8VT be the SVD of U . The length of the geodesic constitutes the geodesic distance on Gr(K,D), d : Gr(K,D)\u00d7 Gr(K,D)\u2192 R+ \u222a {0} which is as follows: Given X ,Y with respective orthonormal bases X and Y ,\nd2(X ,Y) , \u221a\u221a\u221a\u221a K\u2211 i=1 (arccos(\u03c3i)) 2, (14)\nwhere U\u0304\u03a3V\u0304 T = XTY is the SVD of XTY , and, [\u03c31, . . . , \u03c3K ] = diag(\u03a3). Here arccos(\u03c3i) is known as the ith principal angle between subspace X and Y ."}, {"heading": "6.2 Proof of Theorem 2", "text": "Theorem 2. (Convergence rate) Let X1, . . . ,XN be the samples on Gr(K,D) drawn from a distribution P (X ). Then Eq. (5), in the main paper, has a linear convergence rate.\nProof. Let X1, . . . ,XN be the samples drawn from a distribution P (X ) on Gr(K,D).\nLetM be the IGA of {Xi}. Then, using triangle inequality we have,\nd(Mk,M) \u2264 d(Mk\u22121,Mk) + d(Mk\u22121,M)\n= 1\nk d(Mk\u22121,Xk) + d(Mk\u22121,M)\n\u2264 1 k\n( d(Mk\u22121,M) + d(Xk,M) ) + d(Mk\u22121,M)\nHence, d(Mk,M) d(Mk\u22121,M)\n\u2264 ( 1 + 1\nk + d(Xk,M) k d(Mk\u22121,M) ) Now, since d(Mk\u22121,M) and d(Xk,M) are finite as {Xi}s are within a geodesic ball of finite radius, with k \u2192\u221e, d(Mk,M)d(Mk\u22121,M) \u2264 1. But, the equality holds only ifM lies on the geodesic betweenMk\u22121 and Xk. Let, l < \u221e and letM lies on the geodesic betweenMk\u22121 and Xk for some k > l. AsM is fixed, using induction, one can easily show thatM can not lie on the same geodesic for all k > l. Hence, r < 1 and the convergence rate is linear."}], "references": [{"title": "Handbook of mathematical functions", "author": ["M. Abramowitz", "I.A. Stegun"], "venue": "Applied mathematics series,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1966}, {"title": "Riemannian geometry of grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Applicandae Mathematicae, 80(2):199\u2013220,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Riemannian lp center of mass: Existence, uniqueness, and convexity", "author": ["B. Afsari"], "venue": "Proceedings of the American Mathematical Society, 139(2):655\u2013673,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["L. Balzano", "R. Nowak", "B. Recht"], "venue": "Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 704\u2013711. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic gradient descent on riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Transactions on Automatic Control, 58(9):2217\u20132229,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Online principal components analysis", "author": ["C. Boutsidis", "D. Garber", "Z. Karnin", "E. Liberty"], "venue": "Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887\u2013901. SIAM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Online expectation-maximisation", "author": ["O. Capp\u00e9"], "venue": "Mixtures: Estimation and Applications, pages 31\u201353,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive frechet mean computation on the grassmannian and its applications to computer vision", "author": ["R. Chakraborty", "B.C. Vemuri"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 4229\u20134237,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistics on Special Manifolds", "author": ["Y. Chikuse"], "venue": "Springer, February", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust pca in high-dimension: A deterministic approach", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "arXiv preprint arXiv:1206.4628,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Online robust pca via stochastic optimization", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "Advances in Neural Information Processing Systems, pages 404\u2013412,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The geometric median on riemannian manifolds with application to robust atlas estimation", "author": ["P.T. Fletcher", "S. Venkatasubramanian", "S. Joshi"], "venue": "NeuroImage, 45(1):S143\u2013S152,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Les \u00e9l\u00e9ments al\u00e9atoires de nature quelconque dans un espace distanci\u00e9", "author": ["M. Fr\u00e9chet"], "venue": "Annales de l\u2019institut Henri Poincar\u00e9, volume 10, pages 215\u2013310. Presses universitaires de France,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1948}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 23(6):643\u2013660,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust pca with compressed data", "author": ["W. Ha", "R.F. Barber"], "venue": "Advances in Neural Information Processing Systems, pages 1936\u20131944,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable robust principal component analysis using grassmann averages", "author": ["S. Hauberg", "A. Feragen", "R. Enficiaud", "M.J. Black"], "venue": "TPAMI,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental gradient on the grassmannian for online foreground and background separation in subsampled video", "author": ["J. He", "L. Balzano", "A. Szlam"], "venue": "CVPR, pages 1568\u20131575,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of educational psychology, 24(6):417,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1933}, {"title": "Robust statistics", "author": ["P.J. Huber"], "venue": "Springer,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust dictionary learning with capped l 1-norm", "author": ["W. Jiang", "F. Nie", "H. Huang"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, pages 3590\u2013 3596. AAAI Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Riemannian center of mass and mollifier smoothing", "author": ["H. Karcher"], "venue": "Communications on pure and applied mathematics, 30(5):509\u2013541,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1977}, {"title": "Probability, convexity, and harmonic maps with small image i: uniqueness and fine existence", "author": ["W.S. Kendall"], "venue": "Proceedings of the London Mathematical Society, 3(2):371\u2013406,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Anomaly detection in crowded scenes", "author": ["V. Mahadevan", "W. Li", "V. Bhalodia", "N. Vasconcelos"], "venue": "CVPR, volume 249, page 250,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The m-distribution-a general formula of intensity distribution of rapid fading", "author": ["M. Nakagami"], "venue": "Statistical Method of Radio Propagation,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1960}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15(3):267\u2013273,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1982}, {"title": "On lines and planes of closest fit to system of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine, 2(11):559\u2013572,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1901}, {"title": "Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements", "author": ["X. Pennec"], "venue": "Journal of Mathematical Imaging and Vision, 25(1):127\u2013154,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Dyna: A model of dynamic human shape in motion", "author": ["G. Pons-Moll", "J. Romero", "N. Mahmood", "M.J. Black"], "venue": "SIGGRAPH, 34(4):120:1\u2013120:14, Aug.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "The umbral calculus", "author": ["S. Roman"], "venue": "Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "EM algorithms for pca and spca", "author": ["S. Roweis"], "venue": "Advances in neural information processing systems, pages 626\u2013632,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611\u2013622,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "Sectional curvatures of grassmann manifolds", "author": ["Y.-C. Wong"], "venue": "Proceedings of the National Academy of Sciences, 60(1):75\u201379,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1968}, {"title": "Online robust principal component analysis for background subtraction: A system evaluation on toyota car data", "author": ["X. Xu"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix.", "startOffset": 214, "endOffset": 218}, {"referenceID": 32, "context": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix.", "startOffset": 318, "endOffset": 322}, {"referenceID": 17, "context": "[18] noted that the average of all one-dimensional subspaces spanned by normally distributed data coincides with the leading principal component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "There are several existing approaches in literature that tackle the online PCA and the online Robust PCA problems and we discuss some of these approaches here: Oja\u2019s rule [28] is a classic online estimator for the leading principal components of a dataset.", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "EM-PCA [33] is usually derived for probabilistic PCA, but is easily be adapted to the online setting [9].", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "EM-PCA [33] is usually derived for probabilistic PCA, but is easily be adapted to the online setting [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "GROUSE and GRASTA [4, 19] are online PCA and matrix completion algorithms.", "startOffset": 18, "endOffset": 25}, {"referenceID": 18, "context": "GROUSE and GRASTA [4, 19] are online PCA and matrix completion algorithms.", "startOffset": 18, "endOffset": 25}, {"referenceID": 6, "context": "This optimization is carried out in an alternating fashion using the well known ADMM [7] for estimating a set of parameters involving the weights, the sparse outlier vector and the dual vector in the augmented Lagrangian framework.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Recursive covariance estimation [6] is straight-forward, and the principal components can be extracted via standard eigen-decompositions.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "[6] consider", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In [8], Candes et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "In another recent work, Ha and Barber [17] proposed an online RPCA algorithm when X = (L+ S)C where C is a data compression matrix.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "[13] solved RPCA using a stochastic optimization approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Though their algorithm is online, the optimization steps ( Algorithm 1 in [13]) are expensive for high-dimensional data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "[12] proposed a deterministic approach to solve RPCA (dubbed DHR-PCA) for high-dimensional data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Finally, we would like to refer the readers to an excellent source of references on RPCA in a recent MS thesis [36].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "[18], who recently showed that for a data set drawn from a zero-mean multivariate Gaussian distribution, the average subspace spanned by the data coincides with the leading principal component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "As the Grassmannian is geodesically complete, one can extend the geodesics on the Grassmannian indefinitely [2, 11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 10, "context": "As the Grassmannian is geodesically complete, one can extend the geodesics on the Grassmannian indefinitely [2, 11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 0, "context": "Given X ,Y \u2208 Gr(K,D), with their respective orthonormal basis X and Y , the unique geodesic \u0393X : [0, 1]\u2192 Gr(K,D) between X and Y is given by: \u0393X (t) = span ( XV\u0302 cos(\u0398t) + \u00db sin(\u0398t) ) (3)", "startOffset": 97, "endOffset": 103}, {"referenceID": 23, "context": "Then, we call this ball \u201cregular\u201d [24] if 2r \u221a \u03ba < \u03c0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "Using the results in [35], we know that, for RP withD \u2265 2, \u03ba = 1, while for generalGr(K,D) with min(K,D) \u2265 2, 0 \u2264 \u03ba \u2264 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "On Gr(K,D), IGA exists and is unique if the support of P (X ) is within a \u201cregular geodesic ball\u201d of radius < \u03c0/22 [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 28, "context": "The IGA may be computed using a Riemannian steepest descent, but this is computationally expensive and requires selecting a suitable step-size [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 9, "context": "[10] proposed a simple and efficient inductive (intrinsic) mean estimator:", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "(Weak Consistency [10]) Let X1, .", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "1These are also known as Fr\u00e9chet means [23, 15].", "startOffset": 39, "endOffset": 47}, {"referenceID": 14, "context": "1These are also known as Fr\u00e9chet means [23, 15].", "startOffset": 39, "endOffset": 47}, {"referenceID": 17, "context": "[18] we pose the linear dimensionality reduction as an averaging problem on the Grassmannian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "5 [23], which serves as the principal subspace.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "As arccos is a decreasing function and a bijection on [0, 1], we can write an alternative form of Eq.", "startOffset": 54, "endOffset": 60}, {"referenceID": 13, "context": "The FMe is robust as was shown in [14], hence we call our estimator Robust IGA (RoIGA).", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "We refer the readers to [5] for the consistency proof of the estimator.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Since arccos is a decreasing function and is a bijection on [0, 1], we can rewrite Eq.", "startOffset": 60, "endOffset": 66}, {"referenceID": 25, "context": "agami distribution [27].", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "tion [21] of \u03c1 is proportional to \u03c8(m) , \u2202E[ \u221a\u2211K j=1((Si)jj) 2] \u2202m and if we can show that limm\u2192\u221e \u03c8(m) = 0, then we can claim that our objective function in Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "10 is robust [21].", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "5) \u0393(m)2 , where \u03c6 is the polygamma function [1] of order 0.", "startOffset": 45, "endOffset": 48}, {"referenceID": 30, "context": "Here, {Bk} are the Bernoulli numbers of the second kind [32].", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "For EM-PCA we follow the recommendations from Capp\u00e9 [9] and use step-sizes \u03b3t = 1/t with \u03b1-values between 0.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "Expressed Variance = \u2211K k=1 \u2211N n=1(x T nv (est) k ) 2 \u2211K k=1 \u2211N n=1(x T nv (true) k ) 2 \u2208 [0, 1].", "startOffset": 90, "endOffset": 96}, {"referenceID": 29, "context": "As an example, we consider a large collection of threedimensional scans of human body shape [31].", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "We have used UCSD anomaly detection database [26] and Extended YaleB database [16].", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "We have used UCSD anomaly detection database [26] and Extended YaleB database [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "Due to varying lighting condition, some of the face images are shaded/ dark and appeared as outliers (this experimental setup is similar to the one in [22]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Given X ,Y \u2208 Gr(K,D), with their respective orthonormal basis X and Y , the unique geodesic \u0393X : [0, 1]\u2192 Gr(K,D) between X and Y is given by: \u0393X (t) = span ( XV\u0302 cos(\u0398t) + \u00db sin(\u0398t) ) (13)", "startOffset": 97, "endOffset": 103}], "year": 2017, "abstractText": "Principal Component Analysis (PCA) is a fundamental method for estimating a linear subspace approximation to high-dimensional data. Many algorithms exist in literature to achieve a statistically robust version of PCA called RPCA. In this paper, we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces (the Grassmann manifold). Points on this manifold are defined as the subspaces spanned by K-tuples of observations. We show that the intrinsic Grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a Gaussian distribution. Similar results are also shown to hold for the RPCA. Further, we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate. When the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance (accuracy and computation time) gain over a recently published RPCA methods with publicly accessible code. We have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. Experimental results depicting stability of our proposed method are also presented. Furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online RPCA algorithm. In terms of reconstruction error and time required, both our algorithms outperform the competition.", "creator": "LaTeX with hyperref package"}}}