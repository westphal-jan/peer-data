{"id": "1702.01147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Predicting Target Language CCG Supertags Improves Neural Machine Translation", "abstract": "neural machine translation ( nmt ) models are able to partially better learn syntactic information from sequential lexical information. still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. this work aims to answer two challenging questions : 1 ) does explicitly modeling source information or target language syntax should help nmt? 2 ) is tight integration of words and syntax better than multitask training? we introduce syntactic information available in the form of ccg supertags either in the source as an extra feature in the embedding, or in the compiler target, specifically by interleaving the target supertags with the word sequence. our results updated on wmt data show that explicitly modeling syntax improves machine translation quality for english - german, a high - resource skill pair, and for english - romanian, a particular low - resource pair and also several syntactic phenomena including prepositional phrase attachment. furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "histories": [["v1", "Fri, 3 Feb 2017 20:31:34 GMT  (141kb,D)", "http://arxiv.org/abs/1702.01147v1", null], ["v2", "Tue, 18 Jul 2017 12:07:45 GMT  (226kb,D)", "http://arxiv.org/abs/1702.01147v2", "Accepted at the Second Conference on Machine Translation (WMT17). This version includes more results regarding target syntax for Romanian-&gt;English and reports fewer results regarding source syntax"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["maria nadejde", "siva reddy", "rico sennrich", "tomasz dwojak", "marcin junczys-dowmunt", "philipp koehn", "alexandra birch"], "accepted": false, "id": "1702.01147"}, "pdf": {"name": "1702.01147.pdf", "metadata": {"source": "CRF", "title": "Syntax-aware Neural Machine Translation Using CCG", "authors": ["Maria N\u0103dejde", "Siva Reddy", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Alexandra Birch"], "emails": ["m.nadejde@sms.ed.ac.uk,", "rico.sennrich}@ed.ac.uk", "t.dwojak@amu.edu.pl,", "junczys@amu.edu.pl,", "phi@jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed captured by these models. In a detailed analysis, Bentivogli et al. (2016) show that NMT significantly improves over phrase-based SMT, in par-\nticular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled.\nRecent work which incorporates additional linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work we perform a thorough investigation of rich syntactic features in NMT. We examine the benefit of adding syntactic information in the source, as an extra feature in the embedding layer following the approach of Sennrich and Haddow (2016). We also propose a method for generating syntactic information in the target: tightly coupling words and syntax by interleaving target syntactic representation with the word sequence. We compare this to loosely coupling words and syntax using multitask solutions, where the shared parts of the model are trained to produce either a target sequence of words or supertags in a similar fashion to Luong et al. (2016).\nWe use CCG syntactic categories (Steedman, 2000), also known as supertags, to represent syntax explicitly. Supertags provide global syntactic information locally at the lexical level. They encode subcategorization information, capturing short and long range dependencies and attachments, and also tense and morphological aspects of the word in a given context. Consider the sentence in Figure 1. This sentence contains two PP attachments and could lead to several disambiguation possibilities (\u201cin\u201d can attach to \u201cNetanyahu\u201d or \u201creceives\u201d, and \u201cof\u201d can attach to \u201ccapital\u201d,\nar X\niv :1\n70 2.\n01 14\n7v 1\n[ cs\n.C L\n] 3\nF eb\n2 01\n7\n\u201cNetanyahu\u201d or \u201creceives\u201d ). These alternatives may lead to different translations in other languages. However the supertag S\\NP/PP/NP of \u201creceives\u201d indicates that the preposition \u201cin\u201d attaches to the verb, and the supertag NP\\NP/NP of \u201cof\u201d indicates that it attaches to \u201ccapital\u201d, thereby resolving the ambiguity.\nOur research contributions are as follows:\n\u2022 We show that both source and target language syntax improves translation quality for English\u2194German, English\u2194Romanian as measured by BLEU.\n\u2022 We present a fine grained analysis of Syntaxaware NMT (SNMT) and show consistent gains when looking at different linguistic phenomena and sentence lengths.\n\u2022 We propose three novel approaches to integrating target syntax at word level in the decoder, by serializing CCG supertags in the target word sequence and by multitasking with either a shared or distinct attention model and decoder.\n\u2022 Our results suggest that a tight coupling of target words and syntax (by serializing) improves translation quality more than the decoupled signal from multitask training."}, {"heading": "2 Related work", "text": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks. Sennrich and Haddow (2016) generalize the embedding layer of NMT to include explicit linguistic features such as dependency relations and partof-speech tags and we extend their work to using CCG supertags. Other architectures have been proposed to integrate source-side syntax such as Eriguchi et al. (2016) who use the phrase structure of the source sentence to guide the recurrence and attention model in a tree-to-sequence model. Luong et al. (2016) co-train a translation model and a syntactic parser which share the encoder. Our\nmulti-task models extend their work to attentionbased NMT models and we implement a novel multi-task architecture where the tasks share not just the encoder, but also the attention model and the decoder, keeping only the softmax layer separate.\nApplying more tightly coupled linguistic factors on the target for NMT has been previously investigated. Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an nbest list produced by a phrase-based MT system. In recent work Mart\u0131\u0301nez et al. (2016) implemented an NMT model which first generated lemmas and morphology, and then used these to generate the word form. Unfortunately no real gain was reported for these experiments. In our work we do not focus on model architectures, and instead we explore the more general problem of including syntax in NMT: comparing source and target-side syntax and comparing tightly and loosely coupled syntactic information.\nPrevious work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) showed promising results. However the factored models originally proposed for statistical machine translation (Koehn and Hoang, 2007) suffered from data sparsity and did not consider longer sequences as context. In this work we take advantage of the expressive power of neural networks to learn representations that generate both words and CCG supertags."}, {"heading": "3 Modeling Syntax in NMT", "text": "CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT captures long range dependencies using long-term memory, short-term memory is cheap and reliable. Supertags can help by allowing the model to rely more on local information (short-term) and not having to rely heavily on long-term memory.\nConsider a decoder that has to generate the following sentences:\n1. What(S[wq]/(S[q]/NP ))/N city is(S[q]/PP )/NP the Taj Mahal in?\n2. WhereS[wq]/(S[q]/NP ) is(S[q]/NP )/NP the Taj Mahal?\nIf the decoding starts with predicting \u201cWhat\u201d, it is ungrammatical to omit the preposition \u201cin\u201d, and if the decoding starts with predicting \u201cWhere\u201d, it is ungrammatical to predict the preposition. Here the decision to predict \u201cin\u201d depends on the first word, a long range dependency. However if we rely on CCG supertags, the supertags of both these sequences look very different. The supertag (S[q]/PP)/NP for the verb \u201cis\u201d in the first sentence indicates that a preposition is expected in future context. Furthermore it is likely to see this particular supertag of the verb in the context of (S[wq]/(S[q]/NP))/N but it is unlikely in the context of S[wq]/(S[q]/NP). Therefore a succession of local decisions based on CCG supertags will result in the correct prediction of the preposition in the first sentence, and omitting the preposition in the second sentence. Since the vocabulary of CCG supertags is much smaller than that of possible words, the NMT model will do a better job at generalizing over and predicting the correct CCG supertags sequence.\nCCG supertags also help during encoding if they are given in the input, as we saw with the case of PP attachment in Figure 1. Translation of the correct verb form and agreement can be improved with CCG since supertags also encode tense, morphology and agreements. For example, in the sentence \u201cIt is going to rain\u201d, the supertag (S[ng]\\NP[expl])/(S[to]\\NP) of \u201cgoing\u201d indicates the current word is a verb in continuous form looking for an infinitive construction on the right, and an expletive on the left.\nWe explore the effect of syntax by using CCG supertags either on the source-side (encoder) or target-side (decoder) as follows.\nSource-side syntax When modeling the sourceside syntactic information, we include the CCG supertags as extra features in the NMT encoder using the framework of (Sennrich and Haddow, 2016). The model of (Bahdanau et al., 2015) is extended by learning a separate embedding for sev-\neral source-side features such as the word itself or its part-of-speech. All feature embeddings are concatenated into one embedding vector which is used in all parts of the encoder model instead of the word embedding.\nThe baseline features are the subword units obtained using byte-pair-encoding (BPE, (Sennrich et al., 2016b)) together with the annotation of the subword structure using IOB format by marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate tag (O) is used if a symbol corresponds to the full word. The word level supertag is replicated for each BPE unit.\nFigure 1 gives an example of the source-side feature representation. It shows the segmentation of the noun Netanyahu into three BPE sub-units Net+ an+ yahu as well as the duplication of the supertag NP for each of these sub-units.\nTarget-side syntax When modeling the targetside syntactic information we consider different strategies of coupling the CCG supertags with the translated words in the decoder: serializing, multitasking with shared encoder and multitasking with distinct softmax layer.\n\u2022 Serializing In this paper we propose a tight integration in the decoder of the syntactic representation and the surface forms. For each target word we include its supertag as an extra token before the first BPE sub-unit of the word. With this arhitecture a single decoder learns to predict both the target supertags and the target words conditioned on previous syntactic and lexical context. Figure 1 gives an example of the target-side representation in the case of serializing. The supertag NP corresponding to the word Netanyahu is included only once before the three BPE subunits Net+ an+ yahu.\n\u2022 Multitasking (1) \u2013 shared encoder A loose coupling of the syntactic representation and the surface forms can be achieved by co-training a\ntranslation model with a secondary prediction task, in our case CCG supertagging. In the multitask framework (Luong et al., 2016) the encoder part is shared while the decoder is different for each of the prediction tasks: translation and tagging. Different from Luong et al. we also train a separate attention model for each task. Another difference is that we experiment with both source and target syntax as a secondary task, while Luong et al. only used source syntax. We use EasySRL to label the parallel corpus with CCG supertags instead of using a corpus with gold annotations.\n\u2022 Multitasking (2) \u2013 distinct softmax In this approach only the softmax layer is distinct for each task while the encoder, attention model and decoder are shared. The input to the two softmax layers is indentical (the context vector, the previous hidden state and the previous predicted word) but the second softmax layer predicts CCG supertags. The cost of predicting the wrong supertag is added to the cost of predicting the wrong target word. We do not add a connection between the CCG supertag at time step t \u2212 1 and the word predicted at time step t as that would require a modification of the beam search.\nEach of the three models has its disadvantages. The serializing approach increases the length of the target sequence which might lead to loss of information learned at lexical level. For the multitasking (1) approach there is no explicit way to constrain the number of predicted words and tags to match. The multitasking (2) approach does not condition the prediction of target words on the syntactic context."}, {"heading": "4 Experimental Setup and Evaluation", "text": ""}, {"heading": "4.1 Data and methods", "text": "We train the neural MT systems on all the parallel data available at WMT16 (Bojar et al., 2016) for the German\u2194English and Romanian\u2194English language pairs. The English side of the parallel data is annotated with CCG lexical tags using EasySRL (Lewis et al., 2015). Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data sets in Table 1. During training we validate our models with BLEU on devel-\nopment sets: newstest2013 for German\u2194English and newsdev2016 for Romanian\u2194English. We evaluate the systems using BLEU (Papineni et al., 2002) and report results on newstest2016 for both language pairs.\nAll the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit.1 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014).\nWords are segmented into sub-units that are learned jointly for source and target using BPE (Sennrich et al., 2016b), resulting in a vocabulary size of 85,000. The vocabulary size for CCG supertags was 500.\nWe select the best single models according to BLEU on the development set and use the four best single models for the ensembles. Due to small differences in pre-processing with CCG supertags, our results for the baseline systems are similar but do not match the results reported by (Sennrich et al., 2016a).\nFor the experiments with source-side features we use the BPE sub-units and the IOB tags as baseline features. We keep the total word embedding size fixed to 500 dimensions. When adding the CCG feature we allocate 135 dimensions for CCG supertags, 360 for BPE sub-units and 5 for IOB tags.\nFor the experiments where we add CCG supertags in the target sequence we increase the maximum length of sentences from 50 to 100. On average the length of English sentences for newstest2013 in BPE representation is 22.7, while the average length when adding the CCG supertags is 44. Increasing the length of the target recurrence results in larger memory consumption and slower training.2\n1https://github.com/rsennrich/nematus 2Roughly 10h30 per 100,000 sentences (20,000 batches)\nfor SNMT compared to 6h for NMT."}, {"heading": "4.2 Results", "text": "In this section we evaluate whether our syntaxaware NMT model (SNMT) with source-side and target-side CCG supertags improves translation quality as compared to a baseline NMT model (Bahdanau et al., 2015; Sennrich et al., 2016a). In tables and figures we use the symbol \u201c*\u201d to indicate that syntactic information is used on the source (eg. *de-en), the target (eg. de-en*) or both (eg. *de-en*).\nCCG on the source-side We first evaluate the impact of source-side CCG features on overall translation quality. We report results for English\u2192German, a high-resource language pair, and for English\u2192Romanian, a low-resource language pair. Both the target languages are morphologically richer than the source language, with German exhibiting more word order flexibility than Romanian.\nWe report BLEU scores in Table 2 and Table 3 for both the best single models and ensemble models. However, we will only refer to the results with ensemble models since these are generally better.\nAdding the source-side CCG feature improves translation quality for both language pairs as well as for both single and ensemble models. For English\u2192German BLEU increases by up to 0.7 points and for English\u2192Romanian by up to 0.4 points.\nAlthough the training data for English\u2192German is large, the CCG supertags still improve translation quality. This suggests\nthat syntax provides complementary information that the baseline NMT model is not able to learn from the source word sequence alone.\nNext we make a finer grained analysis of the impact of source-side syntactic features by looking at a breakdown of BLEU scores with respect to different linguistic constructions. We classify sentences into different linguistic constructions based on the CCG supertags that appear in them, e.g., the presence of category (NP\\NP)/(S/NP) indicates subordinate construction. Figure 2 shows the breakdown of BLEU scores for the following linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), questions and subordinate clauses (subordinate). We report the number of sentences for each category in Table 4.\nWe see a consistent improvement for English\u2192German for all linguistic constructions when using the CCG supertags, especially\nfor questions which involve a lot of movement, and control constructions which involve long range verb agreements. In the case of English\u2192Romanian, where word order is closer between languages, we still see improvements across most constructions when using the CCG supertags.\nCCG on the target-side Here we evaluate the impact of using the target-side CCG supertags for the Romanian\u2192English and German\u2192English translation directions. In Table 5 and Table 6 we report results with BLEU.\nThe target-side CCG feature improves BLEU scores by 0.9 for Romanian\u2192English. For German\u2192English, the ensemble model with target-side CCG supertags improves BLEU scores by 0.6. These results suggest that the baseline NMT decoder benefits from modeling the global syntactic information locally via supertags.\nWe also see consistent improvements across most linguistic constructions for both language pairs as reported in Figure 2. In particular, the increase in BLEU scores for the prepositional phrase and subordinate constructions suggests that target word order is improved. For the control and raising constructions we don\u2019t see any improvement for German\u2192English, while the improvement for English\u2192German was 1.5 BLEU points. These results suggest that syntactic information is more\nhelpful for the control and raising constructions when translating into German.\nCCG on both source-side and target-side Next we combine source-side and target-side syntax in a German\u2192English syntax-aware NMT system. We use CCG supertags on the target-side and dependency labels as source-side features. Although the dependency labels do not encode global syntactic information they disambiguate the grammatical function of words. In Table 7 we compare systems that use syntax either on the source-side or target-side with a system that uses syntax on both sides. First we observe that the source-side dependency labels improve translation quality by only 0.1 BLEU points as compared to the baseline NMT system, while the improvement with target-side CCG supertags is more than 0.5 BLEU points. When using both source-side and target-side factors the syntax-aware NMT system performs better than the baseline by roughly 1 BLEU point.\nNext we observe from Figure 2 that having both source and target syntax helps improve BLEU scores for all syntactic phenomena. As compared to the SNMT system with only target syntax, BLEU scores improve by 0.5 for control and raising constructs, 0.3 for prepositional phrase attachment, 0.4 for subordinate constructs and 0.4 for sentences involving coordination.\nFinally we compare the systems with respect to sentence length. Figure 3 shows the difference in BLEU points between the baseline NMT system and the syntax-aware NMT system with respect to the length of the source sentence measured in BPE sub-units. We report the number of sentences for each category in Table 8. For the majority of systems and sentence lengths adding CCG supertags helps. When using target CCG supertags for German\u2192English there is a decrease in BLEU for short sentences. However when using both source and target syntax, BLEU improves in this case as well. Furthermore combining the\nsource and target syntax seems to help the most for longer sentences. This is an encouraging result since including target CCG supertags increases the target sequence which may lead to information being forgotten.\nWe conclude that when syntactic information is available for both the source language and the target language it is most beneficial to integrate syntax in both the encoder and the decoder.\nSerializing vs multitasking One open question we address in this work is how tightly do we need to integrate syntax in the NMT system. For this purpose we compare different ways of coupling the target word sequence and CCG supertags: serializing, multitasking (1) with shared encoder and multitasking (2) with distinct softmax layer. In our proposed SNMT system with target CCG supertags we take the serializing approach. The different approaches are described in Section 3. For the multitasking (1) approach we explore two scenarios: in the first scenario the secondary task is\nto predict the source-side CCG supertags while in the second scenario it is to predict the target-side CCG supertags.\nFirst we compare the multitask (1) NMT system predicting source CCG supertags and the SNMT system with CCG features on the source. The results in Table 2 show that multitasking also improves BLEU scores, suggesting that the encoder learns a better latent syntactic representation of the source sentence. However the SNMT system performs slightly better, by 0.2 BLEU points.\nNext we compare a SNMT system with target CCG supertags and a multitask (1) NMT system which predicts target CCG supertags as a secondary task. The results in Table 6 show that the multitask approach does not improve BLEU scores as compared to the baseline NMT system, suggesting that the shared encoder is not able to learn a good representation of target syntax. In contrast, the SNMT system which integrates syntax in the decoder improves translation by 0.5 BLEU points.\nFinally the multitask (2) system which predicts independently the target CCG supertags and words does not perform better than the NMT baseline. The BLEU scores are shown in Table 6. Since neither of the two multitasking models improve translation quality as measured by BLEU we conclude that a tight integration of the target syntax and word sequence is important. Conditioning the prediction of words on their corresponding CCG supertag is what sets SNMT apart from the other models and improves the translation quality."}, {"heading": "4.3 Discussion", "text": "Our preliminary experiments show that source syntax improves translation when translating from a morphologically poor language into a morphologically rich language. It would be interesting to evaluate the impact of target syntax for such a language pair. In the future we plan to use the Hindi CCGBank (Ambati et al., 2016) to run experiments for English\u2194Hindi.\nAlthough the focus of this paper is not improving CCG tagging, we can measure that SNMT is accurate at predicting CCG supertags. We compare the CCG sequence predicted by the SNMT models with that predicted by EasySRL and obtain the following accuracies: 93.2 for Romanian\u2192English, 95.6 for German\u2192English, 95.8 for German\u2192English with both source and target syntax. This result might be useful for\ndownstream applications using CCG supertagging, such as multi-lingual question answering.\nWe conclude by giving a few examples in Figure 4 for which the syntax-aware NMT system produced more grammatical translations than the baseline NMT system.\nIn the example DE-EN* Question the baseline NMT system translates the preposition \u201cu\u0308ber\u201d twice as \u201cabout\u201d. The SNMT predicts the correct CCG supertag for \u201cwhat\u201d which expects to be followed by a sentence and not a preposition: NP/(S[dcl]/NP). Therefore the SNMT correctly re-orders the preposition \u201cabout\u201d at the end of the question.\nIn the example DE-EN* Subordinate the baseline NMT system fails to correctly attach \u201cPrentiss\u201d as an object and \u201chis wife\u201d as a modifier to the verb \u201ccalled (bezeichnete)\u201d in the subordinate clause. In contrast the SNMT system predicts the correct sub-categorization frame of the verb \u201cdescribed\u201d and correctly translates the entire predicate-argument structure.\nIn the example *DE - EN* Subordinate the baseline NMT system translates the sentence correctly while the SNMT drops the argument \u201conline\u201d. The SNMT* predicts the wrong CCG supertag (S[dcl]\\NP)/PP for the verb \u201cworked (arbeitete)\u201d, which guides the system to only translate the prepositional modifier and drop the sec-\nond argument. However the *SNMT* system which also has source syntactic factors predicts the correct subcategorization frame for the verb ((S[dcl]\\NP)/PP)/NP and subsequently translates the second argument.\nIn the example RO-EN* Coordination the baseline NMT system makes an agreement mistake between the subject \u201cSanders\u201d and the verb \u201care\u201d. In contrast the SNMT system correctly identifies that the coordination is between sentences, and outputs the correct verb form \u201cis\u201d."}, {"heading": "5 Conclusions", "text": "Our results suggest that having the notion of explicit syntax, here in the form of CCG supertags, in the encoder or the decoder improves machine translation for both English\u2194German and English\u2194Romanian language pairs. Earlier work on syntax-aware NMT mainly modeled syntax in the encoder while our results suggest modeling syntax in the decoder is also useful. Moreover by modeling syntax in both encoder and decoder we obtain the most improvement over the baseline NMT system, in particular for longer sentences and syntactic phenomena such as prepositional attachment and coordination. Finally our results show that a tight integration of syntax in the decoder improves translation quality while decoupling of target words and syntax does not."}, {"heading": "A Appendix", "text": "We give a few more examples in Figure 5 and Figure 6.\nIn the example RO-EN Preposition the baseline NMT system drops the preopositional modifier of the verb \u201cconsider\u201d. The SNMT predicts the correct subcategorization frame for the verb see and correctly translates the prepositional modifier \u201cas a problem\u201d.\nIn the second DE-EN Subordinate example the baseline NMT inverts the role of the arguments for the verb \u201csupport (unterstu\u0308tzen)\u201d. The SNMT system produces the correct syntactic structure of the subordinate clause and translates \u201csie\u201d as \u201cthey\u201d, the subject.\nIn the example EN-DE Raising the baseline NMT system translates the raising construct \u201cwants ... to be seen\u201d with the incorrect infinitive verb form \u201czu sehen\u201d. In contrast the SNMT system produces the correct translation for a subordinate sentence \u201cgesehen werden\u201d. Furthermore the SNMT system produces the correct nominative inflection for the coordinated subject of the raising construct \u201cseine Mitgliedschaft im Schachclub ... und sein freundlicher Kontakt\u201d, while the NMT system inflects the second part as accusative \u201cseinen freundlichen Kontakt\u201d.\nIn the example EN-DE Suboordination the baseline NMT system mistranslates the subordinate clause \u201cwhich lists 17 faculty members\u201d as \u201cdie 17 Fakulta\u0308ten Mitglieder\u201d which drops the verb \u201clists\u201d and the relative pronoun \u201cwhich\u201d. In contrast the SNMT correctly translates the verb at the end of the clause as well as the relative pronoun \u201cin denen 17 Fakulta\u0308tsmitglieder aufgefu\u0308hrt sind\u201d. A mistake that both system make is the incorrect disambiguation of the verb \u201ctook\u201d which is translated as \u201cnahmen\u201d instead of \u201cbesuchten\u201d.\nIn the second EN-DE Suboordination example the baseline NMT system mistranslates the subordinate clause \u201cwho say the same of Trump\u201d, as it fails to correctly order the target verb at the end of the clause \u201cdie sagen , das Gleiche von Trump\u201d. In contrast the SNMT system translates the verb at the end of the subordinate clause \u201cdie das Gleiche von Trump sagen\u201d.\nIn the example EN-DE Question and Coordination the baseline NMT system does not predict the correct target order of the verb \u201cwaste (vergeuden)\u201d and its direct object \u201cpolitical capital (politisches Kapital)\u201d. In contrast the SNMT system\ncorrectly reorders the target verb \u201cverschwenden\u201d at the end of the clause. Moreover the SNMT system correctly identifies the coordinated subject \u201cParis or Berlin\u201d and correctly inflects the auxiliary verb \u201cshould\u201d to the plural form in German \u201csollten\u201d."}], "references": [{"title": "Hindi CCGbank: CCG Treebank from the Hindi Dependency Treebank", "author": ["Bharat Ram Ambati", "Tejaswini Deoskar", "Mark Steedman."], "venue": "Language Resources and Evaluation.", "citeRegEx": "Ambati et al\\.,? 2016", "shortCiteRegEx": "Ambati et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR)..", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Ccg supertags in factored statistical machine translation", "author": ["Alexandra Birch", "Miles Osborne", "Philipp Koehn."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, PA, USA,", "citeRegEx": "Birch et al\\.,? 2007", "shortCiteRegEx": "Birch et al\\.", "year": 2007}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation. Association for", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Comput. Linguist. 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "Proceedings of the IWSLT 2016.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Factored translation models", "author": ["Philipp Koehn", "Hieu Hoang."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pages 868\u2013876.", "citeRegEx": "Koehn and Hoang.,? 2007", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Joint a* ccg parsing and semantic role labelling", "author": ["Mike Lewis", "Luheng He", "Luke Zettlemoyer."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Lewis et al\\.,? 2015", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proceedings of International Conference on Learning Representations (ICLR 2016).", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Factored Neural Machine Translation Architectures", "author": ["Mercedes Garc\u0131\u0301a Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "In International Workshop on Spoken Language Translation (IWSLT\u201916)", "citeRegEx": "Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Using dependency order templates to improve generality in translation", "author": ["Arul Menezes", "Chris Quirk."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. pages 1\u20138.", "citeRegEx": "Menezes and Quirk.,? 2007", "shortCiteRegEx": "Menezes and Quirk.", "year": 2007}, {"title": "Using factored word representation in neural network language models", "author": ["Jan Niehues", "Thanh-Le Ha", "Eunah Cho", "Alex Waibel."], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation", "author": ["Rico Sennrich."], "venue": "Transactions of the Association for Computational Linguistics 3:169\u2013182.", "citeRegEx": "Sennrich.,? 2015", "shortCiteRegEx": "Sennrich.", "year": 2015}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 371\u2013", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational Linguistics, Austin, Texas, pages", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems. NIPS\u201914, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Ghkm rule extraction and scope-3 parsing in moses", "author": ["Philip Williams", "Philipp Koehn."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation. pages 388\u2013394.", "citeRegEx": "Williams and Koehn.,? 2012", "shortCiteRegEx": "Williams and Koehn.", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al.", "startOffset": 61, "endOffset": 127}, {"referenceID": 7, "context": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al.", "startOffset": 61, "endOffset": 127}, {"referenceID": 1, "context": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al.", "startOffset": 61, "endOffset": 127}, {"referenceID": 15, "context": "Recent work which incorporates additional linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even", "startOffset": 79, "endOffset": 126}, {"referenceID": 21, "context": "Recent work which incorporates additional linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even", "startOffset": 79, "endOffset": 126}, {"referenceID": 20, "context": "mation in the source, as an extra feature in the embedding layer following the approach of Sennrich and Haddow (2016). We also propose a method for generating syntactic information in the target: tightly coupling words and syntax by interleav-", "startOffset": 91, "endOffset": 118}, {"referenceID": 15, "context": "We compare this to loosely coupling words and syntax using multitask solutions, where the shared parts of the model are trained to produce either a target sequence of words or supertags in a similar fashion to Luong et al. (2016).", "startOffset": 210, "endOffset": 230}, {"referenceID": 25, "context": "We use CCG syntactic categories (Steedman, 2000), also known as supertags, to represent syntax explicitly.", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 17, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 27, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 20, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 5, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 5, "context": ", 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al.", "startOffset": 75, "endOffset": 211}, {"referenceID": 5, "context": ", 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks.", "startOffset": 75, "endOffset": 231}, {"referenceID": 8, "context": "proposed to integrate source-side syntax such as Eriguchi et al. (2016) who use the phrase structure of the source sentence to guide the recurrence and attention model in a tree-to-sequence model.", "startOffset": 49, "endOffset": 72}, {"referenceID": 8, "context": "proposed to integrate source-side syntax such as Eriguchi et al. (2016) who use the phrase structure of the source sentence to guide the recurrence and attention model in a tree-to-sequence model. Luong et al. (2016) co-train a translation model and a syntactic parser which share the encoder.", "startOffset": 49, "endOffset": 217}, {"referenceID": 17, "context": "Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an nbest list produced by a phrase-based MT system.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "In recent work Mart\u0131\u0301nez et al. (2016) implemented an NMT model which first generated lemmas and morphology, and then used these to generate the word form.", "startOffset": 15, "endOffset": 39}, {"referenceID": 3, "context": "Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) showed promising results.", "startOffset": 75, "endOffset": 95}, {"referenceID": 13, "context": "factored models originally proposed for statistical machine translation (Koehn and Hoang, 2007) suffered from data sparsity and did not consider longer sequences as context.", "startOffset": 72, "endOffset": 95}, {"referenceID": 21, "context": "Source-side syntax When modeling the sourceside syntactic information, we include the CCG supertags as extra features in the NMT encoder using the framework of (Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 187}, {"referenceID": 1, "context": "The model of (Bahdanau et al., 2015) is extended by learning a separate embedding for several source-side features such as the word itself or its part-of-speech.", "startOffset": 13, "endOffset": 36}, {"referenceID": 23, "context": "The baseline features are the subword units obtained using byte-pair-encoding (BPE, (Sennrich et al., 2016b)) together with the annotation of the subword structure using IOB format by marking if", "startOffset": 84, "endOffset": 108}, {"referenceID": 15, "context": "In the multitask framework (Luong et al., 2016) the encoder part is shared while the decoder is different for each of the prediction tasks: translation and tagging.", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "The English side of the parallel data is annotated with CCG lexical tags using EasySRL (Lewis et al., 2015).", "startOffset": 87, "endOffset": 107}, {"referenceID": 19, "context": "We evaluate the systems using BLEU (Papineni et al., 2002) and report results on newstest2016 for both language pairs.", "startOffset": 35, "endOffset": 58}, {"referenceID": 1, "context": "All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit.", "startOffset": 67, "endOffset": 90}, {"referenceID": 22, "context": "1 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a) with minor modifications:", "startOffset": 55, "endOffset": 79}, {"referenceID": 12, "context": "we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014).", "startOffset": 51, "endOffset": 72}, {"referenceID": 23, "context": "(Sennrich et al., 2016b), resulting in a vocabulary size of 85,000.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "In this section we evaluate whether our syntaxaware NMT model (SNMT) with source-side and target-side CCG supertags improves translation quality as compared to a baseline NMT model (Bahdanau et al., 2015; Sennrich et al., 2016a).", "startOffset": 181, "endOffset": 228}, {"referenceID": 22, "context": "In this section we evaluate whether our syntaxaware NMT model (SNMT) with source-side and target-side CCG supertags improves translation quality as compared to a baseline NMT model (Bahdanau et al., 2015; Sennrich et al., 2016a).", "startOffset": 181, "endOffset": 228}, {"referenceID": 0, "context": "In the future we plan to use the Hindi CCGBank (Ambati et al., 2016) to run experiments for English\u2194Hindi.", "startOffset": 47, "endOffset": 68}], "year": 2017, "abstractText": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English\u2194German, a high-resource pair, and for English\u2194Romanian, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "creator": "LaTeX with hyperref package"}}}