{"id": "1702.01182", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Uncertainty-Aware Reinforcement Learning for Collision Avoidance", "abstract": "reinforcement engineering learning can enable complex, adaptive operator behavior to be learned automatically for autonomous robotic platforms. however, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. in this paper, we consider the specific case of constructing a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. in order to learn collision avoidance, the robot must experience collisions at training time. however, high - speed collisions, unusual even at training time, could damage the whole robot. a successful learning method must therefore proceed cautiously, experiencing only low - speed collisions until feasible it gains confidence. to this end, we present an uncertainty - aware model - based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. by formulating an uncertainty - dependent inverse cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity magnitude of the robot in settings where it has high confidence. our predictive model is based directly on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high - bandwidth sensors such as cameras. our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real - world quadrotor, and a real - world smart rc car. videos of the experiments can be found at", "histories": [["v1", "Fri, 3 Feb 2017 21:57:13 GMT  (8323kb,D)", "http://arxiv.org/abs/1702.01182v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["gregory kahn", "adam villaflor", "vitchyr pong", "pieter abbeel", "sergey levine"], "accepted": false, "id": "1702.01182"}, "pdf": {"name": "1702.01182.pdf", "metadata": {"source": "CRF", "title": "Uncertainty-Aware Reinforcement Learning for Collision Avoidance", "authors": ["Gregory Kahn", "Adam Villaflor", "Vitchyr Pong", "Pieter Abbeel", "Sergey Levine"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nPolicy search via reinforcement learning holds the promise of automating a wide range of decision making and control tasks in safety-critical domains, ranging from self-driving vehicles to drones. However, many reinforcement learning algorithms experience failures at training time, which can be catastrophic in safety-critical domains. Other reinforcement learning algorithms ensure safety by assuming complete state and environment knowledge at training time; however, these assumptions often severely restrict the feasibility of realworld robot deployment. Developing reinforcement learning algorithms that reason about perception and control in unknown environments, understand uncertainty, and explore safely is crucial to deploying reinforcement learning algorithms on safety-critical systems.\nOne of the central challenges in reinforcement learning is that a robot can only learn the outcome of an action by executing the action itself. Consider a robot learning to navigate an unknown environment while avoiding collisions. This scenario seemingly presents a quandary: the robot needs to learn how to avoid collisions in order to achieve the desired task, but to learn how to avoid collisions, the robot must experience (possibly\ncatastrophic) collisions during training. The robot can overcome this quandary by first experiencing gentle collisions in order to learn about the environment; once the robot is confident about the environment, the robot can avoid catastrophic failures in the future. Central to this approach is that the robot must be able to reason about its own uncertainty because these catastrophic failures are likely to occur in novel scenarios.\nConsider an example scenario in which an autonomous drone is learning to fly in an obstacle-rich building. If the drone encounters a novel scenario, the drone will likely crash because the novel scenario is not contained within the training distribution of the reinforcement learning algorithm policy. However, by reasoning about its own policy\u2019s uncertainty, the drone can safely interact with the environment and avoid catastrophic failures while also increasing the diversity of its training distribution.\nTo realize this kind of safe, uncertainty-aware navigation in unknown environments, we propose a model-based learning approach in which the robot learns a collision prediction model and uses estimates of the model\u2019s uncertainty to adjust its navigation strategy. By using a speed-dependent collision cost together with uncertainty-aware collision estimates, our navigation strategy naturally chooses to move cautiously when uncertainty is high so as to experience only harmless lowspeed collisions, and increases speed only in regions where the confidence of the prediction model is high.\nOur main contribution is an uncertainty-aware collision prediction model that enables a robot to learn how to accomplish a desired task in an unknown environment while only experiencing gentle collisions. The collision prediction\nar X\niv :1\n70 2.\n01 18\n2v 1\n[ cs\n.L G\n] 3\nF eb\n2 01\n7\nmodel takes as input the current robot observation and a sequence of controls, computes the probability of a collision occurring along with an estimate of its uncertainty, and outputs a speed-dependent collision cost. The speed-dependent collision cost is a function of the model and its uncertainty, which enables the robot to automatically avoid catastrophic high-speed collisions by acting cautiously in novel situations. We use a deep neural network for the collision prediction model, which allows the model to cope with raw, highdimensional sensory inputs. To obtain uncertainty estimates from the neural network, we leverage uncertainty estimation methods for discriminatively trained neural networks based on a combination of bootstrapping [5] and dropout [28, 7]. A model-based reinforcement learning algorithm then gathers samples using the neural network collision prediction model, which are aggregated and used to further improve the collision prediction model. Our empirical results demonstrate that a robot equipped with our uncertainty-aware neural network collision prediction model experiences substantially fewer dangerous collisions during training while still learning to achieve the desired task. We present an evaluation of our method with various parameter settings for both a simulated and real-world quadrotor, and a real-world RC car (Fig. 1), and demonstrate that our method offers a favorable tradeoff between trainingtime collisions and final task performance compared to baseline approaches that do not explicitly reason about uncertainty."}, {"heading": "II. RELATED WORK", "text": "In this work, we investigate how model-based reinforcement learning for robot collision avoidance can be made safe and reliable at both training and test time. Reinforcement learning has been applied to a wide range of robotic problems, ranging from locomotion and manipulation to autonomous helicopter flight [14, 4]. Model-free methods have been particularly popular due to their simplicity and favorable computational properties [24]. However, model-based methods are generally known to be more sample-efficient [3]. In this work, we adopt a model-based approach and learn an uncertainty-aware collision avoidance model; however, similar uncertainty estimation techniques could be extended also to model-free methods.\nSeveral model-based robotic learning algorithms have been proposed that explicitly reason about uncertainty [3, 26]. Uncertainty estimates have been used to perform both riskaverse and risk-seeking, optimistic exploration [19]. The role of uncertainty estimation in our work is to avoid unsafe actions at training time until the model has gained sufficient confidence, which is largely orthogonal and complementary to prior work that seeks to improve exploration in order to accelerate learning. Combining these two directions is a promising direction for future work.\nUncertainty-aware model-based reinforcement learning has been explored in previous work using Bayesian models [25, 1]. While our work is similar in the overall aim, one of the central goals of our method is to directly process raw inputs from high-bandwidth sensors such as cameras, which necessitates the use of rich and expressive models, such as deep neural\nnetworks. Uncertainty estimation for deep neural networks is substantially more challenging, since these models are inherently discriminative. Recent work has proposed to use a Bayesian formulation of neural networks based on dropout [8], as well as to use the bootstrap for exploration [22], but not, to the best of our knowledge, for uncertainty estimation for the purpose of safety. In this work, we demonstrate that combining both dropout and bootstrap can yield actionable uncertainty estimates for reinforcement learning tasks.\nThere is much prior work on safe robot control for safetycritical systems such as autonomous cars [29], legged robots [31], and quadrotors [20, 30, 9]. A number of recent works have sought to address the question of safety for learningbased robotic systems. Methods based on reachability provide appealing theoretical guarantees, but cannot cope with rich sensory input and are often difficult to scale to highdimensional systems [23, 17, 10]. Several works have suggested using discriminative models, including neural networks, to learn safety predictors [2]. These methods generally take the approach of training a model to predict whether an unsafe action will occur, and reverting to a hand-designed safety controller if such a potential failure is detected. Our method offers two advantages over this approach. First, by directly estimating model uncertainty, we do not rely on a discriminative safety estimator. This approach is preferred in environments where the model might encounter previously unseen inputs because a discriminative safety estimator cannot provide meaningful predictions for completely novel inputs; in short, the discriminative safety estimator may erroneously conclude that an unsafe environment is safe. In contrast, a statistical uncertainty prediction such as bootstrapping is more likely to estimate high uncertainty in novel environments. Secondly, our approach does not assume the existence of a manually designed safety control, but instead naturally reverts to more cautious exploratory behavior in the presence of uncertainty. This makes the approach more automated, and does not require a safety mechanism that can recover from arbitrary unsafe situations.\nWe use deep neural networks to estimate the probability of collision from raw sensory inputs. Combining deep networks with reinforcement learning has been an active area of research in recent years, with applications to video game playing [18], control of simulated robots [27, 16], and manipulation [15]. However, most of these applications focus on task complexity or learning speed, rather than explicitly considering uncertainty and safety during training. Prior work has considered safety at training time by using model-predictive control (MPC) with ground truth state information [11]. In contrast, our work does not assume any access to ground truth state, which is advantageous for real-world deployment."}, {"heading": "III. PRELIMINARIES", "text": "Our goal is to control a mobile robot, such as a quadrotor or a car, attempting to navigate an unknown environment. The task may be formally defined in terms of states x, actions u, dynamics xt+1 = f(xt,ut), and observations o. We use M to represent the environment, including any potential obstacles.\nWe assume the robot\u2019s objective is encoded as a scalar cost function of the form C(xt,ut,M) = CTASK(xt,ut) + 1COLL(xt,M)CCOLL(xt). That is, the cost consists of an obstacle-independent task term CTASK(xt,ut), which might include, for instance, flying to a desired position or in a desired direction, as well as an obstacledependent collision cost, which is given by the product of an indicator for collision 1COLL(xt,M), which is the only term that depends on the environment, and a collision cost CCOLL(xt) that may, for instance, penalize high-speed collisions more than relatively harmless low-speed collisions.\nIn a fully observable environment where M is known, the collision indicator can be evaluated exactly, and the problem can be solved by a standard optimal control method, such as the receding-horizon model-predictive control (MPC) approach we use in this work. In receding-horizon MPC, the robot solves an optimal control problem of the form\nmin ut,...,ut+H H\u2211 h=0 C(xt+h,ut+h,M) s.t. xt+h+1 = f(xt+h,ut+h)\nat each time step, it executes the action ut, advances to time step t + 1, and repeats the optimization, effectively performing replanning at each time step. In this work, we assume that the dynamics, which might correspond, for instance, to the equations of motion of a quadrotor, are known at least approximately in advance. We instead focus on estimation of the cost, which depends on the unknown environment M. If the environment is unknown and the indicator 1COLL(xt,M) cannot be estimated exactly, we can attempt instead to evaluate the probability of a collision using sensor observations, such as LIDAR or camera images. In this case, we can approximate the collision indicator according to\n1COLL(xt+h,M) \u2248 P (COLLt+h|xt,ut:t+hot). That is, we can estimate the probability of collision at a future time step t + h based on the current state xt, the sequence of actions ut:t+h that we intend to take, and the current observation ot, which might be used to deduce where the obstacles are located and thereby estimate the probability of collision, without prior knowledge about the environment.\nIn practice, we will slightly simplify the problem by predicting the probability of a collision at any time step h within the MPC horizon H . This approximation is not required, but yields a somewhat simpler model that we found performed equally well in practice, especially for relatively short-horizon MPC problems where CCOLL(xt+H) doesn\u2019t change much over the MPC horizon. In this case, the full approximate cost at time t+ h evaluated using observation at time step t is given by C(xt+h,ut+h) \u2248CTASK(xt+h,ut+h)+\nP\u03b8(COLL|xt,ut:t+H ,ot)CCOLL(xt+H), where we parameterize the probability of collision by model parameters \u03b8, which corresponds to a class of parameteric conditional models. In our case, we present P\u03b8(COLL|xt,ut:t+H ,ot) with a neural network that outputs the parameter of a Bernoulli random variable, as we will discuss in Section IV-C. Our goal\nnow is to learn the probability of collision model P\u03b8 in such a way that avoids catastrophic failures (i.e., high-speed collisions) at both training and test time. However, for the robot to be able to act appropriately in novel situations, the robot must be able to reason about the uncertainty of the collision prediction model P\u03b8, as we will discuss in the next section."}, {"heading": "IV. UNCERTAINTY-AWARE COLLISION PREDICTION", "text": "The core component of our approach is an uncertaintyaware collision prediction model P\u03b8. Training this collision prediction model from experience presents a dilemma: the robot must first experience collisions in order to learn how to avoid collisions. We formulate a speed-dependent collision cost that uses uncertainty-aware collision estimates, resulting in the robot exploring cautiously when uncertainty is high and moving faster when uncertainty is low. This naturally arising behavior enables the robot to learn about collisions without experiencing catastrophic failures, and subsequently use these safe collision experiences to act more aggressively in the future.\nAn example application domain and desired application of the uncertainty-aware collision prediction model is the following: consider a quadrotor navigation task in which the objective is to fly fast and avoid collisions in an unknown environment. The quadrotor seeks to learn a collision prediction model that takes as input an image and a sequence of velocity commands and outputs the probability of collision. Initially, the quadrotor flies conservatively because the speeddependent collision cost favors low-speed actions due to high uncertainty estimates of the collision prediction model. While flying conservatively, the quadrotor experiences safe collisions. These safe collisions, coupled with the associated images, are used to train the collision prediction model; the collision prediction model then learns how to associate images and velocity commands with the likelihood of colliding. As the algorithm continues and the collision prediction model uncertainty becomes low enough, the speed-dependent collision cost will favor high-speed flight."}, {"heading": "A. Collision Prediction with Uncertainty", "text": "The collision prediction model P\u03b8 takes as input the current state xt and observation ot, a sequence of H controls ut:t+H , and outputs the probability the robot experiences a collision within the horizon. We formulate P\u03b8 as a discriminative model using the logistic function L(y) = 1/(1 + exp(\u2212y)), so that\nP\u03b8(COLL|xt,ut:t+H ,ot) = L ( E[f\u03b8(xt,ut:t+H ,ot)]).\nHere, f\u03b8(xt,ut:t+H ,ot) is a random variable that corresponds to the real-valued output of our stochastic discriminatively trained model, which in our case corresponds to a modified neural network model that can produce uncertainty estimates. In general a variety of alternative models, including stochastic Bayesian models, could be used. Under this model, we can also define a risk-averse collision estimator P\u0303\u03b8(COLL|xt,ut:t+H ,ot), given by P\u0303\u03b8(COLL|xt,ut:t+H ,ot) = L ( E[f\u03b8(xt,ut:t+H ,ot)] + \u03bbSTD \u221a Var[f\u03b8(xt,ut:t+H ,ot)] ) , (1)\nwhere \u03bbSTD is a non-negative user-defined scalar and f\u03b8 is scalar-valued function of the current state, observation, and a sequence of controls.\nThe risk-averse collision prediction model P\u0303\u03b8 accounts for uncertainty using the variance of the function f\u03b8: the larger the variance of f\u03b8, the less certain the underlying stochastic model is about the probability of collision. The standard model P\u03b8 ignores this uncertainty, while the risk-averse model P\u0303\u03b8 uses the uncertainty to produce a conservative guess about the collision probability. Note that we use the variance of the sigmoid pre-activation value f\u03b8, since sigmoid probabilities are always in the range [0, 1]. Our goal is to increase the conservative estimate of collision if the model f\u03b8 is uncertain (has high variance). However, if we use the sigmoid values, we might systematically underestimate the uncertainty. For example, imagine that the expected value of f\u03b8 is a large negative number. Then, even if the variance is very large, the sigmoid expectation will be zero, which means that the sigmoid variance will be low. This is because the tails of the sigmoid flatten any variance in the model, making it invisible in situations where the mean prediction is close to 0 or 1. The hyperparameter \u03bbSTD allows us to set how conservative the risk-averse model P\u0303\u03b8 should be, which allows the user to make intuitive tradeoffs between safety and task completion.\nB. Velocity-Dependent Collision Cost\nBased on the previously defined risk-averse model, we can now formulate a collision cost that will naturally favor slow, cautious exploration in regions of high uncertainty. The particular cost that we use has the form\nCCOLL(xt) = \u03bbCOLL\u2016VELt\u20162, (2) where VELt is the robot velocity at time t and \u03bbCOLL is a nonnegative user-defined scalar that weights the relative importance of CCOLL versus CTASK. The full cost is then approximated using the risk-averse collision prediction model, according to C(xt+H ,ut+H) \u2248CTASK(xt+H ,ut+H)+ (3)\nP\u0303\u03b8(COLL|xt,ut:t+H ,ot)CCOLL(xt+H), With P\u0303\u03b8 and CCOLL defined, let us now confirm that Eqn. 3 will naturally favor cautious behavior when the collision prediction model is uncertain, and favor more aggressive behavior when the collision prediction model is confident. If the risk-averse collision prediction probability P\u0303\u03b8 is large, the robot is encouraged to move slowly in order to minimize CCOLL. The collision prediction probability P\u03b8 is large when E[f\u03b8] + \u221a Var[f\u03b8] is large, which occurs whenever the model predicts a collision (i.e., E[f\u03b8] 0) or when the model is uncertain (i.e., Var[f\u03b8] 0). On the other hand, if the riskaverse collision prediction probability is small, corresponding to a confident no-collision prediction, the robot can focus on minimizing CTASK and move at fast speeds. The collision prediction probability P\u0303\u03b8 is small when E[f\u03b8] + \u221a Var[f\u03b8] is small, which occurs when the model predicts no collision (i.e., E[f\u03b8] 0) and the model is certain (i.e., Var[f\u03b8] \u2248 0)."}, {"heading": "C. Neural Network Collision Prediction Model", "text": "In order to be able to predict collisions from rich, highdimensional sensory inputs, such as cameras or LIDAR measurements, we will use deep neural networks to estimate the probability of a collision. In the case of a standard deterministic, discriminatively trained neural network, f\u03b8 would represent the pre-activation values in the network at the last layer, while P\u03b8 is obtained by applying a sigmoidal nonlinearity to the preactivations. Such a network can be trained on prior trajectories experienced by the robot simply by slicing all prior data into subsequences of length H , and inputting the states xt, observations ot, and the concatenated sequence of controls ut:t+H into the model. The probability of collision labels are binary values recorded by the robot indicating whether a collision occurred, and we can obtain the label for each subsequence simply by checking whether a collision occurred between time steps t and t + H . The network can then be trained using standard stochastic gradient descent (SGD) with a cross-entropy loss on the final sigmoid output.\nWhile such a model can provide accurate predictions about collision probability in regions of the environment close to the training data, it is inherently discriminative and deterministic. Such a deterministic model does not provide an estimate of its variance, and therefore is not by itself suitable for risk-averse collision prediction."}, {"heading": "D. Estimating Uncertainty with Neural Networks", "text": "Standard predictive neural network models are trained discriminatively, which means that, even though the network might achieve a high accuracy on samples drawn from the same distribution as the training data, it is very difficult to predict how the network would behave on data drawn from a different distribution. While it is possible to train a neural network model that outputs a mean and a variance as its prediction [2], this model is not in general guaranteed to output high variances for unfamiliar inputs because the network is by definition trained only on the datapoints that are in the training set. Indeed, such a method for estimating variance is only effective at estimating the inherent noise in the data, and the variance estimates are not a meaningful indication of the model\u2019s own uncertainty about its predictions. To produce accurate uncertainty estimates for data that is outside of the training distribution, we must explore techniques that go beyond direct discriminative training. In order to obtain accurate uncertainty estimates from our model, we use two techniques: bootstrapping and dropout.\nBootstrapping: Bootstrapping [5, 6] is a simple and effective method of estimating model uncertainty using resampling that can be used with any discriminatively trained model. Given a dataset D, B new datasets D(b) are sampled with replacement from D such that |D(b)| = |D|. Then, instead of training a single model M on the entire dataset D, B different models M(b) are trained on the datasets D(b). The output prediction and uncertainty estimates are the sample mean and standard deviation of the outputs from the population of models.\nThe intuition behind bootstrapping is that, by generating multiple populations (using sampling with replacement) and\nAlgorithm 1 Neural net training with bootstrapping and dropout\n1: input: dataset D = {x(i)t ,u (i) t:t+H ,o (i) t }, neural network\nmodel NN 2: for b = 1 to B do 3: Sample a dataset of subsequences D(b) from the full dataset D with replacement 4: Initialize neural network NN(b) with random weights 5: for number of SGD iterations do 6: Sample datapoint (xt,ut:t+H ,ot) from D(b) 7: Sample NN(b)d by masking the units in NN\n(b) using dropout\n8: Run forward pass on NN(b)d using (xt,ut:t+H ,ot) 9: Run backward pass on NN(b)d to get gradient g (b) d\n10: Update model NN(b) parameters using g(b)d 11: end for 12: end for\ntraining one model per population, the models will agree in high-density areas of the population (i.e., low uncertainty regions) and disagree in low-density areas of the population (i.e., high uncertainty regions). This intuition is backed with theoretical guarantees [13]. However, for time- and resourceconstrained applications such as robotics, usually only a limited number of bootstraps can be used, which often leads to inaccurate estimates of the model uncertainty.\nDropout: Dropout [7] is, by comparison, a computationally cheap method to improve uncertainty estimates. Dropout is commonly used to reduce overfitting in neural networks by randomly dropping units from the neural network during training [28]. Specifically, a given unit with dropout is set to 0 with probability p and left as its original value with probability 1\u2212 p during training. Dropout prevents units from co-adapting (and thus overfitting) too much because different units are sampled for each forward pass, which effectively samples a new, but related, network during each step of training. Given a neural network NN(b), dropout in effect constructs a new randomized version of this network NN(b)d by sampling independent Bernoulli random variables to act as masks on each neuron.\nWhen dropout is used to reduce overfitting, it is only applied during training in order to force the units in the network to cope with stochastic removal of other units. In order to achieve high accuracy at test time, the dropout regularization is removed and all network weights are scaled by p to compensate for the increased level of activation. However, Gal and Ghahramani [7] showed that dropout can be used to obtain uncertainty estimates at test time by calculating the sample mean and standard deviation of multiple stochastic forward passes of the neural network using dropout. In this way, dropout can be viewed as an economical approximation to an ensemble method (such as bootstrapping) in which each sampled dropout mask corresponds to a different model. However, dropout underestimates the uncertainy because it acts roughly as a variational lower bound [7].\nAlgorithm 2 RL with Risk-Averse Collision Estimates 1: Initialize empty dataset D 2: Initialize collision prediction model P\u0303\u03b8 3: for iter=1 to max_iter do 4: Sample trajectories {\u03c4i} using MPC with cost C 5: Add samples {\u03c4i} to D 6: Train P\u0303\u03b8 using D (Alg. 1) 7: end for\nNeural Networks with Bootstrapping and Dropout: Alg. 1 provides an overview of training neural networks with bootstrapping and dropout. From an initial dataset, multiple datasets are resampled with replacement, along with corresponding neural network model instantiations. While performing stochastic gradient descent on each bootstrap, different units are dropped each time a forward pass occurs; the gradient calculated by backpropagation is then used to update that specific bootstrap model\u2019s parameters.\nAt test time, we can evaluate the mean and variance of the ensemble by performing multiple forward passes on each network NN(b) using multiple instantiations of the dropout process, corresponding to NN(b)d . The random function f\u03b8(xt,ut:t+H ,ot) then corresponds to sampling a network, sampling a dropout process, and evaluating the output. Thus, using neural networks with bootstrapping and dropout, we can estimate E[f\u03b8(xt,ut:t+H ,ot)] and Var[f\u03b8(xt,ut:t+H ,ot)] for use in the risk-averse model P\u0303\u03b8(COLL|xt,ut:t+H ,ot)."}, {"heading": "E. Reinforcement Learning with Risk-Averse Collision Estimation", "text": "Alg. 2 provides an overview of how the uncertainty-aware collision prediction model is used in a model-based reinforcement learning algorithm. Each iteration of the algorithm, the cost function C is formed using the current uncertainty-aware collision prediction model P\u0303\u03b8. The model predictive controller then samples trajectories using cost C. These sample trajectories are aggregated into a dataset containing all previous sampled trajectories. Then P\u0303\u03b8 is trained on the dataset according to Alg. 1 and the next iteration begins."}, {"heading": "V. EXPERIMENTS", "text": "We present simulated and real-world experiments to evaluate our uncertainty-aware collision prediction model, as well as our proposed model-based RL algorithm. We compare different settings for the parameters in our model, as well as evaluate its performance against a model-based approach that directly estimates the probability of collision, without explicitly accounting for uncertainty. Videos of the experiments can be found at https://sites.google.com/site/probcoll/.\nOur collision prediction model P\u0303\u03b8(COLL|xt,ut:t+H ,ot) is a fully connected neural network with two layers with 40 ReLU [21] hidden units each. The activation of the last layer, which outputs the collision probability, is a sigmoid (see Eqn. 1). The model inputs are the concatenation of xt,ut:t+H and ot. We trained the network using ADAM [12] and a standard\ncross-entropy loss. For uncertainty estimation, the simulation experiments used 50 bootstraps and a dropout ratio of 0.2, while the real-world experiments used 5 bootstraps (due to real-time constraints) and a dropout ratio of 0.05.\nAt each time step, the receding-horizon MPC planner chooses among a set of fixed action sequences of horizon length H by evaluating cost C on each action sequence, and executes the first action of the minimal cost action sequence."}, {"heading": "A. Quadrotor experiments", "text": "The simulated and real-world quadrotors have the same states, controls, and observations. We use a high-level representation of the quadrotor in which the control u \u2208 R2 is the commanded planar linear velocity, and therefore we assume the state x is estimated such that this level of control is feasible. However, we do not provide the state x as input to the collision prediction model. The observation o \u2208 R256 is a 16 by 16 grayscale image. The set of action sequences considered by the MPC planner at each time step consists of 190 straight-line, constant-velocity trajectories at various angles and speeds.\nSimulated quadrotor: We first evaluate our uncertainty-aware collision prediction model in a simulated environment consisting of a cylindrical obstacle of radius 0.2m (Fig. 3). The objective CTASK is to fly forward at 0.5 m/s, which is encoded as an `2 norm. The time horizon is H = 6 and each discrete time step corresponds to \u03b4 = 0.2 seconds,\ntherefore the planning horizon is \u03b4H seconds. At each time step, the quadrotor must decide on the sequence of actions using only the observation from a simulated monocular camera.\nFig. 2 compares safety versus task performance for different variants of Alg. 2. All experiments consist of 20 training iterations, with each iteration consisting of 20 on-policy rollouts from start states drawn from the same distribution. Each experiment was run 5 times with different random seeds.\nFirst, we investigate the benefits of incorporating uncertainty into the cost by evaluating different values for \u03bbSTD (Eqn. 1). Fig. 2a shows that, when not accounting for uncertainty (i.e.,\n\u03bbSTD = 0), the final task performance approaches the desired speed of 0.5 m/s. However, the quadrotor experiences highspeed collisions during training, as shown by the vertical axis. By accounting for uncertainty (i.e., \u03bbSTD > 0), the quadrotor experiences lower speed collisions during training. The final task performance decreases if \u03bbSTD is increased too much, which is expected: the more conservative the vehicle behaves during training, the longer it takes to learn the task. These results show that \u03bbSTD allows the user to control their desired degree of risk during training and trade off safety against learning efficiency.\nOne reasonable question is whether accounting for uncertainty improves safety due to good uncertainty estimates, or simply because adding uncertainty to the collision probability simply makes the vehicle more cautious by penalizing high speeds. To answer this question, we compare our uncertaintyaware approach against a conservative baseline that replaces the uncertainty in Eqn. 1 with a constant \u03bbCONST (Fig. 2b). The experiments for \u03bbCONST = 0.1, 1, and 10 show no safety improvement, and also show decreased task performance compared to the baseline \u03bbCONST = 0 \u2261 \u03bbSTD = 0. The experiment for \u03bbCONST = 100 shows substantial safety improvement, but task performance is also substantially diminished. Compared to our uncertainty-aware approach with different settings of \u03bbSTD, the baseline constant penalty approach with \u03bbCONST is ineffective at trading off between safety and performance, and always produces overly conservative motions. This indicates that uncertainty estimation is in fact reasoning about the vehicle\u2019s surroundings, rather than uniformly encouraging slower flight.\nAnother reasonable question to ask is whether simply increasing the collision cost \u03bbCOLL induces safer training behavior. Our experimental results, included in the appendix, show that increasing \u03bbCOLL does not lead to safer training behavior. Further simulation experiments and results are also provided in the appendix.\nReal-world quadrotor: We evaluated our approach in a realworld environment consisting of a single obstacle, in which the objective is to fly around the obstacle (Fig. 1). Although the\ntask of avoiding a single static obstacle is relatively simple, it is worth noting that the vehicle must perform this task entirely using real-world training data and only monocular images, while minimizing the number of collisions experienced during training. As such, the task is in fact quite challenging.\nWe ran our experiments using a Parrot Bebop 2 quadrotor. We used the ROS bebop_autonomy package, which allows the laptop to send linear velocity commands and receive the onboard images in real-time. The quadrotor\u2019s objective CTASK is to fly forward at 1.6 m/s, which is encoded as an `2 norm. The time horizon H = 3 and each time step corresponds to \u03b4 = 0.5 seconds.\nAll experiments consist of 5 training iterations, with each iteration consisting of 5 rollouts from 4 different initial positions. This experimental setup can be viewed in the online video. After each rollout, the quadrotor was manually reset to the next initial state. Note that this reset was solely done for minimizing experimental confounds for the purpose of evaluation, and is not a requirement of our approach. In principle, the vehicle could simply continue flying around the room and collecting data until good performance is achieved. Each experiment was initialized with 6 flight demonstrations provided by a human pilot. These demonstrations were the exact same for all experiments and consisted of 2 crashes and 4 successful flights around the obstacle. To prevent damage to the quadrotor, particularly for the baselines, a human pilot intervened if a crash was imminent; the algorithm therefore treated each intervention as a collision. Each experiment was run 5 times.\nFig. 4 shows images of our approach during the training process for an example experiment. In the beginning iterations, the quadrotor makes little progress and experiences collisions. As the RL algorithm progresses, the quadrotor is eventually able to fly around the obstacle at high speed.\nFig. 5 compares safety versus task performance when running our model-based RL algorithm (Alg. 2) without uncertainty (\u03bbSTD = 0) and with uncertainty (\u03bbSTD = 2). When accounting for uncertainty, the quadrotor experiences substantially fewer collisions, especially at higher speeds, but takes longer to approach the desired task performance."}, {"heading": "B. Real-world RC car experiments", "text": "We evaluated our approach on an RC car (Fig. 8) in a simple obstacle avoidance task (Fig. 1). The car is parameterized by control u \u2208 R2 consisting of speed and steering angle and observation o \u2208 R576 consisting of a 32 by 18 grayscale image. We do not assume access to any underlying state x.\nThe car\u2019s objective CTASK is to drive at 1.2 m/s in any direction, which is encoded as an `2-norm. The time horizon was set to H = 4 and each discrete time step corresponds\nto \u03b4 = 0.5 seconds. The set of action sequences considered by the MPC planner at each time step consists of 49 curving, constant-velocity trajectories at various steering angles and speeds.\nAll experiments consist of 10 training iterations, with each iteration consisting of 5 on-policy rollouts from 4 different initial states. Each rollout ended after either a collision or 10 time steps, therefore each experiment consists of approximately 15 minutes of real-world experience. After each rollout, the car was manually reset to the next initial state. No human demonstrations were used for initialization and each experiment was ran twice. Unlike in the quadrotor experiments, the car was allowed to collide at full speed and automatically registered collisions using limit switches mounted on the front of the car.\nFig. 6 shows images of our approach during the training process for an example experiment. Initially, the car is unable to avoid the obstacle and side walls, but eventually learns to avoid collisions.\nFig. 7 compares safety versus task performance when running our model-based RL algorithm (Alg. 2) without uncertainty (\u03bbSTD = 0) and with uncertainty (\u03bbSTD = 1). The final modelbased planner for both approaches succeeds in navigating without colliding for almost 70% of the rollouts, which is a significant improvement over the initial policy. When accounting for uncertainty, the car experiences fewer highspeed collisions and achieves comparable speeds compared to when not accounting for uncertainty."}, {"heading": "VI. DISCUSSION AND FUTURE WORK", "text": "We presented a model-based combined perception and control method for learning obstacle avoidance strategies that uses uncertainty estimates to automatically generate safe strategies. Our method is based on predicting the probability of collision conditioned on raw sensory inputs and a sequence of\nactions, using deep neural networks. This predictor can be used within a model-predictive control pipeline to choose actions that avoid collisions with high probability. In regions of high uncertainty, our risk-averse cost function naturally causes the robot to revert to a cautious low-speed strategy, without any explicit manual engineering of safety controllers or fail-safe mechanisms. We demonstrate our approach is safer compared to methods without uncertainty estimates in both a simulated and real-world quadrotor obstacle avoidance task, as well as a real-world RC car task.\nAlthough our method produces cautious, uncertainty-aware behavior, it does not attempt to explicitly seek out successful strategies except through the MPC optimization. This can cause the algorithm to become stuck in bad local optima. For example, the suboptimal final performance of our approach in the real-world quadrotor experiments with \u03bbSTD = 2 (Fig. 5). A promising direction of future work is to combine our method with optimistic\u2014but still cautious\u2014exploration strategies.\nThe success of our approach depends strongly on the accuracy of the uncertainty estimates. If the uncertainty estimates are overly optimistic, the robot may experience catastrophic failures. However, if the uncertainty estimates are overly pessimistic, the robot will be perpetually scared and the resulting policy will be suboptimal. This latter case may be another explanation for the suboptimal final performance of our uncertainty-aware approach in the real-world experiments (Fig. 5), therefore future work on developing new uncertainty estimators and characterizing their qualities is important for deploying RL algorithms on robotic systems.\nAnother promising direction for future work is to generalize our approach beyond collision prediction to other model-based reinforcement learning scenarios. The principle of uncertaintyaware prediction of future events can be readily applied to any feature of the environment, including the expected\ncost, and exploring this extension to general reinforcement learning problems could produce effective and safe exploration techniques for a wide range of robotic scenarios."}, {"heading": "VII. ACKNOWLEDGEMENTS", "text": "This research was funded in part by the Army Research Office through the MAST program, the National Science Foundation under IIS-1637443 and IIS-1614653, and the Berkeley Deep Drive consortium."}], "references": [{"title": "Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics", "author": ["F. Berkenkamp", "A. Krause", "A. Schoellig"], "venue": "arXiv:1602.04450", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Introspective Perception: Learning to Predict Failures in Vision Systems", "author": ["S. Daftry", "S. Zeng", "J.A. Bagnell", "M. Hebert"], "venue": "IROS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "PILCO: A Modelbased and Data-Efficient Approach to Policy Search", "author": ["M. Deisenroth", "C. Rasmussen"], "venue": "ICML", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "A Survey on Policy Search for Robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "The Jackknife", "author": ["B. Efron", "R. Tibshirani"], "venue": "the Bootstrap and Other Resampling Plans. In SIAM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "An introduction to the bootstrap", "author": ["B. Efron", "R. Tibshirani"], "venue": "CRC press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving PILCO with Bayesian Neural Network Dynamics Models", "author": ["Y. Gal", "R. Mcallister", "C. Rasmussen"], "venue": "Data-Efficient Machine Learning workshop, ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Guaranteed Safe Online Learning via Reachability: Tracking a Ground Target using a Quadrotor", "author": ["J. Gillula", "C. Tomlin"], "venue": "ICRA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing Conservativeness in Safety Guarantees by Learning Disturbances Online: Iterated Guaranteed Safe Online Learning", "author": ["J. Gillula", "C. Tomlin"], "venue": "RSS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "PLATO: Policy Learning using Adaptive Trajectory Optimization", "author": ["G. Kahn", "C. Zhang", "S. Levine", "P. Abbeel"], "venue": "ICRA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The Big Data Bootstrap", "author": ["A. Kleiner", "A. Talwalkar", "P. Sarkar", "M.I. Jordan"], "venue": "ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "In Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "volume 32, pages 1238\u2013 1274", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "End-to-End Training of Deep Visuomotor Policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Continuous control  with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv:1411.0247", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Funnel Libraries for Real-Time Robust Feedback Motion Planning", "author": ["A. Majumdar", "R. Tedrake"], "venue": "arXiv:1601.04037", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "and Riedmiller M", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra"], "venue": "Playing Atari with Deep Reinforcement Learning. In Workshop on Deep Learning, NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Safe Exploration in Markov Decision Processes", "author": ["T. Moldovan", "P. Abbeel"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Relaxed hover solutions for multicopters: Application to algorithmic redundancy and novel vehicles", "author": ["M. Mueller", "R. D\u2019Andrea"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep Exploration via Bootstrapped DQN", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Lyapunov Design for Safe Reinforcement Learning", "author": ["T. Perkins", "A. Barto"], "venue": "JMLR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Policy Gradient Methods for Robotics", "author": ["J. Peters", "S. Schaal"], "venue": "IROS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian Learning for Safe High-Speed Navigation in Unknown Environments", "author": ["C. Richter", "W. Vega-Brown", "N. Roy"], "venue": "ISRR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting Model Uncertainty Estimates for Safe Dynamic Control Learning", "author": ["J. Schneider"], "venue": "NIPS", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Trust Region Policy Optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "ICML", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutidnov"], "venue": "JMLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Autonomous Driving in Urban Environments: Boss and the Urban Challenge", "author": ["C. Urmson", "et. al"], "venue": "In Journal of Field Robotics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Safe receding horizon control for aggressive MAV flight with limited range sensing", "author": ["M. Watterson", "V. Kumar"], "venue": "IROS", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Viability and Predictive Control for Safe Locomotion", "author": ["P. Wieber"], "venue": "IROS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "To obtain uncertainty estimates from the neural network, we leverage uncertainty estimation methods for discriminatively trained neural networks based on a combination of bootstrapping [5] and dropout [28, 7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 27, "context": "To obtain uncertainty estimates from the neural network, we leverage uncertainty estimation methods for discriminatively trained neural networks based on a combination of bootstrapping [5] and dropout [28, 7].", "startOffset": 201, "endOffset": 208}, {"referenceID": 6, "context": "To obtain uncertainty estimates from the neural network, we leverage uncertainty estimation methods for discriminatively trained neural networks based on a combination of bootstrapping [5] and dropout [28, 7].", "startOffset": 201, "endOffset": 208}, {"referenceID": 13, "context": "Reinforcement learning has been applied to a wide range of robotic problems, ranging from locomotion and manipulation to autonomous helicopter flight [14, 4].", "startOffset": 150, "endOffset": 157}, {"referenceID": 3, "context": "Reinforcement learning has been applied to a wide range of robotic problems, ranging from locomotion and manipulation to autonomous helicopter flight [14, 4].", "startOffset": 150, "endOffset": 157}, {"referenceID": 23, "context": "Model-free methods have been particularly popular due to their simplicity and favorable computational properties [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "known to be more sample-efficient [3].", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Several model-based robotic learning algorithms have been proposed that explicitly reason about uncertainty [3, 26].", "startOffset": 108, "endOffset": 115}, {"referenceID": 25, "context": "Several model-based robotic learning algorithms have been proposed that explicitly reason about uncertainty [3, 26].", "startOffset": 108, "endOffset": 115}, {"referenceID": 18, "context": "averse and risk-seeking, optimistic exploration [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Uncertainty-aware model-based reinforcement learning has been explored in previous work using Bayesian models [25, 1].", "startOffset": 110, "endOffset": 117}, {"referenceID": 0, "context": "Uncertainty-aware model-based reinforcement learning has been explored in previous work using Bayesian models [25, 1].", "startOffset": 110, "endOffset": 117}, {"referenceID": 7, "context": "Recent work has proposed to use a Bayesian formulation of neural networks based on dropout [8], as well as to use the bootstrap for exploration [22], but not, to the best of our knowledge, for uncertainty estimation for the purpose of safety.", "startOffset": 91, "endOffset": 94}, {"referenceID": 21, "context": "Recent work has proposed to use a Bayesian formulation of neural networks based on dropout [8], as well as to use the bootstrap for exploration [22], but not, to the best of our knowledge, for uncertainty estimation for the purpose of safety.", "startOffset": 144, "endOffset": 148}, {"referenceID": 28, "context": "critical systems such as autonomous cars [29], legged robots [31], and quadrotors [20, 30, 9].", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "critical systems such as autonomous cars [29], legged robots [31], and quadrotors [20, 30, 9].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "critical systems such as autonomous cars [29], legged robots [31], and quadrotors [20, 30, 9].", "startOffset": 82, "endOffset": 93}, {"referenceID": 29, "context": "critical systems such as autonomous cars [29], legged robots [31], and quadrotors [20, 30, 9].", "startOffset": 82, "endOffset": 93}, {"referenceID": 8, "context": "critical systems such as autonomous cars [29], legged robots [31], and quadrotors [20, 30, 9].", "startOffset": 82, "endOffset": 93}, {"referenceID": 22, "context": "rich sensory input and are often difficult to scale to highdimensional systems [23, 17, 10].", "startOffset": 79, "endOffset": 91}, {"referenceID": 16, "context": "rich sensory input and are often difficult to scale to highdimensional systems [23, 17, 10].", "startOffset": 79, "endOffset": 91}, {"referenceID": 9, "context": "rich sensory input and are often difficult to scale to highdimensional systems [23, 17, 10].", "startOffset": 79, "endOffset": 91}, {"referenceID": 1, "context": "Several works have suggested using discriminative models, including neural networks, to learn safety predictors [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 17, "context": "in recent years, with applications to video game playing [18], control of simulated robots [27, 16], and manipulation [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "in recent years, with applications to video game playing [18], control of simulated robots [27, 16], and manipulation [15].", "startOffset": 91, "endOffset": 99}, {"referenceID": 15, "context": "in recent years, with applications to video game playing [18], control of simulated robots [27, 16], and manipulation [15].", "startOffset": 91, "endOffset": 99}, {"referenceID": 14, "context": "in recent years, with applications to video game playing [18], control of simulated robots [27, 16], and manipulation [15].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "ground truth state information [11].", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Note that we use the variance of the sigmoid pre-activation value f\u03b8, since sigmoid probabilities are always in the range [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 1, "context": "While it is possible to train a neural network model that outputs a mean and a variance as its prediction [2], this model is not in general guaranteed to output high variances for unfamiliar inputs because the network is by definition trained only on the datapoints that are in the training set.", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "Bootstrapping: Bootstrapping [5, 6] is a simple and effective method of estimating model uncertainty using resampling that can be used with any discriminatively trained model.", "startOffset": 29, "endOffset": 35}, {"referenceID": 5, "context": "Bootstrapping: Bootstrapping [5, 6] is a simple and effective method of estimating model uncertainty using resampling that can be used with any discriminatively trained model.", "startOffset": 29, "endOffset": 35}, {"referenceID": 12, "context": "This intuition is backed with theoretical guarantees [13].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "Dropout: Dropout [7] is, by comparison, a computationally cheap method to improve uncertainty estimates.", "startOffset": 17, "endOffset": 20}, {"referenceID": 27, "context": "commonly used to reduce overfitting in neural networks by randomly dropping units from the neural network during training [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "However, Gal and Ghahramani [7] showed that dropout can be used to obtain uncertainty estimates at test time by calculating the sample mean and standard deviation of multiple stochastic forward passes of", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "However, dropout underestimates the uncertainy because it acts roughly as a variational lower bound [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 20, "context": "Our collision prediction model P\u0303\u03b8(COLL|xt,ut:t+H ,ot) is a fully connected neural network with two layers with 40 ReLU [21] hidden units each.", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "We trained the network using ADAM [12] and a standard", "startOffset": 34, "endOffset": 38}], "year": 2010, "abstractText": "Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a realworld RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.", "creator": "LaTeX with hyperref package"}}}