{"id": "1702.01287", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2017", "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "abstract": "we introduce a multi - modal neural machine translation model in which a doubly - responsive attentive decoder naturally repeatedly incorporates functional spatial sparse visual features previously obtained using pre - trained convolutional neural networks, bridging the gap between image description and translation. our decoder learns to attend to source - language words and parts of an image model independently by means of two separate attention mechanisms as it generates words in the target language. we find that our functional model can efficiently explicitly exploit not just back - translated in - domain multi - modal data but quite also large general - domain text - only mt corpora. we also report state - of - the - art results on the multi30k data tree set.", "histories": [["v1", "Sat, 4 Feb 2017 13:46:53 GMT  (454kb,D)", "http://arxiv.org/abs/1702.01287v1", "8 pages (11 including references), 2 figures"]], "COMMENTS": "8 pages (11 including references), 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["iacer calixto", "qun liu", "nick campbell"], "accepted": true, "id": "1702.01287"}, "pdf": {"name": "1702.01287.pdf", "metadata": {"source": "CRF", "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "authors": ["Iacer Calixto", "Qun Liu"], "emails": ["iacer.calixto@adaptcentre.ie"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has been successfully tackled as a sequence to sequence learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two.\nIn the context of NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder, which is trained to attend to the relevant source-language words as it generates each word of the target sentence. Similarly, Xu et al. (2015) proposed an attention-based model for the task of image description generation (IDG) where a model learns to attend to specific parts of an image representation (the source) as it generates its description (the target) in natural language.\nWe are inspired by recent successes in applying attention-based models to NMT and IDG. In this\nwork, we propose an end-to-end attention-based multi-modal neural machine translation (MNMT) model which effectively incorporates two independent attention mechanisms, one over sourcelanguage words and the other over different areas of an image.\nOur main contributions are:\n\u2022 We propose a novel attention-based MNMT model which incorporates spatial visual features in a separate visual attention mechanism;\n\u2022 We use a medium-sized, back-translated multi-modal in-domain data set and large general-domain text-only MT corpora to pretrain our models and show that our MNMT model can efficiently exploit them;\n\u2022 We show that images bring useful information into an NMT model, in situations in which sentences describe objects illustrated in the image.\nTo the best of our knowledge, previous MNMT models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovicky\u0301 et al., 2016; Specia et al., 2016). In this work, we wish to address this issue and propose an MNMT model that uses, in addition to an attention mechanism over the source-language words, an additional visual attention mechanism to incorporate spatial visual features, and still improves on simpler text-only and multi-modal attention-based NMT models.\nThe remainder of this paper is structured as follows. We first briefly revisit the attentionbased NMT framework (\u00a72) and expand it into an MNMT framework (\u00a73). In \u00a74, we introduce the\nar X\niv :1\n70 2.\n01 28\n7v 1\n[ cs\n.C L\n] 4\nF eb\n2 01\n7\ndatasets we use to train and evaluate our models, in \u00a75 we discuss our experimental setup and analyse and discuss our results. Finally, in \u00a76 we discuss relevant previous related work and in \u00a77 we draw conclusions and provide some avenues for future work."}, {"heading": "2 Background and Notation", "text": ""}, {"heading": "2.1 Attention-based NMT", "text": "We describe the attention-based NMT model introduced by Bahdanau et al. (2015) in this section. Given a source sequence X = (x1, x2, \u00b7 \u00b7 \u00b7 , xN ) and its translation Y = (y1, y2, \u00b7 \u00b7 \u00b7 , yM ), an NMT model aims to build a single neural network that translates X into Y by directly learning to model p(Y | X). The entire network consists of one encoder and one decoder with one attention mechanism, typically implemented as two Recurrent Neural Networks (RNN) and one multilayer perceptron, respectively. Each xi is a row index in a source lookup or word embedding matrix Ex \u2208 R|Vx|\u00d7dx , as well as each yj being an index in a target lookup or word embedding matrix Ey \u2208 R|Vy |\u00d7dy , Vx and Vy are source and target vocabularies, and dx and dy are source and target word embeddings dimensionalities, respectively.\nThe encoder is a bi-directional RNN with GRU (Cho et al., 2014a), where a forward RNN\u2212\u2192 \u03a6 enc reads X word by word, from left to right, and generates a sequence of forward annotation vectors ( \u2212\u2192 h 1, \u2212\u2192 h 2, \u00b7 \u00b7 \u00b7 , \u2212\u2192 hN ) at each encoder time step i \u2208 [1, N ]. Similarly, a backward RNN \u2190\u2212 \u03a6 enc reads X from right to left, word by word, and generates a sequence of backward annotation vectors ( \u2190\u2212 hN , \u2190\u2212 hN\u22121, \u00b7 \u00b7 \u00b7 , \u2190\u2212 h 1). The final annotation vector is the concatenation of forward and backward vectors hi = [\u2212\u2192 hi; \u2190\u2212 hi ] , and C = (h1,h2, \u00b7 \u00b7 \u00b7 ,hN ) is the set of source annotation vectors.\nThese annotation vectors are in turn used by the decoder, which is essentially a neural language model (LM) (Bengio et al., 2003) conditioned on the previously emitted words and the source sentence via an attention mechanism. A multilayer perceptron is used to initialise the decoder\u2019s hidden state s0 at time step t = 0, where the input to this network is the concatenation of the last forward and backward vectors [\u2212\u2192 hN ; \u2190\u2212 h1 ] .\nAt each time step t of the decoder, a timedependent source context vector ct is computed based on the annotation vectors C and the decoder\nprevious hidden state st\u22121. This is part of the formulation of the conditional GRU and is described further in \u00a72.2. In other words, the encoder is a bi-directional RNN with GRU and the decoder is an RNN with a conditional GRU.\nGiven a hidden state st, the probabilities for the next target word are computed using one projection layer followed by a softmax layer as described in eq. (1), where the matrices Lo, Ls, Lw and Lc are transformation matrices and ct is a timedependent source context vector generated by the conditional GRU."}, {"heading": "2.2 Conditional GRU", "text": "The conditional GRU1 has three main components computed at each time step t of the decoder:\n\u2022 REC1 computes a hidden state proposal s\u2032t based on the previous hidden state st\u22121 and the previously emitted word y\u0302t\u22121;\n\u2022 ATTsrc2 is an attention mechanism over the hidden states of the source-language RNN and computes ct using all source annotation vectors C and the hidden state proposal s\u2032t;\n\u2022 REC2 computes the final hidden state st using the hidden state proposal s\u2032t and the timedependent source context vector ct.\nWe use the conditional GRU in our text-only attention-based NMT model. First, a single-layer feed-forward network is used to compute an expected alignment esrct,i between each source annotation vector hi and the target word y\u0302t to be emitted at the current time step t, as shown in Equations (2) and (3):\nesrct,i = (v src a ) T tanh(U srca s \u2032 t +W src a hi), (2)\n\u03b1srct,i = exp (esrct,i )\u2211N j=1 exp (e src t,j) , (3)\nwhere \u03b1srct,i is the normalised alignment matrix between each source annotation vector hi and the word y\u0302t to be emitted at time step t, and vsrca , U src a andW srca are model parameters. Finally, a time-dependent source context vector ct is computed as a weighted sum over the source annotation vectors, where each vector is weighted\n1https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf.\n2ATTsrc is named ATT in the original technical report.\np(yt = k | y<t, ct) = Softmax(Lo tanh(Lsst +LwEy[y\u0302t\u22121] +Lcct)). (1)\nby the attention weight \u03b1srct,i , as in eq. (4):\nct = N\u2211 i=1 \u03b1srct,ihi. (4)"}, {"heading": "3 Multi-modal NMT", "text": "Our MNMT model can be seen as an expansion of the attention-based NMT framework described in \u00a72.1 with the addition of a visual component to incorporate spatial visual features, and is comparable to the model evaluated by Calixto et al. (2016).\nWe use publicly available pre-trained CNNs for image feature extraction. Specifically, we extract spatial image features for all images in our dataset using the 50-layer Residual network (ResNet-50) of He et al. (2015). These spatial features are\nthe activations of the res4f layer, which can be seen as encoding an image in a 14\u00d714 grid, where each of the entries in the grid is represented by a 1024D feature vector that only encodes information about that specific region of the image. We vectorise this 3-tensor into a 196\u00d71024 matrix A = (a1,a2, \u00b7 \u00b7 \u00b7 ,aL),al \u2208 R1024 where each of the L = 196 rows consists of a 1024D feature vector and each column, i.e. feature vector, represents one grid in the image."}, {"heading": "3.1 NMTSRC+IMG: decoder with two independent attention mechanisms", "text": "Model NMTSRC+IMG integrates two separate attention mechanisms over the source-language words and visual features in a single decoder RNN. Our doubly-attentive decoder RNN is conditioned on the previous hidden state of the decoder and the previously emitted word, as well as the source sentence and the image via two independent attention mechanisms, as illustrated in Figure 1.\nWe implement this idea expanding the conditional GRU described in \u00a72.2 onto a doublyconditional GRU. To that end, in addition to the source-language attention, we introduce a new attention mechanism ATTimg to the original conditional GRU proposal. This visual attention computes a time-dependent image context vector it given a hidden state proposal s\u2032t and the image annotation vectors A = (a1,a2, \u00b7 \u00b7 \u00b7 ,aL) using the \u201csoft\u201d attention (Xu et al., 2015).\nThis attention mechanism is very similar to the source-language attention with the addition of a gating scalar, explained further below. First, a single-layer feed-forward network is used to compute an expected alignment eimgt,l between each image annotation vector al and the target word to be emitted at the current time step t, as in eqs. (6) and (7):\ne img t,l = (v img a ) T tanh(U imga s \u2032 t +W img a al), (6)\n\u03b1 img t,l = exp (e img t,l )\u2211L\nj=1 exp (e img t,j )\n, (7)\nwhere \u03b1imgt,l is the normalised alignment matrix between all the image patches al and the target word to be emitted at time step t, and vimga , U img a and W imga are model parameters. Note that Equations (2) and (3), that compute the expected source\np(yt = k | y<t, C,A) = softmax(Lo tanh(Lsst +LwEy[y\u0302t\u22121] +Lcsct +Lciit)), (5)\nalignment esrct,i and the weight matrices \u03b1 src t,i , and eqs. (6) and (7) that compute the expected image alignment eimgt,l and the weight matrices\u03b1 img t,l , both compute similar statistics over the source and image annotations, respectively.\nIn eq. (8) we compute \u03b2t \u2208 [0, 1], a gating scalar used to weight the expected importance of the image context vector in relation to the next target word at time step t:\n\u03b2t = \u03c3(W\u03b2st\u22121 + b\u03b2), (8)\nwhere W\u03b2 , b\u03b2 are model parameters. It is in turn used to compute the time-dependent image context vector it for the current decoder time step t, as in eq. (9):\nit = \u03b2t L\u2211 l=1 \u03b1 img t,l al. (9)\nThe only difference between Equations (4) (source context vector) and (9) (image context vector) is that the latter uses a gating scalar, whereas the former does not. We use \u03b2 following Xu et al. (2015) who empirically found it to improve the variability of the image descriptions generated with their model.\nFinally, we use the time-dependent image context vector it as an additional input to a modified version of REC2 (\u00a72.2), which now computes the final hidden state st using the hidden state proposal s\u2032t, and the time-dependent source and image context vectors ct and it, as in (10):\nzt = \u03c3(W src z ct +W img z it +Uzs \u2032 j), rt = \u03c3(W src r ct +W img r it +Urs \u2032 j), st = tanh(W srcct +W\nimgit + rt (Us\u2032t)), st = (1\u2212 zt) st + zt s\u2032t. (10)\nIn Equation (5), the probabilities for the next target word are computed using the new multimodal hidden state st, the previously emitted word y\u0302t\u22121, and the two context vectors ct and it, where Lo, Ls, Lw, Lcs and Lci are projection matrices and trained with the model."}, {"heading": "4 Data", "text": "The Flickr30k data set contains 30k images and 5 descriptions in English for each image (Young\net al., 2014). In this work, we use the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual expansions of the original Flickr30k: one with translated data and another one with comparable data, henceforth referred to as M30kT and M30kC, respectively.\nFor each of the 30k images in the Flickr30k, the M30kT has one of the English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29k, 1,014 and 1k images respectively, each accompanied by a sentence pair (the original English sentence and its translation into German). For each of the 30k images in the Flickr30k, the M30kC has five descriptions in German collected independently from the English descriptions. Training, validation and test sets contain 29k, 1,014 and 1k images respectively, each accompanied by five sentences in English and five sentences in German.\nWe use the entire M30kT training set for training our MNMT models, its validation set for model selection with BLEU (Papineni et al., 2002), and its test set for evaluation. In addition, since the amount of training data available is small, we build a back-translation model using the text-only NMT model described in \u00a72.1 trained on the Multi30kT data set (German\u2192English), without images. We use this model to back-translate the 145k German descriptions in the Multi30kC into English and include the triples (synthetic English description, German description, image) as additional training data (Sennrich et al., 2016a).\nWe also use the WMT 2015 text-only parallel corpora available for the English\u2013German language pair, consisting of about 4.3M sentence pairs (Bojar et al., 2015). These include the Europarl v7 (Koehn, 2005), News Commentary and Common Crawl corpora, which are concatenated and used for pre-training.\nWe use the scripts in the Moses SMT Toolkit (Koehn et al., 2007) to normalise and tokenize English and German descriptions, and we also convert space-separated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of 83, 093 English and 91, 141 German subword tokens. If sentences in English or German are longer than 80 tokens, they are discarded. We train models to translate\nfrom English into German and report evaluation of cased, tokenized sentences with punctuation."}, {"heading": "5 Experimental setup", "text": "Our encoder is a bidirectional RNN with GRU, one 1024D single-layer forward and one 1024D single-layer backward RNN. Source and target word embeddings are 620D each and trained jointly with the model. Word embeddings and other non-recurrent matrices are initialised by sampling from a Gaussian N (0, 0.012), recurrent matrices are random orthogonal and bias vectors are all initialised to zero.\nVisual features are obtained by feeding images to the pre-trained ResNet-50 and using the activations of the res4f layer (He et al., 2015). We apply dropout with a probability of 0.5 in the encoder bidirectional RNN, the image features, the decoder RNN and before emitting a target word. We follow Gal and Ghahramani (2016) and apply dropout to the encoder bidirectional and the decoder RNN using one same mask in all time steps.\nAll models are trained using stochastic gradient descent with ADADELTA (Zeiler, 2012) with minibatches of size 80 (text-only NMT) or 40 (MNMT), where each training instance consists of one English sentence, one German sentence and one image (MNMT). We apply early stopping for model selection based on BLEU4, so that if a model does not improve on BLEU4 in the validation set for more than 20 epochs, training is halted.\nThe translation quality of our models is evaluated quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and chrF3 (Popovic\u0301, 2015).3 We report statistical significance with approximate randomisation for the first three metrics using the MultEval tool (Clark et al., 2011)."}, {"heading": "5.1 Baselines", "text": "We train a text-only phrase-based SMT (PBSMT) system and a text-only NMT model for comparison. Our PBSMT baseline is built with Moses and uses a 5\u2013gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). It is trained on the English\u2013German descriptions of the M30kT, whereas its LM is trained on the German descriptions only. We use minimum error rate training to\n3We specifically compute character 6-gram F3, and additionally character precision and recall for comparison.\ntune the model (Och, 2003) with BLEU. The textonly NMT baseline is the one described in \u00a72.1 and is trained on the M30kT\u2019s English\u2013German descriptions.\nWe also compare our model against two multimodal attention-based NMT models. The first model is Huang et al. (2016)\u2019s best model trained on the same data, and the second is their best model using additional object detections, respectively models m1 (image at head) and m3 in the authors\u2019 paper."}, {"heading": "5.2 Results", "text": "In Table 1, we show results for our text-only baselines NMT and PBSMT, the multi-modal models of Huang et al. (2016) and our MNMT models trained on the M30kT, and pre-trained on the indomain back-translated M30kC and the generaldomain text-only English-German MT corpora from WMT 2015.\nTraining on M30kT One main finding is that our model consistently outperforms the comparable model of Huang et al. (2016), with improvements of +1.4 BLEU and +2.7 METEOR. In fact, even when their model has access to more data our model still improves by +0.9 METEOR, while maintaining the same BLEU4 scores.\nMoreover, we can also conclude from Table 1 that PBSMT performs better at recall-oriented metrics, i.e. METEOR and chrF3, whereas NMT is better at precision-oriented ones, i.e. BLEU4. This is somehow expected, since the attention mechanism in NMT (Bahdanau et al., 2015) does not explicitly take attention weights from previous time steps into account, an thus lacks the notion of source coverage as in SMT (Koehn et al., 2003; Tu et al., 2016). We note that these ideas are complementary and incorporating coverage into model NMTSRC+IMG could lead to more improvements, especially in recall-oriented metrics. Nonetheless, our doubly-attentive model shows consistent gains in both precision- and recall-oriented metrics in comparison to the text-only NMT baseline, i.e. it is significantly better according to BLEU4, METEOR and TER (p < 0.01), and it also improves chrF3 by +2.1. In comparison to the PBSMT baseline, our proposed model still significantly improves according to both BLEU4 and TER (p < 0.01), also increasing METEOR by +0.7 but with an associated p-value of p = 0.071, therefore not significant for p < 0.05. Although chrF3 is the\nonly metric in which the PBSMT model scores best, the difference between our model and the latter is only 0.1, meaning that they are practically equivalent. We note that model NMTSRC+IMG consistently increases character recall in comparison to the text-only NMT baseline. Although it can happen at the expense of character precision, gains in recall are always much higher than any eventual loss in precision, leading to consistent improvements in chrF3.\nPre-training We now discuss results for models pre-trained using different data sets. We first pre-trained the two text-only baselines PBSMT and NMT, and our MNMT model on the backtranslated M30kC, a medium-sized in-domain image description data set (145k training instances). We also pre-trained the same models on the English\u2013German parallel sentences of much larger MT data sets, i.e. the concatenation of the Europarl (Koehn, 2005), Common Crawl and\nNews Commentary corpora, used in WMT 2015 (\u223c4.3M parallel sentences). Model PBSMT (concat.) used the concatenation of the pre-training and training data for training, and model PBSMT (LM) used the general-domain German sentences as additional data to train the LM. From Table 1, it is clear that model NMTSRC+IMG can learn from both in-domain, multi-modal pre-training data sets as well as text-only, general domain ones.\nPre-training on M30kC When pre-training on the back-translated M30kC, the recall-oriented chrF3 shows a difference of 1.4 points between PBSMT and our model, mostly due to character recall; nonetheless, our model still improved by the same margin on the text-only NMT baseline. Our model still outperforms the PBSMT baseline according to BLEU4 and TER, and the text-only NMT baseline according to all metrics (p < .05).\nPre-training on WMT 2015 corpora We also pre-trained our models on the WMT 2015 corpora, which took 10 days, i.e. \u223c6\u20137 epochs. Results show that model NMTSRC+IMG improves significantly over the NMT baseline according to BLEU4, and is consistently better than the PBSMT baseline according to all four metrics.4 This is a strong indication that model NMTSRC+IMG can exploit the additional pre-training data efficiently, both general- and in-domain. While the PBSMT model is still competitive when using additional in-domain data\u2014according to METEOR and chrF3\u2014 the same cannot be said when using general-domain pre-training corpora. From our experiments, NMT models in general, and especially model NMTSRC+IMG, thrive when training and test domains are mixed, which is a very common real-world scenario.\nTextual and visual attention In Figure 2, we visualise the visual and textual attention weights for an entry of the M30kT test set. In the visual attention, the \u03b2 gate (written in parentheses after each word) caused the image features to be used mostly to generate the words Mann (man) and Hut (hat), two highly visual terms in the sentence. We observe that in general visually grounded terms, e.g. Mann and Hut, usually have a high associated \u03b2 value, whereas other less visual terms like mit (with) or auf (at) do not. That causes the model to use the image features when it is describing a vi-\n4In order for PBSMT models to remain competitive, we believe more advanced data selection techniques are needed, which are out of the scope of this work.\nsual concept in the sentence, which is an interesting feature of our model. Interestingly, our model is very selective when choosing to use image features: it only assigned \u03b2 > 0.5 for 20% of the outputted target words, and \u03b2 > 0.8 to only 8%. A manual inspection of translations shows that these words are mostly concrete nouns with a strong visual appeal.\nLastly, using two independent attention mechanisms is a good compromise between model compactness and flexibility. While the attentionbased NMT model baseline has \u223c200M parameters, model NMTSRC+IMG has \u223c213M, thus using just \u223c6.6% more parameters than the latter."}, {"heading": "6 Related work", "text": "Multi-modal MT was just recently addressed by the MT community by means of a shared task (Specia et al., 2016). However, there has been a considerable amount of work on natural language generation from non-textual inputs. Mao et al. (2014) introduced a multi-modal RNN that integrates text and visual features and applied it to the tasks of image description generation and image\u2013sentence ranking. In their work, the authors incorporate global image features in a separate multi-modal layer that merges the RNN textual representations and the global image features. Vinyals et al. (2015) proposed an influential neural IDG model based on the sequenceto-sequence framework, which is trained end-toend. Elliott et al. (2015) put forward a model to generate multilingual descriptions of images by\nlearning and transferring features between two independent, non-attentive neural image description models.5 Venugopalan et al. (2015) introduced a model trained end-to-end to generate textual descriptions of open-domain videos from the video frames based on the sequence-to-sequence framework. Finally, Xu et al. (2015) introduced the first attention-based IDG model where an attentive decoder learns to attend to different parts of an image as it generates its description in natural language.\nIn the context of NMT, Dong et al. (2015) proposed a multi-task learning approach where a model is trained to translate from one source language into multiple target languages. They used attention-based decoders where each language has one decoder RNN with a separate attention mechanism. Each translation task has a shared sourcelanguage encoder in common with all the other translation tasks. Firat et al. (2016) proposed a multi-way model trained to translate between many different source and target languages. Instead of one attention mechanism per language pair as in Dong et al. (2015), which would lead to a quadratic number of attention mechanisms in relation to language pairs, they use a shared attention mechanism where each target language has one attention shared by all source languages. Luong et al. (2016) proposed a multi-task approach where they train a model using two tasks and a shared decoder: the main task is to translate from German into English and the secondary task is to generate English image descriptions. They show improvements in the main translation task when also training for the secondary image description task. Although not an NMT model, Hitschler et al. (2016) recently used image features to re-rank translations of image descriptions generated by an SMT model and reported significant improvements.\nAlthough no purely neural multi-modal model to date significantly improves on both text-only NMT and SMT models (Specia et al., 2016), different research groups have proposed to include global and spatial visual features in re-ranking n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovicky\u0301 et al., 2016; Shah et al., 2016). To the best of our knowledge, the\n5Although their model has not been devised with translation as its primary goal, theirs is one of the baselines of the first shared task in multi-modal MT in WMT 2016 (Specia et al., 2016).\nbest published results of a purely MNMT model are those of Huang et al. (2016), who proposed to use global visual features extracted with the VGG19 network (Simonyan and Zisserman, 2015) for an entire image, and also for regions of the image obtained using the RCNN of Girshick et al. (2014). Their best model improves over a strong text-only NMT baseline and is comparable to results obtained with an SMT model trained on the same data. For that reason, their models are used as baselines in our experiments.\nOur work differs from previous work in that, first, we propose attention-based MNMT models. This is an important difference since the use of attention in NMT has become standard and is the current state-of-the-art (Jean et al., 2015; Luong et al., 2015; Firat et al., 2016; Sennrich et al., 2016b). Second, we propose a doublyattentive model where we effectively fuse two mono-modal attention mechanisms into one multimodal decoder, training the entire model jointly and end-to-end. In addition, we are interested in how to merge textual and visual representations into multi-modal representations when generating words in the target language, which differs substantially from text-only translation tasks even when these translate from many source languages into many target languages (Dong et al., 2015; Firat et al., 2016). To the best of our knowledge, we are the first to integrate multi-modal inputs in NMT via independent attention mechanisms."}, {"heading": "7 Conclusions and Future Work", "text": "We have introduced a novel attention-based, multi-modal NMT model to incorporate spatial visual information into NMT. We have reported new state-of-the-art results on the M30kT test set, improving on previous multi-modal attentionbased models. We also pre-trained our model on one in-domain multi-modal data set and many general-domain text-only MT corpora, finding that it learns efficiently and is able to exploit the additional data regardless of the domain. Our model also compares favourably to both NMT and PBSMT baselines evaluated on the same training data.\nIn the future, we will incorporate coverage into our model and study how to apply it to other Natural Language Processing tasks."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations, ICLR 2015. San Diego, California.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res. 3:1137\u20131155. http://dl.acm.org/citation.cfm?id=944919.944966.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Dcu-uva multimodal mt system report", "author": ["Iacer Calixto", "Desmond Elliott", "Stella Frank."], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 634\u2013638. http://www.aclweb.org/anthology/W/W16/W16-", "citeRegEx": "Calixto et al\\.,? 2016", "shortCiteRegEx": "Calixto et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Syntax, Semantics and Structure in Statistical Translation. page 103.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Associa-", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi-Task Learning for Multiple Language Translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-Language Image Description with Neural Sequence Models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR abs/1510.04709. http://arxiv.org/abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision and Language,", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems, NIPS, Barcelona, Spain, pages 1019\u20131027. http://papers.nips.cc/paper/6241-", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik."], "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "Girshick et al\\.,? 2014", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385 .", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "citeRegEx": "Hitschler et al\\.,? 2016", "shortCiteRegEx": "Hitschler et al\\.", "year": 2016}, {"title": "Europarl: A Parallel Corpus", "author": ["Philipp Koehn"], "venue": "troit, Michigan,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "BLEU: A Method", "author": ["Wei-Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "chrf: character n-gram f", "author": [], "venue": null, "citeRegEx": "Popovi\u0107.,? \\Q2015\\E", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. Cambridge, MA, pages", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. Montr\u00e9al, Canada, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Sequence to sequence - video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond J. Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "2015 IEEE International Conference on Computer Vision,", "citeRegEx": "Venugopalan et al\\.,? 2015", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Santiago, Chile, pages 4534\u20134542", "author": ["ICCV"], "venue": "https://doi.org/10.1109/ICCV.2015.515.", "citeRegEx": "ICCV,? 2015", "shortCiteRegEx": "ICCV", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. Boston, Massachusetts, pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "ing problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two.", "startOffset": 12, "endOffset": 87}, {"referenceID": 21, "context": "ing problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two.", "startOffset": 12, "endOffset": 87}, {"referenceID": 0, "context": "In the context of NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder, which is trained to attend to the relevant source-language words as it generates each word of the target sentence.", "startOffset": 23, "endOffset": 46}, {"referenceID": 2, "context": "models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Specia et al., 2016).", "startOffset": 180, "endOffset": 290}, {"referenceID": 3, "context": "models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Specia et al., 2016).", "startOffset": 180, "endOffset": 290}, {"referenceID": 20, "context": "models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Specia et al., 2016).", "startOffset": 180, "endOffset": 290}, {"referenceID": 0, "context": "We describe the attention-based NMT model introduced by Bahdanau et al. (2015) in this section.", "startOffset": 56, "endOffset": 79}, {"referenceID": 4, "context": "GRU (Cho et al., 2014a), where a forward RNN \u2212 \u2192 \u03a6 enc reads X word by word, from left to right, and generates a sequence of forward annota-", "startOffset": 4, "endOffset": 23}, {"referenceID": 1, "context": "These annotation vectors are in turn used by the decoder, which is essentially a neural language model (LM) (Bengio et al., 2003) conditioned on the previously emitted words and the source sentence via an attention mechanism.", "startOffset": 108, "endOffset": 129}, {"referenceID": 3, "context": "1 with the addition of a visual component to incorporate spatial visual features, and is comparable to the model evaluated by Calixto et al. (2016).", "startOffset": 126, "endOffset": 148}, {"referenceID": 14, "context": "using the 50-layer Residual network (ResNet-50) of He et al. (2015). These spatial features are the activations of the res4f layer, which can be seen as encoding an image in a 14\u00d714 grid, where each of the entries in the grid is represented by a 1024D feature vector that only encodes information about that specific region of the image.", "startOffset": 51, "endOffset": 68}, {"referenceID": 26, "context": "This visual attention computes a time-dependent image context vector it given a hidden state proposal st and the image annotation vectors A = (a1,a2, \u00b7 \u00b7 \u00b7 ,aL) using the \u201csoft\u201d attention (Xu et al., 2015).", "startOffset": 188, "endOffset": 205}, {"referenceID": 26, "context": "We use \u03b2 following Xu et al. (2015) who empirically found it to", "startOffset": 19, "endOffset": 36}, {"referenceID": 27, "context": "The Flickr30k data set contains 30k images and 5 descriptions in English for each image (Young et al., 2014).", "startOffset": 88, "endOffset": 108}, {"referenceID": 10, "context": "In this work, we use the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual expansions of the original Flickr30k: one with translated data and another one with comparable data, henceforth referred to as M30kT and M30kC, respectively.", "startOffset": 42, "endOffset": 64}, {"referenceID": 16, "context": "These include the Europarl v7 (Koehn, 2005), News Commentary and", "startOffset": 30, "endOffset": 43}, {"referenceID": 14, "context": "to the pre-trained ResNet-50 and using the activations of the res4f layer (He et al., 2015).", "startOffset": 74, "endOffset": 91}, {"referenceID": 12, "context": "We follow Gal and Ghahramani (2016) and apply dropout to the encoder bidirectional and the decoder RNN using one same mask in all time steps.", "startOffset": 10, "endOffset": 36}, {"referenceID": 28, "context": "All models are trained using stochastic gradient descent with ADADELTA (Zeiler, 2012) with minibatches of size 80 (text-only NMT) or 40", "startOffset": 71, "endOffset": 85}, {"referenceID": 7, "context": ", 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al.", "startOffset": 16, "endOffset": 43}, {"referenceID": 19, "context": ", 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and chrF3 (Popovi\u0107, 2015).", "startOffset": 49, "endOffset": 70}, {"referenceID": 18, "context": ", 2006), and chrF3 (Popovi\u0107, 2015).", "startOffset": 19, "endOffset": 34}, {"referenceID": 6, "context": "nificance with approximate randomisation for the first three metrics using the MultEval tool (Clark et al., 2011).", "startOffset": 93, "endOffset": 113}, {"referenceID": 0, "context": "This is somehow expected, since the attention mechanism in NMT (Bahdanau et al., 2015) does not explicitly take attention weights from previous", "startOffset": 63, "endOffset": 86}, {"referenceID": 22, "context": "time steps into account, an thus lacks the notion of source coverage as in SMT (Koehn et al., 2003; Tu et al., 2016).", "startOffset": 79, "endOffset": 116}, {"referenceID": 16, "context": "the concatenation of the Europarl (Koehn, 2005), Common Crawl and News Commentary corpora, used in WMT 2015 (\u223c4.", "startOffset": 34, "endOffset": 47}, {"referenceID": 20, "context": "Multi-modal MT was just recently addressed by the MT community by means of a shared task (Specia et al., 2016).", "startOffset": 89, "endOffset": 110}, {"referenceID": 25, "context": "Vinyals et al. (2015) proposed an influential neural IDG model based on the sequenceto-sequence framework, which is trained end-to-", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Elliott et al. (2015) put forward a model to generate multilingual descriptions of images by", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "5 Venugopalan et al. (2015) introduced a model trained end-to-end to generate textual descriptions of open-domain videos from the video frames based on the sequence-to-sequence frame-", "startOffset": 2, "endOffset": 28}, {"referenceID": 26, "context": "Finally, Xu et al. (2015) introduced the first attention-based IDG model where an attentive decoder learns to attend to different parts of an image as it generates its description in natural language.", "startOffset": 9, "endOffset": 26}, {"referenceID": 8, "context": "In the context of NMT, Dong et al. (2015) proposed a multi-task learning approach where a model is trained to translate from one source language into multiple target languages.", "startOffset": 23, "endOffset": 42}, {"referenceID": 11, "context": "Firat et al. (2016) proposed a multi-way model trained to translate between", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Instead of one attention mechanism per language pair as in Dong et al. (2015), which would lead to a quadratic number of attention mechanisms in relation to language pairs, they use a shared attention", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "Although not an NMT model, Hitschler et al. (2016)", "startOffset": 27, "endOffset": 51}, {"referenceID": 20, "context": "Although no purely neural multi-modal model to date significantly improves on both text-only NMT and SMT models (Specia et al., 2016), different research groups have proposed to include global and spatial visual features in re-ranking", "startOffset": 112, "endOffset": 133}, {"referenceID": 2, "context": "n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Shah et al., 2016).", "startOffset": 90, "endOffset": 198}, {"referenceID": 3, "context": "n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Shah et al., 2016).", "startOffset": 90, "endOffset": 198}, {"referenceID": 20, "context": "Although their model has not been devised with translation as its primary goal, theirs is one of the baselines of the first shared task in multi-modal MT in WMT 2016 (Specia et al., 2016).", "startOffset": 166, "endOffset": 187}, {"referenceID": 8, "context": "when these translate from many source languages into many target languages (Dong et al., 2015; Firat et al., 2016).", "startOffset": 75, "endOffset": 114}, {"referenceID": 11, "context": "when these translate from many source languages into many target languages (Dong et al., 2015; Firat et al., 2016).", "startOffset": 75, "endOffset": 114}], "year": 2017, "abstractText": "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.", "creator": "LaTeX with hyperref package"}}}