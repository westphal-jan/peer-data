{"id": "1702.01569", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Neural Semantic Parsing over Multiple Knowledge-bases", "abstract": "a fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language semantics utterances annotated with logical form. in this paper, we propose to exploit structural system regularities in language in different domains, and train semantic dictionary parsers over multiple knowledge - bases ( kbs ), while sharing information transmitted across datasets. we consequently find that we can substantially improve parsing accuracy by training a single sequence - to - sequence model over multiple kbs, when providing an encoding of the entire domain at decoding time. our model achieves state - of - the - art theoretical performance on the common overnight dataset ( containing eight domains ), improves performance over a static single kb baseline from 75. 6 % to 79. 6 %, while obtaining a 7x reduction in the number of model - parameters.", "histories": [["v1", "Mon, 6 Feb 2017 11:22:15 GMT  (60kb,D)", "http://arxiv.org/abs/1702.01569v1", null], ["v2", "Mon, 24 Apr 2017 08:34:47 GMT  (62kb,D)", "http://arxiv.org/abs/1702.01569v2", "Accepted to ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jonathan herzig", "jonathan berant"], "accepted": true, "id": "1702.01569"}, "pdf": {"name": "1702.01569.pdf", "metadata": {"source": "CRF", "title": "Neural Semantic Parsing over Multiple Knowledge-bases", "authors": ["Jonathan Herzig", "Jonathan Berant"], "emails": ["hjon@il.ibm.com", "joberant@cs.tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).\nA fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014).\nIn this paper, we suggest an orthogonal solution: to pool examples from multiple datasets in different domains, each corresponding to a separate knowledge-base (KB), and train a model over all examples. This is motivated by an observation that while KBs differ in their entities and properties, the structure of language composition repeats across domains (Figure 1). E.g., a superlative in language will correspond to an \u2018argmax\u2019, and a verb followed by a noun often denotes a join operation. A model that shares information across domains can improve generalization compared to a model that is trained on a single domain only.\nRecently, Jia and Liang (2016) and Dong and Lapata (2016) proposed sequence-to-sequence models for semantic parsing. Such neural models substantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation\nar X\niv :1\n70 2.\n01 56\n9v 1\n[ cs\n.C L\n] 6\nF eb\n2 01\n7\n(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016). We find that by providing the decoder with a representation of the domain, we can train a single model over multiple domains and substantially improve accuracy compared to models trained on each domain separately. On the OVERNIGHT dataset, this improves accuracy from 75.6% to 79.6%, setting a new state-of-the-art, while reducing the number of parameters by a factor of 7. To our knowledge, this work is the first to train a semantic parser over multiple KBs."}, {"heading": "2 Problem Setup", "text": "We briefly review the model presented by Jia and Liang (2016), which we base our model on.\nSemantic parsing can be viewed as a sequenceto-sequence problem (Sutskever et al., 2014), where a sequence of input language tokens x = x1, . . . , xm is mapped to a sequence of output logical tokens y1, . . . , yn .\nThe encoder converts x1, . . . , xm into a sequence of context sensitive embeddings b1, . . . , bm using a bidirectional RNN (Bahdanau et al., 2015): a forward RNN generates hidden states hF1 , . . . , h F m by applying the LSTM recurrence (Hochreiter and Schmidhuber, 1997): hFi = LSTM(\u03c6 (in)(xi), h F i\u22121), where \u03c6\n(in) is an embedding function mapping a word xi to a fixed-dimensional vector. A backward RNN similarly generates hidden states hBm, . . . , h B 1 by processing the input sequence in reverse. Finally, for each input position i, the representation bi is the concatenation [hFi , h B i ] . An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time. At each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj+1 based on sj and yj . Formally, the decoder is defined by the following equations:\ns1 = tanh(W (s)[hFm, h B 1 ]),\neji = s > j W (a)bi, \u03b1ji = exp(eji)\u2211m i\u2032=1 eji\u2032 ,\ncj = m\u2211 i=1 \u03b1jibi,\np(yj = w | x, y1:j\u22121) \u221d exp(U [sj , cj ]), sj+1 = LSTM([\u03c6 (out)(yj), cj ], sj),\n(1)\nwhere i \u2208 {1, . . . ,m} and j \u2208 {1, . . . , n}. The matrices W (s), W (a), U , and the embedding function \u03c6(out) are decoder parameters. We also employ attention-based copying as described by Jia and Liang (2016), but omit details for brevity.\nThe entire model is trained end-to-end by maximizing p(y | x) = \u220fn j=1 p(yj | x, y1:j\u22121)."}, {"heading": "3 Models over Multiple KBs", "text": "In this paper, we focus on a setting where we have access to K training sets from different domains, and each domain corresponds to a different KB. In all domains the input is a language utterance and the label is a logical form (we assume annotated logical forms can be converted to a single formal language such as lambda-DCS in Figure 1). While the mapping from words to KB constants is specific to each domain, we expect that the manner in which language expresses composition of meaning to be shared across domains. We now describe architectures that share information between the encoders and decoders of different domains."}, {"heading": "3.1 One-to-one model", "text": "This model is similar to the baseline model described in Section 2. As illustrated in Figure 2, it consists of a single encoder and a single decoder, which are used to generate outputs for all domains. Thus, all model parameters are shared across domains, and the model is trained from all examples. Note that the number of parameters does not depend on the number of domains K.\nSince there is no explicit representation of the domain that is being decoded, the model must learn to identify the domain given only the input. To alleviate that, we encode the k\u2019th domain by a one-hot vector dk \u2208 RK . At each step, the decoder updates the hidden state conditioned on the domain\u2019s one-hot vector, as well as on the previous hidden state, the output token and the context. Formally, for domain k, Equation 1 is changed:1\nsj+1 = LSTM([\u03c6 (out)(yj), cj , dk], sj). (2)\nRecently Johnson et al. (2016) used a similar intuition for neural machine translation, where they added an artificial token at the beginning of each source sentence to specify the target language. We implemented their approach and compare to it in Section 4.\n1For simplicity, we omit the domain index k from our notation whenever it can be inferred from context.\nSince we have one decoder for multiple domains, tokens which are not in the domain vocabulary could possibly be generated. We prevent that at test time by excluding out-of-domain tokens before the softmax (p(yj | x, y1:j\u22121)) takes place."}, {"heading": "3.2 Many-to-many model", "text": "In this model, we keep a separate encoder and decoder for every domain, but augment the model with an additional encoder that consumes examples from all domains (see Figure 2). This is motivated by prior work on domain adaptation (Daume III, 2007; Blitzer et al., 2011), where each example has a representation that captures domain-specific aspects of the example and a representation that captures domain-general aspects. In our case, this is achieved by encoding examples with a domainspecific encoder as well as a domain-general encoder, and passing both representations to the decoder.\nFormally, we now have K + 1 encoders and K decoders, and denote by hF,ki , h B,k i , b k i the forward state, backward state and their concatenation at position i (the domain-general encoder has index K + 1). The hidden state of the decoder in domain k is initialized from the domain-specific and domain-general encoder:\ns1 = tanh(W (s)[hF,km , h B,k 1 , h F,K+1 m , h B,K+1 1 ]).\nThen, we compute unnormalized attention scores based on both encoders, and represent the language context with both domain-general and domain-specific representations. Equation 1 for domain k is changed as follows:\neji = s > j W (a)[bki , b K+1 i ],\ncj = m\u2211 i=1 \u03b1ji[b k i , b K+1 i ].\nIn this model, the number of encoding parameters grows by a factor of 1k , and the number of decoding parameters grows by less than a factor of 2."}, {"heading": "3.3 One-to-many model", "text": "Here, a single encoder is shared, while we keep a separate decoder for each domain. The shared encoder captures the fact that the input in each domain is a sequence of English words. The domainspecific decoders learn to output tokens from the right domain vocabulary."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data", "text": "We evaluated our system on the OVERNIGHT semantic parsing dataset, which contains 13, 682 examples of language utterances paired with logical forms across eight domains. OVERNIGHT was constructed by generating logical forms from a grammar and annotating them with language through crowdsourcing. We evaluated on the same train/test split as Wang et al. (2015), using the same accuracy metric, that is, the proportion of questions for which the denotations of the predicted and gold logical forms are equal."}, {"heading": "4.2 Implementation Details", "text": "We replicate the experimental setup of Jia and Liang (2016): We used the same hyper-parameters without tuning; we used 200 hidden units and 100- dimensional word vectors; we initialized parameters uniformly within the interval [\u22120.1, 0.1], and maximized the log likelihood of the correct logical form with stochastic gradient descent. We trained the model for 30 epochs with an initial learning rate of 0.1, and halved the learning rate every 5 epochs, starting from epoch 15. We replaced word vectors for words that occur only once in the training set with a universal <unk> word vector. At test time, we used beam search with beam size 5. We then picked the highest-scoring logical form that does not yield an executor error when its denotation is computed. Our models were implemented in Theano (Bergstra et al., 2010)."}, {"heading": "4.3 Results", "text": "For our main result, we trained on all eight domains all models described in Section 3: ONE2ONE, DOMAINENCODING and INPUTTOKEN representing respectively the basic one-toone model, with extensions of one-hot domain encoding or an extra input token, as described in Section 3.1. MANY2MANY and ONE2MANY are the models described in Sections 3.2 and 3.3, respectively. INDEP is the baseline sequence-tosequence model described in Section 2, which trained independently on each domain.\nResults show (Table 1) that training on multiple KBs improves average accuracy over all domains for all our proposed models, and that performance improves as more parameters are shared. The strongest results come when parameter sharing is maximal (i.e., single encoder and single decoder), coupled with a one-hot domain representation at decoding time (DOMAINENCODING). In this case accuracy improves not only on average, but also for each domain separately. Moreover, the number of model parameters necessary for training the model is reduced by a factor of 7.\nOur baseline, INDEP, is a reimplementation of the NORECOMBINATION model described in Jia and Liang (2016), which achieved average accuracy of 75.8% (corresponds to our 75.6% result). Jia and Liang (2016) also introduced a framework for generating new training examples in a single domain through recombination. Their model that uses the most training data achieved state-of-theart average accuracy of 77.5% on OVERNIGHT. We show that by training over multiple KBs we can achieve higher average accuracy, and our best model, DOMAINENCODING, sets a new state-ofthe-art average accuracy of 79.6%."}, {"heading": "4.4 Analysis", "text": "Learning a semantic parser involves mapping language phrases to KB constants, as well as learning how language composition corresponds to logical form composition. We hypothesized that the main\nbenefit of training on multiple KBs lies in learning about compositionality. To verify that, we append the domain index to the name of every constant in every KB, and therefore constant names are disjoint across datasets. We train DOMAINENCODING on this dataset and obtain an accuracy of 79.1% (comparing to 79.6%), which hints that most of the gain is attributed to compositionality rather than mapping of language to KB constants.\nWe also inspected cases where DOMAINENCODING performed better than INDEP, by analyzing errors on a development set (20% of the training data). We found 45 cases where INDEP makes an error (and DOMAINENCODING does not) by predicting a wrong comparative or superlative structure (e.g., > instead of \u2265). However, the opposite case occurs only 29 times. This reiterates how we learn structural linguistic regularities when sharing parameters.\nLastly, we observed that the domain\u2019s training set size negatively correlates with its relative improvement in performance (DOMAINENCODING accuracy compared to INDEP), where Spearman\u2019s \u03c1 = \u22120.86. This could be explained by the tendency of smaller domains to cover a smaller fraction of structural regularities in language, thus, they gain more by sharing information."}, {"heading": "5 Conclusion", "text": "In this paper we address the challenge of obtaining training data for semantic parsing from a new perspective. We propose that one can improve parsing accuracy by training models over multiple KBs and demonstrate this on the eight domains of the OVERNIGHT dataset.\nIn future work, we would like to further reduce the burden of data gathering by training characterlevel models that learn to map language phrases to KB constants across datasets, and by pre-training language side models that improve the encoder from data that is independent of the KB. We also plan to apply this method on datasets where only denotations are provided rather than logical forms."}], "references": [{"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 1:49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semantic parsing via paraphrasing", "author": ["J. Berant", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Imitation learning of agenda-based semantic parsers", "author": ["J. Berant", "P. Liang."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3:545\u2013558.", "citeRegEx": "Berant and Liang.,? 2015", "shortCiteRegEx": "Berant and Liang.", "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio."], "venue": "Python for Scientific Computing Conference.", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Domain adaptation with coupled subspaces", "author": ["J. Blitzer", "S. Kakade", "D.P. Foster."], "venue": "Artificial Intelligence and Statistics (AISTATS). pages 173\u2013181.", "citeRegEx": "Blitzer et al\\.,? 2011", "shortCiteRegEx": "Blitzer et al\\.", "year": 2011}, {"title": "Multitask learning", "author": ["R. Caruana."], "venue": "Machine Learning 28:41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth."], "venue": "Computational Natural Language Learning (CoNLL). pages 18\u201327.", "citeRegEx": "Clarke et al\\.,? 2010", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa."], "venue": "Journal of Machine Learning Research (JMLR) 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daume III."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["O. Firat", "K. Cho", "Y. Bengio."], "venue": "North American Association for Computational Linguistics (NAACL).", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Data recombination for neural semantic parsing", "author": ["R. Jia", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["M. Johnson", "M. Schuster", "Q.V. Le", "M. Krikun", "Y. Wu", "Z. Chen", "N. Thorat", "F. Vigas", "M. Wattenberg", "G. Corrado", "M. Hughes", "J. Dean."], "venue": "arXiv", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Weakly supervised training of semantic parsers", "author": ["J. Krishnamurthy", "T. Mitchell."], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL). pages 754\u2013765.", "citeRegEx": "Krishnamurthy and Mitchell.,? 2012", "shortCiteRegEx": "Krishnamurthy and Mitchell.", "year": 2012}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1512\u20131523.", "citeRegEx": "Kwiatkowski et al\\.,? 2011", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Lambda dependency-based compositional semantics", "author": ["P. Liang."], "venue": "arXiv .", "citeRegEx": "Liang.,? 2013", "shortCiteRegEx": "Liang.", "year": 2013}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Association for Computational Linguistics (ACL). pages 590\u2013599.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Multi-task sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C.D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Largescale semantic parsing without question-answer pairs", "author": ["S. Reddy", "M. Lapata", "M. Steedman."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 2(10):377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["M. Zelle", "R.J. Mooney."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI). pages 1050\u20131055.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins."], "venue": "Uncertainty in Artificial Intelligence (UAI). pages 658\u2013 666.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 24, "context": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", "startOffset": 171, "endOffset": 325}, {"referenceID": 25, "context": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", "startOffset": 171, "endOffset": 325}, {"referenceID": 16, "context": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", "startOffset": 171, "endOffset": 325}, {"referenceID": 18, "context": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", "startOffset": 171, "endOffset": 325}, {"referenceID": 0, "context": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", "startOffset": 171, "endOffset": 325}, {"referenceID": 3, "context": "Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", "startOffset": 171, "endOffset": 325}, {"referenceID": 7, "context": "To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al.", "startOffset": 90, "endOffset": 160}, {"referenceID": 18, "context": "To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al.", "startOffset": 90, "endOffset": 160}, {"referenceID": 0, "context": "To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al.", "startOffset": 90, "endOffset": 160}, {"referenceID": 2, "context": ", 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al.", "startOffset": 55, "endOffset": 98}, {"referenceID": 23, "context": ", 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al.", "startOffset": 55, "endOffset": 98}, {"referenceID": 15, "context": ", 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014).", "startOffset": 39, "endOffset": 93}, {"referenceID": 21, "context": ", 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014).", "startOffset": 39, "endOffset": 93}, {"referenceID": 17, "context": "Figure 1: Examples for natural language utterances with logical forms in lambda-DCS (Liang, 2013) in different domains that share structural regularity (a comparative structure in the first example and a superlative in the second).", "startOffset": 84, "endOffset": 97}, {"referenceID": 12, "context": "Recently, Jia and Liang (2016) and Dong and Lapata (2016) proposed sequence-to-sequence models for semantic parsing.", "startOffset": 10, "endOffset": 31}, {"referenceID": 10, "context": "Recently, Jia and Liang (2016) and Dong and Lapata (2016) proposed sequence-to-sequence models for semantic parsing.", "startOffset": 35, "endOffset": 58}, {"referenceID": 6, "context": "(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016).", "startOffset": 42, "endOffset": 143}, {"referenceID": 8, "context": "(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016).", "startOffset": 42, "endOffset": 143}, {"referenceID": 19, "context": "(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016).", "startOffset": 42, "endOffset": 143}, {"referenceID": 11, "context": "(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016).", "startOffset": 42, "endOffset": 143}, {"referenceID": 14, "context": "(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016).", "startOffset": 42, "endOffset": 143}, {"referenceID": 22, "context": "Semantic parsing can be viewed as a sequenceto-sequence problem (Sutskever et al., 2014), where a sequence of input language tokens x = x1, .", "startOffset": 64, "endOffset": 88}, {"referenceID": 1, "context": ", bm using a bidirectional RNN (Bahdanau et al., 2015): a forward RNN generates hidden states h1 , .", "startOffset": 31, "endOffset": 54}, {"referenceID": 12, "context": ", h F m by applying the LSTM recurrence (Hochreiter and Schmidhuber, 1997): hi = LSTM(\u03c6 (xi), h F i\u22121), where \u03c6 (in) is an embedding function mapping a word xi to a fixed-dimensional vector.", "startOffset": 40, "endOffset": 74}, {"referenceID": 1, "context": "An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time.", "startOffset": 27, "endOffset": 70}, {"referenceID": 20, "context": "An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time.", "startOffset": 27, "endOffset": 70}, {"referenceID": 11, "context": "We briefly review the model presented by Jia and Liang (2016), which we base our model on.", "startOffset": 41, "endOffset": 62}, {"referenceID": 13, "context": "We also employ attention-based copying as described by Jia and Liang (2016), but omit details for brevity.", "startOffset": 55, "endOffset": 76}, {"referenceID": 14, "context": "Recently Johnson et al. (2016) used a similar intuition for neural machine translation, where they added an artificial token at the beginning of each source sentence to specify the target language.", "startOffset": 9, "endOffset": 31}, {"referenceID": 5, "context": "This is motivated by prior work on domain adaptation (Daume III, 2007; Blitzer et al., 2011), where each example has a representation that captures domain-specific aspects of the example and a representation that captures domain-general aspects.", "startOffset": 53, "endOffset": 92}, {"referenceID": 23, "context": "We evaluated on the same train/test split as Wang et al. (2015), using the same accuracy metric, that is, the proportion of questions for which the denotations of the predicted and gold logical forms are equal.", "startOffset": 45, "endOffset": 64}, {"referenceID": 4, "context": "Our models were implemented in Theano (Bergstra et al., 2010).", "startOffset": 38, "endOffset": 61}, {"referenceID": 12, "context": "We replicate the experimental setup of Jia and Liang (2016): We used the same hyper-parameters without tuning; we used 200 hidden units and 100dimensional word vectors; we initialized parameters uniformly within the interval [\u22120.", "startOffset": 39, "endOffset": 60}, {"referenceID": 13, "context": "Our baseline, INDEP, is a reimplementation of the NORECOMBINATION model described in Jia and Liang (2016), which achieved average accuracy of 75.", "startOffset": 85, "endOffset": 106}, {"referenceID": 13, "context": "Our baseline, INDEP, is a reimplementation of the NORECOMBINATION model described in Jia and Liang (2016), which achieved average accuracy of 75.8% (corresponds to our 75.6% result). Jia and Liang (2016) also introduced a framework for generating new training examples in a single domain through recombination.", "startOffset": 85, "endOffset": 204}], "year": 2017, "abstractText": "A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-tosequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-ofthe-art performance on the OVERNIGHT dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.", "creator": "LaTeX with hyperref package"}}}