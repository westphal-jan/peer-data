{"id": "1702.02390", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "abstract": "in this paper proposal we explore to the effect of architectural choices needed on learning a variational autoencoder ( vae ) for text generation. in partial contrast to the previously introduced vae model for text where both the encoder and decoder are rnns, we propose a promising novel hybrid architecture that blends simple fully feed - forward convolutional and deconvolutional components generated with a recurrent language model. our architecture exhibits several proven attractive properties such as their faster run time and convergence, ability to better handle long sequences and, more importantly, it best helps to avoid some of the major difficulties posed by training vae models on textual data.", "histories": [["v1", "Wed, 8 Feb 2017 12:11:41 GMT  (468kb,D)", "http://arxiv.org/abs/1702.02390v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanislau semeniuta", "aliaksei severyn", "erhardt barth"], "accepted": true, "id": "1702.02390"}, "pdf": {"name": "1702.02390.pdf", "metadata": {"source": "CRF", "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "authors": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "emails": ["stas@inb.uni-luebeck.de", "barth@inb.uni-luebeck.de", "severyn@google.com", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid"], "sections": [{"heading": "1 Introduction", "text": "Generative models of texts are currently at the cornerstone of natural language understanding enabling recent breakthroughs in machine translation (Bahdanau et al., 2014; Wu et al., 2016), dialogue modelling (Serban et al., 2016), abstractive summarization (Rush et al., 2015), etc.\nCurrently, RNN-based generative models hold state of the art results in both unconditional (Jo\u0301zefowicz et al., 2016; Ha et al., 2016) and conditional (Vinyals et al., 2014) text generation. At a high level, these models represent a class of autoregressive models that work by generating outputs sequentially one step at a time where the next predicted element is conditioned on the history of elements generated thus far.\nVariational Autoencoders (VAE), recently introduced by (Kingma and Welling, 2013; Rezende et al., 2014), offer a different approach to generative modeling by integrating stochastic latent variables into the conventional autoencoder architec-\nture. The primary purpose of learning VAE-based generative models is to be able to generate realistic examples as if they were drawn from the input data distribution by simply feeding noise vectors through the decoder. Additionally, the latent representations obtained by applying the encoder to input examples give a fine-grained control over the generation process that is harder to achieve with more conventional autoregressive models. Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style (Yan et al., 2015; Larsen et al., 2015), in text generation it should be possible to also control various attributes of the generated sentences, such as, for example, sentiment or writing style.\nWhile training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al., 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015). As discussed by Bowman et al. (2016), the core difficulty of training VAE models is the collapse of the latent loss (represented by the KL divergence term) to zero. In this case the generator tends to completely ignore latent represen-\nar X\niv :1\n70 2.\n02 39\n0v 1\n[ cs\n.C L\n] 8\nF eb\n2 01\n7\ntations and reduces to a standard language model. This is largely due to the high modeling power of the RNN-based decoders, which with sufficiently small history can achieve low reconstruction errors while not relying on the latent vector provided by the encoder. Table 1 shows that a VAE model where the KL term collapses to zero generates repeating and uninteresting samples.\nIn this paper, we propose a novel VAE model for texts that is more effective at forcing the decoder to make use of latent vectors. Contrary to existing work, where both encoder and decoder layers are LSTMs, the core of our model is a feed-forward architecture composed of one-dimensional convolutional and deconvolutional (Zeiler et al., 2010) layers. This choice of architecture helps to gain more control over the KL term, which is crucial for training a VAE model. Given the difficulty of generating long sequences in a fully feed-forward manner, we augment our network with an RNN language model layer. To the best of our knowledge, this paper is the first work that successfully applies deconvolutions in the decoder of a latent variable generative model of natural text. We empirically verify that our model is easier to train than its fully recurrent alternative, which, in our experiments, fails to converge on longer texts. To better understand why training VAEs for texts is difficult we carry out detailed experiments, discuss optimization difficulties, and propose effective ways to address them. Finally, we demonstrate that sampling from our model yields realistic texts."}, {"heading": "2 Related Work", "text": "Currently, there are three major streams of approaches to generative modeling: the Variational Autoencoder (Kingma and Welling, 2013; Rezende et al., 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al., 2016) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014).\nAutoregressive models are built on the assumption that the current data element can be accurately predicted given sufficient history of elements generated thus far. The conventional RNN based language models fall into this category and currently dominate the language modeling and generation problem in NLP. Neural architectures based on recurrent (Jo\u0301zefowicz et al., 2016; Zoph and Le, 2016; Ha et al., 2016) or convolutional decoders\n(Kalchbrenner et al., 2016; Dauphin et al., 2016) provide an effective solution to this problem.\nGANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al., 2016; Zhang and Gan, 2016).\nBowman et al. (2016) is a recent work that tackles language generation problem within the VAE framework. The authors demonstrate that with some care it is possible to successfully learn a latent variable generative model of text. Although their model is slightly outperformed by a traditional LSTM (Hochreiter and Schmidhuber, 1997) language model, their model achieves a similar effect as in computer vision where one can (i) sample realistic sentences by feeding randomly generated novel latent vectors through the decoder and (ii) linearly interpolate between two points in the latent space. Miao et al. (2015) apply VAE to bagof-words representations of documents and the answer selection problem achieving good results on both tasks. Zhang et al. (2016) and Serban et al. (2016) apply VAE to sequence-to-sequence problems, improving over deterministic alternatives.\nVarious techniques to improve training of VAE models where the total cost represents a trade-off between the reconstruction cost and KL term have been used so far: KL-term annealing and input dropout (Bowman et al., 2016; S\u00f8nderby et al., 2016), imposing structured sparsity on latent variables (Yeung et al., 2016). In Section 3.4 we propose another efficient technique to control the trade-off between KL and reconstruction terms."}, {"heading": "3 Model", "text": "In this section we first briefly explain the VAE framework of Kingma and Welling (2013), then describe our hybrid architecture where the feedforward part is composed of a fully convolutional encoder and a decoder that combines deconvolutional layers and a conventional RNN. Finally, we discuss optimization recipes that help VAE to respect latent variables, which is critical in smooth-\ning out the latent space and being able to sample realistic sentences."}, {"heading": "3.1 Variational Autoencoder", "text": "The VAE is a recently introduced latent variable generative model, which combines Variational Inference with Deep Learning. It modifies the conventional Autoencoder framework in two key ways. Firstly, a deterministic internal representation z (provided by the encoder) of an input x is replaced with a posterior distribution q(z|x). Inputs are then reconstructed by sampling z from this posterior and passing them through a decoder. To make sampling easy, the posterior distribution is usually parametrized by a Gaussian with its mean and variance predicted by the encoder. Secondly, to ensure that we can sample from any point of the latent space and still generate valid and diverse outputs, the posterior q(z|x) is regularized with its KL divergence from a prior distribution p(z). The prior is typically chosen to be also a Gaussian with zero mean and unit variance, such that the KL term between posterior and prior can be computed in closed form (Kingma and Welling, 2013). The total VAE cost is composed of the reconstruction term, i.e., negative log-likelihood of the data, and the KL regularizer:\nJvae = KL(q(z|x)||p(z)) \u2212Eq(z|x)[log p(x|z)]\n(1)\nKingma and Welling (2013) show that the loss function from Eq (1) can be derived from the probabilistic model perspective and it is an upper bound on the true negative likelihood of the data.\nOne can view a VAE as a traditional Autoencoder with some restrictions imposed on the internal representation space. Specifically, using a sample from the q(z|x) to reconstruct the input instead of a deterministic z, forces the model to map an input to a region of the space rather than to a single point. The most straight-forward way to achieve a good reconstruction error in this case is to predict a very sharp probability distribution effectively corresponding to a single point in the latent space (Raiko et al., 2014). The additional KL term in Eq (1) prevents this behavior and forces the model to find a solution with, on one hand, low reconstruction error and, on the other, predicted posterior distributions close to the prior. Thus, the decoder part of the VAE is capable of reconstructing a sensible data sample from every point in the latent space that has non-zero probability under the\nprior. This allows for straightforward generation of novel samples and linear operations on the latent codes. Bowman et al. (2016) demonstrate that this does not work in the fully deterministic Autoencoder framework . In addition to regularizing the latent space, KL term indicaes how much information the VAE stores in the latent vector.\nBowman et al. (2016) propose a VAE model for text generation where both encoder and decoder are LSTM networks (Figure 1). We will refer to this model as LSTM VAE in the remainder of the paper. The authors show that adapting VAEs to text generation is more challenging as the decoder tends to ignore the latent vector (KL term is close to zero) and falls back to a language model. Two training tricks are required to mitigate this issue: (i) KL-term annealing where its weight in Eq (1) gradually increases from 0 to 1 during the training; and (ii) applying dropout to the inputs of the decoder to limit its expressiveness and thereby forcing the model to rely more on the latent variables. We will discuss these tricks in more detail in Section 3.4. Next we describe a deconvolutional layer, which is the core element of the decoder in our VAE model."}, {"heading": "3.2 Deconvolutional Networks", "text": "A deconvolutional layer (also referred to as transposed convolutions (Gulrajani et al., 2016) and fractionally strided convolutions (Radford et al., 2015)) performs spatial up-sampling of its inputs and is an integral part of latent variable generative models of images (Radford et al., 2015; Gulrajani et al., 2016) and semantic segmentation algorithms (Noh et al., 2015). Its goal is to perform an \u201cinverse\u201d convolution operation and increase spatial size of the input while decreasing the number of feature maps. This operation can be viewed as a backward pass of a convolutional layer and can be implemented by simply switching the forward and backward passes of the convolution operation. In the context of generative modeling based on global representations, the deconvolutions are typically used as follows: the global representation is first linearly mapped to another representation with small spatial resolution and large number of feature maps. A stack of deconvolutional layers is then applied to this representation, each layer progressively increasing spatial resolution and decreasing the amount of feature channels. The output of the last layer is an image or, in our case, a\ntext fragment. A notable example of such a model is the Deep Network of (Radford et al., 2015) trained with adversarial objective. Our model uses a similar approach but is instead trained with the VAE objective.\nThere are two primary motivations for choosing deconvolutional layers instead of the dominantly used recurrent ones: firstly, such layers have extremely efficient GPU implementations due to their fully parallel structure. Secondly, feed-forward architectures are typically easier to optimize than their recurrent counterparts, as the number of back-propagation steps is constant and potentially much smaller than in RNNs. Both points become significant as the length of the generated text increases. Next, we describe our VAE architecture that blends deconvolutional and RNN layers in the decoder to allow for better control over the KL-term."}, {"heading": "3.3 Hybrid Convolutional-Recurrent VAE", "text": "Our model is composed of two relatively independent modules. The first component is a standard VAE where the encoder and decoder modules are parametrized by convolutional and deconvolutional layers respectively (see Figure 2(a)). This\narchitecture is attractive for its computational efficiency and simplicity of training.\nThe other component is a recurrent language model consuming activations from the deconvolutional decoder concatenated with the previous output characters. We consider two flavors of recurrent functions: a conventional LSTM network (Figure 2(b)) and a stack of masked convolutions also known as the ByteNet decoder from Kalchbrenner et al. (2016) (Figure 2(c)). The primary reason for having a recurrent component in the decoder is to capture dependencies between elements of the text sequences \u2013 a hard task for a fully feed-forward architecture. Indeed, the conditional distribution P (x|z) = P (x1, . . . , xn|z) of generated sentences cannot be richly represented with a feedforward network. Instead, it is closer to: P (x1, . . . , xn|z) = \u220f i P (xi|z) where components are independent of each other and are conditioned only on z. To minimize the reconstruction cost the model is forced to encode every detail of a text fragment. A recurrent language model instead models the full joint distribution of output sequences without having to make independence assumptions P (x1, . . . , xn|z) =\u220f\ni P (xi|xi\u22121, . . . , x1, z). Thus, adding a re-\ncurrent layer on top of our fully feed-forward encoder-decoder architecture relieves it from encoding every aspect of a text fragment into the latent vector and allows it to instead focus on more high-level semantic and stylistic features.\nNote that the feed-forward part of our model is different from the existing fully convolutional approaches of Dauphin et al. (2016) and Kalchbrenner et al. (2016) in two respects: firstly, while being fully parallelizable during training, these models still require predictions from previous time steps during inference and thus behave as a variant of recurrent networks. In contrast, expansion of the z vector is fully parallel in our model (except for the recurrent component). Secondly, our model down- and up-samples a text fragment during processing while the existing fully convolutional decoders do not. Preserving spatial resolution can be beneficial to the overall result, but comes at a higher computational cost."}, {"heading": "3.4 Optimization Difficulties", "text": "The addition of the recurrent component results in optimization difficulties that are similar to those described by Bowman et al. (2016). In most cases the model converges to a solution with a vanishingly small KL term, thus effectively falling back to a conventional language model. Bowman et al. (2016) have proposed to use input dropout and KL term annealing to encourage their model to encode meaningful representations into the z vector. We found that these techniques also help our model to achieve solutions with non-zero KL term.\nKL term annealing can be viewed as a gradual transition from conventional deterministic Autoencoder to a full VAE. In this work we use linear annealing from 0 to 1. We have experimented with other schedules but did not find them to have a significant impact on the final result. As long as the KL term weight starts to grow sufficiently slowly, the exact shape and speed of its growth does not seem to affect the overall result.\nWhile helping to regularize the latent vector, input dropout tends to slow down convergence. We propose an alternative technique to encourage the model to compress information into the latent vector: in addition to the reconstruction cost computed on the outputs of the recurrent language model, we also add an auxiliary reconstruction term computed from the activations of the last de-\nconvolutional layer:\nJaux = \u2212Eq(z|x)[log p(x|z)]. (2)\nSince at this layer the model does not have access to previous output elements it needs to rely on the z vector to produce a meaningful reconstruction. The final cost minimized by our model is:\nJhybrid = Jvae + \u03b1Jaux (3)\nwhere \u03b1 is a hyperparameter, Jaux is the intermediate reconstruction term and Jvae is the bound from Eq (1). The objective function from Eq (3) puts a mild constraint on the latent vector to produce features useful for historyless reconstruction. Since the autoregressive part reuses these features, it also improves the main reconstruction term. We are thus able to encode information in the latent vector without hurting expressiveness of the decoder."}, {"heading": "4 Experiments", "text": "We use KL term annealing and input dropout when training the LSTM VAE models from Bowman et al. (2016) and KL term annealing and regularized objective function from Eq (3) when training our models. All models were trained with the Adam optimization algorithm (Kingma and Ba, 2014) with decaying learning rate. We use Layer Normalization (Ba et al., 2016) in LSTM layers and Batch Normalization (Ioffe and Szegedy, 2015) in convolutional and deconvolutional layers. To make our results easy to reproduce we have released the source code of all our experiments1.\nData. Our first task is character-level language generation performed on the standard Penn Treebank dataset (Marcus et al., 1993). One of the goals is to test the ability of the models to successfully learn the representations of long sequences. For training, fixed-size data samples are selected from random positions in the standard training and validation sets."}, {"heading": "4.1 Comparison with LSTM VAE", "text": "Historyless decoding. We start with an experiment where the decoder is forced to ignore the history and has to rely fully on the latent vector. By conditioning the decoder only on the latent vector z we can directly compare the expressiveness of the compared models. For the LSTM\n1https://github.com/stas-semeniuta/ textvae\nVAE model historyless decoding is achieved by using the dropout on the input elements with the dropout rate equal to 1. We compare it to our fullyfeedforward model without the recurrent layer in the decoder (Figure 2(a)). Both networks are parametrized to have equal number of parameters.\nTo test how well both models can cope with the stochasticity of the latent vectors, we minimize only the reconstruction term from Eq. (1). This is equivalent to a pure Autoencoder setting with stochastic internal representation and no regularization of the latent space. This experiment corresponds to an initial stage of training with KL term annealing when its weight is set to 0.\nThe results are presented in Figure 3. Note that when the length of input samples reaches 30 characters, the historyless LSTM autoencoder fails to fit the data well, while the convolutional architecture converges almost instantaneously. The results appear even worse for LSTMs on sequences of 50 characters. To make sure that this effect is not caused by optimization difficulties, i.e. exploding gradients (Pascanu et al., 2013), we have searched over learning rates, gradient clipping thresholds and sizes of LSTM layers but were only able to get results comparable to those shown in Figure 3. Note that LSTM networks make use of Layer Normalization (Ba et al., 2016) which has been shown to make training of such networks easier. These results suggest that our model is easier to train than the LSTM-based model, especially for modeling longer pieces of text. Additionally, our model is computationally faster by a factor of roughly two, since we run only one recurrent network per sample and time complexity of the convolutional part is negligible in comparison.\nDecoding with history. We now move to a case where the decoder is conditioned on both the latent vector and previous output elements. In these experiments we pursue two goals: firstly, we verify whether the results obtained on the historyless de-\ncoding task also generalize to a less restricted case. Secondly, we study how well the models cope with stochasticity introduced by the latent variables.\nWe fix input dropout rates at 0.2 and 0.5 for LSTM VAE and use auxiliary reconstruction loss (Section 3.4) with 0.2 weight in our Hybrid model. The bits-per-character scores on differently sized text samples are presented in Figure 4. As discussed in Section 3.1, the KL term value indicates how much information the network stores in the latent vector. We observe that the amount of information stored in the latent vector by our model and the LSTM VAE is comparable when we train on short samples and largely depends on hyper-parameters \u03b1 and p . When the length of a text fragment increases, LSTM VAE is able to put less information into the latent vector (i.e., the KL component is small) and for texts longer than 48 characters, the KL term drops to almost zero while for our model the ratio between KL and reconstruction terms stays roughly constant. This suggests that our model is better at encoding latent representations of long texts since the amount of\ninformation in the latent vector does not decrease as the length of a text fragment grows. In contrast, there is a steady decline of the KL term of the LSTM VAE model. This result is consistent with our findings from the historyless decoding experiment. Note that in both of these experiments the LSTM VAE model fails to produce meaningful latent vectors with inputs over 50 characters long. This further suggests that our Hybrid model encodes long texts better than the LSTM VAE."}, {"heading": "4.2 Controlling the KL term", "text": "We study the effect of various training techniques that help control the KL term which is crucial for training a generative VAE model.\nAux cost weight. First, we provide a detailed view of how optimization tricks discussed in Section 3.4 affect the performance of our Hybrid model. Figure 5 presents results of our model trained with different values of \u03b1 from Eq. (3). Note that the inclusion of the auxiliary reconstruction loss slightly harms the bound on the likelihood of the data but helps the model to rely more on the latent vector as \u03b1 grows. A similar effect on model\u2019s bound was observed by Bowman et al. (2016): increased input dropout rates force their model to put more information into the z vector but at the cost of increased final loss values. This is a trade-off that allows for sampling outputs in the VAE framework. Note that our model can find a solution with non-trivial latent vectors when trained with the full VAE loss provided that the \u03b1 hyper-parameter is large enough. Combining it\nwith KL term annealing helps to find non-zero KL term solutions at smaller \u03b1 values.\nReceptive field. The goal of this experiment is to study the relationship between the KL term values and the expressiveness of the decoder. Without KL term annealing and input dropout, the RNN decoder in LSTM VAE tends to completely ignore information stored in the latent vector and essentially falls back to an RNN language model. To have a full control over the receptive field size of the recurrent component in our decoder, we experiment with masked convolutions (Figure 2(c)), which is similar to the decoder in ByteNet model from Kalchbrenner et al. (2016). We fix the size of the convolutional kernels to 2 and do not use dilated convolutions and skip connections as in the original ByteNet.\nThe resulting receptive field size of the recurrent layer in our decoder is equal to N + 1 characters, where N is the number of convolutional layers. We vary the number of layers to find the amount of preceding characters that our model can consume without collapsing the KL term to zero.\nResults of these experiments are presented in Figure 6. Interestingly, with the receptive field size larger than 3 and without the auxiliary reconstruction term from Eq. (3) (\u03b1 = 0) the KL term collapses to zero and the model falls back to a pure language model. This suggests that the training signal received from the previous characters is much stronger than that from the input to be reconstructed. Using the auxiliary reconstruction term, however, helps to find solutions with non-\nzero KL term component irrespective of receptive field size. Note that increasing the value of \u03b1 results in stronger values of KL component. This is consistent with the results obtained with LSTM decoder in Figure 5."}, {"heading": "4.3 Generating Tweets", "text": "In this section we present qualitative results on the task of generating tweets.\nData. We use 1M tweets2 to train our model and test it on a held out dataset of 10k samples. We minimally preprocess tweets by only replacing user ids and urls with \u201c@userid\u201d and \u201curl\u201d.\nSetup. We use 5 convolutional layers with the ReLU non-linearity, kernel size 3 and stride 2 in the encoder. The number of feature maps is [128, 256, 512, 512, 512] for each layer respectively. The decoder is configured equivalently but with the amount of feature maps decreasing in each consecutive layer. The top layer is an LSTM with 1000 units. We have not observed significant overfitting. The baseline LSTM VAE model contained two distinct LSTMs both with 1000 cells. The models have comparable number of parameters: 10.5M for the LSTM VAE model and 10.8M for our hybrid model.\nResults. Both VAE models are trained on the character-level generation. The breakdown of to-\n2a random sample collected using the Twitter API\ntal cost into KL and reconstruction terms is given in Table 3. Note that while the total cost values are comparable, our model puts more information into the latent vector, further supporting our observations from Section 4.1. This is reflected in the random samples from both models, presented in Table 2. We perform greedy decoding during generation so any variation in samples is only due to the latent vector. LSTM VAE produces very limited range of tweets and tends to repeat \u201d@userid\u201d sequence, while our model produces much more diverse samples."}, {"heading": "5 Conclusions", "text": "We have introduced a novel generative model of natural texts based on the VAE framework. Its core components are a convolutional encoder and a deconvolutional decoder combined with a recurrent layer. We have shown that the feed-forward part of our model architecture makes it easier to train a VAE and avoid the problem of KL-term collapsing to zero, where the decoder falls back to a standard language model thus inhibiting the sampling ability of VAE. Additionally, we propose a more natural way to encourage the model to rely on the latent vector by introducing an additional cost term in the training objective. We observe that it works well on long sequences which is hard to achieve with purely RNN-based VAEs using the previously proposed tricks such as KL-term annealing and input dropout. Finally, we have extensively evaluated the trade-off between the KLterm and the reconstruction loss. In particular, we investigated the effect of the receptive field size on the ability of the model to respect the latent vector which is crucial for being able to generate realistic and diverse samples. In future work we plan to apply our VAE model to semi-supervised NLP tasks and experiment with conditioning generation on various text attributes such as sentiment and writing style."}, {"heading": "Acknowledgments", "text": "We thank Enrique Alfonseca, Katja Filippova, Sylvain Gelly, Jason Lee and David Weiss for their useful feedback while preparing this draft. This project has received funding from the European Union\u2019s Framework Programme for Research and Innovation HORIZON 2020 (2014- 2020) under the Marie Skodowska-Curie Agreement No. 641805. Stanislau Semeniuta thanks the support from Pattern Recognition Company GmbH. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research."}, {"heading": "A Supplementary Material", "text": "In this supplemental material we include more random samples from our hybrid VAE model and LSTM VAE from (Bowman et al., 2016). We also perform linear operations in the latent space to demonstrate that they result in smooth and syntactically correct transitions between generated texts.\nA.1 Random Samples Table 4 contains random samples generated by our hybrid model with LSTM decoder while Table 5 contains samples from LSTM VAE. As already demonstrated by our experiments in Section 4.3, for LSTM VAE model it is much harder to find solutions with KL terms significantly different from zero, which results in redundant and uninteresting samples generated by the model. We have experimented with larger dropout rates but found that this did not lead to better samples. In contrast, samples from our model have much more variation both in the length and the content.\nA.2 Linear operations in the latent space Bowman et al. (2016) have shown that LSTMbased VAE for text produces latent representations such that linear operations on latent vectors result in meaningful transformations of generated texts. In Table 6 we present interpolation examples produced by our hybrid model. Note gradual changes in length of generated tweets, consistent usage of topics along trajectories and generally sensible structure."}], "references": [{"title": "Layer normalization", "author": ["Lei Jimmy Ba", "Ryan Kiros", "Geoffrey E. Hinton."], "venue": "CoRR abs/1607.06450.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "An architecture for deep, hierarchical generative models", "author": ["Philip Bachman."], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, NIPS, pages 4826\u20134834.", "citeRegEx": "Bachman.,? 2016", "shortCiteRegEx": "Bachman.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal J\u00f3zefowicz", "Samy Bengio."], "venue": "CONLL. pages 10\u201321.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Language modeling with gated convolutional networks", "author": ["Yann N. Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "CoRR abs/1612.08083.", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Robert Fergus."], "venue": "CoRR abs/1506.05751.", "citeRegEx": "Denton et al\\.,? 2015", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Sequential neural models with stochastic layers", "author": ["Marco Fraccaro", "S\u00f8ren Kaae S\u00f8 nderby", "Ulrich Paquet", "Ole Winther."], "venue": "NIPS, pages 2199\u20132207.", "citeRegEx": "Fraccaro et al\\.,? 2016", "shortCiteRegEx": "Fraccaro et al\\.", "year": 2016}, {"title": "Generative adversarial networks", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio."], "venue": "CoRR abs/1406.2661.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Pixelvae: A latent variable model for natural images", "author": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David V\u00e1zquez", "Aaron C. Courville."], "venue": "CoRR abs/1611.05013.", "citeRegEx": "Gulrajani et al\\.,? 2016", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2016}, {"title": "Hypernetworks", "author": ["David Ha", "Andrew M. Dai", "Quoc V. Le."], "venue": "CoRR abs/1609.09106.", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "ICML. pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "CoRR abs/1602.02410.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2016", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "CoRR abs/1610.10099.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Autoencoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling."], "venue": "CoRR abs/1312.6114.", "citeRegEx": "Kingma and Welling.,? 2013", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "The neural autoregressive distribution estimator", "author": ["Hugo Larochelle", "Iain Murray."], "venue": "AISTATS. pages 29\u201337.", "citeRegEx": "Larochelle and Murray.,? 2011", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther."], "venue": "CoRR abs/1512.09300. http://arxiv.org/abs/1512.09300.", "citeRegEx": "Larsen et al\\.,? 2015", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom."], "venue": "CoRR abs/1511.06038.", "citeRegEx": "Miao et al\\.,? 2015", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["Hyeonwoo Noh", "Seunghoon Hong", "Bohyung Han."], "venue": "CoRR abs/1505.04366.", "citeRegEx": "Noh et al\\.,? 2015", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML. pages 1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "CoRR abs/1511.06434.", "citeRegEx": "Radford et al\\.,? 2015", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Tapani Raiko", "Mathias Berglund", "Guillaume Alain", "Laurent Dinh."], "venue": "CoRR abs/1406.2989.", "citeRegEx": "Raiko et al\\.,? 2014", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."], "venue": "ICML. pages 1278\u20131286.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "CoRR abs/1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen."], "venue": "CoRR abs/1606.03498.", "citeRegEx": "Salimans et al\\.,? 2016", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio."], "venue": "CoRR abs/1605.06069.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Ladder variational autoencoders", "author": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther."], "venue": "CoRR abs/1602.02282. https://arxiv.org/abs/1602.02282.", "citeRegEx": "S\u00f8nderby et al\\.,? 2016", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu."], "venue": "CoRR abs/1601.06759.", "citeRegEx": "Oord et al\\.,? 2016", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CoRR abs/1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee."], "venue": "CoRR abs/1512.00570. http://arxiv.org/abs/1512.00570.", "citeRegEx": "Yan et al\\.,? 2015", "shortCiteRegEx": "Yan et al\\.", "year": 2015}, {"title": "Epitomic variational autoencoder", "author": ["Serena Yeung", "Anitha Kannan", "Yann Dauphin", "Li Fei-Fei."], "venue": "submission to ICLR 2017 http://ai.stanford.edu/ syyeung/resources/YeuKanDauLi16.pdf.", "citeRegEx": "Yeung et al\\.,? 2016", "shortCiteRegEx": "Yeung et al\\.", "year": 2016}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."], "venue": "CoRR abs/1609.05473.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Deconvolutional networks", "author": ["Matthew D. Zeiler", "Dilip Krishnan", "Graham W. Taylor", "Robert Fergus."], "venue": "CVPR. pages 2528\u20132535.", "citeRegEx": "Zeiler et al\\.,? 2010", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}, {"title": "Variational neural machine translation", "author": ["Biao Zhang", "Deyi Xiong", "Jinsong Su."], "venue": "CoRR abs/1605.07869.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Generating text via", "author": ["Yizhe Zhang", "Zhe Gan"], "venue": null, "citeRegEx": "Zhang and Gan.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Gan.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Generative models of texts are currently at the cornerstone of natural language understanding enabling recent breakthroughs in machine translation (Bahdanau et al., 2014; Wu et al., 2016), dialogue modelling (Serban et al.", "startOffset": 147, "endOffset": 187}, {"referenceID": 27, "context": ", 2016), dialogue modelling (Serban et al., 2016), abstractive summarization (Rush et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 25, "context": ", 2016), abstractive summarization (Rush et al., 2015), etc.", "startOffset": 35, "endOffset": 54}, {"referenceID": 12, "context": "Currently, RNN-based generative models hold state of the art results in both unconditional (J\u00f3zefowicz et al., 2016; Ha et al., 2016) and conditional (Vinyals et al.", "startOffset": 91, "endOffset": 133}, {"referenceID": 9, "context": "Currently, RNN-based generative models hold state of the art results in both unconditional (J\u00f3zefowicz et al., 2016; Ha et al., 2016) and conditional (Vinyals et al.", "startOffset": 91, "endOffset": 133}, {"referenceID": 30, "context": ", 2016) and conditional (Vinyals et al., 2014) text generation.", "startOffset": 24, "endOffset": 46}, {"referenceID": 15, "context": "Variational Autoencoders (VAE), recently introduced by (Kingma and Welling, 2013; Rezende et al., 2014), offer a different approach to genera-", "startOffset": 55, "endOffset": 103}, {"referenceID": 24, "context": "Variational Autoencoders (VAE), recently introduced by (Kingma and Welling, 2013; Rezende et al., 2014), offer a different approach to genera-", "startOffset": 55, "endOffset": 103}, {"referenceID": 31, "context": "Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style (Yan et al., 2015; Larsen et al., 2015), in", "startOffset": 167, "endOffset": 206}, {"referenceID": 17, "context": "Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style (Yan et al., 2015; Larsen et al., 2015), in", "startOffset": 167, "endOffset": 206}, {"referenceID": 1, "context": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al.", "startOffset": 119, "endOffset": 158}, {"referenceID": 8, "context": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al.", "startOffset": 119, "endOffset": 158}, {"referenceID": 6, "context": ", 2016) and speech (Fraccaro et al., 2016), their application to natural text generation requires additional care (Bowman et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 3, "context": ", 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015).", "startOffset": 79, "endOffset": 119}, {"referenceID": 19, "context": ", 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015).", "startOffset": 79, "endOffset": 119}, {"referenceID": 1, "context": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al., 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015). As discussed by Bowman et al. (2016), the core difficulty of training VAE models is the collapse of the latent loss (represented by the KL divergence term) to zero.", "startOffset": 120, "endOffset": 344}, {"referenceID": 34, "context": "lutional and deconvolutional (Zeiler et al., 2010) layers.", "startOffset": 29, "endOffset": 50}, {"referenceID": 15, "context": "Currently, there are three major streams of approaches to generative modeling: the Variational Autoencoder (Kingma and Welling, 2013; Rezende et al., 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al.", "startOffset": 107, "endOffset": 155}, {"referenceID": 24, "context": "Currently, there are three major streams of approaches to generative modeling: the Variational Autoencoder (Kingma and Welling, 2013; Rezende et al., 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al.", "startOffset": 107, "endOffset": 155}, {"referenceID": 16, "context": ", 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al., 2016) and Generative Adversarial Networks (GAN) (Goodfellow et al.", "startOffset": 31, "endOffset": 87}, {"referenceID": 7, "context": ", 2016) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014).", "startOffset": 50, "endOffset": 75}, {"referenceID": 3, "context": ", 2016) or convolutional decoders Figure 1: LSTM VAE model of (Bowman et al., 2016)", "startOffset": 62, "endOffset": 83}, {"referenceID": 13, "context": "(Kalchbrenner et al., 2016; Dauphin et al., 2016) provide an effective solution to this problem.", "startOffset": 0, "endOffset": 49}, {"referenceID": 4, "context": "(Kalchbrenner et al., 2016; Dauphin et al., 2016) provide an effective solution to this problem.", "startOffset": 0, "endOffset": 49}, {"referenceID": 22, "context": "GANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al.", "startOffset": 68, "endOffset": 134}, {"referenceID": 5, "context": "GANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al.", "startOffset": 68, "endOffset": 134}, {"referenceID": 26, "context": "GANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al.", "startOffset": 68, "endOffset": 134}, {"referenceID": 33, "context": ", 2016) but so far have gained little traction in the NLP community (Yu et al., 2016; Zhang and Gan, 2016).", "startOffset": 68, "endOffset": 106}, {"referenceID": 36, "context": ", 2016) but so far have gained little traction in the NLP community (Yu et al., 2016; Zhang and Gan, 2016).", "startOffset": 68, "endOffset": 106}, {"referenceID": 3, "context": "Bowman et al. (2016) is a recent work that tackles language generation problem within the VAE framework.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Although their model is slightly outperformed by a traditional LSTM (Hochreiter and Schmidhuber, 1997) language model, their model achieves a similar effect as in computer vision where one can (i) sam-", "startOffset": 68, "endOffset": 102}, {"referenceID": 19, "context": "Miao et al. (2015) apply VAE to bagof-words representations of documents and the an-", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Various techniques to improve training of VAE models where the total cost represents a trade-off between the reconstruction cost and KL term have been used so far: KL-term annealing and input dropout (Bowman et al., 2016; S\u00f8nderby et al., 2016), imposing structured sparsity on latent variables (Yeung et al.", "startOffset": 200, "endOffset": 244}, {"referenceID": 28, "context": "Various techniques to improve training of VAE models where the total cost represents a trade-off between the reconstruction cost and KL term have been used so far: KL-term annealing and input dropout (Bowman et al., 2016; S\u00f8nderby et al., 2016), imposing structured sparsity on latent variables (Yeung et al.", "startOffset": 200, "endOffset": 244}, {"referenceID": 32, "context": ", 2016), imposing structured sparsity on latent variables (Yeung et al., 2016).", "startOffset": 58, "endOffset": 78}, {"referenceID": 31, "context": "Zhang et al. (2016) and Serban et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "(2016) and Serban et al. (2016) apply VAE to sequence-to-sequence problems, improving over deterministic alternatives.", "startOffset": 11, "endOffset": 32}, {"referenceID": 15, "context": "framework of Kingma and Welling (2013), then describe our hybrid architecture where the feedforward part is composed of a fully convolutional encoder and a decoder that combines deconvolutional layers and a conventional RNN.", "startOffset": 13, "endOffset": 39}, {"referenceID": 15, "context": "The prior is typically chosen to be also a Gaussian with zero mean and unit variance, such that the KL term between posterior and prior can be computed in closed form (Kingma and Welling, 2013).", "startOffset": 167, "endOffset": 193}, {"referenceID": 23, "context": "The most straight-forward way to achieve a good reconstruction error in this case is to predict a very sharp probability distribution effectively corresponding to a single point in the latent space (Raiko et al., 2014).", "startOffset": 198, "endOffset": 218}, {"referenceID": 3, "context": "Bowman et al. (2016) demonstrate that this does not work in the fully deterministic Autoencoder framework .", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "A deconvolutional layer (also referred to as transposed convolutions (Gulrajani et al., 2016) and fractionally strided convolutions (Radford et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 22, "context": "2015)) performs spatial up-sampling of its inputs and is an integral part of latent variable generative models of images (Radford et al., 2015; Gulrajani et al., 2016) and semantic segmentation algorithms (Noh et al.", "startOffset": 121, "endOffset": 167}, {"referenceID": 8, "context": "2015)) performs spatial up-sampling of its inputs and is an integral part of latent variable generative models of images (Radford et al., 2015; Gulrajani et al., 2016) and semantic segmentation algorithms (Noh et al.", "startOffset": 121, "endOffset": 167}, {"referenceID": 20, "context": ", 2016) and semantic segmentation algorithms (Noh et al., 2015).", "startOffset": 45, "endOffset": 63}, {"referenceID": 22, "context": "A notable example of such a model is the Deep Network of (Radford et al., 2015) trained with adversarial objective.", "startOffset": 57, "endOffset": 79}, {"referenceID": 13, "context": "We consider two flavors of recurrent functions: a conventional LSTM network (Figure 2(b)) and a stack of masked convolutions also known as the ByteNet decoder from Kalchbrenner et al. (2016) (Figure 2(c)).", "startOffset": 164, "endOffset": 191}, {"referenceID": 4, "context": "Note that the feed-forward part of our model is different from the existing fully convolutional approaches of Dauphin et al. (2016) and Kalchbrenner et al.", "startOffset": 110, "endOffset": 132}, {"referenceID": 4, "context": "Note that the feed-forward part of our model is different from the existing fully convolutional approaches of Dauphin et al. (2016) and Kalchbrenner et al. (2016) in two respects: firstly, while being fully parallelizable during training, these models still require predictions from previous time steps during inference and thus behave as a variant of recurrent networks.", "startOffset": 110, "endOffset": 163}, {"referenceID": 3, "context": "The addition of the recurrent component results in optimization difficulties that are similar to those described by Bowman et al. (2016). In most cases the model converges to a solution with a vanishingly small KL term, thus effectively falling back", "startOffset": 116, "endOffset": 137}, {"referenceID": 3, "context": "Bowman et al. (2016) have proposed to use input dropout and KL term annealing to encourage their model to encode meaningful representations into the z vector.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "All models were trained with the Adam optimization algorithm (Kingma and Ba, 2014) with decaying learning rate.", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "training the LSTM VAE models from Bowman et al. (2016) and KL term annealing and regularized objective function from Eq (3) when training our models.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": "Normalization (Ba et al., 2016) in LSTM layers and Batch Normalization (Ioffe and Szegedy, 2015) in convolutional and deconvolutional layers.", "startOffset": 14, "endOffset": 31}, {"referenceID": 11, "context": ", 2016) in LSTM layers and Batch Normalization (Ioffe and Szegedy, 2015) in convolutional and deconvolutional layers.", "startOffset": 47, "endOffset": 72}, {"referenceID": 18, "context": "Our first task is character-level language generation performed on the standard Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 102, "endOffset": 123}, {"referenceID": 21, "context": "gradients (Pascanu et al., 2013), we have searched over learning rates, gradient clipping thresholds and sizes of LSTM layers but were only able to get results comparable to those shown in Figure 3.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "Note that LSTM networks make use of Layer Normalization (Ba et al., 2016) which has been shown to make training of such networks easier.", "startOffset": 56, "endOffset": 73}, {"referenceID": 3, "context": "A similar effect on model\u2019s bound was observed by Bowman et al. (2016): increased input dropout rates force their model to put more information into the z vector but at the cost of increased final loss values.", "startOffset": 50, "endOffset": 71}, {"referenceID": 13, "context": "periment with masked convolutions (Figure 2(c)), which is similar to the decoder in ByteNet model from Kalchbrenner et al. (2016). We fix the size of the convolutional kernels to 2 and do not use dilated convolutions and skip connections as in the original ByteNet.", "startOffset": 103, "endOffset": 130}], "year": 2017, "abstractText": "In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models", "creator": "LaTeX with hyperref package"}}}