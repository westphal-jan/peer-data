{"id": "1702.02463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Video Frame Synthesis using Deep Voxel Flow", "abstract": "where we address the problem of synthesizing new video frames in an existing video, either in - between existing frames ( interpolation ), or subsequent to them ( extrapolation ). this problem is challenging anymore because video object appearance and motion can be deemed highly complex. traditional optical - flow - based solutions often fail where flow estimation is challenging, essentially while newer neural - network - based methods that hallucinate pixel values directly often produce blurry results. we combine the advantages of these two methods utilized by training a deep filter network that actively learns to synthesize potential video frames simultaneously by flowing pixel values from affecting existing ones, which we call deep voxel flow. our method requires no human supervision, and any simulated video can be used as training data by dropping, and then learning prior to predict, existing frames. the technique, is efficient, and can uniquely be applied at any video resolution. we should demonstrate that our objective method produces results that to both quantitatively and qualitatively improve upon the state - of - one the - art.", "histories": [["v1", "Wed, 8 Feb 2017 15:20:14 GMT  (2024kb,D)", "http://arxiv.org/abs/1702.02463v1", "Project page:this https URL"], ["v2", "Sat, 5 Aug 2017 04:43:44 GMT  (2074kb,D)", "http://arxiv.org/abs/1702.02463v2", "To appear in ICCV 2017 as an oral paper. More details at the project page:this https URL"]], "COMMENTS": "Project page:this https URL", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["ziwei liu", "raymond a yeh", "xiaoou tang", "yiming liu", "aseem agarwala"], "accepted": false, "id": "1702.02463"}, "pdf": {"name": "1702.02463.pdf", "metadata": {"source": "CRF", "title": "Video Frame Synthesis using Deep Voxel Flow", "authors": ["Ziwei Liu", "Raymond Yeh", "Xiaoou Tang", "Yiming Liu", "Aseem Agarwala"], "emails": ["lz013@ie.cuhk.edu.hk", "xtang@ie.cuhk.edu.hk", "yeh17@illinois.edu", "yimingl@google.com", "aseemaa@google.com"], "sections": [{"heading": "1. Introduction", "text": "Videos of natural scenes observe a complicated set of phenomena; objects deform and move quickly, occlude and dis-occlude each other, scene lighting changes, and cameras move. Parametric models of video appearance are often too simple to accurately model, interpolate, or extrapolate video. None the less, video interpolation, i.e., synthesizing video frames between existing ones, is a common process in video and film production. The popular commercial plug-in Twixtor1 is used both to resample video into new framerates, and to produce a slow-motion effect from regularspeed video. A related problem is video extrapolation; predicting the future by synthesizing future video frames.\n1http://revisionfx.com/products/twixtor/\nThe traditional solution to these problems estimates optical flow between frames, and then interpolates or extrapolates along optical flow vectors. This approach is \u201coptical-flow-complete\u201d; it works well when optical flow is accurate, but generates significant artifacts when it is not. A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames. While these techniques are promising, directly synthesizing RGB values is not yet as successful as flow-based methods, and the results are often blurry.\nIn this paper we aim to combine the strengths of these two approaches. Most of the pixel patches in video are nearcopies of patches in nearby existing frames, and copying pixels is much easier than hallucinating them from scratch. On the other hand, an end-to-end trained deep network is an incredibly powerful tool. This is especially true for video interpolation and extrapolation, since training data is nearly infinite; any video can be used to train an unsupervised deep network.\nWe therefore use existing videos to train a CNN in an unsupervised fashion. We drop frames from the training videos, and employ a loss function that measures similarity between generated pixels and the ground-truth dropped frames. However, like optical-flow approaches our network generates pixels by interpolating pixel values from nearby frames. The network includes a voxel flow layer \u2014 a perpixel, 3D optical flow vector across space and time in the input video. The final pixel is generated by trilinear interpolation across the input video volume (which is typically just two frames). Thus, for video interpolation, the final output pixel can be a blend of pixels from the previous and next frames. This voxel flow layer is similar to an optical flow field. However, it is only an intermediate layer, and its correctness is never directly evaluated. Thus, our method requires no optical flow supervision, which is challenging to produce at scale.\nar X\niv :1\n70 2.\n02 46\n3v 1\n[ cs\n.C V\n] 8\nF eb\nWe train our method on the public UCF-101 dataset, but test it on a wide variety of videos. Our method can be applied at any resolution, since it is fully convolutional, and produces remarkably high-quality results which are significantly better than both optical flow and CNN-based methods. While our results are quantitatively better than existing methods, this improvement is especially noticeable qualitatively when viewing output videos, since existing quantitative measures are poor at measuring perceptual quality."}, {"heading": "2. Related Work", "text": "Video interpolation is commonly used for video retiming, novel-view rendering, and motion-based video compression [27]. Optical flow is the most common approach to video interpolation, and frame prediction is often used to evaluate optical flow accuracy [2]. As such, the quality of flow-based interpolation depends entirely on the accuracy of flow, which is often challenged by large and fast motions. Mahajan et al. [18] explore a variation on optical flow that computes paths in the source images and copies pixel gradients along them to the interpolated images, followed by a Poisson reconstruction. Meyer et al. [20] employ a Eulerian, phase-based approach to interpolation, but the method is limited to smaller motions.\nConvolutional neural networks have been used to make recent and dramatic improvements in image and video recognition [16]. They can also be used to predict optical flow [4], which suggests that CNNs can understand temporal motion. However, these techniques require supervision, i.e., optical flow ground-truth. A related unsupervised approach [17] uses a CNN to predict optical flow by synthesizing interpolated frames, and then inverting the CNN. However, they do not use an optical flow layer in the network, and their end-goal is to generate optical flow. They do not numerically evaluate the interpolated frames, themselves, and qualitatively the frames appear blurry.\nThere are a number of papers that use CNNs to directly generate images [10] and videos [29, 34]. Blur is often a problem for these generative techniques, since natural images follow a multimodal distribution, while the loss functions used often assume a Gaussian distribution. Our approach can avoid this blurring problem by copying coherent regions of pixels from existing frames. Generative CNNs can also be used to generate new views of a scene from existing photos taken at nearby viewpoints [7, 33]. These methods reconstruct images by separately computing depth and color layers at each hypothesized depth. This approach cannot account for scene motion, however.\nOur technical approach is inspired by recent techniques for including differentiable motion layers in CNNs [13]. Optical flow layers have also been used to render novel views of objects [36] and change eye gaze direction while\nvideoconferencing [8]. We apply this approach to video interpolation and extrapolation. LSTMs have been used to extrapolate video [26], but the results can be blurry. Mathieu et al. [19] reduce blurriness by using adversarial training [10] and unique loss functions, but the results still contain artifacts (we compare our results against this method). Finally, Finn et al. [6] use LSTMs and differentiable motion models to better sample the multimodal distribution of video future predictions. However, their results are still blurry, and are trained to videos in very constrained scenarios (e.g., a robot arm, or human motion within a room from a fixed camera). Our method is able to produce sharp results for widely diverse videos. Also, we do not pre-align our input videos; other video prediction papers either assume a fixed camera, or pre-align the input."}, {"heading": "3. Our Approach", "text": "We propose Deep Voxel Flow (DVF) \u2014 an end-toend fully differentiable network for video frame synthesis. The only training data we need are triplets of consecutive video frames. During the training process, two frames are provided as inputs and the remaining frame is used as a reconstruction target. Our approach is self-supervised and learns to reconstruct a frame by borrowing voxels from nearby frames, which leads to more realistic and sharper results (Fig. 4) than techniques that hallucinate pixels from scratch. Furthermore, due to the flexible motion modeling of our approach, no pre-processing (e.g., pre-alignment or lighting adjustment) is needed for the input videos, which is a necessary component for most existing systems [30, 34].\nFig. 1 illustrates the pipeline of DVF, where a convolutional encoder-decoder predicts the 3D voxel flow, and then a volume sampling layer synthesizes the desired frame, accordingly. DVF learns to synthesize target frame Y \u2208 RH\u00d7W from the input video X \u2208 RH\u00d7W\u00d7L, where H, W, L are the height, width and frame number of the input video. The target frame Y can be the in-between frame (interpolation) or the next frame (extrapolation) of the input video. For ease of exposition we focus here on interpolation between two frames, where L = 2. We denote the convolutional encoder-decoder as H(X; \u0398), where \u0398 are the network parameters. The output of H is a 3D voxel flow field F on a 2D grid of integer target pixel locations:\nF = (\u2206x,\u2206y,\u2206t) = H(X; \u0398) (1)\nThe spatial component of voxel flow F represents optical flow from the target frame to the next frame; the negative of this optical flow is used to identify the corresponding location in the previous frame. That is, we we assume optical flow is locally linear and temporally symmetric around the in-between frame. Specifically, we can define the absolute coordinates of the corresponding locations in\nthe earlier and later frames as L0 = (x \u2212 \u2206x, y \u2212 \u2206y) and L1 = (x + \u2206x, y + \u2206y), respectively. The temporal component of voxel flow F is a linear blend weight between the previous and next frames to form a color in the target frame. We use this voxel flow to sample the original input video X with a volume sampling function Tx,y,t to form the final synthesized frame Y\u0302:\nY\u0302 = Tx,y,t(X,F) = Tx,y,t(X,H(X; \u0398)) (2)\nThe volume sampling function samples colors by interpolating within an optical-flow-aligned video volume computed from X. Given the correspnding locations (L0,L1), we construct a virtual voxel of this volume and use trilinear interpolation from the colors at the voxel\u2019s corners to compute an output video color Y\u0302(x, y). We compute the integer locations of the eight vertices of the virtual voxel in the input video X as:\nV000 = (bL0xc, bL0yc, 0) V100 = (dL0xe, bL0yc, 0)\n...\nV011 = (bL1xc, dL1ye, 1) V111 = (dL1xe, dL1ye, 1).\n(3)\nwhere b\u00b7c is the floor function, and we define the temporal range for interpolation such that t = 0 for the first input frame and t = 1 for the second. Given this virtual voxel, the 3D voxel flow generates each target voxel Y\u0302(x, y) through trilinear interpolation:\nY\u0302(x, y) = Tx,y,t(X,F) = \u2211\ni,j,k\u2208[0,1]\nWijkX(Vijk) (4)\nW000 = (1\u2212 (L0x \u2212 bL0xc))(1\u2212 (L0y \u2212 bL0yc))(1\u2212\u2206t) W100 = (L1x \u2212 bL1xc)(1\u2212 (L0y \u2212 bL0yc))(1\u2212\u2206t)\n...\nW011 = (1\u2212 (L1x \u2212 bL1xc))(L1y \u2212 bL1yc)\u2206t W111 = (L1x \u2212 bL1xc)(L1y \u2212 bL1yc)\u2206t.\n(5)\nwhere Wijk is the trilinear resampling weight. This 3D voxel flow can be understood as the joint modeling of a 2D motion field and a mask selecting between the earlier and later frame. Specifically, we can separate F into Fmotion = (\u2206x,\u2206y) and Fmask = (\u2206t), as illustrated in Fig. 2 (e-f). (These definitions are later used in Eqn. 6 to allow different weights for spatial and temporal regularization.)\nNetwork Architecture. DVF adopts a fullyconvolutional encoder-decoder architecture, containing three convolution layers, three deconvolution layers and one bottleneck layer. Therefore, arbitrary-sized videos can be used as inputs for DVF. The network hyperparamters (e.g., the size of feature maps, the number of channels and activation functions) are specified in Fig. 1.\nFor the encoder section of the network, each processing unit contains both convolution and max-pooling. The convolution kernel sizes here are 5 \u00d7 5, 5 \u00d7 5 and 3 \u00d7 3, respectively. The bottleneck layer is also connected by convolution with kernel size 3 \u00d7 3. For the decoder section, each processing unit contains bilinear upsampling and convolution. The convolution kernel sizes here are 3\u00d73, 5\u00d75 and 5\u00d75, respectively. To better maintain spatial information we add skip connections between the corresponding convolution and deconvolution layers. Specifically, the corresponding deconvolution layers and convolution layers are concatenated together before being fed forward."}, {"heading": "3.1. Learning", "text": "For our DVF training, we exploit the l1 reconstruction loss with spatial and temporal coherence regularizations to reduce visual artifacts. Total variation (TV) regularizations are used here to enforce coherence. Since these regularizers are imposed on the output of the network it can be easily incorporated into the back-propagation scheme. Our overall objective function that we minimize is:\nL = 1 N \u2211 \u3008X,Y\u3009\u2208D ( \u2016Y \u2212 Tx,y,t(X,F)\u20161\n+ \u03bb1\u2016\u2207Fmotion\u20161 + \u03bb2\u2016\u2207Fmask\u20161 ) (6) where D is the training set of all frame triplets, N is its cardinality and Y is the target frame to be reconstructed. \u2016\u2207Fmotion\u20161 is the total variation term on the (x, y) components of voxel flow, and \u03bb1 is the corresponding regularization weight; similarly, \u2016\u2207Fmask\u20161 is the regularizer on the temporal component of voxel flow, and \u03bb2 its weight. (We experimentally found it useful to weight the coherence of the spatial component of the flow more strongly than the temporal selection.) To optimize the l1 norm, we use the Charbonnier penalty function \u03a6(x) = (x2 + 2)1/2 for approximation. Here we empirically set \u03bb1 = 0.01, \u03bb2 = 0.005 and = 0.001.\nWe initialize the weights in DVF using Gaussian distribution with standard deviation of 0.01. Learning the network is achieved via ADAM solver [15] with learning rate of 0.0001, \u03b21 = 0.9, \u03b22 = 0.999 and batch size of 32.\nBatch normalization [12] is adopted for faster convergence. Differentiable Volume Sampling. To make our DVF an end-to-end fully differentiable system, we define the gradients with respect to 3D voxel flow F = (\u2206x,\u2206y,\u2206t) so that the reconstruction error can be backpropagated through a volume sampling layer. Similar to [13], the partial derivative of the synthesized voxel color Y\u0302(x, y) w.r.t. \u2206x is\n\u2202Y\u0302(x, y)\n\u2202(\u2206x) = \u2211 i,j,k\u2208[0,1] EijkX(Vijk) (7)\nE000 = \u2212 (1\u2212 (L0y \u2212 bL0yc))(1\u2212\u2206t) E100 = (1\u2212 (L0y \u2212 bL0yc))(1\u2212\u2206t)\n...\nE011 = \u2212 (L1y \u2212 bL1yc)\u2206t E111 = (L1y \u2212 bL1yc)\u2206t.\n(8)\nwhere Eijk is the error reassignment weight w.r.t. \u2206x. Similarly, we can compute \u2202Y\u0302(x, y)/\u2202(\u2206y) and \u2202Y\u0302(x, y)/\u2202(\u2206t). This gives us a sub-differentiable sampling mechanism, allowing loss gradients to flow back to the 3D voxel flow F. This sampling mechanism can be implemented very efficiently by just looking at the kernel support region for each output voxel."}, {"heading": "3.2. Multi-scale Flow Fusion", "text": "As stated in Sec. 3.1, the gradients of reconstruction error are obtained by only looking at the kernel support region for each output voxel, which makes it hard to find large motions that fall outside the kernel. Therefore, we\npropose a multi-scale Deep Voxel Flow (multi-scale DVF) to better encode both large and small motions.\nSpecifically, we have a series of convolutional encoderdecoder HN ,HN\u22121, \u00b7 \u00b7 \u00b7 ,H0 working on video frames from coarse scale sN to fine scale s0, respectively. Typically, in our experiments, we set s2 = 64 \u00d7 64, s1 = 128 \u00d7 128 and s0 = 256 \u00d7 256. In each scale k, the subnetwork Hk predicts 3D voxel flow Fk at that resolution. Intuitively, large motions will have a relatively small offset vector Fk in coarse scale sN . Thus, the sub-networks HN , \u00b7 \u00b7 \u00b7 ,H1 in coarser scales sN , \u00b7 \u00b7 \u00b7 , s1 are capable of producing the correct multi-scale voxel flows FN , \u00b7 \u00b7 \u00b7 ,F1 even for large motions.\nWe fuse these multi-scale voxel flows to the finest network H0 to get our final result. The fusion is conducted by upsampling and concatenating the multi-scale voxel flow Fx,yk (only the spatial components (\u2206x,\u2206y) are retained) to the final decoder layer of H0, which has the desired spatial resolution s0. Then, the fine-scale voxel flow F0 is obtained by further convolution on the fused flow fields. The network architecture of multi-scale DVF is illustrated in Fig. 3. And it can be formulated as\nY\u03020 = T (X,F0) = T (X,H(X; \u0398,FN , \u00b7 \u00b7 \u00b7 ,F1)) (9)\nSince each sub-network Hk is fully differentiable, the multi-scale DVF can also be trained end-to-end with reconstruction loss \u2016Yk \u2212 T (Xk,Fk)\u20161 for each scale sk."}, {"heading": "3.3. Multi-step Prediction", "text": "Our framework can be naturally extended to multi-step prediction in either interpolation or extrapolation. For example, the goal is to predict the next D frames given the current L frames. In this case, the target Y becomes\na 3D volume (Y \u2208 RH\u00d7W\u00d7D) instead of a 2D frame (Y \u2208 RH\u00d7W ). Similar to Eqn. 4, each output voxel Y\u0302(x, y, t) can be obtained by performing trilinear interpolation on the input video X, according to its projected virtual voxel. We have observed that spatiotemporal coherence is preserved in our output volume because convolutions across the temporal layers allow local correlations to be maintained. Since multi-step prediction is more challenging, we reduce the learning rate to 0.00005 to maintain stability when training."}, {"heading": "4. Experiments", "text": "We trained Deep Voxel Flow (DVF) on videos from the UCF101 dataset [25]. We sampled frame triplets with obvious motion, creating a training set of approximately 240, 000 triplets. Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks. The pixel values are normalized into the range of [\u22121, 1]. We use both PSNR and SSIM [32] to evaluate the image quality of video frame synthesis; higher values of PSNR and SSIM indicate better results. However, quantitative evaluation of video interpolation often does not match perceptual evaluation, and temporal coherence can not be evaluated in images; we encourage examination of the video results in supplemental materials.\nCompeting Methods. We compare our approach against several methods, including the state-of-the-art optical flow technique EpicFlow [23]. To synthesize the in-between images given the computed flow fields we apply the interpolation algorithm used in the Middlebury interpolation benchmark [2]. For the CNN-based methods, we compare DVF to BeyondMSE [19], which achieved the bestperforming results in video prediction. However, their\nresults are trained using 4 and 8 input frames, whereas our method uses only 2, so we cannot compare using the already trained model released by the authors. None the less, the main contributions of BeyondMSE are novel loss functions. We therefore use their best-performing loss (ADV+GDL), and replace the backbone networks in BeyondMSE [19] with ours and train using the same data and protocol as in DVF. Furthermore, we note that our Figure 2 depicts the same video as Figure 6 in [19]; our result is much more realistic.\nResults. As shown in Table 1 (left), our method outperforms the baselines for video interpolation. BeyondMSE is a hallucination-based method and produces blurry predictions. EpicFlow outperforms BeyondMSE by 1.4dB because it copies pixel based on estimated flow fields. Our approach further improves the results by 0.7dB. Some qualitative comparisons are provided in Fig. 4 (a).\nVideo extrapolation results are shown in Table 1 (middle). The gap between BeyondMSE and EpicFlow shrinks to 0.9dB for video extrapolation since this task requires more semantic inference, which is a strength of deep models. Our approach combines the advantages of both, and achieves the best performance (29.6dB). Qualitative comparisons are provided in Fig. 4 (c).\nFinally, we explore the possibility of multi-step prediction, i.e., interpolate/extrapolate three frames (step = 1, 2, 3) at a time instead of one. From Fig. 5 (c-d), we can see that our approach consistently outperforms other alternatives along all time steps. The advantages become even larger when evaluating on long-range predictions (e.g., step = 3 in extrapolation). DVF is able to learn long-term temporal dependencies through large-scale unsupervised training. The qualitative illustrations are provided in Fig. 4 (b) and (d)."}, {"heading": "4.1. Effectiveness of Multi-scale Voxel Flow", "text": "In this section, we demonstrate the merits of Multiscale Voxel Flow (Multi-scale VF); we also examine results separately along two axes: appearance, and motion. For appearance modeling, we identify the texture regions by local edge magnitude. For motion modeling, we identify (large) motion regions according to the flow maps provided by [23]. Fig. 5 (a-b) compare the PSNR performance on UCF-101 test set without and with multi-scale voxel flow.\nThe multi-scale architecture further enables DVF to deal with large motions, as shown in Fig. 5 (b). Large motions become small after downsampling, and these motion estimates are mixed with higher-resolution estimates at the final layer of our network. The plots show that the multi-scale architecture add the most benefit in large-motion regions."}, {"heading": "4.2. Generalization to View Synthesis", "text": "In this section, we demonstrate that DVF can be readily generalized to view synthesis even without re-training. We directly apply the model trained on UCF-101 to the view synthesis task. The KITTI odometry dataset [9] is used here for evaluation, following [36].\nTable 1 (right) lists the performance comparisons of different methods. Surprisingly, without fine-tuning, our approach already outperforms [28] and [36] by 0.164 and 0.135 respectively. We find that fine-tuning on the KIITI training set could further reduce the reconstruction error. Note that KITTI dataset exhibits large camera motion, which is much different from our original training data. (UCF-101 mainly focuses on human actions.) This observation implies that voxel flow has good generalization ability\nand can be used as a universal frame/view synthesizer. The qualitative comparisons are provided in Fig. 6."}, {"heading": "4.3. Frame Synthesis as Self-Supervision", "text": "In addition to making progress on the quality of video interpolation/extrapolation, we demonstrate that video frame synthesis can serve as a self-supervision task for representation learning. Here, the internal representation learned by DVF is applied to unsupervised flow estimation and pretraining of action recognition.\nAs Unsupervised Flow Estimation. Recall that 3D voxel flow can be projected into a 2D motion field, which is illustrated in Fig. 2 (e). We quantitatively evaluate the flow\nestimation of DVF by comparing the projected 2D motion field to the ground truth optical flow field. The KITTI flow 2012 dataset [9] is used as a test set. Table 2 (left) reports the average endpoint error (EPE) over all the labeled pixels. After fine-tuning, the unsupervised flow generated by DVF surpasses traditional methods [3] and performs comparably to some of the supervised deep models [5]. Learning to synthesize frames on a large-scale video corpus can encode essential motion information into our model.\nAs Unsupervised Representation Learning. Here we replace the reconstruction layers in DVF with classification layers (i.e., fully-connected layer + softmax loss). The model is fine-tuned and tested with an action recognition loss on the UCF-101 dataset (split-1) [25]. This is equivalent to using frame synthesis by voxel flow as a pre-training task. As demonstrated in Table 2 (right), our approach outperforms random initialization by a large margin and also shows superior performance to other representation learning alternatives [31]. To synthesize frames using voxel flow, DVF has to encode both appearance and motion information, which implicitly mimics a two-stream CNN [24]."}, {"heading": "4.4. Applications", "text": "DVF can be used to produce slow-motion effects on high-definition (HD) videos. We collect HD videos (1080\u00d7 720, 30fps) from the web with various content and motion types as our real-world benchmark. We drop every other frame to act as ground truth. Note that the model used here is trained on the UCF-101 dataset without any further adaptation. Since the DVF is fully-convolutional, it can be applied to videos of an arbitrary size. More video quality comparisons are available on our project page2.\nVisual Comparisons. Existing video slo-mo software relies on explicit optical flow estimation to generate inbetween frames. Thus, we choose EpicFlow [23] to serve as a strong baseline. Fig. 7 illustrates slo-mo effects on the \u201cThrow\u201d and \u201cStreet\u201d sequences, respectively. Both techniques tend to produce spatially coherent results, though our method performs even better. For example, in the \u201cThrow\u201d sequence, DVF maintains the structure of the logo, while in the \u201cStreet\u201d sequence, DVF can better handle the occlusion between the pedestrian and the advertisement. However, the advantage is much more obvious when the temporal axis is examined. We show this advantage in static form by showing xt slices of the interpolated videos (Fig. 7 (c)); the EpicFlow results are much more jagged across time. Our observation is that EpicFlow often produces zerolength flow vectors for confusing motions, leading to spatial coherence but temporal discontinuities. Deep learning is, in general, able to produce more temporally smooth results than linearly scaling optical flow vectors.\n2Our project page: https://liuziwei7.github.io/ projects/VoxelFlow.html\nUser Study. We conducted a user study on the final slo-mo video sequences to objectively compare the quality of different methods. We compare DVF against both EpicFlow and ground truth. For side-by-side comparisons, synthesized videos of the two different methods are stitched together using a diagonal split, as illustrated in Fig. 8 (a). The left/right positions are randomly placed. Twenty subjects were enrolled in this user study; they had no previous experience with computer vision. We asked participants to select their preferences on 10 stitched video sequences, i.e., to determine whether the left-side or right-side videos were more visually pleasant. As Fig. 8 (b) shows, our approach is significantly preferred to EpicFlow among all testing sequences. For the null hypothesis: \u201cEpicFlow is better than our method\u201d, the p-value is p < 0.00001, and the hypothesis can be safely rejected. Moreover, for half of the sequences participants choose the result of our method roughly equally as often as the ground truth, which suggests that they are of equal visual quality. For the null hypothesis: \u201cour method is better than ground truth\u201d, the p-value is\n0.838193; statistical significance is not reached in this case. Overall, we conclude that DVF is capable of generating high-quality slo-mo effects across a wide range of videos.\nFailure Cases. The most typical failure mode of DVF is in scenes with repetitive patterns (e.g., the \u201cPark\u201d sequence). In these cases, It can be ambiguous to determine the true source voxel to copy by just referring to RGB differences. Stronger regularization terms could be added to address this problem.\nRunning Time. Our model is implemented using TensorFlow [1]. For a typical 1080\u00d7720 video sequence, DVF takes around 160ms to synthesize one in-between frame on a single GTX Titan X GPU. It can be further accelerated by first operating on the low-res images and then upsampling the estimated flow field."}, {"heading": "5. Discussion", "text": "In this paper, we propose an end-to-end deep network, Deep Voxel Flow (DVF), for video frame synthesis. Our method is able to copy pixels from existing video frames,\nrather than hallucinate them from scratch. On the other hand, our method can be trained in an unsupervised manner using any video. Our experiments show that this approach improves upon both optical flow and recent CNN techniques for interpolating and extrapolating video. In the future, it may useful to combine flow layers with pure synthesis layers to better predict pixels that cannot be copied from other video frames. Also, the way we extend our method to multi-frame prediction is fairly simple; there are a number of interesting alternatives, such as using the desired temporal step (e.g., t = .25 for the first out of three interpolated frames) as an input to the network. Compressing our network so that it may be run on a mobile device is also a direction we hope to explore."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "A database and evaluation methodology for optical flow", "author": ["S. Baker", "D. Scharstein", "J. Lewis", "S. Roth", "M.J. Black", "R. Szeliski"], "venue": "IJCV, 92(1):1\u201331", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "PAMI, 33(3):500\u2013513", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["A. Dosovitskiy", "P. Fischer", "E. Ilg", "P. H\u00e4usser", "C. Hazirbas", "V. Golkov"], "venue": "v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In ICCV", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["A. Dosovitskiy", "P. Fischery", "E. Ilg", "C. Hazirbas", "V. Golkov"], "venue": "van der Smagt, D. Cremers, T. Brox, et al. Flownet: Learning optical flow with convolutional networks. In ICCV, pages 2758\u20132766", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "arXiv preprint arXiv:1605.07157", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepstereo: Learning to predict new views from the world\u2019s imagery", "author": ["J. Flynn", "I. Neulander", "J. Philbin", "N. Snavely"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepwarp: Photorealistic image resynthesis for gaze manipulation", "author": ["Y. Ganin", "D. Kononenko", "D. Sungatullina", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1607.07215", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR, pages 3354\u20133361", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "THUMOS challenge: Action recognition with a large number of classes", "author": ["A. Gorban", "H. Idrees", "Y.-G. Jiang", "A. Roshan Zamir", "I. Laptev", "M. Shah", "R. Sukthankar"], "venue": "http: //www.thumos.info/", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "Spatial transformer networks. In NIPS, pages 2017\u20132025", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR, pages 1725\u20131732", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning image matching by simply watching video", "author": ["G. Long", "L. Kneip", "J.M. Alvarez", "H. Li", "X. Zhang", "Q. Yu"], "venue": "ECCV, pages 434\u2013450", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Moving gradients: A path-based method for plausible image interpolation", "author": ["D. Mahajan", "F.-C. Huang", "W. Matusik", "R. Ramamoorthi", "P. Belhumeur"], "venue": "TOG, 28(3):42:1\u201342:11", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "arXiv preprint arXiv:1511.05440", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Phase-based frame interpolation for video", "author": ["S. Meyer", "O. Wang", "H. Zimmer", "M. Grosse", "A. Sorkine- Hornung"], "venue": "CVPR, pages 1410\u20131418", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Shuffle and learn: unsupervised learning using temporal order verification", "author": ["I. Misra", "C.L. Zitnick", "M. Hebert"], "venue": "ECCV, pages 527\u2013544", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": "arXiv preprint arXiv:1412.6604", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Epicflow: Edge-preserving interpolation of correspondences for optical flow", "author": ["J. Revaud", "P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": "CVPR, pages 1164\u20131172", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, pages 568\u2013576", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A. Roshan Zamir", "M. Shah"], "venue": "CRCV-TR-12-01", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction error as a quality metric for motion and stereo", "author": ["R. Szeliski"], "venue": "ICCV", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Singleview to multi-view: Reconstructing unseen views with a convolutional network", "author": ["M. Tatarchenko", "A. Dosovitskiy", "T. Brox"], "venue": "arXiv preprint arXiv:1511.06702", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating Videos with Scene Dynamics", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "In NIPS. 2016", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "ECCV, pages 835\u2013851", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "ICCV, pages 2794\u20132802", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "TIP, 13(4):600\u2013612", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks", "author": ["J. Xie", "R.B. Girshick", "A. Farhadi"], "venue": "ECCV, pages 842\u2013857", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["T. Xue", "J. Wu", "K.L. Bouman", "W.T. Freeman"], "venue": "arXiv preprint arXiv:1607.02586", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness", "author": ["J.J. Yu", "A.W. Harley", "K.G. Derpanis"], "venue": "arXiv preprint arXiv:1608.05842", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "View synthesis by appearance flow", "author": ["T. Zhou", "S. Tulsiani", "W. Sun", "J. Malik", "A.A. Efros"], "venue": "arXiv preprint arXiv:1605.03557", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames.", "startOffset": 15, "endOffset": 27}, {"referenceID": 18, "context": "A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames.", "startOffset": 15, "endOffset": 27}, {"referenceID": 25, "context": "A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames.", "startOffset": 15, "endOffset": 27}, {"referenceID": 26, "context": "Video interpolation is commonly used for video retiming, novel-view rendering, and motion-based video compression [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "Optical flow is the most common approach to video interpolation, and frame prediction is often used to evaluate optical flow accuracy [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 17, "context": "[18] explore a variation on optical flow that computes paths in the source images and copies pixel gradients along them to the interpolated images, followed by a Poisson reconstruction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] employ a Eulerian, phase-based approach to interpolation, but the method is limited to smaller motions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Convolutional neural networks have been used to make recent and dramatic improvements in image and video recognition [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "They can also be used to predict optical flow [4], which suggests that CNNs can understand temporal motion.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "A related unsupervised approach [17] uses a CNN to predict optical flow by synthesizing interpolated frames, and then inverting the CNN.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "There are a number of papers that use CNNs to directly generate images [10] and videos [29, 34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 28, "context": "There are a number of papers that use CNNs to directly generate images [10] and videos [29, 34].", "startOffset": 87, "endOffset": 95}, {"referenceID": 33, "context": "There are a number of papers that use CNNs to directly generate images [10] and videos [29, 34].", "startOffset": 87, "endOffset": 95}, {"referenceID": 6, "context": "Generative CNNs can also be used to generate new views of a scene from existing photos taken at nearby viewpoints [7, 33].", "startOffset": 114, "endOffset": 121}, {"referenceID": 32, "context": "Generative CNNs can also be used to generate new views of a scene from existing photos taken at nearby viewpoints [7, 33].", "startOffset": 114, "endOffset": 121}, {"referenceID": 12, "context": "Our technical approach is inspired by recent techniques for including differentiable motion layers in CNNs [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 35, "context": "Optical flow layers have also been used to render novel views of objects [36] and change eye gaze direction while videoconferencing [8].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "Optical flow layers have also been used to render novel views of objects [36] and change eye gaze direction while videoconferencing [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 25, "context": "LSTMs have been used to extrapolate video [26], but the results can be blurry.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "[19] reduce blurriness by using adversarial training [10] and unique loss functions, but the results still contain artifacts (we compare our results against this method).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[19] reduce blurriness by using adversarial training [10] and unique loss functions, but the results still contain artifacts (we compare our results against this method).", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "[6] use LSTMs and differentiable motion models to better sample the multimodal distribution of video future predictions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": ", pre-alignment or lighting adjustment) is needed for the input videos, which is a necessary component for most existing systems [30, 34].", "startOffset": 129, "endOffset": 137}, {"referenceID": 33, "context": ", pre-alignment or lighting adjustment) is needed for the input videos, which is a necessary component for most existing systems [30, 34].", "startOffset": 129, "endOffset": 137}, {"referenceID": 0, "context": "i,j,k\u2208[0,1] WX(V) (4) W = (1\u2212 (Lx \u2212 bLxc))(1\u2212 (Ly \u2212 bLyc))(1\u2212\u2206t) W = (Lx \u2212 bLxc)(1\u2212 (Ly \u2212 bLyc))(1\u2212\u2206t) .", "startOffset": 6, "endOffset": 11}, {"referenceID": 14, "context": "Learning the network is achieved via ADAM solver [15] with learning rate of 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Batch normalization [12] is adopted for faster convergence.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Similar to [13], the partial derivative of the synthesized voxel color \u0176(x, y) w.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "i,j,k\u2208[0,1] EX(V) (7)", "startOffset": 6, "endOffset": 11}, {"referenceID": 24, "context": "We trained Deep Voxel Flow (DVF) on videos from the UCF101 dataset [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "We use both PSNR and SSIM [32] to evaluate the image quality of video frame synthesis; higher values of PSNR and SSIM indicate better results.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "We compare our approach against several methods, including the state-of-the-art optical flow technique EpicFlow [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "To synthesize the in-between images given the computed flow fields we apply the interpolation algorithm used in the Middlebury interpolation benchmark [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 18, "context": "For the CNN-based methods, we compare DVF to BeyondMSE [19], which achieved the bestperforming results in video prediction.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "BeyondMSE [19] 28.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "87 EpicFlow [23] 30.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "Views [28] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "Flow [36] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "We therefore use their best-performing loss (ADV+GDL), and replace the backbone networks in BeyondMSE [19] with ours and train using the same data and protocol as in DVF.", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "Furthermore, we note that our Figure 2 depicts the same video as Figure 6 in [19]; our result is much more realistic.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "For motion modeling, we identify (large) motion regions according to the flow maps provided by [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "The KITTI odometry dataset [9] is used here for evaluation, following [36].", "startOffset": 27, "endOffset": 30}, {"referenceID": 35, "context": "The KITTI odometry dataset [9] is used here for evaluation, following [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Surprisingly, without fine-tuning, our approach already outperforms [28] and [36] by 0.", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "Surprisingly, without fine-tuning, our approach already outperforms [28] and [36] by 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "LD Flow [3] 12.", "startOffset": 8, "endOffset": 11}, {"referenceID": 34, "context": "Basics [35] 9.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "9 FlowNet [5] 9.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "1 EpicFlow [23] 3.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "Video [31] 43.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "8 Shuffle&Learn [21] 50.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "2 ImageNet [14] 63.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "The KITTI flow 2012 dataset [9] is used as a test set.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "After fine-tuning, the unsupervised flow generated by DVF surpasses traditional methods [3] and performs comparably to some of the supervised deep models [5].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "After fine-tuning, the unsupervised flow generated by DVF surpasses traditional methods [3] and performs comparably to some of the supervised deep models [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 24, "context": "The model is fine-tuned and tested with an action recognition loss on the UCF-101 dataset (split-1) [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "As demonstrated in Table 2 (right), our approach outperforms random initialization by a large margin and also shows superior performance to other representation learning alternatives [31].", "startOffset": 183, "endOffset": 187}, {"referenceID": 23, "context": "To synthesize frames using voxel flow, DVF has to encode both appearance and motion information, which implicitly mimics a two-stream CNN [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 22, "context": "Thus, we choose EpicFlow [23] to serve as a strong baseline.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Our model is implemented using TensorFlow [1].", "startOffset": 42, "endOffset": 45}], "year": 2017, "abstractText": "We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}