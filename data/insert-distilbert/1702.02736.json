{"id": "1702.02736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Challenges in Providing Automatic Affective Feedback in Instant Messaging Applications", "abstract": "instant messaging is foremost one of the major channels of computer : mediated communication. however, humans are known to be very limited learners in understanding manipulating others'emotions via text - based communication. aiming on introducing emotion sensing technologies to instant messaging, we developed emotionpush, a system that automatically detects the emotions of the messages end - users received on facebook messenger and provides colored cues on their smartphones accordingly. we conducted a deployment study with 20 participants during a time \u2010 span of taking two frustrating weeks. in compiling this paper, we revealed five challenges, along with examples, that we observed in our previous study based on demonstrating both user's updated feedback and updated chat logs, including ( i ) the continuum of emotions, ( brother ii ) random multi - user conversations, ( iii ) different dynamics between different users, ( iv ) misclassification of desired emotions and ( v ) unconventional content. we believe this discussion arc will benefit the future exploration of affective computing for internet instant messaging, and also contribute shed light on research areas of conversational emotion sensing.", "histories": [["v1", "Thu, 9 Feb 2017 08:00:09 GMT  (8050kb,D)", "http://arxiv.org/abs/1702.02736v1", "7 pages, 2017 AAAI Spring Symposia"]], "COMMENTS": "7 pages, 2017 AAAI Spring Symposia", "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["chieh-yang huang", "ting-hao huang", "lun-wei ku"], "accepted": false, "id": "1702.02736"}, "pdf": {"name": "1702.02736.pdf", "metadata": {"source": "CRF", "title": "Challenges in Providing Automatic Affective Feedback in Instant Messaging Applications", "authors": ["Chieh-Yang Huang", "Ting-Hao (Kenneth) Huang", "Lun-Wei Ku"], "emails": ["appleternity@iis.sinica.edu.tw,", "lwku@iis.sinica.edu.tw.", "tinghaoh@cs.cmu.edu."], "sections": [{"heading": "Introduction", "text": "Text-based emotion detection and classification has a longlasting history of research (Alm, Roth, and Sproat 2005). It is to become increasingly important in the area of machine learning with the increasing emphasis on assistants as frontline interactions for service design. Such assistantship is becoming more manifest in the form of \u201cchatbots\u201d (Huang et al. 2016), suggesting the research in our work is getting relevant. However, compared to content recommendation (Bohus et al. 2007; Zhao, Lee, and Eskenazi 2016) or behavioral modeling (Levin, Pieraccini, and Eckert 2000), it is still under discussed. Still less, it has rarely been used in applications for individual users such as instant messengers. To understand the feasibility of text-based affective computing in the era of mobile devices, we introduced EmotionPush1, a mobile application that automatically detects the emotion of the text message that user received via Facebook Messenger, and provides emotion cues by colors in real-time (Wang et al. 2016). EmotionPush uses 7 colors to represent 7 emo-\nCopyright \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1EmotionPush is available at Google Play: https: //play.google.com/store/apps/details?id= tw.edu.sinica.iis.emotionpush\ntions, which is based on Plutchik\u2019s Emotion Wheel color theme (Figure 1.)\nFor instance, when the user receives a message saying \u201cHi, How are you?\u201d, EmotionPush first classifies this message\u2019s emotion as Joy, and then pushes a notification on the user\u2019s smartphone with a yellow icon (Figure 2b), which is the corresponding color of Joy. Later when the user clicks the notification to open Messenger to start the conversation, EmotionPush keeps track on each message that the user receives and uses a color bubble on the top of the screen to continually provide emotion cues (Figure 2c). In Figure 2d, the other party suggests a lunch meeting, which keeps the emotion cue as Joy; then the next message about feeling tired changes the emotion cue from Joy to Tired as in Figure 2e. After giving the last reply which ends this chat session (Figure 2f), users can go back to the desktop but still see the emotion cue of the last message as shown in Figure 2g. Later users can start over and check the notifications from EmotionPush again by pulling down the notification bar (Figure 2h).\nWe conducted a deployment study of EmotionPush with 20 participants during a time span of two weeks. 20 English native speakers who identified themselves as Messenger frequent users (ages ranged from 18 to 31 years) were recruited. Participants were asked to install EmotionPush on their personal smartphones, keep it open, and actively use it during the whole study. Each participant was compensated with $20 US dollars. In our study, totally 62,378 messages were recorded during 10,623 conversational sessions, which were automatically segmented with 5-minute user timeouts.\nar X\niv :1\n70 2.\n02 73\n6v 1\n[ cs\n.C L\n] 9\nF eb\n2 01\n7\nIn this paper, we first list the potential use cases of EmotionPush. Then we describe the challenges we identified during the deployment study of EmotionPush. While prior work has shown that identifying emotions based on text is possible, we detail the challenges emerged from the deployment of such a system to the real world. Challenges that we identified included modeling the continuum of emotional expressions; referring the detected emotion to the right speaker in a multi-user conversation; considering various expression levels per familiarity between users; handling the classifier errors; and problems derived from nonconventional contents such as long messages, code switching, and emojis."}, {"heading": "EmotionPush System", "text": "The task to predict the emotion of a given text can be regarded as a document (sentence) classification problem if the emotion categories are given. To make sure the devel-\noped system achieves good performance, we adopted the powerful classification module LibSVM (Fan et al. 2008), a supervised-learning tool for Support Vector Machines, and the widely-recognized well-performed feature, Word Embedding, to train the classifier.\nWord Embedding, generated from the neural language models, has been shown to be a successful feature for classification problems (Bengio et al. 2006; Huang et al. 2012; Mikolov et al. 2013a; Mikolov et al. 2013b). We adopted pre-trained 300 dimension word vectors trained on part of Google News2 and represented each sentence or document as a vector by averaging all the word vectors of the words in the sentence or the document.\nThe emotion classifiers of EmotionPush were trained on\n2https://code.google.com/archive/p/ word2vec/\nLJ40k (Leshed and Kaye 2006), a dataset contains 1,000 blog posts for each of 40 common emotions. These classifiers were shown to be comparable to the state-of-theart methods in terms of performance (Wang et al. 2016). Then EmotionPush adopted the categorical representation with the designed color scheme to visualize detected 7 emotions, simplifying the connection between emotions and texts, such that users can easily interact instantly by their instinct."}, {"heading": "Use Case of EmotionPush", "text": "The goal of EmotionPush is to enable end-users to understand better of emotions of their conversational partners. Hence, the most apparent use case we can think of is to prioritize the messages according to the emotions they convey. Participants confirmed this use case by indicating that EmotionPush can help them organize their messages, read messages quickly, and managing multiple messages at once. However, other use cases are not clear before the deployment. In the use study, participants did name some interesting use cases. We list these use cases as follows, along with quotes of participants.\n\u2022 Emotion Management Participants will be able to decide whether they want to receive some information in order to keep their emotion stable. They can either choose to reach the people needed to be taken care, or not to read any message from them to keep themselves neutral: \u201cOne of the major advantages was to identify who was angry and needed to talk immediately. That helped me in my interactions a lot.\u201d \u201cIf there is a red color chat, I wouldn\u2019t read it as it might ruin my mood.\u201d\n\u2022 Interacting with People of Little Acquaintance Participants mentioned that EmotionPush helps them when talking to strangers or new friends. This makes perfect sense as EmotionPush learns from large datasets and should interpret emotions of messages in a general way. Hence, emotion cues from EmotionPush are of valuable reference when we don\u2019t know much about the other party.\n\u2022 Fun Topics to Have When participants see some suggested emotions which are different from they expect or interpret, they will confirm with the other party and hence have more topics to create an interesting conversation: \u201cIt\u2019s a funny topic of conversation when the app predicts the emotion interestingly.\u201d\nAnother use case, which is more implicit but draws our attention, is that users may rely on the prompted emotions instead of interpreting received messages by themselves. It is raised by one participant by saying\n\u201c... has the team thought about the social impact this kind of app would have if many people used it? ..., but if everyone were to use an app like this, I feel like people would start to\nrely on the app too much to determine how they should respond as opposed to figuring out the other person\u2019s emotions on their own.\u201d\nHowever, even with this hidden social concern which should be investigate further, from the result of the user study we can still expect that the system could help people in their interactions from many aspects. The quantitative summary of the user study is reported in Appendix for reference, while the expressed opinions are discussed in this paper. In the following sections, we further detail 5 mentioned challenges emerged from our experiments.\nChallenge 1: The Continuum of Emotion EmotionPush uses a categorical representation (e.g. Anger, Joy, etc.) (Klein, Moon, and Picard 2002) of emotions instead of a dimensional representation (valence, arousal) (Sa\u0301nchez et al. 2006) to reduce users\u2019 cognitive load. One natural limitation of applying a categorical representation is the lack of capability of expressing continuum of emotion. For instance, in the following conversation, the user B sent five consecutive messages, which is less likely to express four different emotions, as predicted3:\n\u25cf A: Aww thanks!! \u25cf A: How\u2019s being home? \u25cf B: Studying, haha \u25cf B: But it doesn\u2019t feel like I have been away for one year \u25cf B: Nothing has changed here \u25cf B: Time is running so slow now \u25cf B: And I\u2019m still jetlagged, haha\nWhile prior work explored modeling continuum of information in text, speech (Yeh et al. 2011) and video streaming (Gunes and Schuller 2013), literature had little to say about modeling continuous emotions in a text-based conversation. To the best of our knowledge, none of the existent conversational datasets contain emotion labels, and the continuum property has not been considered in modern emotion labeling systems for conversations. We believe that considering the hidden emotion states to develop the computational models of humans consecutive dynamics of emotion is a promising direction, where a middle-layered computation which captures the nature flow of emotions is necessary.\nChallenge 2: Multi-User Conversations Unexpected challenges were raised by multi-party chatting, which is also known as Group or Channel in modern messengers. In our study, in which 22.46% of messages were recorded in multi-user chatting groups, we found that providing emotion cues on top of a multi-user conversation would make it difficult for users to concentrate on the running dialog. For instance, in the following conversation between four different users, it is hard to keep track of both the dialog and the emotion of each user at the same time.\n\u25cf A: Oh I\u2019ll have it tonight, just can\u2019t rsvp on mobile arm 3 EmotionPush users do not receive any affective feedback for\nthe messages sent by themselves. In this paper we show colored emotion cues for all messages only for readers\u2019 reference. The example conversations will be lightly disguised based on the techniques suggested by Bruckman (Bruckman 2006) on publication.\n\u25cb A: *atm \u25cb B: ACK \u25cf B: I\u2019ll mark you down \u25cf B: yup, it\u2019s tonight :) \u25cf C: holy shit this sounds awesome! \u25cf B: John is super nerdy rad \u25cf D: I want in on this. I\u2019ll see if I can make it work with tech \u25cf D: I can make it \u25cf D: unfortunately my grandparents are coming in tonight so I\ndon\u2019t think I\u2019ll be able to join :( ha\nFurthermore, multi-party conversations also raised challenges in designing user experience . As shown in the Introduction section, EmotionPush uses two ways to provide emotion cues: 1) a colored push notification, and 2) a colored bubble that floats on the top layer of the screen. However, both methods were not capable to efficiently convey emotions in multiple-user conversations. While a notification can show the message and its emotion cue simultaneously, it only displays the name of the chat group instead of the name of message sender; users would also find it difficult to identify the corresponding speaker based on bubble\u2019s color changes when multiple users are talking. These design challenges of providing affective feedback that considers emotions, texts and users are beyond prior research of online chatting interfaces (Vronay, Smith, and Drucker 1999; Roddy and Epelman-Wang 1998).\nChallenge 3: Different Dynamics Between"}, {"heading": "Different Users", "text": "Different interaction dynamics occur between people in different context and relationships. One risk of classifying emotions solely based on texts is the neglect of user context, which is known to have strong correlations with user behavior (Baym et al. 2007; Gilbert and Karahalios 2009). Prior work has also shown that language comprehension is more than a process of decoding the literal meaning of a speaker\u2019s utterance but making pragmatic inferences that go beyond the linguistic data (Frank and Goodman 2014).\nFor instance, in our study, we observed that emotion classification worked better on conversations between users who rarely interacted with each other, in which the languages were more formal. The following is an example.\n\u25cb A: Hey man.. hows it going! \u25cf B: Hey! It\u2019s going well :-) \u25cb B: THings are pretty hectic, but I\u2019m trying to get used to the\nassignments and whatnotn \u25cb A: haha sounds like grad school \u25cf B: Yup! Haha \u25cf B: Weren\u2019t you planning a trip to Eastern United States ? \u25cf A: I was! But I never ended up coming.. I would still like to but\nmy best bet was recruiting and I asked not to go as there was soem work that came up\nOn the other hand, the conversations between users who frequently talked with each other often contain informal expressions. The following is an example.\n\u25cb A: Okay I was thinking of getting pierced tomorrow after 6:30? I could theoretically do today between like 4:30-6 but I worry about cutting it too close? \u25cf B: I\u2019M DOWN \u25cf A: what time would work best 4 u?\n\u25cf B: a little after 6:30 might work better bc of activity fair? \u25cf A: Yeah that makes sense! \u25cf [Discussing about inviting other friends] \u25cb B: cooooooooooool \u25cb B: i can prob get out of helping with teardown haha \u25cf A: Its no big if u cant, its open until 9 \u25cb B: yeeeee\nOur observation suggested that EmotionPush could be more helpful for some conversations, in this case, the conversations between people who talked less frequently with each other, than for others. User context could be helpful to both directly improve emotion classification or identify which conversations EmotionPush can assist better.\nChallenge 4: Misclassification of Emotions Emotion classification is not perfect. It is inevitable that some emotion cues that EmotionPush send are incorrect. For instance, the message of user B in the following conversation should be of Anticipation (orange) instead of Fear (green). \u25cf A: Will it be factory reset, does it have Microsoft office preset \u25cf B: Yes, I will factory reset it tonight and if you want, you can\nhave a look at it :)\nIn the following example, the message of user A should be of Anticipation (orange) instead of Fear (green), and the message of user B was apparently not of Sadness (blue). \u25cf A: Hey guys so does 2:30 sound good for volunteering tomor-\nrow? We\u2019ll take next week off because of fall break \u25cf B: We can leave at 230\nMisclassified cases raised the questions that what level of performance is good enough for an realistic application. EmotionPush\u2019s classifier achieved an average AUC (the area under the receiver operating characteristic curve) of 0.68, which is comparable to the state-of-the-art performance (Wang et al. 2016). It is noteworthy that humans are not good at identify emotions in text. Prior work showed that humans on average can only correctly predict 63% of emotion labels of articles in LiveJournal (Mishne and others 2005). Our post-study survey also showed that participants did not think the wrongly-predicted emotion colors are harmful to their chatting experiences (average rating = 0.85, ranges from 0 to 4), while they felt the correctly-predicted emotion colors are helpful (average rating = 2.5). Given all these factors, we believe that our emotion classifiers\u2019 performances are practical for real-world applications.\nIn addition to improving emotion classification, challenges also come from designing good user experience around error cases. EmotionPush is good at identifying Joy, Anger, and Sadness (Wang et al. 2016). One potential direction is to use different feedback types (e.g., vibration) to distinguish reliable predictions from uncertain ones.\nChallenge 5: Unconventional Content Similar to most text-processing systems deployed to the real world, EmotionPush faced challenges in handling unconventional content in instant messages. In this section we describe three types of unconventional content we observed in our study: multiple languages, graphic symbols such as emojis, and long messages.\nMultiple Languages & Code Switching Real-world users speak various languages. Even though we recruited English native speakers in our study, participants occurred to speak in, or switch to, various languages when talking with friends. For example, user A switched between English and Chinese in the following conversation.\n\u25cf A: How\u2019s ur weekend \u25cf A: Sorry last night I didn\u2019t sleep well and needed to work ..Feel\nlike I\u2019m a zombie today haha \u25cb A:\u6574\u5929\u8166\u888b\u7a7a\u7a7a\u7684 \u25cb A:\u4f60\u5011\u90fd\u642c\u5230\u5317\u53f0\u7063\uff1f \u25cb B:\u54c8\u54c8\u52a0\u6cb9\u5594\u5594\u5594 \u25cb B:\u5c0d\u5440! \u25cb B:\u5317\u6d77\u5cb8\u9644\u8fd1 \u25cf A: How r u\nNot only text-based emotion classification require sufficient labeled data for training, but also code-switching processing techniques relies heavily on training data (Brooke, Tofiloski, and Taboada 2009; Vilares, Alonso, and Go\u0301mezRodr\u0131guez 2015). All of these technologies are not capable of processing unseen languages. While prior work explored cross-language acoustic features for emotion recognition in speech (Pell et al. 2009), detecting emotions in arbitrary languages\u2019 texts is still infeasible. For deployed systems such as EmotionPush, making design decisions around languages it can not understand is inevitable. Currently EmotionPush supports two languages, English and Chinese, but these two modules were developed separately and still can not handle code-switching case such as the example above. In the future, we are looking forward to incorporating a language identifier to provide more concrete feedback (e.g., \u201cSorry I do not understand French.\u201d) to users.\nEmoji, Emoticons, and Stickers Graphic symbols such as emojis, emoticons and stickers are widely used in instant messages, often for expressing emotions. For example, the emoticon \u201c\u00af\\_(\u30c4)_/\u00af\u201d (also known as \u201csmugshrug\u201d), which represents the face and arms of a smiling person with hands raised in a shrugging gesture, was used in the following conversation.\n\u25cf A: when can i come and pick up my Jam and also Goat \u25cf B: whenever you want tbh? \u25cf B: we\u2019re home rn if ur down \u25cf B: or tomorrow sometime \u25cf B: \u00af\\_(\u30c4)_/\u00af\nThe usages and effects of graphic symbols in on-line chatting have been thoroughly studied (Jibril and Abdullah 2013; Walther and D\u2019Addario 2001; Wang 2015), and techniques of handling emojis in text processing has also been developed (Barbieri, Ronzano, and Saggion 2016; Eisner et al. 2016). However, the current technologies are still not capable to identify emotions from any arbitrary emojis and emoticons, not to mention new graphic symbols are created everyday (e.g., \u201csmugshrug\u201d was just approved as part of Unicode 9.0 in 2016) and stickers are not even text.\nParagraph-like Long Messages Often instant messaging users chunk a long message into smaller pieces and send them consecutively. However, we observed that in our study occasionally users send exceptionally long messages. For instance, a user sent one message that contains 10 sentences (134 words) to warn the former owner of his/her house to clean up as soon as possible, a user sent a 10-sentence message (201 words) to advertise his/her incoming stand-up comedy performance, and a user sent a 9-sentence message (152 words) to discuss an reunion event. In each of these long messages, the user used multiple sentences to express complex issues or emotions, which made it difficult to conclude the message with one single emotion. While literature showed that emotion classification yielded a better performance on long sentences that contain more words because they bear more information (Calvo and Mac Kim 2013), our observation suggested that long messages that contain many sentences often result in a less-confident or incorrect emotion classification as a whole."}, {"heading": "Conclusion & Future Work", "text": "In this paper, we describe challenges in deploying an emotion detection system, EmotionPush, for instant messaging applications. These challenges included the continuum of emotions, multi-user conversations, different dynamics between different users, misclassification, and unconventional content. These challenges are not only about providing automatic affective feedback by using text-processing technologies, but also about designing an user experience given the interrelated factors including humanity and languages. Through these discussions, we expect to gain insight into the deployment of applications of affective computing and motivate researchers to elaborate the solutions of tomorrow.\nIn the future, with the advantage of the developed EmotionPush, we plan to design a mechanism which encourages users to contribute their contents and feedback their emotions to advance this technology where it is most needed."}, {"heading": "Acknowledgements", "text": "Research of this paper was partially supported by Ministry of Science and Technology, Taiwan, under the contract MOST 104-2221-E-001-024-MY2."}, {"heading": "10% 10% 20% 50% 10%", "text": ""}, {"heading": "10% 10% 35% 30% 15%", "text": ""}, {"heading": "20% 25% 20% 30% 5%", "text": ""}, {"heading": "10% 10% 30% 35% 15%", "text": ""}, {"heading": "50% 20% 25% 5% 0%", "text": ""}, {"heading": "5% 5% 50% 35% 5%", "text": ""}], "references": [{"title": "C", "author": ["Alm"], "venue": "O.; Roth, D.; and Sproat, R.", "citeRegEx": "Alm. Roth. and Sproat 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "What does this emoji mean? a vector space skip-gram model for twitter emojis", "author": ["Ronzano Barbieri", "F. Saggion 2016] Barbieri", "F. Ronzano", "H. Saggion"], "venue": "In Language Resources and Evaluation conference,", "citeRegEx": "Barbieri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2016}, {"title": "Y", "author": ["Baym, N.K.", "Zhang"], "venue": "B.; Kunkel, A.; Ledbetter, A.; and Lin, M.-C.", "citeRegEx": "Baym et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural probabilistic language models", "author": ["Bengio"], "venue": "In Innovations in Machine Learning", "citeRegEx": "Bengio,? \\Q2006\\E", "shortCiteRegEx": "Bengio", "year": 2006}, {"title": "A", "author": ["D. Bohus", "A. Raux", "T.K. Harris", "M. Eskenazi", "Rudnicky"], "venue": "I.", "citeRegEx": "Bohus et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Cross-linguistic sentiment analysis: From english to spanish", "author": ["Tofiloski Brooke", "J. Taboada 2009] Brooke", "M. Tofiloski", "M. Taboada"], "venue": "RANLP,", "citeRegEx": "Brooke et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brooke et al\\.", "year": 2009}, {"title": "and Mac Kim", "author": ["R.A. Calvo"], "venue": "S.", "citeRegEx": "Calvo and Mac Kim 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "emoji2vec: Learning emoji representations from their description", "author": ["Eisner"], "venue": "arXiv preprint arXiv:1609.08359", "citeRegEx": "Eisner,? \\Q2016\\E", "shortCiteRegEx": "Eisner", "year": 2016}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Fan,? \\Q2008\\E", "shortCiteRegEx": "Fan", "year": 2008}, {"title": "N", "author": ["M.C. Frank", "Goodman"], "venue": "D.", "citeRegEx": "Frank and Goodman 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Karahalios", "author": ["E. Gilbert"], "venue": "K.", "citeRegEx": "Gilbert and Karahalios 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Schuller", "author": ["H. Gunes"], "venue": "B.", "citeRegEx": "Gunes and Schuller 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "Ng"], "venue": "Y.", "citeRegEx": "Huang et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["T.-H.K. Huang", "W.S. Lasecki", "A. Azaria", "Bigham"], "venue": "P.", "citeRegEx": "Huang et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["T.A. Jibril", "Abdullah"], "venue": "H.", "citeRegEx": "Jibril and Abdullah 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "R", "author": ["J. Klein", "Y. Moon", "Picard"], "venue": "W.", "citeRegEx": "Klein. Moon. and Picard 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Kaye", "author": ["G. Leshed"], "venue": "J.", "citeRegEx": "Leshed and Kaye 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A stochastic model of humanmachine interaction for learning dialog strategies", "author": ["Pieraccini Levin", "E. Eckert 2000] Levin", "R. Pieraccini", "W. Eckert"], "venue": "IEEE Transactions on speech and audio processing", "citeRegEx": "Levin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Mishne and others", "author": ["G Mishne"], "venue": "In Proceedings of ACM SIGIR 2005 workshop on stylistic analysis of text for information access,", "citeRegEx": "Mishne,? \\Q2005\\E", "shortCiteRegEx": "Mishne", "year": 2005}, {"title": "S", "author": ["M.D. Pell", "L. Monetta", "S. Paulmann", "Kotz"], "venue": "A.", "citeRegEx": "Pell et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Epelman-Wang", "author": ["B.J. Roddy"], "venue": "H.", "citeRegEx": "Roddy and Epelman.Wang 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "J", "author": ["J.A. S\u00e1nchez", "N.P. Hern\u00e1ndez", "Penagos"], "venue": "C.; and Ostr\u00f3vskaya, Y.", "citeRegEx": "S\u00e1nchez et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "M", "author": ["Vilares, D.", "Alonso"], "venue": "A.; and G\u00f3mez-Rodr\u0131guez, C.", "citeRegEx": "Vilares. Alonso. and G\u00f3mez.Rodr\u0131guez 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alternative interfaces for chat", "author": ["Smith Vronay", "D. Drucker 1999] Vronay", "M. Smith", "S. Drucker"], "venue": "In Proceedings of the 12th annual ACM symposium on User interface software and technology,", "citeRegEx": "Vronay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Vronay et al\\.", "year": 1999}, {"title": "and D\u2019Addario", "author": ["J.B. Walther"], "venue": "K. P.", "citeRegEx": "Walther and D.Addario 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Sensing emotions in text messages: An application and deployment study of emotionpush", "author": ["Wang"], "venue": "arXiv preprint arXiv:1610.04758", "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "S", "author": ["Wang"], "venue": "S.", "citeRegEx": "Wang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Segment-based emotion recognition from continuous mandarin chinese speech. Computers in Human Behavior 27(5):1545\u20131552", "author": ["Yeh"], "venue": null, "citeRegEx": "Yeh,? \\Q2011\\E", "shortCiteRegEx": "Yeh", "year": 2011}, {"title": "Dialport: Connecting the spoken dialog research community to real user data", "author": ["Lee Zhao", "T. Eskenazi 2016] Zhao", "K. Lee", "M. Eskenazi"], "venue": "arXiv preprint arXiv:1606.02562", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Instant messaging is one of the major channels of computer mediated communication. However, humans are known to be very limited in understanding others\u2019 emotions via textbased communication. Aiming on introducing emotion sensing technologies to instant messaging, we developed EmotionPush, a system that automatically detects the emotions of the messages end-users received on Facebook Messenger and provides colored cues on their smartphones accordingly. We conducted a deployment study with 20 participants during a time span of two weeks. In this paper, we revealed five challenges, along with examples, that we observed in our study based on both user\u2019s feedback and chat logs, including (i) the continuum of emotions, (ii) multi-user conversations, (iii) different dynamics between different users, (iv) misclassification of emotions, and (v) unconventional content. We believe this discussion will benefit the future exploration of affective computing for instant messaging, and also shed light on research of conversational emotion sensing.", "creator": "LaTeX with hyperref package"}}}