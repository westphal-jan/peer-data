{"id": "1702.03342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "abstract": "explicit concept space models typically have proven valuable efficacy for text representation in shaping many natural language and text mining applications. the idea is to embed other textual structures into a semantic space of concepts which captures the relevant main topics of these three structures. that so called bag - of - concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. in this paper we propose two neural embedding semantic models in advance order to learn continuous concept abstract vectors. once learned, furthermore we already propose an efficient vector aggregation method to generate fully dense bag - of - parts concepts representations. empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other textual concept embedding simpler models. in addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness value of the simpler densified vector representation over the typical sparse representations for dataless classification where successively we can achieve at least same or better accuracy with much rather less dimensions.", "histories": [["v1", "Fri, 10 Feb 2017 22:44:59 GMT  (344kb)", "http://arxiv.org/abs/1702.03342v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["walid shalaby", "wlodek zadrozny"], "accepted": false, "id": "1702.03342"}, "pdf": {"name": "1702.03342.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wshalaby@uncc.edu", "wzadrozn@uncc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n03 34\n2v 1\n[ cs\n.C L\n] 1\n0 Fe\nb 20\n17\nproven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions."}, {"heading": "1 Introduction", "text": "Vector-based semantic mapping models are used to represent textual structures (words, phrases, and documents) as high-dimensional meaning vectors. Typically, these models utilize textual corpora and/or Knowledge Bases (KBs) to acquire world knowledge, which is then used to generate a vector representation for the given text in the semantic space. The goal is thus to accurately place semantically similar structures close to each other in that semantic space. On the other hand, dissimilar structures should be far apart.\nExplicit concept space models are motivated by the idea that high level cognitive tasks such learning and reasoning are supported by the knowledge we acquire from concepts (Song et al., 2015). Therefore, such models utilize concept vectors (a.k.a bag-of-concepts (BOC)) as the underlying semantic representation of a given text through a process called conceptualization, which is mapping the text into relevant concepts capturing its main topics. The concept space typically include concepts obtained from KBs such as Wikipedia, Probase (Wu et al., 2012), and others. Once the concept vectors are generated, similarity between two concept vectors can be computed using a suitable similarity/distance measure such as cosine.\nThe BOC representation has proven efficacy for semantic analysis of textual data especially short texts where contextual information is missing or insufficient. For example, measuring semantic similarity/relatedness (Gabrilovich and Markovitch, 2007; Kim et al., 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al., 2008; Song and Roth, 2014, 2015; Li et al., 2016), short text clustering (Song et al., 2015), search and relevancy ranking (Egozi et al., 2011), event detection and coreference resolution (Peng et al., 2016).\nSimilar to the traditional bag-of-words representation, the BOC vector is a high dimensional sparse vector whose dimensionality is the same as the number of concepts in the employed KB (typically millions). Consequently, it suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. Formally, given a text snippet T = {t1, t2, ..., tn} of n terms where n \u2265 1, and a concept space C = {c1, c2, ..., cN} of size N . The BOC vector v = {w1, w2, ..., ws} \u2208 R N of T is a vector of weights of each concept where each wi of concept\nci is calculated as in equation 1:\nwi =\nn \u2211\nj=1\nf(ci, tj), 1 \u2264 i \u2264 N (1)\nHere f(c, t) is a scoring function which indicates the degree of association between term t and concept c. For example, Gabrilovich and Markovitch (2007) proposed Explicit Semantic Analysis (ESA) which uses Wikipedia articles as concepts and the TF-IDF score of the terms in these article as the association score. Another scoring function might be the co-occurrence count or Pearson correlation score between t and c. As we can notice, only very small subset of the concept space would have non-zero scores with the given terms. Moreover, the BOC vector is generated from the topn concepts which have relatively high association scores with the input terms (typically few hundreds). Thus each text snippet is mapped to a very sparse vector of millions of dimensions having only few hundreds non-zero values (Peng et al., 2016).\nTypically, the cosine similarity measure is used compute the similarity between a pair of BOC vectors u and v. Because the concept vectors are very sparse, we can rewrite each vector as a vector of tuples (ci, wi). Suppose that u = {(cn1 , u1), . . . , (cn|u| , u|u|)} and v = {(cm1 , v1), . . . , (cm|v| , v|v|)}, where ui and vj are the corresponding weights of concepts cni and cmj respectively. And ni, mj are the indices of these concepts in the concept space C such that 1\u2264 ni,mj \u2264N . Then, the relatedness score can be written as in equation 2:\nSimcos(u,v) =\n\u2211|u| i=1 \u2211|v| j=1 1(ni=mj)uivj\n\u221a\n\u2211|u| i=1 u 2 i\n\u221a\n\u2211|v| j=1 v 2 j\n(2)\nwhere 1 is the indicator function which returns 1 if ni=mj and 0 otherwise. Having such sparse representation and using exact match similarity scoring measure, we can expect that two very similar text snippets might have zero similarity score if they map to different but very related set of concepts (Song and Roth, 2015).\nNeural embedding models have been proposed to overcome the BOC sparsity problem. The basic idea is to learn fixed size continuous vectors for each concept. These vectors can then be used to compute concept-concept similarity and thus overcome the concept mismatch problem.\nIn this paper we propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model (Mikolov et al., 2013b). Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only). After learning the concept vectors, we propose an efficient concept vector aggregation method to generate fully dense BOC representations. Our efficient aggregation method allows measuring the similarity between pairs of BOC vectors in linear time. This is more efficient than prior methods which require quadratic time or at least log-linear time if optimized (see equation 2).\nWe evaluate our embedding models on two\ntasks:\n1. An intrinsic task of measuring entity seman-\ntic relatedness where our CRC model outperforms other concept embedding models.\n2. An extrinsic task of dataless classifica-\ntion. Experimental results show that we can achieve better accuracy using our efficient BOC densification method compared to the original BOC sparse representation. The contributions of this paper are threefold: First, we propose two low cost concept embedding models which requires few hours rather than days to train. Second, we propose simple and efficient vector aggregation method to obtain fully densified BOC vectors in linear time. Third, we demonstrate through experiments that we can obtain same or better accuracy using the densified BOC representation with much less dimensions (few in most cases), reducing the computational cost of generating the BOC vector significantly."}, {"heading": "2 Related Work", "text": "Concept/Entity Embeddings: neural embedding models have been proposed to learn distributed representations of concepts/entities. Song and Roth (2015) proposed using the popular Word2Vec model (Mikolov et al., 2013a) to obtain the embeddings of each concept by averaging the vectors of the concept\u2019s individual words. For example, the embeddings of Microsoft Office would be obtained by averaging the embeddings of Mi-\ncrosoft and Office obtained from the Word2Vec model. Clearly, this method disregards the fact that the semantics of multi-word concepts is different from the semantics of their individual words.\nMore robust concept embeddings can be learned from the concept\u2019s corresponding article and/or from the structure of the employed KB (e.g., its link graph). Such concept embedding models were proposed by Hu et al. (2015); Li et al. (2016); Yamada et al. (2016) who all utilize the skip-gram model (Mikolov et al., 2013b), but differ in how they define the context of the target concept.\nLi et al. (2016) extended the embedding model proposed by Hu et al. (2015) by jointly learning concept and category embeddings from contexts defined by all other concepts in the target concept\u2019s article as well as its category hierarchy in Wikipedia. This method has the advantage of learning embeddings of both concepts and categories simultaneously. However, defining the concept contexts as pairs of the target concept and all other concepts appearing in its corresponding article might introduce noisy contexts, especially for long articles. For example, the Wikipedia article for United States contains links to Kindergarten, First grade, and Secondary school under the Education section.\nYamada et al. (2016) proposed a method based on the skip-gram model to jointly learn embeddings of words and concepts using contexts generated from surrounding words of the target concept or word. The authors also proposed incorporating the KB link graph by generating contexts from all concepts with outgoing link to the target concept to better model concept-concept relatedness.\nUnlike Li et al. (2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al. (2016) models as they require few hours rather than days to train on similar computing resources.\nBOC Densification: distributed concept vectors have been used by BOC densification mechanisms to overcome the BOC sparsity problem. Song and Roth (2015) proposed three different mechanisms for aligning the concepts at different indices given a sparse BOC pair (u,v) in order to increase their similarity score.\nThe many-to-many mechanism works by averaging all pairwise similarities. The many-toone mechanism works by aligning each concept in u with the most similar concept in v (i.e., its best match). Clearly, the complexity of these two mechanisms is quadratic. The third mechanism is the one-to-one. It utilizes the Hungarian method in order to find an optimal alignment on a one-to-one basis (Papadimitriou and Steiglitz, 1982). This mechanism performed the best on dataless classification and was also utilized by Li et al. (2016). However, the Hungarian method is a combinatorial optimization algorithm whose complexity is polynomial. Our proposed densification mechanism is more efficient than these three mechanisms as its complexity is linear with respect to the number of non-zero elements in the BOC vector. Additionally, it is simpler as it does not require tuning a cut off threshold for the minimum similarity between two aligned concepts as in prior work."}, {"heading": "3 Concept Embeddings for BOC Densification", "text": "A main objective of learning concept embeddings is to overcome the inherent problem of data sparsity associated with the BOC representation. Here we try to learn continuous concept vectors by building upon the skip-gram embedding model (Mikolov et al., 2013b). In the conventional skipgram model, a set of contexts are generated by sliding a context window of predefined size over sentences of a given text corpus. Vector representation of a target word is learned with the objective to maximize the ability of predicting surrounding words of that target word.\nFormally, given a training corpus of V words w1, w2, ..., wV . The skip-gram model aims to maximize the average log probability:\n1\nV\nV \u2211\ni=1\n\u2211\n\u2212s\u2264j\u2264s,j 6=0\nlog p(wi+j |wi) (3)\nwhere s is the context window size, wi is the target word, and wi+j is a surrounding context word.\nThe softmax function is used to estimate the probability p(wO|wI) as follows:\np(wO|wI) = exp(v\u22bawOuwI )\n\u2211W w=1 exp(v \u22ba wuwI ) (4)\nwhere uw and vw are the input and output vectors respectively and W is the vocabulary size. Mikolov et al. (2013b) proposed hierarchical softmax and negative sampling as efficient alternatives to approximate the softmax function which becomes computationally intractable whenW becomes huge.\nOur approach genuinely learns distributed concept representations by generating concept contexts from mentions of those concepts in large encyclopedic KBs such as Wikipedia. Utilizing such annotated KBs eliminates the need to manually annotate concept mentions and thus comes at no cost."}, {"heading": "3.1 Concept Raw Context Model (CRC)", "text": "In this model, we jointly learn the embeddings of both words and concepts. First, all concept mentions are identified in the given corpus. Second, contexts are generated for both words and concepts from both other surrounding words and other surrounding concepts as well. After generating all the contexts, we use the skip-gram model to jointly learn words and concepts embeddings. Formally, given a training corpus of V words w1, w2, ..., wV . We iterate over the corpus identifying words and concept mentions and thus generating a sequence of T tokens t1, t2, ...tT where T < V (as multiword concepts will be counted as one token). Afterwards we train the a skip-gram model aiming to maximize:\n1\nT\nT \u2211\ni=1\n\u2211\n\u2212s\u2264j\u2264s,j 6=0\nlog p(ti+j|ti) (5)\nwhere as in the conventional skip-gram model, s is the context window size. In this model, ti is the target token which would be either a word or a concept mention, and ti+j is a surrounding context word or concept mention.\nThis model is different from Yamada et al. (2016) anchor context model in three aspects: 1) while generating target concept contexts, we utilize not only surrounding words but other surrounding concepts as well, 2) our model aims to maximize p(ti+j|ti) where t could be a word or a concept, while Yamada et al. (2016) model\nmaximizes p(wi+j |ei) where ei is the target concept/entity (see Yamada et al. (2016) Eq. 6), and 3) in case ti is a concept, our model captures all the contexts in which it appeared, while Yamada et al. (2016) model generates for each entity one context of s previous and s next words. We hypothesize that considering both concepts and individual words in the optimization function would generate more robust embeddings."}, {"heading": "3.2 Concept-Concept Context Model (3C)", "text": "Inspired by the distributional hypothesis (Harris, 1954), we, in this model, hypothesize that \u201dsimilar concepts tend to appear in similar conceptual contexts\u201d. In order to test this hypothesis, we learn concept embeddings by training a skip-gram model on contexts generated solely from concept mentions. As in the CRC model, we start by identifying all concept mentions in the given corpus. Then, contexts are generated from only surrounding concepts. Formally, given a training corpus of V words w1, w2, ..., wV . We iterate over the corpus identifying concept mentions and thus generating a sequence of C concept tokens c1, c2, ...cC where C < V . Afterwards we train the skip-gram model aiming to maximize:\n1\nC\nC \u2211\ni=1\n\u2211\n\u2212s\u2264j\u2264s,j 6=0\nlog p(ci+j |ci) (6)\nwhere s is the context window size, ci is the target concept, and ci+j is a surrounding concept mention within s mentions.\nThis model is different from Li et al. (2016) and Hu et al. (2015) as they define the context of a target concept by all the other concepts which appear in the concept\u2019s corresponding article. Clearly, some of these concepts might be irrelevant especially for very long articles which cite hundreds of other concepts. Our 3C model, alternatively, learns concept semantics from surrounding concepts and not only from those that are cited in its article. We also extend the context window beyond pairs of concepts allowing more influence to other nearby concepts.\nThe main advantage of the 3C model over the CRC model is its computational efficiency where the model vocabulary is limited to the corpus concepts (Wikipedia in our case). One the other hand, the CRC model is advantageous because it jointly learns the embeddings of words and concepts and is therefore expected to generate higher\nquality vectors. In other words, the CRC model can capture more contextual signals such as actions, times, and relationships at the expense of training computational cost."}, {"heading": "3.3 Training", "text": "We utilize a recent Wikipedia dump of August 20161, which has about 7 million articles. We extract articles plain text discarding images and tables. We also discard References and External links sections (if any). We pruned both articles not under the main namespace and pruned all redirect pages as well. Eventually, our corpus contained about 5 million articles in total.\nWe preprocess each article replacing all its references to other Wikipedia articles with the their corresponding page IDs. In case any of the references is a title of a redirect page, we use the page ID of the original page to ensure that all concept mentions are normalized.\nFollowing Mikolov et al. (2013b), we utilize negative sampling to approximate the softmax function by replacing every log p(wO|wI) term in the softmax function (equation 4) with:\nlog \u03c3(v\u22bawOuwI )+ k \u2211\ns=1\nEws\u223cPn(w)[log \u03c3(\u2212v \u22ba ws uwI )]\n(7)\nwhere k is the number of negative samples drawn for each word and \u03c3(x) is the sigmoid function ( 1 1+e\u2212x\n). In the case of the CRC model wI and wO would be replaced with ti and ti+j respectively. And in the case of the 3C model wI and wO would be replaced with ci and ci+j respectively.\nFor both the CRC & 3C models with use a context window of size 9 and a vector of 500 dimensions. We train the skip-gram model for 10 iterations using 12 cores machine with 64GB of RAM. The CRCmodel took\u223c15 hours to train for a total of\u223c12.7 million tokens. The 3C model took \u223c1.5 hours to train for a total of \u223c4.5 million concepts."}, {"heading": "3.4 BOC Densification", "text": "As we mentioned in the related work section, the current mechanisms for BOC densification are inefficient as their complexity is least quadratic with respect to the number of non-zero elements in the BOC vector. Here, we propose simple and efficient vector aggregation method to obtain fully\n1http://dumps.wikimedia.org/enwiki/\ndensified BOC vectors in linear time. Our mechanism works by performing a weighted average of the embedding vectors of all concepts in the given BOC. This operation scales linearly with the number of non-zero dimensions in the BOC vector. In addition, it produces a fully dense vector representing the semantics of the original concepts and considering their weights. Formally, given a sparse BOC vector s= {(c1, w1), . . . , (c|s|, w|s|)} where wi is weight of concept ci. We can obtain the dense representation of s as in equation 8:\nsdense = \u2211|s| i=1wi.uci \u2211|s|\ni=1 wi (8)\nwhere uci is the embedding vector of concept ci. Once we have this dense BOC vector, we can apply the cosine measure to compute the similarity between a pair of dense BOC vectors.\nAs we can notice, this weighted average is done once and for all for a given BOC vector. Other mechanisms that rely on concept alignment (Song and Roth, 2015), require realignment every time a given BOC vector is compared to another BOC vector. Our approach improves the efficiency especially in the context of dataless classification with large number of classes. Using our densification mechanism, we apply the weighted average for each class vector and for each instance once.\nInterestingly, our densification mechanism allows us to densify the sparse BOC vector using only the top few dimensions. As we will show in the experiments section, we can get (near-)best results using these few dimensions compared to densifying with all the dimensions in the original sparse vector. This property reduces the cost of obtaining a BOC vector with a few hundred dimensions in the first place."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Entity Semantic Relatedness", "text": "We evaluate the \u201dgoodness\u201d of our concept embeddings on measuring entity semantic relatedness as an intrinsic evaluation. Entity relatedness has been recently used to model entity coherence in many named entity disambiguation systems."}, {"heading": "4.1.1 Dataset", "text": "We use a benchmark dataset created by Ceccarelli et al. (2013) from the CoNLL 2003 data. As in previous studies (Yamada et al., 2016;\nHuang et al., 2015), we model measuring entity relatedness as a ranking problem. We use the test split of the dataset to create 3,314 queries. Each query has a query entity and \u223c91 response entities labeled as related or unrelated. The quality is measured by the ability of the system to rank related entities on top of unrelated ones."}, {"heading": "4.1.2 Compared Systems", "text": "We compare our models with two prior methods:\n1. Yamada et al. (2016)who used the skip-gram\nmodel to learn embeddings of words and entities jointly. The authors also utilized Wikipedia link graph to better model entityentity relatedness.\n2. Witten and Milne (2008) who proposed\nWikipedia Link-based Measure (WLM) as a simple mechanism for modeling the semantic relatedness between Wikipedia concepts. The authors utilized Wikipedia link structure under the assumption that related concepts would have similar incoming links."}, {"heading": "4.1.3 Results", "text": "We report Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (nDCG) scores as commonly used measures for evaluating the ranking quality. Table 1 shows the performance of our CRC and 3C models compared to previous models. As we can see, the 3C model performs poorly on this task compared to prior models. On the other hand, our CRC model outperforms all the other methods by 2-4% in terms of nDCG and by 3% percent in terms of MAP."}, {"heading": "4.2 Dataless Classification", "text": "Chang et al. (2008) proposed dataless classification as a learning protocol to perform text categorization without the need for labeled data to train a classifier. Given only label names and few descriptive keywords of each label, classification is performed on the fly by mapping each label into a BOC representation using ESA. Likewise, each data instance is mapped into the same BOC semantic space and assigned to the most similar label using a proper similarity measure such as cosine. Formally, given a set of n labels L = {l1, ..., ln}, a text document d, a BOC mapping model f , and a similarity function Sim, then d is assigned to the ith label li such that li = arg maxi Sim(f(li), f(d)). We evaluate the effectiveness of our concept embedding models on the dataless classification task as an extrinsic evaluation. We demonstrate through empirical results the efficiency and effectiveness of our proposed BOC densification scheme in obtaining better classification results compared to the original sparse BOC representation."}, {"heading": "4.2.1 Dataset", "text": "We use the 20 Newsgroups dataset (20NG) (Lang, 1995) which is commonly used for benchmarking text classification algorithms. The dataset contains 20 categories each has \u223c1000 news posts. We obtained the BOC representations using ESA from Song and Roth (2014) who utilized a Wikipedia index containing pages with 100+ words and 5+ outgoing links to create ESA mappings of 500 dimensions for both the categories and news posts of the 20NG. We designed two types of classification tasks: 1) fine-grained classification involving closely related classes such as Hockey vs. Baseball, Autos vs. Motorcycles, and Guns vs. Mideast vs. Misc, and 2) coarse-grained classification involving top-level categories such as Sport vs. Politics and Sport vs. Religion. The top-level categories are created by combining instances of the\nfine-grained categories as shown in Table 2."}, {"heading": "4.2.2 Compared Systems", "text": "We compare our models with three prior methods:\n1. ESA which computes the cosine similarity\nusing the sparse BOC vectors.\n2. WEmax &WEhung which were proposed by\nSong and Roth (2015) for BOC densification using embeddings obtained from Word2Vec. As the authors reported, we fix the minimum similarity threshold to 0.85. WEmax finds the best match for each concept, while WEhung utilizes the Hungarian algorithm to find the best concept-concept alignment on one-toone basis. Both mechanisms have polynomial time complexity."}, {"heading": "4.2.3 Results", "text": "Table 3 presents the results of fine-grained dataless classification measured in micro-averaged F1. As we can notice, ESA achieves its peak performance with a few hundred dimensions of the sparse BOC vector. Using our densification mechanism, both the CRC & 3C models achieve equal performance to ESA at much less dimensions. Densification using the CRC model embeddings gives the best F1 scores on the three tasks. Interestingly, the CRC model improves the F1 score by \u223c7% using only 14 concepts on Autos vs. Motorcycles, and by \u223c3% using 70 concepts on Guns vs. Mideast vs. Misc. The 3C model, still performs better than ESA on 2 out of the 3 tasks. Both WEmax and WEhung improve the performance over ESA but\nnot as our CRC model.\nIn order to better illustrate the robustness of our densification mechanism when varying the # of BOC dimensions, we measured F1 scores of each task as a function of the # of BOC dimensions used for densification. As we see in Figure 1, with one concept we can achieve high F1 scores compared to ESA which achieves zero or very low F1. Moreover, near-peak performance is achievable with the top 50 or less dimensions. We can also notice that, as we increase the # of dimensions, both WEmax and WEhung densification methods have the same undesired monotonic pattern like ESA. Actually, the imposed threshold by these methods does not allow for full dense representation of the BOC vector and therefore at low dimensions we still see low overall F1 score. Our proposed densification mechanisms besides their low cost, produce fully densified representations allowing good similarities at low dimensions.\nResults of coarse-grained classification are presented in Table 4. Classification at the top level is easier than the fine-grained level. Nevertheless,\nas with fine-grained classification, ESA still peaks with a few hundred dimensions of the sparse BOC vector. Both the CRC & 3C models achieve equal performance to ESA at very few dimensions (\u22646). Densification using the CRC model embeddings still performs the best on both tasks. Interestingly, the 3C model gives very close F1 scores to the CRC model at less dimensions (@4 with Sport vs. Politics, and @60 with Sport vs. Religion) indicating its competitive advantage when computational cost is a decisive criteria. The 3C model, still performs better than ESA, WEmax, and WEhung on both tasks.\nFigure 2 shows F1 scores of coarse-grained classification when varying the # of BOC dimensions used for densification. The same pattern of achieving near-peak performance at very few dimensions recur with the CRC & 3C models. ESA using the sparse BOC vectors achieves low F1 up until few hundred dimensions are considered. Even with the costly WEmax and WEhung densifications, performance sometimes decreases."}, {"heading": "5 Conclusion", "text": "In this paper we proposed two models for learning concept embeddings based on the skip-gram model. We also proposed an efficient and effective mechanism for BOC densification which outperformed the prior proposed densification schemes on dataless classification. Unlike these prior densification mechanisms, our method scales linearly with the # of the BOC dimensions. In addition, we demonstrated through the results how this efficient mechanism allows generating high quality dense BOC vectors from few concepts alleviating the need of obtaining hundreds of concepts when generating the concept vector."}], "references": [{"title": "Learning relatedness measures for entity linking", "author": ["Diego Ceccarelli", "Claudio Lucchese", "Salvatore Orlando", "Raffaele Perego", "Salvatore Trani."], "venue": "Proceedings of the 22nd ACM international conference on Information & Knowledge Management.", "citeRegEx": "Ceccarelli et al\\.,? 2013", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 2013}, {"title": "Importance of semantic representation: Dataless classification", "author": ["Ming-Wei Chang", "Lev-Arie Ratinov", "Dan Roth", "Vivek Srikumar."], "venue": "AAAI. volume 2, pages 830\u2013835.", "citeRegEx": "Chang et al\\.,? 2008", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Concept-based information retrieval using explicit semantic analysis", "author": ["Ofer Egozi", "Shaul Markovitch", "Evgeniy Gabrilovich."], "venue": "ACM Transactions on Information Systems (TOIS) 29(2):8.", "citeRegEx": "Egozi et al\\.,? 2011", "shortCiteRegEx": "Egozi et al\\.", "year": 2011}, {"title": "Computing semantic relatedness using wikipediabased explicit semantic analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "IJCAI. volume 7, pages 1606\u20131611.", "citeRegEx": "Gabrilovich and Markovitch.,? 2007", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word .", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Entity hierarchy embedding", "author": ["Zhiting Hu", "Poyao Huang", "Yuntian Deng", "Yingkai Gao", "Eric P Xing."], "venue": "Proceedings of The 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hu et al\\.,? 2015", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Leveraging deep neural networks and knowledge graphs for entity disambiguation", "author": ["Hongzhao Huang", "Larry Heck", "Heng Ji."], "venue": "arXiv preprint arXiv:1504.07678 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Context-dependent conceptualization", "author": ["Dongwoo Kim", "Haixun Wang", "Alice H Oh."], "venue": "IJCAI. pages 2330\u20132336.", "citeRegEx": "Kim et al\\.,? 2013", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Newsweeder: Learning to filter netnews", "author": ["Ken Lang."], "venue": "Proceedings of the 12th international conference on machine learning. pages 331\u2013339.", "citeRegEx": "Lang.,? 1995", "shortCiteRegEx": "Lang.", "year": 1995}, {"title": "Joint embedding of hierarchical categories and entities for concept categorization and dataless classification", "author": ["Yuezhang Li", "Ronghuo Zheng", "Tian Tian", "Zhiting Hu", "Rahul Iyer", "Katia Sycara."], "venue": "arXiv preprint arXiv:1607.07956 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["Christos H Papadimitriou", "Kenneth Steiglitz."], "venue": "Courier Corporation.", "citeRegEx": "Papadimitriou and Steiglitz.,? 1982", "shortCiteRegEx": "Papadimitriou and Steiglitz.", "year": 1982}, {"title": "On dataless hier", "author": ["Yangqiu Song", "Dan Roth"], "venue": null, "citeRegEx": "Song and Roth.,? \\Q2014\\E", "shortCiteRegEx": "Song and Roth.", "year": 2014}, {"title": "Joint learning of the em", "author": ["Yoshiyasu Takefuji"], "venue": null, "citeRegEx": "Takefuji.,? \\Q2016\\E", "shortCiteRegEx": "Takefuji.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "For example, measuring semantic similarity/relatedness (Gabrilovich and Markovitch, 2007; Kim et al., 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al.", "startOffset": 55, "endOffset": 135}, {"referenceID": 7, "context": "For example, measuring semantic similarity/relatedness (Gabrilovich and Markovitch, 2007; Kim et al., 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al.", "startOffset": 55, "endOffset": 135}, {"referenceID": 1, "context": ", 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al., 2008; Song and Roth, 2014, 2015; Li et al., 2016), short text clustering (Song et al.", "startOffset": 61, "endOffset": 125}, {"referenceID": 9, "context": ", 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al., 2008; Song and Roth, 2014, 2015; Li et al., 2016), short text clustering (Song et al.", "startOffset": 61, "endOffset": 125}, {"referenceID": 2, "context": ", 2015), search and relevancy ranking (Egozi et al., 2011), event detection and coreference resolution (Peng et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 3, "context": "For example, Gabrilovich and Markovitch (2007) proposed Explicit Semantic Analysis (ESA) which uses Wikipedia articles as concepts and the TF-IDF score of the terms in these article as the association score.", "startOffset": 13, "endOffset": 47}, {"referenceID": 11, "context": "In this paper we propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model (Mikolov et al., 2013b).", "startOffset": 127, "endOffset": 150}, {"referenceID": 10, "context": "Song and Roth (2015) proposed using the popular Word2Vec model (Mikolov et al., 2013a) to obtain the embeddings of each concept by averaging the vectors of the concept\u2019s individual words.", "startOffset": 63, "endOffset": 86}, {"referenceID": 11, "context": "Song and Roth (2015) proposed using the popular Word2Vec model (Mikolov et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "(2016) who all utilize the skip-gram model (Mikolov et al., 2013b), but differ in how they define the context of the target concept.", "startOffset": 43, "endOffset": 66}, {"referenceID": 5, "context": "Such concept embedding models were proposed by Hu et al. (2015); Li et al.", "startOffset": 47, "endOffset": 64}, {"referenceID": 5, "context": "Such concept embedding models were proposed by Hu et al. (2015); Li et al. (2016); Yamada et al.", "startOffset": 47, "endOffset": 82}, {"referenceID": 5, "context": "Such concept embedding models were proposed by Hu et al. (2015); Li et al. (2016); Yamada et al. (2016) who all utilize the skip-gram model (Mikolov et al.", "startOffset": 47, "endOffset": 104}, {"referenceID": 5, "context": "(2016) extended the embedding model proposed by Hu et al. (2015) by jointly learning concept and category embeddings from contexts defined by all other concepts in the target concept\u2019s article as well as its category hierarchy in Wikipedia.", "startOffset": 48, "endOffset": 65}, {"referenceID": 8, "context": "Unlike Li et al. (2016) and Hu et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space.", "startOffset": 11, "endOffset": 28}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts.", "startOffset": 11, "endOffset": 308}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al.", "startOffset": 11, "endOffset": 562}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al. (2016) models as they require few hours rather than days to train on similar computing resources.", "startOffset": 11, "endOffset": 587}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al. (2016) models as they require few hours rather than days to train on similar computing resources. BOC Densification: distributed concept vectors have been used by BOC densification mechanisms to overcome the BOC sparsity problem. Song and Roth (2015) proposed three different mechanisms for aligning the concepts at different indices given a sparse BOC pair (u,v) in order to increase their similarity score.", "startOffset": 11, "endOffset": 831}, {"referenceID": 12, "context": "It utilizes the Hungarian method in order to find an optimal alignment on a one-to-one basis (Papadimitriou and Steiglitz, 1982).", "startOffset": 93, "endOffset": 128}, {"referenceID": 9, "context": "This mechanism performed the best on dataless classification and was also utilized by Li et al. (2016). However, the Hungarian method is a combinatorial optimization algorithm whose complexity is polynomial.", "startOffset": 86, "endOffset": 103}, {"referenceID": 11, "context": "Here we try to learn continuous concept vectors by building upon the skip-gram embedding model (Mikolov et al., 2013b).", "startOffset": 95, "endOffset": 118}, {"referenceID": 10, "context": "Mikolov et al. (2013b) proposed hierarchical softmax and negative sampling as efficient alternatives to approximate the softmax function which becomes computationally intractable whenW becomes huge.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Inspired by the distributional hypothesis (Harris, 1954), we, in this model, hypothesize that \u201dsimilar concepts tend to appear in similar conceptual contexts\u201d.", "startOffset": 42, "endOffset": 56}, {"referenceID": 8, "context": "This model is different from Li et al. (2016) and Hu et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) as they define the context of a target concept by all the other concepts which appear in the concept\u2019s corresponding article.", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "Following Mikolov et al. (2013b), we utilize negative sampling to approximate the softmax function by replacing every log p(wO|wI) term in the softmax function (equation 4) with:", "startOffset": 10, "endOffset": 33}, {"referenceID": 6, "context": "WLM (Huang et al., 2015) 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 8, "context": "We use the 20 Newsgroups dataset (20NG) (Lang, 1995) which is commonly used for benchmarking text classification algorithms.", "startOffset": 40, "endOffset": 52}, {"referenceID": 8, "context": "We use the 20 Newsgroups dataset (20NG) (Lang, 1995) which is commonly used for benchmarking text classification algorithms. The dataset contains 20 categories each has \u223c1000 news posts. We obtained the BOC representations using ESA from Song and Roth (2014) who utilized a Wikipedia index containing pages with 100+ words and 5+ outgoing links to create ESA mappings of 500 dimensions for both the categories and news posts of the 20NG.", "startOffset": 41, "endOffset": 259}, {"referenceID": 13, "context": "WEmax &WEhung which were proposed by Song and Roth (2015) for BOC densification using embeddings obtained from Word2Vec.", "startOffset": 37, "endOffset": 58}], "year": 2017, "abstractText": "Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions.", "creator": "LaTeX with hyperref package"}}}