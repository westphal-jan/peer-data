{"id": "1702.03525", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Learning to Parse and Translate Improves Neural Machine Translation", "abstract": "there has been relatively little attention to ways incorporating linguistic prior to neural machine dog translation. much of the previous work was further constrained to considering linguistic prior on the translated source side. in this paper, we propose a hybrid model, called nmt + rg, that learns to parse and indirectly translate by combining the recurrent neural path network grammar into the attention - based neural algorithm machine translation. our approach encourages the neural machine translation model to incorporate linguistic prior cues during training, recognize and lets it translate on its own afterward. extensive experiments with the four language coding pairs show the effectiveness of integrating the proposed nmt + rg.", "histories": [["v1", "Sun, 12 Feb 2017 13:19:03 GMT  (135kb,D)", "https://arxiv.org/abs/1702.03525v1", null], ["v2", "Sun, 23 Apr 2017 16:52:03 GMT  (78kb,D)", "http://arxiv.org/abs/1702.03525v2", "Accepted as a short paper at the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akiko eriguchi", "yoshimasa tsuruoka", "kyunghyun cho"], "accepted": true, "id": "1702.03525"}, "pdf": {"name": "1702.03525.pdf", "metadata": {"source": "CRF", "title": "Learning to Parse and Translate Improves Neural Machine Translation", "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho"], "emails": ["tsuruoka}@logos.t.u-tokyo.ac.jp", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge. Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary.\nOn the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized\nform and dependency label. Eriguchi et al. (2016) instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree. Stahlberg et al. (2016) let the lattice from a hierarchical phrase-based system guide the decoding process of neural machine translation, which results in two separate models rather than a single end-to-end one. Despite the promising improvements, these explicit approaches are limited in that the trained translation model strictly requires the availability of external tools during inference time. More recently, researchers have proposed methods to incorporate target-side syntax into NMT models. Alvarez-Melis and Jaakkola (2017) have proposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown. Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences.\nWe propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1, that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG. We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence. We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time.\n1Our code is available at https://github.com/ tempra28/nmtrnng.\nar X\niv :1\n70 2.\n03 52\n5v 2\n[ cs\n.C L\n] 2\n3 A\npr 2\n01 7\nWe evaluate the proposed NMT+RNNG on four language pairs ({JP, Cs, De, Ru}-En). We observe significant improvements in terms of BLEU scores on three out of four language pairs and RIBES scores on all the language pairs."}, {"heading": "2 Neural Machine Translation", "text": "Neural machine translation is a recently proposed framework for building a machine translation system based purely on neural networks. It is often built as an attention-based encoder-decoder network (Cho et al., 2015) with two recurrent networks\u2014encoder and decoder\u2014and an attention model. The encoder, which is often implemented as a bidirectional recurrent network with long short-term memory units (LSTM, Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU, Cho et al., 2014), first reads a source sentence represented as a sequence of words x = (x1, x2, . . . , xN ). The encoder returns a sequence of hidden states h = (h1, h2, . . . , hN ). Each hidden state hi is a concatenation of those from the forward and backward recurrent network: hi =[\u2212\u2192 h i; \u2190\u2212 h i ] , where\n\u2212\u2192 h i = \u2212\u2192 f enc( \u2212\u2192 h i\u22121, Vx(xi)), \u2190\u2212 h i = \u2190\u2212 f enc( \u2190\u2212 h i+1, Vx(xi)).\nVx(xi) refers to the word vector of the i-th source word.\nThe decoder is implemented as a conditional recurrent language model which models the target sentence, or translation, as\nlog p(y|x) = \u2211 j log p(yj |y<j ,x),\nwhere y = (y1, . . . , yM ). Each of the conditional probabilities in the r.h.s is computed by\np(yj = y|y<j ,x) = softmax(W>y s\u0303j), (1) s\u0303j = tanh(Wc[sj ; cj ]), (2)\nsj = fdec(sj\u22121, [Vy(yj\u22121); s\u0303j\u22121]), (3)\nwhere fdec is a recurrent activation function, such as LSTM or GRU, and Wy is the output word vector of the word y. cj is a time-dependent context vector that is computed by the attention model using the sequence h of hidden states from the encoder. The attention model first compares the current hidden\nstate sj against each of the hidden states and assigns a scalar score: \u03b2i,j = exp(h>i Wdsj) (Luong et al., 2015). These scores are then normalized across the hidden states to sum to 1, that is \u03b1i,j =\n\u03b2i,j\u2211 i \u03b2i,j\n. The time-dependent context vector is then a weighted-sum of the hidden states with these attention weights: cj = \u2211 i \u03b1i,jhi."}, {"heading": "3 Recurrent Neural Network Grammars", "text": "A recurrent neural network grammar (RNNG, Dyer et al., 2016) is a probabilistic syntax-based language model. Unlike a usual recurrent language model (see, e.g., Mikolov et al., 2010), an RNNG simultaneously models both tokens and their tree-based composition. This is done by having a (output) buffer, stack and action history, each of which is implemented as a stack LSTM (sLSTM, Dyer et al., 2015). At each time step, the action sLSTM predicts the next action based on the (current) hidden states of the buffer, stack and action sLSTM. That is,\np(at = a|a<t) \u221d eW > a faction(h buffer t ,h stack t ,h action t ), (4)\nwhere Wa is the vector of the action a. If the selected action is shift, the word at the beginning of the buffer is moved to the stack. When the reduce action is selected, the top-two words in the stack are reduced to build a partial tree. Additionally, the action may be one of many possible non-terminal symbols, in which case the predicted non-terminal symbol is pushed to the stack.\nThe hidden states of the buffer, stack and action sLSTM are correspondingly updated by\nhbuffert = StackLSTM(h buffer top , Vy(yt\u22121)), (5) hstackt = StackLSTM(h stack top , rt), hactiont = StackLSTM(h action top , Va(at\u22121)),\nwhere Vy and Va are functions returning the target word and action vectors. The input vector rt of the stack sLSTM is computed recursively by\nrt = tanh(Wr[r d; rp;Va(at)]),\nwhere rd and rp are the corresponding vectors of the parent and dependent phrases, respectively (Dyer et al., 2015). This process is iterated until a complete parse tree is built. Note that the original paper of RNNG (Dyer et al., 2016) uses constituency trees, but we employ dependency trees in this paper. Both types of trees are\nrepresented as a sequence of the three types of actions in a transition-based parsing model.\nWhen the complete sentence is provided, the buffer simply summarizes the shifted words. When the RNNG is used as a generator, the buffer further generates the next word when the selected action is shift. The latter can be done by replacing the buffer with a recurrent language model, which is the idea on which our proposal is based."}, {"heading": "4 Learning to Parse and Translate", "text": ""}, {"heading": "4.1 NMT+RNNG", "text": "Our main proposal in this paper is to hybridize the decoder of the neural machine translation and the RNNG. We continue from the earlier observation that we can replace the buffer of RNNG to a recurrent language model that simultaneously summarizes the shifted words as well as generates future words. We replace the RNNG\u2019s buffer with the neural translation model\u2019s decoder in two steps.\nConstruction First, we replace the hidden state of the buffer hbuffer (in Eq. (5)) with the hidden state of the decoder of the attention-based neural machine translation from Eq. (3). As is clear from those two equations, both the buffer sLSTM and the translation decoder take as input the previous hidden state (hbuffertop and sj\u22121, respectively) and the previously decoded word (or the previously shifted word in the case of the RNNG\u2019s buffer), and returns its summary state. The only difference is that the translation decoder additionally considers the state s\u0303j\u22121. Once the buffer of the RNNG is replaced with the NMT decoder in our proposed model, the NMT decoder is also under control of the actions provided by the RNNG.2 Second, we let the next word prediction of the translation decoder as a generator of RNNG. In other words, the generator of RNNG will output a word, when asked by the shift action, according to the conditional distribution defined by the translation decoder in Eq. (1). Once the buffer sLSTM is replaced with the neural translation decoder, the action sLSTM naturally takes as input the translation decoder\u2019s hidden state when computing the action conditional distribution in Eq. (4). We call this hybrid model NMT+RNNG.\n2The j-th hidden state in Eq. (3) is calculated only when the action (shift) is predicted by the RNNG. This is why our proposed model can handle the sequences of words and actions which have different lengths.\nLearning and Inference After this integration, our hybrid NMT+RNNG models the conditional distribution over all possible pairs of translation and its parse given a source sentence, i.e., p(y,a|x). Assuming the availability of parse annotation in the target-side of a parallel corpus, we train the whole model jointly to maximize E(x,y,a)\u223cdata [log p(y,a|x)]. In doing so, we notice that there are two separate paths through which the neural translation decoder receives error signal. First, the decoder is updated in order to maximize the conditional probability of the correct next word, which has already existed in the original neural machine translation. Second, the decoder is updated also to maximize the conditional probability of the correct parsing action, which is a novel learning signal introduced by the proposed hybridization. Furthermore, the second learning signal affects the encoder as well, encouraging the whole neural translation model to be aware of the syntactic structure of the target language. Later in the experiments, we show that this additional learning signal is useful for translation, even though we discard the RNNG (the stack and action sLSTMs) in the inference time."}, {"heading": "4.2 Knowledge Distillation for Parsing", "text": "A major challenge in training the proposed hybrid model is that there is not a parallel corpus augmented with gold-standard target-side parse, and vice versa. In other words, we must either parse the target-side sentences of an existing parallel corpus or translate sentences with existing goldstandard parses. As the target task of the proposed model is translation, we start with a parallel corpus and annotate the target-side sentences. It is however costly to manually annotate any corpus of reasonable size (Table 6 in Alonso et al., 2016).\nWe instead resort to noisy, but automated annotation using an existing parser. This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al., 2009). In knowledge distillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our approach where the external parser could be thought of as a teacher and the proposed hybrid network\u2019s RNNG as a student. On the other hand, what we\npropose here is a special case of distant supervision in that the external parser provides noisy annotations to otherwise an unlabeled training set.\nSpecifically, we use SyntaxNet, released by Andor et al. (2016), on a target sentence.3 We convert a parse tree into a sequence of one of three transition actions (SHIFT, REDUCE-L, REDUCE-R). We label each REDUCE action with a corresponding dependency label and treat it as a more finegrained action."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Language Pairs and Corpora", "text": "We compare the proposed NMT+RNNG against the baseline model on four different language pairs\u2013Jp-En, Cs-En, De-En and Ru-En. The basic statistics of the training data are presented in Table 1. We mapped all the low-frequency words to the unique symbol \u201cUNK\u201d and inserted a special symbol \u201cEOS\u201d at the end of both source and target sentences.\nJa We use the ASPEC corpus (\u201ctrain1.txt\u201d) from the WAT\u201916 Jp-En translation task. We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT\u201916 (WAT, 2016). We use the first 100K sentence pairs of length shorter than 50 for training. The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus. We use \u201cdev.txt\u201d and \u201ctest.txt\u201d provided by WAT\u201916 respectively as development and test sets.\nCs, De and Ru We use News Commentary v8. We removed noisy metacharacters and used the tokenizer from Moses (Koehn et al., 2007) to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, Ru and De. The target-side (English) vocabulary was constructed with all the unique tokens\n3When the target sentence is parsed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words. We use the plain SyntaxNet and do not train it furthermore.\nappearing more than three times in each corpus. We also excluded the sentence pairs which include empty lines in either a source sentence or a target sentence. We only use sentence pairs of length 50 or less for training. We use \u201cnewstest2015\u201d and \u201cnewstest2016\u201d as development and test sets respectively."}, {"heading": "5.2 Models, Learning and Inference", "text": "In all our experiments, each recurrent network has a single layer of LSTM units of 256 dimensions, and the word vectors and the action vectors are of 256 and 128 dimensions, respectively. To reduce computational overhead, we use BlackOut (Ji et al., 2015) with 2000 negative samples and \u03b1 = 0.4. When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016). For the proposed NMT+RNNG, we share the target word vectors between the decoder (buffer) and the stack sLSTM.\nEach weight is initialized from the uniform distribution [\u22120.1, 0.1]. The bias vectors and the weights of the softmax and BlackOut are initialized to be zero. The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recommended in Jo\u0301zefowicz et al. (2015). We use stochastic gradient descent with minibatches of 128 examples. The learning rate starts from 1.0, and is halved each time the perplexity on the development set increases. We clip the norm of the gradient (Pascanu et al., 2012) with the threshold set to 3.0 (2.0 for the baseline models on RuEn and Cs-En to avoid NaN and Inf). When the perplexity of development data increased in training time, we halved the learning rate of stochastic gradient descent and reloaded the previous model. The RNNG\u2019s stack computes the vector of a dependency parse tree which consists of the generated target words by the buffer. Since the complete parse tree has a \u201cROOT\u201d node, the special token of the end of a sentence (\u201cEOS\u201d) is considered as the ROOT. We use beam search in the inference time, with the beam width selected based on the development set performance.\nIt took about 15 minutes per epoch and about 20 minutes respectively for the baseline and the proposed model to train a full JP-EN parallel corpus in our implementation.4\n4We run all the experiments on multi-core CPUs (10"}, {"heading": "5.3 Results and Analysis", "text": "In Table 2, we report the translation qualities of the tested models on all the four language pairs. We report both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). Except for DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT+RNNG over the baseline model. It is worthwhile to note that these significant improvements have been achieved without any additional parameters nor computational overhead in the inference time.\nAblation Since each component in RNNG may be omitted, we ablate each component in the proposed NMT+RNNG to verify their necessity.5 As shown in Table 3, we see that the best performance could only be achieved when all the three components were present. Removing the stack had the most adverse effect, which was found to be the case for parsing as well by Kuncoro et al. (2017).\nGenerated Sentences with Parsed Actions The decoder part of our proposed model consists of two components: the NMT decoder to gener-\nthreads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz) 5 Since the buffer is the decoder, it is not possible to completely remove it. Instead we simply remove the dependency of the action distribution on it.\nate a translated sentence and the RNNG decoder to predict its parsing actions. The proposed model can therefore output a dependency structure along with a translated sentence. Figure 1 shows an example of JP-EN translation in the development dataset and its dependency parse tree obtained by the proposed model. The special symbol (\u201cEOS\u201d) is treated as the root node (\u201cROOT\u201d) of the parsed tree. The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3. The parsing actions were obtained by greedy search. The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between \u201cThe\u201d and \u201c transition\u201d should not be \u201cpobj\u201d."}, {"heading": "6 Conclusion", "text": "We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate simultaneously, and training it encourages both the encoder and decoder to better incorporate linguistic priors. Our experiments confirmed its effectiveness on four language pairs ({JP, Cs, De, Ru}En). The RNNG can in principle be trained without ground-truth parses, and this would eliminate the need of external parsers completely. We leave the investigation into this possibility for future research."}, {"heading": "Acknowledgments", "text": "We thank Yuchen Qiao and Kenjiro Taura for their help to speed up the implementations of training and also Kazuma Hashimoto for his valuable comments and discussions. This work was supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number 15J12597 and\n16H01715. KC thanks support by eBay, Facebook, Google and NVIDIA."}], "references": [{"title": "Towards string-to-tree neural machine translation", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. to appear.", "citeRegEx": "Aharoni and Goldberg.,? 2017", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2017}, {"title": "From noisy questions to minecraft texts: Annotation challenges in extreme syntax scenario", "author": ["H\u00e9ctor Mart\u0131\u0301nez Alonso", "Djam\u00e9 Seddah", "Beno\u0131\u0302t Sagot"], "venue": "In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)", "citeRegEx": "Alonso et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2016}, {"title": "Tree-structured decoding with doubly-recurrent neural networks", "author": ["David Alvarez-Melis", "Tommi S. Jaakkola."], "venue": "Proceedings of International Conference on Learning Representations 2017.", "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017", "shortCiteRegEx": "Alvarez.Melis and Jaakkola.", "year": 2017}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of the Asso-", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio."], "venue": "IEEE Transactions on Multimedia 17(11):1875\u20131886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Associ-", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "A. Noah Smith."], "venue": "Proceedings of the 53rd Annual", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "A. Noah Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 823\u2013833.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Neural Machine Translation with SourceSide Latent Graph Parsing", "author": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "arXiv preprint arXiv:1702.02265 .", "citeRegEx": "Hashimoto and Tsuruoka.,? 2017", "shortCiteRegEx": "Hashimoto and Tsuruoka.", "year": 2017}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "venue": "arXiv preprint arXiv:1503.02531 .", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Automatic evaluation of translation quality for distant language pairs", "author": ["Hideki Isozaki", "Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Isozaki et al\\.,? 2010", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "S.V.N. Vishwanathan", "Nadathur Satish", "Michael J. Anderson", "Pradeep Dubey."], "venue": "Proceedings of International Conference on Learning", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning. pages 2342\u20132350.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In Proceedings of the 15th Conference of the European Chapter", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "arXiv preprint arXiv:1610.03017 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTER-", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Pointwise prediction for robust, adaptable japanese morphological analysis", "author": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Neubig et al\\.,? 2011", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5063 abs/1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "pages 1526\u2013 1534.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 299\u2013305.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies", "author": ["Barret Zoph", "Ashish Vaswani", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al.", "startOffset": 204, "endOffset": 242}, {"referenceID": 7, "context": "Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al.", "startOffset": 204, "endOffset": 242}, {"referenceID": 28, "context": ", 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016).", "startOffset": 70, "endOffset": 110}, {"referenceID": 7, "context": ", 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary.", "startOffset": 8, "endOffset": 150}, {"referenceID": 26, "context": "Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized form and dependency label.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "Eriguchi et al. (2016) instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Eriguchi et al. (2016) instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree. Stahlberg et al. (2016) let the lattice from a hierarchical phrase-based system guide the decod-", "startOffset": 0, "endOffset": 196}, {"referenceID": 2, "context": "Alvarez-Melis and Jaakkola (2017) have proposed a doubly-recurrent neural network that", "startOffset": 0, "endOffset": 34}, {"referenceID": 0, "context": "Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences.", "startOffset": 0, "endOffset": 28}, {"referenceID": 4, "context": "We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011).", "startOffset": 95, "endOffset": 134}, {"referenceID": 8, "context": "We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011).", "startOffset": 95, "endOffset": 134}, {"referenceID": 3, "context": "We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time.", "startOffset": 26, "endOffset": 46}, {"referenceID": 5, "context": "It is often built as an attention-based encoder-decoder network (Cho et al., 2015) with two recurrent networks\u2014encoder and decoder\u2014and an attention model.", "startOffset": 64, "endOffset": 82}, {"referenceID": 21, "context": "The attention model first compares the current hidden state sj against each of the hidden states and assigns a scalar score: \u03b2i,j = exp(hi Wdsj) (Luong et al., 2015).", "startOffset": 145, "endOffset": 165}, {"referenceID": 9, "context": "where rd and rp are the corresponding vectors of the parent and dependent phrases, respectively (Dyer et al., 2015).", "startOffset": 96, "endOffset": 115}, {"referenceID": 10, "context": "Note that the original paper of RNNG (Dyer et al., 2016) uses constituency trees, but we employ dependency trees in this paper.", "startOffset": 37, "endOffset": 56}, {"referenceID": 13, "context": "This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al.", "startOffset": 129, "endOffset": 150}, {"referenceID": 23, "context": ", 2015) and distant supervision (Mintz et al., 2009).", "startOffset": 32, "endOffset": 52}, {"referenceID": 3, "context": "Specifically, we use SyntaxNet, released by Andor et al. (2016), on a target sentence.", "startOffset": 44, "endOffset": 64}, {"referenceID": 24, "context": "We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT\u201916 (WAT, 2016).", "startOffset": 46, "endOffset": 67}, {"referenceID": 16, "context": "To reduce computational overhead, we use BlackOut (Ji et al., 2015) with 2000 negative samples and \u03b1 =", "startOffset": 50, "endOffset": 67}, {"referenceID": 12, "context": "When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al.", "startOffset": 107, "endOffset": 137}, {"referenceID": 31, "context": "When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016).", "startOffset": 177, "endOffset": 196}, {"referenceID": 17, "context": "The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recommended in J\u00f3zefowicz et al. (2015). We use stochastic gradient descent with minibatches of 128 examples.", "startOffset": 87, "endOffset": 112}, {"referenceID": 26, "context": "We clip the norm of the gradient (Pascanu et al., 2012) with the threshold set to 3.", "startOffset": 33, "endOffset": 55}, {"referenceID": 18, "context": "We use the bootstrap resampling method from Koehn (2004) to compute the statistical significance.", "startOffset": 44, "endOffset": 57}, {"referenceID": 25, "context": "We report both BLEU (Papineni et al., 2002) and", "startOffset": 20, "endOffset": 43}, {"referenceID": 15, "context": "RIBES (Isozaki et al., 2010).", "startOffset": 6, "endOffset": 28}, {"referenceID": 19, "context": "Removing the stack had the most adverse effect, which was found to be the case for parsing as well by Kuncoro et al. (2017).", "startOffset": 102, "endOffset": 124}], "year": 2017, "abstractText": "There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.", "creator": "LaTeX with hyperref package"}}}