{"id": "1702.03812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Reservoir Computing Using Non-Uniform Binary Cellular Automata", "abstract": "the reservoir computing ( rc ) paradigm utilizes a dynamical system, i. and e., a reservoir, and a linear classifier, i. e., a read - out spectral layer, to process data from sequential functional classification tasks. in this paper wherein the usage of cellular automata ( ca ) installed as a reservoir is investigated. the use of ca simulator in rc has been showing promising results. in this paper, selected state - of - the - art experiments are reproduced. it is shown that some ca - rules perform better than others, and the reservoir performance is improved by increasing the size of the ca individual reservoir itself. in addition, the usage of parallel loosely coupled ca - reservoirs, where each reservoir has a different ca - rule, is investigated. the experiments performed on quasi - uniform ca reservoir provide valuable insights in ca reservoir design. the results herein show caution that some rules don't work well together, while other combinations work remarkably well. this suggests that non - uniform ca could represent a powerful tool for novel ca, reservoir ecology implementations.", "histories": [["v1", "Mon, 13 Feb 2017 15:23:06 GMT  (1660kb,D)", "http://arxiv.org/abs/1702.03812v1", null]], "reviews": [], "SUBJECTS": "cs.ET cs.AI", "authors": ["stefano nichele", "magnus s gundersen"], "accepted": false, "id": "1702.03812"}, "pdf": {"name": "1702.03812.pdf", "metadata": {"source": "CRF", "title": "Reservoir Computing Using Non-Uniform Binary Cellular Automata", "authors": ["Stefano Nichele", "Magnus S. Gundersen"], "emails": ["stefano.nichele@hioa.no", "magnugun@stud.ntnu.no"], "sections": [{"heading": null, "text": "Keywords\u2014Reservoir Computing, Cellular Automata, Parallel Reservoir, Recurrent Neural Networks, Non-Uniform Cellular Automata.\nI. INTRODUCTION\nReal life problems often require processing of time-series data. Systems that process such data must remember inputs from previous time-steps in order to make correct predictions in future time-step, i.e, they must have some sort of memory. Recurrent Neural Networks (RNN) have been shown to possess such memory [11].\nUnfortunately, training RNNs using traditional methods, i.e., gradient descent, is difficult [2]. A fairly novel approach called Reservoir Computing (RC) has been proposed [13], [20] to mitigate this problem. RC splits the RNN into two parts; the non-trained recurrent part, i.e., a reservoir, and the trainable feed-forward part, i.e. a read-out layer.\nIn this paper, an RC-system is investigated, and a computational model called Cellular Automata (CA) [26] is used as the reservoir. This approach to RC was proposed in [30], and further studied in [31], [5], and [19]. The term ReCA is used as an abbreviation for \u201dReservoir Computing using Cellular Automata\u201d, and is adopted from the latter paper.\nIn this paper a fully functional ReCA system is implemented and extended into a parallel CA reservoir system (loosely coupled). Various configurations of parallel reservoir are tested, and compared to the results of a single-reservoir system. This approach is discussed and insights of different configurations of CA-reservoirs are given."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Reservoir Computing", "text": "Feed-forward Neural Networks (NNs) are neural network models without feedback-connections, i.e. they are not aware of their own outputs [11]. They have gained popularity because of their ability to be trained to solve classification tasks. Examples include image classification [25], or playing the board game GO [22]. However, when trying to solve problems that include sequential data, such as sentence-analysis, they often fall short [11]. For example, sentences may have different lengths, and the important parts may be spatially separated even for sentences with equal semantics. Recurrent Neural Networks (RNNs) can overcome this problem [11], being able to process sequential data through memory of previous inputs which are remembered by the network. This is done by relieving the neural network of the constraint of not having feedback-connections. However, networks with recurrent connections are notoriously difficult to train by using traditional methods [2].\nReservoir Computing (RC) is a paradigm in machine learning that combines the powerful dynamics of an RNN with the trainability of a feed-forward neural network. The first\nar X\niv :1\n70 2.\n03 81\n2v 1\n[ cs\n.E T\n] 1\n3 Fe\nb 20\n17\npart of an RC-system consists of an untrained RNN, called reservoir. This reservoir is connected to a trained feed-forward neural network, called readout-layer. This setup can be seen in fig. 1\nThe field of RC has been proposed independently by two approaches, namely Echo State Networks (ESN) [13] and Liquid State Machines (LSM) [20]. By examining these approaches, important properties of reservoirs are outlined.\nPerhaps the most important feature is the Echo state property [13]. Previous inputs \u201decho\u201d through the reservoir for a given number of time steps after the input has occurred, and thereby slowly disappearing without being amplified. This property is achieved in traditional RC-approaches by clever reservoir design. In the case of ESN, this is achieved by scaling of the connection weights of the recurrent nodes in the reservoir [18].\nAs discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].\nB. Various RC-approaches\nDifferent RC-approaches use reservoir substrates that exhibit the desired properties. In [8] an actual bucket of water is implemented as a reservoir for speech-recognition, and in [15] the E.coli-bacteria is used as a reservoir. In [24] and more recently in [4], the usage of Random Boolean Networks (RBN) reservoirs is explored. RBNs can be considered as an abstraction of CA [9], and is thereby a related approach to the one presented in this paper."}, {"heading": "C. Cellular Automata", "text": "Cellular Automaton (CA) is a computational model, first proposed by Ulam and von Neumann in the 1940s [26]. It is a complex, decentralized and highly parallel system, in which computations may emerge [23] through local interactions and without any form of centralized control. Some CA have been proved to be Turing complete [7], i.e. having all properties required for computation; that is transmission, storage and modification of information [16].\nA CA usually consists of a grid of cells, each cell with a current state. The state of a cell is determined by the updatefunction f , which is a function of the neighboring states n. This update-function is applied to the CA for a given number of iterations. These neighbors are defined as a number of cells in the immediate vicinity of the cell itself.\nIn this paper, only one-dimensional elementary CA is used. This means that the CA only consists of a one-dimensional vector of cells, named A, each cell with state S \u2208 {0, 1}. In all the figures in this paper, S = 0 is shown as white, while S = 1 is shown as black. The cells have three neighbors; the cell to the left, itself, and the cell to the right. A cell is a neighbor of itself by convention. The boundary conditions at each end of the 1D-vector is usually solved by wrap-around, where the leftmost cell becomes a neighbor of the rightmost, and vice versa.\nThe update-function f , hereafter denoted rule Z, works accordingly by taking three binary inputs, and outputting one\nbinary value. This results in 28 = 256 different rules. An example of such a rule is shown in fig. 2, where rule 110 is depicted. The numbering of the rules follows the naming convention described by Wolfram [29], where the resulting binary string is converted to a base 10 number. The CA is usually updated in synchronous steps, where all the cells in the 1D-vector are updated at the same time. One update is called an iteration, and the total number of iterations is denoted by I .\nThe rules may be divided into four qualitative classes [29], that exhibit different properties when evolved; class I: evolves to a static state, class II: evolves to a periodic structure, class III: evolves to chaotic patterns and class IV: evolves to complex patterns. Class I and II rules will fall into an attractor after a short while [16], and behave orderly. Class III rules are chaotic, which means that the organization quickly descends into randomness. Class IV rules are the most interesting ones, as they reside at a phase transition between the chaotic and ordered phase, i.e., at the edge of chaos [16]. In uniform CA, all cells share the same rule, whether non-uniform CA cells are governed by different rules. Quasi-uniform CA are nonuniform with a small number of diverse rules."}, {"heading": "D. Cellular automata in reservoir computing", "text": "As proposed in [30], CA may be used as reservoir of dynamical systems. The conceptual overview is shown in fig. 3. Such system is referred to as ReCA in [19], and the same name is therefore adopted in this paper. The projection of the input to the CA-reservoir can be done in two different ways [30]. If the input is binary, the projection is straightforward, where each feature dimension of the input is mapped to a cell. If the input is non-binary, the projection can be done by a weighted summation from the input to each cell. See [31] for more details.\nThe time-evolution of the reservoir can be represented as follows:\nA1 = Z(A0)\nA2 = Z(A1)\n...\nAI = Z(AI\u22121)\nWhere Am is the state of the 1D CA at iteration m and Z is the CA-rule that was applied. A0 is the initial state of the CA, often an external input, as discussed later.\nAs discussed in section II-A, a reservoir often operates at the edge of chaos [10]. Selecting CA-based reservoirs that exhibit this property is trivial, as rules that lie inside Wolfram class IV can provide this property. Additionally, to fully exploit\nsuch property, all I iterations of a the CA evolution are used for classification, and this can be stated as follows:\nA = [A1;A2; ...AI ]\nWhere A is used for classification.\nThe ReCA system must also exhibit the echo state property, as described in section II-A. This is done by allowing the CA to take external input, while still remembering the current state. As descibed in more details later, ReCA-systems address this issue by using some time-transition function, named F, which allows some previous inputs to echo through the CA.\nCA also provide additional advantages to RC. In [31] a speedup of 1.5-3X in the number of operations compared to the ESN [14] approach is reported. This is mainly due to a CA relying on bit-wise operations, while ESN uses floating point operations. This can be additionally exploited by utilizing custom made hardware like FPGAs. In addition, if edge-ofchaos rules are selected, Turing complete computational power is present in the reservoir. CA theoretical analysis is easier than RNNs, and they allow Boolean logic and Galois field algebra."}, {"heading": "E. ReCA system implementations", "text": "ReCA systems are a very novel concept and therefore there are only few implemented examples at the current stage of research. Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6]. Bye [5] also demonstrated a functioning ReCA-system in his master\u2019s thesis (supervised by Nichele). The used approaches are similar, however, there are some key differences:\n1) Encoding and random mappings: In the encoding stage, [31] used random permutations over the same input-vector. This encoding scheme can be seen in fig. 4. The permutation procedure is repeated R number of times, because it was experimentally observed that multiple random mappings improve performance.\nIn [5] a similar approach was used. The main difference is that the input is mapped to a vector that is larger than the input-vector itself. The size of this mapping-vector is given by a parameter \u201dautomaton size\u201d. This approach can be seen in fig. 5. The input-bits are randomly mapped to one of the bits in the mapping-vector. The ones that do not have any mapping to them are left to zero.\nIn the work herein, the approach described in [5] is used, but with a modification. Instead of using the automaton sizeparameter, the C-parameter is introduced. The total length of the permutation is given by the number C multiplied by the length of the input-vector. In the case of fig. 5, the automation size would be 8, and C would be 2.\n2) Feed-forward or recurrent: [31] proposed both a feedforward and a recurrent design. The difference was whether the whole input-sequence is presented to the system in one chunk or step-by-step. [5] only described a recurrent design. Only the recurrent architectures will be investigated in this paper. This is because it is more in line with traditional RNNs and RC-systems, and is conceptually more biologically plausible.\n3) Concatenation of the encoded inputs before propagating into the reservoir: After random mappings have been created, there is another difference in the proposed approaches. In the recurrent architecture, [31] concatenates the R number of permutations into one large vector of length (R \u2217 input length) before propagating it in a reservoir of the same width as this vector. The 1D input-vector at time-step t can be expressed as follows:\nXPt = [X P1 t ;X P2 t ;X P3 t ; ...X PR t ]\nXPt is inserted into the reservoir as described in section II-D, and then iterated I times. The iterations are then concatenated into the vector At, which is used for classification at time-step t.\nAt = [A1;A2; ...AI ]\n[5] adapted a different approach, the same one that was also used by the feed-forward architecture in [31], where the R different permutations are iterated in separate reservoirs, and the different reservoirs are then concatenated before they are used by the classifier. The vector which is used for classification at time-step t is as follows:\nAt = [AtP1 ;A t P2 ; ...A t PR ]\nWhere AtPn is the vector from the concatenated reservoir. In this paper, the recurrent architecture approach is used.\n4) Time-transition: In order to allow the system to remember previous inputs, a time-transition function is needed to translate between the current time-step and the next. One possibility is to use normalized addition as time-transition function, as shown in fig. 6, with F as normalized addition. This function works as follows: The cell values are added, and if the sum is 2 (1+1) the output-value becomes 1, if the sum is 0, the output-value becomes 0 and if the sum is 1, the cell-value is decided randomly (0 or 1). The initial 1D-CAvector of the reservoir at time-step t is then expressed as:\nA0 = F (Xt, AI t\u22121), t > 0\nWhere F may be any bit-wise operation, Xt is the input from the sequential task at time-step t, and AI t\u22121 is the last iteration of the previous time-step. At the first time-step (t=0), the transition-function is bypassed, and the input Xt is used directly in the reservoir.\nAnother possibility is to use \u201dpermutation transition\u201d as time-transition function, as seen in fig. 7. Here, all cells that have a mapping to them (from the encoder) are bit-wise filled with the value of input-vector X . If the cells do not have any mapping to them, the values from AI t\u22121 are inserted. This allows the CA to have memory across time-steps in sequential tasks. By adjusting the automaton-size, or C-parameter, the interaction between each time-step can be regulated.\nThe described approaches have different effects on the parameters R and I, and also the resulting size of the reservoir.\nThis is relevant when discussing the computational complexity of ReCA systems.\nIn this paper, the \u201dpermutation transition\u201d is used."}, {"heading": "III. EXPERIMENTAL SETUP", "text": "The basic architecture implemented in this paper is shown in fig. 9. The encoder is based on the architecture described in [5]. In this paper, the parameter C is introduced as a metric on how large resulting mapping-vector should be. The concatenation procedure is adapted from [31]. The vectors, after the encoding (random mappings), are concatenated into one large vector. This vector is then propagated into the reservoir, as described in section II-E3. The time-transition function is adapted from [5]. The mappings from the encoder are saved, and used as a basis where new inputs are mapped to, as described in section II-E4. The values from the last step in the previous time-step are directly copied. The classifier used in this paper is a Support Vector Machine, as implemented in the Python machine learning framework scikit-learn [21]. The code-base that was used in this paper is available for download [1].\nAn example run with rule 90 is shown in fig. 8. This visualisation gives valuable insights in how the reservoir behaves when parameters are changed, and makes it easier to understand the reservoir dynamics. Most natural systems come in the form of a temporal system (sequential), i.e., an input to the system depends on previous inputs. Classical feed-forward architectures are known to have issues with temporal tasks [11]. In order to test the ReCA-system at a temporal task, the 5-bit task [12] is chosen in this paper. Such task has become a popular and widely used benchmark for reservoir computing, in particular because it tests the long-short-term memory of the system. An example data set from this task is presented in fig. 10. The length of the sequence is given by T . a1, a2, a3 and a4 are the input-signals, and y1, y2 and y3 are the outputsignals. At each time-step t only one input-signal, and one output-signal, can have the value 1. The values of a1 and a2 at the first five time-steps give the pattern that the system shall learn. The next Td time-steps represent the distractor-period, where the system is distracted from the previous inputs. This is done by setting the value of a3 to 1. After the disctractor period, the a4 signal is fired which marks the cue-signal. The system is then asked to repeat the input-pattern on the outputs y1 and y2. The output y3 is a waiting signal, which is supposed to be 1 right until the input-pattern is repeated. More details on the 5-bit memory task can be found in [14]."}, {"heading": "A. Use of parallel CA-reservoirs in RC", "text": "In this paper the use of parallel reservoirs is proposed. The concept is shown in fig. 11. At the boundary conditions, i.e. the cell at the very end of the reservoir, the rule will treat the cell that lies within the other reservoir, as a cell in its own reservoir. This causes information/computation to flow between the reservoirs (loosely coupled).\nBy having different rules in the reservoirs, one might be able to solve different aspects of the same problem, or even two problems at the same time. In [5], both the temporal parity and the temporal density task [14] are investigated.\nWhich rule is most suited for a task is still an open research question. The characteristics and classes described in section II-C are useful knowledge, however it does not precisely describe why some rules perform better than others on different tasks. In fig. 12 an example run of the parallel system is showed, with rule 90 on the left, and 182 on the right. This visualization gives useful insights on how the rules interact."}, {"heading": "B. Measuring computational complexity of a CA-reservoir", "text": "The size of the reservoir is crucial for the success of the system. In this paper, the reservoir size is measured by R \u2217 I \u2217C. As seen in section III-A, the size of the reservoirs will remain the same both for the one-rule reservoirs and the tworule reservoirs. This is crucial in order to be able to directly compare their performances."}, {"heading": "IV. RESULTS", "text": "The parameters for the used 5-bit memory task can be seen in table I. The same parameters as in the single-reservoir system are used in the quasi-uniform CA reservoir system with\na combination of two rules. The tested combinations of rules are shown in table II."}, {"heading": "A. Results from the single ReCA-system", "text": "The results from the single reservoir ReCA-system can be seen in table III. The results in this paper are significantly better than what was reported in [5]. We can however see a similar trend. Rules 102 and 105 were able to give promising results, while rule 180 was not very well suited for this task. An exception is rule 90 and 165, where the results in table III show very high accuracy. In [31] very promising results from rule 90 are also achieved."}, {"heading": "B. Results from the parallel (non-uniform) ReCA-system", "text": "Results can be seen in table IV. It can be observed that combination of rules that were performing well in table III seem to give good results when combined. However, some combination of rules, e.g., 60 and 102, 153 and 195, gave worse results than the rules by themselves. We can observe the same tendencies as in the single-runs; higher R and I generally yields better results."}, {"heading": "V. ANALYSIS", "text": ""}, {"heading": "A. Single reservoir ReCA-system", "text": "The complexity of the reservoir is a useful metric when comparing different approaches. If we examine rule 90, we can observe that it achieves 100% success rate at I = 4, R = 8 and C = 10. The size of the reservoir is 4 \u2217 8 \u2217 10 = 320 at this configuration. Note that even though lower values of R and I also give 100%, at R = 4 and I = 4 the success is 97.5%, yet again 100% at I = 4 and R = 8. [31] reported a 100% success-rate on the same task with R = 32 and I = 16. The C-parameter was set to 1. As such, the size of the reservoir is 32 \u2217 16 \u2217 1 = 512 (feed-forward architecture).\n[31] also presented results on the 5-bit task using the recurrent architecture. 100% success-rate was achieved with I = 32\nand R = 45. This yields a reservoir size of 32 \u2217 45 = 1440. Those results were intended to study the relationship between the distractor period of the 5-bit task, and the R number of random mappings. The I was kept fixed at 32 during this experiment. Even if the motivation for the experiments were different, the comparison of results gives insight that the reservoir size itself may not be the only factor that determines the performance of the ReCA system."}, {"heading": "B. Parallel reservoir (non-uniform) ReCA-system", "text": "Why are some combinations better than others? As observed in section IV-B, rules that are paired with others rules that perform well on their own, also perform well together. The combination of rule 90 and rule 165 is observed to be very successful. As described in [28], rule 165 is the complement of rule 90. If we observe the single-CA results in table III we can see that rule 90 and 165 perform very similarly.\nExamining one of the worst-performing rule-combinations\nof the experiments, i.e., rule 153 and rule 195, we get some useful insight as seen in fig. 13. Here it is possible to notice that the interaction of rules creates a \u201dblack\u201d region in the middle (between the rules), thereby effectively reducing the size of the reservoir. As described in [27], rule 153 and 192 are the mirrored complements.\nRule 105 is an interesting rule to be combined with others. As described in [29], the rule does not have any compliments or any mirrored compliments. Nevertheless, as seen in table IV-B, it performs well in combination with most other rules."}, {"heading": "VI. CONCLUSION", "text": "A framework for using cellular automata in reservoir computing has been implemented, which makes use of uniform CA and quasi-uniform CA. Relationship between reservoir size and performances of the system are presented. The implemented configuration using parallel CA reservoir is tested in this paper for the first time (to the best of the authors\u2019 knowledge). Results have shown that some CA rules work better in combination than other. Good combinations tend to have some relation, e.g. being complementary. Rules that are mirrored compliments do not work well together, because they effectively reduce the size of the reservoir. The concept is still very novel, and a lot of research is left to be done, both regarding the use of non-uniform CA reservoir, as well as ReCA-systems in general.\nAs previously discussed, finding the best combination of rules is not trivial. If we only consider the usage of two distinct rules, the rule space grows from only 256 singlereservoir options to 256!2!\u2217254! = 32640 different combinations. Matching two rules that perform well together can be quite a challenge. By investigating the characteristics of the rules, e.g., with lambda-parameter [16], Lyapunov exponent [17] or other metrics, it may be possible to pinpoint promising rules.\nIdeally, the usage of more than two different rules could prove a powerful tool. The rule space would then grow even larger, and an exhaustive search would be infeasible. However, one possibility would be to use evolutionary algorithms to search for suitable rules. Adding more and more rules would bring the reservoir closer to a true non-uniform CA.\nIn [14] a wide range of different tasks is presented. In this paper only one (5-bit task) is used as a benchmark. By combining different rules\u2019 computational power, one could design a reservoir that performs well on a variety of tasks."}], "references": [{"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["Nils Bertschinger", "Thomas Natschl\u00e4ger"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Exploring physical reservoir computing using random boolean networks", "author": ["Aleksander Vognild Burkow"], "venue": "Master\u2019s thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Investigation of elementary cellular automata for reservoir computing", "author": ["Emil Taylor Bye"], "venue": "Master\u2019s thesis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "The game of life", "author": ["John Conway"], "venue": "Scientific American,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "Universality in elementary cellular automata", "author": ["Matthew Cook"], "venue": "Complex systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Pattern recognition in a bucket", "author": ["Chrisantha Fernando", "Sampsa Sojakka"], "venue": "In European Conference on Artificial Life,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Introduction to random boolean networks", "author": ["Carlos Gershenson"], "venue": "arXiv preprint nlin/0408006,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Unifying quality metrics for reservoir networks", "author": ["Thomas E Gibbons"], "venue": "In Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["Herbert Jaeger"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Is there a liquid state machine in the bacterium escherichia coli", "author": ["Ben Jones", "Dov Stekel", "Jon Rowe", "Chrisantha Fernando"], "venue": "IEEE Symposium on Artificial Life,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Computation at the edge of chaos: phase transitions and emergent computation", "author": ["Chris G Langton"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Edge of chaos and prediction of computational performance for neural circuit models", "author": ["Robert Legenstein", "Wolfgang Maass"], "venue": "Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Reservoir computing trends", "author": ["Mantas Luko\u0161evi\u010dius", "Herbert Jaeger", "Benjamin Schrauwen"], "venue": "KI-Ku\u0308nstliche Intelligenz,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "An experimental study on cellular automata reservoir in pathological sequence learning", "author": ["Mrwan Margem", "Ozg\u00fcr Yilmaz"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "The\u201d liquid computer\u201d: A novel strategy for real-time computing on time series", "author": ["Thomas Natschl\u00e4ger", "Wolfgang Maass", "Henry Markram"], "venue": "Special issue on Foundations of Information Processing of Telematik,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "The emergence of cellular computing", "author": ["Moshe Sipper"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Computational capabilities of random automata networks for reservoir computing", "author": ["David Snyder", "Alireza Goudarzi", "Christof Teuscher"], "venue": "Physical Review E,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Theory of self-reproducing automata", "author": ["John Von Neumann", "Arthur W Burks"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1966}, {"title": "rule 60.\u201d from mathworld \u2013 a wolfram web resource. http://mathworld.wolfram.com/Rule60.html", "author": ["Eric W Weisstein"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "rule 90.\u201d from mathworld \u2013 a wolfram web resource. http://mathworld.wolfram.com/Rule90.html", "author": ["Eric W Weisstein"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "A new kind of science, volume 5", "author": ["Stephen Wolfram"], "venue": "Wolfram media Champaign,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Reservoir computing using cellular automata", "author": ["Ozgur Yilmaz"], "venue": "arXiv preprint arXiv:1410.0162,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Connectionist-symbolic machine intelligence using cellular automata based reservoir-hyperdimensional computing", "author": ["Ozgur Yilmaz"], "venue": "arXiv preprint arXiv:1503.00851,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent Neural Networks (RNN) have been shown to possess such memory [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": ", gradient descent, is difficult [2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "A fairly novel approach called Reservoir Computing (RC) has been proposed [13], [20] to mitigate this problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "A fairly novel approach called Reservoir Computing (RC) has been proposed [13], [20] to mitigate this problem.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "In this paper, an RC-system is investigated, and a computational model called Cellular Automata (CA) [26] is used as the reservoir.", "startOffset": 101, "endOffset": 105}, {"referenceID": 28, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "they are not aware of their own outputs [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "Examples include image classification [25], or playing the board game GO [22].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "Examples include image classification [25], or playing the board game GO [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "However, when trying to solve problems that include sequential data, such as sentence-analysis, they often fall short [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) can overcome this problem [11], being able to process sequential data through memory of previous inputs which are remembered by the network.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "However, networks with recurrent connections are notoriously difficult to train by using traditional methods [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 11, "context": "The field of RC has been proposed independently by two approaches, namely Echo State Networks (ESN) [13] and Liquid State Machines (LSM) [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "The field of RC has been proposed independently by two approaches, namely Echo State Networks (ESN) [13] and Liquid State Machines (LSM) [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Perhaps the most important feature is the Echo state property [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "In the case of ESN, this is achieved by scaling of the connection weights of the recurrent nodes in the reservoir [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "As discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "As discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "As discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "In [8] an actual bucket of water is implemented as a reservoir for speech-recognition, and in [15] the E.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In [8] an actual bucket of water is implemented as a reservoir for speech-recognition, and in [15] the E.", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "In [24] and more recently in [4], the usage of Random Boolean Networks (RBN) reservoirs is explored.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [24] and more recently in [4], the usage of Random Boolean Networks (RBN) reservoirs is explored.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "RBNs can be considered as an abstraction of CA [9], and is thereby a related approach to the one presented in this paper.", "startOffset": 47, "endOffset": 50}, {"referenceID": 24, "context": "Cellular Automaton (CA) is a computational model, first proposed by Ulam and von Neumann in the 1940s [26].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "It is a complex, decentralized and highly parallel system, in which computations may emerge [23] through local interactions and without any form of centralized control.", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "Some CA have been proved to be Turing complete [7], i.", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "having all properties required for computation; that is transmission, storage and modification of information [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "The numbering of the rules follows the naming convention described by Wolfram [29], where the resulting binary string is converted to a base 10 number.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "The rules may be divided into four qualitative classes [29], that exhibit different properties when evolved; class I: evolves to a static state, class II: evolves to a periodic structure, class III: evolves to chaotic patterns and class IV: evolves to complex patterns.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Class I and II rules will fall into an attractor after a short while [16], and behave orderly.", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": ", at the edge of chaos [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "As proposed in [30], CA may be used as reservoir of dynamical systems.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Such system is referred to as ReCA in [19], and the same name is therefore adopted in this paper.", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": "The projection of the input to the CA-reservoir can be done in two different ways [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "See [31] for more details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "As discussed in section II-A, a reservoir often operates at the edge of chaos [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "Figure adapted from [31]", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "In [31] a speedup of 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "5-3X in the number of operations compared to the ESN [14] approach is reported.", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6].", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6].", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Bye [5] also demonstrated a functioning ReCA-system in his master\u2019s thesis (supervised by Nichele).", "startOffset": 4, "endOffset": 7}, {"referenceID": 29, "context": "1) Encoding and random mappings: In the encoding stage, [31] used random permutations over the same input-vector.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "The encoding used in [31].", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "The encoding used in [5].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "In [5] a similar approach was used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In the work herein, the approach described in [5] is used, but with a modification.", "startOffset": 46, "endOffset": 49}, {"referenceID": 29, "context": "2) Feed-forward or recurrent: [31] proposed both a feedforward and a recurrent design.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "[5] only described a recurrent design.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "In the recurrent architecture, [31] concatenates the R number of permutations into one large vector of length (R \u2217 input length) before propagating it in a reservoir of the same width as this vector.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Time transition used in [31].", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "[5] adapted a different approach, the same one that was also used by the feed-forward architecture in [31], where the R different permutations are iterated in separate reservoirs, and the different reservoirs are then concatenated before they are used by the classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[5] adapted a different approach, the same one that was also used by the feed-forward architecture in [31], where the R different permutations are iterated in separate reservoirs, and the different reservoirs are then concatenated before they are used by the classifier.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The encoder is based on the architecture described in [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 29, "context": "The concatenation procedure is adapted from [31].", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "The time-transition function is adapted from [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "The classifier used in this paper is a Support Vector Machine, as implemented in the Python machine learning framework scikit-learn [21].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "Classical feed-forward architectures are known to have issues with temporal tasks [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "In order to test the ReCA-system at a temporal task, the 5-bit task [12] is chosen in this paper.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "More details on the 5-bit memory task can be found in [14].", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "In [5], both the temporal parity and the temporal density task [14] are investigated.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [5], both the temporal parity and the temporal density task [14] are investigated.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "The results in this paper are significantly better than what was reported in [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 29, "context": "In [31] very promising results from rule 90 are also achieved.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "[31] reported a 100% success-rate on the same task with R = 32 and I = 16.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] also presented results on the 5-bit task using the recurrent architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "As described in [28], rule 165 is the complement of rule 90.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "As described in [27], rule 153 and 192 are the mirrored complements.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "As described in [29], the rule does not have any compliments or any mirrored compliments.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": ", with lambda-parameter [16], Lyapunov exponent [17] or other metrics, it may be possible to pinpoint promising rules.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": ", with lambda-parameter [16], Lyapunov exponent [17] or other metrics, it may be possible to pinpoint promising rules.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "In [14] a wide range of different tasks is presented.", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "The Reservoir Computing (RC) paradigm utilizes a dynamical system, i.e., a reservoir, and a linear classifier, i.e., a read-out layer, to process data from sequential classification tasks. In this paper the usage of Cellular Automata (CA) as a reservoir is investigated. The use of CA in RC has been showing promising results. In this paper, selected state-of-the-art experiments are reproduced. It is shown that some CA-rules perform better than others, and the reservoir performance is improved by increasing the size of the CA reservoir itself. In addition, the usage of parallel loosely coupled CA-reservoirs, where each reservoir has a different CA-rule, is investigated. The experiments performed on quasi-uniform CA reservoir provide valuable insights in CAreservoir design. The results herein show that some rules do not work well together, while other combinations work remarkably well. This suggests that non-uniform CA could represent a powerful tool for novel CA reservoir implementations. Keywords\u2014Reservoir Computing, Cellular Automata, Parallel Reservoir, Recurrent Neural Networks, Non-Uniform Cellular Automata.", "creator": "LaTeX with hyperref package"}}}