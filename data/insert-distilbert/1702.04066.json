{"id": "1702.04066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "abstract": "we present a new parallel corpus, jhu fluency - extended gug corpus ( jfleg ) for developing and evaluating grammatical error correction ( gec ). unlike other corpora, it represents a new broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more reliable native sounding. we describe broadly the 25 types of corrections made and benchmark four dozen leading gec systems on this corpus, identifying specific areas broadly in which they do well and how they can improve. arabic jfleg fulfills the complex need for a new gold arabic standard to properly assess the current state of language gec.", "histories": [["v1", "Tue, 14 Feb 2017 03:47:34 GMT  (56kb,D)", "http://arxiv.org/abs/1702.04066v1", "To appear in EACL 2017 (short papers)"]], "COMMENTS": "To appear in EACL 2017 (short papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["courtney napoles", "keisuke sakaguchi", "joel tetreault"], "accepted": false, "id": "1702.04066"}, "pdf": {"name": "1702.04066.pdf", "metadata": {"source": "CRF", "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault"], "emails": ["napoles@cs.jhu.edu,", "keisuke@cs.jhu.edu,", "joel.tetreault@grammarly.com"], "sections": [{"heading": "1 Introduction", "text": "Automatic grammatical error correction (GEC) progress is limited by the corpora available for developing and evaluating systems. Following the release of the test set of the CoNLL\u20132014 Shared Task on GEC (Ng et al., 2014), systems have been compared and new evaluation techniques proposed on this single dataset. This corpus has enabled substantial advancement in GEC beyond the shared tasks, but we are concerned that the field is over-developing on this dataset. This is problematic for two reasons: 1) it represents one specific population of language learners; and 2) the corpus only contains minimal edits, which correct the grammaticality of a sentence but do not necessarily make it fluent or native-sounding.\nTo illustrate the need for fluency edits, consider the example in Table 1. The correction with only minimal edits is grammatical but sounds awkward (unnatural to native speakers). The fluency correction has more extensive changes beyond addressing grammaticality, and the resulting sen-\ntence sounds more natural and its intended meaning is more clear. It is not unrealistic to expect these changes from automatic GEC: the current best systems use machine translation (MT) and are therefore capable of making broader sentential rewrites but, until now, there has not been a gold standard against which they could be evaluated.\nFollowing the recommendations of Sakaguchi et al. (2016), we release a new corpus for GEC, the JHU FLuency-Extended GUG corpus (JFLEG), which adds a layer of annotation to the GUG corpus (Heilman et al., 2014). GUG represents a cross-section of ungrammatical data, containing sentences written by English language learners with different L1s and proficiency levels. For each of 1,511 GUG sentences, we have collected four human-written corrections which contain holistic fluency rewrites instead of just minimal edits. This corpus represents the diversity of edits that GEC needs to handle and sets a gold standard to which the field should aim. We overview the current state of GEC by evaluating the performance of four leading systems on this new dataset. We analyze the edits made in JFLEG and summarize which types of changes the systems successfully make, and which they need to address. JFLEG will enable the field to move beyond minimal error corrections."}, {"heading": "2 GEC corpora", "text": "There are four publicly available corpora of nonnative English annotated with corrections, to our\nar X\niv :1\n70 2.\n04 06\n6v 1\n[ cs\n.C L\n] 1\n4 Fe\nb 20\n17\nknowledge. The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). The CoNLL Shared Tasks used this data (Ng et al., 2014; Ng et al., 2013), and the 1,312 sentence test set from the 2014 task has become de rigueur for benchmarking GEC. This test set has been augmented with ten additional annotations from Bryant et al. (2015) and eight from Sakaguchi et al. (2016). The Cambridge Learner Corpus First Certificate in English (FCE) has essays coded by one rater using about 80 error types, alongside the score and demographic information (Yannakoudakis et al., 2011). The Lang-8 corpus of learner English is the largest, with text from the social platform lang-8.com automatically aligned to user-provided corrections (Tajiri et al., 2012). Unlimited annotations are allowed per sentence, but 87% were corrected once and 12% twice. The AESW 2016 Shared Task corpus contains text from scientific journals corrected by a single editor. To our knowledge, AESW is the only corpus that has not been used to develop a GEC system.\nWe consider NUCLE1 and FCE to contain minimal edits, since the edits were constrained by error codes, and the others to contain fluency edits since there were no such restrictions. English proficiency levels vary across corpora: FCE and NUCLE texts were written by English language learners with relatively high proficiency, but Lang8 is open to any internet user. AESW has technical writing by the most highly proficient English writers. Roughly the same percent of sentences from each corpus is corrected, except for FCE which has significantly more. This may be due to the rigor of the annotators and not the writing quality.\nThe following section introduces the JFLEG corpus, which represents a diversity of potential corrections with four corrections of each sentence. Unlike NUCLE and FCE, JFLEG does not restrict corrections to minimal error spans, nor are the er-\n1Not including the additional fluency edits collected for the CoNLL-2014 test set by Sakaguchi et al. (2016).\nrors coded. Instead, it contains holistic sentence rewrites, similar to Lang-8 and AESW, but contains more reliable corrections than Lang-8 due to perfect alignments and screened editors, and more extensive corrections than AESW, which contains fewer edits than the other corpora with a mean Levenshtein distance (LD) of 3 characters. Table 2 provides descriptive statistics for the available corpora. JFLEG is also the only corpus that provides corrections alongside sentence-level grammaticality scores of the uncorrected text."}, {"heading": "3 The JFLEG corpus", "text": "Our goal in this work is to create a corpus of fluency edits, following the recommendations of (Sakaguchi et al., 2016), who identify the shortfalls of minimal edits: they artificially restrict the types of changes that can be made to a sentence and do not reflect the types of changes required for native speakers to find sentences fluent, or natural sounding. We collected annotations on a public corpus of ungrammatical text, the GUG (Grammatical/Ungrammatical) corpus (Heilman et al., 2014). GUG contains 3.1k sentences written by English language learners for the TOEFL R\u00a9 exam, covering a range of topics. The original GUG corpus is annotated with grammaticality judgments for each sentence, ranging from 1\u20134, where 4 is perfect or native sounding, and 1 incomprehensible. The sentences were coded by five crowdsourced workers and one expert. We refer to the mean grammaticality judgment of each sentence from the original corpus as the GUG score.\nIn our extension, JFLEG, the 1,511 sentences which comprise the GUG development and test sets were corrected four times each on Amazon Mechanical Turk. Annotation instructions are included in Table 3. 50 participants from the United States passed a qualifying task of correcting five sentences, which was reviewed by the authors (two native and one proficient non-native speakers of American English). Annotators also rated how difficult it was for them to correct each sentence on a 5-level Likert scale (5 being very easy and 1 very difficult). On average, the sentences were relatively facile to correct (mean difficulty of 3.5 \u00b1 1.3), which moderately correlates with the GUG score (Pearson\u2019s r = 0.47), indicating that less grammatical sentences were generally more difficult to correct. To create a blind test set for the community, we withhold half (747) of the sentences from the analysis and evaluation herein.\nThe mean LD between the original and corrected sentences is more than twice that of existing corpora (Table 2). LD negatively correlates with the GUG score (r = \u22120.41) and the annotation difficulty score (\u22120.37), supporting the intuition that less grammatical sentences require more extensive changes, and it is harder to make corrections involving more substantive edits. Because there is no clear way to quantify agreement between annotators, we compare the annotations of each sentence to each other. The mean LD between all pairs of annotations is greater than the mean LD between the original and corrected sentences (15 characters), however 36% of the sentences were corrected identically by at least two participants.\nNext, the English L1 authors examined 100 randomly selected original and human-corrected sentence pairs and labeled them with the type of error present in the sentence and the type of edit(s) applied in the correction. The three error types are sounds awkward or has an orthographic or grammatical error.2 The majority of the original sentences have at least one error (81%), and, for 68% of these sentences, the annotations are error free. Few annotated sentences have orthographic (4%) or grammatical (10%) errors, but awkward errors are more frequent (23% of annotations were labeled awkward)\u2014which is not very surprising given how garbled some original sentences are and the dialectal variation of what sounds awkward.\nThe corrected sentences were also labeled with 2Due to their frequency, we separate orthographic errors\n(spelling and capitalization) from other grammatical errors.\nthe type of changes made (minimal and/or fluency edits). Minimal edits reflect a minor change to a small span (1\u20132 tokens) addressing an immediate grammatical error, such as number agreement, tense, or spelling. Fluency edits are more holistic and include reordering or rewriting a clause, and other changes that involve more than two contiguous tokens. 69% of annotations contain at least one minimal edit, 25% a fluency edit, and 17% both fluency and minimal edits. The distribution of edit types is fairly uniform across the error type present in the original sentence (Table 4). Notably, fewer than half of awkward sentences were corrected with fluency edits, which may explain why so many of the corrections were still awkward."}, {"heading": "4 Evaluation", "text": "To assess the current state of GEC, we collected automated corrections of JFLEG from four leading GEC systems with no modifications. They take different approaches but all use some form of MT. The best system from the CoNLL-2014 Shared Task is a hybrid approach, combining a rule-based system with MT and language-model reranking (CAMB14; Felice et al., 2014). Other systems have been released since then and report improvements on the 2014 Shared Task. They include a neural MT model (CAMB16; Yuan and Briscoe, 2016), a phrase-based MT (PBMT) with sparse features (AMU; Junczys-Dowmunt and Grundkiewicz, 2016), and a hybrid system that incorporates a neural-net adaptation model into PBMT (NUS; Chollampatt et al., 2016).\nWe evaluate system output against the four sets of JFLEG corrections with GLEU, an automatic fluency metric specifically designed for this task (Napoles et al., 2015) and the Max-Match metric (M2) (Dahlmeier and Ng, 2012). GLEU is based on the MT metric BLEU, and represents the n-gram overlap of the output with the humancorrected sentences, penalizing n-grams that were been changed in the human corrections but left unchanged by a system. It was developed to score fluency in addition to minimal edits since it does not require an alignment between the original and corrected sentences. M2 was designed to score minimal edits and was used in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013; Ng et al., 2014). Its score is the F0.5 measure of word and phrase-level changes calculated over a lattice of changes made between the aligned origi-\nnal and corrected sentences. Since both GLEU and M2 have only been evaluated on the CoNLL-2014 test set, we additionally collected human rankings of the outputs to determine whether human judgments of relative grammaticality agree with the metric scores when the reference sentences have fluency edits.\nThe two native English-speaking authors ranked six versions of each of 150 JFLEG sentences: the four system outputs, one randomly selected human correction, and the original sentence. The absolute human ranking of systems was inferred using TrueSkill, which computes a relative score from pairwise comparisons, and we cluster systems with overlapping ranges into equivalence classes by bootstrap resampling (Sakaguchi et al., 2014; Herbrich et al., 2006). The two best ranked systems judged by humans correspond to the two best GLEU systems, but GLEU switches the order of the bottom two. The M2 ranking does not perform as well, reversing the order of the top two systems and the bottom two (Table 5).3 The upper bound is GLEU = 55.3 and M2 = 63.2, the mean metric scores of each human correction compared to the other three. CAMB16 and NUS are halfway to the gold-standard performance measured by GLEU and, according to M2, they achieve approximately 80% of the human performance. The neural methods (CAMB16 and NUS) are substantially better than the other two according to both metrics. This ranking is in contrast to the ranking of systems on the CoNLL-14 shared task test against minimally edited references. On these sentences, AMU, which was tuned to M2, achieves the highest M2 score with 49.5 and CAMB16, which was the best on the fluency corpus, ranks third with 39.9.\nWe find that the extent of changes made in the system output is negatively correlated to the qual-\n3No conclusive recommendation about the best-suited metric for evaluating fluency corrections can be drawn from these results. With only four systems, there is no significant difference between the metric rankings, and even the human rank has no significant difference between three systems.\nity as measured by GLEU (Figure 1). The neural systems have the highest scores for nearly all edit distances, and generate the most sentences with higher LDs. CAMB14 has the most consistent GLEU scores. The AMU scores of sentences with LD > 6 are erratic due to the small number of sentences it outputs with that extent of change."}, {"heading": "5 Qualitative analysis", "text": "We examine the system outputs of the 100 sentences analyzed in Section 3, and label them by the type of errors they contain (Figure 2) and edit types made (Table 6). The system rankings in Table 5 correspond to the rank of systems by the percent of output sentences with errors and the percent of error-ful sentences changed. Humans make significantly more fluency and minimal edits\nthan any of the systems. The models with neural components, CAMB16 followed by NUS, make the most changes and produce fewer sentences with errors. Systems often change only one or two errors in a sentence but fail to address others. Minimal edits are the primary type of edits made by all systems (AMU and CAMB14 made one fluency correction each, NUS two, and CAMB16 five) while humans use fluency edits to correct nearly 30% of the sentences.\nSpelling mistakes are often ignored: AMU corrects very few spelling errors, and even CAMB16, which makes the most corrections, still ignores misspellings in 30% of sentences. Robust spelling correction would make a noticeable difference to output quality. Most systems produce corrections that are meaning preserving, however, CAMB16 changed the meaning of 15 sentences. This is a downside of neural models that should be considered, even though neural MT generates the best output by all other measures.\nThe examples in Table 7 illustrate some of these successes and shortcomings. The first sentence can be corrected with minimal edits, and both AMU and CAMB14 correct the number agreement but leave the incorrect unplanly and the infinitival to. In addition, AMU does not correct the spelling of advertissement or some thing. CAMB16 changes the meaning of the sentence altogether, even though the output is fluent, and NUS makes no changes. The next set of sentences contains many errors and requires inference and fluency rewrites to correct. The human annotator deduces that the last clause is about coaches, not mollusks, and rewrites it grammatically given the context of the rest of the sentence. Systems handle the second clause moderately well but are unable to correct the final clause: only CAMB16 attempts to cor-\nrect it, but the result is nonsensical."}, {"heading": "6 Conclusions", "text": "This paper presents JFLEG, a new corpus for developing and evaluating GEC systems with respect to fluency as well as grammaticality.4 Our hope is that this corpus will serve as a starting point for advancing GEC beyond minimal error corrections. We described qualitative and quantitative analysis of JFLEG, and benchmarked four leading systems on this data. The relative performance of these systems varies considerably when evaluated on a fluency corpus compared to a minimal-edit corpus, underlining the need for a new dataset for evaluating GEC. Overall, current systems can successfully correct closed-class targets such as number agreement and prepositions errors (with incomplete coverage), but ignore many spelling mistakes and long-range context-dependent errors. Neural methods are better than other systems at making fluency edits, but this may be at the expense of maintaining the meaning of the input. As there is still a long way to go in approaching the performance of a human proofreader, these results and benchmark analyses help identify specific issues that GEC systems can improve in future research."}, {"heading": "Acknowledgments", "text": "We are grateful to Benjamin Van Durme for his support in this project. We especially thank the following people for providing their respective system outputs on this new corpus: Roman Grundkiewicz and Marcin Jnuczys-Dowmunt for the AMU system outputs, Mariano Felice for CAMB14, Zheng Yuan for CAMB16, and Shamil Chollampatt and Hwee Tou Ng for NUS. Finally we thank the anonymous reviewers for their feedback.\n4https://github.com/keisks/jfleg"}], "references": [{"title": "How far are we from fully automatic high quality grammatical error correction", "author": ["Christopher Bryant", "Hwee Tou Ng"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-", "citeRegEx": "Bryant and Ng.,? \\Q2015\\E", "shortCiteRegEx": "Bryant and Ng.", "year": 2015}, {"title": "Adapting grammatical error correction based on the native language of writers with neural network joint models", "author": ["Shamil Chollampatt", "Duc Tam Hoang", "Hwee Tou Ng."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Chollampatt et al\\.,? 2016", "shortCiteRegEx": "Chollampatt et al\\.", "year": 2016}, {"title": "Better evaluation for grammatical error correction", "author": ["Daniel Dahlmeier", "Hwee Tou Ng."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Dahlmeier and Ng.,? 2012", "shortCiteRegEx": "Dahlmeier and Ng.", "year": 2012}, {"title": "Building a large annotated corpus of learner english: The NUS Corpus of Learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Lan-", "citeRegEx": "Felice et al\\.,? 2014", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Predicting grammaticality on an ordinal scale", "author": ["Michael Heilman", "Aoife Cahill", "Nitin Madnani", "Melissa Lopez", "Matthew Mulholland", "Joel Tetreault."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Heilman et al\\.,? 2014", "shortCiteRegEx": "Heilman et al\\.", "year": 2014}, {"title": "TrueSkillTM: A Bayesian skill rating system", "author": ["Ralf Herbrich", "Tom Minka", "Thore Graepel."], "venue": "Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, pages 569\u2013 576, Vancouver, British Columbia, Canada, Decem-", "citeRegEx": "Herbrich et al\\.,? 2006", "shortCiteRegEx": "Herbrich et al\\.", "year": 2006}, {"title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Ground truth for grammatical error correction metrics", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Napoles et al\\.,? 2015", "shortCiteRegEx": "Napoles et al\\.", "year": 2015}, {"title": "The CoNLL2013 Shared Task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 Shared Task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of the Eighteenth Conference on Computational Natu-", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Efficient elicitation of annotations for human evaluation of machine translation", "author": ["Keisuke Sakaguchi", "Matt Post", "Benjamin Van Durme."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1\u201311, Baltimore, Mary-", "citeRegEx": "Sakaguchi et al\\.,? 2014", "shortCiteRegEx": "Sakaguchi et al\\.", "year": 2014}, {"title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality", "author": ["Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault."], "venue": "Transactions of the Association for Computational Linguistics, 4:169\u2013182.", "citeRegEx": "Sakaguchi et al\\.,? 2016", "shortCiteRegEx": "Sakaguchi et al\\.", "year": 2016}, {"title": "Tense and aspect error correction for ESL learners using global context", "author": ["Toshikazu Tajiri", "Mamoru Komachi", "Yuji Matsumoto."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 198\u2013202, Jeju", "citeRegEx": "Tajiri et al\\.,? 2012", "shortCiteRegEx": "Tajiri et al\\.", "year": 2012}, {"title": "A new dataset and method for automatically grading ESOL texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}, {"title": "Grammatical error correction using neural machine translation", "author": ["Zheng Yuan", "Ted Briscoe."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Yuan and Briscoe.,? 2016", "shortCiteRegEx": "Yuan and Briscoe.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "the release of the test set of the CoNLL\u20132014 Shared Task on GEC (Ng et al., 2014), systems have been compared and new evaluation techniques proposed on this single dataset.", "startOffset": 65, "endOffset": 82}, {"referenceID": 11, "context": "Following the recommendations of Sakaguchi et al. (2016), we release a new corpus for GEC, the JHU FLuency-Extended GUG corpus (JFLEG), which adds a layer of annotation to the GUG cor-", "startOffset": 33, "endOffset": 57}, {"referenceID": 5, "context": "pus (Heilman et al., 2014).", "startOffset": 4, "endOffset": 26}, {"referenceID": 3, "context": "The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013).", "startOffset": 168, "endOffset": 192}, {"referenceID": 10, "context": "The CoNLL Shared Tasks used this data (Ng et al., 2014; Ng et al., 2013), and the 1,312 sentence test set from the 2014 task has become de rigueur for benchmarking GEC.", "startOffset": 38, "endOffset": 72}, {"referenceID": 9, "context": "The CoNLL Shared Tasks used this data (Ng et al., 2014; Ng et al., 2013), and the 1,312 sentence test set from the 2014 task has become de rigueur for benchmarking GEC.", "startOffset": 38, "endOffset": 72}, {"referenceID": 3, "context": "The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). The CoNLL Shared Tasks used this data (Ng et al., 2014; Ng et al., 2013), and the 1,312 sentence test set from the 2014 task has become de rigueur for benchmarking GEC. This test set has been augmented with ten additional annotations from Bryant et al. (2015) and eight from", "startOffset": 169, "endOffset": 454}, {"referenceID": 14, "context": "The Cambridge Learner Corpus First Certificate in English (FCE) has essays coded by one rater using about 80 error types, alongside the score and demographic information (Yannakoudakis et al., 2011).", "startOffset": 170, "endOffset": 198}, {"referenceID": 13, "context": "com automatically aligned to user-provided corrections (Tajiri et al., 2012).", "startOffset": 55, "endOffset": 76}, {"referenceID": 11, "context": "Not including the additional fluency edits collected for the CoNLL-2014 test set by Sakaguchi et al. (2016). rors coded.", "startOffset": 84, "endOffset": 108}, {"referenceID": 12, "context": "Our goal in this work is to create a corpus of fluency edits, following the recommendations of (Sakaguchi et al., 2016), who identify the shortfalls of minimal edits: they artificially restrict the types of changes that can be made to a sentence and do not reflect the types of changes required", "startOffset": 95, "endOffset": 119}, {"referenceID": 5, "context": "We collected annotations on a public corpus of ungrammatical text, the GUG (Grammatical/Ungrammatical) corpus (Heilman et al., 2014).", "startOffset": 110, "endOffset": 132}, {"referenceID": 4, "context": "The best system from the CoNLL-2014 Shared Task is a hybrid approach, combining a rule-based system with MT and language-model reranking (CAMB14; Felice et al., 2014).", "startOffset": 137, "endOffset": 166}, {"referenceID": 15, "context": "They include a neural MT model (CAMB16; Yuan and Briscoe, 2016), a phrase-based MT (PBMT) with sparse features (AMU; Junczys-Dowmunt and Grund-", "startOffset": 31, "endOffset": 63}, {"referenceID": 1, "context": "kiewicz, 2016), and a hybrid system that incorporates a neural-net adaptation model into PBMT (NUS; Chollampatt et al., 2016).", "startOffset": 94, "endOffset": 125}, {"referenceID": 8, "context": "We evaluate system output against the four sets of JFLEG corrections with GLEU, an automatic fluency metric specifically designed for this task (Napoles et al., 2015) and the Max-Match metric (M2) (Dahlmeier and Ng, 2012).", "startOffset": 144, "endOffset": 166}, {"referenceID": 2, "context": ", 2015) and the Max-Match metric (M2) (Dahlmeier and Ng, 2012).", "startOffset": 38, "endOffset": 62}, {"referenceID": 9, "context": "M2 was designed to score minimal edits and was used in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013; Ng et al., 2014).", "startOffset": 99, "endOffset": 133}, {"referenceID": 10, "context": "M2 was designed to score minimal edits and was used in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013; Ng et al., 2014).", "startOffset": 99, "endOffset": 133}, {"referenceID": 11, "context": "tems with overlapping ranges into equivalence classes by bootstrap resampling (Sakaguchi et al., 2014; Herbrich et al., 2006).", "startOffset": 78, "endOffset": 125}, {"referenceID": 6, "context": "tems with overlapping ranges into equivalence classes by bootstrap resampling (Sakaguchi et al., 2014; Herbrich et al., 2006).", "startOffset": 78, "endOffset": 125}], "year": 2017, "abstractText": "We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this corpus, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC.", "creator": "TeX"}}}