{"id": "1702.04423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "Efficient Multi-task Feature and Relationship Learning", "abstract": "in this paper we propose a structured multi - convex framework for multi - thread task learning that improves predictions by learning relationships both between tasks and between features. our current framework is a generalization of related methods in multi - concurrent task learning, that either learn task relationships, or feature relationships, but not both. we start with a hierarchical bayesian model, and use correctly the empirical bayes method to transform the underlying inference problem into a multi - process convex optimization problem. we propose a coordinate - wise minimization algorithm that has a nonlinear closed form solution applied for each block subproblem. naively these solutions would be expensive to compute, but hopefully by using the theory of doubly stochastic distance matrices, which we are likely able systematically to reduce the underlying matrix optimization subproblem into a minimum weight perfect matching problem on approaching a complete bipartite graph, and therefore solve it analytically and efficiently. to simply solve the weight learning subproblem, we propose three continuously different strategies, including a gradient descent method with linear convergence guarantee when the instances are together not shared by multiple tasks, and a partial numerical improvement solution based on sylvester equation when instances are shared. we demonstrate the efficiency of our method on both synthetic datasets processes and real - world datasets. experiments successfully show that the proposed optimization method is orders of high magnitude improvement faster than an ongoing off - the - shelf projected gradient method, and our performance model is able to exploit smoothing the correlation structures among multiple tasks and features.", "histories": [["v1", "Tue, 14 Feb 2017 23:43:32 GMT  (186kb,D)", "http://arxiv.org/abs/1702.04423v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han zhao", "otilia stretcu", "renato negrinho", "alex smola", "geoff gordon"], "accepted": false, "id": "1702.04423"}, "pdf": {"name": "1702.04423.pdf", "metadata": {"source": "CRF", "title": "Efficient Multi-task Feature and Relationship Learning", "authors": ["Han Zhao", "Otilia Stretcu", "Renato Negrinho", "Alex Smola", "Geoff Gordon"], "emails": ["HAN.ZHAO@CS.CMU.EDU", "OSTRETCU@CS.CMU.EDU", "NEGRINHO@CS.CMU.EDU", "ALEX@SMOLA.ORG", "GGORDON@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015). One of the underlying assumptions behind many multi-task learning algorithms is that the tasks are related to each other. Hence, a key question is how to define the notion of task relatedness, and how to capture it in the learning formulation. A common assumption is that tasks can be described by weight vectors, either from finite dimensional space or Reproducing Kernel Hilbert Space (RKHS), and that they are sampled from a shared prior distribution over their space (Liu et al., 2009; Zhang and Yeung, 2010a,b). Another strand of work assumes common feature representations to be shared among multiple tasks, and the goal is to learn the shared representation as well as task-specific parameters simultaneously (Thrun, 1996a; Caruana, 1997; Evgeniou and Pontil, 2007; Argyriou et al., 2008). Moreover, when structure about multiple tasks is available, e.g., task-specific descriptors (Bonilla et al., 2007a) or a task similarity graph (Evgeniou and Pontil, 2004), regularizers can often be incorporated into the learning formulation to explicitly penalize hypotheses that are not consistent with the given structure. In this paper we follow the first line of work and propose a multi-convex framework for multi-task learning. Our method improves predictions over tabula rasa learning by assuming that all the task\nar X\niv :1\n70 2.\n04 42\n3v 1\n[ cs\n.L G\n] 1\n4 Fe\nb 20\nvectors are sampled from a common, shared prior. There have been several attempts to improve predictions along this direction by either learning the relationships between different tasks (Zhang and Yeung, 2010a), known as Multi-Task Relationship Learning (MTRL), or by exploiting the relationships between different features (Argyriou et al., 2008), which is known as Multi-Task Feature Learning (MTFL). Zhang and Schneider (2010) proposed a multi-task learning framework where both the task and feature relationships are inferred from data by assuming a sparse matrix-normal penalty on both the task and feature representations. As in that paper, our multi-task learning framework is a generalization of both MTRL and MTFL, which learns the relationships both between tasks and between features simultaneously. This property is favorable for applications where we not only aim for better generalization, but also seek to have a clear understanding about the relationships among different tasks. Compared to the sparse regularization approach in Zhang and Schneider (2010), our framework is free of the sparsity assumption, and as a result, it admits an efficient optimization scheme where we are able to derive analytic solutions to each subproblem, whereas in (Zhang and Schneider, 2010) iterative methods have to be used for each subproblem. We term our proposed framework FEature and Task Relationship learning (FETR). Our main contributions are summarized as follows:\n\u2022 First, we formulate FETR as a hierarchical Bayes model, where a carefully chosen prior is imposed on the task vectors in order to capture the relatedness between tasks and features at the same time. Our model is free of sparsity assumption that was usually made in the literature to make the algorithm scalable. We apply an empirical Bayes (Carlin and Louis, 1997) method to approximate the exact posterior distribution, leading to a learning formulation where the goal is to optimize over both the model parameters, i.e., the task vectors, as well as two covariance matrices in the prior.\n\u2022 Second, we transform the optimization problem in FETR into a multi-convex structure, and design an alternating direction algorithm using block coordinate-wise minimization to solve this problem efficiently. Specifically, we achieve this by reducing an underlying matrix optimization problem with positive definite constraints into a minimum weight perfect matching problem on a complete bipartite graph, where we are able to solve analytically using combinatorial techniques. To solve the weight learning subproblem, we propose three different strategies, including a closed form solution, a gradient descent method with linear convergence guarantee when the instances are not shared by multiple tasks, and a numerical solution based on Sylvester equation when instances are shared.\n\u2022 Next, we demonstrate the efficiency of the proposed optimization algorithm by comparing it with an off-the-shelf projected gradient descent algorithm on synthetic data. Experiments show that the proposed optimization method is orders of magnitude faster than its competitor, and it often converges to a better local solution.\n\u2022 Lastly, we test the statistical performance of FETR on modeling real-world related tasks by comparing it with single task learning, as well as MTFL and MTRL. Results show that FETR is not only able to give better predictions, but can also effectively exploit the correlation structures among multiple tasks. We also study FETR and its relationship with existing multitask learning algorithms, including MTFL and MTRL, in a general regularization framework, showing that some of them can be viewed as special cases of FETR."}, {"heading": "2. Multi-task Feature and Relationship Learning", "text": ""}, {"heading": "2.1 Notation and Setup", "text": "We use Sm+ and Sm++ to denote the m-dimensional symmetric positive semidefinite cone and mdimensional symmetric positive definite cone, respectively. We write tr(A) for the trace of a matrix A. Finally, we use G = (A,B,E,w) to denote a weighted bipartite graph with vertex sets A, B, edge set E, and weight function w : E \u2192 R+. [d] denotes the set {1, 2, . . . , d}. We consider the following setup. Suppose we are given m learning tasks {Ti}mi=1, where for each learning task Ti we have access to a training set Di with ni data instances (xji , y j i ), j \u2208 [ni]. Here we focus on the supervised learning setting where xji \u2208 Xi \u2286 Rd and y j i \u2208 Yi, where Yi = R for a regression problem and Yi = {1,\u22121} for a binary classification problem. Let fi(wi, \u00b7) : Xi \u2192 Yi be our predictor/model with parameter wi and `(\u00b7, \u00b7) : Yi \u00d7 Yi \u2192 R+ be the loss function for task Ti. For ease of discussion, in what follows, we will assume our model for each task Ti to be a linear regression, i.e., fi(wi,x) = wTi x. Our approach can also be translated into a classification problem, e.g., logistic regression, linear SVM, etc. We refer interested readers to the appendix for proofs of claims and theorems in the paper."}, {"heading": "2.2 Hierarchical Bayes model", "text": "Based on the linear regression model, the likelihood function for task i is given by:\nyji | x j i ,wi, i \u223c N (wTi x, 2i ) (1)\nwhere N (m,\u03a3) is the multivariate normal distribution with mean m and covariance matrix \u03a3. Let W = (w1, . . . ,wm) \u2208 Rd\u00d7m be the model parameter (regression weights) for m different tasks. Applying a hierarchical Bayes approach, we specify a prior distribution over the model parameter W . Specifically, we define the prior distribution over W to be\nW | \u03be,\u21261,\u21262 \u223c (\nm\u220f i=1\nN (wi | 0, \u03beiId) ) q(W | \u21261,\u21262) (2)\nwhere Id is a d \u00d7 d identity matrix and 0 \u2208 Rd is a d-dimensional vector of all 0s. The form of q(W | \u21261,\u21262) is given by\nq(W | \u21261,\u21262) =MN d\u00d7m(W | 0d\u00d7m,\u21261,\u21262)\nwhere MN d\u00d7m(M,A,B) denotes a matrix-variate normal distribution (Gupta and Nagar, 1999) with mean M \u2208 Rd\u00d7m, row covariance matrix A \u2208 Sd++ and column covariance matrix B \u2208 Sm++.1 As we will see later, the first term in the RHS of Eq. 2 can be interpreted as a regularizer to penalize the model complexity of fi and q(W | \u21261,\u21262) encodes the structure about the task vectors W . Intuitively, by imposing structure on the row covariance and column covariance matrices, we can incorporate our prior knowledge about the correlation among features as well as the relationship among different tasks. It is worth pointing out that one can specify other forms of prior distributions instead of (2), as in (Liu et al., 2009) and (Zhang and Yeung, 2010b) where the authors use a Laplacian prior and a generalized t process, respectively. As we will see shortly, the advantage of\n1. The probability density function is p(X | M,A,B) = exp(\u2212 1 2 tr(A\u22121(X\u2212M)B\u22121(X\u2212M)T )) (2\u03c0)md/2|A|m/2|B|d/2 .\nusing the specific prior in Eq. 2 is that, when applying an empirical Bayes approach to solve our model, we will explicitly have access to the optimal covariance matrices \u21261 and \u21262 over the row vectors and column vectors of W ."}, {"heading": "2.3 Empirical Bayes method", "text": "Given the prior distribution over W and the likelihood function as specified in Eq. 2 and Eq. 1, the posterior distribution of the model parameter W is given by\np(W | X,y) \u221d p(W )p(y | X,W ) (3)\nA standard Bayesian inference approach applied here would be to specify another prior distribution p(\u21261,\u21262) over both covariance matrices \u21261 and \u21262 and then compute the above exact posterior distribution as follows:\np(W | X,y) \u221d p(W )p(y | X,W ) = p(y | X,W ) \u222b p(W | \u21261,\u21262)p(\u21261,\u21262) d\u21261d\u21262\nThis is computationally intractable in our case, since we do not have an analytic form for the posterior distribution. So instead of computing the integration exactly, we take an empirical Bayes approach: we approximate the intractable integration above by\np(W | X,y) \u2248 max \u21261,\u21262 p(y | X,W )p(W | \u21261,\u21262) (4)\nThe underlying assumption behind this approximation is that when the exact posterior distribution is sharply concentrated, the integral for the marginal p(W ) = \u222b p(W | \u21261,\u21262)p(\u21261,\u21262) d\u21261d\u21262 may be not much changed by replacing the prior distribution over \u21261 and \u21262 with a point estimate representing the peak of the distribution. Substituting Eq. 2 and Eq. 1 into Eq. 4 and omitting the constant terms which do not depend on W , \u21261 and \u21262, we maximize the approximate posterior distribution by optimizing over W , \u21261 and \u21262, leading us to the following optimization scheme:\nminimize W,\u21261,\u21262 m\u2211 i=1 1 \u03be2i wTi wi +m log |\u21261|+ d log |\u21262|+\nm\u2211 i=1 1 2i ni\u2211 j=1 (yji \u2212wTi x j i ) 2 + tr(\u2126\u221211 W\u2126 \u22121 2 W T )\nsubject to \u21261 0,\u21262 0\n(5)\nIt is worth pointing out here the optimal value of (5) may not be achieved since the constraint set is open. In fact, we can always decrease the objective function by setting W = 0d\u00d7m and make the eigenvalues of both \u21261 and \u21262 infinitely close to 0 but strictly greater than 0. In this case, the value of m log |\u21261| and d log |\u21262| will approach\u2212\u221e hence (5) has no lower bound. We will fix this technical issue in the next section by imposing boundedness constraints on both \u21261 and \u21262. For simplicity of later discussion, we will assume that \u03bei = \u03be and i = , \u2200i \u2208 [m]. Before discussing the details on how to efficiently solve the above optimization program, let us inspect the equation and discuss several of its special cases. If we fix \u21261 = Id and \u21262 = Im, the above\noptimization problem can be decomposed into m independent single task learning problems, where each task corresponds to a ridge regression problem. More generally, block diagonal designs of the covariance matrices \u21261 and \u21262 will exhibit group/clustering properties of the features or tasks. On the other hand, when m = 1, i.e., in the single task learning setting, and we fix \u21261 and \u21262 manually, then the above optimization problem reduces to linear regression with a weighted linear smoother, where the smoother is specified by \u03bbId + \u2126\u221211 . An optimal solution can be obtained in closed form. In this case \u2126\u221211 plays the role of a stabilizer and reweights the relative importance of different features. Similarly, \u2126\u221212 models the correlation between different tasks and their relative strength."}, {"heading": "3. Multi-convex Optimization", "text": "Although the empirical Bayes framework is appealing, the optimization posed in (5) is not easy to solve directly. In this section we first show that (5) can be converted into a multi-convex optimization problem. A multi-convex function is a generalization of a bi-convex function (Gorski et al., 2007) into multiple variables or blocks of variables.\nDefinition 3.1 (Multi-convex function (Xu and Yin, 2013)). A function g(x1, . . . ,xk) is called multi-convex if for each block of variables xi, g is a convex function of xi while keeping all the other blocks of variables fixed."}, {"heading": "3.1 Multi-convex Formulation", "text": "It is not hard to see that the optimization problem in (5) is not convex sincem log |\u21261|+d log |\u21262| is a concave function of \u21261 and \u21262 (Boyd and Vandenberghe, 2004). Also, (5) is unbounded from below as we analyzed in the last section. To handle these technical issues, we introduce a boundedness constraint into the constraint set of \u21261 and \u21262. More concretely, instead of constraining \u21261 0 and \u21262 0, we make 1uId \u21261 1l Id and 1uIm \u21262 1l Im, where u > l > 0 are constants. One can understand this constraint as putting an implicit uniform prior distribution p(\u21261,\u21262) over the set specified by the constraint. Technically, the boundedness constraint make the feasible sets for \u21261 and \u21262 compact, hence by the extreme value theorem minimum is guaranteed to be achieved since the objective function is continuous. Next, we apply a well known transformation to both \u21261 and \u21262 so that the new optimization problem is multi-convex in terms of the transformed variables. We define \u03a31 , \u2126\u221211 and \u03a32 , \u2126 \u22121 2 . Both \u03a31 and \u03a32 are well-defined because \u21261 and \u21262 are constrained to be positive definite matrices. The transformed optimization formulation based on W,\u03a31 and \u03a32 is\nminimize W,\u03a31,\u03a32 m\u2211 i=1 ni\u2211 j=1 (yji \u2212wTi x j i ) 2 + \u03b7 m\u2211 i=1 wTi wi\n\u2212 \u03c1(m log |\u03a31|+ d log |\u03a32|) + \u03c1tr(\u03a31W\u03a32W T )\nsubject to lId \u03a31 uId, lIm \u03a32 uIm\n(6)\nwhere we define \u03b7 = ( /\u03be)2 and \u03c1 = 2 to simplify the notation.\nClaim 3.1. The objective function in (6) is multi-convex."}, {"heading": "3.2 Block Coordinate Minimization", "text": "Based on the multi-convex formulation developed in the last section, we propose an alternating direction algorithm using block coordinate-wise minimization to optimize the objective given in (6). In each iteration k we alternatively minimize over W with \u03a31 and \u03a32 fixed, then minimize over \u03a31 with W and \u03a32 fixed, and lastly minimize \u03a32 with W and \u03a31 fixed. The whole procedure is repeated until a stationary point is found or the decrease in the objective function is less than a pre-specified threshold. In what follows, we assume n = ni, \u2200i \u2208 [m] to simplify the notation. Let Y = (y1, . . . ,ym) \u2208 Rn\u00d7m be the labeling matrix and X \u2208 Rn\u00d7d be the feature matrix shared by all the tasks. Using this notation, the objective function can be equivalently expressed in matrix form as:\nminimize W,\u03a31,\u03a32\n||Y \u2212XW ||2F + \u03b7||W ||2F + \u03c1||\u03a31/21 W\u03a3 1/2 2 ||2F\n\u2212 \u03c1(m log |\u03a31|+ d log |\u03a32|) subject to lId \u03a31 uId, lIm \u03a32 uIm\n(7)\nThere are lots of ways to try to solve the above, and we discuss next how to do so efficiently."}, {"heading": "3.2.1 OPTIMIZATION W.R.T W", "text": "In order to minimize over W when both \u03a31 and \u03a32 are fixed, we solve the following subproblem:\nminimize W\nh(W ) , ||Y \u2212XW ||2F + \u03b7||W ||2F + \u03c1||\u03a31/21 W\u03a3 1/2 2 ||2F (8)\nAs shown in the last section, this is an unconstrained convex optimization problem. We present three different algorithms to find the optimal solution of this subproblem. The first one guarantees to find an exact solution in closed form in O(m3d3) time, by using the isomorphism between Rd\u00d7m and Rdm. The second one does gradient descent with fixed step size to iteratively refine the solution, and we show that in our case a linear convergence speed can be guaranteed. The third one finds the optimal solution by solving the Sylvester equation (Bartels and Stewart, 1972) characterized by the first-order optimality condition, after a proper transformation.\nA closed form solution. It is worth noting that it is not obvious how to obtain a closed form solution directly from the formulation in (8). An application of the first order optimality condition to (8) will lead to the following equation:\n(XTX + \u03b7Id)W + \u03c1\u03a31W\u03a32 = X TY (9)\nExcept for the special case where \u03a32 = cIm with c > 0 a constant, the above equation does not admit an easy closed form solution in its matrix representation. The workaround is based on the fact that Rd\u00d7m ' Rdm, i.e., the d \u00d7m dimensional matrix space is isomorphic to the dm dimensional vector space, with the vec(\u00b7) operator implementing the isomorphism from Rd\u00d7m to Rdm. Using this property, we claim\nClaim 3.2. (8) can be solved in closed form in O(m3d3 + mnd2) time; the optimal vec(W \u2217) has the following form: (\nIm \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31 )\u22121 vec(XTY ) (10)\nW \u2217 can then be obtained simply by reformatting vec(W \u2217) into a d\u00d7m matrix. The computational bottleneck in the above procedure is in solving an md \u00d7 md system of equations, which scales as O(m3d3) if no further sparsity structure is available. The overall computational complexity is O(m3d3 +mnd2).\nGradient descent. The closed form solution shown above scales cubically in both m and d, and requires us to explicitly form a matrix of size md\u00d7md. This can be intractable even for moderate m and d. In such cases, instead of computing an exact solution to (8), we can use gradient descent with fixed step size to obtain an approximate solution. The objective function h(W ) in (8) is differentiable and its gradient can be obtained in O(m2d + md2) time as follows:\n\u2207Wh(W ) = XT (Y \u2212XW ) + \u03b7W + \u03c1\u03a31W\u03a32 (11)\nNote that we can compute in advance both XTY and XTX in O(nd2) time, and cache them so that we do not need to recompute them in each gradient update step. Let \u03bbi(A) be the ith largest eigenvalue of a real symmetric matrixA. We provide a linear convergence guarantee for the gradient method in Thm. 3.1. Our proof technique is adapted from Nesterov (2013) where we extend it to matrix function.\nTheorem 3.1. Let \u03bbl = \u03bbd(XTX) + \u03b7 + \u03c1l2, \u03bbu = \u03bb1(XTX) + \u03b7 + \u03c1u2 and \u03b3 = (\u03bbu\u2212\u03bbl\u03bbu+\u03bbl ) 2. Choose 0 < t \u2264 2/(\u03bbu + \u03bbl). For all \u03b5 > 0, gradient descent with step size t converges to the optimal solution within O(log1/\u03b3(1/\u03b5)) steps.\nRemark. The computational complexity to achieve an \u03b5 approximate solution using gradient descent is O(nd2 + log1/\u03b3(1/\u03b5)(m\n2d + md2)). Compared with the O(m3d3 + mnd2) complexity for the exact solution, the gradient descent algorithm scales much better provided \u03b3 1, i.e., the condition number \u03ba , \u03bbu/\u03bbl is not too large. As a side note, when the condition number is large, we can effectively reduce it to \u221a k by using conjugate gradient method (Shewchuk et al., 1994).\nSylvester equation. In the field of control theory, a Sylvester equation (Bhatia and Rosenthal, 1997) is a matrix equation of the form AX + XB = C, where the goal is to find a solution matrix X given A,B and C. For this problem, there are efficient numerical algorithms with highly optimized implementations that can obtain a solution within cubic time. For example, the BartelsStewart algorithm (Bartels and Stewart, 1972) solves the Sylvester equation by first transforming A and B into Schur forms by QR factorization, and then solves the resulting triangular system via back-substitution. Our third approach is based on the observation that we can equivalently transform the first-order optimality equation given in (9) into a Sylvester equation by multiplying \u03a3\u221211 at both sides of the equation:\n\u03a3\u221211 (X TX + \u03b7Id)W +W (\u03c1\u03a32) = \u03a3 \u22121 1 X TY (12)\nThen finding the optimal solution of the subproblem amounts to solving the above Sylvester equation. Specifically, the solution to the above equation can be obtained using the Bartels-Stewart algorithm within O(m3 + d3 + nd2). Both the gradient descent and the Bartels-Stewart algorithm find the optimal solution in cubic time. However, the gradient descent algorithm is more widely applicable than the Bartels-Stewart algorithm: the Bartels-Stewart algorithm only applies to the case where all the tasks share the same\ninstances, so that we can write down the matrix equation explicitly, while gradient descent can be applied in the case where each task has different number of inputs and those inputs are not shared among tasks. On the other hand, as we will see shortly in the experiments, in practice the BartelsStewart algorithm is faster than gradient descent, and provides a more numerically stable solution."}, {"heading": "3.2.2 OPTIMIZATION W.R.T. \u03a31 AND \u03a32", "text": "Before we delve into the detailed analysis below, we first list the final algorithms used to optimize \u03a31 and \u03a32 in Alg. 1 and Alg. 2, respectively. They are remarkably simple: each algorithm only involves one SVD, one truncation and two matrix multiplications. The computational complexity of Alg. 1 (Alg. 2) is bounded by O(m2d+md2 + d3) (O(m2d+md2 +m3)).\nAlgorithm 1 Minimize \u03a31 Input: W , \u03a32 and l, u.\n1: [V, \u03bd]\u2190 SVD(W\u03a32W T ). 2: \u03bb\u2190 T[l,u](m/\u03bd). 3: \u03a31 \u2190 V diag(\u03bb)V T .\nAlgorithm 2 Minimize \u03a32 Input: W , \u03a31 and l, u.\n1: [V, \u03bd]\u2190 SVD(W T\u03a31W ). 2: \u03bb\u2190 T[l,u](d/\u03bd). 3: \u03a32 \u2190 V diag(\u03bb)V T .\nWe focus on analyzing the optimization w.r.t. \u03a31. A similar analysis applies to \u03a32. In order to minimize over \u03a31 when W and \u03a32 are fixed, we solve the following subproblem:\nminimize \u03a31 tr(\u03a31W\u03a32W T )\u2212m log |\u03a31| subject to lId \u03a31 uId (13)\nAlthough (13) is a convex optimization problem, it is computationally expensive to solve using offthe-shelf algorithms because of the constraints as well as the nonlinearity of the objective function. Surprisingly, we can find a closed form optimal solution to this problem as well, using tools from the theory of doubly stochastic matrices (Dufosse\u0301 and Uc\u0327ar, 2016) and perfect bipartite graph matching. Since \u03a32 \u2208 Sm++, it follows that W\u03a32W T \u2208 Sd+. Without loss of generality, we can reparametrize \u03a31 = U\u039bU\nT , where \u039b = diag(\u03bb1, . . . , \u03bbd) with u \u2265 \u03bb1 \u2265 \u03bb2 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd \u2265 l and U \u2208 Rd\u00d7d with UTU = UUT = Id using spectral decomposition. Similarly, we can representW\u03a32W T = V NV T where V \u2208 Rd\u00d7d, V TV = V V T = Id and N = diag(\u03bd1, . . . , \u03bdd) with 0 \u2264 \u03bd1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bdd. Note that the eigenvectors in N corresponds to eigenvalues in increasing order rather than decreasing order, for reasons that will become clear below. Using the new representation and realizing that U is an orthonormal matrix, we have\nlog |\u03a31| = log |U\u039bUT | = log |\u039b| (14)\nand tr(\u03a31W\u03a32W T ) = tr(\u039bUTV NV TU) (15)\nSet K = UTV . Since both U and V are orthonormal matrices, K is also an orthonormal matrix. We can further transform (15) to be\ntr(\u039bUTV NV TU) = tr((\u039bK)(KN)T )\nNote that the mapping between U and K is bijective since V is a fixed orthonormal matrix. Using K and \u039b, we can equivalently transform the optimization problem (13) into the following new form:\nminimize K,\u039b\n\u2212m log |\u039b|+ tr((\u039bK)(KN)T )\nsubject to l diag(1d) \u2264 \u039b \u2264 u diag(1d) KTK = KKT = Id\n(16)\nwhere 1d is a d-dimensional vector of all ones. At first glance it seems that the new form of optimization is more complicated to solve since it is even not a convex problem due to the quadratic equality constraint. However, as we will see shortly, the new form helps to decouple the interaction between K and \u039b in that K does not influence the first term \u2212m log |\u039b|. This implies that we can first partially optimize over K, finding the optimal solution as a function of \u039b, and then optimize over \u039b. Mathematically, it means:\nmin K,\u039b \u2212m log |\u039b|+ tr((\u039bK)(KN)T )\u21d4 min \u039b \u2212m log |\u039b|+ min K tr((\u039bK)(KN)T ) (17)\nConsider the minimization over K:\ntr((\u039bK)(KN)T ) = d\u2211 i=1 d\u2211 j=1 \u03bbiK 2 ij\u03bdj = \u03bb TP\u03bd\nwhere we define P = K \u25e6 K, \u03bb = (\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbd)T and \u03bd = (\u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdd)T . Since K is an orthonormal matrix, we have the following two equations: \u2211d j=1 Pij = \u2211d j=1K\n2 ij = 1, \u2200i \u2208 [d],\u2211d i=1 Pij = \u2211d i=1K 2 ij = 1, \u2200j \u2208 [d], which implies that P is a doubly stochastic matrix (Dufosse\u0301 and Uc\u0327ar, 2016). The partial minimization over K can be equivalently solved by the partial minimization over P :\nmin K tr((\u039bK)(KN)T ) = min P \u03bbTP\u03bd (18)\nIn order to solve the minimization over the doubly stochastic matrix P , we need to introduce the following theorems.\nTheorem 3.2 (Optimality of extreme points (Bertsimas and Tsitsiklis, 1997)). Consider the minimization of a linear programming problem over a polyhedron P . Suppose that P has at least one extreme point and that there exists an optimal solution. Then there exists an optimal solution which is an extreme point of P .\nDefinition 3.2 (Birkhoff polytope). The Birkhoff polytope Bd is the set of d \u00d7 d doubly stochastic matrices. Bd is a convex polytope.\nTheorem 3.3 (Birkhoff-von Neumann theorem). LetBd be the Birkhoff polytope. Bd is the convex hull of the set of d \u00d7 d permutation matrices. Furthermore, the vertices (extreme points) of Bd are the permutation matrices.\nThe following lemma follows immediately from the above two theorems:\nLemma 3.1. There exists an optimal solution P to the optimization problem (18) that is a d \u00d7 d permutation matrix.\nGiven that there exists an optimal solution that is a permutation matrix, we can reduce (18) into a minimum-weight perfect matching problem on a complete bipartite graph. The problem of minimum-weight perfect matching is defined as follows.\nDefinition 3.3 (Minimum-weight perfect matching). Let G = (V,E) be an undirected graph with edge weight w : E \u2192 R+. A matching in G is a set M \u2286 E such that no two edges in M have a vertex in common. A matching M is called perfect if every vertex from V occurs as the endpoint of some edge in M . The weight of a matching M is w(M) = \u2211 e\u2208M w(e). A matching M is called a minimum-weight perfect matching if it is a perfect matching that has the minimum weight among all the perfect matchings of G.\nFor any \u03bb, \u03bd \u2208 Rd+, we can construct a weighted d \u2212 d bipartite graph G = (V\u03bb, V\u03bd , E, w) as follows:\n\u2022 For each \u03bbi, construct a vertex v\u03bbi \u2208 V\u03bb, \u2200i. \u2022 For each \u03bdj , construct a vertex v\u03bdj \u2208 V\u03bd , \u2200j. \u2022 For each pair (v\u03bbi , v\u03bdj ), construct an edge e(v\u03bbi , v\u03bdj ) \u2208 E with edge wight w(e(v\u03bbi , v\u03bdj )) = \u03bbi\u03bdj .\nTheorem 3.4. The minimum value of (18) is equal to the minimum weight of a perfect matching on G = (V\u03bb, V\u03bd , E, w). Furthermore, the optimal solution P of (18) can be constructed from the minimum-weight perfect matching M on G.\nWe do not even need to run standard graph matching algorithms to solve our matching problem. Instead, Thm. 3.5 gives a closed form solution.\nTheorem 3.5. Let \u03bb = (\u03bb1, . . . , \u03bbd) and \u03bd = (\u03bd1, . . . , \u03bdd) with \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and \u03bd1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bdd. The minimum-weight perfect matching on G is \u03c0\u2217 = {(v\u03bbi , v\u03bdi) : 1 \u2264 i \u2264 d} with the minimum weight w(\u03c0\u2217) = \u2211d i=1 \u03bbi\u03bdi. The permutation matrix that achieves the minimum weight is P \u2217 = Id since \u03c0\u2217(\u03bbi) = \u03bdi. Note that P = K \u25e6K, it follows that the optimalK\u2217 is also Id. Hence we can solve for the optimal U\u2217 matrix by solving the equation U\u2217TV = Id, which leads to U\u2217 = V . Now plug in the optimal K\u2217 = Id into (17). The optimization w.r.t. \u039b decomposes into d independent optimization problems, each of which being a simple scalar optimization problem:\nminimize \u03bb d\u2211 i=1 \u03bbi\u03bdi \u2212m log \u03bbi\nsubject to l \u2264 \u03bbi \u2264 u, \u2200i = 1, . . . , d (19)\nDepending on whether the value m/\u03bdi is within the range [l, u], the optimal solution \u03bb\u2217i for each scalar minimization problem may take different forms. Define a soft-thresholding operator T[l,u](x) as follows:\nT[l,u](z) =  l, z < l\nz, l \u2264 z \u2264 u u, z > u\n(20)\nUsing this soft-thresholding operator, we can express the optimal solution \u03bb\u2217i as \u03bb \u2217 i = T[l,u](m/\u03bdi). Combining all the analysis above, we get the algorithms listed at the beginning of this section to optimize \u03a31 (Alg. 1) and \u03a32 (Alg. 2)."}, {"heading": "4. Experiments", "text": "In this section we analyze the statistical performance of FETR as well as the efficiency of the proposed coordinate minimization algorithm for solving the underlying multi-convex optimization problem."}, {"heading": "4.1 Convergence Analysis", "text": "We first investigate the efficiency and scalability of the three different algorithms for minimizing w.r.t. W on synthetic data sets. To this end, for each experiment, we generate a synthetic data set which consists of n = 104 instances that are shared among all the tasks. All the instances are randomly sampled uniformly from [0, 1]d. We gradually increase the dimension of features, d, and the number of tasks, m to test scalability. The first algorithm implements the closed form solution (21) by explicitly computing themd\u00d7md tensor product matrix and then solving the linear system. The second one is the proposed gradient descent, whose stop condition is met when either the norm of the gradient is less than 0.01 or the algorithm has looped more than 50 iterations. The last one uses the Bartels-Stewart algorithm to solve the equivalent Sylvester equation to compute W . We use open source toolkit scipy whose backend implementation uses highly optimized Fortran code. For all the synthetic experiments we set l = 0.01 and u = 100, which corresponds to a condition number of 104. We fix the coefficients \u03b7 = 1.0 and \u03c1 = 1.0 since they will not affect the convergence speed. The experimental results are shown in Fig. 1. As expected, the closed form solution does not scale to problems of even moderate size due to its large memory requirement. In practice the Bartels-Stewart algorithm is about one order of magnitude faster than the gradient descent method when either m or d is large (hundreds of features or hundreds of tasks). It is also worth pointing out here that the Bartels-Stewart algorithm is the most numerically stable algorithm among the three based on our observations. Hence in the following experiments where the Bartels-Stewart solver can be applied we will use it, while in the case where it cannot be applied, we will use the gradient descent method. We now compare our proposed coordinate minimization algorithm with an off-the-shelf projected gradient method to solve the optimization problem (7). Specifically, the projected gradient method updates W,\u03a31 and \u03a32 based on the gradient direction in each iteration and then projects \u03a31 and \u03a32 into the corresponding feasible regions. In this experiment we set the number of instances to be 104, the dimension of feature vectors to be 104 and the number of tasks to be 10. All the instances are shared among all the tasks, so that the Sylvester solver is used to optimize W in coordinate minimization. We repeat the experiments 10 times and report the log function values versus the time used by these two algorithms; see Fig. 2. We use a desktop with twelve i7-6850K CPU cores to conduct this experiment. It is clear from this synthetic experiment that our proposed algorithm not only converges much faster than projected gradient descent, but also achieves better results."}, {"heading": "4.2 Synthetic Toy Problem", "text": "Before we proceed to do experiments on real-world data sets, we generate a toy data set and conduct a synthetic experiment to show that the proposed model can indeed learn the relationships between task vectors and feature vectors simultaneously. We randomly generate 10 data points uniformly from the range [0, 100]2. We consider the following three regression tasks: y1 = 10x1 + 10x2 + 10,\ny2 = 0, and y3 = \u22128x1 \u2212 8x2 \u2212 8. The outputs of those regression functions are corrupted by Gaussian noise with mean 0 and variance 1. The true weight matrix is given by\nW \u2217 = ( 10 0 \u22128 10 0 \u22128 ) For this problem, the first row of W \u2217 corresponds to the weight vector of the feature x1 across the tasks and the second row corresponds to the weight vector of x2. Each column represents a task vector. Hence we expect that the correlation between x1 and x2 is 1.0, and we also expect that the\ncorrelation between the first and the third task to be -1.0 while the correlations for the other two pairs of tasks are 0.0. We apply FETR to this problem, setting \u03b7, \u03c1 = 10\u22123, l = 10\u22123 and u = 103. After the algorithm converges, the estimated regression functions are y\u03021 = 10.00x1 + 9.99x2 + 9.43, y\u03022 = 0.00 and y\u03023 = \u22127.99x1 \u2212 8.01x2 \u2212 7.56, with the following feature and task correlation matrices (after normalization):\n\u21261 = ( 1.00 0.99 0.99 1.00 ) , \u21262 =  1.00 0.00 \u22121.010.00 1.00 0.00 \u22121.01 0.00 1.00  This synthetic experiment shows that both the learned feature correlation matrix and task correlation matrix conform to our expectation, demonstrating the effectiveness of FETR on this synthetic problem."}, {"heading": "4.3 Robot Inverse Dynamics", "text": "We evaluate FETR against other multi-task learning algorithms on the inverse dynamics problem for a seven degree-of-freedom (DOF) SARCOS anthropomorphic robot arm.2 The goal of this task is to map from a 21-dimensional input space (7 joint positions, 7 joint velocities, 7 joint accelerations) to the corresponding 7 joint torques. Hence there are 7 tasks and the inputs are shared among all the tasks. The training set and test set contain 44,484 and 4,449 examples, respectively. We further partition the training set into a development set and a validation set, which contain 31,138 and 13,346 instances. We use the validation set to do the model selection. Specifically, we train multiple models on the development set with different configurations of hyperparameters ranging\n2. http://www.gaussianprocess.org/gpml/data/\nfrom \u03b7 = {10\u22125, . . . , 102} and \u03c1 = {10\u22125, . . . , 102}, and for each model we select the one with the best mean-squared error (MSE) on the validation set. In all the experiments we fix l = 10\u22123 and u = 103 as we observe the results are not very sensitive to these hyperparameters. We compare FETR with multi-task feature learning (Evgeniou and Pontil, 2007) (MTFL) and multitask relationship learning (Zhang and Yeung, 2010a) (MTRL), which can be treated as two different special cases of our model. To have a more controlled, fair comparison between all the methods, we implement both MTFL and MTRL, and use the same experimental setting, including model selection, as FETR. For each task, we use ridge regression as our baseline model, and denote it as single task learning (STL). For each method, the best model on the validation set is used to do prediction, and we report its MSE score on the test set for each task. The smaller the MSE score, the better the predictive result. The results are summarized in Table 1. As expected, when the size of the training data set is large, all the multi-task learning methods perform at least as well as the baseline STL method.\nAmong the three multi-task learning methods, FETR achieves the lowest test set mean square error. FETR can learn both covariance matrices over features and tasks simultaneously, while the other two methods can only estimate one of them. To illustrate this, we show the covariance matrices\nestimated by MTFL, MTRL and FETR in the appendix. We also test the performance of all four methods as the size of the training set increases. In order to summarize an overall performance score for better visualization, we use the log total normalized mean squared error of seven tasks on the original test data set, that is, the log sum of the mean squared error divided by the variance of the ground truth. This helps to illustrate the advantages of multi-task learning methods over single task learning as the data set gets larger. For each training set size, we repeat the experiments 10 times, by randomly sampling a subset of the corresponding size from the whole training set. We depict the mean log nMSE of all four methods as well as the standard deviation in Fig. 3.\nWe also plot the covariance matrices estimated by MTFL, MTRL and FETR to compare the correlation structures found by these algorithms; see Fig. 4. It is worth mentioning that not only does FETR learn both correlation matrices, but it also detects more correlations between features than MTFL. For example, the task correlation matrix learned by FETR exhibits a block diagonal structure, meaning that the weight vectors for the first 4 tasks are roughly uncorrelated. Such pattern is not shown in the task correlation matrix learned by MTRL. On the other hand, FETR detects more correlations among features than MTFL, as shown in Fig. 4."}, {"heading": "5. Related Work", "text": "Multi-task learning is an area of active research in machine learning and has received a lot of attention in the past years (Thrun, 1996b; Caruana, 1997; Yu et al., 2005; Bonilla et al., 2007a,b; Argyriou et al., 2007, 2008; Zhang and Yeung, 2010a). Research on multi-task learning has been carried in several strands. Early work focuses on sharing knowledge in separate tasks by sharing hidden representations, for example, hidden layer of units in neural networks (Caruana, 1997). Another stream of works constructs a Gaussian process prior over the regression functions in different tasks and the transfer of knowledge between tasks is enforced by the structure of the covariance functions/kernels of the Gaussian process prior. Bonilla et al. (2007a) presents a kernel multi-task learning approach with Gaussian process prior where task-specific features are assumed to be available. They concatenate the feature for input instance with the task feature and then define a kernel covariance function over the regression functions among different tasks. The learning is reduced to inferring the hyperparameters of the kernel function by maximizing the marginal log-likelihood of the instances by either EM or gradient ascent. This approach essentially decomposes the joint kernel function over regression functions into two separate kernels: one measures the similarity among tasks and the other measures the similarity among instances. A closely related approach in this strand is to consider a nonparametric kernel matrix over the tasks rather than a parametrized kernel function (Bonilla et al., 2007b). However, the kernel over input instances is still restricted to be parametrized in order to avoid the expensive computation of semidefinite programming over input features. In contrast, FETR optimizes both of the feature and task covariance matrices directly without assumption about their parametrized forms.\nArgyriou et al. (2008) and (Zhang and Yeung, 2010a) develop MTFL and MTRL, which can be viewed as convex frameworks of learning the feature and the task-relatedness covariance matrices, respectively. Both MTFL and MTRL are essentially convex regularization methods that exploit the feature and task relatedness by imposing trace constraints on W after proper transformations. Covariance matrices in MTFL and MTRL are not restricted to have specific parametrized forms, and from this perspective, (Evgeniou et al., 2005; Evgeniou and Pontil, 2004; Kato et al., 2008) can be understood as special cases of MTFL and MTRL where the covariance matrices are constrained to be (weighted) graph Laplacians. However, in MTFL only the feature covariance matrix is being optimized and the task-relatedness covariance matrix is assumed to be the identity, while in MTRL only the task-relatedness covariance matrix is being optimized and the feature covariance matrix is, again, assumed to be the identity. We take inspiration from these two previous works and propose a general regularization framework, FETR, that models and optimizes both task and feature relationships directly. It is worth emphasizing here that both MTFL and MTRL can be treated as special cases of FETR where one of the two covariance matrices are assumed to be the identity, which corresponds to left/right spherical matrix normal distributions. The objective function of FETR is not convex, but multi-convex. Efficient coordinate-wise minimization algorithm with closed form solutions for each subproblem is designed to tackle the multi-convex objective in FETR.\nPerhaps the most related work to FETR is Zhang and Schneider (2010). Both that work and our approach consider a multi-task learning framework where both the task and feature covariance matrices are inferred from data. However, while the authors make a sparsity assumption on the two covariance matrices, our approach is more general in the sense that we are free of this assumption. Secondly and more importantly, while in that work the optimization w.r.t the two covariance matrices employs an iterative method (either proximal method or coordinate descent) to approximate\nthe optimal solution, we obtain an efficient, non-iterative solution by reduction to a graph matching problem. Note that in order to optimize the covariance matrix using our method, one SVD is enough, while in Zhang et al. each iteration requires an SVD. This makes our approach computationally more efficient than the one used in Zhang and Schneider (2010), and statistically our approach makes less assumption than the framework proposed in Zhang and Schneider (2010)."}, {"heading": "6. Discussion and future work", "text": "We develop a multi-convex framework for multi-task learning that improves predictions by learning relationships both between tasks and between features. Our framework is a generalization of related approaches in multi-task learning, that either learn task relationships, or feature relationships. We develop a multi-convex formulation of the problem, as well as an algorithm for block coordinatewise minimization. By using the theory of doubly stochastic matrices, we are able to reduce the underlying matrix optimization subproblem into a minimum weight perfect matching problem on a complete bipartite graph, and solve it in closed form. Our method is orders of magnitude faster than an off-the-shelf projected gradient descent method, and it shows improved performance on synthetic datasets as well as on a real-world dataset for robotic modeling. While the current paper discusses our approach in the context of linear regression, it can be extended to other types of prediction tasks, such as logistic regression, linear SVM, etc."}, {"heading": "Appendix A. Proofs", "text": "A.1 Proofs of Claim 3.1\nClaim 3.1. The objective function in (6) is multi-convex.\nProof. First, it is straightforward to check that the constraint set lId \u03a31 uId and lIm \u03a32 uIm are convex. For any fixed \u03a31 and \u03a32, the objective function in terms of W can be expressed as\nm\u2211 i=1  ni\u2211 j=1 (yji \u2212wTi x j i ) 2 + \u03b7||wi||22 + \u03c1tr(\u03a31W\u03a32W T ) The first term decomposes over tasks and for each task vector wi, \u2211ni j=1(y j i \u2212wTi x j i )\n2 + \u03b7||wi||22 is a quadratic function for each wi, so the summation is also convex in W . Since \u03a31 \u2208 Sd+ and \u03a32 \u2208 Sm+ , we can further rewrite the second term above as\n\u03c1tr(\u03a31W\u03a32W T ) = \u03c1tr((\u03a3 1/2 1 W\u03a3 1/2 2 )(\u03a3 1/2 1 W\u03a3 1/2 2 ) T ) = \u03c1||\u03a31/21 W\u03a3 1/2 2 ||2F\nThis is the Frobenius norm of the matrix \u03a31/21 W\u03a3 1/2 2 , which is a linear transformation of W when both \u03a31 and \u03a32 are fixed, hence this is also a convex function of W . Overall the objective function with respect to W when both \u21261 and \u21262 are fixed is convex. For any fixed W and \u03a32, consider the objective function with respect to \u03a31:\n\u2212\u03c1m log |\u03a31|+ \u03c1tr(\u03a31C)\nwhere C = W\u03a32W T is a constant matrix. Since log |\u03a31| is concave in \u03a31 and tr(\u03a31C) is a linear function of \u03a31, it directly follows that \u2212\u03c1m log |\u03a31| + \u03c1tr(\u03a31C) is a convex function of \u03a31. A similar argument can be applied to \u03a32 as well.\nA.2 Proof of Claim 3.2\nClaim 3.2. (8) can be solved in closed form in O(m3d3 + mnd2) time; the optimal vec(W \u2217) has the following form: (\nIm \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31 )\u22121 vec(XTY ) (10)\nTo prove this claim, we need the following facts about tensor product:\nFact A.1. Let A be a matrix. Then ||A||F = ||vec(A)||2.\nFact A.2. LetA \u2208 Rm1\u00d7n1 , B \u2208 Rn1\u00d7n2 and C \u2208 Rn2\u00d7m2 . Then vec(ABC) = (CT \u2297A)vec(B).\nFact A.3. Let S1 \u2208 Rm1\u00d7n1 , S2 \u2208 Rn1\u00d7p1 and T1 \u2208 Rm2\u00d7n2 , T2 \u2208 Rn2\u00d7p2 . Then (S1 \u2297 S2)(T1 \u2297 T2) = (S1S2)\u2297 (T1T2).\nFact A.4. LetA \u2208 Rn\u00d7n andB \u2208 Rm\u00d7m. Let {\u00b51, . . . , \u00b5n} be the spectrum ofA and {\u03bd1, . . . , \u03bdm} be the spectrum of B. Then the spectrum of A\u2297B is {\u00b5i\u03bdj : 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m}.\nwe can show the following result by transforming W into its isomorphic counterpart:\nProof.\n||Y \u2212XW ||2F + \u03b7||W ||2F + \u03c1||\u03a31/21 W\u03a3 1/2 2 ||2F\n= ||vec(Y \u2212XW )||22 + \u03b7||vec(W )||22 + \u03c1||vec(\u03a31/21 W\u03a3 1/2 2 )||22 (By Fact A.1) = ||vec(Y )\u2212 (Im \u2297X)vec(W )||22 + \u03b7||vec(W )||22 + \u03c1||(\u03a31/22 \u2297 \u03a3 1/2 1 )vec(W )||22 (By Fact A.2)\n= vec(W )T (\n(Im \u2297X)T (Im \u2297X) + \u03b7Imd + \u03c1(\u03a31/22 \u2297 \u03a3 1/2 1 ) T (\u03a3 1/2 2 \u2297 \u03a3 1/2 1 )\n) vec(W )\n\u2212 2vec(W )T (Im \u2297XT )vec(Y ) + vec(Y )T vec(Y ) = vec(W )T ( (Im \u2297XTX) + \u03b7Imd + \u03c1(\u03a32 \u2297 \u03a31) ) vec(W )\n\u2212 2vec(W )T (Im \u2297XT )vec(Y ) + vec(Y )T vec(Y ) (By Fact A.3)\nThe last equation above is a quadratic function of vec(W ), from which we can read off that the optimal solution W \u2217 should satisfy:\nvec(W \u2217) = ( Im \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31 )\u22121 vec(XTY ) (21)\nW \u2217 can then be obtained simply by reformatting vec(W \u2217) into a d\u00d7m matrix. The computational bottleneck in the above procedure is in solving an md \u00d7md system of equations, which scales as O(m3d3) if no further structure is available. The overall computational complexity is O(m3d3 + mnd2).\nA.3 Proof of Thm. 3.1\nTo analyze the convergence rate of gradient descent in this case, we start by bounding the smallest and largest eigenvalue of the quadratic system shown in Eq. 21.\nLemma A.1 (Weyl\u2019s inequality). Let A,B and C be n-by-n Hermitian matrices, and C = A+B. Let a1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 an, b1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 bn and c1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 cn be the eigenvalues ofA,B andC respectively. Then the following inequalities hold for r + s\u2212 1 \u2264 i \u2264 j + k \u2212 n, \u2200i = 1, . . . , n:\naj + bk \u2264 ci \u2264 ar + bs Let \u03bbk(A) be the k-th largest eigenvalue of matrix A.\nLemma A.2. If \u03a31 and \u03a32 are feasible in (7), then\n\u03bb1(Im \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31) \u2264 \u03bb1(XTX) + \u03b7 + \u03c1u2\n\u03bbmd(Im \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31) \u2265 \u03bbd(XTX) + \u03b7 + \u03c1l2\nProof. By Weyl\u2019s inequality, setting r = s = i = 1, we have c1 \u2264 a1 + b1. Set j = k = i = n, we have cn \u2265 an + bn. We can bound the largest and smallest eigenvalues of Im \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31 as follows:\n\u03bb1(Im \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31) \u2264 \u03bb1(Im \u2297 (XTX)) + \u03bb1(\u03b7Imd) + \u03bb1(\u03c1\u03a32 \u2297 \u03a31) (By Weyl\u2019s inequality) = \u03bb1(Im)\u03bb1(X TX) + \u03b7 + \u03c1\u03bb1(\u03a31)\u03bb1(\u03a32) (By Fact A.4)\n\u2264 \u03bb1(XTX) + \u03b7 + \u03c1u2 (By the feasibility assumption)\nand\n\u03bbmd(Im \u2297 (XTX) + \u03b7Imd + \u03c1\u03a32 \u2297 \u03a31) \u2265 \u03bbmd(Im \u2297 (XTX)) + \u03bbmd(\u03b7Imd) + \u03bbmd(\u03c1\u03a32 \u2297 \u03a31) (By Weyl\u2019s inequality) = \u03bbm(Im)\u03bbd(X TX) + \u03b7 + \u03c1\u03bbm(\u03a31)\u03bbd(\u03a32) (By Fact A.4)\n\u2265 \u03bbd(XTX) + \u03b7 + \u03c1l2 (By the feasibility assumption)\nWe will first prove the following two lemmas using the fact that the spectral norm of the Hessian matrix\u22072h(W ) is bounded.\nLemma A.3. Let f(W ) : Rd\u00d7m 7\u2192 R be a twice differentiable function with \u03bb1(\u22072f(W )) \u2264 L. L > 0 is a constant. The minimum value of f(W ) can be achieved. Let W \u2217 = arg minW f(W ), then\nf(W \u2217) \u2264 f(W )\u2212 1 2L ||\u2207f(W )||2F\nProof. Since f(W ) is twice differentiable with \u03bb1(\u22072f(W )) \u2264 L, by the Lagrangian mean value theorem, \u2200W, W\u0303 , we can find a value 0 < t(W, W\u0303 ) < 1, such that\nf(W\u0303 ) = f(W ) + tr(\u2207f(W )T (W\u0303 \u2212W )) + 1 2 vec(W\u0303 \u2212W )T\u22072f(tW + (1\u2212 t)W\u0303 )vec(W\u0303 \u2212W )\n\u2264 f(W ) + tr(\u2207f(W )T (W\u0303 \u2212W )) + L 2 ||W\u0303 \u2212W ||2F\nSince W \u2217 achieves the minimum value of f(W ), we can use the above result to obtain:\nf(W \u2217) = inf W\u0303 f(W\u0303 )\n\u2264 inf W\u0303 f(W ) + tr(\u2207f(W )T (W\u0303 \u2212W )) + L 2 ||W\u0303 \u2212W ||2F = f(W )\u2212 1 2L ||\u2207f(W )||2F\nwhere the last equation comes from the fact that the minimum of a quadratic function with respect to W\u0303 can be achieved at W\u0303 = W \u2212 1L\u2207f(W ).\nLemma A.4. Let f(W ) : Rd\u00d7m 7\u2192 R be a convex, twice differentiable function with \u03bb1(\u22072f(W )) \u2264 L. L > 0 is a constant, then \u2200W1,W2:\ntr ( (\u2207f(W1)\u2212\u2207f(W2))T (W1 \u2212W2) ) \u2265 1 L ||\u2207f(W1)\u2212\u2207f(W2)||2F\nProof. For all W1,W2, we can construct the following two functions:\nfW1(Z) = f(Z)\u2212 tr ( \u2207f(W1)TZ ) , fW2(Z) = f(Z)\u2212 tr ( \u2207f(W2)TZ )\nSince f(W ) is a convex, twice differentiable function with respect toW , it follows that both fW1(Z) and fW2(Z) are convex, twice differentiable functions with respect to Z. The first-order optimality condition of convex functions gives the following conditions to hold for Z which achieves the optimality:\n\u2207fW1(Z) = \u2207f(Z)\u2212\u2207f(W1) = 0, \u2207fW2(Z) = \u2207f(Z)\u2212 f(W2) = 0\nPlug in W1 and W2 into the above optimality conditions respectively. From the first-order optimality condition we know that W1 and W2 achieves the optimal solutions of fW1(Z) and fW2(Z), respectively. Now applying Lemma A.3 to fW1(Z) and fW2(Z), we have:(\n(W2)\u2212 tr ( \u2207f(W1)TW2 )) \u2212 ( (W1)\u2212 tr ( \u2207f(W1)TW1 )) \u2265 1\n2L ||\u2207f(W1)\u2212\u2207f(W2)||2F\n( (W1)\u2212 tr ( \u2207f(W2)TW1 )) \u2212 ( (W2)\u2212 tr ( \u2207f(W2)TW2 )) \u2265 1\n2L ||\u2207f(W1)\u2212\u2207f(W2)||2F\nAdding the above two equations leads to\ntr ( (\u2207f(W1)\u2212\u2207f(W2))T (W1 \u2212W2) ) \u2265 1 L ||\u2207f(W1)\u2212\u2207f(W2)||2F\nWe can now proceed to show Thm. 3.1.\nTheorem 3.1. Let \u03bbl = \u03bbd(XTX) + \u03b7 + \u03c1l2, \u03bbu = \u03bb1(XTX) + \u03b7 + \u03c1u2 and \u03b3 = (\u03bbu\u2212\u03bbl\u03bbu+\u03bbl ) 2. Choose 0 < t \u2264 2/(\u03bbu + \u03bbl). For all \u03b5 > 0, gradient descent with step size t converges to the optimal solution within O(log1/\u03b3(1/\u03b5)) steps.\nProof. Define function g(W ) as follows:\ng(W ) = h(W )\u2212 \u03bbl 2 ||W ||2F\nSince we have already bounded that \u03bbmd(\u22072h(W )) \u2265 \u03bbl, it follows that g(W ) is a convex function and furthermore \u03bb1(\u22072g(W )) \u2264 \u03bbu\u2212 \u03bbl. Applying Lemma A.4 to g, \u2200W1,W2 \u2208 Rd\u00d7m, we have:\ntr ( (\u2207g(W1)\u2212\u2207g(W2))T (W1 \u2212W2) ) \u2265 1 \u03bbu \u2212 \u03bbl ||\u2207g(W1)\u2212\u2207g(W2)||2F\nPlug in\u2207g(W ) = \u2207h(W )\u2212\u03bblW into the above inequality and after some algebraic manipulations, we have: tr ( (\u2207h(W1)\u2212\u2207h(W2))T (W1 \u2212W2) ) \u2265 1 \u03bbu + \u03bbl ||\u2207h(W1)\u2212\u2207h(W2)||2F+ \u03bbu\u03bbl \u03bbu + \u03bbl\n||W1\u2212W2||2F (22)\nLet W \u2217 = arg minW h(W ). Within each iteration of Alg. 3, we have the update formula as W + = W \u2212 t\u2207h(W ), we can bound ||W+ \u2212W \u2217||2F as follows\n||W+ \u2212W \u2217||2F = ||W \u2212W \u2217 \u2212 t\u2207h(W )||2F = ||W \u2212W \u2217||2F + t2||\u2207h(W )||2F \u2212 2ttr ( (W \u2212W \u2217)T\u2207h(W ) ) \u2264 (1\u2212 2t \u03bbu\u03bbl\n\u03bbu + \u03bbl )||W \u2212W \u2217||2F + t(t\u2212\n2\n\u03bbu + \u03bbl )||\u2207h(W )||2F (By inequality 22)\n\u2264 (1\u2212 2t \u03bbu\u03bbl \u03bbu + \u03bbl )||W \u2212W \u2217||2F (For 0 < t \u2264 2/(\u03bbu + \u03bbl))\nApply the above inequality recursively for T times, we have\n||W (T ) \u2212W \u2217||2F \u2264 \u03b3T ||W (0) \u2212W \u2217||2F\nwhere \u03b3 = 1\u2212 2t \u03bbu\u03bbl\u03bbu+\u03bbl . For t = 2/(\u03bbu + \u03bbl), we have\n\u03b3 = 1\u2212 4\u03bbu\u03bbu/(\u03bbl + \u03bbu)2 = ( \u03bbu \u2212 \u03bbl \u03bbu + \u03bbl )2 Now pick \u2200\u03b5 > 0, setting the upper bound \u03b3T ||W (0) \u2212W \u2217||2F \u2264 \u03b5 and solve for T , we have\nT \u2265 log1/\u03b3(C/\u03b5) = O(log1/\u03b3(1/\u03b5))\nwhere C = ||W (0) \u2212W \u2217||2F is a constant.\nThe pseudocode of the gradient descent is shown in Alg. 3.\nAlgorithm 3 Gradient descent with fixed step-size. Input: Initial W , X , Y and approximation accuracy \u03b5.\n1: \u03bbu \u2190 \u03bb1(XTX) + \u03b7 + \u03c1u2. 2: \u03bbl \u2190 \u03bbd(XTX) + \u03b7 + \u03c1l2. 3: Step size t\u2190 2/(\u03bbl + \u03bbu). 4: while ||\u2207h(W )||F > \u03b5 do 5: W \u2190W \u2212 t ( XT (Y \u2212XW ) + \u03b7W + \u03c1\u03a31W\u03a32 ) . 6: end while\nA.4 Proof of Lemma 3.1\nLemma 3.1. There exists an optimal solution P to the optimization problem (18) that is a d \u00d7 d permutation matrix.\nProof. Note that the optimization problem (18) in terms of P is a linear program with the Birkhoff polytope being the feasible region. It follows from Thm. 3.2 and Thm. 3.3 that at least one optimal solution P is an d\u00d7 d permutation matrix.\nA.5 Proof of Thm. 3.4\nTheorem 3.4. The minimum value of (18) is equal to the minimum weight of a perfect matching on G = (V\u03bb, V\u03bd , E, w). Furthermore, the optimal solution P of (18) can be constructed from the minimum-weight perfect matching M on G.\nProof. By Lemma 3.1, the optimal value is achieved when P is a permutation matrix. Given a permutation matrix P , we can understand P as a bijective mapping from the index of rows to the index of columns. Specifically, construct a permutation \u03c0P : [d]\u2192 [d] from P as follows. For each row index i \u2208 [d], \u03c0P (i) = j iff Pij = 1. It follows that \u03c0P is a permutation of [d] since P is assumed to be a permutation matrix. The objective function in (18) can be written in terms of \u03c0P as\n\u03bbTP\u03bd = d\u2211 i=1 \u03bbi\u03bd\u03c0P (i)\nwhich is exactly the weight of the perfect matching on G(V\u03bb, V\u03bd , E, w) given by \u03c0P :\nw(\u03c0P ) = w({(i, \u03c0P (i)) : 1 \u2264 i \u2264 d}) = d\u2211 i=1 \u03bbi\u03bd\u03c0P (i)\nSimilarly, in the other direction, given any perfect matching \u03c0 : [d] \u2192 [d] on the bipartite graph G(V\u03bb, V\u03bd , E, w), we can construct a corresponding permutation matrix P\u03c0: P\u03c0,ij = 1 iff \u03c0(i) = j, otherwise 0. Since \u03c0 is a perfect matching, the constructed P\u03c0 is guaranteed to be a permutation matrix. Hence the problem of finding the optimal value of (18) is equivalent to finding the minimum weight perfect matching on the constructed bipartite graph G(V\u03bb, V\u03bd , E, w). Note that the above constructive process also shows how to recover the optimal permutation matrix P\u03c0\u2217 from the minimum weight perfect matching \u03c0\u2217.\nA.6 Proof of Thm. 3.5\nNote that \u03bb = (\u03bb1, . . . , \u03bbd) and \u03bd = (\u03bd1, . . . , \u03bdd) are assumed to satisfy \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and \u03bd1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bdd. To make the discussion more clear, we first make the following definition of an inverse pair.\nDefinition A.1 (Inverse pair). Given a perfect match \u03c0 of G(V\u03bb, V\u03bd , E, w), (\u03bbi, \u03bbj , \u03bdk, \u03bdl) is called an inverse pair if i \u2264 j, k \u2264 l and (v\u03bbi , v\u03bdl) \u2208 \u03c0, (v\u03bbj , v\u03bdk) \u2208 \u03c0.\nLemma A.5. Given a perfect match \u03c0 of G(V\u03bb, V\u03bd , E, w) and assuming \u03c0 contains an inverse pair (\u03bbi, \u03bbj , \u03bdk, \u03bdl). Construct \u03c0\u2032 = \u03c0\\{(v\u03bbi , v\u03bdl), (v\u03bbj , v\u03bdk)} \u222a {((v\u03bbi , v\u03bdk), (v\u03bbj , v\u03bdl)}. Then w(\u03c0\u2032) \u2264 w(\u03c0).\nProof. Let us compare the weights of \u03c0 and \u03c0\u2032. Note that since i \u2264 j, k \u2264 l, we have \u03bbi \u2265 \u03bbj and \u03bdk \u2264 \u03bdl.\nw(\u03c0\u2032)\u2212 w(\u03c0) = (\u03bbi\u03bdk + \u03bbj\u03bdl)\u2212 (\u03bbi\u03bdl + \u03bbj\u03bdk) = (\u03bbi \u2212 \u03bbj)(\u03bdk \u2212 \u03bdl) \u2264 0\nIntuitively, this lemma says that we can always decrease the weight of a perfect matching by rematching an inverse pair. Fig. 5 illustrates this process. It is worth emphasizing here that the above re-matching process only involves four nodes, i.e., v\u03bbi , v\u03bdl , v\u03bbj and v\u03bdk . In other words, the other parts of the matching stay unaffected.\nUsing Lemma A.5, we are now ready to prove Thm. 3.5:\nTheorem 3.5. Let \u03bb = (\u03bb1, . . . , \u03bbd) and \u03bd = (\u03bd1, . . . , \u03bdd) with \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and \u03bd1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bdd. The minimum-weight perfect matching on G is \u03c0\u2217 = {(v\u03bbi , v\u03bdi) : 1 \u2264 i \u2264 d} with the minimum weight w(\u03c0\u2217) = \u2211d i=1 \u03bbi\u03bdi.\nProof. We will prove by induction.\n\u2022 Base case. The base case is d = 2. In this case there are only two valid perfect matchings, i.e., {(v\u03bb1 , v\u03bd1), (v\u03bb2 , v\u03bd2)} or {(v\u03bb1 , v\u03bd2), (v\u03bb2 , v\u03bd1)}. Note that the second perfect matching {(v\u03bb1 , v\u03bd2), (v\u03bb2 , v\u03bd1)} is an inverse pair. Hence by Lemma A.5,w({(v\u03bb1 , v\u03bd1), (v\u03bb2 , v\u03bd2)}) = \u03bb1\u03bd1 + \u03bb2\u03bd2 \u2264 \u03bb1\u03bd2 + \u03bb2\u03bd1 = w({(v\u03bb1 , v\u03bd2), (v\u03bb2 , v\u03bd1)}).\n\u2022 Induction step. Assume Thm. 3.5 holds for d = n. Consider the case when d = n+ 1. Start from any perfect matching \u03c0. Check the matches of node v\u03bbn+1 and v\u03bdn+1 . Here we have two subcases to discuss:\n\u2013 If v\u03bbn+1 is matched to v\u03bdn+1 in \u03c0. Then we can remove nodes v\u03bbn+1 and v\u03bdn+1 from current graph, and this reduces to the case when n = d. By induction assumption, the minimum weight perfect matching on the new graph is given by \u2211n i=1 \u03bbi\u03bdi, so the\nminimum weight on the original graph is \u2211n i=1 \u03bbi\u03bdi + \u03bbn+1\u03bdn+1 = \u2211n+1 i=1 \u03bbi\u03bdi.\n\u2013 If v\u03bbn+1 is not matched to v\u03bdn+1 in \u03c0. Let v\u03bdj be the match of v\u03bbn+1 and v\u03bbi be the match of v\u03bdn+1 , where i 6= n + 1 and j 6= n + 1. In this case we have i < n + 1 and j < n+ 1, so (\u03bbi, \u03bbn+1, \u03bdj , \u03bdn+1) forms an inverse pair by definition. By Lemma A.5, we can first re-match v\u03bbn+1 to v\u03bdn+1 and v\u03bbi to v\u03bdj to construct a new match \u03c0\n\u2032 with w(\u03c0\u2032) \u2264 w(\u03c0). In the new matching \u03c0\u2032 we have the property that v\u03bbn+1 is matched to v\u03bdn+1 , and this becomes the above case that we have already analyzed, so we still have the minimum weight perfect matching to be \u2211n+1 i=1 \u03bbi\u03bdi.\nIntuitively, as shown in Fig. 5, an inverse pair corresponds to a cross in the matching graph. The above inductive proof basically works from right to left to recursively remove inverse pairs (crosses) from the matching graph. Each re-matching step in the proof will decrease the number of inverse pairs at least by one. The whole process stops until there is no inverse pair in the current perfect matching. Since the total number of possible inverse pairs, the above process can stop in finite steps. We illustrate the process of removing inverse pairs in Fig. 6."}], "references": [{"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "M. Pontil", "Y. Ying", "C.A. Micchelli"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Solution of the matrix equation AX+ XB= C [F4", "author": ["R.H. Bartels", "G. Stewart"], "venue": "Communications of the ACM,", "citeRegEx": "Bartels and Stewart.,? \\Q1972\\E", "shortCiteRegEx": "Bartels and Stewart.", "year": 1972}, {"title": "Introduction to linear optimization, volume 6", "author": ["D. Bertsimas", "J.N. Tsitsiklis"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "Bertsimas and Tsitsiklis.,? \\Q1997\\E", "shortCiteRegEx": "Bertsimas and Tsitsiklis.", "year": 1997}, {"title": "How and why to solve the operator equation ax- xb=", "author": ["R. Bhatia", "P. Rosenthal"], "venue": "y. Bulletin of the London Mathematical Society,", "citeRegEx": "Bhatia and Rosenthal.,? \\Q1997\\E", "shortCiteRegEx": "Bhatia and Rosenthal.", "year": 1997}, {"title": "Kernel multi-task learning using task-specific features", "author": ["E.V. Bonilla", "F.V. Agakov", "C. Williams"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Multi-task gaussian process prediction", "author": ["E.V. Bonilla", "K.M. Chai", "C. Williams"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Bayes and empirical bayes methods for data analysis", "author": ["B.P. Carlin", "T.A. Louis"], "venue": "Statistics and Computing,", "citeRegEx": "Carlin and Louis.,? \\Q1997\\E", "shortCiteRegEx": "Carlin and Louis.", "year": 1997}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Notes on birkhoff\u2013von neumann decomposition of doubly stochastic matrices", "author": ["F. Dufoss\u00e9", "B. U\u00e7ar"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Dufoss\u00e9 and U\u00e7ar.,? \\Q2016\\E", "shortCiteRegEx": "Dufoss\u00e9 and U\u00e7ar.", "year": 2016}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Evgeniou and Pontil.,? \\Q2007\\E", "shortCiteRegEx": "Evgeniou and Pontil.", "year": 2007}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Evgeniou and Pontil.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou and Pontil.", "year": 2004}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "Biconvex sets and optimization with biconvex functions: a survey and extensions", "author": ["J. Gorski", "F. Pfeuffer", "K. Klamroth"], "venue": "Mathematical Methods of Operations Research,", "citeRegEx": "Gorski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gorski et al\\.", "year": 2007}, {"title": "Matrix variate distributions, volume 104", "author": ["A.K. Gupta", "D.K. Nagar"], "venue": "CRC Press,", "citeRegEx": "Gupta and Nagar.,? \\Q1999\\E", "shortCiteRegEx": "Gupta and Nagar.", "year": 1999}, {"title": "Multi-task learning via conic programming", "author": ["T. Kato", "H. Kashima", "M. Sugiyama", "K. Asai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kato et al\\.", "year": 2008}, {"title": "Multi-task model and feature joint learning", "author": ["Y. Li", "X. Tian", "T. Liu", "D. Tao"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Multi-task feature learning via efficient l 2, 1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2013\\E", "shortCiteRegEx": "Nesterov.", "year": 2013}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": null, "citeRegEx": "Shewchuk,? \\Q1994\\E", "shortCiteRegEx": "Shewchuk", "year": 1994}, {"title": "Explanation-based neural network learning. In Explanation-Based Neural Network Learning, pages 19\u201348", "author": ["S. Thrun"], "venue": null, "citeRegEx": "Thrun.,? \\Q1996\\E", "shortCiteRegEx": "Thrun.", "year": 1996}, {"title": "Is earning the n-th thing any easier than learning the first? Advances in neural information processing", "author": ["S. Thrun"], "venue": null, "citeRegEx": "Thrun.,? \\Q1996\\E", "shortCiteRegEx": "Thrun.", "year": 1996}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM Journal on imaging sciences,", "citeRegEx": "Xu and Yin.,? \\Q2013\\E", "shortCiteRegEx": "Xu and Yin.", "year": 2013}, {"title": "Learning gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Yu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2005}, {"title": "Learning multiple tasks with a sparse matrix-normal penalty", "author": ["Y. Zhang", "J.G. Schneider"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang and Schneider.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Schneider.", "year": 2010}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": null, "citeRegEx": "Zhang and Yeung.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Yeung.", "year": 2010}, {"title": "Multi-task learning using generalized t process", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "In AISTATS,", "citeRegEx": "Zhang and Yeung.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Yeung.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Introduction Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015).", "startOffset": 88, "endOffset": 264}, {"referenceID": 11, "context": "Introduction Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015).", "startOffset": 88, "endOffset": 264}, {"referenceID": 15, "context": "Introduction Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015).", "startOffset": 88, "endOffset": 264}, {"referenceID": 17, "context": "Introduction Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015).", "startOffset": 88, "endOffset": 264}, {"referenceID": 24, "context": "Introduction Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015).", "startOffset": 88, "endOffset": 264}, {"referenceID": 16, "context": "Introduction Multi-task learning has received considerable interest in the past decades (Caruana, 1997; Evgeniou and Pontil, 2004; Argyriou et al., 2007, 2008; Kato et al., 2008; Liu et al., 2009; Zhang and Yeung, 2010a; Zhang and Schneider, 2010; Li et al., 2015).", "startOffset": 88, "endOffset": 264}, {"referenceID": 8, "context": "Another strand of work assumes common feature representations to be shared among multiple tasks, and the goal is to learn the shared representation as well as task-specific parameters simultaneously (Thrun, 1996a; Caruana, 1997; Evgeniou and Pontil, 2007; Argyriou et al., 2008).", "startOffset": 199, "endOffset": 278}, {"referenceID": 10, "context": "Another strand of work assumes common feature representations to be shared among multiple tasks, and the goal is to learn the shared representation as well as task-specific parameters simultaneously (Thrun, 1996a; Caruana, 1997; Evgeniou and Pontil, 2007; Argyriou et al., 2008).", "startOffset": 199, "endOffset": 278}, {"referenceID": 1, "context": "Another strand of work assumes common feature representations to be shared among multiple tasks, and the goal is to learn the shared representation as well as task-specific parameters simultaneously (Thrun, 1996a; Caruana, 1997; Evgeniou and Pontil, 2007; Argyriou et al., 2008).", "startOffset": 199, "endOffset": 278}, {"referenceID": 11, "context": ", 2007a) or a task similarity graph (Evgeniou and Pontil, 2004), regularizers can often be incorporated into the learning formulation to explicitly penalize hypotheses that are not consistent with the given structure.", "startOffset": 36, "endOffset": 63}, {"referenceID": 1, "context": "There have been several attempts to improve predictions along this direction by either learning the relationships between different tasks (Zhang and Yeung, 2010a), known as Multi-Task Relationship Learning (MTRL), or by exploiting the relationships between different features (Argyriou et al., 2008), which is known as Multi-Task Feature Learning (MTFL).", "startOffset": 276, "endOffset": 299}, {"referenceID": 24, "context": "Compared to the sparse regularization approach in Zhang and Schneider (2010), our framework is free of the sparsity assumption, and as a result, it admits an efficient optimization scheme where we are able to derive analytic solutions to each subproblem, whereas in (Zhang and Schneider, 2010) iterative methods have to be used for each subproblem.", "startOffset": 266, "endOffset": 293}, {"referenceID": 0, "context": "There have been several attempts to improve predictions along this direction by either learning the relationships between different tasks (Zhang and Yeung, 2010a), known as Multi-Task Relationship Learning (MTRL), or by exploiting the relationships between different features (Argyriou et al., 2008), which is known as Multi-Task Feature Learning (MTFL). Zhang and Schneider (2010) proposed a multi-task learning framework where both the task and feature relationships are inferred from data by assuming a sparse matrix-normal penalty on both the task and feature representations.", "startOffset": 277, "endOffset": 382}, {"referenceID": 0, "context": "There have been several attempts to improve predictions along this direction by either learning the relationships between different tasks (Zhang and Yeung, 2010a), known as Multi-Task Relationship Learning (MTRL), or by exploiting the relationships between different features (Argyriou et al., 2008), which is known as Multi-Task Feature Learning (MTFL). Zhang and Schneider (2010) proposed a multi-task learning framework where both the task and feature relationships are inferred from data by assuming a sparse matrix-normal penalty on both the task and feature representations. As in that paper, our multi-task learning framework is a generalization of both MTRL and MTFL, which learns the relationships both between tasks and between features simultaneously. This property is favorable for applications where we not only aim for better generalization, but also seek to have a clear understanding about the relationships among different tasks. Compared to the sparse regularization approach in Zhang and Schneider (2010), our framework is free of the sparsity assumption, and as a result, it admits an efficient optimization scheme where we are able to derive analytic solutions to each subproblem, whereas in (Zhang and Schneider, 2010) iterative methods have to be used for each subproblem.", "startOffset": 277, "endOffset": 1024}, {"referenceID": 7, "context": "We apply an empirical Bayes (Carlin and Louis, 1997) method to approximate the exact posterior distribution, leading to a learning formulation where the goal is to optimize over both the model parameters, i.", "startOffset": 28, "endOffset": 52}, {"referenceID": 14, "context": "The form of q(W | \u03a91,\u03a92) is given by q(W | \u03a91,\u03a92) =MN d\u00d7m(W | 0d\u00d7m,\u03a91,\u03a92) where MN d\u00d7m(M,A,B) denotes a matrix-variate normal distribution (Gupta and Nagar, 1999) with mean M \u2208 Rd\u00d7m, row covariance matrix A \u2208 S++ and column covariance matrix B \u2208 S++.", "startOffset": 139, "endOffset": 162}, {"referenceID": 17, "context": "It is worth pointing out that one can specify other forms of prior distributions instead of (2), as in (Liu et al., 2009) and (Zhang and Yeung, 2010b) where the authors use a Laplacian prior and a generalized t process, respectively.", "startOffset": 103, "endOffset": 121}, {"referenceID": 13, "context": "A multi-convex function is a generalization of a bi-convex function (Gorski et al., 2007) into multiple variables or blocks of variables.", "startOffset": 68, "endOffset": 89}, {"referenceID": 22, "context": "1 (Multi-convex function (Xu and Yin, 2013)).", "startOffset": 25, "endOffset": 43}, {"referenceID": 2, "context": "The third one finds the optimal solution by solving the Sylvester equation (Bartels and Stewart, 1972) characterized by the first-order optimality condition, after a proper transformation.", "startOffset": 75, "endOffset": 102}, {"referenceID": 4, "context": "In the field of control theory, a Sylvester equation (Bhatia and Rosenthal, 1997) is a matrix equation of the form AX + XB = C, where the goal is to find a solution matrix X given A,B and C.", "startOffset": 53, "endOffset": 81}, {"referenceID": 2, "context": "For example, the BartelsStewart algorithm (Bartels and Stewart, 1972) solves the Sylvester equation by first transforming A and B into Schur forms by QR factorization, and then solves the resulting triangular system via back-substitution.", "startOffset": 42, "endOffset": 69}, {"referenceID": 16, "context": "Our proof technique is adapted from Nesterov (2013) where we extend it to matrix function.", "startOffset": 36, "endOffset": 52}, {"referenceID": 9, "context": "Surprisingly, we can find a closed form optimal solution to this problem as well, using tools from the theory of doubly stochastic matrices (Dufoss\u00e9 and U\u00e7ar, 2016) and perfect bipartite graph matching.", "startOffset": 140, "endOffset": 164}, {"referenceID": 9, "context": "Since K is an orthonormal matrix, we have the following two equations: \u2211d j=1 Pij = \u2211d j=1K 2 ij = 1, \u2200i \u2208 [d], \u2211d i=1 Pij = \u2211d i=1K 2 ij = 1, \u2200j \u2208 [d], which implies that P is a doubly stochastic matrix (Dufoss\u00e9 and U\u00e7ar, 2016).", "startOffset": 204, "endOffset": 228}, {"referenceID": 3, "context": "2 (Optimality of extreme points (Bertsimas and Tsitsiklis, 1997)).", "startOffset": 32, "endOffset": 64}, {"referenceID": 10, "context": "We compare FETR with multi-task feature learning (Evgeniou and Pontil, 2007) (MTFL) and multitask relationship learning (Zhang and Yeung, 2010a) (MTRL), which can be treated as two different special cases of our model.", "startOffset": 49, "endOffset": 76}, {"referenceID": 8, "context": "Multi-task learning is an area of active research in machine learning and has received a lot of attention in the past years (Thrun, 1996b; Caruana, 1997; Yu et al., 2005; Bonilla et al., 2007a,b; Argyriou et al., 2007, 2008; Zhang and Yeung, 2010a).", "startOffset": 124, "endOffset": 248}, {"referenceID": 23, "context": "Multi-task learning is an area of active research in machine learning and has received a lot of attention in the past years (Thrun, 1996b; Caruana, 1997; Yu et al., 2005; Bonilla et al., 2007a,b; Argyriou et al., 2007, 2008; Zhang and Yeung, 2010a).", "startOffset": 124, "endOffset": 248}, {"referenceID": 8, "context": "Early work focuses on sharing knowledge in separate tasks by sharing hidden representations, for example, hidden layer of units in neural networks (Caruana, 1997).", "startOffset": 147, "endOffset": 162}, {"referenceID": 12, "context": "Covariance matrices in MTFL and MTRL are not restricted to have specific parametrized forms, and from this perspective, (Evgeniou et al., 2005; Evgeniou and Pontil, 2004; Kato et al., 2008) can be understood as special cases of MTFL and MTRL where the covariance matrices are constrained to be (weighted) graph Laplacians.", "startOffset": 120, "endOffset": 189}, {"referenceID": 11, "context": "Covariance matrices in MTFL and MTRL are not restricted to have specific parametrized forms, and from this perspective, (Evgeniou et al., 2005; Evgeniou and Pontil, 2004; Kato et al., 2008) can be understood as special cases of MTFL and MTRL where the covariance matrices are constrained to be (weighted) graph Laplacians.", "startOffset": 120, "endOffset": 189}, {"referenceID": 15, "context": "Covariance matrices in MTFL and MTRL are not restricted to have specific parametrized forms, and from this perspective, (Evgeniou et al., 2005; Evgeniou and Pontil, 2004; Kato et al., 2008) can be understood as special cases of MTFL and MTRL where the covariance matrices are constrained to be (weighted) graph Laplacians.", "startOffset": 120, "endOffset": 189}, {"referenceID": 0, "context": ", 2007a,b; Argyriou et al., 2007, 2008; Zhang and Yeung, 2010a). Research on multi-task learning has been carried in several strands. Early work focuses on sharing knowledge in separate tasks by sharing hidden representations, for example, hidden layer of units in neural networks (Caruana, 1997). Another stream of works constructs a Gaussian process prior over the regression functions in different tasks and the transfer of knowledge between tasks is enforced by the structure of the covariance functions/kernels of the Gaussian process prior. Bonilla et al. (2007a) presents a kernel multi-task learning approach with Gaussian process prior where task-specific features are assumed to be available.", "startOffset": 11, "endOffset": 570}, {"referenceID": 0, "context": ", 2007a,b; Argyriou et al., 2007, 2008; Zhang and Yeung, 2010a). Research on multi-task learning has been carried in several strands. Early work focuses on sharing knowledge in separate tasks by sharing hidden representations, for example, hidden layer of units in neural networks (Caruana, 1997). Another stream of works constructs a Gaussian process prior over the regression functions in different tasks and the transfer of knowledge between tasks is enforced by the structure of the covariance functions/kernels of the Gaussian process prior. Bonilla et al. (2007a) presents a kernel multi-task learning approach with Gaussian process prior where task-specific features are assumed to be available. They concatenate the feature for input instance with the task feature and then define a kernel covariance function over the regression functions among different tasks. The learning is reduced to inferring the hyperparameters of the kernel function by maximizing the marginal log-likelihood of the instances by either EM or gradient ascent. This approach essentially decomposes the joint kernel function over regression functions into two separate kernels: one measures the similarity among tasks and the other measures the similarity among instances. A closely related approach in this strand is to consider a nonparametric kernel matrix over the tasks rather than a parametrized kernel function (Bonilla et al., 2007b). However, the kernel over input instances is still restricted to be parametrized in order to avoid the expensive computation of semidefinite programming over input features. In contrast, FETR optimizes both of the feature and task covariance matrices directly without assumption about their parametrized forms. Argyriou et al. (2008) and (Zhang and Yeung, 2010a) develop MTFL and MTRL, which can be viewed as convex frameworks of learning the feature and the task-relatedness covariance matrices, respectively.", "startOffset": 11, "endOffset": 1757}, {"referenceID": 0, "context": ", 2007a,b; Argyriou et al., 2007, 2008; Zhang and Yeung, 2010a). Research on multi-task learning has been carried in several strands. Early work focuses on sharing knowledge in separate tasks by sharing hidden representations, for example, hidden layer of units in neural networks (Caruana, 1997). Another stream of works constructs a Gaussian process prior over the regression functions in different tasks and the transfer of knowledge between tasks is enforced by the structure of the covariance functions/kernels of the Gaussian process prior. Bonilla et al. (2007a) presents a kernel multi-task learning approach with Gaussian process prior where task-specific features are assumed to be available. They concatenate the feature for input instance with the task feature and then define a kernel covariance function over the regression functions among different tasks. The learning is reduced to inferring the hyperparameters of the kernel function by maximizing the marginal log-likelihood of the instances by either EM or gradient ascent. This approach essentially decomposes the joint kernel function over regression functions into two separate kernels: one measures the similarity among tasks and the other measures the similarity among instances. A closely related approach in this strand is to consider a nonparametric kernel matrix over the tasks rather than a parametrized kernel function (Bonilla et al., 2007b). However, the kernel over input instances is still restricted to be parametrized in order to avoid the expensive computation of semidefinite programming over input features. In contrast, FETR optimizes both of the feature and task covariance matrices directly without assumption about their parametrized forms. Argyriou et al. (2008) and (Zhang and Yeung, 2010a) develop MTFL and MTRL, which can be viewed as convex frameworks of learning the feature and the task-relatedness covariance matrices, respectively. Both MTFL and MTRL are essentially convex regularization methods that exploit the feature and task relatedness by imposing trace constraints on W after proper transformations. Covariance matrices in MTFL and MTRL are not restricted to have specific parametrized forms, and from this perspective, (Evgeniou et al., 2005; Evgeniou and Pontil, 2004; Kato et al., 2008) can be understood as special cases of MTFL and MTRL where the covariance matrices are constrained to be (weighted) graph Laplacians. However, in MTFL only the feature covariance matrix is being optimized and the task-relatedness covariance matrix is assumed to be the identity, while in MTRL only the task-relatedness covariance matrix is being optimized and the feature covariance matrix is, again, assumed to be the identity. We take inspiration from these two previous works and propose a general regularization framework, FETR, that models and optimizes both task and feature relationships directly. It is worth emphasizing here that both MTFL and MTRL can be treated as special cases of FETR where one of the two covariance matrices are assumed to be the identity, which corresponds to left/right spherical matrix normal distributions. The objective function of FETR is not convex, but multi-convex. Efficient coordinate-wise minimization algorithm with closed form solutions for each subproblem is designed to tackle the multi-convex objective in FETR. Perhaps the most related work to FETR is Zhang and Schneider (2010). Both that work and our approach consider a multi-task learning framework where both the task and feature covariance matrices are inferred from data.", "startOffset": 11, "endOffset": 3427}, {"referenceID": 24, "context": "This makes our approach computationally more efficient than the one used in Zhang and Schneider (2010), and statistically our approach makes less assumption than the framework proposed in Zhang and Schneider (2010).", "startOffset": 76, "endOffset": 103}, {"referenceID": 24, "context": "This makes our approach computationally more efficient than the one used in Zhang and Schneider (2010), and statistically our approach makes less assumption than the framework proposed in Zhang and Schneider (2010).", "startOffset": 76, "endOffset": 215}], "year": 2017, "abstractText": "In this paper we propose a multi-convex framework for multi-task learning that improves predictions by learning relationships both between tasks and between features. Our framework is a generalization of related methods in multi-task learning, that either learn task relationships, or feature relationships, but not both. We start with a hierarchical Bayesian model, and use the empirical Bayes method to transform the underlying inference problem into a multi-convex optimization problem. We propose a coordinate-wise minimization algorithm that has a closed form solution for each block subproblem. Naively these solutions would be expensive to compute, but by using the theory of doubly stochastic matrices, we are able to reduce the underlying matrix optimization subproblem into a minimum weight perfect matching problem on a complete bipartite graph, and solve it analytically and efficiently. To solve the weight learning subproblem, we propose three different strategies, including a gradient descent method with linear convergence guarantee when the instances are not shared by multiple tasks, and a numerical solution based on Sylvester equation when instances are shared. We demonstrate the efficiency of our method on both synthetic datasets and real-world datasets. Experiments show that the proposed optimization method is orders of magnitude faster than an off-the-shelf projected gradient method, and our model is able to exploit the correlation structures among multiple tasks and features.", "creator": "LaTeX with hyperref package"}}}