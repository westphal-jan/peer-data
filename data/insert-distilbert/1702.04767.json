{"id": "1702.04767", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Efficient Computation of Moments in Sum-Product Networks", "abstract": "bayesian online learning algorithms for sum - product networks ( spns ) need users to compute moments of model parameters under the one - step update posterior distribution. the best existing method intended for computing such moments scales quadratically in the size of the estimated spn, although it scales inverse linearly for trees. we propose a linear - time algorithm that works even when the spn is a directed acyclic graph ( dag ). we achieve this goal by heavily reducing the moment computation problem into a joint inference problem modelled in spns and y by finally taking advantage of a special structure of the one - step update posterior distribution : it is a multilinear polynomial with exponentially many monomials, 4 and we can evaluate moments by differentiating. the latest latter is known as generating the \\ emph { distributed differential trick }. we apply beside the proposed reduction algorithm essentially to develop a linear time assumed cumulative density filter ( adf ) for spn parameter learning. as an additional contribution, we conduct extensive quantitative experiments comparing seven different online learning algorithms for combining spns on 20 benchmark datasets. the new linear - dependency time adf method quite consistently achieves low runtime due significantly to the efficient linear - log time algorithm for moment computation computation ; however, we discover that two other methods ( today cccp and sma ) typically perform better statistically, while a third ( bmm ) is comparable according to adf. interestingly, cccp can be completely viewed as implicitly using the same alternative differentiation trick that we make explicit here. the fact that two of whom the top four fastest methods use this trick suggests that the same trick might theoretically find other uses for spn related learning algorithms in the future.", "histories": [["v1", "Wed, 15 Feb 2017 20:40:12 GMT  (42kb,D)", "http://arxiv.org/abs/1702.04767v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han zhao", "geoff gordon"], "accepted": true, "id": "1702.04767"}, "pdf": {"name": "1702.04767.pdf", "metadata": {"source": "CRF", "title": "Efficient Computation of Moments in Sum-Product Networks", "authors": ["Han Zhao", "Geoff Gordon"], "emails": ["HAN.ZHAO@CS.CMU.EDU", "GGORDON@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016). They are general-purpose inference machines over which one can perform exact joint, marginal and conditional queries in linear time in the size of the network. It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014). SPNs are also closely connected to probabilistic graphical models: by interpreting each sum node in the network as a hidden variable and each product node as a rule encoding context-specific conditional independence (Boutilier et al., 1996), every SPN can be equivalently converted into a Bayesian network where compact data structures are used to represent the local probability distributions (Zhao et al., 2015). This relationship characterizes the probabilistic semantics encoded by the network structure and allows practitioners to design principled and efficient parameter learning algorithms for SPNs (Zhao et al., 2016a,b). Most of the existing batch learning algorithms can be straightforwardly adapted to the online setting where the network updates its parameters after it receives one instance at each time step. This online learning setting makes SPNs more widely applicable in various real-world scenarios. This includes\nar X\niv :1\n70 2.\n04 76\n7v 1\n[ cs\n.L G\n] 1\n5 Fe\nb 20\n17\nthe case where either the data set is too large to store at once, or the network needs to adapt to the change of external data distributions. Recently Rashwan et al. (2016) proposed an online Bayesian Moment Matching (BMM) algorithm to learn the probability distribution of the model parameters of SPNs based on the method of moments. Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions. Empirically they show that BMM is superior to online extensions of projected gradient descent and exponentiated gradient. At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments. Specifically, BMM for SPNs works by matching the first and second order moments of the approximate tractable posterior distribution to the exact but intractable posterior. An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf. 3.2). Rashwan et al. (2016) designed a recursive algorithm to achieve this goal in linear time when the underlying network structure is a tree, and this algorithm is also used by Jaini et al. (2016) in the continuous case. However, the algorithm only works when the underlying network structure is a tree, and a naive computation of such moments in a DAG will scale quadratically w.r.t. the network size. Often this quadratic computation is prohibitively expensive even for SPNs with moderate sizes. In this paper we propose a linear time and space algorithm that is able to compute, under mild assumptions, any moments of all the network parameters simultaneously even when the underlying network structure is a DAG. The key technique in the design of our algorithm dates back to the differential approach (Darwiche, 2003) used for exact inference in Bayesian networks. This technique has also been implicitly used in the recent development of a concave-convex procedure (CCCP) for optimizing the weights of SPNs (Zhao et al., 2016b). Essentially, by reducing the moment computation problem into a joint inference problem in SPNs, we are able to exploit the fact that the network polynomial of an SPN is a multilinear function in terms of model parameters, so we can efficiently evaluate this polynomial by differentiation even if the polynomial may contain exponentially many monomials, provided that the polynomial admits a tractable circuit complexity. To illustrate this, let us start with a simple example. A formal proof will be given in Sec. 3.3.\nExample 1.1. Assume we have a multilinear function g of (w1, w2, w3, w4) as g(w1, w2, w3, w4) = w1(3w2 + 4w3) + w4(5w2 + 6w3). We want to select all the monomials inside g that contain w1. A brute force approach is to expand g, and check sequentially each monomial to see whether it contains w1 or not, and then remove those monomials that do not contain w1. But we can do better than brute force, by using the following differential trick. First, compute \u2202g\u2202w1 = 3w2 + 4w3, and then multiply back w1 into \u2202g\u2202w1 , which gives us: w1 \u2202g \u2202w1\n= w1(3w2 + 4w3). As long as g admits a tractable circuit complexity, i.e., g has a circuit representation whose size is much smaller than the total number of monomials in the polynomial, we can skip all the factors that do not contain w1 when doing the differentiation, and this will save us unnecessary computations that do not contribute to the final answer.\nAs we will see shortly, the network polynomial function computed by an SPN is indeed a multilinear function in terms of both model weights and the leaf indicators. Furthermore, the network structure gives us a natural and tractable factorization of this network polynomial, allowing us to be able to apply the above differential trick efficiently.\nWith the linear time sub-routine for computing moments, we are able to design an assumed density filter (Sorenson and Stubberud, 1968) (ADF) to learn the parameters of SPNs in an online fashion. ADF runs in linear time and space due to our efficient sub-routine. We also show that ADF and BMM can both be understood under a general framework of moment matching, where the only difference lies in the moments chosen to be matched and how to match the chosen moments. Finally, we make an extensive empirical comparison among all the online learning algorithms proposed to date for SPNs on 20 benchmark data sets in terms of both computational efficiency as well as statistical accuracy. We show that by using the proposed sub-routine for moment computations, ADF and BMM run faster than all the competitors as the data gets larger and larger. This extensive empirical comparison will also help practitioners to choose the most suitable algorithm when faced with various application scenarios."}, {"heading": "2. Preliminaries", "text": "We use [n] to abbreviate {1, 2, . . . , n}. We use a capital letter X to denote a random variable and a bold capital letter X to denote a set of random variables (random vector). Similarly, a lowercase letter x is used to denote a value taken by X and a bold lowercase letter x denotes a joint value taken by the corresponding random vector X. We use calligraphic letters to denote graphs (e.g., G). In particular, we reserve S to represent an SPN, and we use h to mean the height of S. |S| is the size of an SPN, i.e., the number of edges plus the number of nodes in the graph."}, {"heading": "2.1 Sum-Product Networks", "text": "A sum-product network S is a computational circuit over a set of random variables X = {X1, . . . , Xn}. It is a rooted directed acyclic graph. The internal nodes of S are sums or products and the leaves are univariate distributions over Xi. In its simplest form, the leaves of S are indicator variables IX=x, which can also be understood as categorical distributions whose entire probability mass is on a single value. Edges from sum nodes are parameterized with positive weights. Let x be an instantiation of the random vector X. We associate an unnormalized probability, or density, Vk(x | w) with each node k when the input to the network is x with network weights set to be w:\nVk(x | w) =  p(Xi = xi) k is a leaf node over Xi\u220f j\u2208Ch(k) Vj(x | w) k is a product node\u2211 j\u2208Ch(k)wkjVj(x | w) k is a sum node\n(1)\nwhere Ch(k) is the child list of node k in the graph and wkj is the edge weight associated with sum node k and its child node j. The probability/density of a joint assignment X = x is computed by the value at the root of S with input x divided by a normalization constant Vroot(1 | w):\np(x) = Vroot(x | w) Vroot(1 | w)\n(2)\nwhere Vroot(1 | w) is the value of the root node when all the probabilities/densities at the leaf nodes are set to be 1. This essentially corresponds to marginalizing/integrating out the random vector X, which will ensure Eq. 2 defines a proper probability distribution. Eq. 2 can also be used to compute the marginal probability of a partial assignment Y = y: simply set V at leaf nodes whose corresponding random variable is not in Y to be 1 and other leaf nodes based on the assignment\nY = y. Again, this corresponds to integrating out variables outside of the partial assignment. As a corollary, conditional probabilities can also be computed by evaluating two partial assignments. Due to this property, joint, marginal, and conditional queries can all be answered in linear time in the size of the network."}, {"heading": "2.2 Bayesian Networks and Mixture Models", "text": "We provide two alternative interpretations of SPNs that will be useful later to design our linear time moment computation algorithm. The first one relates SPNs with Bayesian networks (BNs). Informally, any complete and decomposable SPN S over X = {X1, . . . , Xn} can be converted into a bipartite BN withO(n|S|) size (Zhao et al., 2015). Each internal sum node in S corresponds to one latent variable in the constructed BN, and each leaf distribution node corresponds to one observable variable in the BN. Furthermore, the constructed BN will be a simple bipartite graph with one layer of local latent variables pointing to one layer of observable variables X. An observable variable is a child of a local latent variable if and only if the observable variable appears as a descendant of the latent variable (sum node) in the original SPN. This means that the SPN S can be understood as a BN where the number of latent variables per instance is O(|S|). Since the constructed BN is a directed bipartite graph, we know that all the local latent variables are independent from each other given that no observation is made. Or equivalently, taking a Bayesian perspective, the prior distribution over the model parameters should be fully decomposable if no observation on X is known. The second perspective is to view an SPN S as a mixture model with exponentially many mixture components (Dennis and Ventura, 2015; Zhao et al., 2016b). More specifically, we can decompose each complete and decomposable SPN S into a sum of induced trees, where each tree corresponds to a product of univariate distributions. To proceed, we first formally define what we called induced trees, which will also be useful in our later discussion and derivation.\nDefinition 2.1 (Induced tree SPN (Zhao et al., 2016a)). Given a complete and decomposable SPN S over X = {X1, . . . , Xn}, T = (TV , TE) is called an induced tree SPN from S if\n1. Root(S) \u2208 TV .\n2. If v \u2208 TV is a sum node, then exactly one child of v in S is in TV , and the corresponding edge is in TE .\n3. If v \u2208 TV is a product node, then all the children of v in S are in TV , and the corresponding edges are in TE .\nHere TV is the node set of T and TE is the edge set of T .\nIt has been shown that Def. 2.1 produces subgraphs of S that are trees as long as the original SPN S is complete and decomposable. One useful result based on the concept of induced trees is: Theorem 2.1 ((Zhao et al., 2016b)). Let \u03c4S = Vroot(1 | 1). Then Vroot(x | w) can be written as\u2211\u03c4S t=1 \u220f (k,j)\u2208TtE wkj \u220fn i=1 pt(Xi = xi), where Tt is the tth unique induced tree of S and pt(Xi) is a univariate distribution over Xi in Tt as a leaf node.\nIn Thm. 2.1, \u03c4S equals the number of induced trees in S . Without loss of generality assuming that sum layers alternate with product layers in S , we have \u03c4S = \u2126(2h), where h is the height of S .\nHence the mixture model represented by S has number of mixture components that is exponential in the height of S. Thm. 2.1 characterizes both the number of components and the form of each component in the mixture model, as well as their mixture weights. For the convenience of later discussion, we call Vroot(x | w) the network polynomial of S.\nCorollary 2.1. The network polynomial Vroot(x | w) is a multilinear function of w with positive coefficients on each monomial.\nCorollary 2.1 holds since each monomial corresponds to an induced tree and each edge only appears at most once in the tree. Such a polynomial function is also a special case of a posynomial where the exponents of all the variables are at most 1. This property will be crucial and useful in our derivation of a linear time algorithm for moment computation in SPNs."}, {"heading": "2.3 Differential Approach for Inference in Bayesian Networks", "text": "We briefly introduce the idea of performing inference in BNs using the differential trick we mentioned above. This novel idea is due to Darwiche (2003). We also refer readers to Darwiche (2003) for a more complete and detailed discussion of this approach. We start from a simple BN with 2 binary random variables. Using indicator variable notations, we can represent the joint probability distribution of an BN as the following polynomial function:\ng(X1, X2) = p00Ix\u03041Ix\u03042 + p01Ix\u03041Ix2 + p10Ix1Ix\u03042 + p11Ix1Ix2\nwhere pij = Pr(X1 = i,X2 = j), \u2200i, j \u2208 {0, 1}. Given the joint probability distribution, the above function g can be viewed as a polynomial in terms of the boolean indicator variables over X1 and X2. Several properties can be observed from such polynomials: 1). g(X) = Pr(X). 2). For an BN with n random variables, there are 2n monomials in g. 3). g(X) is a multilinear function in terms of the boolean indicators. 4). g(X) is a homogeneous polynomial, that is, each monomial has the same degree, which is n for an BN with n random variables. Let consider the probabilistic semantics of partial differentiation of g w.r.t. a specific indicator, say, Ix1 :\n\u2202g\n\u2202Ix1 (X1, X2) = p10Ix\u03042 + p11Ix2 = Pr(X1 = 1, X2)\ni.e., the partial differentiation of the network polynomial w.r.t. Ix1 corresponds to evaluating the joint probability by fixing X1 = 1. Note that the above equality holds due to property 1) and 3) listed above. In general, in a BN B with joint distribution Pr(X), we have the following theorem hold:\nTheorem 2.2. (Darwiche, 2003) Let B be a Bayesian network representing probability distribution Pr(X) and having polynomial g. For every random variable X and instantiation x of X, we have:\n\u2202g \u2202Ix (x) = Pr(X = x,x\u2212X)\nwhere x\u2212X means the restriction of x on X\\{X}.\nIn words, if we differentiate the polynomial g w.r.t. indicator Ix and evaluate the result at evidence x, we obtain the probability of instantiation x,x \u2212 X . The above theorem can be extended to second-order derivative, or any higher order derivative, as well:\nCorollary 2.2. Let B be a Bayesian network representing probability distribution Pr(X) and having polynomial g. For every pair of random variable Xi 6= Xj and instantiation x of X, we have:\n\u22022g\n\u2202Ixi\u2202Ixj (x) = Pr(Xi = xi, Xj = xj ,x\u2212Xi \u2212Xj)\nMarginal probability and conditional probability can also be computed via partial differentiation:\nCorollary 2.3. For every random variable X and instantiation x of X, we have:\u2211 x \u2202g \u2202Ix (x) = Pr(x\u2212X)\n\u2202g \u2202Ix\u2032 (x)\u2211 x \u2202g \u2202Ix (x) = Pr(x\u2032 | x\u2212X)\nThe above theorem and corollaries show that we can compute specific forms of joint, marginal and conditional probabilities via partial differentiation w.r.t. specific indicators. However, due to property 2), one may ask what advantages does this alternative computation provide us since there are exponentially many monomials in g and the cost of a naive differentiation scales linearly in the number of monomials? The insight here is that for many distributions of practical interests we can often find arithmetic circuits/sum-product networks whose size are only polynomial in n. Formally, the circuit complexity (Darwiche, 2003) of a BN B is the size of the smallest arithmetic circuit that computes the network polynomial of B. Hence for distributions with tractable circuit complexity, we can compute probabilistic queries efficiently as long as we can perform partial differentiation on the circuit efficiently. As we will see shortly, this is the case for both arithmetic circuits and sum-product networks."}, {"heading": "3. Linear Time Exact Moment Computation", "text": ""}, {"heading": "3.1 Exact Posterior Has Exponentially Many Modes", "text": "Let M be the number of sum nodes in S. Suppose we are given a fully factorized prior distribution p0(w;\u03b1) = \u220fM k=1 p0(wk;\u03b1k) over w. It is worth pointing out the fully factorized prior distribution is well justified by the bipartite graph structure of the equivalent BN we introduced in section 2.2. We are interested in computing the moments of the posterior distribution after we receive one observation from the world. Essentially, this is the Bayesian online learning setting where we update the belief about the distribution of model parameters as we observe data from the world sequentially. Note that wk corresponds to the weight vector associated with sum node k, so wk is a vector that satisfies wk > 0 and 1Twk = 1. For the ease of discussion let us assume that the prior distribution for each wk is Dirichlet, i.e.,\np0(w;\u03b1) = M\u220f k=1 Dir(wk;\u03b1k) = M\u220f k=1\n\u0393( \u2211\nj \u03b1k,j)\u220f j \u0393(\u03b1k,j) \u220f j w \u03b1k,j\u22121 k,j\nAfter observing one instance x, we have the exact posterior distribution to be:\np(w | x) = p0(w;\u03b1)p(x | w) p(x)\n(3)\nLet Zx , p(x) and realize that the network polynomial function also computes the likelihood p(x | w). Plugging the expression for the prior distribution as well as the network polynomial into the above Bayes formula, we have\np(w | x) = 1 Zx \u03c4S\u2211 t=1 M\u220f k=1 Dir(wk;\u03b1k) \u220f\n(k,j)\u2208TtE\nwk,j n\u220f i=1 pt(xi)\nSince Dirichlet is a conjugate distribution to the multinomial, each term in the summation is an updated Dirichlet with a multiplicative constant. So, the above equation suggests that the exact posterior distribution becomes a mixture of \u03c4S Dirichlets after one observation. In a data set of D instances, the exact posterior will become a mixture of \u03c4DS components, which is intractable to maintain in practice since \u03c4S = \u2126(2h). The hardness of maintaining the exact posterior distribution appeals for an approximate scheme where we can sequentially update our belief about the distribution while at the same time efficiently maintain the approximation. Assumed density filtering (Sorenson and Stubberud, 1968) is such a framework: the algorithm chooses an approximate distribution from a tractable family of distributions after observing each instance. A typical choice is to match the moments of an approximation to the exact posterior."}, {"heading": "3.2 The Hardness of Computing Moments", "text": "In order to find an approximate distribution to match the moments of the exact posterior, we need to be able to compute those moments under the exact posterior. This is not a problem for traditional mixture models including mixture of Gaussians, latent Dirichlet allocation, etc., since the number of mixture components in those models are assumed to be small. However, this is not the case for SPNs, where the effective number of mixture components is \u03c4S = \u2126(2h). In this section we will show how to use the network polynomial of S to reduce this complexity to O(|S|). To simplify the notation, for each t \u2208 [\u03c4S ], we define ct , \u220fn i=1 pt(xi) 1 and\nut , \u222b w p0(w) \u220f (k,j)\u2208TtE wk,j dw\nThat is, ct corresponds to the product of leaf distributions in the tth induced tree Tt, and ut is the moment of \u220f (k,j)\u2208TtE wk,j , i.e., the product of tree edges, under the prior distribution p0(w). Realizing that the posterior distribution needs to satisfy the normalization constraint, we have:\n\u03c4S\u2211 t=1 ct \u222b w p0(w) \u220f (k,j)\u2208TtE wk,j dw = \u03c4S\u2211 t=1 ctut = Zx (4)\nNote that the prior distribution for a sum node is a Dirichlet distribution. In this case we can compute a closed form expression for ut as:\nut = \u220f\n(k,j)\u2208TtE\n\u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032\n(5)\n1. We omit the explicit dependency of ct on the instance x .\nMore generally, let f(\u00b7) be a function applied to each edge weight in an SPN. We use the notation Mp(f) to mean the moment of function f evaluated under distribution p. For example, if we let f(wk,j) = wk,j , then Mp(f) corresponds to the first order moment; or if we let f(wk,j) = logwk,j , then Mp(f) corresponds to the log moment, etc. We are interested in computing Mp(f) where p = p(w | x), which we call the one-step update posterior distribution. More specifically, for each edge weight wk,j , we would like to compute the following quantity:\nMp(f(wk,j)) = \u222b w f(wk,j)p(w | x) dw = 1 Zx \u03c4S\u2211 t=1 ct \u222b w p0(w)f(wk,j) \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 dw (6)\nWe first note that Eq. 6 is not trivial to compute as it involves \u03c4S = \u2126(2h) terms. Furthermore, in order to conduct moment matching, we need to compute the above moment for each edge (k, j) from a sum node. A naive computation will lead to a total time complexity \u2126(|S|2h). A linear time algorithm that uses dynamic programming to compute these moments has been designed by Rashwan et al. (2016) when the underlying structure of S is a tree. The algorithm recursively computes the above moments in a top-down fashion along the tree. However, this algorithm breaks down when the graph is a DAG."}, {"heading": "3.3 Efficient Polynomial Evaluation by Differentiation", "text": "In what follows we will present a O(|S|) time and space algorithm that is able to compute all the moments simultaneously for general SPNs with DAG structures. We will first show that the moment in Eq. 6 can be expressed as a convex combination of two easily computed moments, then we proceed to use the differential trick to efficiently compute those convex coefficients. Let us first compute Eq. 6 for a fixed edge (k, j). Our strategy is to partition all the induced trees based on whether they contain the tree edge (k, j) or not. Define TF = {Tt | (k, j) 6\u2208 Tt, t \u2208 [\u03c4S ]} and TT = {Tt | (k, j) \u2208 Tt, t \u2208 [\u03c4S ]}. In other words, TF corresponds to the set of trees that do not contain edge (k, j) and TT corresponds to the set of trees that contain edge (k, j). Then,\n(6) = 1\nZx \u2211 Tt\u2208TT ct \u222b w p0(w)f(wk,j) \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 dw\n+ 1\nZx \u2211 Tt\u2208TF ct \u222b w p0(w)f(wk,j) \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 dw\nFor the induced trees that contain edge (k, j), we have\n1\nZx \u2211 Tt\u2208TT ct \u222b w p0(w)f(wk,j) \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 dw\n= 1\nZx \u2211 Tt\u2208TT ct \u222b w\u2212k p\u22120,k(w\u2212k) \u220f\n(k\u2032,j\u2032)\u2208TtE (k\u2032,j\u2032)6=(k,j)\nwk\u2032,j\u2032 dw\u2212k \u00d7 \u222b wk p0,k(wk)wk,jf(wk,j) dwk\n= 1\nZx \u2211 Tt\u2208TT ctutMp\u20320,k(f(wk,j))\nwhere p\u20320,k is the one-step update posterior Dirichlet distribution for sum node k after absorbing the term wk,j , and w\u2212k = w\\{wk}. Similarly, for the induced trees that do not contain the edge (k, j), we have:\n1\nZx \u2211 Tt\u2208TF ct \u222b w p0(w)f(wk,j) \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 dw\n= 1\nZx \u2211 Tt\u2208TF ct \u222b w\u2212k p\u22120,k(w\u2212k) \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 dw\u2212k \u00d7 \u222b wk p0,k(wk)f(wk,j) dwk\n= 1\nZx \u2211 Tt\u2208TF ctutMp0,k(f(wk,j))\nwhere p\u22120,k = p0/p0,k and p0,k is the prior Dirichlet distribution for sum node k. The second equation holds by changing the order of integration. The third equation holds because (k, j) is not in tree Tt so that \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 does not contain the term wk,j . Note that both Mp0,k(f) and Mp\u20320,k(f) are independent of specific induced trees, so we can combine the above two parts to express Mp(f) as:\nMp(f) =  1 Zx \u2211 Tt\u2208TF ctut Mp0,k(f) +  1 Zx \u2211 Tt\u2208TT ctut Mp\u20320,k(f) (7) From Eq. 4 we have\n1\nZx \u03c4S\u2211 t=1 ctut = 1 and \u03c4S\u2211 t=1 ctut = \u2211 Tt\u2208TT ctut + \u2211 Tt\u2208TF ctut\nThis implies that Mp(f) is in fact a convex combination of Mp0,k(f) and Mp\u20320,k(f). So in order to compute Eq. 6, we need to be able to compute the two coefficients efficiently. Recall that for each induced tree Tt, we have the expression of ut as\nut = \u220f\n(k,j)\u2208TtE\n\u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032\nThe term \u2211\u03c4S\nt=1 ctut can be expressed as\n\u03c4S\u2211 t=1 ctut = \u03c4S\u2211 t=1 \u220f (k,j)\u2208TtE \u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032 n\u220f i=1 pt(xi) (8)\nLemma 3.1. \u2211\u03c4S\nt=1 ctut can be computed in O(|S|) time and space in a bottom-up evaluation pass of S .\nProof. Compare the form of Eq. 8 to the network polynomial:\np(x|w) = Vroot(x | w) = \u03c4S\u2211 t=1 \u220f (k,j)\u2208TtE wk,j n\u220f i=1 pt(xi) (9)\nClearly Eq. 8 and Eq. 9 share the same functional form and the only difference lies in that the edge weight used in Eq. 8 is given by \u03b1k,j\u2211\nj\u2032 \u03b1k,j\u2032 while the edge weight used in Eq. 9 is given by wk,j , both\nof which are constrained to be positive and locally normalized. This implies that in order to compute the value of Eq. 8, we can replace all the edge weightswk,j with \u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032\nin S, and a bottom-up pass evaluation of S will give us the desired result at the root of the network. In other words, we reduce the original problem to a joint inference problem in S with a set of weights determined by \u03b1. The linear time and space complexity then follows from the linear time and space inference complexity of SPNs.\nTo evaluate Eq. 7, we also need to compute \u2211 Tt\u2208TT ctut efficiently. Since \u2211 Tt\u2208TT ctut contains a subset of monomials that contain wk,j in Zx, a brute force computation is infeasible in the worst case. The key observation is that we can use the differential trick to solve this problem by realizing the fact that Zx is a multilinear function in\n\u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032 , \u2200k, j and has a compact factorization.\nLemma 3.2. \u2211 Tt\u2208TT ctut and \u2211 Tt\u2208TF ctut can be computed in O(|S|) time and space in a topdown differentiation pass of S. Proof. Define wk,j , \u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032 , then\n\u2211 Tt\u2208TT ctut = \u2211 Tt\u2208TT \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 n\u220f i=1 pt(xi)\n= wk,j \u2211 Tt\u2208TT \u220f (k\u2032,j\u2032)\u2208TtE (k\u2032,j\u2032)6=(k,j) wk\u2032,j\u2032 n\u220f i=1 pt(xi) + 0 \u00b7 \u2211 Tt\u2208TF ctut\n= wk,j  \u2202 \u2202wk,j \u2211 Tt\u2208TT \u220f (k\u2032,j\u2032)\u2208TtE wk\u2032,j\u2032 n\u220f i=1 pt(xi) + \u2202 \u2202wk,j \u2211 Tt\u2208TF ctut  = wk,j  \u2202 \u2202wk,j \u2211 Tt\u2208TT ctut + \u2202 \u2202wk,j \u2211 Tt\u2208TF ctut\n = wk,j ( \u2202\n\u2202wk,j \u03c4S\u2211 t=1 ctut ) where the second equality is by Corollary 2.1 that the network polynomial is a multilinear function of wk,j and the third equality holds because TF is the set of trees that do not contain wk,j . The rest of the equalities follow by simple algebraic transformations. In summary, the above lemma holds because of the fact that differential operator applied to a multilinear function acts as a selector for all the monomials containing a specific variable, as illustrated in Example 1.1. Hence,\u2211\nTt\u2208TF\nctut = \u03c4S\u2211 t=1 ctut \u2212 \u2211 Tt\u2208TT ctut\ncan also be computed. To show the linear time and space complexity, recall that the differentiation w.r.t. wk,j can be efficiently computed by back-propagation in a top-down pass of S once we have computed \u2211\u03c4S t=1 ctut in a bottom-up pass of S.\nDefine Dk(x | w) = \u2202Vroot(x | w)/\u2202Vk(x | w). Then the differentiation term \u2202 \u2211\u03c4S t=1 ctut \u2202wk,j\ncan be computed via back-propagation in a top-down pass of the network as follows:\n\u2202 \u2211\u03c4S\nt=1 ctut \u2202wk,j = \u2202Vroot(x | w) \u2202Vk(x | w) \u2202Vk(x | w) \u2202wk,j = Dk(x | w)Vj(x | w) (10)\nTheorem 3.1. For each edge (k, j), Eq. 7 can be computed in O(|S|) time and space.\nProof. The proof simply follows from Lemma 3.1 and Lemma 3.2 with the assumption that the moments under the prior has closed form solution.\nLet \u03bbk,j = (wk,jVj(x | w)Dk(x | w)) /Vroot(x | w) and fk,j = f(wk,j). The final formula for the exact moment of wk,j under the one-step udpate posterior p is given by\nMp(fk,j) = (1\u2212 \u03bbk,j)Mp0(fk,j) + \u03bbk,jMp\u20320(fk,j) (11)\nRemark 1. Recall that \u2200k, j, we have 0 \u2264 \u03bbk,j \u2264 1:\n\u03bbk,j = 1\nZx \u2211 Tt\u2208TT ctut = 1 p(x | w) \u2211 Tt\u2208TT \u220f (k,j)\u2208TtE wk,j n\u220f i=1 pt(xi) (12)\nwhere we remind the readers that TT is the set of induced trees that contain the edge (k, j). Essentially, Eq. 12 means that \u03bbk,j is the ratio of all the induced trees that contain edge (k, j) to the network. Roughly speaking, this measures how important the contribution of a specific edge is to the whole network polynomial. As a result, we can interpret Eq. 11 as follows: the more important the edge is, the more portion of the moment comes from the new observation. Remark 2. CCCP for SPNs was originally derived using a sequential convex relaxation technique, where in each iteration a concave surrogate function is constructed and optimized. The key update in each iteration of CCCP (Zhao et al. (2016b) Eq. (7)) is given as follows:\nw\u2032kj \u221d wkj Vj(x | w) Vroot(x | w) Dk(x | w) (13)\nNote that the R.H.S. of Eq. 13 is exactly the same as \u03bbk,j defined in Eq. 11. From this perspective, CCCP can also be understood as implicitly applying the differential trick to compute \u03bbk,j , i.e., the relative importance of edge (k, j), and then take updates according to this importance measure. Remark 3. Note that \u03bbk,j only depends on three terms, i.e., the forward evaluation value Vj , the backward differentiation value Dk and the original weight of the edge wk,j . Readers familiar with backpropagation in neural networks may find this functional form to be similar to the delta rule (Hecht-Nielsen et al., 1988), where the weight update for each neuron depends on the forward input value, the backward error signal as well as the strength of the connection between two neurons. This is quite interesting by itself as it makes a more close connection of SPNs to traditional neural networks. In fact, one can indeed interpret SPNs as a special kind of feed-forward neural networks with two kinds of activation functions, and sparse connections between neurons to satisfy the structural constraints imposed by the completeness and decomposability. Finally, we summarize the linear time algorithm for moment computation in Alg. 1.\nAlgorithm 1 Linear Time Exact Moment Computation Input: Prior p0(w | \u03b1), moment f , SPN S and input x. Output: Mp(f(wk,j)), \u2200(k, j).\n1: wk,j \u2190 \u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032\n,\u2200(k, j). 2: Compute Mp0(f(wk,j)) and Mp\u20320(f(wk,j)),\u2200(k, j). 3: Bottom-up evaluation pass of S with input x. Record Vk(x | w) at each node k. 4: Top-down differentiation pass of S with input x. Record Dk(x | w) at each node k. 5: Compute the exact moment for each (k, j) based on Eq. 11.\nCorollary 3.1. Alg. 1 runs in O(|S|) time and space.\nProof. To see this, from Eq. 10, we only need to cache the forward evaluation value Vj and the backward differentiation value Dk for each edge (k, j). This can be achieved by two additional copies of the network. Note that line 2 of Alg. 1 can be computed in O(|S|) for all the edges. Line 3-5 can all be computed in O(|S|), so the corollary follows."}, {"heading": "4. Online Moment Matching", "text": "In this section we use Alg. 1 as a sub-routine to develop a new Bayesian online learning algorithm for SPNs based on assumed density filtering (Sorenson and Stubberud, 1968). To do so, we find an approximate distribution by minimizing the KL divergence between the one-step update posterior and the approximate distribution. Let P = {q | q = \u220fM k=1 Dir(wk;\u03b2k)}, i.e., P is the space of product of Dirichlet densities that are decomposable over all the sum nodes in S. Note that since p0(w;\u03b1) is fully decomposable, we have p0 \u2208 P . One natural choice is to try to find an approximate distribution q \u2208 P such that q minimizes the KL-divergence between p(w|x) and q, i.e.,\np\u0302 = arg min q\u2208P\nKL(p(w | x) || q) (14)\nIt is not hard to show (proof in appendix) that when q is an exponential family distribution, which is the case in our setting, the above minimization problem corresponds to solving the following moment matching equation:\nEp(w|x)[T (wk)] = Eq(w)[T (wk)] (15)\nwhere T (wk) is the vector of sufficient statistics of q(wk). When q(\u00b7) is a Dirichlet, we have T (wk) = logwk. This principle of finding an approximate distribution is also known as reverse information projection in the literature of information theory (Csisza\u0301r and Matus, 2003).2 By utilizing our efficient linear time algorithm for exact moment computation, we propose a Bayesian online learning algorithm for SPNs based on the above moment matching principle, called ADF. The pseudocode is shown in Alg. 2. Details on how to solve the moment matching equation (Eq. 15) are presented in appendix. As a note, we can also extend the above ADF algorithm to the batch setting via a recently proposed technique known as stochastic expectation propagation (Li et al., 2015).\n2. As a comparison, information projection corresponds to minimizing KL(q || p(w | x)) within the same family of distributions q \u2208 P . Finding an approximate distribution for SPNs based on information projection has recently been studied by Zhao et al. (2016a), and an algorithm called CVB-SPN is proposed therein.\nAlgorithm 2 Assumed Density Filtering for SPN Input: Prior p0(w | \u03b1), SPN S and input {xi}\u221ei=1.\n1: p(w)\u2190 p0(w | \u03b1) 2: for i = 1, . . . ,\u221e do 3: Apply Alg. 1 to compute Ep(w|xi)[logwk,j ] for all edges (k, j). 4: Find p\u0302 = arg minq\u2208P KL(p(w | xi) || q) by solving the moment matching equation Eq. 15. 5: p(w)\u2190 p\u0302(w). 6: end for"}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1 Experimental Setting", "text": "We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015). Statistics about the 20 data sets and their corresponding SPN models are shown in Table 1. All the random variables in these 20 data sets are binary. We use LearnSPN as the structure learning algorithm to build structures for the 20 data sets. Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures. Note that more complicated and compression-efficient post-processing strategy exists for SPNs (Rahman and Gogate, 2016). We emphasize how we are not interested in obtaining the most compact SPN representations. Instead, we would like to investigate how the proposed linear time moment computation algorithm scales on general SPNs with DAG structures, compared with the existing quadratic moment computation algorithm. As the last step, we equivalently transform the network structures by removing consecutive sum nodes or product nodes, using the same technique introduced by Vergari et al. (2015). This step helps to reduce the size of the networks while keeping the distributions unchanged. As a result, the size of the reduced network is roughly twice the number of parameters to be estimated, as shown in Table 1.\nIn the experiments we aim to show that using the proposed linear time algorithm for computing desired moments of the one-step update posterior, both BMM and ADF can run in linear time, having the same order of time complexity as the other online learning algorithms for SPNs. ADF and BMM are methods based on the general principle of moment matching, but they differ at which moments to match. On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al., 2016b), concave-convex procedure/expectation maximization (CCCP) (Peharz, 2015; Zhao et al., 2016b), collapsed variational Bayes (CVB) (Zhao et al., 2016a), online Bayesian moment matching (BMM) (Rashwan et al., 2016) and assumed density filtering (ADF). We compare both the training time and the test set log-likelihood scores of those online learning algorithms, evaluating both the computational efficiency as well as the statistical performance. We implement all the above learning algorithms in C++ and conduct our experiments on a server with 32 E5-2650 2.00GHz CPUs. For\nBayesian online learning algorithms, we use their posterior means as the estimators to evaluate on test data sets, and we use uniform Dirichlets as priors."}, {"heading": "5.2 Results", "text": "All the SPNs used in our experiments are DAGs, by using the merging techniques introduced in the last section. As a result, the moment computation algorithm introduced in Rashwan et al. (2016) for trees cannot be applied here. A naive algorithm for moment computation on DAGs scales quadratically in the size of the network, and it does not terminate within 24 hours even for the smallest SPN (NLTCS). As a comparison, by using Alg. 1 to compute the exact moments of the posterior distribution after each observation, both BMM and ADF can be run in linear time in the size of the network as well as the size of the training data set. Alg. 1 significantly broadens the applicability of the method of moments to the set of SPNs used in practice. We plot the runtime of different online learning algorithms on the 20 data sets, and sort them according to the time used. From Fig. 1 it is clear that the runtime of these 7 algorithms are within the same order. This is as expected since all the 7 algorithms have linear time complexity. In practice BMM and ADF are faster than all the other methods except CCCP. For example, on the netflix data set, both ADF and BMM uses around 55 hours while SMA uses around 84 hours. All the experiments are restricted to run in 84 hours. None of the 7 online algorithms finishes within 84 hours on the 20Newsgp data set.\nWe summarize the test set log-likelihood scores for the 7 algorithms in Table 2. Among all the algorithms, online CCCP achieves the best performance on 15 out of 20 data sets. Online CVB is second to CCCP, obtaining 4 highest test set scores. BMM and ADF are comparable to each other, while both of these two algorithms perform slightly worse than SMA. Except PGD, all the other algorithms admit multiplicative updates. Based on our experiments, EG is the one that is most\nsensitive to the choice of learning rates, while BMM and ADF are free of learning rate. The fact that both ADF and BMM usually perform worse than CCCP and SMA can be understood from their design principles: while CCCP and SMA are optimization based algorithms that are explicitly designed to maximize the log-likelihood scores on the training set, BMM and ADF are projection based algorithms by moment matching. By the concentration of the log-likelihood scores, one may also expect CCCP and SMA to have good log-likelihood scores on the test set provided these two algorithms achieve good scores on the training data set. On the other hand, both ADF and BMM work by matching specific moments, and this usually provides a tradeoff between computational efficiency and statistical performance. As a take-home-message, we recommend CCCP (if one does not have any prior information about the weights) and CVB (if one has useful prior information about the weights) if one is more interested in building good generative models, or BMM/ADF if one cares more about the speed of learning."}, {"heading": "6. Conclusion", "text": "We propose a linear time algorithm to efficiently compute the moments of model parameters in SPNs. The key technique used in the design of our algorithm is the differential trick that is able to efficiently evaluate a multilinear function given that the network polynomial admits a tractable factorization. Using the proposed algorithm as a sub-routine, we are able to improve the time complexity of BMM from quadratic to linear on general SPNs with DAG structures. We also use this sub-routine to design a new online algorithm, ADF. Extensive experiments are conducted to evaluate the online learning algorithms proposed to date. We show that by using the proposed moment computation algorithm ADF and BMM are faster than all the competitors except CCCP. As a future direction, we hope to apply the proposed moment computation algorithm in the design of efficient structure learning algorithms for SPNs. We also expect that the same differential trick that we make explicit here might find other uses for SPN learning in the future."}, {"heading": "Appendix A. Details about Solving Moment Matching Equations in BMM and ADF", "text": "In the appendix we show that minimizing the KL divergence between the one-step update posterior distribution and the approximate distribution leads to a moment matching equation to be solved when the approximate q comes from the exponential family distributions. We will also show in detail the system of equations to be solved in both BMM and ADF for SPNs. Starting from the approximate distribution q = q(w; \u03b8) with natural parameter \u03b8, since q is assumed to be an exponential family distribution, we have\nq(w; \u03b8) = h(w)g(\u03b8) exp(\u03b8TT (w))\nwhere T (w) is the sufficient statistics of the exponential family distribution. Consider the minimization problem over q, we have\nmin q\u2208P KL(p(w | x) || q) = min q\u2208P \u222b w p(w | x) log p(w | x) q(w; \u03b8) dw\n\u21d4 max \u03b8 \u222b w p(w | x) log h(w)g(\u03b8) exp(\u03b8TT (w)) dw\n\u21d4 max \u03b8 \u222b w p(w | x) ( log g(\u03b8) + \u03b8TT (w) ) dw\nNote that g(\u03b8), the partition function, of q is guaranteed to be log-concave since q is assumed to be an exponential family distribution. It follows that the above maximization problem is a smooth concave maximization problem where the optimal \u03b8\u2217 must have 0 gradient. Differentiating the above function w.r.t. \u03b8, leading to the following equation to be solved:\n\u2212\u2207\u03b8 log g(\u03b8) = Ep(w|x)[T (w)]\nRealizing \u2212\u2207\u03b8 log g(\u03b8) = Eq(w)[T (w)], we arrive at the moment matching Eq. 15:\nEp(w|x)[T (wk)] = Eq(w)[T (wk)]\nFor q being a product of Dirichlets, we have T (wk) = logwk, where the log is understood to be taken elementwisely. In the ADF algorithm, for each edge wk,j the above moment matching equation amounts to solving the following equation:\n\u03c8(\u03b8k,j)\u2212 \u03c8( \u2211 j\u2032 \u03b8w,j\u2032) = Ep(w|x)[logwk,j ]\nwhere \u03c8(\u00b7) is the digamma function. This is a system of nonlinear equation about \u03b8 where the R.H.S. of the above equation can be computed using Alg. 1 in O(|S|) time for all the edges (k, j).\nTo efficiently solve it, we take exp(\u00b7) at both sides of the equation and approximate the L.H.S. using the fact that exp(\u03c8(\u03b8k,j)) \u2248 \u03b8k,j \u2212 12 for \u03b8k,j > 1. Expanding the R.H.S. of the above equation using the identity from Eq. 11, we have:\nexp \u03c8(\u03b8k,j)\u2212 \u03c8(\u2211 j\u2032 \u03b8w,j\u2032)  = exp (Ep(w|x)[logwk,j ])\n\u21d4 \u03b8k,j \u2212 12\u2211 j\u2032 \u03b8k,j\u2032 \u2212 1 2 = ( \u03b1k,j \u2212 12\u2211 j\u2032 \u03b1k,j\u2032 \u2212 1 2 )(1\u2212\u03bbk,j) \u00d7 ( \u03b1k,j + 1 2\u2211 j\u2032 \u03b1k,j\u2032 + 1 2 )\u03bbk,j (16)\nNote that \u03b1k,j\u2212 1 2\u2211\nj\u2032 \u03b1k,j\u2032\u2212 1 2\nis approximately the mean of the prior Dirichlet under p0 and \u03b1k,j+\n1 2\u2211\nj\u2032 \u03b1k,j\u2032+ 1 2\nis approximately the mean of p\u20320, where p \u2032 0 is the posterior by adding one pseudo-count to wk,j . So Eq.16 is essentially finding a posterior with hyperparameter \u03b8 such that the posterior mean is approximately the weighted geometric mean of the means given by p0 and p\u20320, weighting by \u03bbk,j . From line 1 of Alg. 1, since the only thing we need is the mean of the prior at each iteration, we do not need to fully solve the above equation for \u03b8. What we need is the normalized \u03b8, i.e., the mean of the prior Dirichlet at the beginning of each iteration. This observation helps us to get rid of solving the above equation exactly. We can simply use the R.H.S. of Eq. 16 to approximate the mean of the prior at next iteration. In practice this approximation greatly accelerates ADF and as long as \u03b8k,j > 1, the approximation is accurate. Instead of matching the moments given by the sufficient statistics, also known as the natural moments, BMM is trying to find an approximate distribution q by matching the first order moments, i.e., the mean of the prior and the one-step update posterior. Using the same notation, we want q to match the following equation:\nEq(w)[wk] = Ep(w|x)[wk] \u21d4 \u03b8k,j\u2211 j\u2032 \u03b8k,j\u2032 = (1\u2212 \u03bbk,j) \u03b1k,j\u2211 j\u2032 \u03b1k,j\u2032 + \u03bbk,j \u03b1k,j + 1\u2211 j\u2032 \u03b1k,j\u2032 + 1 (17)\nAgain, we can interpret the above equation as to finding the posterior hyperparameter \u03b8 such that the posterior mean is given by the weighted arithmetic mean of the means given by p0 and p\u20320, weighting by \u03bbk,j . Notice that due to the normalization constraint, we cannot solve for \u03b8 directly from the above equations, and in order to solve for \u03b8 we will need one more equation to be added into the system. However, from line 1 of Alg. 1 again, what we really need is not \u03b8, but only its normalized version in the next iteration. So we can get rid of the additional equation and use Eq. 17 as the update formula directly in our algorithm. This completes the discussion about implementation details of Alg. 2 and BMM for SPNs."}], "references": [{"title": "Learning the structure of sum-product networks via an svdbased algorithm", "author": ["T. Adel", "D. Balduzzi", "A. Ghodsi"], "venue": "In Proceedings of UAI,", "citeRegEx": "Adel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2015}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Information projections revisited", "author": ["I. Csisz\u00e1r", "F. Matus"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Csisz\u00e1r and Matus.,? \\Q2003\\E", "shortCiteRegEx": "Csisz\u00e1r and Matus.", "year": 2003}, {"title": "A differential approach to inference in bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Darwiche.,? \\Q2003\\E", "shortCiteRegEx": "Darwiche.", "year": 2003}, {"title": "Greedy structure search for sum-product networks", "author": ["A. Dennis", "D. Ventura"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dennis and Ventura.,? \\Q2015\\E", "shortCiteRegEx": "Dennis and Ventura.", "year": 2015}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gens and Domingos.,? \\Q2012\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2012}, {"title": "Learning the structure of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Gens and Domingos.,? \\Q2013\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2013}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks,", "citeRegEx": "Hecht.Nielsen,? \\Q1988\\E", "shortCiteRegEx": "Hecht.Nielsen", "year": 1988}, {"title": "Online algorithms for sum-product networks with continuous variables", "author": ["P. Jaini", "A. Rashwan", "H. Zhao", "Y. Liu", "E. Banijamali", "Z. Chen", "P. Poupart"], "venue": "In Proceedings of the Eighth International Conference on Probabilistic Graphical Models,", "citeRegEx": "Jaini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaini et al\\.", "year": 2016}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Stochastic expectation propagation", "author": ["Y. Li", "J.M. Hern\u00e1ndez-Lobato", "R.E. Turner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A differential semantics for jointree algorithms", "author": ["J.D. Park", "A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Park and Darwiche.,? \\Q2004\\E", "shortCiteRegEx": "Park and Darwiche.", "year": 2004}, {"title": "Foundations of Sum-Product Networks for Probabilistic Modeling", "author": ["R. Peharz"], "venue": "PhD thesis, Graz University of Technology,", "citeRegEx": "Peharz.,? \\Q2015\\E", "shortCiteRegEx": "Peharz.", "year": 2015}, {"title": "On theoretical properties of sum-product networks", "author": ["R. Peharz", "S. Tschiatschek", "F. Pernkopf", "P. Domingos"], "venue": "In AISTATS,", "citeRegEx": "Peharz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2015}, {"title": "On the latent variable interpretation in sumproduct networks", "author": ["R. Peharz", "R. Gens", "F. Pernkopf", "P. Domingos"], "venue": "arXiv preprint arXiv:1601.06180,", "citeRegEx": "Peharz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2016}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. 12th Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "Merging strategies for sum-product networks: From trees to graphs", "author": ["T. Rahman", "V. Gogate"], "venue": "In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rahman and Gogate.,? \\Q2016\\E", "shortCiteRegEx": "Rahman and Gogate.", "year": 2016}, {"title": "Online and distributed bayesian moment matching for parameter learning in sum-product networks", "author": ["A. Rashwan", "H. Zhao", "P. Poupart"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rashwan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rashwan et al\\.", "year": 2016}, {"title": "Learning sum-product networks with direct and indirect variable interactions", "author": ["A. Rooshenas", "D. Lowd"], "venue": "In ICML,", "citeRegEx": "Rooshenas and Lowd.,? \\Q2014\\E", "shortCiteRegEx": "Rooshenas and Lowd.", "year": 2014}, {"title": "Non-linear filtering by approximation of the a posteriori density", "author": ["H.W. Sorenson", "A.R. Stubberud"], "venue": "International Journal of Control,", "citeRegEx": "Sorenson and Stubberud.,? \\Q1968\\E", "shortCiteRegEx": "Sorenson and Stubberud.", "year": 1968}, {"title": "Simplifying, regularizing and strengthening sum-product network structure learning", "author": ["A. Vergari", "N. Di Mauro", "F. Esposito"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Vergari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vergari et al\\.", "year": 2015}, {"title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "author": ["H. Zhao", "M. Melibari", "P. Poupart"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Collapsed variational inference for sum-product networks", "author": ["H. Zhao", "T. Adel", "G. Gordon", "B. Amos"], "venue": "EFFICIENT COMPUTATION OF MOMENTS IN SUM-PRODUCT", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Introduction Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016).", "startOffset": 209, "endOffset": 335}, {"referenceID": 13, "context": "Introduction Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016).", "startOffset": 209, "endOffset": 335}, {"referenceID": 14, "context": "Introduction Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016).", "startOffset": 209, "endOffset": 335}, {"referenceID": 3, "context": "It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014).", "startOffset": 81, "endOffset": 122}, {"referenceID": 11, "context": "It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014).", "startOffset": 81, "endOffset": 122}, {"referenceID": 18, "context": "It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014).", "startOffset": 263, "endOffset": 289}, {"referenceID": 1, "context": "SPNs are also closely connected to probabilistic graphical models: by interpreting each sum node in the network as a hidden variable and each product node as a rule encoding context-specific conditional independence (Boutilier et al., 1996), every SPN can be equivalently converted into a Bayesian network where compact data structures are used to represent the local probability distributions (Zhao et al.", "startOffset": 216, "endOffset": 240}, {"referenceID": 21, "context": ", 1996), every SPN can be equivalently converted into a Bayesian network where compact data structures are used to represent the local probability distributions (Zhao et al., 2015).", "startOffset": 161, "endOffset": 180}, {"referenceID": 19, "context": "At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments.", "startOffset": 104, "endOffset": 134}, {"referenceID": 17, "context": "An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf.", "startOffset": 53, "endOffset": 95}, {"referenceID": 8, "context": "An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf.", "startOffset": 53, "endOffset": 95}, {"referenceID": 3, "context": "The key technique in the design of our algorithm dates back to the differential approach (Darwiche, 2003) used for exact inference in Bayesian networks.", "startOffset": 89, "endOffset": 105}, {"referenceID": 15, "context": "Recently Rashwan et al. (2016) proposed an online Bayesian Moment Matching (BMM) algorithm to learn the probability distribution of the model parameters of SPNs based on the method of moments.", "startOffset": 9, "endOffset": 31}, {"referenceID": 7, "context": "Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions.", "startOffset": 6, "endOffset": 26}, {"referenceID": 7, "context": "Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions. Empirically they show that BMM is superior to online extensions of projected gradient descent and exponentiated gradient. At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments. Specifically, BMM for SPNs works by matching the first and second order moments of the approximate tractable posterior distribution to the exact but intractable posterior. An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf. 3.2). Rashwan et al. (2016) designed a recursive algorithm to achieve this goal in linear time when the underlying network structure is a tree, and this algorithm is also used by Jaini et al.", "startOffset": 6, "endOffset": 955}, {"referenceID": 7, "context": "Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions. Empirically they show that BMM is superior to online extensions of projected gradient descent and exponentiated gradient. At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments. Specifically, BMM for SPNs works by matching the first and second order moments of the approximate tractable posterior distribution to the exact but intractable posterior. An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf. 3.2). Rashwan et al. (2016) designed a recursive algorithm to achieve this goal in linear time when the underlying network structure is a tree, and this algorithm is also used by Jaini et al. (2016) in the continuous case.", "startOffset": 6, "endOffset": 1126}, {"referenceID": 19, "context": "With the linear time sub-routine for computing moments, we are able to design an assumed density filter (Sorenson and Stubberud, 1968) (ADF) to learn the parameters of SPNs in an online fashion.", "startOffset": 104, "endOffset": 134}, {"referenceID": 21, "context": ", Xn} can be converted into a bipartite BN withO(n|S|) size (Zhao et al., 2015).", "startOffset": 60, "endOffset": 79}, {"referenceID": 4, "context": "The second perspective is to view an SPN S as a mixture model with exponentially many mixture components (Dennis and Ventura, 2015; Zhao et al., 2016b).", "startOffset": 105, "endOffset": 151}, {"referenceID": 3, "context": "(Darwiche, 2003) Let B be a Bayesian network representing probability distribution Pr(X) and having polynomial g.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "This novel idea is due to Darwiche (2003). We also refer readers to Darwiche (2003) for a more complete and detailed discussion of this approach.", "startOffset": 26, "endOffset": 42}, {"referenceID": 3, "context": "This novel idea is due to Darwiche (2003). We also refer readers to Darwiche (2003) for a more complete and detailed discussion of this approach.", "startOffset": 26, "endOffset": 84}, {"referenceID": 3, "context": "Formally, the circuit complexity (Darwiche, 2003) of a BN B is the size of the smallest arithmetic circuit that computes the network polynomial of B.", "startOffset": 33, "endOffset": 49}, {"referenceID": 19, "context": "Assumed density filtering (Sorenson and Stubberud, 1968) is such a framework: the algorithm chooses an approximate distribution from a tractable family of distributions after observing each instance.", "startOffset": 26, "endOffset": 56}, {"referenceID": 17, "context": "A linear time algorithm that uses dynamic programming to compute these moments has been designed by Rashwan et al. (2016) when the underlying structure of S is a tree.", "startOffset": 100, "endOffset": 122}, {"referenceID": 21, "context": "The key update in each iteration of CCCP (Zhao et al. (2016b) Eq.", "startOffset": 42, "endOffset": 62}, {"referenceID": 19, "context": "1 as a sub-routine to develop a new Bayesian online learning algorithm for SPNs based on assumed density filtering (Sorenson and Stubberud, 1968).", "startOffset": 115, "endOffset": 145}, {"referenceID": 2, "context": "This principle of finding an approximate distribution is also known as reverse information projection in the literature of information theory (Csisz\u00e1r and Matus, 2003).", "startOffset": 142, "endOffset": 167}, {"referenceID": 10, "context": "As a note, we can also extend the above ADF algorithm to the batch setting via a recently proposed technique known as stochastic expectation propagation (Li et al., 2015).", "startOffset": 153, "endOffset": 170}, {"referenceID": 2, "context": "This principle of finding an approximate distribution is also known as reverse information projection in the literature of information theory (Csisz\u00e1r and Matus, 2003).2 By utilizing our efficient linear time algorithm for exact moment computation, we propose a Bayesian online learning algorithm for SPNs based on the above moment matching principle, called ADF. The pseudocode is shown in Alg. 2. Details on how to solve the moment matching equation (Eq. 15) are presented in appendix. As a note, we can also extend the above ADF algorithm to the batch setting via a recently proposed technique known as stochastic expectation propagation (Li et al., 2015). 2. As a comparison, information projection corresponds to minimizing KL(q || p(w | x)) within the same family of distributions q \u2208 P . Finding an approximate distribution for SPNs based on information projection has recently been studied by Zhao et al. (2016a), and an algorithm called CVB-SPN is proposed therein.", "startOffset": 143, "endOffset": 921}, {"referenceID": 6, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 18, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 0, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 20, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 0, "context": "Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures.", "startOffset": 72, "endOffset": 117}, {"referenceID": 18, "context": "Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures.", "startOffset": 72, "endOffset": 117}, {"referenceID": 16, "context": "Note that more complicated and compression-efficient post-processing strategy exists for SPNs (Rahman and Gogate, 2016).", "startOffset": 94, "endOffset": 119}, {"referenceID": 15, "context": "On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al.", "startOffset": 209, "endOffset": 259}, {"referenceID": 5, "context": "On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al.", "startOffset": 209, "endOffset": 259}, {"referenceID": 9, "context": "On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al.", "startOffset": 289, "endOffset": 316}, {"referenceID": 12, "context": ", 2016b), concave-convex procedure/expectation maximization (CCCP) (Peharz, 2015; Zhao et al., 2016b), collapsed variational Bayes (CVB) (Zhao et al.", "startOffset": 67, "endOffset": 101}, {"referenceID": 17, "context": ", 2016a), online Bayesian moment matching (BMM) (Rashwan et al., 2016) and assumed density filtering (ADF).", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": ", 2016a; Adel et al., 2015; Vergari et al., 2015). Statistics about the 20 data sets and their corresponding SPN models are shown in Table 1. All the random variables in these 20 data sets are binary. We use LearnSPN as the structure learning algorithm to build structures for the 20 data sets. Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures. Note that more complicated and compression-efficient post-processing strategy exists for SPNs (Rahman and Gogate, 2016). We emphasize how we are not interested in obtaining the most compact SPN representations. Instead, we would like to investigate how the proposed linear time moment computation algorithm scales on general SPNs with DAG structures, compared with the existing quadratic moment computation algorithm. As the last step, we equivalently transform the network structures by removing consecutive sum nodes or product nodes, using the same technique introduced by Vergari et al. (2015). This step helps to reduce the size of the networks while keeping the distributions unchanged.", "startOffset": 9, "endOffset": 1192}, {"referenceID": 17, "context": "As a result, the moment computation algorithm introduced in Rashwan et al. (2016) for trees cannot be applied here.", "startOffset": 60, "endOffset": 82}], "year": 2017, "abstractText": "Bayesian online learning algorithms for Sum-Product Networks (SPNs) need to compute moments of model parameters under the one-step update posterior distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. We propose a linear-time algorithm that works even when the SPN is a directed acyclic graph (DAG). We achieve this goal by reducing the moment computation problem into a joint inference problem in SPNs and by taking advantage of a special structure of the one-step update posterior distribution: it is a multilinear polynomial with exponentially many monomials, and we can evaluate moments by differentiating. The latter is known as the differential trick. We apply the proposed algorithm to develop a linear time assumed density filter (ADF) for SPN parameter learning. As an additional contribution, we conduct extensive experiments comparing seven different online learning algorithms for SPNs on 20 benchmark datasets. The new linear-time ADF method consistently achieves low runtime due to the efficient linear-time algorithm for moment computation; however, we discover that two other methods (CCCP and SMA) typically perform better statistically, while a third (BMM) is comparable to ADF. Interestingly, CCCP can be viewed as implicitly using the same differentiation trick that we make explicit here. The fact that two of the top four fastest methods use this trick suggests that the same trick might find other uses for SPN learning in the future.", "creator": "LaTeX with hyperref package"}}}