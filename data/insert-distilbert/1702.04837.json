{"id": "1702.04837", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging", "abstract": "we address the statistical and optimization impacts of therapy using classical or hessian sketch to approximately solve the matrix ridge regression ( mrr ) problem. prior research has considered the statistical effects of nonlinear classical sketch on least squares rapid regression ( lsr ), a strictly almost simpler problem. we independently establish that classical sketch has a similar effect upon the optimization properties somewhat of linear mrr as it ultimately does partly on those of lsr - - namely, it recovers nearly optimal solutions. in contrast, hessian sketch does not have this guarantee ; instead, the approximation error is likely governed by discovering a subtle interplay between the \" mass \" in the responses and the optimal objective value.", "histories": [["v1", "Thu, 16 Feb 2017 02:01:26 GMT  (997kb,D)", "https://arxiv.org/abs/1702.04837v1", null], ["v2", "Sat, 25 Feb 2017 02:59:41 GMT  (1007kb,D)", "http://arxiv.org/abs/1702.04837v2", null], ["v3", "Sat, 10 Jun 2017 17:52:18 GMT  (909kb,D)", "http://arxiv.org/abs/1702.04837v3", "A short version will appear in the thirty-fourth International Conference on Machine Learning (ICML 2017)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA", "authors": ["shusen wang", "alex gittens", "michael w mahoney"], "accepted": true, "id": "1702.04837"}, "pdf": {"name": "1702.04837.pdf", "metadata": {"source": "CRF", "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging", "authors": ["Shusen Wang", "Alex Gittens", "Michael W. Mahoney"], "emails": ["shusen@berkeley.edu", "gittea@rpi.edu", "mmahoney@stat.berkeley.edu"], "sections": [{"heading": null, "text": "For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions.\nWe establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.\nKeywords: randomized linear algebra, matrix sketching, ridge regression\nar X\niv :1\n70 2.\n04 83\n7v 3"}, {"heading": "1. Introduction", "text": "Regression is one of the most fundamental problems in machine learning. The simplest and most thoroughly studied regression model is least squares regression (LSR). Given features X = [xT1 ; . . . ,x T n ] \u2208 Rn\u00d7d and responses y = [y1, . . . , yn]T \u2208 Rn, the LSR problem minw \u2016Xw \u2212 y\u201622 can be solved in O(nd2) time using the QR decomposition or in O(ndt) time using accelerated gradient descent algorithms. Here, t is the number of iterations, which depends on the initialization, the condition number of X, and the stopping criterion.\nThis paper considers the n d problem, where there is much redundancy in X. Matrix sketching, as used within Randomized Linear Algebra (RLA) (Mahoney, 2011, Woodruff, 2014), works by reducing the size of X without losing too much information; this operation can be modeled as taking actual rows or linear combinations of the rows of X with a sketching matrix S to form the sketch STX. Here S \u2208 Rn\u00d7s satisfies d < s n so that STX generically has the same rank but much fewer rows as X. Sketching has been used to speed up LSR (Drineas et al., 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguye\u0302n, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 STy\u201622 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013).\nThere has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.\nThe concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w?\u201622 in terms of the difference in the objective function values and the condition number of XTX.\nA more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance. In particular, Pilanci and Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.\nBoth of these perspectives are important and of practical interest. The optimization perspective is relevant when the data can be taken as deterministic values. The statistical perspective is relevant in machine learning and statistics applications where the data are random, and the regression coefficients are therefore themselves random variables.\nIn practice, regularized regression, e.g., ridge regression and LASSO, exhibit more attractive bias-variance trade-offs and generalization errors than vanilla LSR. Furthermore,\n1. The condition number of XTSSTX is very close to that of XTX, and thus the number of iterations t is almost unchanged.\nthe matrix generalization of LSR, where multiple responses are to be predicted, is often more useful than LSR. However, the properties of sketched regularized matrix regression are largely unknown. Hence, the question: how, if at all, does our understanding of the optimization and statistical properties of sketched LSR generalize to sketched regularized regression? We answer this question for sketched matrix ridge regression (MRR).\nRecall that X is n\u00d7 d. Let Y \u2208 Rn\u00d7m denote the matrix of corresponding responses. We study the MRR problem\nmin W\n{ f(W) , 1n \u2225\u2225XW \u2212Y\u2225\u22252 F + \u03b3\u2016W\u20162F } , (1)\nwhich has optimal solution\nW? = (XTX + n\u03b3Id) \u2020XTY. (2)\nHere, (\u00b7)\u2020 denotes the Moore-Penrose inversion operation. LSR is a special case of MRR, with m = 1 and \u03b3 = 0. The optimal solution W? can be obtained in O(nd2 + nmd) time using a QR decomposition of X. Sketching can be applied to MRR in two ways:\nWc = (XTSSTX + n\u03b3Id) \u2020(XTSSTY), (3) Wh = (XTSSTX + n\u03b3Id) \u2020XTY. (4)\nFollowing the convention of Pilanci and Wainwright (2015), Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch. Table 1 lists the time costs of the three solutions to MRR."}, {"heading": "1.1 Main Results and Contributions", "text": "We summarize all of our upper bounds in Table 2. Our optimization analysis bounds the objective function values, while our statistical analysis guarantees the bias and variance.\nWe first study classical and Hessian sketches from the optimization perspective. Theorems 1 and 2 show that\n\u2022 Classical sketch achieves relative error in the objective value. With sketch size s = O\u0303(d/ ), the objective satisfies f(Wc) \u2264 (1 + )f(W?).\n\u2022 Hessian sketch does not achieve relative error in the objective value. In particular, if 1 n\u2016Y\u2016 2 F is much larger than f(W ?), then f(Wh) can be far larger than f(W?).\nWe then study classical sketch and Hessian sketch from the statistical perspective, by modeling Y = XW0 +\u039e as the sum of a true linear model and random noise, decomposing the risk R(W) = E\u2016XW\u2212XW0\u20162F into bias and variance terms, and bounding these terms. The risk R(W) determines how well the trained model W generalizes to the test samples; see the discussions in Appendix A. We draw the following conclusions (see Theorems 4, 5, 7 for the details):\n\u2022 The bias of classical sketch can be nearly as small as that of the optimal solution. The variance is \u0398 ( n s ) times that of the optimal solution; this bound is optimal. Therefore\nover-regularization2 should be used to supress the variance. (As \u03b3 increases, the bias increases, and the variance decreases.)\n\u2022 Since Y is not sketched with Hessian sketch, the variance of Hessian sketch can be close to the optimal solution. However, Hessian sketch has high bias, especially when n\u03b3 is small compared to \u2016X\u201622. This indicates that over-regularization is necessary for Hessian sketch to have low bias.\nOur empirical evaluations bear out these theoretical results. In particular, in Section 4, we show in Figure 3 that even when the regularization parameter \u03b3 is fine-tuned, the risks of classical sketch and Hessian sketch are worse than that of the optimal solution by an order of magnitude. This is an empirical demonstration of the fact that the near-optimal properties\n2. By over-regularization, we mean choosing a larger value of the regularization parameter \u03b3 than what we would optimally choose for the unsketched problem.\u201d\nof sketching from the optimization perspective are much less relevant in a statistical setting than its sub-optimal statistical properties.\nWe propose to use model averaging, which averages the solutions of g sketched MRR problems, to attain lower optimization and statistical errors. Without ambiguity, we denote classical and Hessian sketches with model averaging by Wc and Wh, respectively. Theorems 8, 9, 11, 13 establish the following results:\n\u2022 Classical Sketch. Model averaging improves the objective function value and the variance and does not increase the bias. Specifically, with the same sketch size s, model averaging makes f(W\nc)\u2212f(W?) f(W?) and var(Wc) var(W?) respectively decrease to almost 1 g of\nthose of classical sketch without model averaging, provided that s d. See Table 2 for the details.\n\u2022 Hessian Sketch. Model averaging decreases the objective function value and the bias and does not increase the variance.\nNote that classical sketch with uniform sampling and model averaging is essentially bagging (synonym bootstrap aggregating) (Breiman, 1996) (or a variant called pasting (Breiman, 1999)) for ridge regression. Our work lends strong guarantees to bagging for ridge regression. Different from bagging, our approach is not limited to uniform sampling.\nClassical sketch with model averaging has three immediate applications. In the singlemachine setting,\n\u2022 Classical sketch with model averaging offers a way to improve the statistical performance in the presence of heavy noise. Assume the sketch size is s = O\u0303( \u221a nd). As\ng grows larger than ns , the variance of the averaged solution can be even lower than the optimal solution. See Corollary 12 for further discussions. Using sketching methods other than uniform sampling, the performance is independent of matrix coherence, which is an improvement over the traditional bagging (Breiman, 1996).\nIn the distributed setting, the feature-response pairs (x1,y1), \u00b7 \u00b7 \u00b7 , (xn,yn) \u2208 Rd \u00d7 Rm are divided among g machines. Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement. We illustrate this procedure in Figure 1. In this setting, the model averaging procedure will communicate the g local models only once to return the final estimate; this process has very low communication and latency costs, and it suggests two further applications of classical sketch with model averaging:\n\u2022 Model Averaging for Machine Learning. If a low-precision solution is acceptable, the averaged solution can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication. If ng is big enough compared to d and the row coherence of X is small, then \u201cone-shot\u201d model averaging has bias and variance comparable to the optimal solution.\n\u2022 Model Averaging for Optimization. If a high-precision solution to MRR is required, then an iterative numerical optimization algorithm must be used. The cost of such\nnumerical optimization algorithms heavily depends on the quality of the initialization.3 A good initialization saves lots of iterations. The averaged model is provably close to the optimal solution, so model averaging provides a high-quality initialization for more expensive algorithms."}, {"heading": "1.2 Prior Work", "text": "The body of work on sketched LSR mentioned earlier (Drineas et al., 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguye\u0302n, 2013) shares many similarities with our results. However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered.\nLu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis. Our setting differs in that we consider n d, reduce the number of samples by sketching, and allow for multiple responses.\nThe model averaging analyzed in this paper is similar in spirit to the AvgM algorithm of (Zhang et al., 2013). When classical sketch is used with uniform row sampling without replacement, our model averaging procedure is a special case of AvgM. However, our results do not follow from those of (Zhang et al., 2013). First, we make no assumption on the data, whereas they assumed x1, \u00b7 \u00b7 \u00b7 ,xn are i.i.d. from an unknown distribution. Second, the objectives are different: we study the distances \u2016XWc \u2212XW?\u20162F and E\u2016XWc \u2212XW0\u20162F , where Wc is the averaged classical sketches, W0 is the unknown ground truth, and W\n? is the optimal solution based on the observed data; they essentially studied E\u2016Wc \u2212W?\u20162F ; our expectation is taken w.r.t. the random noise in the responses, while their expectation is w.r.t. the distribution of x1, \u00b7 \u00b7 \u00b7 ,xn. Third, our results apply to many other sketching\n3. For example, the conjugate gradient method satisfies \u2016W(t)\u2212W?\u20162F \u2016W(0)\u2212W?\u20162\nF\n\u2264 \u03b8t1; the stochastic block coordinate\ndescent (Tu et al., 2016) satisfies Ef(W (t))\u2212f(W?)\nf(W(0))\u2212f(W?) \u2264 \u03b8 t 2. Here W (t) is the output of the t-th iteration;\n\u03b81, \u03b82 \u2208 (0, 1) depend on the condition number of XTX + n\u03b3Id and some other factors.\nensembles than uniform sampling without replacement. Our results clearly indicate that the performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013). For similar reasons, our work is different from the divide-and-conquer kernel ridge regression algorithm of (Zhang et al., 2015).\nIterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al. (2016a). By way of comparison, all the algorithms in this paper are \u201cone-shot\u201d rather than iterative. Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging."}, {"heading": "1.3 Paper Organization", "text": "Section 2 defines our notation and introduces the sketching schemes we consider. Section 3 presents our theoretical results. Sections 4 and 5 conduct experiments to verify our theories and demonstrates the usefulness of model averaging. Section 6 shows the sketch of proof. The proofs of the theorems are in the appendix."}, {"heading": "2. Preliminaries", "text": "Throughout, we take In to be the n \u00d7 n identity matrix and 0 to be a vector or matrix of all zeroes of the appropriate size. Given a matrix A = [aij ], the i-th row is denoted by ai:, and the j-th column is by a:j . The Frobenius and spectral norms of A are written as, respectively, \u2016A\u2016F and \u2016A\u20162. The set {1, 2, \u00b7 \u00b7 \u00b7 , n} is written [n]. Let O, \u2126, and \u0398 be the standard asymptotic notation. Let O\u0303 conceal logarithm factors.\nThroughout, we fix X \u2208 Rn\u00d7d as our matrix of features. We set \u03c1 = rank(X) and write the SVD of X as X = U\u03a3VT , where U, \u03a3, V are respectively n \u00d7 \u03c1, \u03c1 \u00d7 \u03c1, and d \u00d7 \u03c1 matrices. We let \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3\u03c1 > 0 be the singular values of X. The Moore-Penrose inverse of X is defined by X\u2020 = V\u03a3\u22121UT . The row leverage scores of X are li = \u2016u:i\u201622 for i \u2208 [n]. The row coherence of X is \u00b5(X) = n\u03c1 maxi \u2016u:i\u2016 2 2. Throughout, we let \u00b5 be shorthand for \u00b5(X). The notation defined in Table 3 are used throughout this paper.\nMatrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear regression. We denote the process of sketching a matrix X \u2208 Rn\u00d7d by X\u2032 = STX. Here, S \u2208 Rn\u00d7s is called a sketching matrix and X\u2032 \u2208 Rs\u00d7d is called a sketch of X. In practice, except for Gaussian projection (where the entries of S are i.i.d. sampled from N (0, 1/s)), the sketching matrix S is not formed explicitly.\nMatrix sketching can be accomplished by random selection or random projection. Random sampling corresponds to sampling rows of X i.i.d. with replacement according to given row sampling probabilities p1, \u00b7 \u00b7 \u00b7 , pm \u2208 (0, 1). The corresponding (random) sketching matrix S \u2208 Rn\u00d7s has exactly one non-zero entry, whose position indicates the index of the selected row; in practice, this S is not explicitly formed. Uniform sampling fixes p1 = \u00b7 \u00b7 \u00b7 = pn = 1n . Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X. In practice shrinked leverage score sampling can be a better choice than leverage score sampling (Ma et al.,\n2015). The sampling probabilities of shrinked leverage score sampling are defined by pi = 1 2 ( li\u2211n j=1 lj + 1n ) .4\nThe exact leverage scores are unnecessary in practice; constant-factor approximation to the leverage scores is sufficient. Leverage scores can be efficiently approximated by the algorithms of (Drineas et al., 2012). Let l1, \u00b7 \u00b7 \u00b7 , ln be the true leverage scores. We denote the approximate leverages by l\u03031, \u00b7 \u00b7 \u00b7 , l\u0303n such that\nl\u0303q \u2208 [lq, \u03c4 lq] for all q \u2208 [n], (5)\nwhere \u03c4 \u2265 1 indicates the quality of approximation. We can use pq = l\u0303q/ \u2211\nj l\u0303j as the sampling probability. For the shrinked leverage score sampling, we define the sampling probabilities\npi = 1\n2 ( l\u0303i\u2211n j=1 l\u0303j + 1 n ) for i = 1, . . . , n. (6)\nUsing the approximate leverage scores to replace the exact ones, we only need to make the sketch size \u03c4 times larger. As long as \u03c4 is a small constant, the orders of the sketch sizes of the exact and approximate leverage score sampling are the same. Thus we do not distinguish the exact and approximate leverage scores in this paper.\nGaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson and Lindenstrauss, 1984). Let G \u2208 Rm\u00d7s be a standard Gaussian matrix, i.e., each entry is sampled independently from N (0, 1). The matrix S = 1\u221a\ns G is a\nGaussian projection matrix. It takes O(nds) time to apply S \u2208 Rn\u00d7s to any n \u00d7 d dense matrix, which makes Gaussian projection inefficient relative to other forms of sketching.\nSubsampled randomized Hadamard transform (SRHT) (Drineas et al., 2011, Lu et al., 2013, Tropp, 2011) is a more efficient alternative to Gaussian projection. Let\n4. In fact, pi can be any convex combination of li\u2211n j=1 lj and 1 n (Ma et al., 2015). We use the weight 1 2 for\nconvenience; our conclusions extend in a straightforward manner to other weightings.\nHn \u2208 Rn\u00d7n be the Walsh-Hadamard matrix with +1 and \u22121 entries, D \u2208 Rn\u00d7n be a diagonal matrix with diagonal entries sampled uniformly from {+1,\u22121}, and P \u2208 Rn\u00d7s be the uniform row sampling matrix defined above. The matrix S = 1\u221a\nn DHnP \u2208 Rn\u00d7s is an\nSRHT matrix, and can be applied to any n \u00d7 d matrix in O(nd log s) time. In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n. Their performance and theoretical analyses are very similar.\nCountSketch can be applied to any X \u2208 Rn\u00d7d in O(nd) time (Charikar et al., 2004, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguye\u0302n, 2013, Pham and Pagh, 2013, Weinberger et al., 2009). Though more efficient to apply, CountSketch requires a bigger sketch size than Gaussian projections, SRHT, and leverage score sampling to attain the same theoretical guarantees. The readers can refer to (Woodruff, 2014) for a detailed description of CountSketch. CountSketch may not be applicable to model averaging as a theoretically sound approach like the other sketching methods. See Remark 20 for the reasons."}, {"heading": "3. Main Results", "text": "Sections 3.1 and 3.2 analyze sketched MRR from, respectively, optimization and statistical perspectives. Sections 3.3 and 3.4 capture the impacts of model averaging on, respectively, the optimization and statistical properties of sketched MRR.\nWe described six sketching methods in Section 2. For simplicity, in this section, we refer to leverage score sampling, shrinked leverage score sampling, Gaussian projection, and SRHT as the four sketching methods; and we will mention explicitly uniform sampling and CountSketch. Throughout, let \u00b5 be the row coherence of X and \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1."}, {"heading": "3.1 Sketched MRR: Optimization Perspective", "text": "Theorem 1 shows that f(Wc), the objective value of classical sketch, is very close to the optimal objective value f(W?). The approximation quality improves as \u03b3 increases.\nTheorem 1 (Classical Sketch) Let \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1. For the four sketching methods with s = O\u0303 (\u03b2d ) , uniform sampling with s = O ( \u00b5\u03b2d log d ) , and CountSketch with s = O (\u03b2d2 ) , the inequality\nf(Wc)\u2212 f(W?) \u2264 f(W?)\nholds with probability at least 0.9.\nThe corresponding guarantee for the performance of Hessian sketch is given in Theorem 2. It is weaker than the guarantee for classical sketch, especially when 1n\u2016Y\u2016 2 F is far larger than f(W?). If Y is nearly noiseless\u2014Y is well-explained by a linear combination of the columns of X\u2014and \u03b3 is small, then f(W?) is close to zero, and consequently f(W?) can be far smaller than 1n\u2016Y\u2016 2 F . Therefore, in this case which is ideal for MRR, f(W\nh) is not close to f(W?) and our theory suggests Hessian sketch does not perform as well as classical sketch. This is verified by our experiments (see Figure 2), which show that unless\n\u03b3 is big or a large portion of Y is outside the column space of X, the ratio f(W h)\nf(W?) can be large.\nTheorem 2 (Hessian Sketch) Let \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1. For the four sketching methods with s = O\u0303 (\u03b22d ) , uniform sampling with s = O (\u00b5\u03b22d log d ) , and CountSketch with s = O(\u03b2 2d2 ), the inequality\nf(Wh)\u2212 f(W?) \u2264 ( \u2016Y\u20162F n \u2212 f(W ?) ) .\nholds with probability at least 0.9.\nThese two results imply that f(Wc) and f(Wh) can be close to f(W?). When this is the case, curvature of the objective function ensures that the sketched solutions Wc and Wh are close to the optimal solution W?. Lemma 3 studies the Mahalanobis distance \u2016M(W \u2212W?)\u20162F . Here M is any non-singular matrix; in particular, it can be the identity matrix or (XTX)1/2. Lemma 3 is a consequence of Lemma 31.\nLemma 3 Let f be the objective function of MRR defined in (1), W \u2208 Rd\u00d7m be arbitrary, and W? be the optimal solution defined in (2). For any non-singular matrix M, the Mahalanobis distance satisfies\n1 n \u2225\u2225M(W \u2212W?)\u2225\u22252 F \u2264 f(W)\u2212 f(W ?) \u03c32min [ (XTSSTX + n\u03b3Id)1/2M\u22121\n] . By choosing M = (XTX)1/2, we can bound 1n\u2016XW\u2212XW\n?\u20162F in terms of the difference in the objective values:\n1 n \u2225\u2225XW \u2212XW?\u2225\u22252 F \u2264 \u03b2 [ f(W)\u2212 f(W?) ] ,\nwhere \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3\n\u2264 1. With Lemma 3, we can directly apply Theorems 1 or 2 to bound 1 n\u2016XW c \u2212XW?\u20162F or 1 n\u2016XW h \u2212XW?\u20162F ."}, {"heading": "3.2 Sketched MRR: Statistical Perspective", "text": "We consider the following fixed design model. Let X \u2208 Rn\u00d7d be the observed feature matrix, W0 \u2208 Rd\u00d7m be the true and unknown model, \u039e \u2208 Rn\u00d7m contain unknown random noise, and\nY = XW0 + \u039e (7)\nbe the observed responses. We make the following standard weak assumptions on the noise:\nE[\u039e] = 0 and E[\u039e\u039eT ] = \u03be2In.\nWe observe X and Y and seek to estimate W0. We can evaluate the quality of the estimate by the risk:\nR(W) = 1nE \u2225\u2225XW \u2212XW0\u2225\u22252F , (8)\nwhere the expectation is taken w.r.t. the noise \u039e. In Appendix A we explain that R(W) determines how well W generalize to test data. We study the risk functions R(W?), R(Wc), and R(Wh) in the following.\nTheorem 4 (Bias-Variance Decomposition) We consider the data model described in this subsection. Let W be W?, Wc, or Wh, as defined in (2), (3), (4), respectively; then the risk function can be decomposed as\nR(W) = bias2(W) + var(W).\nRecall the SVD of X defined in Section 2: X = U\u03a3VT . The bias and variance terms can be written as\nbias ( W? ) = \u03b3 \u221a n \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u2225\u2225\nF ,\nvar ( W? ) = \u03be 2\nn \u2225\u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u2225\u2225\u22252 F ,\nbias ( Wc ) = \u03b3 \u221a n \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u03a3\u22121VTW0\u2225\u2225\u2225\nF ,\nvar ( Wc ) = \u03be 2\nn \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u2020UTSST\u2225\u2225\u22252 F ,\nbias ( Wh ) = \u03b3 \u221a n \u2225\u2225\u2225(\u03a3\u22122 + UTSSTU\u2212I\u03c1n\u03b3 )(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u03a3VTW0\u2225\u2225\u2225\nF ,\nvar ( Wh ) = \u03be 2\nn \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u2225\u2225\u22252 F .\nThroughout this paper, we compare the bias and variance of classical sketch and Hessian sketch to those of the optimal solution W?. We first study the bias, variance, and risk of W?, which will help us understand the subsequent comparisons. Here we can assume that \u03a32 = VTXTXV is linear with n, which is reasonable because XTX = \u2211n i=1 xix T i and V is orthogonal matrix.\n\u2022 Bias. The bias of W? is independent of n and is increasing with \u03b3. The bias is a price paid for supressing the variance; for least squares regression, \u03b3 is zero, and the bias equals to zero.\n\u2022 Variance. The variance of W? is inversely proportional to n. As n grows, the variance decreases to zero. Thus it is highly interesting to compare the variance of approximate solutions to var(W?).\n\u2022 Risk. Note that W? is not the minimizer of R(\u00b7); W0 is the minimizer because R(W0) = 0. Nevertheless, because W0 is unknown, W\n? with fine-tuned \u03b3 is a standard choice in practice. It is thus highly interesting to find solutions with risks comparable to or even better than R(W?).\nTheorem 5 provides upper and lower bounds on the bias and variance of classical sketch. In particular, we see that that bias(Wc) is within a factor of (1\u00b1 ) of bias(W?). However, var(Wc) can be \u0398(ns ) times worse than var(W ?).\nTheorem 5 (Classical Sketch) For Gaussian projection and SRHT sketching with s = O\u0303( d 2 ), uniform sampling with s = O(\u00b5d log d 2 ), or CountSketch with s = O(d2 2\n), the inequalities\n1\u2212 \u2264 bias(W c)\nbias(W?) \u2264 1 + ,\n(1\u2212 )n s \u2264 var(W\nc)\nvar(W?) \u2264 (1 + )n s\nhold with probability at least 0.9. For shrinked leverage score sampling with s = O(d log d\n2 ), theses inequalities, except for\nthe lower bound on the variance, hold with probability at least 0.9.\nRemark 6 To establish upper (lower) bound on the variance, we need the upper (lower) bound on \u2016S\u201622. For leverage score sampling, there is neither nontrivial upper nor lower bound on \u2016S\u201622, which is why the variance of leverage score sampling cannot be bounded. For shrinked leverage score sampling, we only have the upper bound \u2016S\u201622 \u2264 2ns ; but \u2016S\u2016 2 2 does not have a nontrivial lower bound, which is why shrinked leverage score sampling lacks lower bound on variance. In Remark 18, we explain why for (shrinked) leverage score sampling, \u2016S\u201622 does not have upper and (or) lower bound.\nTheorem 7 establishes similar upper and lower bounds on the bias and variance of Hessian sketch. The situation is the reverse of that with classical sketch: the variance of Wh is close to that of W? if s is large enough, but as the regularization parameter \u03b3 goes to zero, bias(Wh) becomes much larger than bias(W?).\nTheorem 7 (Hessian Sketch) For the four sketching methods with s = O\u0303( d 2 ), uniform sampling with s = O(\u00b5d log d 2 ), and CountSketch with s = O(d2 2 ), the inequalities\nbias(Wh) bias(W?) \u2264 (1 + )\n( 1 +\n\u2016X\u201622 n\u03b3\n) ,\n1\u2212 \u2264 var(W h)\nvar(W?) \u2264 1 +\nhold with probability at least 0.9. Further assume that \u03c32\u03c1 \u2265 n\u03b3 . Then\nbias(Wh)\nbias(W?) \u2265 1 1 + ( \u03c32\u03c1 n\u03b3 \u2212 1 )\nholds with probability at least 0.9.\nThe lower bound on the bias shows that Hessian sketch can suffer from a much higher bias than the optimal solution. The gap between bias(Wh) and bias(W?) can be lessened by increasing the regularization parameter \u03b3, but such over-regularization increases the baseline bias(W?) itself. It is also worth mentioning that unlike bias(W?) and bias(Wc), bias(Wh) is not monotonically increasing with \u03b3, as is empirically verified in Figure 3.\nIn sum, our theories show that the classical and Hessian sketches are not statistically comparable to the optimal solutions: classical sketch has too high a variance, and Hessian sketch has too high a bias for reasonable amounts of regularization. In practice, the regularization parameter \u03b3 should be tuned to optimize the prediction accuracy. Our experiments in Figure 3 show that even with fine-tuned \u03b3, the risks of classical and Hessian sketches can be higher than the risk of the optimal solution by an order of magnitude. Formally speaking, min\u03b3 R(W\nc) min\u03b3 R(W?) and min\u03b3 R(Wh) min\u03b3 R(W?) hold in practice.\nOur empirical study in Figure 3 suggests classical and Hessian sketches both require overregularization, i.e., setting \u03b3 larger than what is best for the optimal solution W?. Formally\nspeaking, argmin\u03b3 R(W c) > argmin\u03b3 R(W ?) and argmin\u03b3 R(W h) > argmin\u03b3 R(W ?). Although this is the case for both types of sketches, the underlying explanations are different. Classical sketch has a high variance, so a large \u03b3 is required to supress the variance (its variance is non-increasing with \u03b3). Hessian sketch has very high bias when \u03b3 is small, so a reasonably large \u03b3 is necessary to lower its bias."}, {"heading": "3.3 Model Averaging: Optimization Perspective", "text": "We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions. The model averaging procedure is straightforward: one independently draws g sketching matrices S1, \u00b7 \u00b7 \u00b7 ,Sg \u2208 Rn\u00d7s, uses these to form g sketched MRR solutions, denoted by {Wci} g i=1 or {Whi } g i=1, and averages these solutions to obtain the final estimate\nWc = 1g \u2211g i=1 W c i or W h = 1g \u2211g i=1 W h i . Practical applications of model averaging are enumerated in Section 1.1.\nTheorems 8 and 9 present guarantees on the optimization accuracy of using model averaging to combine the classical/Hessian sketch solutions. We can contrast these with the guarantees provided for sketched MRR in Theorems 1 and 2. For classical sketch with model averaging, we see that when \u2264 1g , the bound on f(W\nh) \u2212 f(W?) is proportional to /g. From Lemma 3 we can see that the distances from Wc to W? also decreases accordingly.\nTheorem 8 (Classical Sketch with Model Averaging) Let \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3\n\u2264 1. For the four methods, let s = O\u0303 (\u03b2d ) ; for uniform sampling, let s = O (\u00b5\u03b2d log d ) . Then the inequality\nf(Wc)\u2212 f(W?) \u2264 ( g + \u03b2 2 2 ) f(W?)\nholds with probability at least 0.8.\nFor Hessian sketch with model averaging, if \u03b22 \u2264 1 g2 , then the bound on f(Wh)\u2212f(W?)\nis proportional to g2 .\nTheorem 9 (Hessian Sketch with Model Averaging) Let \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1. For the four methods let s = O\u0303 (\u03b22d ) , and for uniform sampling let s = O (\u00b5\u03b22d log d ) , then the inequality\nf(Wh)\u2212 f(W?) \u2264 ( g2 + 2 \u03b22 )(\u2016Y\u20162F n \u2212 f(W?) ) .\nholds with probability at least 0.8."}, {"heading": "3.4 Model Averaging: Statistical Perspective", "text": "Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketches. Our first result conducts a bias-variance decomposition for the averaged solution of sketched MRR.\nTheorem 10 (Bias-Variance Decomposition) We consider the fixed design model (7). The risk function defined in (8) can be decomposed as\nR(W) = bias2(W) + var(W).\nThe bias and variance terms are\nbias ( Wc ) = \u03b3 \u221a n \u2225\u2225\u2225\u22251g g\u2211 i=1 ( UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020\u03a3\u22121VTW0\u2225\u2225\u2225\u2225 F ,\nvar ( Wc ) = \u03be2\nn \u2225\u2225\u2225\u22251g g\u2211 i=1 ( UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020UTSiSTi \u2225\u2225\u2225\u22252 F ,\nbias ( Wh ) = \u03b3 \u221a n \u2225\u2225\u2225\u22251g g\u2211 i=1 ( \u03a3\u22122 + UTSiS T i U\u2212I\u03c1 n\u03b3 )( UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020\u03a3VTW0\u2225\u2225\u2225\u2225 F ,\nvar ( Wh ) = \u03be2\nn \u2225\u2225\u2225\u22251g g\u2211 i=1 ( UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020\u2225\u2225\u2225\u22252 F .\nTheorems 11 and 13 provide upper bounds on the bias and variance of model-averaged sketched MRR for, respectively, classical sketch and Hessian sketch. We can contrast them with Theorems 5 and 7 to see the statistical benefits of model averaging.\nTheorem 11 (Classical Sketch with Model Averaging) For shrinked leverage score sampling, Gaussian projection, and SRHT with s = O\u0303 ( d 2 ) , or uniform sampling with s =\nO (\u00b5d log d\n2\n) , the inequalities\nbias(Wc) bias(W?) \u2264 1 + ,\nvar(Wc)\nvar(W?) \u2264 n s (\u221a 1+ /g g + )2 hold with probability at least 0.8.\nBagging (Breiman, 1996) works in the following way: first, randomly generates g datasets by uniform sampling with or without replacement; second, solves the matrix regression/classification problem independently on each generated dataset; last, aggregates the solutions in some way, e.g., model averaging. We show in Corollary 12 to show bagging indeed achieves better variance than the optimal solution W?. Note that our approaches are more general than the traditional bagging: the g datasets can be formed by any sketching and not limited to uniform sampling.\nCorollary 12 (Bagging) Let g = O ( n s ) ; for shrinked leverage score sampling, Gaussian\nprojection, and SRHT with s = O\u0303 (\u221a nd ) , or uniform sampling with s = O (\u221a \u00b5nd log d ) , it holds with probability 0.8 that var(Wc) \u2264 var(W?).\nFurthermore, for shrinked leverage score sampling, Gaussian projection, and SRHT with s = O\u0303 ( dg ) \u2264 n, or uniform sampling with s = O ( \u00b5gd log d ) , it holds with probability 0.8 that var(Wc) = O ( 1 g var(W ?) ) .\nTheorem 13 shows that model averaging decreases the bias of Hessian sketch without increasing the variance. For Hessian sketch without model averaging, recall that bias(Wh) is larger than bias(W?) by a factor of O(\u2016X\u201622/(n\u03b3)). Theorem 13 shows that model averaging reduces this ratio by a factor of g when \u2264 1 g .\nTheorem 13 (Hessian Sketch with Model Averaging) For the four methods with s = O\u0303 ( d 2 ) , or uniform sampling with s = O (\u00b5d log d 2 ) , the inequalities\nbias(Wh) bias(W?) \u2264 1 + + ( g + 2 )\u2016X\u201622 n\u03b3 ,\nvar(Wh) var(W?) \u2264 1 +\nhold with probability at least 0.8."}, {"heading": "4. Experiments on Synthetic Data", "text": "We conduct experiments on synthetic data to verify our main Theorems. Section 4.1 describes the data model and experiment settings. Sections 4.2 and 4.3 study classical and Hessian sketches from the optimization and statistical perspectives, respectively, to verify Theorems 1, 2, 5, 7. Sections 4.4 and 4.5 study the model averaging from the optimization and statistical perspectives, respectively, to corroborate Theorems 8, 9, 11, 13."}, {"heading": "4.1 Settings", "text": "Following (Ma et al., 2015, Yang et al., 2016), we construct X = Udiag(\u03c3)VT \u2208 Rn\u00d7d and y = Xw0 + \u03b5 \u2208 Rn in the following way.\n\u2022 Let the rows of A \u2208 Rn\u00d7d be i.i.d. sampled from multivariate t-distribution with covariance matrix C and v = 2 degree of freedom, where the (i, j)-th entry of C \u2208 Rm\u00d7n is 2 \u00d7 0.5|i\u2212j|. A has high row coherence. Let U be the orthonormal bases of A.\n\u2022 Let the entries of b \u2208 Rd be equally paced between 0 and \u22126. Let \u03c3i = 10bi for all i \u2208 [d].\n\u2022 Let V \u2208 Rd\u00d7d be the orthonormal bases of a d\u00d7 d standard Gaussian matrix.\n\u2022 Let w0 = [10.2d; 0.110.6d; 10.2d].\n\u2022 The entries of \u03b5 \u2208 Rn are i.i.d. sampled from N (0, \u03be2).\nIn this way, X has high row coherence; its condition number is \u03ba(XTX) = 1012. Let S \u2208 Rn\u00d7s be any of the six sketching methods considered in this paper. We fix n = 105, d = 500, and s = 5, 000. Since the sketching methods are randomized, we always repeat sketching 10 times and report the averaged results.\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 20: .\n.\n\u03be = 10\u22123 \u03be = 10\u22122 \u03be = 10\u22121\nWc\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-7\n10-6\n10-5\n10-4\n10-3\n10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-4\n10-3\n10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n0.005\n0.01\n0.02\n0.05\nWh\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-7\n10-6\n10-5\n10-4\n10-3\n10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-4\n10-3\n10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n0.005\n0.01\n0.02\n0.05"}, {"heading": "4.2 Sketched MRR: Optimization Perspective", "text": "We seek to verify Theorems 1 and 2 which study classical and Hessian sketches, respective, from the optimization perspective. In Figure 2, we plot the objective function value f(w) = 1 n\u2016Xw \u2212 y\u2016 2 2 + \u03b3\u2016w\u201622 against \u03b3, under different settings of noise intensity \u03be. The black curves correspond to the optimal solution w?; the color curves are classical or Hessian sketch with different sketching methods. The results verify our theory: classical sketch wc is always close to optimal; Hessian sketch wh is much worse than the optimal when \u03b3 is small and y is mostly in the column space of X."}, {"heading": "4.3 Sketched MRR: Statistical Perspective", "text": "In Figure 3, we plot the analytical expressions for the squared bias, variance, and risk stated in Theorem 4 against the regularization parameter \u03b3. Because the analytical expressions involve the random sketching matrix S, we randomly generate S, repeat this procedure 10 times, and report the average of the computed squared biases, variances, and risks. We fix \u03be = 0.1. The results of this experiment match our theory: classical sketch magnified the variance, and Hessian sketch increased the bias. Even when \u03b3 is fine-tuned, the risks of classical and Hessian sketches can be much higher than those of the optimal solution. Our experiment also indicates that classical and Hessian sketches require setting \u03b3 larger than the best regularization parameter for the optimal solution W?.\n16\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 20: .\n.\nBias2 Var Risk = Bias2 + Var\nWc\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nB ia s2 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nV ar ia nc e 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nR is k 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2 the min risk of the classical sketch the min risk of the optimal solution optimal \u03b3 for the optimal solution optimal \u03b3 for the classical sketch\nWh\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nB ia s2 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nV ar ia nc e 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\n\u03b3 10-12 10-10 10-8 10-6 10-4 10-2\nR is k 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2 the min risk of the Hessian sketch the min risk of the optimal solution optimal \u03b3 for the optimal solution optimal \u03b3 for the Hessian sketch\nClassical sketch and Hessian sketch do not outperform each other in terms of the risk. When variance dominates bias, Hessian sketch is better in terms of the risk; when bias dominates variance, classical sketch is better. In the experiment yielding Figure 3, Hessian sketch had lower risk than classical sketch. This is not generally true: if we used a smaller \u03be, so that the variance is dominated by bias, then classical sketch results in lower risks than Hessian sketch."}, {"heading": "4.4 Model Averaging: Optimization Objective", "text": "We use different intensity of noise\u2014we set \u03be = 10\u22122 or 10\u22121, where \u03be defined in Section 4.1 indicates the intensity of the noise in the response vector y. We calculate the objective function values f(wc[g]) and f(w h [g]) under different settings of g, \u03b3. We use different matrix sketching but fix the sketch size s = 5, 000. Theorem 8 shows that for large s, e.g., Gaussian projection with s = O\u0303 (\u03b2d ) , then\nf ( wc[g] ) \u2212 f ( w? ) \u2264 ( g + \u03b2 2 2 ) f(w?), (9)\nwhere \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1. In Figure 4(a) we plot g against the ratio\nf(wc[1])\u2212 f(w ?) f(wc[g])\u2212 f(w?) . (10)\n17\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 22: .\n23\n\u21e0 = 10 2 \u21e0 = 10 1\n= 10 12\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n= 10 6\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\nFigure 1: avg obj nb classical\nh\n2\n(a) Classical sketch with model averaging.\n\u21e0 = 10 2 \u21e0 = 10 1\n= 10 12\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n= 10 6\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\nFigure 2: avg obj nb hessian\nh\n3\n(b) Hessian sketch with model averaging.\nFigure 4: Empirical study of model averaging from optimization perspective. The x-axis is g, i.e. the number of samples over which model averaging averages. In 4(a), the y-axis is the ratio (log-scale) defined in (10). In 4(b), the y-axis is the ratio (log-scale) defined in (11).\nRapid growth of the ratio indicates high effectiveness of the model averaging. The results in Figure 4(a) indicate model averaging significantly improves the accuracy in terms of the objective function value. For the three random projection methods, the growth rate is almost linear in g. In Figure 4(a), we observe that the regularization parameter \u03b3 affects the ratio (10). The ratio grows faster when \u03b3 = 10\u221212 than when \u03b3 = 10\u22126. However, this phenomenon cannot be explained by our theory.\nTheorem 9 shows that for large sketch size s, e.g., Gaussian projection with s = O\u0303 (\u03b22d ) ,\nthen\nf(wh)\u2212 f(w?) \u2264 ( g2 + 2 \u03b22 )( \u2016y\u201622 n \u2212 f(w ?) ) ,\nwhere \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1. In Figure 4(b), we plot g against the ratio\nf(wh[1])\u2212 f(w ?) f(wh[g])\u2212 f(w?) . (11)\nRapid growth of the ratio indicates high effectiveness of the model averaging. If \u03b22 \u2264 1 g2 , equivalently s is at least O\u0303(dg2), the ratio (11) should grow nearly quadratically with g. However, the requirement on the sketch size s can be hardly satisfied, unless s is big and g is small. The empirical results reflects that the growth rate is moderately rapid for very small g and very slow for large g. This is in accordance with the theory.\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\n00.5110 -12\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 21: .\n22\ns = 1, 000 s = 5, 000\n= 10 12\n0 10 20 30 40 50 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\ng\nVa ria nc e\n0 10 20 30 40 50 10\u22125\n10\u22124\n10\u22123\n10\u22122\ng\nVa ria nc e\n= 10 6\n0 10 20 30 40 50 10\u22126\n10\u22125\n10\u22124\n10\u22123\ng\nVa ria nc e\n0 10 20 30 40 50 10\u22126\n10\u22125\n10\u22124\ng\nVa ria nc e\nFigure 3: avg var nb classical\nh\n4\n(a) The variance var(wc[g]).\ns = 1, 000 s = 5, 000\n= 10 12\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n= 10 6\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\nFigure 4: avg varratio nb classical\nh\n5\n(b) The ratio var(wc[1])\nvar(wc [g]\n) ."}, {"heading": "4.5 Model Averaging: Statistical Perspective", "text": "We study the model averaging from the statistical perspective. We calculate bias(w?), var(w?) (optimal solution) according to Theorem 4 and bias(wc[g]), var(w c [g]) (classical sketch) and bias(wh[g]), var(w h [g]) (Hessian sketch) according to Theorem 10."}, {"heading": "4.5.1 Classical Sketch", "text": "Theorem 11 shows that for large enough s, e.g., Gaussian projection with s = O\u0303 ( d 2 ) , then\nbias(wc[g])\nbias(w?) \u2264 1 + and\nvar(wc[g])\nvar(w?) \u2264 n s (\u221a 1+ /g g + )2 hold with high probability. The theorem indicates that model averaging does not make the bias worse and that it improves the variance. We conduct experiments to verify this point.\nIn Figure 5(a) we plot g agains the variance var(wc[g]); the variance of the optimal solution w? is employed for comparison. Clearly, the variance drops as g grows. In particular, when s is big (s = 5, 000) and g exceeds ns (= 100,000 5,000 = 20), var(w c [g]) can be even lower than var(w?). This verifies Corollary 12. This has an important implication: if y is corrupted by intense noise, we can use the classical sketch with model averaging to obtain a solution which has lower variance than the optimal solution w?.\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 22: .\n23\n1 10 20 30 40 50 1\n1.5\n2\n2.5\n3\n3.5\n4\ng\nR at io\n(a) s = 1, 000\n1 10 20 30 40 50 1\n1.5\n2\n2.5\n3\n3.5\n4\ng\nR at io\n(b) s = 2, 000\n1 10 20 30 40 50 1\n1.5\n2\n2.5\n3\n3.5\n4\ng\nR at io\n(c) s = 5, 000\nFigure 5: avg biasratio nb hessian\nh\nFigure 6: Empirical study of the bias of Hessian sketch with model averaging. The x-axis is g (number of samples over which model averaging averages); the y-axis is the ratio (12).\nTo make the decrease of var(wc[g]) more clear, in Figure 5(b) we plot g against the ratio var(wc\n[1] )\nvar(wc [g] ) . According to Theorem 11, this ratio grows linearly with g if s is at least O\u0303(dg). Otherwise, the ratio is sublinear with g. The theory is verified by the empirical results in Figure 5(b).\nWe plotted g against bias(wc[g]) in the same way as Figures 5(a) and 5(b). All the curves of g against the bias are almost horizontal, indicating that the increase of g does make the bias better or worse. We do not show the plots in the paper because these nearly horizontal curves are not interesting."}, {"heading": "4.5.2 Hessian Sketch", "text": "Theorem 13 shows that for large enough s, e.g., Gaussian projection with s = O\u0303 ( d 2 ) , the inequalities\nbias(wh[g])\nbias(w?) \u2264 1 + + ( g + 2 )\u2016X\u201622 n\u03b3\nand var(wh[g])\nvar(w?) \u2264 1 +\nhold with high probability. Theoretically speaking, model averaging improves the bias without making the variance worse. The bound\nbias(wh[g])\u2212 bias(w ?)\nbias(w?) \u2264 + ( g + 2 )\u2016X\u201622 n\u03b3\nindicates that if (1) n\u03b3 is much smaller than \u2016X\u201622 and (2) \u2264 1g , equivalently s is at least O\u0303(dg2), then the ratio is proportional to g .\nTo verify Theorem 13, we set \u03b3 very small\u2014\u03b3 = 10\u221212\u2014and vary s and g. In Figure 6 we plot the ratio\nbias(wh[1])\u2212 bias(w ?) bias(wh[g])\u2212 bias(w?) , (12)\n20\nby fixing \u03b3 = 10\u221212 and vary s and g. Ideally, for large sketch size s = O\u0303(dg2), the ratio should grow nearly linearly with g. Figure 6 shows only for large s and very small g, the growth can be near linear with g; this verifies our theory.\nWe have also plotted g against var ( wh[g] ) . We observe that var ( wh[g] ) remains nearly\nunaffected as g grows from 1 to 50. Since the curves of g against var ( wh[g] ) are almost horizontal lines, we do not show the plot in the paper."}, {"heading": "5. Model Averaging Experiments on Real-World Data", "text": "In Section 1 we mentioned that in the distributed setting where the feature-response pairs (x1,y1), \u00b7 \u00b7 \u00b7 , (xn,yn) \u2208 Rd\u00d7m are stored across g machines, classical sketch with model averaging requires only one round of communication, and is therefore a communicationefficient algorithm that can be used to: (1) obtain an approximate solution of the MRR problem with risk comparable to a batch solution, and (2) obtain a low-precision solution of the MRR optimization problem that can be used as an initializer for more communicationintensive optimization algorithms. In this section, we demonstrate both applications.\nWe use the Million Song Year Prediction Dataset, which has 463, 715 training samples and 51, 630 test samples with 90 features and one response. We normalize the data by shifting the responses to have zero mean and scaling the range of each feature to [\u22121, 1]. We randomly partition the training data into g parts, which amounts to uniform row selection with sketch size s = ng ."}, {"heading": "5.1 Prediction Error", "text": "We tested the prediction performance of sketched ridge regression by implementing classical sketch with model averaging in PySpark (Zaharia et al., 2010).5 We ran our experiments using PySpark in local mode; the experiments had three steps: (1) use five-fold crossvalidation to determine the regularization parameter \u03b3; (2) learn the model w using\n5. The code is available at https://github.com/wangshusen/SketchedRidgeRegression.git\n\u2016w?\u20162 .\nthe selected \u03b3; and (3) use w to predict on the test set and record the mean squared errors (MSEs). These steps map nicely onto the Map-Reduce programming model used by PySpark.\nWe plot g = ns against the MSE in Figure 7. As g grows, the sketch size s = n g decreases, so the performance of classical sketching deteriorates. However classical sketch with model averaging always has MSE comparable to the optimal solution."}, {"heading": "5.2 Optimization Error", "text": "We mentioned earlier that classical sketch with or without model averaging can be used to initialize optimization algorithms for solving MRR. If w is initialized with zero-mean random variables or deterministically with zeros, then E\u2016w \u2212 w?\u20162/\u2016w?\u20162 \u2265 1. Any w with the above ratio substantially smaller than 1 provides a better initialization. We implemented classical sketch with and without model averaging in Python and calculated the above ratio on the training set of the Year Prediction dataset; to estimate the expectation, we repeated the procedure 100 times and report the average of the ratios.\nIn Figure 8 we plot g against the average of the ratio \u2016w\u2212w ?\u20162\n\u2016w?\u20162 at different settings of the regularization parameter \u03b3. Clearly, classical sketch does not give a very good initialization unless g is small (equivalently, the sketch size s = ng is large). In contrast, the averaged solution is always close to w?."}, {"heading": "6. Sketch of Proof", "text": "In this section, we provide an outline of the proofs of our main results. Detailed proofs can be found in the Appendix. Section 6.1 shows some key properties of matrix sketching. Section 6.2 considers taking average of sketched matrices; the results will be applied to analyze sketched MRR with model averaging. Sections 6.3 to 6.6 establish the structural results of sketched MRR with or without model averaging.\nOur main results in Section 3 (Theorems 1, 2, 5, 7, 8, 9, 11, 13) directly follow from the key properties of matrix sketching and the structural results of sketched MRR. Table 4 summarizes the dependency relationship among the theorems. For example, Theorem 1, which studies classical sketch from optimization perspective, is one of our main theorems and can be proved using Theorems 16 and 21."}, {"heading": "6.1 Properties of Matrix Sketching", "text": "Our analysis of the sketched MRR uses some of the three key properties defined in Assumption 1. Theorem 16 establishes that the six sketching methods consider in this paper under different settings enjoy the three key properties. Finally, we show in Theorem 17 the lower bounds of \u2016S\u201622; the theorem will be used to prove the lower bounds of variance in Theorem 5.\nIn Assumption 1, the subspace embedding property is that sketching preserves the product of a row orthogonal matrix and its transpose. Equivalently, it ensures that all the singular values of any sketched column orthogonal matrix are around one. The matrix multiplication property states that sketching preserves the multiplication of a row orthogonal matrix and an arbitrary matrix. The bounded spectral norm property is that the spectral norm of S \u2208 Rn\u00d7s is around ns .\nAssumption 1 Let \u03b7, \u2208 (0, 1) be any fixed parameters. Let B be any matrix of proper size, \u03c1 = rank(X), and U \u2208 Rn\u00d7\u03c1 be the orthogonal bases of X. Let S \u2208 Rn\u00d7s be a sketching matrix where s depends on \u03b7 and/or . Assume S satisfies\n1.1 \u2225\u2225UTSSTU\u2212 I\u03c1\u2225\u22252 \u2264 \u03b7 (Subspace Embedding Property);\n1.2 \u2225\u2225UTSSTB\u2212UTB\u2225\u22252\nF \u2264 \u2016B\u20162F (Matrix Multiplication Property);\n1.3 \u2016S\u201622 \u2264 \u03b8ns for some constant \u03b8 (Bounded Spectral Norm Property).\nRemark 14 Note that the first two assumptions were identified in (Mahoney, 2011) and are the relevant structural conditions needed to be satisfied to obtain strong results from the optimization perspective. The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.\nRemark 15 We note that UTU = I\u03c1, and thus Assumption 1.1 can actually be expressed in the form of an approximate matrix multiplication bound (Drineas et al., 2006a). We call it the Subspace Embedding Property since, as first highlighted in Drineas et al. (2006b), this subspace embedding property is the key result to obtain high-quality sketching algorithms for regression and related problems.\nTheorem 16 shows that the six sketching methods with sufficiently large s satisfy the three properties. We prove Theorem 16 in Appendix B. Theorem 16 shows that for all the sketching methods except leverage score sampling,6 \u2016S\u201622 has nontrivial upper bound. That is why Theorems 5 and 11 are not applicable to leverage score sampling. That is also the motivation of using the shrinked leverage score sampling.\nTheorem 16 Fix failure probability \u03b4 and error parameters \u03b7 and ; set the sketch size s as Table 5. Assumption 1.1 is satisfied with probability at least 1 \u2212 \u03b41. Assumption 1.2 is satisfied with probability at least 1 \u2212 \u03b42. Assumption 1.3 is satisfied either surely or with probability close to one; the parameter \u03b8 is shown in Table 5.\nTheorem 17 establishes lower bound on \u2016S\u201622. The results will be applied to prove the lower bound of the variance of classical sketch. From Table 6 we can see that the bound of (shrinked) leverage score sampling is not interesting, because \u00b5 can be very large. That is why in Theorems 5, the shrinked leverage score sampling lacks nontrivial lower bound. We prove Theorem 17 in Appendix B.\nTheorem 17 (Lower Bound on the Spectral Norm of Sketching Matrix) The sketching methods and \u03d1 are described in Table 6. Then STS \u03d1ns Is holds either surely or with probability close to one.\n6. If one leverage score approaches zero, then the corresponding sampling probability pi goes to zero. By the definition of S, the scaling 1\u221a\nspi goes to infinity, which makes \u2016S\u201622 unbounded.\nRemark 18 Let p1, \u00b7 \u00b7 \u00b7 , pn be the sampling probabilities. By the definition of S, the nonzero entries of S can be any of 1\u221aspi , for i \u2208 [n].\nFor leverage score sampling, \u2016S\u201622 has neither nontrivial upper nor lower bound.7 It is because mini pi can be close to zero and maxi pi can be large (close to one).\nFor shinked leverage score sampling, because mini pi is at least 1 2n , \u2016S\u2016 2 2 has nontrivial upper bound; unfortunately, similar to leverage score sampling, maxi pi can be large, and thereby \u2016S\u201622 does not have nontrivial lower bound."}, {"heading": "6.2 Matrix Sketching with Averaging", "text": "We have shown that sketching can be applied to approximate matrix multiplication; see Assumptions 1.1 and 1.2. What will happen if we independently draw g sketches, approximately compute the multiplications, and average the g products? Intuitively, the averaged product should better approximate the true product.\nLet us justify the intuition formally. Let S1, \u00b7 \u00b7 \u00b7 ,Sg \u2208 Rn\u00d7s be some sketching matrices and A and B are arbitrary fixed matrices with proper size. It is not hard to show that\n1\ng g\u2211 i=1 ATSiS T i B = A TSSTB,\nwhere S = 1\u221ag [S1, \u00b7 \u00b7 \u00b7 ,Sg] \u2208 R n\u00d7gs. Clearly S is a sketching matrix larger than any of S1, \u00b7 \u00b7 \u00b7 ,Sg. If S1, \u00b7 \u00b7 \u00b7 ,Sg are column selection, SRHT, or Gaussian projection matrices, then S is the same type of sketching matrix.8\nTo analyze the sketched MRR with model averaging, we make the following assumptions. Here Assumption 2.1 is the subspace embedding property; Assumption 2.2 is the matrix multiplication property; Assumption 2.3 is the bounded spectral norm property.\nAssumption 2 Let \u03b7, \u2208 (0, 1) be any fixed parameters. Let B be any matrix of proper size, \u03c1 = rank(X), and U \u2208 Rn\u00d7\u03c1 be the orthogonal bases of X. Let S1, \u00b7 \u00b7 \u00b7 ,Sg \u2208 Rn\u00d7s be\n7. In our application, nontrivial bound means \u2016S\u201622 is of order ns . 8. The CountSketch does not enjoy such property. If Si \u2208 Rn\u00d7s is CountSketch matrix, then it has only\none non-zero entry in each row. In contrast, S \u2208 Rn\u00d7gs has g non-zero entries in each row; thus S is not CountSketch matrix.\ncertain sketching matrices and S = 1\u221ag [S1, \u00b7 \u00b7 \u00b7 ,Sg] \u2208 R n\u00d7gs; here s depends on \u03b7 and/or ."}, {"heading": "Assume Si and S satisfy", "text": "2.1 \u2225\u2225UTSiSTi \u2212 I\u03c1\u2225\u22252 \u2264 \u03b7 for all i \u2208 [g] and \u2225\u2225UTSSTU\u2212 I\u03c1\u2225\u22252 \u2264 \u03b7g ;\n2.2 ( 1 g \u2211g i=1 \u2225\u2225UTSiSTi B\u2212UTB\u2225\u2225F )2 \u2264 \u2016B\u20162F and \u2225\u2225UTSSTB\u2212UTB\u2225\u22252F \u2264 g\u2016B\u20162F ; 2.3 For some constant \u03b8, \u2016Si\u201622 \u2264 \u03b8ns for all i \u2208 [g], and \u2016S\u2016 2 2 \u2264 \u03b8ngs .\nTheorem 19 shows that random column selection, SRHT, and Gaussian projection matrices satisfy Assumptions 2.1, 2.2, 2.3. We prove Theorem 19 in Appendix B.\nTheorem 19 Let S1, \u00b7 \u00b7 \u00b7 ,Sg \u2208 Rn\u00d7s be the same type of random sketching matrices, which can be independently drawn random column selection, SRHT, or Gaussian projection matrices. Fix failure probability \u03b4 and error parameters \u03b7 and ; set the sketch size s as Table 5.\nThen Assumption 2.1 holds with probability at least 1\u2212 g\u03b41 \u2212 \u03b41. Assumption 2.2 holds with probability at least 1 \u2212 2\u03b42. Assumption 2.3 satisfied either surely or with probability close to one; the parameter \u03b8 is defined in Table 5.\nIn Theorem 16, Assumption 1.1 fails with probability at most \u03b41. In contrast, in Theorem 19, the counterpart assumption fails with probability at most (g+ 1)\u03b41. However, this makes little different, because \u03b41 is in the logarithm and can be set very small (recall Table 5).\nRemark 20 We do not know whether CountSketch enjoys the properties in Assumption 2. This problem is difficult for two reasons. First, as aforemented, the concatenation of multiple CountSketch matrices is not a CountSketch matrix. Second, the failure probability of the subspace embedding property of CountSketch is constant, rather than exponentially small."}, {"heading": "6.3 Sketched MRR: Optimization Perspective", "text": "Theorem 21 holds under the subspace embedding property and the matrix multiplication property (Assumptions 1.1 and 1.2). We prove Theorems 21 in Appendix C.\nTheorem 21 (Classical Sketch) Let Assumptions 1.1 and 1.2 hold for the sketching matrix S \u2208 Rn\u00d7s. Let \u03b7 and be defined in Assumption 1. Let \u03b1 = 2max{ ,\u03b7 2}\n1\u2212\u03b7 and\n\u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 . Then\nf(Wc)\u2212 f(W?) \u2264 \u03b1\u03b2f(W?).\nTheorem 22 holds under the subspace embedding property (Assumption 1.1). We prove Theorems 22 in Appendix C.\nTheorem 22 (Hessian Sketch) Let Assumption 1.1 hold for the sketching matrix S \u2208 Rn\u00d7s. Let \u03b7 be defined in Assumption 1 and \u03b2 = \u2016X\u2016 2 2\n\u2016X\u201622+n\u03b3 . Then\nf(Wh)\u2212 f(W?) \u2264 \u03b7 2\u03b22\n(1\u2212 \u03b7)2 ( \u2016Y\u20162F n \u2212 f(W?) ) ."}, {"heading": "6.4 Sketched MRR: Statistical Perspective", "text": "Theorem 23 holds under the subspace embedding property (Assumption 1.1) and the bounded spectral norm property (Assumption 1.3). The theorem shows that for classical sketch, the bias is close to the optimal solution, but the bound on variance is very weak. We prove Theorems 23 in Appendix D.\nTheorem 23 (Classical Sketch) Let \u03b7 and \u03b8 be defined in Assumption 1. Under Assumption 1.1, it holds that\n1 1 + \u03b7 \u2264 bias(W\nc)\nbias(W?) \u2264 1 1\u2212 \u03b7 .\nUnder Assumptions 1.1 and 1.3, it holds that\nvar(Wc)\nvar(W?) \u2264 (1 + \u03b7) (1\u2212 \u03b7)2 \u03b8n s .\nTheorem 24 establishes a lower bound on the variance of classical sketch. We prove Theorems 24 in Appendix D.\nTheorem 24 (Lower Bound on the Variance) Under Assumption 1.1 and the additional assumption that STS \u03d1ns Is, it holds that\nvar(Wc) var(W?) \u2265 1\u2212 \u03b7 (1 + \u03b7)2 \u03d1n s .\nTheorem 25 holds under the subspace embedding property (Assumption 1.1). In the theorem we define \u03c1 = rank(X) and \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3\u03c1 as the singular values of X. We prove Theorems 25 in Appendix D.\nTheorem 25 (Hessian Sketch) Let \u03b7 be defined in Assumption 1. Under Assumption 1.1, it holds that\nbias(Wh)\nbias(W?) \u2264 1 1\u2212 \u03b7\n( 1 +\n\u03b7\u03c321 n\u03b3\n) ,\n1 1 + \u03b7 \u2264 var(W\nh)\nvar(W?) \u2264 1 1\u2212 \u03b7 .\nFurther assume that \u03c32\u03c1 \u2265 n\u03b3 \u03b7 . Then\nbias(Wh)\nbias(W?) \u2265 1 1 + \u03b7 (\u03b7\u03c32\u03c1 n\u03b3 \u2212 1 ) ."}, {"heading": "6.5 Model Averaging: Optimization Perspective", "text": "Theorem 26 holds under the subspace embedding property (Assumption 2.1) and the matrix multiplication property (Assumption 2.2). We prove Theorems 26 in Appendix E.\nTheorem 26 (Classical Sketch with Model Averaging) Let \u03b7 and be defined in Assumption 2. Let \u03b1 = 2 [ max {\u221a\ng , \u03b7 g\n} + 2\u03b2 max { \u03b7 \u221a , \u03b72 }]2 and \u03b2 =\n\u2016X\u201622 \u2016X\u201622+n\u03b3\n\u2264 1. Under Assumption 2.1 and 2.2, we have that\nf(Wc)\u2212 f(W?) \u2264 \u03b1\u03b2f(W?).\nTheorem 27 holds under the subspace embedding property (Assumption 2.1). We prove Theorems 27 in Appendix E.\nTheorem 27 (Hessian Sketch with Model Averaging) Let \u03b7 be defined in Assumption 2. Let \u03b1 = (\u03b7 g + \u03b72 1\u2212\u03b7 ) and \u03b2 = \u2016X\u201622 \u2016X\u201622+n\u03b3 \u2264 1. Under Assumption 2.1, we have that\nf(Wh)\u2212 f(W?) \u2264 \u03b12\u03b22 ( 1 n \u2016Y\u20162F \u2212 f(W?) ) ."}, {"heading": "6.6 Model Averaging: Statistical Perspective", "text": "Theorem 28 requires the subspace embedding property (Assumption 2.1). In addition, to bound the variance, the spectral norms of S1, \u00b7 \u00b7 \u00b7 ,Sg and S = 1\u221ag [S1, \u00b7 \u00b7 \u00b7 ,Sg] must be bounded (Assumption 2.3). The theorem shows that model averaing improves the variance without making the bias worse. We prove Theorems 28 in Appendix F.\nTheorem 28 (Classical Sketch with Model Averaging) Under Assumption 2.1, it holds that\nbias(Wc)\nbias(W?) \u2264 1 1\u2212 \u03b7 .\nUnder Assumptions 2.1 and 2.3, it holds that\nvar(Wc)\nvar(W?) \u2264 \u03b8n s (\u221a 1 + \u03b7/g \u221a g + \u03b7 \u221a 1 + \u03b7 1\u2212 \u03b7 )2 ."}, {"heading": "Here \u03b7 and \u03b8 are defined in Assumption 2.", "text": "Theorem 29 requires the subspace embedding property (Assumption 2.1). It shows that model averaging improves the bias without making the variance worse. We prove Theorems 29 in Appendix F.\nTheorem 29 (Hessian Sketch with Model Averaging) Under Assumption 2.1, it holds that:\nbias(Wh)\nbias(W?) \u2264 1 1\u2212 \u03b7 + (\u03b7 g + \u03b72 1\u2212 \u03b7 )\u2016X\u201622 n\u03b3 , var(Wh)\nvar(W?) \u2264 1 1\u2212 \u03b7 .\nHere \u03b7 is defined in Assumption 2."}, {"heading": "7. Conclusions", "text": "We studied sketched matrix ridge regression (MRR) from optimization and statistical perspectives. Using classical sketch, by taking a large enough sketch, one can obtain an -accurate approximate solution. Counterintuitively and in contrast to classical sketch, the relative error of Hessian sketch increases as the responses Y are better approximated by linear combinations of the columns of X. Both classical and Hessian sketches can have statistical risks that are worse than the risk of the optimal solution by an order of magnitude.\nWe proposed the use of model averaging to attain better optimization and statistical properties. We have shown that model averaging leads to substantial improvements in the theoretical error bounds, suggesting applications in distributed optimization and machine learning. We also empirically verified its practical benefits."}, {"heading": "Appendix A. Risk of Fixed Design Model", "text": "Let the risk R(W) be defined in (8). The risk R(W) determines how well the learned W generalize to test data, which is the reason why we care about the risk. We formally explain this in the following.\nUnder the fixed design model, a test sample x is uniformly drawn from the set {x1, \u00b7 \u00b7 \u00b7 ,xn} \u2282 Rd. The corresponding response is y = \u3008W0,x\u3009+ \u03be\u2032 \u2208 Rm, where \u03be\u2032 \u2208 Rm captures random noise; assume that E[\u03be\u2032] = 0 and that \u03be\u2032 is independ of W, W0, and x. The test mean squared error (MSE) is\nEx,\u03be \u2225\u2225WTx\u2212 y\u2225\u22252\n2 = Ex,\u03be \u2225\u2225WTx\u2212WT0 x + W0x\u2212 y\u2225\u222522 = Ex,\u03be\n\u2225\u2225WTx\u2212WT0 x\u2212 \u03be\u2032\u2225\u222522 = Ex,\u03be\n[\u2225\u2225WTx\u2212WT0 x\u2225\u222522 + \u2225\u2225\u03be\u2032\u2225\u222522] = 1\nn n\u2211 i=1 \u2225\u2225WTxi \u2212WT0 xi\u2225\u222522 + E\u2225\u2225\u03be\u2032\u2225\u222522 = R(W) + E \u2225\u2225\u03be\u2032\u2225\u22252 2 .\nTo this end, it is clear that the test MSE equals to the (training) risk R(W ) plus the \u201cintensity\u201d of the noise in test response."}, {"heading": "Appendix B. Properties of Matrix Sketching: Proofs", "text": "In Section B.1 we prove Theorem 16. In Section B.2, we prove Theorem 17. In Section B.3 we prove Theorem 19."}, {"heading": "B.1 Proof of Theorem 16", "text": "We prove the six sketching methods considered in this paper all satisfy the three key properties. In Section B.1.1 we show the six sketching methods satisfy Assumptions 1.1 and 1.2. In section B.1.2 we show the six sketching methods satisfy Assumption 1.3."}, {"heading": "B.1.1 Proof of Assumptions 1.1 and 1.2", "text": "For uniform sampling, leverage score sampling, Gaussian projection, SRHT, and CountSketch, the subspace embedding property and matrix multiplication property have been established by the previous work (Drineas et al., 2008, 2011, Meng and Mahoney, 2013, Nelson and Nguye\u0302n, 2013, Tropp, 2011, Woodruff, 2014). See also (Wang et al., 2016c) for the summary.\nIn the following we prove only the shrinked leverage score sampling. We cite the following lemma from (Wang et al., 2016b); the lemma was firstly proved by the work (Drineas et al., 2008, Gittens, 2011, Woodruff, 2014).\nLemma 30 (Wang et al. (2016b)) Let U \u2208 Rn\u00d7\u03c1 be any fixed matrix with orthonormal columns. The column selection matrix S \u2208 Rn\u00d7s samples s columns according to arbitrary probabilities p1, p2, \u00b7 \u00b7 \u00b7 , pn. Assume \u03b1 \u2265 \u03c1 and\nmax i\u2208[n] \u2016ui:\u201622 pi \u2264 \u03b1.\nWhen s \u2265 \u03b16+2\u03b7 3\u03b72 log(\u03c1/\u03b41), it holds that\nP {\u2225\u2225I\u03c1 \u2212UTSSTU\u2225\u22252 \u2265 \u03b7} \u2264 \u03b41."}, {"heading": "When s \u2265 \u03b1 \u03b42 , it holds that", "text": "E \u2225\u2225UB\u2212UTSSTB\u2225\u22252\nF \u2264 \u03b42 \u2016B\u20162F ;\nas a consequence of Markov\u2019s inequality, it holds that\nP {\u2225\u2225UB\u2212UTSSTB\u2225\u22252\nF \u2265 \u2016B\u20162F\n} \u2264 \u03b42.\nHere the expectation and probability are all w.r.t. the randomness in S.\nNow we apply the above lemma to analyze shrinked leverage score sampling. For the approximate shrinked leverage scores defined in (5), the sampling probabilities satisfy\npi = 1\n2 ( 1 n + l\u0303i\u2211n q=1 l\u0303q ) \u2265 \u2016ui:\u2016 2 2 2\u03c4\u03c1 .\nHere l\u0303i and \u03c4 are defined in (5). Thus for all i \u2208 [n], \u2016ui:\u201622 \u03c1 < 2\u03c4\u03c1. We can then apply Lemma 30 to show that Assumption 1.1 holds with probability at least 1 \u2212 \u03b41 when s \u2265 2\u03c4\u03c16+2\u03b7\n3\u03b72 log(\u03c1/\u03b41) and that Assumption 1.2 holds with probability at least 1 \u2212 \u03b42 when\ns \u2265 2\u03c4\u03c1 \u03b42 ."}, {"heading": "B.1.2 Proof of Assumption 1.3", "text": "For Uniform Sampling and SRHT, it is easy to show that STS = ns Is. Thus \u2016S\u2016 2 2 = n s .\nLet {psi} and {pui } be the sampling probabilites of the leverage score sampling and uniform sampling, respectively. Obviously psi \u2265 12p u i . Thus for the shrinked leverage score sampling, \u2016S\u201622 \u2264 2ns . Vershynin (2010) showed that the greatest singular value of the standard Gaussian matrix G \u2208 Rn\u00d7s is at most \u221a n + \u221a s + t with probability at least 1 \u2212 2e\u2212t2/2. Thus for Gaussian projection matrix S,\n\u2016S\u201622 = 1\ns \u2016G\u201622 \u2264\n( \u221a n+ \u221a s+ t)2\ns\nholds with probability at least 1\u2212 2e\u2212t2/2. If S is the CountSketch matrix, then each row of S has exactly one nonzero entry, either 1 or \u22121. Because the columns of S are orthogonal to each other, it holds that\n\u2016S\u201622 = max i\u2208[s] \u2016s:i\u201622 = max i\u2208[s] nnz(s:i).\nThe problem of bounding nnz(s:i) is equivalent to assigning n balls into s bins uniformly at random and bounding the number of balls in the bins. Patrascu and Thorup (2012) showed that the maximal number of balls in any bin is at most n/s+O (\u221a n/s logc n ) with probability at least 1\u2212 1n , where c = O(1). Thus\n\u2016S\u201622 = max i\u2208[s]\nnnz(s:i) \u2264 n\ns +O\n(\u221a n logc n\u221a\ns\n) = n\ns\n( 1 + o(1) ) holds with probability at least 1\u2212 1n ."}, {"heading": "B.2 Proof of Theorem 17", "text": "For uniform sampling and SRHT, it holds that STS = ns Is. For non-uniform sampling with probabilities p1, \u00b7 \u00b7 \u00b7 , pn ( \u2211 i pi = 1), let pmax = maxi pi. The smallest entry in S is 1\u221aspmax , and thus S TS 1spmax Is. For the leverage score sampling, pmax = \u00b5 n . For the shrinked leverage score sampling, pmax = 1+\u00b5 2n . The lower bound on \u2016S\u2016 2 2 is established. Vershynin (2010) showed that the smallest singular value of any n\u00d7s standard Gaussian matrix G is at least \u221a n\u2212 \u221a s \u2212 t with probability at least 1 \u2212 2e\u2212t2/2. If S = 1\u221a\ns G is the\nGaussian projection matrix, the smallest eigenvalue of STS is (1\u2212 o(1))ns with probability very close to one.\nIf S is the CountSketch matrix, then each row of S has exactly one nonzero entry, either 1 or \u22121. Because the columns of S are orthogonal to each other, it holds that\n\u03c32min(S) = min i\u2208[s] \u2016s:i\u201622 = min i\u2208[s] nnz(s:i).\nThe problem of bounding nnz(s:i) is equivalent to assigning n balls into s bins uniformly at random and bounding the number of balls in the bins. Standard concentration argument can show that each bin has at least ns (1 \u2212 o(1)) balls w.h.p. Hence \u03c3 2 min(S) \u2265 ns (1 \u2212 o(1)) w.h.p."}, {"heading": "B.3 Proof of Theorem 19", "text": "Assumption 2.1. By Theorem 16 and the union bound, we have that \u2225\u2225UTSiSTi \u2212I\u03c1\u2225\u22252 \u2264 \u03b7 hold simultaneously for all i \u2208 [g] with probability at least 1 \u2212 g\u03b41. Because S \u2208 Rn\u00d7gs is the same type of sketching matrix, it follows from Theorem 16 that\n\u2225\u2225UTSSTU\u2212 I\u03c1\u2225\u22252 \u2264 \u03b7g holds with probability at least 1\u2212 \u03b41.\nAssumption 2.2. By the same proof of Theorem 16, we can easily show that\nE \u2225\u2225UTB\u2212UTSiSTi B\u2225\u22252F \u2264 \u03b42 \u2016B\u20162F ,\nwhere B is any fixed matrix and the expectation is taken w.r.t. S. It follows from Jensen\u2019s inequality that\n( E \u2225\u2225UTSiSTi B\u2212UTB\u2225\u2225F)2 \u2264 E\u2225\u2225UTSiSTi B\u2212UTB\u2225\u22252F \u2264 \u03b42 \u2225\u2225B\u2225\u22252F .\nIt follows that\n1\ng g\u2211 i=1 E \u2225\u2225UTSiSTi B\u2212UTB\u2225\u2225F \u2264 \u221a\u03b42 \u2225\u2225B\u2225\u2225F ,\nand thus\n(1 g g\u2211 i=1 E \u2225\u2225UTSiSTi B\u2212UTB\u2225\u2225F)2 \u2264 \u03b42 \u2225\u2225B\u2225\u22252F .\nIt follows from Markov\u2019s bound that\nP {(1\ng g\u2211 i=1 \u2225\u2225UTSiSTi B\u2212UTB\u2225\u2225F)2 \u2264 \u2225\u2225B\u2225\u22252F} \u2265 1\u2212 \u03b42. Because S \u2208 Rn\u00d7gs is the same type of sketching matrix, it follows from Theorem 16 that\u2225\u2225UTSSTB\u2212UTB\u2225\u22252\nF \u2264 g\u2016B\u2016 2 F holds with probability at least 1\u2212 \u03b42.\nAssumption 2.3. Theorem 16 shows that \u2016Si\u201622 can be bounded either surely or with probability very close to 1 (assume n is big enough). Because g n, \u2016Si\u201622 can be bounded simultaneously for all i \u2208 [g] either surely or with probability close to 1. Because S \u2208 Rn\u00d7gs is the same type of sketching matrix, it follows from Theorem 16 that \u2016S\u201622 \u2264 ngs holds either surely or with probability very close to 1."}, {"heading": "Appendix C. Sketched MRR from Optimization Perspective: Proofs", "text": "In Section C.1 we establish a key lemma. In Section C.2 we prove Theorem 21. In Section C.3 we prove Theorem 22."}, {"heading": "C.1 Key Lemma", "text": "Recall that objective function of the matrix ridge regression (MRR)is\nf(W) , 1\nn\n\u2225\u2225XW \u2212Y\u2225\u22252 F + \u03b3\u2016W\u20162F .\nThe optimal solution is W? = argminW f(W). The following is the key lemma for decomposing difference between any W and W?.\nLemma 31 For any matrix W and any nonsingular matrix M of proper size, it holds that\nf(W) = 1 n tr [ YTY \u2212 (2W? \u2212W)T (XTX + n\u03b3In)W ] ,\nf(W?) = 1\nn\n[\u2225\u2225Y\u22a5\u2225\u22252 F + n\u03b3 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F ],\nf(W)\u2212 f(W?) = 1 n \u2225\u2225\u2225(XTX + n\u03b3Id)1/2(W \u2212W?)\u2225\u2225\u22252 F ,\u2225\u2225\u2225M\u22121(W \u2212W?)\u2225\u2225\u22252\nF \u2264 \u03c3\u22122min\n[ (XTX + n\u03b3Id) 1/2M ] \u2225\u2225\u2225(XTX + n\u03b3Id)1/2(W \u2212W?)\u2225\u2225\u22252\nF .\nHere X = U\u03a3VT is the SVD; Y\u22a5 = Y \u2212XX\u2020Y.\nProof Let U be the left singular vectors of X. The objective function f(W) can be written as\nf(W) = 1\nn\n\u2225\u2225XW \u2212Y\u2225\u22252 F + \u03b3 \u2225\u2225W\u2225\u22252 F\n= 1 n tr [ YTY \u2212 (2W? \u2212W)T (XTX + n\u03b3In)W ] .\nThen\nf(W?) = 1 n tr [ YT ( In \u2212X(XTX + n\u03b3Id)\u22121XT ) Y ]\n= 1 n tr [ YT ( In \u2212U(I\u03c1 + n\u03b3\u03a3\u22122)\u22121UT ) Y ] = 1 n tr [ YTY \u2212YTUUTY + YTUUTY \u2212YTU(I\u03c1 + n\u03b3\u03a3\u22122)\u22121UTY\n] = 1\nn\n{ tr [ YT (In \u2212UUT )Y ] + n\u03b3 \u00b7 tr [ YTU ( \u03a32 + n\u03b3I\u03c1 )\u22121 UTY ]} = 1\nn\n[\u2225\u2225Y\u22a5\u2225\u22252 F + n\u03b3 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F ].\nThe difference in the objective function value is\nf(W)\u2212 f(W?) = 1 n\ntr [ (W \u2212W?)T (XTX + n\u03b3Id)(W \u2212W?) ] = 1\nn \u2225\u2225\u2225(XTX + n\u03b3Id)1/2(W \u2212W?)\u2225\u2225\u22252 F .\nBecause \u03c3min(A)\u2016B\u2016F \u2264 \u2016AB\u2016F holds for any nonsingular A and any B, it holds for any nonsingular matrix M that\n\u03c32min [ (XTX + n\u03b3Id) 1/2M ]\u2225\u2225\u2225M\u22121(W \u2212W?)\u2225\u2225\u22252\nF \u2264 \u2225\u2225\u2225(XTX + n\u03b3Id)1/2MM\u22121(W \u2212W?)\u2225\u2225\u22252 F\n= \u2225\u2225\u2225(XTX + n\u03b3Id)1/2(W \u2212W?)\u2225\u2225\u22252\nF .\nThe last claim in the lemma follows from the above inequality."}, {"heading": "C.2 Proof of Theorem 21", "text": "Proof Let \u03c1 = rank(X), U \u2208 Rn\u00d7\u03c1 be the left singular vectors of X, and Y\u22a5 = Y \u2212 XX\u2020Y = Y \u2212UUTY. It follows from the definition of W? and Wc that\nWc \u2212W? = (XTSSTX + n\u03b3Id)\u22121XTSSTY \u2212 (XTX + n\u03b3Id)\u22121XTY.\nIt follows that\n(XTSSTX + n\u03b3Id)(W c \u2212W?)\n= XTSSTY\u22a5 + XTSSTXX\u2020Y \u2212 (XTSSTX + n\u03b3Id)(XTX + n\u03b3Id)\u22121XTY = XTSSTY\u22a5 \u2212 n\u03b3X\u2020Y + (XTSSTX + n\u03b3Id) [ X\u2020 \u2212 (XTX + n\u03b3Id)\u22121XT ] Y = XTSSTY\u22a5 \u2212 n\u03b3X\u2020Y + n\u03b3(XTSSTX + n\u03b3Id)(XTX + n\u03b3Id)\u22121X\u2020Y = XTSSTY\u22a5 + n\u03b3(XTSSTX\u2212XTX)(XTX + n\u03b3Id)\u22121X\u2020Y.\nIt follows that\n(XTX + n\u03b3Id) \u22121/2(XTSSTX + n\u03b3Id)(W c \u2212W?) = A + B, (13)\nwhere\nA = [ (XTX + n\u03b3Id) 1/2 ]\u2020 XTSSTY\u22a5 = V(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3USSTY\u22a5,\nB = n\u03b3 [ (XTX + n\u03b3Id) 1/2 ]\u2020\n(XTSSTX\u2212XTX)(XTX + n\u03b3Id)\u2020X\u2020Y = n\u03b3V(\u03a32 + n\u03b3I\u03c1)\n\u22121/2\u03a3(UTSSTU\u2212 I\u03c1)\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3\u22121UTY = n\u03b3V\u03a3(\u03a32 + n\u03b3I\u03c1) \u22121/2(UTSSTU\u2212 I\u03c1)(\u03a32 + n\u03b3I\u03c1)\u22121UTY.\nIt follows from (13) that\n(XTX + n\u03b3Id) 1/2 ( Wc \u2212W? ) = [ (XTX + n\u03b3Id) \u22121/2(XTSSTX + n\u03b3Id)(X TX + n\u03b3Id)\n\u22121/2]\u2020(A + B) By Assumption 1.1, we have that\n(1\u2212 \u03b7)(XTX + n\u03b3Id) (XTSSTX + n\u03b3Id) (1 + \u03b7)(XTX + n\u03b3Id)\nIt follows that\u2225\u2225\u2225[(XTX + n\u03b3Id)\u22121/2(XTSSTX + n\u03b3Id)(XTX + n\u03b3Id)\u22121/2]\u2020\u2225\u2225\u2225 2 \u2264 1 1\u2212 \u03b7 . Thus\u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wc \u2212W?)\u2225\u2225\u22252 F \u2264 1 1\u2212 \u03b7 \u2225\u2225\u2225A + B\u2225\u2225\u22252 F \u2264 2 1\u2212 \u03b7 (\u2225\u2225A\u2225\u22252 F + \u2225\u2225B\u2225\u22252 F ) .\nLemma 31 shows f ( Wc ) \u2212 f ( W? ) = 1\nn \u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wc \u2212W?)\u2225\u2225\u22252 F \u2264 2 n(1\u2212 \u03b7) (\u2225\u2225A\u2225\u22252 F + \u2225\u2225B\u2225\u22252 F ) . (14)\nWe respectively bound \u2016A\u20162F and \u2016B\u20162F in the following. It follows from Assumption 1.2 and UTY\u22a5 = 0 that\n\u2016A\u20162F = \u2225\u2225\u2225V(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3USSTY\u22a5\u2225\u2225\u22252\nF \u2264 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3\u2225\u222522 \u2225\u2225UTSSTY\u22a5 \u2212UTY\u22a5\u2225\u22252F\n\u2264 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3\u2225\u222522 \u2225\u2225Y\u22a5\u2225\u22252F .\nBy the definition of B, we have \u2016B\u20162F \u2264 n2\u03b32 \u2225\u2225\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2(UTSSTU\u2212 I\u03c1)(\u03a32 + n\u03b3I\u03c1)\u22121UTY\u2225\u22252F\n\u2264 n2\u03b32 \u2225\u2225\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2(UTSSTU\u2212 I\u03c1)(\u03a32 + n\u03b3I\u03c1)\u22121/2\u2225\u222522\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F\n= n2\u03b32 \u2225\u2225\u03a3N\u2225\u22252\n2 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F , where we define N = (\u03a32 +n\u03b3I\u03c1)\n\u22121/2(UTSSTU\u2212 I\u03c1)(\u03a32 +n\u03b3I\u03c1)\u22121/2. By Assumption 1.1, we have \u2212\u03b7(\u03a32 + n\u03b3I\u03c1)\u22121 N \u03b7(\u03a32 + n\u03b3I\u03c1)\u22121. It follows that\n\u2016B\u20162F \u2264 n2\u03b32 \u2225\u2225\u03a3N2\u03a3\u2225\u2225\n2 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F \u2264 \u03b72n2\u03b32\n\u2225\u2225\u03a3(\u03a32 + n\u03b3I\u03c1)\u22122\u03a3\u2225\u22252\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F = \u03b72n2\u03b32\n\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3\u2225\u222522\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F = \u03b72n\u03b3\n\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3\u2225\u222522\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2UTY\u2225\u22252F . The last equality follows from that \u2016(\u03a32 + n\u03b3I\u03c1)\u22121/2\u20162 \u2264 n\u03b3. It follows that\n\u2016A\u20162F + \u2016B\u20162F \u2264 max { , \u03b72 }\u2225\u2225\u2225(\u03a32 + n\u03b3Id)\u22121\u03a3\u2225\u2225\u2225 2 [\u2225\u2225Y\u22a5\u2225\u22252 F + n\u03b3 \u2225\u2225(\u03a32 + n\u03b3Id)\u22121/2UTY\u2225\u22252F ]\n\u2264 max { , \u03b72 } \u03c32max \u03c32max + n\u03b3 [\u2225\u2225Y\u22a5\u2225\u22252 F + n\u03b3 \u2225\u2225(\u03a32 + n\u03b3Id)\u22121/2UTY\u2225\u22252F ]\n\u2264 max { , \u03b72 } \u03b2nf(W?). (15)\nHere the last inequality follows from Lemma 31. Then the theorem follows from (15) and (14)."}, {"heading": "C.3 Proof of Theorem 22", "text": "Proof By the definition of Wh and W?, we have (XTX + n\u03b3Id) 1/2 ( Wh \u2212W? ) = (XTX + n\u03b3Id) 1/2 [ (XTSSTX + n\u03b3Id) \u2020 \u2212 (XTX + n\u03b3Id)\u2020 ] XTY\n= V(\u03a32 + n\u03b3I\u03c1) 1/2 [ (\u03a3UTSSTU\u03a3 + n\u03b3I\u03c1) \u2020 \u2212 (\u03a32 + n\u03b3I\u03c1)\u22121 ] \u03a3UTY.\nIt follows from Assumption 1.1 that UTSSTU has full rank, and thus (XTX + n\u03b3Id) 1/2 ( Wh \u2212W? ) = V(\u03a32 + n\u03b3I\u03c1) 1/2 [ (\u03a3UTSSTU\u03a3 + n\u03b3I\u03c1) \u22121 \u2212 (\u03a32 + n\u03b3I\u03c1)\u22121 ] \u03a3UTY\n= V(\u03a32 + n\u03b3I\u03c1) 1/2(\u03a32 + n\u03b3I\u03c1) \u22121(\u03a32 \u2212\u03a3UTSSTU\u03a3)(\u03a3UTSSTU\u03a3 + n\u03b3I\u03c1)\u22121\u03a3UTY = V(\u03a32 + n\u03b3I\u03c1) \u22121/2\u03a3(I\u03c1 \u2212UTSSTU)\u03a3(\u03a3UTSSTU\u03a3 + n\u03b3I\u03c1)\u22121\u03a3UTY.\nwhere the second equality follow from M\u22121 \u2212N\u22121 = N\u22121(N\u2212M)M\u22121. We define (XTX + n\u03b3Id) 1/2 ( Wh \u2212W? ) = VABC,\nwhere\nA = (\u03a32 + n\u03b3I\u03c1) \u22121/2\u03a3(I\u03c1 \u2212UTSSTU)\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2, B = (\u03a32 + n\u03b3I\u03c1) 1/2(\u03a3UTSSTU\u03a3 + n\u03b3I\u03c1) \u22121(\u03a32 + n\u03b3I\u03c1)1/2, C = (\u03a32 + n\u03b3I\u03c1) \u22121/2\u03a3UTY.\nIt follows from Assumption 1.1 that \u2016A\u20162 \u2264 \u03b7 \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a32(\u03a32 + n\u03b3I\u03c1)\u22121/2\u2225\u2225\u2225\n2 \u2264 \u03b7\u03b2,\n\u2016B\u20162 \u2264 (1\u2212 \u03b7)\u22121.\nIt holds that\u2225\u2225C\u2225\u22252 F \u2264 \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3UTY\u2225\u2225\u22252 F\n= [ tr ( YTUUTY ) \u2212 n\u03b3 tr ( YTU(\u03a32 + n\u03b3Id) \u22121UTY )]\n= [ \u2212 tr ( YT (Id \u2212UUT )Y ) \u2212 n\u03b3 tr ( YTU(\u03a32 + n\u03b3Id) \u2020UTY ) + tr ( YTY )] = ( \u2212 nf(W?) + \u2225\u2225Y\u2225\u22252 F ) ,\nwhere the last equality follows from Lemma 31. It follows from Lemma 31 that\nf(Wh)\u2212 f(W?) = 1 n \u2225\u2225(XTX + n\u03b3Id)1/2(Wh \u2212W?)\u2225\u22252F = 1\nn\n\u2225\u2225ABC\u2225\u22252 F \u2264 \u03b7 2\u03b22 (1\u2212 \u03b7)2 ( 1 n \u2225\u2225Y\u2225\u22252 F \u2212 f(W?) ) ."}, {"heading": "Appendix D. Sketched MRR from Statistical Perspective: Proofs", "text": "In Section D.1 we prove Theorem 4. In Section D.2 we prove Theorem 23. In Section D.3 we prove Theorem 24. In Section B.2 we prove Theorem 17. In Section D.4 we prove Theorem 25. Recall that the fixed design model is Y = XW0 + \u039e where \u039e is random, E\u039e = 0, and E[\u039e\u039eT ] = \u03be2In."}, {"heading": "D.1 Proofs of Theorem 4", "text": "We prove Theorem 4 in the following. In the proof we exploit several identities. The Frobenius norm and matrix trace satisfies that for any matrix A and B,\n\u2016A\u2212B\u20162F = tr [ (A\u2212B)(A\u2212B)T ) ] = tr(AAT ) + tr(BBT )\u2212 2tr(ABT ).\nThe trace is linear function, and thus for any fixed A and B and random matrix \u03a8 of proper size,\nE [ tr(A\u03a8B) ] = tr [ A(E\u03a8)B ] where the expectation is taken w.r.t. \u03a8. Proof It follows from the definition of the optimal solution W? in (2) that\nXW? = X(XTX + n\u03b3Id) \u2020XT (XW0 + \u039e)\n= U(\u03a32 + n\u03b3I\u03c1) \u22121\u03a33VTW0 + U(\u03a32 + n\u03b3I\u03c1)\u22121\u03a32UT\u039e = U [ I\u03c1 \u2212 n\u03b3(\u03a32 + n\u03b3I\u03c1)\u22121 ] \u03a3VTW0 + U(\u03a3 2 + n\u03b3I\u03c1) \u22121\u03a32UT\u039e\n= XW0 \u2212 n\u03b3U(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0 + U(\u03a32 + n\u03b3I\u03c1)\u22121\u03a32UT\u039e.\nSince E[\u039e] = 0 and E[\u039e\u039eT ] = \u03be2In, it holds that\nR(W?) = 1 n E \u2225\u2225XW? \u2212XW0\u2225\u22252F\n= 1\nn \u2225\u2225\u2225\u2212 n\u03b3(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0 + (\u03a32 + n\u03b3I\u03c1)\u22121\u03a32UT\u039e\u2225\u2225\u22252 F\n= n\u03b32 \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u2225\u22252\nF + \u03be2 n \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a32\u2225\u2225\u22252 F .\nThis shows the bias and variance of the optimal solution W?. We then decompose the risk function R ( Wc ) . It follows from the definition of Wc in (3) that\nXWc = X(XTSSTX + n\u03b3Id) \u2020XTSST (XW0 + \u039e) = U\u03a3 ( \u03a3UTSSTU\u03a3 + n\u03b3Id )\u2020 \u03a3 ( UTSSTU\u03a3VTW0 + U TSST\u039e )\n= U(UTSSTU + n\u03b3\u03a3\u22122)\u22121 [ (UTSSTU + n\u03b3\u03a3\u22122)\u03a3VTW0 \u2212 n\u03b3\u03a3\u22121VTW0 + UTSST\u039e ] = XW0 + U(U TSSTU + n\u03b3\u03a3\u22122)\u22121 ( \u2212 n\u03b3\u03a3\u22121VTW0 + UTSST\u039e ) .\nSince E[\u039e] = 0 and E[\u039e\u039eT ] = \u03be2In, it follows that\nR ( Wc ) = 1 n E \u2225\u2225XWc \u2212XW0\u2225\u22252F\n= 1\nn \u2225\u2225\u2225\u2212 n\u03b3(UTSSTU + n\u03b3\u03a3\u22122)\u22121\u03a3\u22121VTW0 + (UTSSTU + n\u03b3\u03a3\u22122)\u22121UTSST\u039e\u2225\u2225\u22252 F\n= n\u03b32 \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u22121\u03a3\u22121VTW0\u2225\u2225\u22252\nF + \u03be2 n \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u22121UTSST\u2225\u2225\u22252 F .\nThis shows the bias and variance of the approximate solution Wc. We then decompose the risk function R ( Wh ) . It follows from the definition of Wh in (4) that\nXWh \u2212XW0 = X(XTSSTX + n\u03b3In)\u2020XT (XW0 + \u039e)\u2212XW0 = X(XTSSTX + n\u03b3Id)\n\u2020XTXW0 \u2212XW0 + X(XTSSTX + n\u03b3Id)\u2020XT\u039e = U [ (UTSSTU + n\u03b3\u03a3\u22122)\u22121 \u2212 I\u22121\u03c1 ] UTXW0 + U(U TSSTUT + n\u03b3\u03a3\u22122)\u2020UT\u039e\n= U ( I\u03c1 \u2212UTSSTU\u2212 n\u03b3\u03a3\u22122 )( UTSSTU + n\u03b3\u03a3\u22122 )\u22121 \u03a3VTW0\n+ U(UTSSTUT + n\u03b3\u03a3\u22122)\u2020UT\u039e,\nwhere the last equality follows from that A\u22121\u2212B\u22121 = B\u22121(B\u2212A)A\u22121 for any nonsingular matrices A and B of proper size. Since E[\u039e] = 0 and E[\u039e\u039eT ] = \u03be2In, it follows that\nR ( Wh ) = bias2 ( Wh ) + var ( Wh ) where\nbias2 ( Wh ) = 1\nn \u2225\u2225\u2225(n\u03b3\u03a3\u22122 + UTSSTU\u2212 I\u03c1)(UTSSTU + n\u03b3\u03a3\u22122)\u22121\u03a3VTW0\u2225\u2225\u22252 F ,\nvar ( Wh ) = \u03be2\nn \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u22121\u2225\u2225\u22252 F .\nThis shows the bias and variance of Wh."}, {"heading": "D.2 Proof of Theorem 23", "text": "Proof Assumption 1.1 ensures that (1\u2212 \u03b7)I\u03c1 UTSSTU (1 + \u03b7)I\u03c1. It follows that (1\u2212 \u03b7) ( I\u03c1 + n\u03b3\u03a3 \u22122) UTSSTU + n\u03b3\u03a3\u22122 (1 + \u03b7)(I\u03c1 + n\u03b3\u03a3\u22122). The bias term can be written as\nbias2 ( Wc ) = n\u03b32 \u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u03a3\u22121VTW0\u2225\u22252F = n\u03b32 tr ( WT0 V\u03a3\n\u22121[(UTSSTU + n\u03b3\u03a3\u22122)\u2020]2\u03a3\u22121VTW0) \u2264 n\u03b3 2 (1\u2212\u03b7)2 \u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u03a3\u22121VTW0\u2225\u22252F = n\u03b3 2 (1\u2212\u03b7)2 \u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u22252F = 1 (1\u2212\u03b7)2 bias 2(W?).\nWe can analogously show bias2(Wc) \u2265 1 (1+\u03b7)2 bias2(W?). Let B = ( UTSSTU + n\u03b3\u03a3\u22122 )\u2020 UTS \u2208 R\u03c1\u00d7s. By Assumption 1.1, it holds that\n(1\u2212 \u03b7) [( UTSSTU + n\u03b3\u03a3\u22122 )2]\u2020 BBT (1 + \u03b7)[(UTSSTU + n\u03b3\u03a3\u22122)2]\u2020.\nApplying Assumption 1.1 again, we obtain\n(1\u2212 \u03b7)2 ( I\u03c1 + n\u03b3\u03a3 \u22122)2 (UTSSTU + n\u03b3\u03a3\u22122)2 (1 + \u03b7)2(I\u03c1 + n\u03b3\u03a3\u22122)2 Note that both sides are nonsingular. Combining the above two equations, we have\n1\u2212 \u03b7 (1 + \u03b7)2\n( I\u03c1 + n\u03b3\u03a3 \u22122)\u22122 BBT 1 + \u03b7 (1\u2212 \u03b7)2 ( I\u03c1 + n\u03b7\u03a3 \u22122)\u22122. Taking the trace of all the terms, we obtain\n1\u2212 \u03b7 (1 + \u03b7)2\n\u2264 \u2225\u2225B\u2225\u22252\nF\u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u2225\u22252F \u2264 1 + \u03b7 (1\u2212 \u03b7)2\nThe variance term can be written as\nvar ( Wc ) = \u03be2\nn\n\u2225\u2225BST\u2225\u22252 F \u2264 \u03be 2\nn\n\u2225\u2225B\u2225\u22252 F \u2225\u2225S\u2225\u22252 2\n\u2264 \u03be 2(1 + \u03b7) n(1\u2212 \u03b7)2 \u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u2225\u22252F \u2225\u2225S\u2225\u222522 = (1 + \u03b7)\u2016S\u201622\n(1\u2212 \u03b7)2 var(W?).\nThe upper bound the the variance follows from Assumption 1.3."}, {"heading": "D.3 Proof of Theorem 24", "text": "Proof Let B = ( UTSSTU + n\u03b3\u03a3\u22122 )\u2020 UTS \u2208 R\u03c1\u00d7s. In the proof of Theorem 5 we show that\nvar ( Wc ) = \u03be2\nn\n\u2225\u2225BST\u2225\u22252 F .\nIf STS \u03d1ns Is, then it holds that\nvar ( Wc ) = \u03be2\nn\n\u2225\u2225BST\u2225\u22252 F \u2265 \u03d1n\ns\n\u03be2\nn\n\u2225\u2225B\u2225\u22252 F \u2265 \u03d1n\ns 1\u2212 \u03b7 (1 + \u03b7)2 var(W?).\nThis established the lower bounds on the variance."}, {"heading": "D.4 Proof of Theorem 25", "text": "Proof Theorem 4 shows that\nbias ( Wh ) = \u03b3 \u221a n \u2225\u2225\u2225\u2225(\u03a3\u22122 + UTSSTU\u2212 I\u03c1n\u03b3 )(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u03a3VTW0 \u2225\u2225\u2225\u2225 F\n= \u03b3 \u221a n \u2225\u2225A\u03a32B\u2225\u2225 F \u2264 \u03b3 \u221a n \u2225\u2225A\u03a32\u2225\u2225 2 \u2225\u2225B\u2225\u2225 F ,\nvar ( Wh ) = \u03be2\nn \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u2225\u2225\u22252 F .\nwhere we define\nA = \u03a3\u22122 + UTSSTU\u2212 I\u03c1\nn\u03b3 , B = \u03a3\u22122 ( UTSSTU + n\u03b3\u03a3\u22122 )\u2020 \u03a3VTW0.\nWe first analyze then bias. It follows from Assumption 1.1 that \u03a3\u22122 ( I\u03c1 \u2212 \u03b7 n\u03b3 \u03a32 ) A \u03a3\u22122 ( I\u03c1 + \u03b7 n\u03b3 \u03a32 ) . (16)\nSince ( I\u03c1 \u2212 \u03b7n\u03b3\u03a3 2 )2 (I\u03c1 + \u03b7n\u03b3\u03a32)2 (1 + \u03b7\u03c321n\u03b3 )2I\u03c1, it follows that\nA2 \u03a3\u22124 ( I\u03c1 + \u03b7 n\u03b3 \u03a32 )2 ( 1 + \u03b7\u03c321 n\u03b3 )2 \u03a3\u22124.\nThus \u2225\u2225A\u03a32\u2225\u22252 2 = \u2225\u2225\u03a3TA2\u03a32\u2225\u2225 2 \u2264 ( 1 + \u03b7\u03c321 n\u03b3 )2 .\nIt follows from Assumption 1.1 that (1 + \u03b7)\u22121 ( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121 ((1 + \u03b7)I\u03c1 + n\u03b3\u03a3\u22122)\u22121 ( UTSSTU + n\u03b3\u03a3\u22122\n)\u2020 ((1\u2212 \u03b7)I\u03c1 + n\u03b3\u03a3\u22122)\u22121 (1\u2212 \u03b7)\u22121(I\u03c1 + n\u03b3\u03a3\u22122)\u22121. Thus\nBTB = WT0 V\u03a3 3 ( \u03a3\u22122(UTSSTU + n\u03b3\u03a3\u22122)\u2020\u03a3\u22122 )2 \u03a33VTW0 (17)\n(1\u2212 \u03b7)\u22122WT0 V\u03a33 ( \u03a3\u22122(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u03a3\u22122 )2 \u03a33VTW0\n= (1\u2212 \u03b7)\u22122WT0 V\u03a3 ( \u03a32 + n\u03b3I\u03c1 )\u22122 \u03a3VTW0.\nIt follows that\n\u2016B\u20162F = tr ( BTB ) \u2264 (1\u2212 \u03b7)\u22122 \u2225\u2225(\u03a3\u22122 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u22252F = bias2(W?)n\u03b32(1\u2212 \u03b7)2 . where the last equality follows from the definition of bias(W?). By the definition of A and B, we have\nbias2 ( Wh ) \u2264 \u03b32n \u2225\u2225A\u03a32\u2225\u22252 2 \u2225\u2225B\u2225\u22252 F = 1 (1\u2212 \u03b7)2 ( 1 + \u03b7\u03c321 n\u03b3 )2 bias2 ( W? ) .\nTo this end, the upper bound on bias ( Wh ) is established.\nBy the same definition of A and B, we can also show that\nbias ( Wh ) = \u03b3 \u221a n \u2225\u2225A\u03a32B\u2225\u2225\nF \u2265 \u03b3 \u221a n \u03c3min\n( A\u03a32 ) \u2225\u2225B\u2225\u2225 F .\nAssume that \u03c32\u03c1 \u2265 n\u03b3 \u03b7 . It follows from (16) that\nA2 (\u03b7\u03c32\u03c1 n\u03b3 \u2212 1 )2 \u03a3\u22124.\nThus\n\u03c32min ( A\u03a32 ) = \u03c3min(\u03a3 2A2\u03a32) \u2265 (\u03b7\u03c32\u03c1 n\u03b3 \u2212 1 )2 .\nIt follows from (17) that\nBTB (1 + \u03b7)\u22122WT0 V\u03a3 ( \u03a32 + n\u03b3I\u03c1 )\u22122 \u03a3VTW0.\nThus\n\u2016B\u20162F = tr ( BTB ) \u2265 (1 + \u03b7)\u22122 \u2225\u2225(\u03a3\u22122 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u22252F = bias2(W?)n\u03b32(1 + \u03b7)2 . In sum, we obtain\nbias2 ( Wh ) \u2265 \u03b32n \u03c32min ( A\u03a32 ) \u2225\u2225B\u2225\u22252 F = (1 + \u03b7)\u22122 (\u03b7\u03c32\u03c1 n\u03b3 \u2212 1 )2 bias2 ( W? ) .\nTo this end the lower bound on bias ( Wh ) is established.\nIt follows from Assumption 1.1 that\n(1 + \u03b7)\u22121 ( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121 (UTSSTU + n\u03b3\u03a3\u22122)\u22121 (1\u2212 \u03b7)\u22121 (I\u03c1 + n\u03b3\u03a3\u22122)\u22121 It follows from Theorem 4 that\nvar ( Wh ) = \u03be2\nn \u2225\u2225\u2225(UTSSTU + n\u03b3\u03a3\u22122)\u22121\u2225\u2225\u22252 F\n\u2208 1 1\u2213 \u03b7\n\u03be2\nn \u2225\u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u2225\u2225\u22252 F\n= 1 1\u2213 \u03b7 var ( W? ) .\nThis concludes the proof."}, {"heading": "Appendix E. Model Averaging from Optimization Perspective: Proofs", "text": "In Section E.1 we prove Theorem 26. In Section E.2 we prove Theorem 27."}, {"heading": "E.1 Proof of Theorem 26", "text": "Proof By Lemma 31, we only need to show \u2016(XTX+n\u03b3Id)1/2(Wc\u2212W?)\u20162F \u2264 n\u03b1\u03b2f(W?). In the proof, we define \u03c1 = rank(X) and \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3\u03c1 be the singular values of X.\nIn the proof of Theorem 21 we show that\n(XTX + n\u03b3Id) 1/2 ( Wci \u2212W? ) = [ (XTX + n\u03b3Id) \u22121/2(XTSiSTi X + n\u03b3Id)(X TX + n\u03b3Id)\n\u22121/2]\u2020(Ai + Bi) = C\u2020i ( Ai + Bi ) ,\nwhere\nAi = V(\u03a3 2 + n\u03b3I\u03c1) \u22121/2\u03a3USiSTi Y \u22a5, Bi = n\u03b3V\u03a3(\u03a3 2 + n\u03b3I\u03c1) \u22121/2(UTSiSTi U\u2212 I\u03c1)(\u03a32 + n\u03b3I\u03c1)\u22121UTY\nCi = [ (XTX + n\u03b3Id) 1/2 ]\u2020( XTSiS T i X + n\u03b3Id )[ (XTX + n\u03b3Id) 1/2 ]\u2020\n= V(I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2(UTSiSTi U + n\u03b3\u03a3 \u22122)(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2VT = VVT + V(I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2(UTSiSTi U\u2212 I\u03c1)(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2VT .\nBy Assumption 2.1, we have that Ci ( 1\u2212 \u03b7 \u03c3 2 max\n\u03c32max+n\u03b3\n) VVT . Since \u03b7 \u2264 1/2, it follows that\nC\u2020i ( 1+ 2\u03b7 \u03c3 2 max\n\u03c32max+n\u03b3\n) VVT . Let C\u2020i = VV T +V\u2206iV T . It holds that \u2206i 2\u03b7 \u03c3 2 max \u03c32max+n\u03b3 VVT\n2\u03b7\u03b2VVT . By definition, Wc = 1g \u2211g i=1 W c i . It follows that\n\u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wci \u2212W?)\u2225\u2225\u2225 F = \u2225\u2225\u22251 g g\u2211 i=1 C\u2020i (Ai + Bi) \u2225\u2225\u2225 F\n\u2264 \u2225\u2225\u22251 g g\u2211 i=1 (Ai + Bi) \u2225\u2225\u2225 F + \u2225\u2225\u22251 g g\u2211 i=1 V\u2206iV T (Ai + Bi) \u2225\u2225\u2225 F\n\u2264 \u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 F + \u2225\u2225\u22251 g g\u2211 i=1 Bi \u2225\u2225\u2225 F + 1 g g\u2211 i=1 \u2225\u2225\u2206i\u2225\u22252(\u2225\u2225Ai\u2225\u2225F + \u2225\u2225Bi\u2225\u2225F) \u2264 \u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 F + \u2225\u2225\u22251 g g\u2211 i=1 Bi \u2225\u2225\u2225 F + 2\u03b7\u03b2 1 g g\u2211 i=1\n(\u2225\u2225Ai\u2225\u2225F + \u2225\u2225Bi\u2225\u2225F). (18) By Assumption 2.3, we have that\n1\ng g\u2211 i=1 \u2225\u2225Ai\u2225\u2225F = \u2225\u2225(\u03a32 + n\u03b3Id)\u22121/2\u03a3\u2225\u22252 \u00b7 1g g\u2211 i=1\n\u2225\u2225UTSiSTi Y\u22a5\u2225\u2225F \u2264 \u221a\n\u03c32max \u03c32max + n\u03b3\n\u2225\u2225Y\u22a5\u2225\u2225 F .\nWe apply Assumption 2.1 and follow the proof of Theorem 21 to show that\n\u2225\u2225Bi\u2225\u22252F \u2264 \u03b72n\u03b3 \u03c32max\u03c32max + n\u03b3 \u2225\u2225\u2225(\u03a32 + n\u03b3Id)\u22121/2UTY\u2225\u2225\u22252 F .\nIt follows that\n1\ng g\u2211 i=1 (\u2225\u2225Ai\u2225\u2225F + \u2225\u2225Bi\u2225\u2225F) \u2264 max {\u221a , \u03b7 }\u221a \u03c32max\n\u03c32max + n\u03b3\n(\u2225\u2225Y\u22a5\u2225\u2225 F + \u221a n\u03b3 \u2225\u2225(\u03a32 + n\u03b3Id)\u22121/2UTY\u2225\u2225F)\n\u2264 max {\u221a , \u03b7 }\u221a \u03b2 \u221a 2 \u2225\u2225Y\u22a5\u2225\u22252\nF + 2n\u03b3 \u2225\u2225(\u03a32 + n\u03b3Id)\u22121/2UTY\u2225\u22252F = max {\u221a , \u03b7 }\u221a \u03b2 \u221a 2n f(W?). (19)\nHere the equality follows from Lemma 31. Let S = 1g [S1, \u00b7 \u00b7 \u00b7 ,Sg] \u2208 R n\u00d7sg. We have that\n1\ng g\u2211 i=1 Ai = V(\u03a3 2 + n\u03b3Id) \u22121/2\u03a3UTSSTY\u22a5\n1\ng g\u2211 i=1 Bi = n\u03b3V\u03a3(\u03a3 2 + n\u03b3Id) \u22121/2(UTSSTU\u2212 I\u03c1)(\u03a32 + n\u03b3I\u03c1)\u22121UTY.\nFollowing the proof Theorem 21 and applying Assumptions 2.1 and 2.2, we have that\n\u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 F + \u2225\u2225\u22251 g g\u2211 i=1 Bi \u2225\u2225\u2225 F \u2264 \u221a\u221a\u221a\u221a2\u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u22252 F + 2 \u2225\u2225\u22251 g g\u2211 i=1 Bi \u2225\u2225\u22252 F\n\u2264 max {\u221a\ng , \u03b7 g }\u221a \u03c32max \u03c32max + n\u03b3 \u221a 2n f(W?) = max {\u221a g , \u03b7 g }\u221a \u03b2 \u221a 2n f(W?). (20)\nIt follows from (18), (19), and (20) that\u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wci \u2212W?)\u2225\u2225\u2225 F\n\u2264 \u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 F + \u2225\u2225\u22251 g g\u2211 i=1 Bi \u2225\u2225\u2225 F + 2\u03b7\u03b2 1 g g\u2211 i=1 (\u2225\u2225Ai\u2225\u2225F + \u2225\u2225Bi\u2225\u2225F) \u2264 [ max {\u221a\ng , \u03b7 g\n} + 2\u03b2 \u00b7max { \u03b7 \u221a , \u03b72 }]\u221a \u03b2 \u221a 2n f(W?)\n= \u221a \u03b1\u03b2n f(W?).\nThis concludes our proof."}, {"heading": "E.2 Proof of Theorem 27", "text": "Proof By Lemma 31, we only need to show \u2225\u2225(XTX + n\u03b3Id)1/2(Wh \u2212W?)\u2225\u22252F \u2264 \u03b12\u03b22(\u2212\nnf(W?) + \u2016Y\u20162F ) .\nIn the proof of Theorem 2 we show that (XTX + n\u03b3Id) 1/2 ( Whi \u2212W? ) = VAiBiC,\nwhere\nAi = (\u03a3 2 + n\u03b3I\u03c1) \u22121/2\u03a3(I\u03c1 \u2212UTSiSTi U)\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2, Bi = (\u03a3 2 + n\u03b3I\u03c1) 1/2(\u03a3UTSiS T i U\u03a3 + n\u03b3I\u03c1) \u22121(\u03a32 + n\u03b3I\u03c1)1/2,\nC = (\u03a32 + n\u03b3I\u03c1) \u22121/2\u03a3UTY.\nIt follows from Assumption 2.1 that for all i \u2208 [g], 1\n1+\u03b7 (\u03a3 2 + n\u03b3I\u03c1) \u22121 (\u03a3UTSiSTi U\u03a3 + n\u03b3I\u03c1)\u22121 11\u2212\u03b7 (\u03a3 2 + n\u03b3I\u03c1) \u22121.\nWe let Bi = I\u03c1 + \u2206i. Thus \u2212 \u03b71+\u03b7 I\u03c1 \u2206i \u03b7 1\u2212\u03b7 I\u03c1. It follows that\n(XTX + n\u03b3Id) 1/2 ( Wh \u2212W? ) = 1\ng g\u2211 i=1 (XTX + n\u03b3Id) 1/2 ( Whi \u2212W? ) = 1\ng g\u2211 i=1 VAi(I\u03c1 + \u2206i)C = 1 g g\u2211 i=1 VAiC + 1 g g\u2211 i=1 VAi\u2206iC."}, {"heading": "It follows that\u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wh \u2212W?)\u2225\u2225\u2225", "text": "F \u2264 \u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 2 \u2225\u2225\u2225C\u2225\u2225\u2225 F + 1 g g\u2211 i=1 \u2225\u2225Ai\u2225\u22252\u2225\u2225\u2206i\u2225\u22252\u2225\u2225C\u2225\u2225F \u2264 \u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 2 \u2225\u2225\u2225C\u2225\u2225\u2225 F + \u03b7 1\u2212 \u03b7 (1 g g\u2211 i=1\n\u2225\u2225Ai\u2225\u22252)\u2225\u2225C\u2225\u2225F . (21) Let S = 1g [S1, \u00b7 \u00b7 \u00b7 ,Sg] \u2208 R\nn\u00d7gs. It follows from the definition of Ai that\u2225\u2225Ai\u2225\u22252 = \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3(I\u03c1 \u2212UTSiSTi U)\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2\u2225\u2225\u22252 \u2264 \u03b7 \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2\u2225\u2225\u2225 2 = \u03b7 \u03c32max \u03c32max + n\u03b3 = \u03b7\u03b2,\n\u2225\u2225\u22251 g g\u2211 i=1 Ai \u2225\u2225\u2225 2 = \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3(I\u03c1 \u2212UTSSTU)\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2\u2225\u2225\u2225 2\n\u2264 \u03b7 g \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121/2\u03a3\u03a3(\u03a32 + n\u03b3I\u03c1)\u22121/2\u2225\u2225\u2225 2 = \u03b7 g \u03c32max \u03c32max + n\u03b3 = \u03b7\u03b2 g .\nIt follows from (21) that \u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wh \u2212W?)\u2225\u2225\u2225 F\n\u2264 (\u03b7 g + \u03b72 1\u2212 \u03b7 ) \u03b2 \u2225\u2225C\u2225\u2225 F\n\u2264 (\u03b7 g + \u03b72 1\u2212 \u03b7 ) \u03b2 \u221a \u2212nf(W?) + \u2016Y\u20162F ,\nwhere the latter inequality follows from the proof of Theorem 22. This concludes the proof."}, {"heading": "Appendix F. Model Averaging from Statistical Perspective: Proofs", "text": "In Section F.1 we prove Theorem 28. In Section F.2 we prove Theorem 29"}, {"heading": "F.1 Proof of Theorem 28", "text": "Proof The bound on bias ( Wc ) can be shown in the same way as the proof of Theorem 23.\nWe prove the bound on var ( Wc ) in the following. It follows from Assumption 2.1 that\n(1 + \u03b7)\u22121(I\u03c1 + n\u03b3\u03a3\u22122)\u22121 (UTSiSTi U + n\u03b3\u03a3\u22122)\u2020 (1\u2212 \u03b7)\u22121(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\nLet\n(UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020 = (I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2(I\u03c1 + \u2206i)(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2.\nIt holds that\n\u2212 \u03b7 1 + \u03b7 I\u03c1 \u2206i \u03b7 1\u2212 \u03b7 I\u03c1.\nBy the definition of var(Wc) in Theorem 10, we have that\u221a var ( Wc\n) =\n\u03be\u221a n \u2225\u2225\u2225\u22251g g\u2211 i=1 (I\u03c1 + n\u03b3\u03a3 \u22122)\u22121UTSiS T i + 1 g g\u2211 i=1 (I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2UTSiS T i \u2225\u2225\u2225\u2225 F\n\u2264 \u03be\u221a n (\u2225\u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121UTSST\u2225\u2225\u2225 F + 1 g g\u2211 i=1 \u2225\u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2UTSiSTi \u2225\u2225\u2225 F ) \u2264 \u03be\u221a\nn \u2225\u2225(I\u03c1 + n\u03b3\u03a3\u22122)\u22121\u2225\u2225F(\u2225\u2225UTS\u2225\u22252\u2225\u2225S\u2225\u22252 + \u03b71\u2212 \u03b7 1g g\u2211 i=1 \u2225\u2225UTSi\u2225\u22252\u2225\u2225Si\u2225\u22252) = \u221a var ( W? )(\u2225\u2225UTS\u2225\u2225 2 \u2225\u2225S\u2225\u2225 2 + \u03b7 1\u2212 \u03b7 1 g g\u2211 i=1\n\u2225\u2225UTSi\u2225\u22252\u2225\u2225Si\u2225\u22252). Under Assumption 2.1, we have that \u2016STi U\u201622 \u2264 1 +\u03b7 and \u2016STU\u201622 \u2264 1 +\n\u03b7 g . It follows that\u221a\nvar ( Wc ) var ( W? ) \u2264 \u221a1 + \u03b7 g \u2225\u2225S\u2225\u2225 2 + \u03b7 \u221a 1 + \u03b7 1\u2212 \u03b7 1 g g\u2211 i=1 \u2225\u2225Si\u2225\u22252. Then the theorem follows from Assumption 2.3."}, {"heading": "F.2 Proof of Theorem 29", "text": "Proof The bound on var ( Wh ) can be established in the same way as Theorem 25.\nWe prove the bound on bias ( Wh ) in the following. Let\n(UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020 = (I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2(I\u03c1 + \u2206i)(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2.\nUnder Assumption 2.1, we have that \u2206i \u03b71\u2212\u03b7 I\u03c1. It follows from Theorem 10 that\nbias ( Wh ) = \u03b3 \u221a n \u2225\u2225\u2225\u22251g g\u2211 i=1 ( \u03a3\u22122 + UTSiS T i U\u2212 I\u03c1 n\u03b3 ) (UTSiS T i U + n\u03b3\u03a3 \u22122)\u2020\u03a3VTW0 \u2225\u2225\u2225\u2225 F\n\u2264 \u03b3 \u221a n \u2225\u2225\u2225\u22251g g\u2211 i=1 ( \u03a3\u22122 + UTSiS T i U\u2212 I\u03c1 n\u03b3 ) (I\u03c1 + n\u03b3\u03a3 \u22122)\u22121\u03a3VTW0 \u2225\u2225\u2225\u2225 F\n+ \u03b3 \u221a n \u2225\u2225\u2225\u22251g g\u2211 i=1 ( \u03a3\u22122 + UTSiS T i U\u2212 I\u03c1 n\u03b3 ) (I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2\u03a3VTW0 \u2225\u2225\u2225\u2225 F\n, \u03b3 \u221a n ( A+B ) ,\nwhere\nA = \u2225\u2225\u22251 g g\u2211 i=1 ( \u03a3\u22122 + UTSiS T i U\u2212 I\u03c1 n\u03b3 )( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121\u03a3VTW0\u2225\u2225\u2225 F\n= \u2225\u2225\u2225(\u03a3\u22122 + UTSSTU\u2212 I\u03c1\nn\u03b3\n)( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121\u03a3VTW0\u2225\u2225\u2225 F ,\nB = \u2225\u2225\u22251 g g\u2211 i=1 ( \u03a3\u22122 + UTSiS T i U\u2212 I\u03c1 n\u03b3 )( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2\u03a3VTW0\u2225\u2225\u2225 F\n\u2264 1 g g\u2211 i=1 \u2225\u2225\u2225(\u03a3\u22122 + UTSiSTi U\u2212 I\u03c1 n\u03b3 )( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2\u03a3VTW0\u2225\u2225\u2225 F .\nIt follows from Assumption 2.1 that( 1\u2212 \u03b7\u03c3 2 max\ngn\u03b3\n) \u03a3\u22122 \u03a3\u22122 + U\nTSSTU\u2212 I\u03c1 n\u03b3\n(\n1 + \u03b7\u03c32max gn\u03b3\n) \u03a3\u22122,\nand thus ( \u03a3\u22122 +\nUTSSTU\u2212 I\u03c1 n\u03b3\n)2 (\n1 + \u03b7\u03c32max gn\u03b3\n)2 \u03a3\u22124.\nIt follows that A = \u2225\u2225\u2225(\u03a3\u22122 + UTSSTU\u2212 I\u03c1\nn\u03b3\n)( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121\u03a3VTW0\u2225\u2225\u2225 F\n\u2264 (\n1 + \u03b7\u03c32max gn\u03b3 )\u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u2225\u2225 F .\nIt follows from Assumption 2.1 that( 1\u2212 \u03b7\u03c3 2 max\nn\u03b3\n)2 \u03a3\u22124\n( \u03a3\u22122 + \u03b7\nn\u03b3 I\u03c1\n)2 (\n1 + \u03b7\u03c32max n\u03b3\n)2 \u03a3\u22124.\nMoreover, \u03a3\u22122 ( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2\u03a3\u22122 \u03b7 1\u2212 \u03b7 \u03a3\u22122 ( I\u03c1 + n\u03b3\u03a3 \u22122)\u22121\u03a3\u22122\nIt follows that B \u2264 (\n1 + \u03b7\u03c32max n\u03b3 ) \u00b7 1 g g\u2211 i=1 \u2225\u2225\u2225\u03a3\u22122(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2\u2206i(I\u03c1 + n\u03b3\u03a3\u22122)\u22121/2\u03a3\u22122\u03a33VTW0\u2225\u2225\u2225 F\n\u2264 \u03b7 1\u2212 \u03b7\n( 1 +\n\u03b7\u03c32max n\u03b3\n) \u00b7 \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u2225\u2225\nF .\nHence\nbias ( Wh ) \u2264 \u03b3 \u221a n ( A+B ) \u2264 [ 1 1\u2212 \u03b7 + (\u03b7 g + \u03b72 1\u2212 \u03b7 )\u03c32max n\u03b3 ] \u03b3 \u221a n \u2225\u2225\u2225(\u03a32 + n\u03b3I\u03c1)\u22121\u03a3VTW0\u2225\u2225\u2225 F\n= [ 1 1\u2212 \u03b7 + (\u03b7 g + \u03b72 1\u2212 \u03b7 )\u03c32max n\u03b3 ] bias ( W? ) .\nHere the equality follows from Theorem 4."}], "references": [{"title": "Sharper bounds for regression and low-rank approximation with regularization", "author": ["Haim Avron", "Kenneth L. Clarkson", "David P. Woodruff"], "venue": "arXiv preprint arXiv:1611.03225,", "citeRegEx": "Avron et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2016}, {"title": "Pasting small votes for classification in large databases and on-line", "author": ["Leo Breiman"], "venue": "Machine Learning,", "citeRegEx": "Breiman.,? \\Q1999\\E", "shortCiteRegEx": "Breiman.", "year": 1999}, {"title": "Finding frequent items in data streams", "author": ["Moses Charikar", "Kevin Chen", "Martin Farach-Colton"], "venue": "Theoretical Computer Science,", "citeRegEx": "Charikar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2004}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Annual ACM Symposium on theory of computing (STOC),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Sampling algorithms for `2 regression and applications", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "In Annual ACM-SIAM Symposium on Discrete Algorithm (SODA),", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "The spectral norm error of the naive Nystr\u00f6m extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "Gittens.,? \\Q2011\\E", "shortCiteRegEx": "Gittens.", "year": 2011}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Faster ridge regression via the subsampled randomized Hadamard transform", "author": ["Yichao Lu", "Paramveer Dhillon", "Dean P Foster", "Lyle Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "A statistical perspective on algorithmic leveraging", "author": ["Ping Ma", "Michael W Mahoney", "Bin Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["John Nelson", "Huy L Nguy\u00ean"], "venue": "In IEEE Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "The power of simple tabulation-based hashing", "author": ["Mihai Patrascu", "Mikkel Thorup"], "venue": "Journal of the ACM,", "citeRegEx": "Patrascu and Thorup.,? \\Q2012\\E", "shortCiteRegEx": "Patrascu and Thorup.", "year": 2012}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares", "author": ["Mert Pilanci", "Martin J Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pilanci and Wainwright.,? \\Q2015\\E", "shortCiteRegEx": "Pilanci and Wainwright.", "year": 2015}, {"title": "A statistical perspective on randomized sketching for ordinary least-squares", "author": ["Garvesh Raskutti", "Michael W Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Raskutti and Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Raskutti and Mahoney.", "year": 2016}, {"title": "Random projections for large-scale regression", "author": ["Gian-Andrea Thanei", "Christina Heinze", "Nicolai Meinshausen"], "venue": "arXiv preprint arXiv:1701.05325,", "citeRegEx": "Thanei et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Thanei et al\\.", "year": 2017}, {"title": "Improved analysis of the subsampled randomized Hadamard transform", "author": ["Joel A Tropp"], "venue": "Advances in Adaptive Data Analysis,", "citeRegEx": "Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Tropp.", "year": 2011}, {"title": "Large scale kernel learning using block coordinate descent", "author": ["Stephen Tu", "Rebecca Roelofs", "Shivaram Venkataraman", "Benjamin Recht"], "venue": "arXiv preprint arXiv:1602.05310,", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Sketching meets random projection in the dual: A provable recovery algorithm for big and highdimensional data", "author": ["Jialei Wang", "Jason D Lee", "Mehrdad Mahdavi", "Mladen Kolar", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1610.03045,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "SPSD matrix approximation vis column selection: Theories, algorithms, and extensions", "author": ["Shusen Wang", "Luo Luo", "Zhihua Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Towards more efficient SPSD matrix approximation and CUR matrix decomposition", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Computationally feasible near-optimal subset selection for linear regression under measurement constraints", "author": ["Yining Wang", "Adams Wei Yu", "Aarti Singh"], "venue": "arXiv preprint arXiv:1601.02068,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "A fast randomized algorithm for the approximation of matrices", "author": ["Franco Woolfe", "Edo Liberty", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Woolfe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Woolfe et al\\.", "year": 2008}, {"title": "Implementing randomized matrix algorithms in parallel and distributed environments", "author": ["Jiyan Yang", "Xiangrui Meng", "Michael W Mahoney"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Spark: Cluster computing with working", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": "sets. HotCloud,", "citeRegEx": "Zaharia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2010}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Yuchen Zhang", "John C. Duchi", "Martin J. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates", "author": ["Yuchen Zhang", "John Duchi", "Martin Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013).", "startOffset": 15, "endOffset": 507}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.", "startOffset": 15, "endOffset": 583}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.", "startOffset": 15, "endOffset": 747}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.", "startOffset": 15, "endOffset": 764}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.", "startOffset": 15, "endOffset": 1452}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.", "startOffset": 15, "endOffset": 1481}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.", "startOffset": 15, "endOffset": 1512}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance.", "startOffset": 15, "endOffset": 1533}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance. In particular, Pilanci and Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.", "startOffset": 15, "endOffset": 1656}, {"referenceID": 18, "context": "Following the convention of Pilanci and Wainwright (2015), Wang et al.", "startOffset": 28, "endOffset": 58}, {"referenceID": 18, "context": "Following the convention of Pilanci and Wainwright (2015), Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch.", "startOffset": 28, "endOffset": 79}, {"referenceID": 1, "context": "Note that classical sketch with uniform sampling and model averaging is essentially bagging (synonym bootstrap aggregating) (Breiman, 1996) (or a variant called pasting (Breiman, 1999)) for ridge regression.", "startOffset": 169, "endOffset": 184}, {"referenceID": 32, "context": "The model averaging analyzed in this paper is similar in spirit to the AvgM algorithm of (Zhang et al., 2013).", "startOffset": 89, "endOffset": 109}, {"referenceID": 32, "context": "However, our results do not follow from those of (Zhang et al., 2013).", "startOffset": 49, "endOffset": 69}, {"referenceID": 22, "context": "For example, the conjugate gradient method satisfies \u2016W\u2212W\u2016F \u2016W(0)\u2212W?\u20162 F \u2264 \u03b8 1; the stochastic block coordinate descent (Tu et al., 2016) satisfies Ef(W )\u2212f(W) f(W(0))\u2212f(W?) \u2264 \u03b8 t 2.", "startOffset": 120, "endOffset": 137}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) shares many similarities with our results. However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered. Lu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis.", "startOffset": 15, "endOffset": 460}, {"referenceID": 32, "context": "Our results clearly indicate that the performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013).", "startOffset": 131, "endOffset": 151}, {"referenceID": 33, "context": "For similar reasons, our work is different from the divide-and-conquer kernel ridge regression algorithm of (Zhang et al., 2015).", "startOffset": 108, "endOffset": 128}, {"referenceID": 17, "context": "Iterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al.", "startOffset": 45, "endOffset": 75}, {"referenceID": 17, "context": "Iterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al. (2016a). By way of comparison, all the algorithms in this paper are \u201cone-shot\u201d rather than iterative.", "startOffset": 45, "endOffset": 96}, {"referenceID": 0, "context": "Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al.", "startOffset": 66, "endOffset": 128}, {"referenceID": 0, "context": "Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging.", "startOffset": 66, "endOffset": 213}, {"referenceID": 8, "context": "Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X.", "startOffset": 74, "endOffset": 96}, {"referenceID": 8, "context": "Leverage scores can be efficiently approximated by the algorithms of (Drineas et al., 2012).", "startOffset": 69, "endOffset": 91}, {"referenceID": 10, "context": "Gaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson and Lindenstrauss, 1984).", "startOffset": 91, "endOffset": 124}, {"referenceID": 12, "context": "In fact, pi can be any convex combination of li \u2211n j=1 lj and 1 n (Ma et al., 2015).", "startOffset": 66, "endOffset": 83}, {"referenceID": 29, "context": "In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n.", "startOffset": 64, "endOffset": 85}, {"referenceID": 31, "context": "1 Prediction Error We tested the prediction performance of sketched ridge regression by implementing classical sketch with model averaging in PySpark (Zaharia et al., 2010).", "startOffset": 150, "endOffset": 172}, {"referenceID": 13, "context": "Remark 14 Note that the first two assumptions were identified in (Mahoney, 2011) and are the relevant structural conditions needed to be satisfied to obtain strong results from the optimization perspective.", "startOffset": 65, "endOffset": 80}, {"referenceID": 12, "context": "The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.", "startOffset": 32, "endOffset": 49}, {"referenceID": 12, "context": "The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.", "startOffset": 32, "endOffset": 78}, {"referenceID": 4, "context": "1 can actually be expressed in the form of an approximate matrix multiplication bound (Drineas et al., 2006a). We call it the Subspace Embedding Property since, as first highlighted in Drineas et al. (2006b), this subspace embedding property is the key result to obtain high-quality sketching algorithms for regression and related problems.", "startOffset": 87, "endOffset": 208}, {"referenceID": 24, "context": "Lemma 30 (Wang et al. (2016b)) Let U \u2208 Rn\u00d7\u03c1 be any fixed matrix with orthonormal columns.", "startOffset": 10, "endOffset": 30}, {"referenceID": 23, "context": "Vershynin (2010) showed that the greatest singular value of the standard Gaussian matrix G \u2208 Rn\u00d7s is at most \u221a n + \u221a s + t with probability at least 1 \u2212 2e\u2212t/2.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "Patrascu and Thorup (2012) showed that the maximal number of balls in any bin is at most n/s+O (\u221a n/s log n ) with probability at least 1\u2212 1 n , where c = O(1).", "startOffset": 0, "endOffset": 27}, {"referenceID": 23, "context": "Vershynin (2010) showed that the smallest singular value of any n\u00d7s standard Gaussian matrix G is at least \u221a n\u2212 \u221a s \u2212 t with probability at least 1 \u2212 2e\u2212t/2.", "startOffset": 0, "endOffset": 17}], "year": 2017, "abstractText": "We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR\u2014namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the \u201cmass\u201d in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.", "creator": "LaTeX with hyperref package"}}}