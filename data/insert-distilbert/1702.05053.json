{"id": "1702.05053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "abstract": "neural attention models have achieved great proven success in different model nlp tasks. regarding how - ever, they have not fulfilled their promise on the amr parsing task due to the high data sparsity issue. in presenting this paper, we de - scribe a sequence - to - sequence hierarchy model for amr parsing and present different ways to tackle approaching the data sparsity problem. we show that with our methods achieve significant improvement over these a baseline neural atten - tion model and our results are much also compet - itive against state - of - the - art systems paradigm that don't use extra linguistic resources.", "histories": [["v1", "Thu, 16 Feb 2017 17:09:12 GMT  (209kb)", "http://arxiv.org/abs/1702.05053v1", "Accepted by EACL-17"]], "COMMENTS": "Accepted by EACL-17", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaochang peng", "chuan wang", "daniel gildea", "nianwen xue"], "accepted": false, "id": "1702.05053"}, "pdf": {"name": "1702.05053.pdf", "metadata": {"source": "CRF", "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "authors": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea"], "emails": ["gildea}@cs.rochester.edu", "xuen}@brandeis.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n05 05\n3v 1\n[ cs\n.C L\n] 1\n6 Fe\nb 20\n17\nAddressing the Data Sparsity Issue in Neural AMR Parsing\nXiaochang Peng\u22171, Chuan Wang\u22172, Daniel Gildea1 and Nianwen Xue2\n1University of Rochester\n{xpeng, gildea}@cs.rochester.edu 2Brandeis University\n{cwang24, xuen}@brandeis.edu\nAbstract\nNeural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources."}, {"heading": "1 Introduction", "text": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation.\nThe task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b;\n*Both authors contribute equally.\ndescribe-01\nperson\ngenius\nARG0 ARG1\nARG2\nname\n\u201cRyan\u201d\nname\nop1\nFigure 1: An example of AMR graph representing the meaning of: \u201cRyan\u2019s description of himself: a genius.\u201d\nArtzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue.\nRecently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks.\nHowever, sequence-to-sequence models mostly work well for large scale parallel data, usually involving millions of sentence pairs. Vinyals et al. (2015) present a method which linearizes parse trees into a sequence structure and therefore a sequence-to-sequence model can be applied to the constituent parsing task. Competitive results have been achieved with an attention model on the Penn Treebank dataset, with only 40K annotated sentences.\nAMR parsing is a much harder task in that the target vocabulary size is much larger, while the size of the dataset is much smaller. While for constituent parsing we only need to predict non-\nterminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko (2016) applied a similar approach where AMR graphs are linearized using depth-first search and both concepts and relations are treated as tokens (see Figure 3). Due to the data sparsity issue, their AMR parsing results are significantly lower than state-of-the-art models when using the neural attention model.\nIn this paper, we present a method which linearizes AMR graphs in a way that captures the interaction of concepts and relations. To overcome the data sparsity issue for the target vocabulary, we propose a categorization strategy which first maps low frequency concepts and entity subgraphs to a reduced set of category types. In order to map each type to its corresponding target side concepts, we use heuristic alignments to connect source side spans and target side concepts or subgraphs. During decoding, we use the mapping dictionary learned from the training data or heuristic rules for certain types to map the target types to their corresponding translation as a postprocessing procedure.\nExperiments show that our linearization strategy and categorization method are effective for the AMR parsing task. Our model improves significantly in comparison with the previously reported sequence-to-sequence results and provides a competitive benchmark in comparison with state-ofthe-art results without using dependency parses or other external semantic resources."}, {"heading": "2 Sequence-to-sequence Parsing Model", "text": "Our model is based on an existing sequence-tosequence parsing model (Vinyals et al., 2015), which is similar to models used in neural machine translation."}, {"heading": "2.1 Encoder-Decoder", "text": "Encoder. The encoder learns a context-aware representation for each position of the input sequence by mapping the inputs w1, . . . , wm into a sequence of hidden layers h1, . . . , hm. To model the left and right contexts of each input position, we use a bidirectional LSTM (Bahdanau et al., 2014). First, each input\u2019s word embedding representation x1, . . . , xm is obtained though a lookup table. Then these embeddings serve as the input to\ntwo RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows:\nh fw i = f(xi, h fw i\u22121) (1)\nHere the recurrent function f we use is LongShort-Term-Memory (LSTM) (Hochreiter and Schmidhuber, 1997). The backward RNN works similarly by repeating the process in reverse order. The outputs of forward RNN and backward RNN are then depth-concatenated to get the final representation of the input sequence.\nhi = [h fw i , h bw m\u2212i+1] (2)\nDecoder. The decoder is also an LSTM model which generates the hidden layers recurrently. Additionally, it utilizes an attention mechanism to put a \u201cfocus\u201d on the input sequence. At each output time step j, the attention vector d \u2032\nj is defined as\na weighted sum of the input hidden layers, where the masking weight \u03b1 j i is calculated using a feedforward neural network. Formally, the attention vector is defined as follows:\nu j i = v T tanh(W1hi +W2dj) (3) \u03b1 j i = softmax(u j i ) (4)\nd \u2032 j = m\u2211\ni=1\n\u03b1 j ihi (5)\nwhere dj is the output hidden layer at time step j, and v, W1, and W2 are parameters for the model. Here the weight vector \u03b1 j 1 , . . . , \u03b1 j m is also interpreted as a soft alignment in the neural machine translation model, which similarly could also be treated as a soft alignment between token sequences and AMR relation/concept sequences in the AMR parsing task. Finally, we concatenate the hidden layer dj and attention vector d \u2032 j to get the new hidden layer, which is used to predict the output sequence label.\nP (yj |w, y1:j\u22121) = softmax(W3[dj , d \u2032 j ]) (6)"}, {"heading": "2.2 Parse Tree as Target Sequence", "text": "Vinyals et al. (2015) designed a reversible way of converting the parse tree into a sequence, which they call linearization. The linearization is performed in the depth-first traversal order. Figure 2 shows an example of the linearization result. The target vocabulary consists of 128 symbols.\nIn practice, they found that using the attention model is more data efficient and works well on the parsing task. They also reversed the input sentence and normalized the part-of-speech tags. After decoding, the output parse tree is recovered from the output sequence of the decoder in a postprocessing procedure. Overall, the sequence-tosequence model is able to match the performance of the Berkeley Parser (Petrov et al., 2006)."}, {"heading": "3 AMR Linearization", "text": "Barzdins and Gosko (2016) present a similar linearization procedure where the depth-first traversal result of an AMR graph is used as the AMR sequence (see Figure 3). The bracketing structure of AMR is hard to maintain because the prediction of relation (with left parenthesis) and the prediction of an isolated right parenthesis are not correlated. As a result, the output AMR sequences usually have parentheses that do not match.\nWe present a linearization strategy which captures the bracketing structure of AMR and the connection between relations and concepts. Figure 3 shows the linearization result of the AMR graph shown in Figure 1. Each relation connects the head concept to a subgraph structure rooted at the tail concept, which shows one branch below the head concept. We use the relation label and left parenthesis to show the beginning of the branch (subgraph) and use right parenthesis paired with the relation label to show the end of the branch. We additionally add \u201c-TOP-(\u201d at the beginning to show the start of the traversal of the AMR graph and add \u201c)-TOP-\u201d at the end to show the end of traversal. When a symbol is revisited, we replace the symbol with \u201c-RET-\u201d. We additionally add the revisited symbol before \u201c-RET-\u201d to decide where the reentrancy is introduced to.1 We also get rid of\n1This is an approximation because one concept can appear multiple times, and we simply attach the reentrancy to the most recent appearance of the concept. An additional index would be needed to identify the accurate place of reentrancy.\nvariables and only keep the full concept label. For example, \u201cg / genius\u201d to \u201cgenius\u201d.\nWe can easily recover the original AMR graph from its linearized sequence. The sequence also captures the branching information of each relation explicitly by representing it with a start symbol and an end symbol specific to that relation. During our experiments, most of the output sequences have a matching bracketing structure using this linearization strategy. The idea of linearization is basically a depth-first traversal of the AMR where the original graph structure can be reconstructed with the linearization result. Even though we call it a sequence, its core idea is actually generating a graph structure from top-down."}, {"heading": "4 Dealing with the Data Sparsity Issue", "text": "While sequence-to-sequence models can be successfully applied to constituent parsing, they do not work well on the AMR parsing task as shown by Barzdins and Gosko (2016). The main bottleneck is that the size of target vocabulary for AMR parsing is much larger than constituent parsing, tens of thousands in comparison with 128, and the size of training data is less than half of that available for parsing.\nIn this section, we present a categorization method which significantly reduces the target vocabulary size, as the alignment from the attention model does not work well on the relatively small dataset. To adjust for the alignment errors made by the attention model, we propose to add supervision from an alignment produced by an external aligner which can use lexical information to overcome the limit of data size."}, {"heading": "4.1 AMR Categorization", "text": "We define several types of categories and map low frequency words into these categories.\n1. DATE: we reduce all the date entity sub-\ngraphs to this category, ignoring details of the specific date entity.\n2. NE {ent}: we reduce all named entity subgraphs to this category, where ent is the root\nlabel of each subgraph, such as country or person.\n3. -VERB-: we map predicate variables with\nlow frequency (n < 50) to this category\n4. -SURF-: we map non-predicate variables\nwith low frequency (n < 50) to this category\n5. -CONST-: we map constants other than num-\nbers, \u201c-\u201d, \u201cinterrogative\u201d, \u201cexpressive\u201d, \u201cimperative\u201d to this category.\n6. -RET-: we map all revisited concepts to this\ncategory.\n7. -VERBAL-: we additionally use the verbal-\nization list 2 from the AMR website and map matched subgraphs to this category.\nAfter the re-categorization, the vocabulary size is substantially reduced to around 2000, though this vocabulary size is still very large for the relatively small dataset. These categories and the frequent concepts amount to more than 90% of all the target words, and each of these are learned with a larger number of occurrences."}, {"heading": "4.2 Categorize Source Sequence", "text": "The source side tokens also have sparsity issues. For example, even if we have mapped the number 1997 to \u201cDATE\u201d, we can not easily generalize it\n2http://amr.isi.edu/download/lists/verbalization-listv1.06.txt\nto the token 1993 if it does not appear in the training data. Also, some special 6-digit date formats such as \u201cYYMMDD\u201d are hard to learn using cooccurrence statistics.\nOur basic approach to dealing with this issue is to generalize these sparse tokens or spans to some special categories (currently we use the same set of categories defined in the previous section). On the training data, we can use the heuristic alignment. For example, if we learned from the heuristic alignment that \u201c010911\u201d is aligned to a dateentity of September 11, 2001 on the AMR side, we use the same category \u201cDATE\u201d to replace this token. We distinguish this alignment from other date alignments by assigning a unique indexed category \u201cDATE-X\u201d to both sides of the alignment, where \u201cX\u201d counts from 0 and adds one for each new date entity from left to right on the sentence side. The same index strategy goes for all the other categories. Figure 4 shows an example of the linearized parallel sequence. The first infrequent non-predicate variable \u201cseismology\u201d is mapped to \u201c-SURF\u20130\u201d, then \u201cwrong\u201d to \u201c-SURF\u20131\u201d based on its position on the sentence side. The indexed category labels are then projected onto the target side based on the heuristic alignment. During this re-categorization procedure, we build a map Q from each token or span to its most likely concept or category on the target side based on relative frequency. We also dump a DATE template for recognizing new date entities by abstracting away specific date fields such as \u201c1997\u201d to \u201cYEAR\u201d, \u201cSeptember\u201d to \u201cMONTH\u201d. For example, we build a template \u201cMONTH DAY, YEAR\u201d\nfrom the specific date mention \u201cJune 6, 2007\u201d.\nDuring decoding, we are only given the sentence. We first use the date templates learned from the training data to recognize dates in each sentence. We also use a named entity tagger to recognize named entity mentions in the sentence. We use the entity name and wiki information from Q if there is a match of the entity mention, otherwise for convenience we simply use \u201cperson\u201d as the entity name and use wiki \u201c-\u201d. For each of the other tokens, we first look it up in Q and replace it with the most likely mapping. If there is no match, we further look it up in the verbalization list. In case there is still no match, we use the part of speech information to assign its category. We replace verbs with category \u201c-VERB-\u201d and nouns with category \u201c-SURF-\u201d. In accordance with the categorized token sequence, we also index each category in the resulting sequence from left to right."}, {"heading": "4.3 Recovering AMR graph", "text": "During decoding, our output sequences usually have categories and we need to map each category to AMR concepts or subgraphs. When we categorize the tokens in each sentence before decoding, we save the mapping from each category to its original token as table D. As we use the same set of categories on both source and target sides, we heuristically align target side category label to its source side counterpart from left to right. Given table D, we know which source side token it comes from and use the most frequent concept or subgraph of the token to replace the category."}, {"heading": "4.4 Supervised Attention Model", "text": "In this section, we propose to learn the attention vector in a supervised manner. There are two main motivations behind this. First, the neural attention model usually utilizes millions of data points to train the model, which learns a quite reasonable attention vector that at each output time step constrains the decoder to put a focus on the input sequences (Bahdanau et al., 2014; Vinyals et al., 2015). However, we only have 16k sentences in the AMR training data and our output vocabulary size is quite large, which makes it hard for the model to learn a useful alignment between the input sequence and AMR concepts/relations. Second, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simultaneously. This makes it impossible for the learned\nsoft alignment to combine information from the whole output sentence context. However, traditional word alignment can easily use the whole output sequence, which will produce a more informed alignment.\nSimilar to the method used by Liu et al. (2016), we add an additional loss to the original objective function to model the disagreement between the reference alignment and the soft alignment produced by the attention mechanism. Formally, for each input/output sequence pair (w,y) in the training set, the objective function is defined as:\n\u2212 1\nn\nn\u2211\nj=1\nlog p(yj|w, y1:j\u22121) + \u03bb\u0398(\u03b1\u0304 j , \u03b1j) (7)\nwhere \u03b1\u0304j is the reference alignment for output position j, which is provided by the existing aligner, \u03b1j is the soft alignment, \u0398() is cross-entropy function, n is the length of output sequence and \u03bb is the hyperparameter which serves as a trade-off between sequence prediction and alignment supervision. Note that the supervised attention part doesn\u2019t affect the decoder which will predict the output label given learned weights.\nOne issue with this method is how we represent \u03b1\u0304. As the output of conventional aligner is a hard decision, alignment is either one or zero for each input position. In addition, multiple input words could be aligned to one single concept. Finally, in the AMR sequences, there are many output labels (mostly relations) which don\u2019t align to any word in the input sentence. We utilize a heuristic method to process the reference alignment. We assign an equal probability among the words that are aligned to one AMR concept. Then if the output label doesn\u2019t align to any input word, we assign an even probability for every input word."}, {"heading": "5 Experiments", "text": "We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013)."}, {"heading": "5.1 Experiment Settings", "text": "We first preprocess the input sentences with tokenization and lemmatization. Then we extract\nthe named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009).\nFor training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the cross-entropy loss function and we set the drop out rate to be 0.5. We train our model for 150 epochs with initial learning rate of 0.5 and learning rate decay factor 0.95 if the model doesn\u2019t improve for the 3 last epochs."}, {"heading": "5.2 Baseline Model", "text": "Our baseline model is a plain sequence-tosequence model which has been used in the constituent parsing task (Vinyals et al., 2015). While they use a 3-layer deep LSTM, we only use a single-layer LSTM for both encoder and decoder since our data is relatively small and empirical comparison shows that stacking more layers doesn\u2019t help in our case. AMR linearization follows Section 3 without categorization. Since we don\u2019t restrict the input/output vocabulary in this case, our vocabulary size is quite large: 10,886 for output vocabulary and 2,2892 for input vocabulary. We set them to 10,000 and 20,000 respectively and replace the out of vocabulary words with UNK ."}, {"heading": "5.3 Impact of Re-Categorization", "text": "We first inspect the influence of utilizing categorization on the input and output sequence. Table 1 shows the Smatch evaluation score on development set.\nWe see from the table that re-categorization improves the F-score by 13 points on the development set. As mentioned in section 4.1, by setting the low frequency threshold n to 50 and re-categorizing them into a reduced set of types, we now reduce the input/output vocabulary size to (2,000, 6,000). This greatly reduces the label sparsity and enables the neural attention model to learn a better representation on this small scale data. Another advantage of this method\nis that although AMR tries to abstract away surface forms and retain the semantic meaning structure of the sentence, a large portion of the concepts are coming from the surface form and have exactly same string form both in input sentence and AMR graph. For example, nation in sentence is mapped to concept (n / nation) in the AMR. For the frequent concepts in the output sequence, since the model can observe many training instances, we assume that it can be predicted by the attention model. For the infrequent concepts, because of the categorization step, we only require the model to predict the concept type and its relative position in the graph. By applying the post-processing step mentioned in Section 4.3, we can easily recover the categorized concepts to their original form.\nWe also inspect how the value of recategorization frequency threshold n affects the AMR parsing result. As shown in Figure 5, setting n to 0, which means no output labels will be categorized into types -VERB- and -SURF-, doesn\u2019t improve the baseline system. The reason is that we still have a large output vocabulary size and training data is still sparse with respect to the low frequency output labels. Also, if we set n too high, although the output vocabulary size becomes smaller, some of the frequent output labels that the model handles well originally will be put into the coarse-grained types, losing information in the recovery process. Thus we can see from the plot that after the optimal point the Smatch score will drop. Therefore, we choose to set n = 50 in the subsequent experiments."}, {"heading": "5.4 Impact of Supervised Alignment", "text": "Choice of External Aligner. There are two existing AMR aligners: one is a rule-based aligner coming with JAMR (Flanigan et al., 2014), which defines regular expression patterns to greedily match between AMR graph fragment and input token spans; another one is an unsupervised aligner (Pourdamghani et al., 2014) which adopts the traditional word alignment method in machine translation. Although evaluated on different set of manual alignment test sentences, both aligners achieved \u223c90% F1 score. Here we choose to use the second aligner, as it covers broader domains.\nDifferent alignment configurations To balance between the sequence learning and alignment agreement, We empirically tune the hyperparameter \u03bb and set it to 0.3. For the external alignment we use for reference, we convert it to a vector with equal probability as discussed in Section 4.4. We then train a sequence-to-sequence model with recategorized input/output and report the result on development set.\nAs shown in Table 2, the supervised attention model is able to further improve the Smatch score by another 2 points, which are mainly contributed by 3 points increase in recall. Since the reference/external alignment is mostly between the input tokens and AMR graph concepts, we believe that the supervised attention model is able to constrain the decoder so that it will output concepts which can be aligned to some tokens in the input sentence.\nBecause we have relations in the AMR graph, the alignment problem here is different from the\nword alignment in machine translation. To verify the effectiveness of our setup, we also compare our configuration to the condition NO-RELATIONALIGN where we only ignore the alignment between sentence and AMR relations by putting an all zero vector as the reference attention for each output relation label. From Table 3 we see that simply ignoring the reference attention for relations would greatly affect the model performance, and how we effectively represent the reference alignment for relations is crucial for the supervised attention model."}, {"heading": "5.5 Results", "text": "In this section we report our final result on the test set of SemEval 2016 Task 8 and compare our model with other parsers. We train our model utilizing re-categorization and supervised attention with hyperparameters tuned on the development set. Then we apply our trained model on the test set.\nFirstly, we compare our model to the existing sequence-to-sequence AMR parsing model of Barzdins and Gosko (2016). As shown in table 4, the word-level model in Barzdins and Gosko (2016) is basically our baseline model. The second model they use is a character-based sequenceto-sequence model. Our model can also be regarded as a word-level model; however, by utilizing carefully designed categorization and supervised attention, our system outperforms both their results by a large margin.\nTable 5 gives the comparison of our system to some of the teams participating in SemEval16 Task 8. Since a large portion of the teams extend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches. We can see from the table that our system fails to outperform the state-\nof-the-art system. However, the best performing system CAMR uses a dependency structure as a starting point, where dependency parsing has achieved high accuracy recently and can be trained on larger corpora. Also, it utilizes semantic role labeling and complex features, which makes the training process a long pipeline. Our system only needs minimal preprocessing, and doesn\u2019t need the dependency parsing step. Our approach is competitive with the SHRG (Synchronous Hyperedge Replacement Grammar) method of Peng et al. (2015), which does not require a dependency parser and uses SHRG to formalize the string-tograph problem as a chart parsing task. However, they still need a concept identification stage, while our model can learn the concepts and relations jointly."}, {"heading": "6 Discussion", "text": "In this paper, we have proposed several methods to make the sequence-to-sequence model work competitively against conventional AMR parsing systems. Although we haven\u2019t outperformed stateof-the-art system using the conventional methods, our results show the effectiveness of our approaches to reduce the sparsity problem when training sequence-to-sequence model on a relatively small dataset. Our work could be aligned with the effort to handle low-resource data problems when building the end-to-end neural network model.\nIn neural machine translation, the attention model is traditionally trained on millions of sentence pairs, while facing low-resource language pairs, the neural MT system performance tends to downgrade (Zoph et al., 2016). There has been growing interest in tackling sparsity/low-resource problem in neural MT. Zoph et al. (2016) use a transfer learning method to first pre-train the neural model on rich-resource language pairs and then import the learned parameters to continue training on low-resource language pairs so that the model can alleviate the sparsity problem through shared\nparameters. Firat et al. (2016) builds a multilingual neural system where the attention mechanism can be shared between different language pairs. Our work could be seen as parallel efforts to handle the sparsity problem since we focus on the input/output categorization and external alignment, which are both handy for low-resource languages.\nIn this paper, we haven\u2019t used any syntactic parser. However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure. An interesting extension would be to use a linearized dependency tree as the source sequence and apply sequence-to-sequence to generate the AMR graph. Our parser could also benefit from the modeling techniques in Wu et al. (2016)."}, {"heading": "7 Conclusion", "text": "Neural attention models have achieved great success in different NLP tasks. However, they have not been as successful on AMR parsing due to the data sparsity issue. In this paper, we described a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problems. We show that our methods have led to significant improvement over a baseline neural attention model, and our model is also competitive against models that do not use extra linguistic resources.\nAcknowledgments Funded in part by a Google Faculty Award."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699\u20131710, Lisbon, Portugal, September. Associa-", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Abstract meaning representation", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on AMR parsing accuracy", "author": ["Guntis Barzdins", "Didzis Gosko."], "venue": "arXiv preprint arXiv:1604.01278.", "citeRegEx": "Barzdins and Gosko.,? 2016", "shortCiteRegEx": "Barzdins and Gosko.", "year": 2016}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748\u2013752, Sofia, Bulgaria, August.", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Neural Machine Translation with Supervised Attention", "author": ["L. Liu", "M. Utiyama", "A. Finch", "E. Sumita."], "venue": "ArXiv e-prints, September.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Semeval-2016 task 8: Meaning representation parsing", "author": ["Jonathan May."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1063\u20131073, San Diego, California, June. Association for Computational", "citeRegEx": "May.,? 2016", "shortCiteRegEx": "May.", "year": 2016}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."], "venue": "Computational Linguistics, 31(1):71\u2013106.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "UofR at semeval-2016 task 8: Learning synchronous hyperedge replacement grammar for AMR parsing", "author": ["Xiaochang Peng", "Daniel Gildea."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1185\u2013", "citeRegEx": "Peng and Gildea.,? 2016", "shortCiteRegEx": "Peng and Gildea.", "year": 2016}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "author": ["Xiaochang Peng", "Linfeng Song", "Daniel Gildea."], "venue": "Proceedings of the Nineteenth Conference on Computational", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Aligning English strings with abstract meaning representation graphs", "author": ["Nima Pourdamghani", "Yang Gao", "Ulf Hermjakob", "Kevin Knight."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages", "citeRegEx": "Pourdamghani et al\\.,? 2014", "shortCiteRegEx": "Pourdamghani et al\\.", "year": 2014}, {"title": "Parsing English into abstract meaning representation using syntaxbased machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natu-", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147\u2013155, Boulder, Colorado,", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Boosting transition-based AMR parsing with refined actions and auxiliary analyzers", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Wang et al\\.,? 2015a", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Wang et al\\.,? 2015b", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "CAMR at semeval2016 task 8: An extended transition-based AMR parser", "author": ["Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Transfer Learning for Low-Resource Neural Machine Translation", "author": ["B. Zoph", "D. Yuret", "J. May", "K. Knight."], "venue": "ArXiv e-prints, April.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.", "startOffset": 38, "endOffset": 62}, {"referenceID": 10, "context": "AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR.", "startOffset": 69, "endOffset": 90}, {"referenceID": 3, "context": "Barzdins and Gosko (2016) applied a similar approach where AMR graphs are linearized using depth-first search and both concepts and relations are treated as tokens (see Figure 3).", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "Our model is based on an existing sequence-tosequence parsing model (Vinyals et al., 2015), which is similar to models used in neural machine translation.", "startOffset": 68, "endOffset": 90}, {"referenceID": 1, "context": "To model the left and right contexts of each input position, we use a bidirectional LSTM (Bahdanau et al., 2014).", "startOffset": 89, "endOffset": 112}, {"referenceID": 7, "context": "Here the recurrent function f we use is LongShort-Term-Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 69, "endOffset": 103}, {"referenceID": 13, "context": "Overall, the sequence-tosequence model is able to match the performance of the Berkeley Parser (Petrov et al., 2006).", "startOffset": 95, "endOffset": 116}, {"referenceID": 3, "context": "-TOP-( describe-01 ARG0( person name( name op1( \u201cRyan\u201d )op1 )name )ARG0 ARG1( person -RET- )ARG1 ARG2( genius )ARG2 )-TOPBarzdins and Gosko (2016)", "startOffset": 121, "endOffset": 147}, {"referenceID": 3, "context": "While sequence-to-sequence models can be successfully applied to constituent parsing, they do not work well on the AMR parsing task as shown by Barzdins and Gosko (2016). The main bottleneck is that the size of target vocabulary for AMR parsing is much larger than constituent parsing, tens of thousands in comparison with 128, and the", "startOffset": 144, "endOffset": 170}, {"referenceID": 1, "context": "constrains the decoder to put a focus on the input sequences (Bahdanau et al., 2014; Vinyals et al., 2015).", "startOffset": 61, "endOffset": 106}, {"referenceID": 18, "context": "constrains the decoder to put a focus on the input sequences (Bahdanau et al., 2014; Vinyals et al., 2015).", "startOffset": 61, "endOffset": 106}, {"referenceID": 8, "context": "ond, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simulta-", "startOffset": 18, "endOffset": 36}, {"referenceID": 8, "context": "Similar to the method used by Liu et al. (2016), we add an additional loss to the original objective function to model the disagreement between the reference alignment and the soft alignment produced by the attention mechanism.", "startOffset": 30, "endOffset": 48}, {"referenceID": 9, "context": "(LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016).", "startOffset": 71, "endOffset": 82}, {"referenceID": 4, "context": "2) (Cai and Knight, 2013).", "startOffset": 3, "endOffset": 25}, {"referenceID": 16, "context": "the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009).", "startOffset": 58, "endOffset": 82}, {"referenceID": 18, "context": "Our baseline model is a plain sequence-tosequence model which has been used in the constituent parsing task (Vinyals et al., 2015).", "startOffset": 108, "endOffset": 130}, {"referenceID": 6, "context": "There are two existing AMR aligners: one is a rule-based aligner coming with JAMR (Flanigan et al., 2014), which defines regular expression patterns to greedily match between AMR graph fragment and input token spans; another one is an unsupervised aligner (Pourdamghani et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 14, "context": ", 2014), which defines regular expression patterns to greedily match between AMR graph fragment and input token spans; another one is an unsupervised aligner (Pourdamghani et al., 2014) which adopts the traditional word alignment method in machine translation.", "startOffset": 158, "endOffset": 185}, {"referenceID": 3, "context": "Firstly, we compare our model to the existing sequence-to-sequence AMR parsing model of Barzdins and Gosko (2016). As shown in table 4, the word-level model in Barzdins and Gosko (2016) is basically our baseline model.", "startOffset": 88, "endOffset": 114}, {"referenceID": 3, "context": "Firstly, we compare our model to the existing sequence-to-sequence AMR parsing model of Barzdins and Gosko (2016). As shown in table 4, the word-level model in Barzdins and Gosko (2016) is basically our baseline model.", "startOffset": 88, "endOffset": 186}, {"referenceID": 3, "context": "52 Barzdins and Gosko (2016) - - 0.", "startOffset": 3, "endOffset": 29}, {"referenceID": 3, "context": "52 Barzdins and Gosko (2016) - - 0.37 Barzdins and Gosko (2016) - - 0.", "startOffset": 3, "endOffset": 64}, {"referenceID": 3, "context": "Barzdins and Gosko (2016) is the word-level neural AMR parser, Barzdins and Gosko (2016) is the character-level neural AMR", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "Barzdins and Gosko (2016) is the word-level neural AMR parser, Barzdins and Gosko (2016) is the character-level neural AMR", "startOffset": 0, "endOffset": 89}, {"referenceID": 20, "context": "tend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches.", "startOffset": 41, "endOffset": 100}, {"referenceID": 19, "context": "tend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches.", "startOffset": 41, "endOffset": 100}, {"referenceID": 21, "context": "tend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches.", "startOffset": 41, "endOffset": 100}, {"referenceID": 12, "context": "Our approach is competitive with the SHRG (Synchronous Hyperedge Replacement Grammar) method of Peng et al. (2015), which does not require a dependency parser and uses SHRG to formalize the string-tograph problem as a chart parsing task.", "startOffset": 96, "endOffset": 115}, {"referenceID": 11, "context": "52 Peng and Gildea (2016) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 22, "context": "model is traditionally trained on millions of sentence pairs, while facing low-resource language pairs, the neural MT system performance tends to downgrade (Zoph et al., 2016).", "startOffset": 156, "endOffset": 175}, {"referenceID": 22, "context": "model is traditionally trained on millions of sentence pairs, while facing low-resource language pairs, the neural MT system performance tends to downgrade (Zoph et al., 2016). There has been growing interest in tackling sparsity/low-resource problem in neural MT. Zoph et al. (2016) use a transfer learning method to first pre-train the neu-", "startOffset": 157, "endOffset": 284}, {"referenceID": 5, "context": "Firat et al. (2016) builds a multilingual neural system where the attention mechanism can be shared between different language pairs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 20, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 0, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 15, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 0, "context": ", 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure. An interesting extension would be to use a linearized dependency tree as the source sequence and apply sequence-to-sequence to generate the AMR graph. Our parser could also benefit from the modeling techniques in Wu et al. (2016).", "startOffset": 9, "endOffset": 441}], "year": 2017, "abstractText": "Neural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}