{"id": "1702.05993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "An Extended Framework for Marginalized Domain Adaptation", "abstract": "1st we propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semi - supervised scenarios. we argue that the denoising principle should be extended to explicitly promote domain - invariant features as well as help the classification task. therefore we propose to jointly learn the data auto - encoders technique and the target classifiers. genera first, in order to make the sampling denoised features domain - invariant, we propose a domain regularization that may partially be either a domain prediction loss or a maximum mean discrepancy between the source and target data. the noise marginalization in this case is reduced to solving the linear matrix system $ ax = log b $ which has reached a closed - form solution. second, in order to help the classification, we systematically include a class regularization term. adding this component reduces the learning problem to solving a sylvester linear matrix equation $ ax + bx = h c $, for which an efficient iterative procedure exists efficiently as well. we did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios and present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. we report our findings and comparison with state - of - the - art methods.", "histories": [["v1", "Mon, 20 Feb 2017 15:00:13 GMT  (28kb)", "http://arxiv.org/abs/1702.05993v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["gabriela csurka", "boris chidlovski", "stephane clinchant", "sophia michel"], "accepted": false, "id": "1702.05993"}, "pdf": {"name": "1702.05993.pdf", "metadata": {"source": "CRF", "title": "An Extended Framework for Marginalized Domain Adaptation", "authors": ["Gabriela Csurka", "Boris Chidlovski", "St\u00e9phane Clinchant"], "emails": ["Firstname.Lastname@xrce.xerox.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n05 99\n3v 1\n[ cs\n.C V\n] 2\n0 Fe\nb 20\nWe propose an extended framework for marginalized domain adap-\ntation, aimed at addressing unsupervised, supervised and semi-\nsupervised scenarios. We argue that the denoising principle should\nbe extended to explicitly promote domain-invariant features as well\nas help the classification task. Therefore we propose to jointly learn\nthe data auto-encoders and the target classifiers. First, in order to\nmake the denoised features domain-invariant, we propose a domain\nregularization that may be either a domain prediction loss or a max-\nimum mean discrepancy between the source and target data. The\nnoise marginalization in this case is reduced to solving the linear\nmatrix systemAX = B which has a closed-form solution. Second,\nin order to help the classification, we include a class regularization\nterm. Adding this component reduces the learning problem to solv-\ning a Sylvester linear matrix equation AX +BX = C, for which\nan efficient iterative procedure exists as well. We did an extensive\nstudy to assess how these regularization terms improve the baseline\nperformance in the three domain adaptation scenarios. We present\nexperimental results on two image and one text benchmark datasets,\nconventionally used for validating domain adaptation methods. We\nreport our findings and comparison with state-of-the-art methods."}, {"heading": "1 Introduction", "text": "While huge volumes of unlabeled data are generated and made available in many domains, the cost of acquiring data labels remains high. Domain Adaptation (DA) problems arise each time we need to leverage labeled data in one or more related source domains, to learn a classifier for unseen or unlabeled data in a target domain. The domains are assumed to be related, but not identical and this domain shift occurs in multiple real-world applications, such as named entity recognition or opinion extraction across different text corpora, etc.\nIn this paper, we build on the domain adaptation work based on noise marginalization [9]. In deep learning, a denoising autoencoder (DA) learns a robust feature representation from training examples. In the case of domain adaptation, it takes unlabeled instances of both source and target data and learns a new feature representation by reconstructing the original features from their noised counterparts. A marginalized denoising autoencoder (MDA)\nmarginalizes the noise at training time and thus does not require an optimization procedure using explicit data corruptions to learn the model parameters but computes the model in closed form. This makes MDAs scalable and computationally faster than the regular denoising autoencoders. The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].\nIn this paper we extend the previous efforts and propose a larger framework for the marginalized domain adaptation. The marginalized domain adaptation refers to a denoising of source and target instances that explicitly makes their features domain invariant and eases the target prediction. We propose two extensions to the MDA. The first extension is a domain regularization, aimed at generating domain invariant features. Two families of such regularization are considered; one is based on the domain prediction principle, inspired by the adversarial learning of neural networks [25]; the second uses the maximum mean discrepancy (MMD) measure [31].\nThe second extension to the MDA is a class regularization; it allows to generate a classifier for target instances which can be learned jointly with the domain invariant representation.\nOur framework works in supervised, unsupervised and semi-supervised settings, where the source data is completed with a few labeled target data, massive unlabeled target data or both, respectively. In all cases, the noise marginalization is maintained, thus ensuring the scalability and computational efficiency. We show how to jointly optimize the data denoising and the domain regularization, and how to marginalize the noise, which guarantees the closed-form solution and thus the computational efficiency. When the joint optimization is extended to the target prediction, the solution does not have a closed form, but is the solution of a Sylvester linear matrix equation AX +XB = C, for which efficient iterative methods can be used.\nThe remainder of the paper is organized as follows. In Section 2 we revise the prior art. Section 3 presents the components of the marginalized domain adaptation, including in-\nstance denoising, domain and class regularizations and target classifier learning. The joint loss minimization is detailed in Section 4. In Section 5 we describe two image and one text datasets we used, the experimental settings. We report the evaluation results which are grouped and analyzed by the three settings, namely, unsupervised, supervised and semisupervised ones. Section 6 discusses the open questions and concludes the paper."}, {"heading": "2 State of the art", "text": "Domain adaptation for text data has been studied for more than a decade, with applications in statistical machine translation, opinion mining, and document ranking [18, 47]. Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11]. Domain adaptation has equally received a lot of attention in computer vision1. A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16]. These studies distinguished three main categories of domain adaptation methods. The first category aims at correcting sampling bias [44]. The second category is in line with multi-task learning where a common predictor is learned for all domains, which makes it robust to domain shift [8]. The third family seeks to find a common representation for both source and target examples so that the classification task becomes easier [37]. Finally, an important research direction deals with the theory of domain adaptation, namely when adaptation can be effective and guaranteed with generalization bounds [3].\nMore recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34]. One successful method which aims to find common features between source and target collection relies on denoising autoencoders. In deep learning, a denoising autoencoder is a one-layer neural network trained to reconstruct input data from partial random corruption [43]. The denoisers can be stacked into multi-layered architectures where the weights are fine-tuned with costly back-propagation. Alternatively, outputs of intermediate layers can be used as input features to other learning algorithms. This learned feature representation was applied to domain adaptation [26], where stacked denoising autoencoders (SDA) achieved top performance in sentiment analysis tasks. The main drawback of SDAs is the long training time, and Chen et al. [9] proposed a variant of SDA where the random corruption is marginalized out. This crucial step yields a unique optimal solution which is computed in closed form and eliminates therefore the need for back-propagation. In addition, features learned with this approach lead to a clas-\n1For a recent comprehensive survey see\nhttp://arxiv.org/abs/1702.05374\nsification accuracy comparable with SDAs, with a remarkable reduction of the training time [9].\nMore recently, deep learning architectures have demonstrated their ability to learn robust features and that good transfer performances could be obtained by just fine-tuning the neural network on the target task [13]. While such solutions perform relatively well on some tasks, the refinement may require a significant amount of new labeled data. More recent works proposed better strategies than fine-tuning, by designing deep architecture for the domain adaptation task. For example, Ganin et al. [25] has shown that adding a domain prediction task while learning the deep neural network leads to better domain-invariant feature representation. Long et al. [34] proposed to add a multi-layer adaptation regularizer, based on a multi-kernel maximum mean discrepancy (MMD). These approaches obtained a significant performance gain which shows that transfer learning is not completely solved by fine-tuning and that transfer tasks should be addressed by appropriate deep learning representations."}, {"heading": "3 Domain adaptation by feature denoising", "text": "We define a domain D as the composition of a feature space X \u2282 IRd and a label space Y . A given task in the domain D (classification, regression, ranking, etc.) is defined by a function h : X \u2192 Y . In the domain adaptation setting, we assume working with a source domain Ds represented by the feature matrix Xs and the corresponding labels Ys, and a target domainDt with the featuresXt. We distinguish among three scenarios of domain adaptation, depending on what is available in the target domain:\n\u2022 Unsupervised (US) setting, where all available target instances are unlabeled. In this case, Xtl is empty and\nthe labeled data, denoted byXl contain only the labeled source examples,Xl = X s.\n\u2022 Supervised (SUP) setting, where few labeled target instancesXtl are available at training time. In this case,\nwe haveXl = [X s,Xtl ].\n\u2022 Semi-supervised (SS) setting, where massively unlabeled (Xtu) target data are available together with few\nlabeled (Xtl) data at the training time.\nIn what follows we propose a framework to address all three scenarios in one uniform way. It aims at finding such a transformation of source and target data that minimizes the following loss function:\n(3.1) L = L1 + \u03bbL2 + \u03b3L3,\nwhere\n\u2022 L1 is the data denoising loss on all dataX = [X s,Xt],\n\u2022 L2 is the cross-domain classification loss on labeled dataXl = [X s,Xtl ] with labelsYl = [Y s,Ytl ],\n\u2022 L3 is the domain regularization loss on source and target dataX.\nParameters \u03bb and \u03b3 capture the trade-off between the three terms. All losses and parameters are described in the following subsections. Intuitively, minimizing the total loss (3.1) can help exploring the implicit dependencies between the data denoising, the domain regularization and the crossdomain classification.\nIn this paper we study the case when all three terms in (3.1) belong to the class of squared loss functions2. More precisely:\n\u2022 L1 \u2261 L1(X,W) is the instance denoising loss under the dropout law; we minimize the square loss \u2016X \u2212 X\u0303W\u20162 between the corrupted data X\u0303 and the original data X denoised with the linear transformationW.\nThis term is the core element of the marginalized denoising autoencoder (MDA) [9].\n\u2022 L2 \u2261 L2(Xl,Yl,W,Zl) is the class regularization loss, aimed at learning a (multi-class) ridge classifier\nZl from the available corrupted and denoised instances X\u0303lW. The term is defined as \u2016Yl \u2212 X\u0303lWZl\u2016 2. It can be seen as a generalization of the Marginalized Corrupted Features (MCF) framework [36] with a square loss (the MCF corresponds to the case whenW = Id).\n\u2022 L3 \u2261 L3(X s,Xt,W) is the domain regularization loss\nthat expresses the discrepancy between the source and target domains. We explore two options for this term. One is based on the empirical maximum mean discrepancy (MMD), taking into account the class labels when available; the other uses a pre-trained domain classifier to regularize the total loss.\nWe follow the marginalized framework for optimizing the loss on corrupted data [9, 36], and minimize the loss expectation E[L]. To simplify the reading, we denote the expected loss values E[Li] also with Li.\nBy minimizing the marginalized expected loss (3.1), argminW,Zl L, we obtain optimal solutions for the transformation matrix W and classifier Zl. This can be achieved in two different ways, namely:\n\u2022 W and Zl are learned sequentially. In this case we first set \u03bb = 0 and learn W by minimizing L1 + \u03b3L3. Then for the fixed W we learn Zl from L2. Except the supervised MMD in L3, the learning of W remains unsupervised, including the supervised and\nsemi-supervised settings. The target labels in these cases are used at the second step, when the classifier Zl is learned.\n2Other loss functions such as exponential, logistic, hinge losses are interesting to explore as well, but they are beyond of the scope of this paper.\n\u2022 W and Zl are learned jointly. In this case we iteratively optimize the joint loss with respect to W and Zl. To\ninitialize the iterative process, we set W = Id and minimizeL to computeZl, then we fix Zl and optimize Lwith respect toW, and so on. The process is repeated until convergence. In practice we observed that the convergence is achieved after several iterations.\nIn the following subsections we describe in details and discuss each of the three loss terms, in Section 4 we address their different combinations.\n3.1 Domain Instance Denoising The first term we consider is the loss used by theMarginalized Denoising Autoencoder (MDA) [9]. Its basic idea is to reconstruct the input data from a partial random corruption [43] with a marginalization that yields optimal reconstruction weights in a closed form. The MDA loss can be written as\n(3.2) L1 \u2261 1\nM\nM \u2211\nm=1\n\u2016X\u2212 X\u0303mW\u2016 2 + \u03c9\u2016W\u20162,\nwhere X\u0303m \u2208 IR N \u00d7 IRd is the m-th corrupted version of X by random feature dropout with a probability p and \u03c9\u2016W\u20162 is a regularization term. In order to avoid the explicit feature corruption and an iterative optimization, Chen et al. [9] showed that by considering the limit case M \u2192 \u221e, the weak law of large numbers allows to rewrite the loss L1 as its expectation and the optimal W can be written as (see Appendix for details):\n(3.3) W = (Q+ \u03c9Id) \u22121P,\nwhereP andQ depend only on the covariance matrixX\u22a4X and the noise level p. One main advantage of the MDA is that it requires no label and therefore can be applied in all three settings US, SUP and SS. Note in the supervised case Xl = [X s,Xtl ] includes only few target examples to learnW.\n3.2 Learning with marginalized corrupted features Inspired by the Marginalized Corrupted Features (MCF) approach [36], we propose to marginalize the following loss:\n(3.4) L2 \u2261 1\nM\nM \u2211\nm=1\n\u2016Yl \u2212 X\u0303lmWZl\u2016 2 + \u03b4\u2016Zl\u2016 2,\nwhereZl \u2208 IR d\u00d7IRC is a multi-class classifier (each column corresponds to one of the C classes), Yl \u2208 IR N \u00d7 IRC is a label matrix, where ync = 1 if xn belongs to class c = 1, . . . , C, and -1 otherwise, and \u03b4\u2016Zl\u2016 2 is a regularization term. When W = Id, we obtain the MCF baseline where the classifier is learned directly with the corrupted features.\nMoreover, when p = 0, we obtain the ridge classifier learned with the original features.\nGivenW, the multi-class classifier Zl can be computed in closed form using the expected loss of (3.4) (see derivations in the Appendix):\n(3.5) Z\u2217l = (1 \u2212 p)(W \u22a4QlW + \u03b4Id) \u22121W\u22a4X\u22a4l Yl.\nThe computation of Z\u2217l requires the labeled data Xl, that contain the source (US) or possibly target data (SUP, SS).\n3.3 Reducing the discrepancy between domains The domain regularization term L3 in (3.1) is aimed at bringing the target domain closer to the source domain, by minimizing the discrepancy between the domains. In the following, we explore three options for the term L3, namely (1) a classical empirical MMD using the linear kernel, (2) its supervised version where the discrepancy is minimized between the class means and (3) a domain classifier ZD trained on the uncorrupted data to distinguish between the source and target data.\n3.3.1 Reducing the MMD between domains The minimization of maximum mean discrepancy (MMD) [6] between the source and target domains is the state of art approach widely used in the literature. It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42]. The MMD is defined as a distance in the reproducing kernel Hilbert space (RKHS). In practice, its empirical version is used, as it can be written as Tr(KN), where\nK = [ Ks,s Ks,t\nKt,s Kt,t ] and N = [\n1\nN2 s 1s,s 1 NsNt 1s,t 1\nNsNt 1s,t 1\nN2 t\n1t,t ] ,\nwhere Ka,b is the kernel distance matrix between all elements of Xa and Xb, 1a,b is a constant matrix of size Na \u00d7Nb with all elements being equal to 1, and Ns, Nt are the number of source and target examples.\nWe integrate this loss in the total one L by considering the MMD between the source and target data after the denoising. To be able to marginalize out the loss and to keep our solution linearly solvable, we use the MMD with the linear kernel. Intuitively, this corresponds to minimizing the distance between the two centroids of the source and target data after denoising. The corresponding loss can be expressed as follows\n(3.6) Lm \u2261 1\nM\nM \u2211\nm=1\nTr(W\u22a4X\u0303\u22a4mNX\u0303mW).\nAfter marginalizing the expected loss, we obtain EL3 = Tr(W\u22a4MW), whereM = E[X\u0303\u22a4NX\u0303] (see the derivations in the Appendix)."}, {"heading": "3.3.2 Reducing the MMD between the domain class", "text": "means The MMD requires no labels and can be computed between all available source all target instances. If we have labeled source and labeled target examples we can go one step further and modify the MMD to measure the distance between the means (centroids) of corresponding classes in the source and target domains [35]. The corresponding loss is the following\n(3.7) Lc \u2261 1\nM\nM \u2211\nm=1\nTr(W\u22a4X\u0303\u22a4mCX\u0303mW),\nwhere:\nCij =\n\n    \n    \n1\nNc s Nc s\nif xi,xj \u2208 D s & yi = yj = c\n1\nNc t Nt t\nif xi,xj \u2208 D t & yi = yj = c\n\u22121\nNc s Nc t\nif xi \u2208 D s,xj \u2208 D t & yi = yj = c \u22121\nNc t Nc s\nif xi \u2208 D t,xj \u2208 D s & yi = yj = c\n0 otherwise,\nN cs is the number of source instances from the class c andN c t is the number of target instances from the class c. Note that the \u201dotherwise\u201d item above includes all cases where yi 6= yj and where either xi or xj is unlabeled.\nSimilarly to Lm in (3.6), we can marginalize out the expected loss Lc and obtain ELc = Tr(W\n\u22a4McW), where Mc = E[X\u0303 \u22a4CX\u0303].\n3.3.3 Learning a domain classifier As the last option of the domain regularization L3 in (3.1), we explore a loss based on the domain classifier [14]. Inspired by [24] who proposed to regularize intermediate layers in a deep learning model with a domain prediction task, [14] combines the domain prediction regularization with the MDA.We develop a similar regularization term for our extended framework, and use it jointly with the feature denoising term L1 and the class regularization L2.\nThe idea of this domain regularization is to denoise data in such a way that pushes source data towards the target and hence allows the cross-domain classifier to perform better on the target. This is done by first learning a domain classifier ZD \u2208 IR N using a regularized ridge classifier learned on the uncorrupted data. The regularized loss is defined as \u2016YD\u2212XZD\u2016 2+\u03b1\u2016ZD\u2016 2, whereYD \u2208 IR N are the domain labels (-1 for source and +1 for target). The closed form solution is the following\n(3.8) ZD = (X \u22a4X+ \u03b1Id) \u22121(X\u22a4YD).\nThen the loss we consider in our unified framework is:\n(3.9) Ld \u2261 1\nM\nM \u2211\nm=1\n\u2016YT \u2212 X\u0303mWZD\u2016 2,\nwhere YT = 1 N is a vector containing only ones (all denoised instances should be predicted as target)."}, {"heading": "4 Minimizing the total loss", "text": "In the previous section we described three terms of the loss function L. Now we discuss two main cases of minimizing the total loss. First, we discuss the sequential case, where we first learn W using only the data without labels (L1 or L1 + \u03b3L3), and then we learn the classifier Zl or any other classifier. Second, we describe the joint case where W and Zl are learned jointly, by iteratively minimizing the total loss L1 + \u03bbL2 + \u03b3L3. In both cases we consider three options for the domain regularization L3 and discuss three domain adaptation scenarios, US, SUP and SS.\nAll mentioned combinations of the losses form different models; we denote them as follows. The sequential methods are prefixed by a character S followed by the indexes of the losses used. For example, when we learnW with L1+\u03b3Ld, the method is denoted S1D. When we learn W and Zl jointly, we prefix the method by a character J followed by the loss indexes. For example, the method J12C means that we optimize L1 + \u03bbL2 + \u03b3Lc. All the combinations are summarized in Table 1.\n4.1 Sequential framework In this case, we first obtain W in an unsupervised manner and then learn a classifier Zl using the denoised features. To get W, we set \u03bb = 0 and minimize L = L1 + \u03b3L3. For each option of loss L3, we get closed-form solutions for W, denoted S1, S1M, and S1D. All the solutions are presented in Table 1. Any model can be deployed in three domain adaptation scenarios. In the SUP and SS cases, we additionally exploit the class labels using L3 = Lc (see S1C in Table 1). Once W is computed, we learn Zl using (3.5) or use any other classifier by feeding it with the denoised features XlW. In the US case, the classifiers are learned with the denoised source features, while in the SUP and SS cases the classifier exploits additionally the labeled target data.\n4.2 Joint framework In this case, W and Zl are learned jointly, by alternatively optimizing the total loss L in variables W and Zl. We start by initializing W with Id and minimize the loss in Zl, then we fix Zl and computeW, and so on. The process is repeated until convergence for a certain threshold.\nThe partial derivatives of L with respect to Zl depend on L2 only, this makes solution (3.5) always valid. The partial derivatives with respect to W can be written as a Sylvester linear matrix equation AW +WB = C, that we solve using the Bartels-Stewart algorithm [40]. Depending on which loss is used as L3, we obtain three versions of the Sylvester equation, denoted J12, J12M and J12D and detailed in Table 1.\nNote that for J12D we do not use the regularizer term \u03c9\u2016W\u20162 in the loss, in order to be able to reduce the partial derivatives to solving a Sylvester equation. Furthermore, in the SUP case, as Q = Ql and P = Pl, if we remove \u03c9\u2016W\u20162 from J12 we obtain a closed form solution W = Q\u22121l (Pl + \u03bb(1 \u2212 p)X \u22a4 l YlZ \u22a4 l )(Id + \u03bbZlZ \u22a4 l ). Note that in our experiments we found that the results with the term (by solving a Sylvester equation) and without (a closed form solution) are similar, but the latter case is much faster."}, {"heading": "5 Experimental Results", "text": "In the experimental section, we pursue a number of important goals. First, we want to assess all models proposed3 in the previous sections in three domain adaptation scenarios. Second, we evaluate the impact of the domain regularization L3 and the class regularization L2 on the denoising matrix W and target classifier Zl. Finally, we report the performance of the sequential and joint learning cases, we analyze our results and compare them to the state-of-the art.\n3The code for all models is available at\nhttp://github.com/sclincha/xrce_msda_da_regularization\nThis section is organized as follows. In Section 5.1 we briefly describe three datasets used in the experiments, then Section 5.2 describes the experimental setting, including the methods and parameters used. In Section 5.3 we compare the sequential and joint models with different loss combinations in the US, SUP and SS settings, for all datasets. Finally, in Section 5.4 we compare our best performingmodels with the state-of-the art.\n5.1 Datasets All experiments are conducted on three domain adaptation datasets, well known in image processing and sentiment analysis communities.\nOFF31 and OC10. Two most popular datasets used to compare visual domain adaptation methods are the Office31 dataset [39] (OFF31) and the Office+Caltech10 [28] (OC10). The former consists of three domains: Amazon (A), dslr (D) and Webcam (W ) with images of 31 products (classes). The latter contains 10 of the 31 classes for the same domains and includes an extra domain from the Caltech collection. For all images, we use the Decaf TF6 features [19] with the full training protocol [27] where all source data is used for training.\nAMT. A standard dataset for textual domain adaptation is the Amazon dataset of text products reviews; it includes four domains: Books (b), DVD (d), Kitchen (k) and Electronics (b) preprocessed by Blitzer et al. [5]. Reviews are considered as positive if they have more than 3 stars, and negative otherwise. We adopt the experimental setting of [25] where documents are represented by a bag of unigrams and bi-grams with the 5000 most frequent common words selected and a tf-idf weighting scheme.\n5.2 Methods and settings Most models proposed in the previous sections produce the denoising matrix W and the target classifier Zl, which can be inferred sequentially or jointly. The sequential approach learns first the matrix W and then a target classifier Zl for a fixed W. The joint approach learnsW andZl jointly 4 by alternating the updates ofW and Zl using the same loss L. For each dataset, we consider all domains and take all possible source-target pairs as domain adaptation tasks. For example, for OFF31 with three domains, we consider six following adaptation tasks: D\u2192A, W\u2192A, A\u2192D, W\u2192D, A\u2192W, and D\u2192W. Similarly, for OC10 and AMT which include four domains each, we consider 12 different sourcetarget pairs as adaptation tasks. To compare the different models, we report averaged accuracies over all adaptation tasks for a given dataset. In the supervised (SUP) and semi-supervised (SS) scenarios, we randomly select 3 target instances per class to form the target training set, and use the\n4In this case, while we do not have the guarantee a global minimum, we observed in general to quick convergence of the loss (only a few iterations).\nrest for the test.\nIn addition to the sequential and joint model learning, we include in our framework two standard classifiers, the nearest neighbor (NN) classifier and the Domain Specific Class Means (DSCM) as they represent a valuable alternative to the ridge target Zl classifier. classifier [15]. In DSCM, a target test example is assigned to a class by using a softmax distance to the domain specific class means. The main reason for choosing these classifiers is that NN is related to retrieval (equivalent to precision at 1) and NCM with clustering, so the impact of W on these two extra tasks is indirectly assessed.\nFed with the denoised instances, obtained with matrix W, these classifiers help assess the value of our framework for domain adaptation tasks.\nTo ensure the fair comparison of all methods, we run all experiments with a unique parameter setting. All selected parameter values are explained below: Besides, crossvalidation on the source is not the best way to set model parameters for transfer learning and domain adaptation [45].\n\u2022 we set \u03bb = \u03b3 = 1 as term weights, this corresponds to the equal weighting in the global loss (3.1);\n\u2022 we set \u03c9 = 10\u22122 asW norm regularization in (3.3) (as in [9]);\n\u2022 we set the dropping noise level for P and Q in (3.3): p = 0.5 for image datasets and p = 0.9 for AMT, as text representations are initially very sparse and a higher\nnoise level is required;\n\u2022 we set \u03b1 = \u03b4 = 1 for the classifier regularization terms in (3.4) and (3.8);\n\u2022 we consider a single layer MDA only, to enable a fair comparison of different loss combinations and learning\nmethods5.\nTo reveal all strong and weak points of our framework, we compare all models and classifiers with two baselines. The first baseline is denoted BL and provided by the classifier learned on the original features, without denoising. The classifier Zl is learned using (3.5) with p = 0 and hence Q = S. The second baseline refers to the original MDA method [9] and corresponds to S1 method in our framework. It uses the loss L1 to build W in an unsupervised manner and learns a classifier on the denoised features.\nIn the following subsections, we compare the methods of our framework to the baselines, for all domain adaptation settings and all datasets.\n5Similarly to the stacked MDA, our framework can be extended to a stacked version, where the denoised features XW of one layer become the input of a next layer and nonlinear functions such as tangent hyperbolic are applied between the two."}, {"heading": "5.3 Comparing domain adaptation methods", "text": "5.3.1 Unsupervised Setting In this case, labeled data are available from the source domain only, Xl = X s. We compare the different models described in Section 4 (see the summary in Table 1). For each method and each domain adaptation task, we learn the Zl classifier and the NN and DSCM classifiers applied to the denoised features. The accuracy results are averaged per dataset and reported in Table 2.\nThe first observation is that all BL baselines get improved by the MDA (S1). Second, on the text data (AMT), the Zl classifier performs the best 6, Third, on the image collections the picture is more complex, with the NN showing the globally highest accuracy. If we compare the baseline S1 and extended methods, we can conclude the following. In sequential framework, the domain regularization S1D often improves over S1 for the linear classifier Zl and the DSCM, but not of the NN. In the joint framework, the class regularization L2 degrades the linear classifier Zl results but improves the DSCM classifier.\nTo conclude, in the unsupervised domain adaptation, the best strategy may depend on the data type, regularization and classification method. If we compare methods by averaging their results over the rows in Table 2, S1D with the average 75.7% appears as the best strategy over all classifiers and datasets, followed by J12. Both outperform the baseline S1 with the average 74.5%. This suggests that the best strategy is to learn the denoising W with the domain regularization and then to learn any classifier from denoised features.\n5.3.2 Supervised Setting In this case we haveX = Xl = [Xs,Xtl ], Ql = Q and unlabeled target data is unavailable. In Table 3 we report the evaluation results for the different models usingZl, NN and DSCM classifiers. It is easy to note that all models in the supervised case behave quite different from the unsupervised one.\n6NN and DSCM results are not included in the table, due to space\nlimitation.\nSequential framework. The domain regularization Ld seems to harm the baseline performance. One possible explanation is that only few target examples are available, comparing to the much larger source set. In contrast, adding the regularization Lc that exploits the class labels improves in many cases the results.\nJoint framework. Adding L2 improves the results in general, except when using Zl on AMT dataset. While J12 and J1D perform rather similarly, J12C outperforms them globally on all datasets and classifiers. Note however, that the jointly learned Zl performs less well than using the jointly learned W to denoise the data on which a standard classifier is learned. This is the case when the framework implicitly combines the benefit of the joint learning with any classifier; such a strategy seems to work the best.\n5.3.3 Semi-supervised Setting In this case, a large set of unlabeled target data is available together with a small set of labeled target data. We explore if the proposed methods are able to take advantage of the two. Table 4 shows the results of different models on the three datasets, and them to the baselines.\nSequential framework. Compared to the supervised case, having more target examples makes the domain regularization L3 to either have no effect or slightly improve the results.\nJoint framework. Adding class regularization L2 either improves or does not change the results, except learning Zl for AMT, where a significant drop is observed. Adding Ld often decreases the performance, while adding Lc is less harmful. In general, in the semi-supervised case J12 is the best strategy for the image datasets. For the text dataset, like in the unsupervised case, using S1D with W, followed by learning a classifier on the denoised features seems to be a better option.\n5.4 Comparison with the state of the art We complete the experimental section by comparing our results to the state of art results, in the unsupervised and semi-supervised\ncases7. OC10. Using the FC6 features, our S1M accuracy (86.5%) in Table 2 outperforms8 the domain adaptive SVM [7] (70.3%) and domain adaptation via auxiliary classifiers [21] (84%), and slightly underperforms the more complex JDA [35] (87.5%) and TTM [22] (87.5%) methods.\nOFF31. Table 5 compares our S1D+Zl and S12+NN results with the feature transformation methods, using the same deep FC6 features, and with recent deep learning methods. It reports the results for six domain adaptation tasks onOFF31 and their average. We can see that our methods behave similarly or better than feature transformation methods, but below the deep adaptation ones. Designed to be fast, our methods solve a few linear systems at training time and a simple matrix multiplication at test time, while the deep architectures have thousands of parameters to be tuned with the back propagation and GPU computations at the training time.\nAMT. Our L1D+Zl results (82.2%, see Table 2) is similar to the state-of-the art results obtained with the DomainAdversarial Neural Networks (DANN) [25], despite the fact that DANN uses a 5 layer stacked MDA where the 5 outputs are concatenated with input to generate 30,000 dimensional features, on which the network is trained.\nConcerning the semi-supervised scenario, it is much less used and most papers report results with SURF BOV features and the sampling protocol [39, 28]. We therefore tested our methods on OC10 with L12C+DSCM and BOV features averaged over the 20 random samples; and we get an accuracy of 55.8% that is above most state of art results, including GFK [28] (48.6%), SA [23] (53.6%), MMDT [30] (52.5%).\n7We exclude the supervised scenario as rarely addressed in the literature. 8We report results from [22]."}, {"heading": "6 Conclusion", "text": "We proposed an extended framework for domain adaptation, where the state-of-the-art marginalized denoising autoencoder is extended with domain and class regularization terms, aimed at addressing unsupervised, supervised and semi-supervised scenarios. The domain regularization drives the denoising of both source and target data toward domain invariant features. Two families of domain regularization, based on domain prediction and the maximum mean discrepancy, are proposed. The class regularization learns a cross-domain classifier jointly with the common representation learning. In all cases, the models can be reduced to solving a linear matrix equation or its Sylvester version, for which efficient algorithms exist.\nWe presented the results of an extensive set of experiments on two image and one text benchmark datasets, where the proposed framework is tested in different settings. We showed that adding the new regularization terms allow to outperform the baselines and help design best performing strategies for each adaptation scenarios and data types. Compared to the state of art we showed that despite of their speed and relatively low cost, our models yield comparable or better results than existing feature transformation methods but below highly expensive non-linear methods with additional data processing such as the landmark selection or those using deep architectures requiring costly operations both at training and at test time. Furthermore, similarly to the stacked MDA framework, we can easily stack several layers together with only forward learning, where the denoised features of the previous layer become the input of a new layer and nonlinear functions can be applied between the layers."}], "references": [{"title": "Landmarks-based kernelized subspace alignment for unsupervised domain adaptation", "author": ["R. Aljundi", "R. Emonet", "D. Muselet", "M. Sebban"], "venue": "Proc. of CVPR, (IEEE)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M. Harandi", "B. Lovell", "M. Salzmann"], "venue": "Proc. of ICCV, (IEEE)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "EMNLP", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain Adaptation with Coupled Subspaces", "author": ["J. Blitzer", "S. Kakade", "D.P. Foster"], "venue": "AISTATS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["K.M. Borgwardt", "A. Gretton", "M.J. Rasch"], "venue": "Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain adaptation problems: A dasvm classification technique and a circular validation strategy", "author": ["L. Bruzzone", "M. Marconcini"], "venue": "(PAMI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Co-training for domain adaptation", "author": ["M. Chen", "K.Q. Weinberger", "J. Blitzer"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "F. Sha"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Marginalized denoising for link prediction and multi-label learning", "author": ["Z. Chen", "M. Chen", "K.Q. Weinberger", "W. Zhang"], "venue": "AAAI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Topic modeling using topics from many domains", "author": ["Z. Chen", "B. Liu"], "venue": "lifelong learning and big data. In ICML", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A marginalized denoising method for link prediction in relational data", "author": ["Z. Chen", "W. Zhang"], "venue": "ICDM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "DLID: Deep learning for domain adaptation by interpolating between domains", "author": ["S. Chopra", "S. Balakrishnan", "R. Gopalan"], "venue": "ICML Workshop (WREPL)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A domain adaptation regularization for denoising autoencoders", "author": ["S. Clinchant", "G. Csurka", "B. Chidlovskii"], "venue": "Proc. of ACL", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Domain adaptation with a domain specific class means classifier", "author": ["G. Csurka", "B. Chidlovskii", "F. Perronnin"], "venue": "TASK- CV, ECCV workshop", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain Adaptation for Visual Applications: A Comprehensive Survey", "author": ["G. Csurka"], "venue": "CoRR, arXiv:1702.05374", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9"], "venue": "CoRR, arXiv:0907.1815", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Domain adaptation for statistical classifiers", "author": ["H. Daume III", "D. Marcu"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "et al", "author": ["J. Donahue"], "venue": "Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, arXiv:1310.1531", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain transfer multiple kernel learning", "author": ["L. Duan", "I.W. Tsang", "D. Xu"], "venue": "Transactions of Pattern Recognition and Machine Analyses (PAMI), 34(3):465\u2013479", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain adaptation from multiple sources via auxiliary classifiers", "author": ["L. Duan", "I.W. Tsang", "D. Xu", "T.-S. Chua"], "venue": "Proc. of ICML, pages 289\u2013296", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "T", "author": ["N. Farajidavar"], "venue": "deCampos, and J. Kittler. Transductive transfer machines. In Proc. of ACCV, pages 623\u2013639", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "Proc. of ICCV, (IEEE), pages 2960\u20132967", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "Proc. of ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-adversarial training of neural net-  works", "author": ["Y. Ganin"], "venue": "CoRR, arXiv:1505.07818,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of ICML", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Connecting the dots with landmarks: Discriminatively learning domain invariant features for unsupervised domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "ICML", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain adaptation for visual recognition", "author": ["R. Gopalan", "R. Li", "V.M. Patel", "R. Chellappa"], "venue": "Foundations and Trends in Computer Graphics and Vision, 8(4)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient learning of domain-invariant image representations", "author": ["J. Hoffman", "E. Rodner", "J. Donahue", "T. Darrell", "K. Saenko"], "venue": "Proc. of ICLR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A. Smola", "A. Gretton", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Proc. of NIPS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep collaborative filtering via marginalized denoising auto-encode", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "CIKM ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with marginalized corrupted features and labels together", "author": ["Y. Li", "M. Yang", "Z. Xu", "Z. Zhang"], "venue": "Proc. of AAAI, volume arXiv:1602:07332", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "Proc. of ICML", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["M. Long", "J. Wang", "G. Ding", "J. Sun", "P.S. Yu"], "venue": "Proc. of ICCV, (IEEE)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning with marginalized corrupted features", "author": ["L. v. d. Maaten", "M. Chen", "S. Tyree"], "venue": "In Proc. of ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "J", "author": ["S.J. Pan"], "venue": "T. Tsang, Ivor W.and Kwok, and Q. Yang. Domain adaptation via transfer component analysis. Transactions on Neural Networks", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Transactions on Knowledge and Data Engineering", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "Proc. of ECCV", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Direct methods for matrix Sylvester and Lyapunov equations", "author": ["D.C. Sorensen", "Y. Zhou"], "venue": "In Journal of Applied Mathematics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Return of frustratingly easy  domain adaptation", "author": ["B. Sun", "J. Feng", "K. Saenko"], "venue": "Proc. of AAAI", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "CoRR, arXiv:1412.3474", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proc. of ICML", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-source transfer learning with multiview adaboost", "author": ["Z. Xu", "S. Sun"], "venue": "Proc. of NIPS, pages 332\u2013339", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross validation framework to choose amongst models and datasets for transfer learning", "author": ["E. Zhong", "W. Fan", "Q. Yang", "O. Verscheure", "J. Ren"], "venue": "Proc. PKDD (ECML)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Hybrid heterogeneous transfer learning through deep learning", "author": ["J.T. Zhou", "S.J. Pan", "I.W. Tsang", "Y. Yan"], "venue": "Proc. of AAAI", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying learning to rank and domain adaptation: Enabling cross-task document scoring", "author": ["M. Zhou", "K.C. Chang"], "venue": "Proc. of SIGKDD ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "In this paper, we build on the domain adaptation work based on noise marginalization [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 34, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 179, "endOffset": 183}, {"referenceID": 30, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 209, "endOffset": 213}, {"referenceID": 31, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 254, "endOffset": 262}, {"referenceID": 44, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 254, "endOffset": 262}, {"referenceID": 24, "context": "Two families of such regularization are considered; one is based on the domain prediction principle, inspired by the adversarial learning of neural networks [25]; the second uses the maximum mean discrepancy (MMD) measure [31].", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "Two families of such regularization are considered; one is based on the domain prediction principle, inspired by the adversarial learning of neural networks [25]; the second uses the maximum mean discrepancy (MMD) measure [31].", "startOffset": 222, "endOffset": 226}, {"referenceID": 17, "context": "Domain adaptation for text data has been studied for more than a decade, with applications in statistical machine translation, opinion mining, and document ranking [18, 47].", "startOffset": 164, "endOffset": 172}, {"referenceID": 45, "context": "Domain adaptation for text data has been studied for more than a decade, with applications in statistical machine translation, opinion mining, and document ranking [18, 47].", "startOffset": 164, "endOffset": 172}, {"referenceID": 16, "context": "Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11].", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 27, "context": "A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16].", "startOffset": 129, "endOffset": 141}, {"referenceID": 36, "context": "A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16].", "startOffset": 129, "endOffset": 141}, {"referenceID": 15, "context": "A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16].", "startOffset": 129, "endOffset": 141}, {"referenceID": 42, "context": "The first category aims at correcting sampling bias [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "The second category is in line with multi-task learning where a common predictor is learned for all domains, which makes it robust to domain shift [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 35, "context": "The third family seeks to find a common representation for both source and target examples so that the classification task becomes easier [37].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "Finally, an important research direction deals with the theory of domain adaptation, namely when adaptation can be effective and guaranteed with generalization bounds [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "More recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34].", "startOffset": 121, "endOffset": 133}, {"referenceID": 25, "context": "More recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34].", "startOffset": 121, "endOffset": 133}, {"referenceID": 32, "context": "More recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34].", "startOffset": 121, "endOffset": 133}, {"referenceID": 41, "context": "In deep learning, a denoising autoencoder is a one-layer neural network trained to reconstruct input data from partial random corruption [43].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "This learned feature representation was applied to domain adaptation [26], where stacked denoising autoencoders (SDA) achieved top performance in sentiment analysis tasks.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "[9] proposed a variant of SDA where the random corruption is marginalized out.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "05374 sification accuracy comparable with SDAs, with a remarkable reduction of the training time [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "More recently, deep learning architectures have demonstrated their ability to learn robust features and that good transfer performances could be obtained by just fine-tuning the neural network on the target task [13].", "startOffset": 212, "endOffset": 216}, {"referenceID": 24, "context": "[25] has shown that adding a domain prediction task while learning the deep neural network leads to better domain-invariant feature representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] proposed to add a multi-layer adaptation regularizer, based on a multi-kernel maximum mean discrepancy (MMD).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This term is the core element of the marginalized denoising autoencoder (MDA) [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 34, "context": "It can be seen as a generalization of the Marginalized Corrupted Features (MCF) framework [36] with a square loss (the MCF corresponds to the case whenW = Id).", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "We follow the marginalized framework for optimizing the loss on corrupted data [9, 36], and minimize the loss expectation E[L].", "startOffset": 79, "endOffset": 86}, {"referenceID": 34, "context": "We follow the marginalized framework for optimizing the loss on corrupted data [9, 36], and minimize the loss expectation E[L].", "startOffset": 79, "endOffset": 86}, {"referenceID": 8, "context": "1 Domain Instance Denoising The first term we consider is the loss used by theMarginalized Denoising Autoencoder (MDA) [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 41, "context": "Its basic idea is to reconstruct the input data from a partial random corruption [43] with a marginalization that yields optimal reconstruction weights in a closed form.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "[9] showed that by considering the limit case M \u2192 \u221e, the weak law of large numbers allows to rewrite the loss L1 as its expectation and the optimal W can be written as (see Appendix for details):", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "2 Learning with marginalized corrupted features Inspired by the Marginalized Corrupted Features (MCF) approach [36], we propose to marginalize the following loss:", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "1 Reducing the MMD between domains The minimization of maximum mean discrepancy (MMD) [6] between the source and target domains is the state of art approach widely used in the literature.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 58, "endOffset": 65}, {"referenceID": 35, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 58, "endOffset": 65}, {"referenceID": 19, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 132, "endOffset": 144}, {"referenceID": 32, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 132, "endOffset": 144}, {"referenceID": 40, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 132, "endOffset": 144}, {"referenceID": 33, "context": "If we have labeled source and labeled target examples we can go one step further and modify the MMD to measure the distance between the means (centroids) of corresponding classes in the source and target domains [35].", "startOffset": 212, "endOffset": 216}, {"referenceID": 13, "context": "1), we explore a loss based on the domain classifier [14].", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "Inspired by [24] who proposed to regularize intermediate layers in a deep learning model with a domain prediction task, [14] combines the domain prediction regularization with the MDA.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Inspired by [24] who proposed to regularize intermediate layers in a deep learning model with a domain prediction task, [14] combines the domain prediction regularization with the MDA.", "startOffset": 120, "endOffset": 124}, {"referenceID": 38, "context": "The partial derivatives with respect to W can be written as a Sylvester linear matrix equation AW +WB = C, that we solve using the Bartels-Stewart algorithm [40].", "startOffset": 157, "endOffset": 161}, {"referenceID": 37, "context": "Two most popular datasets used to compare visual domain adaptation methods are the Office31 dataset [39] (OFF31) and the Office+Caltech10 [28] (OC10).", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "For all images, we use the Decaf TF6 features [19] with the full training protocol [27] where all source data is used for training.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "For all images, we use the Decaf TF6 features [19] with the full training protocol [27] where all source data is used for training.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "We adopt the experimental setting of [25] where documents are represented by a bag of unigrams and bi-grams with the 5000 most frequent common words selected and a tf-idf weighting scheme.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "classifier [15].", "startOffset": 11, "endOffset": 15}, {"referenceID": 43, "context": "All selected parameter values are explained below: Besides, crossvalidation on the source is not the best way to set model parameters for transfer learning and domain adaptation [45].", "startOffset": 178, "endOffset": 182}, {"referenceID": 8, "context": "3) (as in [9]);", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "The second baseline refers to the original MDA method [9] and corresponds to S1 method in our framework.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "5%) in Table 2 outperforms the domain adaptive SVM [7] (70.", "startOffset": 51, "endOffset": 54}, {"referenceID": 20, "context": "3%) and domain adaptation via auxiliary classifiers [21] (84%), and slightly underperforms the more complex JDA [35] (87.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "3%) and domain adaptation via auxiliary classifiers [21] (84%), and slightly underperforms the more complex JDA [35] (87.", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "5%) and TTM [22] (87.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "2%, see Table 2) is similar to the state-of-the art results obtained with the DomainAdversarial Neural Networks (DANN) [25], despite the fact that DANN uses a 5 layer stacked MDA where the 5 outputs are concatenated with input to generate 30,000 dimensional features, on which the network is trained.", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "Concerning the semi-supervised scenario, it is much less used and most papers report results with SURF BOV features and the sampling protocol [39, 28].", "startOffset": 142, "endOffset": 150}, {"referenceID": 22, "context": "6%), SA [23] (53.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "6%), MMDT [30] (52.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "We report results from [22].", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "We propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semisupervised scenarios. We argue that the denoising principle should be extended to explicitly promote domain-invariant features as well as help the classification task. Therefore we propose to jointly learn the data auto-encoders and the target classifiers. First, in order to make the denoised features domain-invariant, we propose a domain regularization that may be either a domain prediction loss or a maximum mean discrepancy between the source and target data. The noise marginalization in this case is reduced to solving the linear matrix systemAX = B which has a closed-form solution. Second, in order to help the classification, we include a class regularization term. Adding this component reduces the learning problem to solving a Sylvester linear matrix equation AX +BX = C, for which an efficient iterative procedure exists as well. We did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios. We present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. We report our findings and comparison with state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}