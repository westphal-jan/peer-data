{"id": "1702.06086", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Label Distribution Learning Forests", "abstract": "label number distribution learning ( ldl ) is a general learning framework, which normally assigns a distribution over a defined set of labels to determine an instance labels rather than a single label or multiple boundary labels. current ldl methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning. this paper presents label distribution learning forests ( ldlfs ) - a novel label distribution learning algorithm essentially based on differentiable decision trees, which have several advantages : 1 ) producer decision trees have the potential to model any general form of label distributions by the mixture of leaf node predictions. 2 ) the learning of differentiable decision trees can be basically combined with labeled representation learning, e. in g., to learn deep features in an end - to - end manner. meanwhile we define a convex distribution - based loss function for screening forests, enabling all the trees to be learned jointly, specifically and show that thus an update risk function for leaf node predictions, which guarantees a strict decrease of being the loss function, can be derived by variational bounding. the effectiveness of the proposed ldlfs is verified on two ldl sampling problems, including age estimation checking and crowd opinion polls prediction on movies, showing significant improvements to the state - testing of - the - art ldl matching methods.", "histories": [["v1", "Mon, 20 Feb 2017 18:04:31 GMT  (523kb,D)", "http://arxiv.org/abs/1702.06086v1", "Submitted to IJCAI2017"], ["v2", "Tue, 11 Jul 2017 19:17:32 GMT  (475kb,D)", "http://arxiv.org/abs/1702.06086v2", "Submitted to NIPS2017"], ["v3", "Wed, 20 Sep 2017 06:48:22 GMT  (480kb,D)", "http://arxiv.org/abs/1702.06086v3", "Accepted by NIPS2017"], ["v4", "Mon, 16 Oct 2017 21:05:45 GMT  (469kb,D)", "http://arxiv.org/abs/1702.06086v4", "Accepted by NIPS2017"]], "COMMENTS": "Submitted to IJCAI2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wei shen", "kai zhao", "yilu guo", "alan yuille"], "accepted": true, "id": "1702.06086"}, "pdf": {"name": "1702.06086.pdf", "metadata": {"source": "CRF", "title": "Label Distribution Learning Forests", "authors": ["Wei Shen", "Kai Zhao", "Yilu Guo", "Alan Yuille"], "emails": ["shenwei1231@gmail.com", "zhaok1206@gmail.com", "gyl.luan0@gmail.com", "alan.l.yuille@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Label distribution learning (LDL) [Geng, 2016; Geng et al., 2013] is a learning strategy to deal with problems of label ambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [Tsoumakas and Katakis, 2007], which assume an instance is assigned to a single label or multiple labels, LDL aims at learning the relative importance of each label involved in the description of an instance, i.e., a distribution over the set of labels. Such a learning strategy is suitable for many real-world problems, which have label ambiguity. An example is facial age estimation [Geng et al., 2010]. Even humans cannot predict the precise age from a single facial image. They may only say that the person is probably in one age group and less likely to be in another. Hence it is more natural to assign a distribution of age labels to each facial image\n(Fig. 1(a)) instead of using a single age label. Another example is predicting crowd opinion on movies [Geng and Hou, 2015]. Many famous movie review web sites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the distribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a rating distribution for every movie before it is released, movie producers can reduce their investment risk and the audience can better choose which movies to watch. Learning such a system naturally fits the LDL paradigm, since the rating distribution can be viewed as a label distribution for each movie.\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [Berger et al., 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016]. But, the exponential part of this model restricts the generality of the distribution form, e.g., it has difficulty in representing mixture distribution. Some other LDL methods extend the existing learning algorithms, e.g, by boosting and support vector regression, to deal with label distributions [Geng and Hou, 2015; Xing et al., 2016], which avoid making this assumption, but have limitations in representation learning, e.g., they do not learn deep features in an end-to-end manner.\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution learning algorithm inspired by differentiable decision trees [Kontschieder et al., 2015]. Extending differentiable decision trees to deal with the LDL task has two advantages. One is that decision trees have the potential to model any general form of label distributions by mixture of the leaf node predictions, which avoid making strong assumption on the form of the label distributions. The second is that the split node parameters in differ-\nar X\niv :1\n70 2.\n06 08\n6v 1\n[ cs\n.L G\n] 2\n0 Fe\nb 20\n17\nentiable decision trees can be learned by back-propagation, which enables the combination of tree learning and representation learning in an end-to-end manner. We define a distribution-based loss function for a tree by the KullbackLeibler divergence (K-L) between the ground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we show that the optimization of leaf node predictions to minimize the loss function of the tree can be addressed by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003], in which the original loss function to be minimized gets iteratively replaced by upper bounds. Following this optimization strategy, we derive a discrete iterative function to update the leaf node predictions. To learn forests, we average the losses of all the individual trees to be the loss for forests and allow the split nodes from different trees to be connected to the same output unit of the feature learning function. In this way, the split node parameters of all the individual trees can be learned jointly. Fig. 2 illustrates a sketch chart of our LDLFs. Our LDLFs can be integrated with any deep network, and can also be used as a (shallow) stand-alone model. We verify the effectiveness of our algorithm on two LDL problems, including facial age estimation and crowd opinion prediction on movies, showing significant improvements to the state-of-the-art LDL methods."}, {"heading": "2 Related Work", "text": "Since our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review some typical techniques of decision trees. Then, we discuss current LDL methods.\nDecision trees. Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks. In the\npast, learning of a decision tree was based on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each split node [Amit and Geman, 1997], and thus, cannot be integrated into in a deep learning framework, i.e., be combined with representation learning in an end-toend manner.\nThe newly proposed deep neural decision forests (dNDFs) [Kontschieder et al., 2015] overcomes this problem by introducing a soft differentiable decision function at the split nodes and a global loss function defined on a tree. This ensures that the split node parameters can be learned by backpropagation and leaf node predictions can be updated by a discrete iterative function.\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because gradient descent cannot be applied to learn leaf node predictions, as the gradient w.r.t. them is not close-formed. While the discrete iterative function given in dNDFs (used to update leaf node predictions), was only proved to converge for a classification loss. Consequently, it was unclear how to obtain such a function for other losses. We observed, however, that this discrete iterative function can be derived from variational bounding, which allows us to extend it to our LDL problem. In addition, the strategies used in LDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly define a loss function for forests, while only the loss function for a single tree was defined in dNDFs; 2) we allow the split nodes from different trees to be connected to the same output unit of the feature learning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in dNDFs were learned alternatively. These changes in the ensemble learning are important, because we show in our experiments (Sec. 4.3) that LDLFs can get better results by using more trees, but by using the ensemble strategy proposed in dNDFs, the results of forests are even worse than one single tree\u2019s.\nLabel distribution learning. A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al., 2015] and hand orientation estimation [Geng and Xia, 2014].\nGeng et al. [Geng et al., 2010] defined the label distribution for an instance as a vector containing the probabilities of the instance having each label. They also gave a strategy to assign a proper label distribution to an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak is the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization process based on a two-layer energy based model. Yang et al. [Yang et al., 2016] then defined a three-layer energy based model, called SCE-LDL, in which the ability to perform representation learning is improved by adding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model. Geng [Geng, 2016] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton optimization. All the above LDL methods assume that the\nlabel distribution can be represented by a maximum entropy model [Berger et al., 1996]. While the exponential part of this model restricts the generality of the distribution form.\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label distributions. Geng and Hou [Geng and Hou, 2015] proposed LDSVR, a LDL method by extending support vector regressor, which fit a sigmoid function to each component of the distribution simultaneously by a support vector machine. Xing et al. [Xing et al., 2016] then extended boosting to address the LDL task by additive weighted regressors. They showed that using the vector tree model as the weak regressor can lead to better performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model is based on locally-optimal hard data partition functions at each split node, AOSOLDLLogitBoost is unable to be combined with representation learning. Extending current deep learning algorithms to address the LDL task is an interesting topic. There are two deep learning based LDL methods [Yang et al., 2015; Huo et al., 2016], but both of them focused on how to boost the performance by combing multiple deep models trained on different datasets.\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted label distribution for a sample can be expressed by a linear combination of the label distributions of the training data, and thus have no restrictions on the distributions (e.g., maximum entropy). In addition, thanks to the introduction of differentiable decision functions, LDLFs can be combined with representation learning in an end-to-end manner."}, {"heading": "3 Label Distribution Learning Forests", "text": "Forests are an ensemble of decision trees. We first introduce how to learn a single decision tree by label distribution learning, then describe the learning of forests."}, {"heading": "3.1 Problem Formulation", "text": "Let X = Rm denote the input space and Y = {y1, y2, . . . , yC} denote the complete set of labels, where C is the number of possible label values. We consider a label distribution learning (LDL) problem, where for each input sample x \u2208 X , there is a label distribution d = (dy1x , d y2 x , . . . , d yC x ) > \u2208 RC . Here dycx expresses the probability of the sample x having the c-th label yc and thus has the constrains that dycx \u2208 [0, 1] and \u2211C c=1 d yc x = 1. The goal of the LDL problem is to learn a mapping function g : x\u2192 d between an input sample x and its corresponding label distribution d.\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision tree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \u2208 N defines a split function sn(\u00b7; \u0398) : X \u2192 [0, 1] parameterized by \u0398 to determine whether a sample is sent to the left or right subtree. Each leaf node ` \u2208 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )\n> over Y , i.e, q`c \u2208 [0, 1] and\u2211C c=1 q`c = 1. To build a differentiable decision tree, following [Kontschieder et al., 2015], we use a probabilistic split\nfunction sn(x; \u0398) = \u03c3(f\u03d5(n)(x; \u0398)), where \u03c3(\u00b7) is a sigmoid function, \u03d5(\u00b7) is an index function to bring the \u03d5(n)-th output of function f(x; \u0398) in correspondence with split node n, and f : x\u2192 RM is a real-valued feature learning function depending on the sample x and the parameter \u0398, and can be with any form. For a simple form, it can be a linear transformation of x, where \u0398 is the transformation matrix; For a complex form, it can be a deep network, then \u0398 is the network parameter. The correspondence between the split nodes and the output units of function f , indicated by \u03d5(\u00b7), is arbitrary, and is pre-defined. An example to demonstrate \u03d5(\u00b7) is shown in Fig. 2. Then, the probability of the sample x falling into leaf node ` is given by\np`(x|\u0398) = \u220f n\u2208N sn(x; \u0398) 1(`\u2208Lln)(1\u2212 sn(x; \u0398))1(`\u2208L r n),\n(1) where 1(\u00b7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and right subtrees of node n, T ln and T rn , respectively. The output of the tree T w.r.t. x, i.e., the mapping function g, is defined by\ng(x; \u0398, T ) = \u2211 `\u2208L p`(x; \u0398)q`. (2)"}, {"heading": "3.2 Tree Optimization", "text": "Given a training set S = {(xi,di)}Ni=1, our goal is to learn a decision tree T described in Sec. 3.1 which can output a distribution g(xi; \u0398, T ) similar to di for each sample xi. To this end, a straightforward way is to minimize the KullbackLeibler (K-L) divergence between each g(xi; \u0398, T ) and di, or equivalently to minimize the following cross-entropy loss:\nR(q,\u0398;S) = \u2212 1 N N\u2211 i=1 C\u2211 c=1 dycxi log(gc(xi; \u0398, T ))\n= \u2212 1 N N\u2211 i=1 C\u2211 c=1 dycxi log (\u2211 `\u2208L p`(x; \u0398)q`c ) , (3)\nwhere q denote the distributions held by all the leaf nodes L and gc(xi; \u0398, T ) is the c-th output unit of g(xi; \u0398, T ). Learning the tree T requires the estimation of two parameters: 1) the split node parameter \u0398 and 2) the distributions q holden by the leaf nodes. The best parameters (\u0398\u2217,q\u2217) is determined by\n(\u0398\u2217,q\u2217) = arg min \u0398,q R(q,\u0398;S). (4)\nTo solve Eqn. 4, we consider an alternative optimization strategy: First, we fix q and optimize \u0398; Then, we fix \u0398 and optimize q. These two learning steps are alternatively performed, until convergence or a maximum number of iterations is reached.\nLearning Split Nodes In this section, we describe how to learn the parameter \u0398 for split nodes, when the distributions held by the leaf nodes q are fixed. We compute the gradient of the loss R(q,\u0398;S)\nw.r.t. \u0398 by the chain rule:\n\u2202R(q,\u0398;S) \u2202\u0398 = N\u2211 i=1 \u2211 n\u2208N \u2202R(q,\u0398;S) \u2202f\u03d5(n)(xi; \u0398) \u2202f\u03d5(n)(xi; \u0398) \u2202\u0398 ,\n(5) where only the first term depends on the tree and the second term depends on the specific type of the function f\u03d5(n). The first term is given by\n\u2202R(q,\u0398;S) \u2202f\u03d5(n)(xi; \u0398) = 1 N C\u2211 c=1 dycxi ( sn(xi; \u0398) gc(xi; \u0398, T rn ) gc(xi; \u0398, T ) (6)\n\u2212 ( 1\u2212 sn(xi; \u0398) )gc(xi; \u0398, T ln) gc(xi; \u0398, T ) ) ,\nwhere gc(xi; \u0398, T ln) = \u2211\n`\u2208Lln p`(x; \u0398)q`c and gc(xi; \u0398, T rn ) = \u2211\n`\u2208Lrn p`(x; \u0398)q`c . Note that, let\nTn be the tree rooted at the node n, then we have gc(xi; \u0398, Tn) = gc(xi; \u0398, T ln) + gc(xi; \u0398, T rn ). This means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a bottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\nLearning Leaf Nodes Now, fixing the parameter \u0398, we show how to learn the distributions held by the leaf nodes q, which is a constrained optimization problem:\nmin q R(q,\u0398;S), s.t.,\u2200`, C\u2211 c=1 q`c = 1. (7)\nUnfortunately, this optimization problem can not be solved by gradient descend, as the Lagrangian of R(q,\u0398;S) does not have a close-formed gradient w.r.t. q. Here, we propose to address this optimization problem by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003]. In variational bounding, an original objective function to be minimized gets replaced by its bound in an iterative manner. A upper bound for the loss function R(q,\u0398;S) can be obtained by Jensen\u2019s inequality:\nR(q,\u0398;S) = \u2212 1 N N\u2211 i=1 C\u2211 c=1 dycxi log (\u2211 `\u2208L p`(x; \u0398)q`c )\n\u2264 \u2212 1 N N\u2211 i=1 C\u2211 c=1 dycxi \u2211 `\u2208L \u03be`(q\u0304`c ,xi) log (p`(x; \u0398)q`c \u03be`(q\u0304`c ,xi) ) , (8)\nwhere \u03be`(q`c ,xi) = p`(xi;\u0398)q`c gc(xi;\u0398,T ) . We define\n\u03c6(q, q\u0304) = \u2212 1 N N\u2211 i=1 C\u2211 c=1 dycxi \u2211 `\u2208L \u03be`(q\u0304`c ,xi) log (p`(x; \u0398)q`c \u03be`(q\u0304`c ,xi) ) . (9) Then \u03c6(q, q\u0304) is a upper bound for R(q,\u0398;S), which has the property that for any q and q\u0304, \u03c6(q, q\u0304) \u2265 R(q,\u0398;S), and \u03c6(q,q) = R(q,\u0398;S). Assume that we are at a point q(t) corresponding to the t-th iteration, then \u03c6(q,q(t)) is an upper bound for R(q,\u0398;S). In the next iteration, q(t+1) is\nchosen such that \u03c6(q(t+1),q) \u2264 R(q(t),\u0398;S), which implies R(q(t+1),\u0398;S) \u2264 R(q(t),\u0398;S). Consequently, we can minimize \u03c6(q, q\u0304) instead of R(q,\u0398;S) after ensuring that R(q(t),\u0398;S) = \u03c6(q(t), q\u0304), i.e., q\u0304 = q(t). So we have\nq(t+1) = arg min q \u03c6(q,q(t)), s.t.,\u2200`, C\u2211 c=1 q`c = 1, (10)\nwhich leads to minimizing the Lagrangian defined by\n\u03d5(q,q(t)) = \u03c6(q,q(t)) + \u2211 `\u2208L \u03bb`( C\u2211 c=1 q`c \u2212 1), (11)\nwhere \u03bb` is the Lagrange multiplier. By setting \u2202\u03d5(q,q(t))\n\u2202q`c =\n0, we have\n\u03bb` = 1\nN N\u2211 i=1 C\u2211 c=1 dycxi\u03be`(q (t) `c ,xi), (12)\nand\nq (t+1) `c =\n\u2211N i=1 d yc xi\u03be`(q\n(t) `c ,xi)\u2211C\nc=1 \u2211N i=1 d yc xi\u03be`(q (t) `c ,xi) . (13)\nNote that, q(t+1)`c satisfies that q (t+1) `c \u2208 [0, 1] and\u2211C c=1 q (t+1) `c = 1. Eqn. 13 is the update scheme for distributions held by the leaf nodes. The starting point q(0)` can be simply initialized by the uniform distribution: q(0)`c = 1 C ."}, {"heading": "3.3 Learning Forests", "text": "Forests are an ensemble of decision trees F = {T1, . . . , TK}. In the training stage of our LDLFs, all trees share same parameters in \u0398, but each tree has independent leaf node predictions q. The loss function for the forests is given by averaging the loss functions for all individual trees:\nRF = 1\nK K\u2211 k=1 RTk , (14)\nwhere RTk is the loss function for tree Tk defined by Eqn. 3. To learn \u0398 by fixing the leaf node predictions q of all the trees in the forests, based on the derivation in Sec. 3.2 and referring to Fig. 2, we have\n\u2202RF \u2202\u0398 = 1 K N\u2211 i=1 K\u2211 k=1 \u2211 n\u2208Nk \u2202RTk \u2202f\u03d5k(n)(xi; \u0398) \u2202f\u03d5k(n)(xi; \u0398) \u2202\u0398 ,\n(15) whereNk and \u03d5k(\u00b7) are the split node set and the index function of Tk, respectively. Note that, the index function \u03d5k(\u00b7) assigned to each tree is arbitrary, and split nodes can correspond to a subset of output units of f . This strategy is somewhat similar to the random subspace method [Ho, 1998], which increases the randomness in training to reduce the risk of overfitting.\nAs for q, since each tree in forests F has its own leaf node predictions q, we can update them independently by Eqn. 13, given by \u0398. For implementational convenience, we do not\nconduct this update scheme on the whole dataset S but on a set of mini-batches. The training procedure of LDLFs is shown in Fig. 3.\nIn the testing stage, the output of the forests is given by averaging the predictions from all the individual trees:\ng(x; \u0398,F) = 1 K K\u2211 k=1 g(x; \u0398, Tk). (16)"}, {"heading": "4 Experimental Results", "text": "Our realization of LDLFs is based on \u201cCaffe\u201d [Jia et al., 2014], which is modular and implemented as a standard neural network layer. We can either use it as a shallow standalone model (sLDLFs) or integrate it with any deep networks (dLDLFs). We evaluate both sLDLFs and dLDLFs on different LDL tasks and compare them with other LDL methods. The default settings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of the feature learning function (64), iteration times to update leaf node predictions (20)."}, {"heading": "4.1 Crowd Opinion Prediction on Movies", "text": "To perform this task, we use a dataset [Geng and Hou, 2015], which has 7755 movies and 54,242,292 ratings from 478,656 different users. The ratings come from Netflix, which are on a scale from 1 to 5 integral stars. Each movie has, on average, 6,994 ratings. The rating distribution is calculated for each movie as an indicator for the crowd opinion on that movie. The features of each movie are numeric and categorical attributes crawled from IMDb.\nWe evaluate our shallow model sLDLFs on this dataset and compare it with other state-of-the-art stand-alone LDL methods, including AOSO-LDLogitBoost [Xing et al., 2016], LDLogitBoost [Xing et al., 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al., 2013]. For sLDLFs, the feature learning function f(x,\u0398) is a linear transformation of x, i.e., the i-th output unit fi(x,\u03b8i) = \u03b8 > i x, where \u03b8i is the i-th column of the transformation matrix \u0398. Following [Geng and Hou, 2015; Xing et al., 2016], we use 6 measures to evaluate the performances of LDL methods, which compute the average similarity/distance between the predicted rating distributions and the real rating distributions, including 4 distance measures (K-L,\nEuclidean, S\u03c6rensen, Squared \u03c72) and two similarity measures (Fidelity, Intersection). Following [Xing et al., 2016], we also do ten-fold cross validation, which represents the result by \u201cmean\u00b1standard deviation\u201d. The results of sLDLFs and the competitors are summarized in Table 1, where the results of these competitors are quoted from [Xing et al., 2016]. As can be seen from Table 1, sLDLFs perform best on all of the six measures."}, {"heading": "4.2 Facial Age Estimation", "text": "We conduct facial age estimation experiments on Morph [Ricanek and Tesafaye, 2006], which contains more than 50,000 facial images from about 13,000 people of different races. Since Morph provides raw face images, we evaluate our deep model dLDLFs on it, which can be benefited from end-to-end learning. Each facial image is annotated with a chronological age. As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution. To generate an age distribution for each face image, we follow the same strategy used in [Geng et al., 2010; Yang et al., 2016], which uses a Gaussian distribution whose mean is the chronological age of the face image. As far as we know, the state-of-the-art LDL method on Morph Sub is D2LDL [He et al., 2017], which proposed a data-dependent method to learn label distributions. As they used the output of the \u201cfc7\u201d layer in AlexNet [Krizhevsky et al., 2012] as the face image features, we also build our dLDLFs on AlexNet, by replacing the softmax layer in AlexNet by our LDLFs. The predicted age for an instance given by dLDLFs is simply the age having the highest probability in the predicted label distribution. The default values for the hyper-parameters used to train the network parameters of dLDLFs are: base learning rate (1e-1), weight decay (5e-4), momentum (0.9), minibatch size (64), maximum iteration (25000) and the number of mini-batches to update leaf node predictions (100).\nBesides D2LDL, other competitors are AAS [Lanitis et al., 2004], LARR [Guo et al., 2008], IIS-LLD [Geng et al., 2013], IIS-ALDL [Geng et al., 2014], including both SLL and LDL based methods. All of them are trained on the same deep features used by D2LDL. To emphasize the important role of LDLFs played in age estimation, we also compare with a deep label distribution learning method based on maximum entropy model (dLDLME), where the \u201cfc7\u201d layer of AlexNet is connected to a softmax layer (maximum entropy model) to generate age distributions and Kullback-Leibler divergence is also used as the loss. This configuration was also adopted in [Huo et al., 2016] to finetune different deep networks for age estimation. Following the experiment setting used in D2LDL, we evaluate dLDLFs and the competitors under six different training set ratios (10% to 60%) and use mean absolute error (MAE) to measure the difference between the predicted ages and the ground truth ages. We quote the results of the competitors (except for dLDLME) from [He et al., 2017], and compare our results with them in Table. 2. We use the same hyper-parameters to train dLDLME, except\nthat the base learning rate is adjusted to 1e-3 to guarantee that dLDLME can converge. As can be seen from Table 2, dLDLFs outperform others for all training set ratios.\nOur LDLFs can be integrated with any deep network. We also integrate it with another network, called OR-CNN, proposed in [Niu et al., 2016], which obtains the state-of-the-art result on the whole Morph dataset. We replace the final layer (an ordinal regression layer) in OR-CNN by LDLFs. The maximum entropy model based configuration (dLDLME) is verified on this network as well. Following [Niu et al., 2016], all these methods are trained on 80% of the whole Morph dataset and tested on the rest. The results shown in Table. 3 not only demonstrate that lDL is effective for age estimation (LDLFs outperforms a non-LDL method, OR-CNN), but also indicate that LDLFs have good compatibility for different networks (The performance of dLDLME even drops by using the network of OR-CNN)."}, {"heading": "4.3 Parameter Discussion", "text": "Now we discuss the influence of parameter settings on performance. We report the results of crowd opinion prediction on movies (measured by K-L) and age estimation on Morph Sub with 60% training set ratio (measured by MAE) for different parameter settings in this section.\nTree number. As forests are an ensemble model, it is necessary to investigate how performances change by varying the tree number used in forests. Note that, as we discussed in Sec. 2, the ensemble strategy to learn forests proposed in dNDFs [Kontschieder et al., 2015] is different from ours. It is also needed to do a diagnosis to see which ensemble strategy is better to learn forests. We replace our ensemble strategy\nin dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The corresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and output unit number of the feature learning function, as the default setting. As shown in Fig. 4, our ensemble strategy can improve the performance by using more trees, while the one used in dNDFs even leads to a worse performance than one single tree\u2019s.\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an implicit constraint between tree depth h and output unit number of the feature learning function \u03c4 : \u03c4 \u2265 2h\u22121 \u2212 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \u03c4 = 2h\u22121 and fix tree number K = 1. As shown in Fig. 5, to get a good performance, it\u2019s better to set tree depth h > 3."}, {"heading": "5 Conclusion", "text": "We present label distribution learning forests, a novel label distribution learning algorithm inspired by differentiable decision trees. We defined a distribution-based loss function for the forests and found that the leaf node predictions can be optimized via variational bounding, which enables all the trees and the feature they use to be learned jointly in an end-to-end manner. Experimental results showed the superiority of our algorithm for several LDL problems, including age estimation and crowd opinion prediction on movies."}], "references": [{"title": "Neural Computation", "author": ["Yali Amit", "Donald Geman. Shape quantization", "recognition with randomized trees"], "venue": "9(7):1545\u20131588,", "citeRegEx": "Amit and Geman. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Computational Linguistics", "author": ["Adam L. Berger", "Stephen Della Pietra", "Vincent J. Della Pietra. A maximum entropy approach to natural language processing"], "venue": "22(1):39\u201371,", "citeRegEx": "Berger et al.. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Machine Learning", "author": ["Leo Breiman. Random forests"], "venue": "45(1):5\u201332,", "citeRegEx": "Breiman. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["Antonio Criminisi", "Jamie Shotton"], "venue": "Springer,", "citeRegEx": "Criminisi and Shotton. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IJCAI", "author": ["Xin Geng", "Peng Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In Pro"], "venue": "pages 3511\u20133517,", "citeRegEx": "Geng and Hou. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CVPR", "author": ["Xin Geng", "Yu Xia. Head pose estimation based on multivariate label distribution. In Proc"], "venue": "pages 1837\u20131842,", "citeRegEx": "Geng and Xia. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Xin Geng", "Kate Smith-Miles", "ZhiHua Zhou. Facial age estimation by learning from label distributions"], "venue": "AAAI,", "citeRegEx": "Geng et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mach", "author": ["Xin Geng", "Chao Yin", "Zhi-Hua Zhou. Facial age estimation by learning from label distributions. IEEE Trans. Pattern Anal"], "venue": "Intell., 35(10):2401\u2013 2412,", "citeRegEx": "Geng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "ICPR", "author": ["Xin Geng", "Qin Wang", "Yu Xia. Facial age estimation by adaptive label distribution learning. In Proc"], "venue": "pages 4465\u20134470,", "citeRegEx": "Geng et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowl", "author": ["Xin Geng. Label distribution learning. IEEE Trans"], "venue": "Data Eng., 28(7):1734\u20131748,", "citeRegEx": "Geng. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Human age estimation: What is the influence across race and gender? In CVPR Workshops", "author": ["Guodong Guo", "Guowang Mu"], "venue": "pages 71\u201378,", "citeRegEx": "Guo and Mu. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "CVPR", "author": ["Guodong Guo", "Chao Zhang. A study on cross-population age estimation. In Proc"], "venue": "pages 4257\u20134263,", "citeRegEx": "Guo and Zhang. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Image Processing", "author": ["Guodong Guo", "Yun Fu", "Charles R. Dyer", "Thomas S. Huang. Image-based human age estimation by manifold learning", "locally adjusted robust regression. IEEE Trans"], "venue": "17(7):1178\u20131188,", "citeRegEx": "Guo et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Trans", "author": ["Zhouzhou He", "Xi Li", "Zhongfei Zhang", "Fei Wu", "Xin Geng", "Yaqing Zhang", "Ming-Hsuan Yang", "Yueting Zhuang. Data-dependent label distribution learning for age estimation"], "venue": "on Image Processing,", "citeRegEx": "He et al.. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "ICDAR", "author": ["Tin Kam Ho. Random decision forests. In Proc"], "venue": "pages 278\u2013282,", "citeRegEx": "Ho. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Mach", "author": ["Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal"], "venue": "Intell., 20(8):832\u2013844,", "citeRegEx": "Ho. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In CVPR Workshops", "author": ["Zeng-Wei Huo", "Xu Yang", "Chao Xing", "Ying Zhou", "Peng Hou", "Jiaqi Lv", "Xin Geng. Deep age distribution learning for apparent age estimation"], "venue": "pages 722\u2013729,", "citeRegEx": "Huo et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine Learning", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul. An introduction to variational methods for graphical models"], "venue": "37(2):183\u2013233,", "citeRegEx": "Jordan et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "ICCV", "author": ["Peter Kontschieder", "Madalina Fiterau", "Antonio Criminisi", "Samuel Rota Bul\u00f2. Deep neural decision forests. In Proc"], "venue": "pages 1467\u20131475,", "citeRegEx": "Kontschieder et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "NIPS", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Proc"], "venue": "pages 1106\u20131114,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "IEEE Trans", "author": ["Andreas Lanitis", "Chrisina Draganova", "Chris Christodoulou. Comparing different classifiers for automatic age estimation"], "venue": "on Cybernetics,, 34(1):621\u2013628,", "citeRegEx": "Lanitis et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "CVPR", "author": ["Zhenxing Niu", "Mo Zhou", "Le Wang", "Xinbo Gao", "Gang Hua. Ordinal regression with multiple output CNN for age estimation. In Proc"], "venue": "pages 4920\u2013 4928,", "citeRegEx": "Niu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "MORPH: A longitudinal image database of normal adult age-progression", "author": ["Karl Ricanek", "Tamirat Tesafaye"], "venue": "Proc. FG, pages 341\u2013345,", "citeRegEx": "Ricanek and Tesafaye. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining, 3(3):1\u201313,", "citeRegEx": "Tsoumakas and Katakis. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "CVPR", "author": ["Chao Xing", "Xin Geng", "Hui Xue. Logistic boosting regression for label distribution learning. In Proc"], "venue": "pages 4489\u20134497,", "citeRegEx": "Xing et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In ICCV Workshops", "author": ["Xu Yang", "Bin-Bin Gao", "Chao Xing", "Zeng-Wei Huo", "Xiu-Shen Wei", "Ying Zhou", "Jianxin Wu", "Xin Geng. Deep label distribution learning for apparent age estimation"], "venue": "pages 344\u2013350,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IJCAI", "author": ["Xu Yang", "Xin Geng", "Deyu Zhou. Sparsity conditional energy label distribution learning for age estimation. In Proc"], "venue": "pages 2259\u20132265,", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Computation", "author": ["Alan L. Yuille", "Anand Rangarajan. The concave-convex procedure"], "venue": "15(4):915\u2013936,", "citeRegEx": "Yuille and Rangarajan. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "MM", "author": ["Ying Zhou", "Hui Xue", "Xin Geng. Emotion distribution recognition from facial expressions. In Proc"], "venue": "pages 1247\u20131250,", "citeRegEx": "Zhou et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Label distribution learning (LDL) [Geng, 2016; Geng et al., 2013] is a learning strategy to deal with problems of label ambiguity.", "startOffset": 34, "endOffset": 65}, {"referenceID": 7, "context": "Label distribution learning (LDL) [Geng, 2016; Geng et al., 2013] is a learning strategy to deal with problems of label ambiguity.", "startOffset": 34, "endOffset": 65}, {"referenceID": 24, "context": "Unlike single-label learning (SLL) and multi-label learning (MLL) [Tsoumakas and Katakis, 2007], which assume an instance is assigned to a single label or multiple labels, LDL aims at learning the relative importance of each label involved in the description of an instance, i.", "startOffset": 66, "endOffset": 95}, {"referenceID": 6, "context": "An example is facial age estimation [Geng et al., 2010].", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": "Another example is predicting crowd opinion on movies [Geng and Hou, 2015].", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": "Many LDL methods assume the label distribution can be represented by a maximum entropy model [Berger et al., 1996] and learn it by optimizing an energy function based on the model [Geng et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 6, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 7, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 27, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 9, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 4, "context": "g, by boosting and support vector regression, to deal with label distributions [Geng and Hou, 2015; Xing et al., 2016], which avoid making this assumption, but have limitations in representation learning, e.", "startOffset": 79, "endOffset": 118}, {"referenceID": 25, "context": "g, by boosting and support vector regression, to deal with label distributions [Geng and Hou, 2015; Xing et al., 2016], which avoid making this assumption, but have limitations in representation learning, e.", "startOffset": 79, "endOffset": 118}, {"referenceID": 19, "context": "In this paper, we present label distribution learning forests (LDLFs) - a novel label distribution learning algorithm inspired by differentiable decision trees [Kontschieder et al., 2015].", "startOffset": 160, "endOffset": 187}, {"referenceID": 18, "context": "By fixing split nodes, we show that the optimization of leaf node predictions to minimize the loss function of the tree can be addressed by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003], in which the original loss function to be minimized gets iteratively replaced by upper bounds.", "startOffset": 161, "endOffset": 211}, {"referenceID": 28, "context": "By fixing split nodes, we show that the optimization of leaf node predictions to minimize the loss function of the tree can be addressed by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003], in which the original loss function to be minimized gets iteratively replaced by upper bounds.", "startOffset": 161, "endOffset": 211}, {"referenceID": 14, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 0, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 2, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 3, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 0, "context": "In the past, learning of a decision tree was based on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each split node [Amit and Geman, 1997], and thus, cannot be integrated into in a deep learning framework, i.", "startOffset": 157, "endOffset": 179}, {"referenceID": 19, "context": "The newly proposed deep neural decision forests (dNDFs) [Kontschieder et al., 2015] overcomes this problem by introducing a soft differentiable decision function at the split nodes and a global loss function defined on a tree.", "startOffset": 56, "endOffset": 83}, {"referenceID": 6, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 7, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 27, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 16, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 4, "context": ", 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 29, "context": ", 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al., 2015] and hand orientation estimation [Geng and Xia, 2014].", "startOffset": 89, "endOffset": 108}, {"referenceID": 5, "context": ", 2015] and hand orientation estimation [Geng and Xia, 2014].", "startOffset": 40, "endOffset": 60}, {"referenceID": 6, "context": "[Geng et al., 2010] defined the label distribution for an instance as a vector containing the probabilities of the instance having each label.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "[Yang et al., 2016] then defined a three-layer energy based model, called SCE-LDL, in which the ability to perform representation learning is improved by adding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Geng [Geng, 2016] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton optimization.", "startOffset": 5, "endOffset": 17}, {"referenceID": 1, "context": "label distribution can be represented by a maximum entropy model [Berger et al., 1996].", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "Geng and Hou [Geng and Hou, 2015] proposed LDSVR, a LDL method by extending support vector regressor, which fit a sigmoid function to each component of the distribution simultaneously by a support vector machine.", "startOffset": 13, "endOffset": 33}, {"referenceID": 25, "context": "[Xing et al., 2016] then extended boosting to address the LDL task by additive weighted regressors.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "There are two deep learning based LDL methods [Yang et al., 2015; Huo et al., 2016], but both of them focused on how to boost the performance by combing multiple deep models trained on different datasets.", "startOffset": 46, "endOffset": 83}, {"referenceID": 16, "context": "There are two deep learning based LDL methods [Yang et al., 2015; Huo et al., 2016], but both of them focused on how to boost the performance by combing multiple deep models trained on different datasets.", "startOffset": 46, "endOffset": 83}, {"referenceID": 19, "context": "To build a differentiable decision tree, following [Kontschieder et al., 2015], we use a probabilistic split function sn(x; \u0398) = \u03c3(f\u03c6(n)(x; \u0398)), where \u03c3(\u00b7) is a sigmoid function, \u03c6(\u00b7) is an index function to bring the \u03c6(n)-th output of function f(x; \u0398) in correspondence with split node n, and f : x\u2192 R is a real-valued feature learning function depending on the sample x and the parameter \u0398, and can be with any form.", "startOffset": 51, "endOffset": 78}, {"referenceID": 18, "context": "Here, we propose to address this optimization problem by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003].", "startOffset": 78, "endOffset": 128}, {"referenceID": 28, "context": "Here, we propose to address this optimization problem by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003].", "startOffset": 78, "endOffset": 128}, {"referenceID": 15, "context": "This strategy is somewhat similar to the random subspace method [Ho, 1998], which increases the randomness in training to reduce the risk of overfitting.", "startOffset": 64, "endOffset": 74}, {"referenceID": 17, "context": "Our realization of LDLFs is based on \u201cCaffe\u201d [Jia et al., 2014], which is modular and implemented as a standard neural network layer.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "To perform this task, we use a dataset [Geng and Hou, 2015], which has 7755 movies and 54,242,292 ratings from 478,656 different users.", "startOffset": 39, "endOffset": 59}, {"referenceID": 25, "context": "We evaluate our shallow model sLDLFs on this dataset and compare it with other state-of-the-art stand-alone LDL methods, including AOSO-LDLogitBoost [Xing et al., 2016], LDLogitBoost [Xing et al.", "startOffset": 149, "endOffset": 168}, {"referenceID": 25, "context": ", 2016], LDLogitBoost [Xing et al., 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 4, "context": ", 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": ", 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al.", "startOffset": 46, "endOffset": 58}, {"referenceID": 7, "context": ", 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al., 2013].", "startOffset": 71, "endOffset": 90}, {"referenceID": 4, "context": "Following [Geng and Hou, 2015; Xing et al., 2016], we use 6 measures to evaluate the performances of LDL methods, which compute the average similarity/distance between the predicted rating distributions and the real rating distributions, including 4 distance measures (K-L, Euclidean, S\u03c6rensen, Squared \u03c7) and two similarity measures (Fidelity, Intersection).", "startOffset": 10, "endOffset": 49}, {"referenceID": 25, "context": "Following [Geng and Hou, 2015; Xing et al., 2016], we use 6 measures to evaluate the performances of LDL methods, which compute the average similarity/distance between the predicted rating distributions and the real rating distributions, including 4 distance measures (K-L, Euclidean, S\u03c6rensen, Squared \u03c7) and two similarity measures (Fidelity, Intersection).", "startOffset": 10, "endOffset": 49}, {"referenceID": 25, "context": "Following [Xing et al., 2016], we also do ten-fold cross validation, which represents the result by \u201cmean\u00b1standard deviation\u201d.", "startOffset": 10, "endOffset": 29}, {"referenceID": 25, "context": "The results of sLDLFs and the competitors are summarized in Table 1, where the results of these competitors are quoted from [Xing et al., 2016].", "startOffset": 124, "endOffset": 143}, {"referenceID": 23, "context": "We conduct facial age estimation experiments on Morph [Ricanek and Tesafaye, 2006], which contains more than 50,000 facial images from about 13,000 people of different races.", "startOffset": 54, "endOffset": 82}, {"referenceID": 10, "context": "As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution.", "startOffset": 101, "endOffset": 157}, {"referenceID": 11, "context": "As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution.", "startOffset": 101, "endOffset": 157}, {"referenceID": 13, "context": "As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution.", "startOffset": 101, "endOffset": 157}, {"referenceID": 6, "context": "To generate an age distribution for each face image, we follow the same strategy used in [Geng et al., 2010; Yang et al., 2016], which uses a Gaussian distribution whose mean is the chronological age of the face image.", "startOffset": 89, "endOffset": 127}, {"referenceID": 27, "context": "To generate an age distribution for each face image, we follow the same strategy used in [Geng et al., 2010; Yang et al., 2016], which uses a Gaussian distribution whose mean is the chronological age of the face image.", "startOffset": 89, "endOffset": 127}, {"referenceID": 13, "context": "As far as we know, the state-of-the-art LDL method on Morph Sub is D2LDL [He et al., 2017], which proposed a data-dependent method to learn label distributions.", "startOffset": 73, "endOffset": 90}, {"referenceID": 20, "context": "As they used the output of the \u201cfc7\u201d layer in AlexNet [Krizhevsky et al., 2012] as the face image features, we also build our dLDLFs on AlexNet, by replacing the softmax layer in AlexNet by our LDLFs.", "startOffset": 54, "endOffset": 79}, {"referenceID": 21, "context": "Besides D2LDL, other competitors are AAS [Lanitis et al., 2004], LARR [Guo et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 12, "context": ", 2004], LARR [Guo et al., 2008], IIS-LLD [Geng et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 7, "context": ", 2008], IIS-LLD [Geng et al., 2013], IIS-ALDL [Geng et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 8, "context": ", 2013], IIS-ALDL [Geng et al., 2014], including both SLL and LDL based methods.", "startOffset": 18, "endOffset": 37}, {"referenceID": 16, "context": "This configuration was also adopted in [Huo et al., 2016] to finetune different deep networks for age estimation.", "startOffset": 39, "endOffset": 57}, {"referenceID": 13, "context": "We quote the results of the competitors (except for dLDLME) from [He et al., 2017], and compare our results with them in Table.", "startOffset": 65, "endOffset": 82}, {"referenceID": 22, "context": "We also integrate it with another network, called OR-CNN, proposed in [Niu et al., 2016], which obtains the state-of-the-art result on the whole Morph dataset.", "startOffset": 70, "endOffset": 88}, {"referenceID": 22, "context": "Following [Niu et al., 2016], all these methods are trained on 80% of the whole Morph dataset and tested on the rest.", "startOffset": 10, "endOffset": 28}, {"referenceID": 19, "context": "2, the ensemble strategy to learn forests proposed in dNDFs [Kontschieder et al., 2015] is different from ours.", "startOffset": 60, "endOffset": 87}], "year": 2017, "abstractText": "Label distribution learning (LDL) is a general learning framework, which assigns a distribution over a set of labels to an instance rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning. This paper presents label distribution learning forests (LDLFs) a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by the mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning, e.g., to learn deep features in an end-to-end manner. We define a distributionbased loss function for forests, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on two LDL problems, including age estimation and crowd opinion prediction on movies, showing significant improvements to the state-of-the-art LDL methods.", "creator": "LaTeX with hyperref package"}}}