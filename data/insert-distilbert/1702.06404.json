{"id": "1702.06404", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Delving Deeper into MOOC Student Dropout Prediction", "abstract": "in order to obtain reliable accuracy estimates created for automatic mooc dropout predictors, it is important to train and visually test them in a manner consistent with how they will consistently be used in practice. other yet surprisingly most prior research on valid mooc dropout prediction has also measured test accuracy on either the same course used for training the verbal classifier, which can lead to overly optimistic accuracy estimates. in order to understand / better how accuracy is affected by the training + testing assessment regime, further we compared the general accuracy of a standard dropout prediction architecture ( clickstream features + logistic regression ) across versus 4 different training paradigms. results suggest that ( 1 ) training and testing on the usually same course ( \" post - hoc \" ) can overestimate accuracy by several percentage points ; ( 2 ) dropout classifiers trained on proxy labels based on students'persistence are surprisingly competitive with post - hoc hypothesis training ( 87. 33 % compliance versus... 90. 20 % auc averaged over 8 weeks of < 40 harvardx moocs ) ; and ( 3 ) effective classifier performance does not vary significantly with the academic discipline. finally, we also continued research new dropout prediction architectures based on deep, fully - connected, feed - forward neural access networks and quickly find that ( 4 ) networks with distances as many as 5 hidden layers can overcome statistically leverage significantly increase test accuracy over that of logistic regression.", "histories": [["v1", "Tue, 21 Feb 2017 14:35:55 GMT  (248kb,D)", "http://arxiv.org/abs/1702.06404v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY", "authors": ["jacob whitehill", "kiran mohan", "daniel seaton", "yigal rosen", "dustin tingley"], "accepted": false, "id": "1702.06404"}, "pdf": {"name": "1702.06404.pdf", "metadata": {"source": "CRF", "title": "Delving Deeper into MOOC Student Dropout Prediction", "authors": ["Jacob Whitehill", "Kiran Mohan", "Daniel Seaton", "Dustin Tingley"], "emails": ["jrwhitehill@wpi.edu", "kmohan@wpi.edu", "seaton@harvard.edu", "rosen@harvard.edu", "dtingley@gov.harvard.edu"], "sections": [{"heading": "INTRODUCTION AND RELATED WORK", "text": "As the number and diversity of massive open online courses (MOOCs) continues to grow, researchers, teachers, and educational technologists are starting to explore innovative ways of helping more students who participate in these courses to persist longer and learn more. While some interventions (e.g., email reminders [22]) may incur very little cost once the core infrastructure for con-\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive. An automatic detector that could predict automatically \u2013 based on demographics or by analyzing subtle cues in a learner\u2019s interaction history with the MOOC courseware \u2013 which learners are in danger of \u201cdropping out\u201d from the MOOC and which will likely succeed, would be a valuable tool for making smart decisions about who receives a particular intervention.\nRelated work: Within the fields of learning analytics and educational data mining, the possibility of creating automatic MOOC dropout detectors has generated considerable interest within the past few years. Existing approaches vary across several dimensions including the features used for classification, the architecture used for training and testing, and the training set used to optimize the classifier parameters \u2013 see Table 1 for a synopsis of prior work. The most popular feature representations include clickstream logs, natural language processing (NLP) of discussion forum content, and social network metrics. In terms of architecture, most prior approaches have used generalized linear models (including logistic regression and linear SVMs), survival analysis (e.g., Cox proportional hazard model), and logistic regression. A third dimension of variability is the training setting that describes the source of the training data \u2013 e.g., the same course, a prior instance of the same course, or a different course altogether \u2013 relative to the target use of the classifier once it has been trained: most research on MOOC dropout prediction to-date has focused on training and testing on data sampled from the same MOOC.\nWhen designing and implementing educational interventions that depend on automatic MOOC dropout predictors to predict which students are in danger of failing, it is important to know how accurate such predictors are. In order to obtain reliable accuracy estimates, it\nar X\niv :1\n70 2.\n06 40\n4v 1\n[ cs\n.A I]\n2 1\nFe b\n20 17\nis crucial to ensure that the method used for evaluating the classifier is consistent with the manner it will be used for the actual intervention. Live MOOC interventions require dropout predictors that are operational at or near the beginning of a MOOC while students can still benefit from receiving an intervention. But producing such a predictor can be difficult because the target values which indicate whether each student dropped out or completed the MOOC, and which are usually required for training a classifier with standard supervised learning approaches, typically become available only at the end of a MOOC \u2013 at which point any intervention is moot.\nTo-date, most prior research on MOOC dropout detection has largely ignored this issue and both trained and tested on student data sampled from the same course (\u201cpost-hoc\u201d), likely because dropout prediction was a new field and the focus was on exploring different feature representations and classification architectures. The study we present in this paper seeks to fill this gap.\nContributions: (1) In this paper we compare the accuracy of a standard dropout prediction architecture \u2013 clickstream features classified by logistic regression \u2013 across a variety of different training settings in order to understand better the tradeoff between accuracy and practical deployability of the classifier. The dataset we use for training and evaluation consists of 40 popular MOOCs from HarvardX that span a variety of academic\ndisciplines including social science, humanities, STEM, and health sciences. As part of this evaluation, (2) we also explore a novel scheme (similar to the in-situ approach proposed by [2]) for training a classifier while a course is ongoing using proxy labels and show that its performance is surprisingly good even compared to the overly optimistic post-hoc training paradigm. Finally, (3) we propose and evaluate a novel MOOC dropout architecture based on fully-connected, feed-forward neural networks that are trained at each week in a MOOC. We provide evidence that much deeper (5 hidden layers) feature representations than have previously been explored can lead to statistically significantly higher prediction accuracy."}, {"heading": "DATASET", "text": "The experiments and analyses in this paper are based on data from 40 MOOCs from HarvardX \u2013 see Table 2 which lists the code name of each course; the year and term (T1, T2, and T3 are northern hemisphere spring, summer, and fall, respectively) when the course was offered; the academic discipline; number of registered participants; and the number of participants who certified. Each MOOC has a launch date (which we call T0%) when the first materials \u2013 e.g., lecture videos, discussion forums, etc. \u2013 are released, as well as an end date when certificates are issued."}, {"heading": "List of MOOCs Course Year Field #Part. #Cert.", "text": ""}, {"heading": "1368.1x 2014T3 SocialSci 2335 387", "text": ""}, {"heading": "Target labels", "text": "The binary target labels used for training and evaluation were whether (1) or not (0) each student accrued enough points during the MOOC to earn a certificate. The grade threshold for certification differed across the MOOCs but is typically around 70%. Note that, starting in late 2015, some HarvardX MOOCs implemented a policy whereby only students who paid a fee to have their identity verified could officially earn a certificate. For these courses, we still considered the target label for a student to be 1 as long as her/his point total exceeded the verification threshold \u2013 in other words, we ignored the fact of whether or not the student paid money to become ID-verified."}, {"heading": "Features", "text": "The features that were used for MOOC dropout prediction are clickstream features that are computed from the clickstream log that contains all interaction events between every student and the MOOC courseware; this includes answers to quiz questions, play/pause/rewind events on lecture videos, reading and writing to the discussion form (the events, not the actual text), and more. The features we chose as the basis for automatic MOOC dropout detection are similar to prior approaches (e.g., [2]) and can generalize to a wide variety of MOOCs across academic disciplines; they are listed in Table 3."}, {"heading": "Feature extraction", "text": "All features were extracted from two different database tables \u2013 person course and person course day \u2013 that are automatically updated daily by the edx2bigquery data management framework [4] that is used for MITx and HarvardX MOOCs. The person course features\nconsist of the student\u2019s self-reported highest level of education (LoE), year of birth (YoB), gender, and continent (Africa, Europe, etc.). Each of these variables was converted into a vector of binary dummy variables:\n\u2022 Age: dummy variables based on the students approximate age (computed as 2012\u2212YoB) in the following ranges: < 10, 10\u221215, 15\u221220, 20\u221225, 25\u221230, 30\u221235, 35\u2212 40, 40\u2212 45, 45\u2212 50, 50\u2212 55, 55\u2212 60, > 60 years, and null (no response).\n\u2022 LoE: elementary school, junior high school, high school, associate\u2019s degree, bachelor\u2019s degree, master\u2019s degree, professional degree, and null (no response).\n\u2022 Gender: male, female, other, and null (no response).\n\u2022 Continent: Europe, Oceania, Africa, Asia, Americas, North America, South America, and null (no response).\nThe person course day dataset was used to compute clickstream features such as the average and total amount of time spent in the course on a particular day (avg dt, sum dt respectively), total number of clickstream events (nevents), etc. Although person course day contains separate information for each student for each day of the course, we chose to aggregate information across time: Specifically, when making a dropout prediction for a specific student at time t, we summed the values within each feature across all days t\u2032 \u2264 t. This approach yields a feature representation that is both smaller and whose dimensionality does not change as a function of time.1\nFinally, we extracted two additional features: precourse survey, which indicates whether or not the student responded to the course\u2019s pre-course survey (which might be positively correlated with commitment to the MOOC), and the number of days since the student interacted with the MOOC courseware (which can be computed from data within person course day). In total (including all dummy variables and both demographic and tracking log features), there were 73 features used for prediction."}, {"heading": "TRAINING PARADIGMS", "text": "We compared several different paradigms for training automatic MOOC dropout predictors:\n1. Train on same course (post-hoc): When predicting which students from course c will drop out, train using features and target labels from the exact same course c. Note that, since target labels for c become available only after c has ended, this approach essentially would require either that (a) the practitioner go \u201cback in time\u201d to when the MOOC first started, or (b) that a new MOOC with the exact same distribution of students (demographics, prior knowledge, etc.) and with\n1Note that, in pilot experimentation we also tried using separate person course day features for each day, but the accuracy was lower.\nthe exact same content and structure is offered in the future, and that no exogenous factors (e.g., comedian Stephen Colbert talking on television about MOOCs [11]) cause students to behave differently during the later incarnation of the course. These assumptions are unlikely to hold. This approach is, however, a useful benchmark to compare against other training paradigms.\n2. Train on other course from same field : When predicting which students from course c will drop out, train using features and target labels from a different course c\u2032 that has already completed, and for which the target labels are thus already available. Although it is difficult to know which prior course should be used for training, a reasonable choice is a different course from within the same discipline (social sciences, humanities, etc.). We chose to use the largest such course in order to maximize the size of the dataset available for training.\n3. Train on many other courses: When predicting which students from course c will drop out, train using features and target labels from many different courses (not necessarily within the same discipline). Specifically, for each course c, we trained a dropout classifier from each of the 39 other courses (recall that we tested 40 MOOCs in total) and then averaged the classifiers\u2019 hyperplanes together.\nIn addition, we also explore a novel training approach, similar to the in-situ training method proposed by [2], based on using proxy labels:\n4. Train using proxy labels (in-situ): When predicting at week w which students from course c will drop out, train using proxy labels corresponding to whether each student persisted \u2013 i.e., interacted with the MOOC courseware at least once \u2013 within the previous week w \u2212 1 (see Figure 1). This approach can be implemented for any MOOC, and because it does not require \u201cseeing into the future\u201d to obtain target labels, it can deliver a dropout predictor that can be deployed during a live MOOC, not just after it has finished. There is, however, an inherent mismatch between the training target labels (based on persistence) and the\ntesting target labels (based on certification/dropout), and this mismatch may degrade performance."}, {"heading": "CLASSIFICATION ARCHITECTURE", "text": "The classification architecture for the detectors was logistic regression with L2 regularization, which is equivalent to a 2-layer neural network. When estimating parameters of the logistic regression models, we varied the strength of the regularization parameter over the set C \u2208 {10\u22124, 10\u22123, 10\u22122, 10\u22121, 100, 101, 102}. Prediction accuracy on the test set did not substantially change (relative to the package-default value of C = 1) over this set, and we thus report all experimental results for C = 1. We used the open-source sklearn and numpy packages for training."}, {"heading": "Baseline approaches", "text": "In order to gauge how much \u201cadded value\u201d is brought by machine learning approaches to MOOC dropout prediction that utilize detailed clickstream information, we also compared the architectures described above to two simple baseline heuristics. In particular, Baseline 1 uses only demographic information \u2013 consisting of selfreported year of birth, continent of origin (Africa, North America, etc.), level of education (primary/elementary school, high/secondary school, college, etc.), and gender \u2013 to make predictions. These information are available for each student as soon as she/he registers for the course; hence, no clickstream data is required. As with the other approaches, logistic ridge regression was used for classifier training; we trained a separate logistic regression classifier for each course.\nWe also compared against an even simpler Baseline 2 which requires no machine learning at all; rather, the predictor makes predictions based on the number of days since the student last interacted with the courseware. In prior research on dropout detection [22, 14], this variable alone has shown to be highly predictive of dropout."}, {"heading": "Normalization", "text": "For Train on same course (post-hoc), Train on other course from same field, and Train on many other courses, both the training and testing sets were normalized by subtracting each feature value by the feature-wise mean and dividing by the feature-wise standard deviation, where the mean and standard deviation were calculated over only the training data. This normalization is important to ensure that the L2 regularization affects all features similarly. For Train using proxy labels (in-situ), we used a different type of normalization to ensure that feature values are comparable even when they are extracted across time windows of different lengths (see Figure 1). Specifically, we converted all clickstream features from absolute counts to percentiles."}, {"heading": "Accuracy metric", "text": "To measure accuracy of the dropout classifiers, we use the Area Under the receiver operating characteristics\nCurve (AUC) metric. The Receiver Operating Characteristics (ROC) curve itself plots the true positive rate versus the false positive rate of the trained classifier. The AUC is the integral of the ROC curve over the interval [0, 1] and answers the following question: Given one randomly chosen student who drops out from the MOOC and one randomly chosen student who certifies in the MOOC, what is the probability that the classifier can correctly distinguish the two students? A useless classifier that simply flips a coin will have an AUC of 0.5, whereas a perfect classifier will have an AUC of 1. (A classifier with AUC of 0 always makes the wrong discrimination.) Importantly, dropout accuracy as assessed by the AUC statistic is not affected by the overall proportion of students within a particular course who drop out (which is typically very high in MOOCs)."}, {"heading": "When to Measure Accuracy", "text": "In order to obtain reliable estimates of the accuracy of a MOOC dropout detector, it is important to consider over what time period the accuracy is computed. For the goal of automated intervention, it is more useful to be able to predict early in the course, rather than later, which students will eventually drop out. Another point to consider is that, near the end of the MOOC, some students may have already accrued enough points to earn a certificate. A dropout detector that predicts that such students will not drop out is not so much \u201cpredicting\u201d these students\u2019 future performance as it is reporting what they have already achieved; hence, accuracy statistics computed over such students may overestimate the performance of the predictor. For both these reasons, we decided to compute accuracy over all weeks of each MOOC between the course launch date (T0%, when instruction begins) and the earliest date by which students could possibly have earned enough points to earn a certificate (T100%).\nNote that the earliest time at which predictions can be made differs across the training paradigms. Specifically, the Train on same course (post-hoc) approach can make predictions after just the first week of the course. In contrast, Train using proxy labels (in-situ) needs at least 2 weeks of clickstream data before the first set of proxy labels become available. Simple baselines based on students\u2019 demographics that require no clickstream data can be applied as soon as students register, which is a notable advantage compared to the machine learning-based approaches."}, {"heading": "RESULTS", "text": "Dropout prediction accuracy (AUC) for all training approaches are shown in Figure 2. The horizontal axis indexes the week in the MOOC, where week 0 is defined for each course to be T100%. (Week \u22123 corresponds to 3 weeks prior to T100%, etc.) Each data point is shown with error bars corresponding to the standard error of the mean. Since the lengths of the courses varied, the accuracy statistics for each week w were computed using only those MOOCs for which data were available at week w.\nAs shown in the graph, the most accurate prediction paradigm was Train on same course (post-hoc), which is the predominant training paradigm used in most prior dropout prediction research. It achieved an accuracy (averaged over all 8 weeks, and all MOOCs within each week) of 90.20%. Perhaps more surprising is that the second most accurate approach was Train using proxy labels (in-situ). This approach does not require any MOOC \u2013 similar or dissimilar \u2013 to have been offered previously. Despite the inherent mismatch between the labels of persistence (did the student participate during the previous week?) and labels of certification (will the student earn enough points to earn a certificate?), this approach attained an accuracy (averaged over all tested MOOCs and all 8 weeks) of 87.33%.\nThe third most accurate approach was Train on many other courses, with an accuracy of 85.56%. This training paradigm attained a higher accuracy than did Train on other course from same field (76.85%), suggesting that, if it is not possible to exploit course-specific structure via either Train on same course (post-hoc) or Train using proxy labels (in-situ), then it is better to harness prior data from a large variety of courses than from just a single course (even from within the same discipline)."}, {"heading": "COMPARISON TO BASELINE HEURISTICS", "text": "Baseline 1, whose predictions are based solely on each student\u2019s self-reported demographics, achieved an average prediction accuracy of 58.85%, suggesting that only a small amount of information about dropout is contained in the demographics. (Note that we also tried a demographics-based classifier whose parameters were\naveraged over many courses, rather than just for the one course itself; this approach\u2019s accuracy was even lower.)\nBaseline 2, whose predictions are solely on the number of days since the student\u2019s last interaction with the course, performed remarkably well: It attained an average dropout prediction accuracy of 82.45%, which corroborates previous findings [22, 14] that this variable is highly salient for prediction. Nonetheless, Baseline 2 was still substantially less accurate than either Train on same course (post-hoc) or Train using proxy labels (in-situ), suggesting that harnessing more detailed clickstream features does bring a substantial accuracy boost."}, {"heading": "ACCURACY ACROSS ACADEMIC FIELDS", "text": "To explore whether dropout prediction accuracy tended to be higher for MOOCs within a particular academic field \u2013 STEM, Humanities, Social Sciences, or Health Sciences \u2013 we implemented a linear mixed-effects model in which dropout prediction accuracy for the Train on same course (post-hoc) paradigm was estimated using the week w and field f as fixed effects and the course as a random effect.\nResults: The week w was statistically significantly (\u03c72(1) = 184.77, p < 0.0001) correlated with dropout prediction accuracy: for each week later in the course (i.e., closer to T100%), there was an estimated 1.53% increase in prediction accuracy. The difference in average accuracy varied only slightly as a function of the field: the prediction accuracy tended to be slightly lower for STEM courses and slightly higher for humanities courses. However, the difference between the intercepts for these fields was only 2.19%\u2212(\u2212.16%) = 2.35%; it was not statistically significant (\u03c72(3) = 1.9825, p = 0.576). This suggests that the features listed in Table 3 that we extract for dropout prediction are robust to changes in the MOOC subject matter."}, {"heading": "DEEPER PREDICTION ARCHITECTURES", "text": "Most of the prior research on automatic MOOC dropout prediction has used generalized linear models \u2013 including logistic regression, support vector machines, and survival/hazard models \u2013 to classify clickstream, natural language, and/or social network features (see Table 1). Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features. In this section, we explore the potential benefits of employing a deep, fully-connected, feedforward neural network for dropout prediction. Specifically, we performed an experiment in which we systematically vary both the depth (number of hidden layers), as well as the width (number of neurons per hidden layer), of a neural network, whose 73 input features are the same as in Section 2.2. Between the input and the first hidden layer, and between each pair of consecutive hidden layers, are rectified linear units (ReLUs). The final, output layer is a softmax layer. The course we used for experimentation was the MOOC with the greatest number of\ncertifiers \u2013 GSE2X (see Table 2), and we trained neural networks to predict dropout during the last week of the course prior to T100%.\nTo make experimentation more efficient, we utilized a recently developed methodology for neural network training called Net2Net [3]. Net2Net is useful when training a sequence of incrementally more complex networks. In particular, with Net2Net there exists a teacher network from which a new student network is built. The student network is deeper or wider than the teacher network, but the weights of the student network are initialized prior to training either by (a) replicating neurons within a hidden layer and adjusting the weights to/from those neurons to compensate (Net2Wider); or (b) initializing the weights of the neurons in an additional hidden layer to implement the identity function (Net2Deeper). In this manner, the student network is guaranteed to produce identical outputs as the teacher network at the start of the training and can become even more accurate during training.\nUsing Net2Net, we varied the number of hidden layers h as well as the number of neurons per hidden layer w \u2013 see Figure 3. We began with h = 1, w = 2, i.e., a 3-layer neural network with 2 neurons in the hidden layer. We then applied the Net2Wider technique using this network as the teacher and built a student network with w = 3 neurons in the hidden layer. Next, the w = 3 neuron network was used as the teacher, and a network with w = 4 neurons was constructed. This procedure was carried out iteratively until w = 15 was reached, such that a w-neuron student was trained from an w \u2212 1-neuron teacher, where w \u2208 {3, 4, . . . , 15}. The prediction accuracy at each value of w was measured on test data; while the highest accuracy was attained for w = 6, accuracy at w = 5 was very similar (and requires fewer parameters). After maximizing accuracy on test data for h = 1 with respect to w, we fixed w\u2217 = 5 and then applied the Net2Deeper approach to maximize accuracy with respect to h. Specifically, we iteratively trained a student net-\nwork with h hidden layers from a teacher hetwork with h\u2212 1 hidden layers, where h \u2208 {2, 3, . . . , 10}. Each network was trained using stochastic gradient descent with no momentum, a learning rate of 0.1, over 20 epochs. The minibatch size was 10, and the learning rate was annealed by a factor of 1/(1 + 10\u22123) after training on each mini-batch. To account for class imbalance, we tried re-weighting the loss of the two classes; however, this technique did not increase test accuracy and was thus dropped. We used the keras software package for training."}, {"heading": "Results", "text": "Results of maximizing both respect to w and (separately, after fixing w\u2217 = 5) h are shown in Figure 4. For a network with h = 1 hidden layer, the accuracy at w = 5 was 97.43%. When keeping the width fixed at w\u2217, the highest accuracy \u2013 97.55% \u2013 was achieved for h = 5. For comparison, logistic regression \u2013 which is equivalent to a neural network with h = 0 hidden layers \u2013 achieved an accuracy of 97.20%. The difference between these accuracies \u2013 97.55 \u2212 97.20 = 0.35% \u2013 was statistically significant (t(23738) = 78.47, p < 0.00001, two-tailed). For comparison, the difference in accuracies between the first-place and second-place contestants in the 2015 KDD Cup contest on MOOC dropout prediction was 90.92 \u2212 90.89 = 0.03% [13].\nTraining cost: On GSE2x, whose training set contains 11870 examples, and using the Net2Net training technique, it took 80 seconds (an average of 4 seconds per epoch) for the best network (w = 5, h = 5) to train on an NVIDIA Tesla K20Xm GPU."}, {"heading": "SUMMARY AND CONCLUSIONS", "text": "In this paper we have investigated the practical level of accuracy that can be achieved with an automatic MOOC dropout predictor, given the fact that detectors must be trained before they can be deployed, using training data that are collected before their first deployment. In particular, we have compared several different training paradigms that are widely used in the MOOC dropout prediction literature \u2013 e.g., train on the same course (post-hoc), train on different course from same academic discipline, and train on many different courses \u2013 on 40 different MOOCs that span a variety of disciplines including the humanities, social sciences, health sciences, and STEM. Results suggest that the accuracy of classifiers that are trained on data that are collected only after a course has finished \u2013 and which are thus unusable in the MOOC itself \u2013 are often several percentage points higher than classifiers that are trained on other MOOCs. This underlines the importance of careful accuracy estimation before conducting a large-scale intervention.\nIn addition, we have explored a training approach, similar to in-situ training [2], based on the idea of proxy labels \u2013 labels that approximate the quantity of interest (dropout versus certification) but that can be collected\nbefore a course has completed. Surprisingly, the accuracy of this approach is very similar to when classifier are trained post-hoc from data of courses that have already finished. Moreover, this approach enables a MOOC dropout classifier both to be trained and to be deployed while a course is ongoing.\nFinally, we have presented evidence that deeper feature representations of basic demographic and clickstream features \u2013 as implemented with a deep (7 layers including 5 hidden layers with 5 neurons per hidden layer), fullyconnected, feed-forward neural network can statistically significantly increase the accuracy of MOOC dropout prediction relative to generalized linear models that are the standard today. Using an iterative training strategy [3], the total training time to create such a network is modest and amenable to large-scale implementation over many (hundreds) of MOOCs at regular training intervals (e.g., once per week).\nFuture work on dropout detection should focus on moving from prediction to intervention. In particular, more research should be invested in identifying not only those students who will drop out, but specifically those who are likely to benefit most from interventions."}], "references": [{"title": "Predicting student retention in massive open online courses using hidden markov models", "author": ["G. Balakrishnan", "D. Coetzee"], "venue": "Technical report, UC Berkeley,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning for predictive models in massive open online courses", "author": ["S. Boyer", "K. Veeramachaneni"], "venue": "In International Conference on Artificial Intelligence in Education, pages 54\u201363. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["T. Chen", "I.J. Goodfellow", "J. Shlens"], "venue": "CoRR, abs/1511.05641,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "MITx/HarvardX edx2bigquery", "author": ["I. Chuang", "G. Lopez"], "venue": "https://github.com/mitodl/edx2bigquery,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic use cases: Discovering behavioral patterns for predicting certification", "author": ["C. Coleman", "D. Seaton", "I. Chuang"], "venue": "In Learning at Scale,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining click-stream data with nlp tools to better understand mooc completion", "author": ["S. Crossley", "L. Paquette", "M. Dascalu", "D.S. McNamara", "R.S. Baker"], "venue": "In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, pages 6\u201314. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Temporal models for predicting student dropout in massive open online courses", "author": ["M. Fei", "D.-Y. Yeung"], "venue": "In 2015 IEEE International Conference on Data Mining Workshop (ICDMW), pages 256\u2013263. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Case study: using moocs for conventional college coursework", "author": ["R. Firmin", "E. Schiorring", "J. Whitmer", "T. Willett", "E.D. Collins", "S. Sujitparapitaya"], "venue": "Distance Education, 35(2):178\u2013201,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout prediction in MOOCs using learner activity features", "author": ["S. Halawa", "D. Greene", "J. Mitchell"], "venue": "In European MOOC Summit,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Benjamin, I", "author": ["J. He", "J. Bailey"], "venue": "Rubinstein, and R. Zhang. Identifying at-risk students in massive open online courses. In AAAI,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Harvardx and mitx: The first year of open online courses, fall 2012-summer 2013", "author": ["A.D. Ho", "J. Reich", "S.O. Nesterko", "D.T. Seaton", "T. Mullaney", "J. Waldo", "I. Chuang"], "venue": "Ho, AD, Reich, J., Nesterko, S., Seaton, DT, Mullaney, T., Waldo, J., & Chuang, I.(2014). HarvardX and MITx: The first year of open online courses (HarvardX and MITx Working Paper No. 1),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "and D", "author": ["S. Jiang", "A. Williams", "K. Schenke", "M. Warschauer"], "venue": "O\u2019Dowd. Predicting MOOC performance with week 1 behavior. In Educational Data Mining,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Organizers", "author": ["KDD Cu"], "venue": "http://kddcup2015.com/submission-rank.html,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Attrition and achievement gaps in online learning", "author": ["R. Kizilcec", "S. Halawa"], "venue": "In Learning at Scale,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting MOOC dropout over weeks using machine learning methods", "author": ["M. Kloft", "F. Stiehler", "Z. Zheng", "N. Pinkwart"], "venue": "In Proceedings of the EMNLP 2014 Workshop on Analysis of Large Scale Social Interaction in MOOCs, pages 60\u201365,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning is not a spectator sport: Doing is better than watching for learning from a mooc", "author": ["K.R. Koedinger", "J. Kim", "J.Z. Jia", "E.A. McLaughlin", "N.L. Bier"], "venue": "In Proceedings of the Second (2015) ACM Conference on Learning@ Scale, pages 111\u2013120. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Forecasting student achievement in moocs with natural language processing", "author": ["C. Robinson", "M. Yeomans", "J. Reich", "C. Hulleman", "H. Gehlbach"], "venue": "In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, pages 383\u2013387. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Social factors that contribute to attrition in moocs", "author": ["C.P. Ros\u00e9", "R. Carlson", "D. Yang", "M. Wen", "L. Resnick", "P. Goldman", "J. Sherer"], "venue": "In Proceedings of the first ACM conference on Learning@ scale conference, pages 197\u2013198. ACM,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The impact on retention of interventions to support distance learning students", "author": ["O. Simpson"], "venue": "Open Learning: The Journal of Open, Distance and e-Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Mass attrition: An analysis of drop out from a principles of microeconomics MOOC", "author": ["R. Stein", "G. Allione"], "venue": "PIER Working Paper, 14(031),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "K", "author": ["C. Taylor"], "venue": "Veeramachaneni, and U.-M. O\u2019Reilly. Likely to stop? Predicting stopout in massive open online courses. arXiv,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond prediction: Toward automatic intervention to reduce mooc student stopout", "author": ["J. Whitehill", "J. Williams", "G. Lopez", "C. Coleman", "J. Reich"], "venue": "In Educational Data Mining,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The impact of instructor interaction in massive open online course (mooc) discussion forums", "author": ["T. Wong", "D. Lichtenstein", "J. Whitehill", "G. Lopez", "I. Chuang", "A. Ho"], "venue": "In American Educational Research Association workshop,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Temporal predication of dropouts in moocs: Reaching the low hanging fruit through stacking generalization", "author": ["W. Xing", "X. Chen", "J. Stein", "M. Marcinkowski"], "venue": "Computers in Human Behavior, 58:119\u2013129,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Turn on, tune in, drop out\u201d: Anticipating student dropouts in massive open online courses", "author": ["D. Yang", "T. Sinha", "D. Adamson", "C.P. Rose"], "venue": "In NIPS Workshop on Data-Driven Education,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Early prediction of student dropout and performance in moocs using higher granularity temporal information", "author": ["C. Ye", "G. Biswas"], "venue": "Journal of Learning Analytics, 1(3),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": ", email reminders [22]) may incur very little cost once the core infrastructure for con-", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive.", "startOffset": 167, "endOffset": 174}, {"referenceID": 7, "context": "1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive.", "startOffset": 167, "endOffset": 174}, {"referenceID": 22, "context": "1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive.", "startOffset": 281, "endOffset": 285}, {"referenceID": 0, "context": "Balakrishnan & Coetzee [1] 1 Clickstream HMM + SVM Same course", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Boyer & Veeramachaneni [2] 3 Clickstream TL+LR Different offering In-situ", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "[5] 1 Clickstream LDA+LR Same course", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] 1 Clickstream; NLP DFA Same course", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Fei & Yeung [7] 2 Clickstream RNN Same course", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "[10] 2 Clickstream Smoothed LR Different offering", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] 1 Social network; grades LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14, 9] 20 Clickstream LR Different course Same course", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[14, 9] 20 Clickstream LR Different course Same course", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[15] 1 Clickstream SVM Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] 1 Clickstream; grades LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] 1 Survey; NLP LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25, 18] 1 Forum; social network SA Same course", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[25, 18] 1 Forum; social network SA Same course", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "Stein & Allione [20] 1 Clickstream; survey SA Same course", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "[21] 1 Clickstream LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] 10 Clickstream LR Different course", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 1 Clickstream; social network PCA+{BN,DT} Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Ye & Biswas [26] 1 Clickstream LR Same course", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "also explore a novel scheme (similar to the in-situ approach proposed by [2]) for training a classifier while a course is ongoing using proxy labels and show that its performance is surprisingly good even compared to the overly optimistic post-hoc training paradigm.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": ", [2]) and can generalize to a wide variety of MOOCs across academic disciplines; they are listed in Table 3.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "All features were extracted from two different database tables \u2013 person course and person course day \u2013 that are automatically updated daily by the edx2bigquery data management framework [4] that is used for MITx and HarvardX MOOCs.", "startOffset": 186, "endOffset": 189}, {"referenceID": 10, "context": ", comedian Stephen Colbert talking on television about MOOCs [11]) cause students to behave differently during the later incarnation of the course.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "In addition, we also explore a novel training approach, similar to the in-situ training method proposed by [2], based on using proxy labels:", "startOffset": 107, "endOffset": 110}, {"referenceID": 21, "context": "In prior research on dropout detection [22, 14], this variable alone has shown to be highly predictive of dropout.", "startOffset": 39, "endOffset": 47}, {"referenceID": 13, "context": "In prior research on dropout detection [22, 14], this variable alone has shown to be highly predictive of dropout.", "startOffset": 39, "endOffset": 47}, {"referenceID": 0, "context": "The AUC is the integral of the ROC curve over the interval [0, 1] and answers the following question: Given one randomly chosen student who drops out from the MOOC and one randomly chosen student who certifies in the MOOC, what is the probability that the classifier can correctly distinguish the two students? A useless classifier that simply flips a coin will have an AUC of 0.", "startOffset": 59, "endOffset": 65}, {"referenceID": 21, "context": "45%, which corroborates previous findings [22, 14] that this variable is highly salient for prediction.", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "45%, which corroborates previous findings [22, 14] that this variable is highly salient for prediction.", "startOffset": 42, "endOffset": 50}, {"referenceID": 6, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 4, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 0, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 23, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 2, "context": "cently developed methodology for neural network training called Net2Net [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 12, "context": "03% [13].", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "In addition, we have explored a training approach, similar to in-situ training [2], based on the idea of proxy labels \u2013 labels that approximate the quantity of interest (dropout versus certification) but that can be collected", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Using an iterative training strategy [3], the total training time to create such a network is modest and amenable to large-scale implementation over many (hundreds) of MOOCs at regular training intervals (e.", "startOffset": 37, "endOffset": 40}], "year": 2017, "abstractText": "In order to obtain reliable accuracy estimates for automatic MOOC dropout predictors, it is important to train and test them in a manner consistent with how they will be used in practice. Yet most prior research on MOOC dropout prediction has measured test accuracy on the same course used for training the classifier, which can lead to overly optimistic accuracy estimates. In order to understand better how accuracy is affected by the training+testing regime, we compared the accuracy of a standard dropout prediction architecture (clickstream features + logistic regression) across 4 different training paradigms. Results suggest that (1) training and testing on the same course (\u201cpost-hoc\u201d) can overestimate accuracy by several percentage points; (2) dropout classifiers trained on proxy labels based on students\u2019 persistence are surprisingly competitive with post-hoc training (87.33% versus 90.20% AUC averaged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performance does not vary significantly with the academic discipline. Finally, we also research new dropout prediction architectures based on deep, fully-connected, feed-forward neural networks and find that (4) networks with as many as 5 hidden layers can statistically significantly increase test accuracy over that of logistic regression. INTRODUCTION AND RELATED WORK As the number and diversity of massive open online courses (MOOCs) continues to grow, researchers, teachers, and educational technologists are starting to explore innovative ways of helping more students who participate in these courses to persist longer and learn more. While some interventions (e.g., email reminders [22]) may incur very little cost once the core infrastructure for conACM ISBN 978-1-4503-2138-9. DOI: 10.1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive. An automatic detector that could predict automatically \u2013 based on demographics or by analyzing subtle cues in a learner\u2019s interaction history with the MOOC courseware \u2013 which learners are in danger of \u201cdropping out\u201d from the MOOC and which will likely succeed, would be a valuable tool for making smart decisions about who receives a particular intervention. Related work: Within the fields of learning analytics and educational data mining, the possibility of creating automatic MOOC dropout detectors has generated considerable interest within the past few years. Existing approaches vary across several dimensions including the features used for classification, the architecture used for training and testing, and the training set used to optimize the classifier parameters \u2013 see Table 1 for a synopsis of prior work. The most popular feature representations include clickstream logs, natural language processing (NLP) of discussion forum content, and social network metrics. In terms of architecture, most prior approaches have used generalized linear models (including logistic regression and linear SVMs), survival analysis (e.g., Cox proportional hazard model), and logistic regression. A third dimension of variability is the training setting that describes the source of the training data \u2013 e.g., the same course, a prior instance of the same course, or a different course altogether \u2013 relative to the target use of the classifier once it has been trained: most research on MOOC dropout prediction to-date has focused on training and testing on data sampled from the same MOOC. When designing and implementing educational interventions that depend on automatic MOOC dropout predictors to predict which students are in danger of failing, it is important to know how accurate such predictors are. In order to obtain reliable accuracy estimates, it ar X iv :1 70 2. 06 40 4v 1 [ cs .A I] 2 1 Fe b 20 17 Survey of Prior Research on MOOC Dropout Prediction Study #MOOCs Features Architecture Training setting Balakrishnan & Coetzee [1] 1 Clickstream HMM + SVM Same course Boyer & Veeramachaneni [2] 3 Clickstream TL+LR Different offering In-situ Coleman et al. [5] 1 Clickstream LDA+LR Same course Crossley et al. [6] 1 Clickstream; NLP DFA Same course Fei & Yeung [7] 2 Clickstream RNN Same course He et al. [10] 2 Clickstream Smoothed LR Different offering Jiang et al. [12] 1 Social network; grades LR Same course Kizilcec et al. [14, 9] 20 Clickstream LR Different course Same course Kloft et al. [15] 1 Clickstream SVM Same course Koedinger et al. [16] 1 Clickstream; grades LR Same course Robinson et al. [17] 1 Survey; NLP LR Same course Rose et al. [25, 18] 1 Forum; social network SA Same course Stein & Allione [20] 1 Clickstream; survey SA Same course Taylor et al. [21] 1 Clickstream LR Same course Whitehill et al. [22] 10 Clickstream LR Different course Xing et al. [24] 1 Clickstream; social network PCA+{BN,DT} Same course Ye & Biswas [26] 1 Clickstream LR Same course Our paper 40 Clickstream {LR, DNN} Same course In-situ", "creator": "LaTeX with hyperref package"}}}