{"id": "1702.06776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Causal Inference by Stochastic Complexity", "abstract": "the algorithmic markov condition states conversely that the most likely causal direction between two two random variables x and y can be identified as that direction with the lowest maximum kolmogorov complexity. due to ignoring the halting problem, however, this notion effectively is not computable.", "histories": [["v1", "Wed, 22 Feb 2017 12:36:21 GMT  (1032kb,D)", "http://arxiv.org/abs/1702.06776v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["kailash budhathoki", "jilles vreeken"], "accepted": false, "id": "1702.06776"}, "pdf": {"name": "1702.06776.pdf", "metadata": {"source": "META", "title": "Causal Inference by Stochastic Complexity", "authors": ["KAILASH BUDHATHOKI", "JILLES VREEKEN"], "emails": [], "sections": [{"heading": null, "text": "Causal Inference by Stochastic Complexity\nKAILASH BUDHATHOKI, Max Planck Institute for Informatics and Saarland University, Germany\nJILLES VREEKEN, Max Planck Institute for Informatics and Saarland University, Germany\ne algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identi ed as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable.\nWe hence propose to do causal inference by stochastic complexity. at is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. is means that even in an adversarial se ing, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class.\nWe instantiate this framework, which we call cisc, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that cisc is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes.\nCCS Concepts: \u2022Mathematics of computing\u2192 Causal networks; Information theory; \u2022Information systems\u2192 Data mining;\nAdditional Key Words and Phrases: Causal Inference, Stochastic Complexity, NML\nACM Reference format: Kailash Budhathoki and Jilles Vreeken. 2017. Causal Inference by Stochastic Complexity. 1, 1, Article 1 (January 2017), 15 pages. DOI: 10.475/123 4"}, {"heading": "1 INTRODUCTION", "text": "Causal inference from observational data\u2014that is, identifying cause and e ect in data that was not collected through carefully controlled randomised trials\u2014is a fundamental problem in both business and science [18, 28]. A particularly interesting se ing is to tell cause from e ect between a pair of random variables X and Y , given data over the joint distribution. at is, to identify which of X \u2192 Y or Y \u2192 X is the most likely causal direction.\nIn recent years, a number of important ideas have been proposed that allow for accurate causal inference based on properties of the joint distribution. ese ideas include that of the Additive Noise Model (ANM), where we assume the e ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity. Loosely speaking, the key idea is that if X causes Y , the shortest description of the joint distribution P(X ,Y ) is given by the separate descriptions of P(X ) and P(Y | X ). at is, if X \u2192 Y , these two distributions will be less dependent than P(Y ) and P(X | Y ). However, as Kolmogorov\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). \u00a9 2017 Copyright held by the owner/author(s). Manuscript submi ed to ACM\nManuscript submi ed to ACM 1\nar X\niv :1\n70 2.\n06 77\n6v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\n17\ncomplexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].\nIn this paper, for the rst time, we de ne a causal inference rule based on the algorithmic Markov condition using stochastic complexity. More in particular, we approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle using a score that is mini-max optimal with regard to the model class under consideration. is means that even if the true data generating distribution does not reside in the model classM under consideration, we still obtain the optimal encoding for the data relative toM [3]. Best of all, unlike Kolmogorov complexity, stochastic complexity is computable.\nWe show the strength of this approach by instantiating it for pairs of univariate discrete data using the class of multinomials. For this class the stochastic complexity is computable remarkably e ciently, by which our score has only a linear-time computational complexity. rough experiments we show that our method, cisc, for causal inference by stochastic complexity, performs very well in practice. e strength of the mini-max property shows when we consider synthetic data where we vary the data generating process\u2014cisc outperforms the state of the art by a margin, including for out-of-model distributions such as geometric, hypergeometric, and Poisson. On the Tu\u0308bingen benchmark data set of 95 univariate pairs, cisc signi cantly outperforms the existing proposals for discrete data, with an accuracy of 100% over the 21 pairs it is most certain about, and an overall accuracy of 67%, which is comparable to the state of the art for causal inference on continuous-valued data. Last, but not least, we perform three case studies which show cisc indeed infers sensible causal directions from real-world data.\nIn sum, the main contributions of this paper are as follows.\n(a) we propose the rst computable framework for causal inference by the algorithmic Markov condition with provable mini-max optimality guarantees, (b) de ne a causal indicator for pairs of discrete variables based on stochastic complexity, (c) show how to e ciently compute it, (d) provide extensive experimental results on synthetic, benchmark, and real-world data, and (e) make our implementation and all used data available\ne paper is structured as usual. We introduce notation and give preliminaries in Sec. 2, and give a brief primer to causal inference by Kolmogorov complexity in Sec. 3. We present cisc, our practical instantiation based on stochastic complexity score in Sec. 4. Related work is discussed in Sec. 5, and we evaluate cisc empirically in Sec. 6. We round up with discussion in Sec. 7 and conclude in Sec. 8."}, {"heading": "2 PRELIMINARIES", "text": "In this section, we introduce notations and background de nitions we will use in subsequent sections."}, {"heading": "2.1 Kolmogorov Complexity", "text": "e Kolmogorov complexity of a nite binary string x is the length of the shortest binary program p\u2217 for a Universal Turing machineU that generates x , and then halts [10, 11]. Formally, we have\nK(x) = min { |p | : p \u2208 {0, 1}\u2217,U(p) = x } .\nManuscript submi ed to ACM\nSimply put, p\u2217 is the most succinct algorithmic description of x , and the Kolmogorov complexity of x is the length of its ultimate lossless compression. Conditional Kolmogorov complexity, K(x | y) \u2264 K(x), is then the length of the shortest binary program p\u2217 that generates x , and halts, given y as input.\ne amount of algorithmic information contained in y about x is I (y : x) = K(y) \u2212 K(y | x\u2217), where x\u2217 is the shortest binary program for x , de ning I (x : y) analogously. Intuitively, it is the number of bits that can be saved in the description of y when the shortest description of x is already known. Algorithmic information is symmetric, i.e. I (y : x) += I (x : y), where += denotes equality up to an additive constant, and therefore also called algorithmic mutual information [11]. Two strings x and y are algorithmically independent if they have no algorithmic mutual information, i.e. I (x : y) += 0.\nFor our purpose, we also need the Kolmogorov complexity of a distribution. e Kolmogorov complexity of a probability distribution P , K(P), is the length of the shortest program that outputs P(x) to precision q on input \u3008x ,q\u3009 [4]. More formally, we have\nK(P) = min { |p | : p \u2208 {0, 1}\u2217, |U(\u3008x , \u3008q,p\u3009\u3009) \u2212 P(x)| \u2264 1/q } .\nWe refer the interested reader to Li & Vita\u0301nyi [11] for more details on Kolmogorov complexity."}, {"heading": "3 CAUSAL INFERENCE BY COMPLEXITY", "text": "Given two correlated variables X and Y , we are interested in inferring their causal relationship. In particular, we want to infer whether X causes Y , whether Y causes X , or they are only correlated. In doing so, we assume causal su ciency. at is, there is no confounding variable, i.e. hidden common cause Z of X and Y . We use X \u2192 Y to indicate X causes Y .\nWe base our causal inference method on the following postulate:\nPostulate 1 (independence of input and mechanism [25]). If X \u2192 Y , the marginal distribution of the cause P(X ), and the conditional distribution of the e ect given the cause, P(Y | X ) are independent \u2014 P(X ) contains no information about P(Y | X ) \u2014 and vice versa since they correspond to independent mechanisms of nature.\nis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25]. We can think of conditional P(Y |X ) as the mechanism that transforms x-values into y-values, i.e. generates e ect Y for cause X . e postulate is justi ed if we are dealing with a mechanism of nature that does not care what input we provide to it (P(X ) in this case). is independence will not hold in the other direction as P(Y ) and P(X | Y ) may contain information about each other as both inherit properties from P(Y | X ) and P(X ). is creates an asymmetry between cause and e ect.\nIt is insightful to consider the following example where amount of radiation per cm2 solar cell (cause) causes power generation in the cell (e ect). We can just a ect P(cause) only by actions such as moving the solar cell to a shady place, and varying the angle to the sun to a ect P(cause). Likewise we can change only P(e ect | cause) by actions such as using more e cient cells. However it is hard to nd actions that change P(e ect) without a ecting P(cause | e ect) or vice versa.\ne notion of independence, however, is abstract. Accordingly, di erent formalisations have been proposed. Janzing et al. [7] de ne independence in terms of information geometry. Liu & Chan [12] formulate independence in terms of the distance correlation between marginal and conditional empirical distribution. Janzing & Scho\u0308lkopf [8] formalise independence using algorithmic information theory, and postulate algorithmic independence of P(X ) and P(Y | X ).\nManuscript submi ed to ACM\nSince algorithmic formulation captures all types of dependencies, and has a sound theoretical foundation, it is, arguably, a be er mathematical formalisation of Postulate 1. Using algorithmic information theory, we arrive at the following postulate.\nPostulate 2 (algorithmic independence of Markov kernels [8]). If X \u2192 Y , the marginal distribution of the cause P(X ) and the conditional distribution of the cause given the e ect P(Y | X ) are algorithmically independent, i.e. I (P(X ) : P(Y | X )) += 0.\nPostulate 2 is equivalent to saying that if X \u2192 Y , factorizing the joint distribution over X and Y into P(X ) and P(Y | X ), will lead to simpler \u2014 in terms of Kolmogorov complexity \u2014 models than factorizing it into P(Y ) and P(X | Y ) [8]. e following theorem is hence a consequence of the algorithmic independence of input and mechanism.\nTheorem 3.1 (Th. 1 in Mooij et al. [15]). If X is a cause of Y ,\nK(P(X )) + K(P(Y | X )) \u2264 K(P(Y )) + K(P(X | Y )) .\nholds up to an additive constant.\nIn other words, we can perform causal inference simply by identifying that direction between X and Y for which the factorization of the joint distribution has the lowest Kolmogorov complexity. Although this inference rule has sound theoretical foundations, the problem remains that Kolmogorov complexity is not computable because of the widely known halting problem. In practice, we therefore need other, computable, notions of independence or information. We can, for instance, approximate Kolmogorov complexity from above through lossless compression [11]. More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29]."}, {"heading": "4 CAUSAL INFERENCE BY COMPRESSION", "text": "In this section, we discuss how stochastic complexity can be used for practical causal inference. We gradually move towards that goal starting with MDL, and covering the basics along the way."}, {"heading": "4.1 Minimum Description Length Principle", "text": "e Minimum Description Length (MDL) [21] principle is a practical version of Kolmogorov complexity. Instead of all possible programs, it considers only programs for which we know they generate x and halt. at is, lossless compressors.\nIn MDL theory, programs are o en referred to as models. e MDL principle has its root in the two-part decomposition of the Kolmogorov complexity [11]. It can be roughly described as follows [3]. Given a set of modelsM and data D, the best model M \u2208 M is the one that minimises L(D,M) = L(M) + L(D | M), where L(M) is the length, in bits, of the description of the model, and L(D | M) is the length, in bits, of the description of the data when encoded with the model M . Intuitively L(M) represents the compressible part of the data, and L(D | M) represents the noise in the data.\nis is called two-part MDL, or crude MDL. To use crude MDL in practice, we have to de ne our model classM, and the description methods for L(M) as well as L(D | M). If the modelsM under consideration de ne probability distributions, we can use optimal pre x code given by Shannon entropy, L(D | M) = \u2212 log P(D | M), where P(D | M) is the probability mass or density function of D according to M . e de nition of L(M), however, is tricky \u2014 L(M) can vary from one encoding to the other, introducing arbitrariness in the process. Manuscript submi ed to ACM\ne re ned version of MDL overcomes this arbitrariness by encoding M and D together. Unlike crude MDL, re ned MDL encodes D with the (entire) model classM, resulting in single one-part code L\u0304(D | M) [3]. e one-part code length L\u0304(D | M) is also called the stochastic complexity of D with respect to M .\ne code is designed in such a way that if there exists a model M\u2217 \u2208 M for which L(D | M\u2217) is minimal then L\u0304(D | M) will also be minimal. Codes with such property are also called universal codes. ere exist various types of universal codes. Although the coding schemes are di erent across those codes, the resulting code lengths L\u0304(D | M) are almost the same [3]. In this work, we consider the NML universal code in particular.\nNext we explain stochastic complexity in detail using the NML universal code."}, {"heading": "4.2 Stochastic Complexity", "text": "Let Xn = (x1,x2, . . . ,xn ) be an i.i.d. sample of n observed outcomes, where each outcome xi is an element of a space of observations X. Let \u0398 \u2208 Rd , where d \u2208 Z+, be the parameter space. A model classM is a family of probability distributions consisting of all the di erent distributions P(. | \u03b8 ) that can be produced by varying the parameters \u03b8 . Formally, a model classM is de ned as\nM = {P(\u00b7 | \u03b8 ) : \u03b8 \u2208 \u0398} .\nTo encode the data Xn optimally with respect to the model class M, we can use the code corresponding to the distribution P(\u00b7 | \u03b8\u0302 (Xn ,M)) induced by the maximum likelihood estimate \u03b8\u0302 (Xn ,M) of the data Xn for a given model classM, since this distribution assigns shorter code length, i.e. higher likelihood, to the data than any of the other distributions in the model class. e Normalized Maximum Likelihood (NML) distribution is then de ned as\nPNML(Xn | M) = P(Xn | \u03b8\u0302 (Xn ,M))\nR(M,n) ,\nwhere the normalizing term R(M,n) is the sum over maximum likelihoods of all possible datasets of size n under the model classM. For discrete data, R(M,n) is de ned as\nR(M,n) = \u2211\nY n \u2208Xn P(Yn | \u03b8\u0302 (Yn ,M)) ,\nwhere Xn is the n-fold Cartesian product X \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X indicating set of all possible datasets of size n with domain X. When the data Xn is de ned over a continuous sample space, the summation symbol in Equation 4.2 is replaced by an integral.\ne NML distribution has a number of important theoretical properties. First, it gives a unique solution to the minimax problem posed by Shtarkov [27],\nmin P\u0302 max Xn log P(X n | \u03b8\u0302 (Xn ,M)) P\u0302(Xn | M) .\nat is, for any data Xn , PNML(Xn | M) assigns a probability, which di ers from the highest achievable probability within the model class \u2014 the maximum likelihood P(Xn | \u03b8\u0302 (Xn ,M)) \u2014 by a constant factor R(M,n). In other words, the NML distribution is the mini-max optimal universal model with respect to the model class [16]. e NML distribution represents the behaviour of all the distributions in the model classM.\nSecond, it also provides solution to another mini-max problem formulated by Rissanen [22], which is given by\nmin P\u0302 max Q\nEQ log P(Xn | \u03b8\u0302 (Xn ,M))\nP\u0302(Xn | M) ,\nManuscript submi ed to ACM\nwhere Q is the worst-case data generating distribution, and EQ is the expectation over Xn . at is, even if the true data generating distribution does not reside in the model classM under consideration, PNML(Xn | M) still gives the optimal encoding for the data Xn relative toM.\nese properties are very important and relevant when modelling real-world problems. In most cases, we do not know the true data generating distribution. In such cases, ideally we would want to encode our data as best as possible \u2014 close to the optimal under the true distribution. e NML distribution provides a theoretically sound means for that.\ne stochastic complexity of data Xn relative to a model classM using the NML distribution is de ned as\nS(Xn | M) = \u2212 log PNML(Xn | M)\n= \u2212 log P(Xn | \u03b8\u0302 (Xn ,M)) + logR(M,n) . (1)\ne term logR(M,n) is the parametric complexity of the model classM. It indicates how wellM can t random data. e stochastic complexity of data under a model classM gives the shortest description of the data relative toM. Hence the richer theM, the closer we are to Kolmogorov complexity. Intuitively, it is also the amount of information, in bits, in the data relative to the model class. Moreover, it is evident from the formulation that the stochastic complexity of data, relative to a model class, depends only on the data and the model class, but not on the particular way the models are speci ed."}, {"heading": "4.3 Causal Inference by Stochastic Complexity", "text": "Unless stated otherwise, we write X for Xn , and Y for Yn . e stochastic complexity of data X relative to model classM corresponds to the complexity of the NML distribution of the data relative toM. is means we can use the stochastic complexity of X as an approximation of the Kolmogorov complexity of P(X ). As such, it provides a general, yet computable, theoretically sound foundation for causal inference based on algorithmic information theory.\nFor ease of notation, wherever clear from context we write S(X ) for S(X | M). To infer the causal direction, we look over total stochastic complexity in two directions \u2014 X to Y and vice versa. e total stochastic complexity from X to Y , approximating K(P(X )) + K(P(Y | X )) is given by\nSX\u2192Y = S(X ) + S(Y | X ) ,\nand that from Y to X is given by SY\u2192X = S(Y ) + S(X | Y ) .\nFollowing eorem 3.1, using the above indicators we arrive at the following causal inference rules.\n\u2022 If SX\u2192Y < SY\u2192X , we infer X \u2192 Y . \u2022 If SX\u2192Y > SY\u2192X , we infer Y \u2192 X . \u2022 If SX\u2192Y = SY\u2192X , we are undecided.\nat is, if describing X and then describing Y given X is easier \u2014 in terms of stochastic complexity \u2014 than vice versa, we infer X is likely the cause of Y . If it is the other way around, we infer Y is likely the cause of X . If both ways of describing are the same, we remain undecided. We refer to this framework as cisc, which stands for causal inference by stochastic complexity.\nCausal inference using stochastic complexity has a number of powerful properties. First, unlike Kolmogorov complexity, stochastic complexity is computable. Second, the inference rule is generic in the sense that we are not restricted to one data type or distribution\u2014we are only constrained by the model classM under consideration, yet by Manuscript submi ed to ACM\nthe mini-max property of NML we know that even if the data generating distribution is adversarial, we still identify the best encoding relative toM.\nNext we discuss how can we instantiate cisc for discrete data."}, {"heading": "4.4 Multinomial Stochastic Complexity", "text": "We consider discrete random variable X with m values. Furthermore we assume that our data Xn = (x1, . . . ,xn ) is multinomially distributed. e space of observations X is then {1, 2, . . . ,m}. e multinomial model classMm is de ned as\nMm = {P(X | \u03b8 ) : \u03b8 \u2208 \u0398m } ,\nwhere \u0398 is the simplex-shaped parameter space given by\n\u0398m = {\u03b8 = (\u03b81, . . . ,\u03b8m ) : \u03b8 j \u2265 0,\u03b81 + \u00b7 \u00b7 \u00b7 + \u03b8m = 1} ,\nwith \u03b8 j = P(X = j | \u03b8 ), j = 1, . . . ,m. e maximum likelihood parameters for a multinomial distribution are given by \u03b8\u0302 (Xn ,Mm ) = (h1/n, . . . ,hm/n), where hj is the number of times an outcome j is seen in Xn . en the distribution induced by the maximum likelihood parameters for Xn under the model classMm is given by\nP(Xn | \u03b8\u0302 (Xn ,Mm )) = n\u220f i=1 P(xi | \u03b8\u0302 (Xn ,Mm ))\n= m\u220f j=1 ( hj n )hj .\ne normalizing term R(Mm ,n) is given by R(Mm ,n) = \u2211\nY n \u2208Xn P(Yn | \u03b8\u0302 (Yn ,Mm ))\n= \u2211\nh1+\u00b7 \u00b7 \u00b7+hm=n\nn! h1! \u00b7 \u00b7 \u00b7hm ! m\u220f j=1 ( hj n )hj . (2)\nen the NML distribution for Xn under the model classMm is given by PNML(Xn | Mm ) = \u220fm j=1(hj/n)hj\nR(Mm ,n) .\nen the stochastic complexity of Xn for the model classMm is given by\nS(Xn | Mm ) = \u2212 log m\u220f j=1 (hj/n)hj + logR(Mm ,n)\n= m\u2211 j=1 hj (logn \u2212 loghj ) + logR(Mm ,n)\n= n logn \u2212 m\u2211 j=1 hj loghj + logR(Mm ,n) . (3)\nComputational Complexity \u2014 We can compute the counts hj in O(n) by going through the data once. However, computing the normalizing sum (Equation 2), and hence the parametric complexity, is exponential in the number of\nManuscript submi ed to ACM\nvaluesm. As a result, the computational complexity of the multinomial stochastic complexity (Equation 3) is dominated by by computation time of the normalizing sum.\nHowever, we can approximate the normalising sum up to a nite oating-point precision in sub-linear time with respect to the data size n given precomputed counts hi [13]. More precisely, the computational complexity of the sub-linear algorithm is O( \u221a dn +m), where d is the oating-point precision in digits. In the experiments we use d = 10. Altogether we can compute the multinomial stochastic complexity in O(n)."}, {"heading": "4.5 Computing Conditional Complexity", "text": "So far we only discussed how to compute the stochastic complexity of data under a model class. For our purpose, we also need to compute the conditional stochastic complexity S(Y | X ) and vice versa. Let S(Y | X = x) be the stochastic complexity of Y conditioned on X = x . en the conditional stochastic complexity S(Y | X ) is the sum of S(Y | X = x) over all possible values of X .\nLet X be the domain of X . en the stochastic complexity of Y given X is de ned as S(Y | X ) = \u2211 x \u2208X S(Y | X = x) .\nComputational Complexity \u2014 We can compute S(Y | X = x) in O(n). To compute the conditional stochastic complexity S(Y | X ), we have to compute S(Y | X = x) over all x \u2208 X. Hence the computational complexity of conditional stochastic complexity is O(n |X|). Likewise, for S(X | Y ), we have O(n |Y|). Altogether the computational complexity of cisc is O(n max(|X|, |Y|))."}, {"heading": "5 RELATEDWORK", "text": "Inferring causal direction from observational data is a challenging task due to the lack of controlled randomised experiments. However, it has also a racted quite a lot of a ention over the years [8, 17, 26, 28]. Yet, most of the causal inference frameworks are built for continuous real-valued data.\nConstraint-based approaches like conditional independence test [17, 28] are one of the widely used causal inference frameworks. However, they require at least three observed random variables. erefore they cannot distinguish between X \u2192 Y and Y \u2192 X as the factorization of the joint distribution P(X ,Y ) is the same in both direction, i.e. P(X )P(Y | X ) = P(Y )P(X | Y ).\nIn recent years, several methods have been proposed that exploit the sophisticated properties of the joint distribution. e linear trace method [6, 32] infers linear causal relations of the form Y = AX , where A is the structure matrix that maps the cause to the e ect, using the linear trace condition. e kernelized trace method [2] can infer non-linear causal relations, but requires the causal relation to be deterministic, functional, and invertible. In contrast, we do not make any assumptions on the causal relation between the variables.\nOne of the key frameworks for causal inference are the Additive Noise Models (ANMs) [26]. ANMs assume that the e ect is a function of the cause and the additive noise that is independent of the cause. Causal inference is then done by nding the direction that admits such a model. Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].\nAlgorithmic information theory provides a sound general theoretical foundation for causal inference [8]. e key idea is that if X causes Y , the shortest description of the joint distribution P(X ,Y ) is given by the separate descriptions Manuscript submi ed to ACM\nof the distributions P(X ) and P(Y | X ) [8]. It has also been used in justifying the additive noise model based causal discovery [9].\nHowever, as Kolmogorov complexity is not computable, practical instantiations require computable notions of independence. For instance, the information-geometric approach [7] de nes independence via orthogonality in information space. Cure [25] de nes independence in terms of the accuracy of the estimations of P(Y | X ) and P(X | Y ). Using algorithmic information theory, Vreeken [30] proposes a causal framework based on relative conditional complexity and instantiates it with cumulative entropy to infer the causal direction in continuous real-valued data. Budhathoki & Vreeken [1] propose a decision tree based approach for causal inference on univariate and multivariate binary data.\nAll above methods consider either continuous real-valued or binary data. Causal inference from discrete data has received much less a ention. Peters et al. [19] (dr) extend additive noise models to discrete data. However regression is not ideal for modelling categorical variables, and as it relies on the dependence measure, the choice of which a ects the outcome. Liu & Chan [12] (dc) de ne independence in terms of the distance correlation between empirical distributions P(X ) and P(Y | X ) to infer the causal direction from categorical data. As such, it does not look over all possible space of the observed samples and hence over ts.\nIn contrast, we look over all possible space of the observed samples. Moreover, we provide a general, yet computable, theory for causal inference that is applicable to any type of data. In particular, we directly approximate Kolmogorov complexity using a score that is mini-max optimal with regard to the model class under consideration. e computational complexity of our instantiation, cisc, is linear in sample size, regardless of the domain of the variables. In the experiments, we consider both dc and dr for comparison."}, {"heading": "6 EXPERIMENTS", "text": "We implemented cisc in Python and provide the source code for research purposes, along with the used datasets, and synthetic dataset generator.1 All experiments were executed single-threaded on Intel Xeon E5-2643 v3 machine with 256GB memory running Linux. We consider synthetic, benchmark, and real-world data. In particular, we note that cisc is parameter-free. We compare cisc against Discrete Regression (dr) [19], and dc [12]. In particular, we use signi cance level of \u03b1 = 0.05 for the independence test in dr, and threshold of \u03f5 = 0.0 for dc."}, {"heading": "6.1 Synthetic Data", "text": "To evaluate cisc on the data with known ground truth, we consider synthetic data. Generating non-trivial synthetic data with identi able causal direction is surprisingly di cult, though.2 We generate synthetic cause-e ect pairs with ground truth X \u2192 Y using the additive noise model (ANM). at is, rst we generate the cause X , and then generate the e ect Y using the model given by\nY = f (X ) + N ,N \u22a5 X ,\nwhere f is a function, and N is additive noise that is independent of X . Following Peters et al. [19], we sample X from the following distributions, using independently generated uniform noise.\n\u2022 uniform from {1, . . . ,L}, \u2022 binomial with parameters (n,p),\n1h p://eda.mmci.uni-saarland.de/cisc/ 2Ideally we would generate data with known K (P (X ))+K (P (Y | X )) < K (P (Y ))+K (P (X | Y )), and evaluate our inference methods accordingly, yet as Kolmogorov complexity is not computable it is not apparent how to do this in general. Manuscript submi ed to ACM\n\u2022 geometric with parameter p, \u2022 hypergeometric with parameters (M,K ,N ), \u2022 poisson with parameter \u03bb, \u2022 negative binomial with parameters (n,p), and \u2022 multinomial with parameters \u03b8 .\nWe note that even though we generate data following ANM from X to Y , the joint distribution P(X ,Y ) might admit an additive noise model in the reverse direction. erefore in some cases where we say that X \u2192 Y is the true direction, Y \u2192 X might also be equally plausible, and hence full accuracy might not be achievable in some cases. However, this happens in only few trivial instances [19].\nWe choose parameters of the distributions randomly for each model class. We choose L uniformly between 1 and 10, M,K uniformly between 1 and 40, N uniformly between 1 and min(41,M + K), p uniformly between 0.1 and 0.9, \u03bb uniformly between 1 and 10, \u03b8 randomly s.t. \u2211 \u03b8 \u2208\u03b8 \u03b8 = 1.0, function f (x) uniformly between \u22127 to +7, and noise N uniformly between \u2212t to +t , where t is uniformly randomly chosen between 1 and 7. Accuracy \u2014 From each model class, we sample 1000 di erent models, and hence 1000 di erent cause-e ect pairs. For each model, we sample 1000 points, i.e. n = 1000. In Figure 1, we compare the accuracy (percentage of correct decisions) of cisc against dc and dr for various model classes. We see that cisc either outperforms or is as good as the other methods in all but one case. is certainly proves the generality of cisc.\nAlthough we compute the stochastic complexity under multinomial model class, we are still able to perform as good with other model classes. is is due to the optimality property of the NML distribution \u2013 even though the true data generating distribution is not inside the model classM under consideration, the NML distribution still gives the optimal encoding relative toM. And as we see, it works well in most cases.\nDecision Rate \u2014 Next we investigate the accuracy of cisc against the fraction of decisions cisc is forced to make. To this end, for each model class, we sample 1000 di erent cause-e ect pairs. For each cause-e ect pair, we sample 1000 points. We sort the pairs by their absolute score di erence in two directions (X \u2192 Y vs. Y \u2192 X ), i.e. |SX\u2192Y \u2212SY\u2192X | in descending order. en we compute the accuracy over top-k% pairs. e decision rate is the fraction of top cause-e ect pairs that we consider. Alternatively, it is also the fraction of cause-e ect pairs whose |SX\u2192Y \u2212 SY\u2192X | is greater than Manuscript submi ed to ACM\nsome threshold \u03b4 . For undecided pairs, we ip the coin. For other methods, we follow the similar procedure with their respective absolute score di erence.\nIn Figure 2, we show the decision rate versus accuracy for di erent model classes. We see that both cisc and dr are highly accurate up to a very high decision rate in all cases. Both cisc and dr are highly accurate on the cause-e ect pairs where the absolute score di erence is very high \u2014 where the methods are most decisive. dc, on the other hand, doesn\u2019t perform well in all cases. e only se ing where dc has a relatively good performance is in the family of Uniform distributions.\ne results indicate that we can increase the threshold \u03b4 , and hence the decision rate, for higher accuracy. Scalability \u2014 Next we empirically investigate the scalability of cisc. First, we examine runtime with regard to the sample size. To this end, we x the domain size of the cause-e ect pairs to 20, i.e. |X| = |Y| = 20. en for a given sample size, we sample X uniformly randomly between 1 and |X|. Likewise for Y .\nIn Figure 3a, we show the runtime of cisc, dc, and dr for various sample sizes. We observe that both cisc and dc (overlapping line) nish within seconds. dr, on the other hand, takes in the order of hours.\nNext we x the sample size to n = 100 000 and vary the domain size |X| = |Y|. We observe that both cisc and dc again nish within seconds over the whole range. As dr iteratively searches over the entire domain, it shows a non-linear runtime behaviour with respect to the domain size.\nOverall, these results indicate that dr is fairly accurate, but relatively slow. dc, on the other hand, is fast, yet inaccurate. cisc is both highly accurate, and fast."}, {"heading": "6.2 Benchmark Data", "text": "Next we evaluate cisc on benchmark cause-e ect pairs with known ground truth [14]. In particular, we take 95 univariate cause-e ect pairs. So far there does not exist a discretization strategy that provably preserves the causal relationship between variables. Since each cause-e ect pair is from a di erent domain, using one discretization strategy\nManuscript submi ed to ACM\nover all the pairs is also unfair. Moreover, we do not know the underlying domain of the data. As a result, we treat the data as discrete for all the pairs.\nIn Figure 4, we compare the accuracy of cisc against dc and dr at various decision rate together with the 95% con dence interval for a random coin ip. If we look over all the pairs, we nd that cisc infers correct direction in roughly 67% of all the pairs. When we consider only those pairs where cisc is most decisive\u2014with a very high value of |S(X \u2192 Y ) \u2212 S(Y \u2192 X )|, it is 100% accurate on top 22% of the pairs, 80% accurate on top 45% of the pairs, which is on-par with the top-performing causal inference frameworks for continuous real-valued data [7, 25]. On the other hand, the results from both dc and dr are insigni cant at almost every decision rate."}, {"heading": "6.3 alitative Case Studies", "text": "Next we evaluate cisc on real-world data for exploratory purpose. Manuscript submi ed to ACM\nAbalone \u2014 First we consider the Abalone dataset, which is available from the UCI machine learning repository.3 e dataset contains the physical measurements of 4 177 abalones, which are large, edible sea snails.\nOut of the nine measurements, we consider the sex (X ), length (Y1), diameter (Y2), and height (Y3). e length, diameter, and height of the abalone are all measured in millimetres, and have 70, 57 and 28 di erent values, respectively whereas the sex of the abalone is nominal (male = 1, female = 2, or infant = 3). Following Peters et al. [19], we regard the data as discrete, and consider X \u2192 Y1, X \u2192 Y2, and X \u2192 Y3 as the ground truth as sex causes the size of the abalone and not the other way around. cisc infers correct direction in all three cases.\nCar Evaluation \u2014 e Car Evaluation dataset is available from the UCI machine learning repository. It has 1728 rows, and is derived from a hierarchical decision model. It contains the evaluation of a car for buying purpose based on six characteristics of the car.\nWe consider the estimated safety (X ) of the car against the evaluation (Y ) of the car. e safety feature of the car takes a nominal value (low = 1, medium = 2, or high = 3), and the evaluation feature of the car also takes a nominal value (unacceptable = 1, acceptable = 2, good = 3, or very good = 4). We regard X \u2192 Y as the ground truth as safety of the car causes the decision on buying the car, but not vice versa. cisc identi es the correct direction.\nAdult \u2014 e Adult dataset is taken from the UCI machine learning repository and consists of 48 832 records from the census database of the US in 1994.\nOut of 14 a ributes, we consider only three \u2013 education (X1), occupation (X2), and income (Y ). e domain of education a ribute consists of dropout, associates, bachelors, doctorate, hs-graduate, masters, and prof-school. For occupation, we have admin, armed-force, blue-collar, white-collar, service, sales, professional, and other-occupation as possible values. Lastly, for income a ribute, we have two values: >50K and <=50.\nAs intuitively education causes income, and not vice versa, we regard X1 \u2192 Y as the ground truth. Similarly, as occupation causes income, we regard X2 \u2192 Y as the ground truth. We run cisc on both pairs (X1,Y ) and (X2,Y ). We observe that for both pairs cisc infers the causal direction correctly.\nOverall, these results illustrate that cisc nds sensible causal directions from real-world data."}, {"heading": "7 DISCUSSION", "text": "e experiments show that cisc works well in practice. cisc reliably identi es true causal direction regardless of the data distribution. It is remarkably fast. On benchmark data, it\u2019s performance is comparable to the state-of-the-art causal inference frameworks for continuous real-valued data. Moreover, the qualitative case studies show that the results are sensible.\nIn this work, we give a general framework for causal inference based on the solid foundations of information theory. To apply the framework in practice, we just have to compute the stochastic complexity relative to a model class. e richer the model class, the be er the solution. Although computing the stochastic complexity involves looking over all possible datasets, theoretically it is still computable, and there do exist e cient algorithms for certain model classes. e proposed framework lays a clear computable foundation for algorithmic causal inference principle postulated by Janzing & Scho\u0308lkopf [8].\nAlthough the results show the strength of the proposed framework, and of cisc in particular, we see many possibilities to further improve. We instantiated the framework using multinomial stochastic complexity on discrete data. We see that cisc performs relatively well even in cases where the data is not sampled from the multinomial model class. is\n3h p://archive.ics.uci.edu/ml/ Manuscript submi ed to ACM\nis due to the optimality property of the multinomial distribution \u2014 even if the true data generating distribution is not inside the model classM under consideration, the NML distribution still gives the optimal encoding for the data relative toM. It would be an engaging future work to instantiate the framework for other types of data (e.g. continuous real-valued, mixed, etc.) and model classes (e.g. family of Gaussians, Dirichlets, etc.). e key aspect to study would be e cient algorithms for computing the stochastic complexity for such model classes.\nWe de ne conditional stochastic complexity S(Y | X ) as the sum of the stochastic complexities of Y conditioned on X = x over all x . is way we look over local stochastic complexities of parts of Y relative to each value of x . Perhaps we can compute the conditional stochastic complexity globally relative to X . It would also be interesting to explore factorized normalized maximum likelihood models [23] to instantiate the framework for multivariate data [1].\nTo infer the causal relationship between variables X and Y , we assume that there is no confounding variable Z . It would be interesting to use the framework to additionally discover the confounding variables. e rough idea is that factorizing the joint complexity P(X ,Y ) in presence of the confounding variable Z leads to the smallest stochastic complexity compared to factorizing into P(X ) and P(Y | X ) or P(Y ) and P(X | Y ).\nAnother avenue for future work would be to use the framework for causal discovery. e proposed framework infers causal relationship between given two variables X and Y . It would be interesting to explore how the framework can be employed to discover (mine) the causal models directly from the data."}, {"heading": "8 CONCLUSION", "text": "We considered causal inference from observational data. We proposed a general, yet computable framework for information-theoretic causal inference with optimality guarantees. In particular, we proposed to perform causal inference by stochastic complexity.\nTo illustrate the strength of this, we proposed cisc for pairs of univariate discrete variables, using stochastic complexity over the class of multinomial distributions. Extensive evaluation on synthetic, benchmark, and real-world data showed that cisc is highly accurate, outperforming the state of the art by a margin, and scales extremely well with regard to both sample and domain sizes.\nFuture work includes considering richer model classes, as well as structure learning for the discovery of causal models from data."}, {"heading": "ACKNOWLEDGMENTS", "text": "Kailash Budhathoki is supported by the International Max Planck Research School for Computer Science. Both authors are supported by the Cluster of Excellence \u201cMultimodal Computing and Interaction\u201d within the Excellence Initiative of the German Federal Government."}], "references": [{"title": "Causal Inference by Compression", "author": ["Kailash Budhathoki", "Jilles Vreeken"], "venue": "In ICDM", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Nonlinear Causal Discovery for High Dimensional Data: A Kernelized Trace Method", "author": ["Z. Chen", "K. Zhang", "L. Chan"], "venue": "In ICDM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "\u008ae Minimum Description Length Principle", "author": ["Peter Gr\u00fcnwald"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Algorithmic Information \u008ceory", "author": ["Peter D. Gr\u00fcnwald", "Paul M.B. Vit\u00e1nyi"], "venue": "CoRR abs/0809.2754", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Nonlinear causal discovery with additive noise models", "author": ["PO. Hoyer", "D. Janzing", "JM. Mooij", "J. Peters", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Telling cause from e\u0082ect based on high-dimensional observations", "author": ["D. Janzing", "P. Hoyer", "B. Sch\u00f6lkopf"], "venue": "In ICML", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Information-geometric approach to inferring causal directions", "author": ["Dominik Janzing", "Joris Mooij", "Kun Zhang", "Jan Lemeire", "Jakob Zscheischler", "Povilas Daniu\u0161is", "Bastian Steudel", "Bernhard Sch\u00f6lkopf"], "venue": "AIJ", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Causal Inference Using the Algorithmic Markov Condition", "author": ["D. Janzing", "B. Sch\u00f6lkopf"], "venue": "IEEE TIT 56,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Justifying Additive Noise Model-Based Causal Discovery via Algorithmic Information \u008ceory", "author": ["D. Janzing", "B. Steudel"], "venue": "OSID 17,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "\u008cree Approaches to the \u008bantitative De\u0080nition of Information", "author": ["A.N. Kolmogorov"], "venue": "Problemy Peredachi Informatsii", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1965}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Causal Inference on Discrete Data via Estimating Distance Correlations", "author": ["Furui Liu", "Laiwan Chan"], "venue": "Neur. Comp. 28,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Computing the Multinomial Stochastic Complexity in Sub-Linear Time", "author": ["Tommi Mononen", "Petri Myllym\u00e4ki"], "venue": "In PGM", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Distinguishing Cause from E\u0082ect Using Observational Data: Methods and Benchmarks", "author": ["Joris M. Mooij", "Jonas Peters", "Dominik Janzing", "Jakob Zscheischler", "Bernhard Sch\u00f6lkopf"], "venue": "JMLR 17,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Probabilistic latent variable models for distinguishing between cause and e\u0082ect", "author": ["J.M. Mooij", "O. Stegle", "D. Janzing", "K. Zhang", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Model selection by normalized maximum likelihood", "author": ["Jay I. Myung", "Daniel J. Navarro", "Mark A. Pi"], "venue": "J. Math. Psych. 50,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Causality: Models, Reasoning and Inference (2nd ed.)", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Identifying Cause and E\u0082ect on Discrete Data using Additive Noise Models", "author": ["J. Peters", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "In AISTATS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Causal Discovery with Continuous Additive Noise Models", "author": ["J. Peters", "JM. Mooij", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "JMLR", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Modeling by shortest data description", "author": ["Jorma Rissanen"], "venue": "Automatica 14,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1978}, {"title": "Strong optimality of the normalized ML models as universal codes and information in data", "author": ["Jorma Rissanen"], "venue": "IEEE TIT 47,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Bayesian network structure learning using factorized NML universal models", "author": ["T. Roos", "T. Silander", "P. Kontkanen", "P. Myllym\u00e4ki"], "venue": "In Proc. Information \u008aeory and Applications Workshop (ITA). IEEE", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Inference of Cause and E\u0082ect with Unsupervised Inverse Regression", "author": ["E. Sgouritsa", "D. Janzing", "P. Hennig", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A Linear Non-Gaussian Acyclic Model for Causal Discovery", "author": ["Shohei Shimizu", "Patrik O. Hoyer", "Aapo Hyv\u00e4rinen", "An\u008ai Kerminen"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Universal sequential coding of single messages", "author": ["Y.M. Shtarkov"], "venue": "Problems of Information Transmission 23,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1987}, {"title": "Kolmogorov\u2019s Structure functions and model selection", "author": ["N.K. Vereshchagin", "P.M.B. Vitanyi"], "venue": "IEEE TIT 50,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Causal Inference by Direction of Information", "author": ["Jilles Vreeken"], "venue": "In SDM", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "On the Identi\u0080ability of the Post-nonlinear Causal Model. In UAI", "author": ["Kun Zhang", "Aapo Hyv\u00e4rinen"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Testing whether linear equations are causal: A free probability theory approach", "author": ["J. Zscheischler", "D. Janzing", "K. Zhang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "1 INTRODUCTION Causal inference from observational data\u2014that is, identifying cause and e\u0082ect in data that was not collected through carefully controlled randomised trials\u2014is a fundamental problem in both business and science [18, 28].", "startOffset": 225, "endOffset": 233}, {"referenceID": 18, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 157, "endOffset": 169}, {"referenceID": 19, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 157, "endOffset": 169}, {"referenceID": 24, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 157, "endOffset": 169}, {"referenceID": 0, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 216, "endOffset": 222}, {"referenceID": 7, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 216, "endOffset": 222}, {"referenceID": 6, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 11, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 23, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 27, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 2, "context": "\u008cis means that even if the true data generating distribution does not reside in the model classM under consideration, we still obtain the optimal encoding for the data relative toM [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "1 Kolmogorov Complexity \u008ce Kolmogorov complexity of a \u0080nite binary string x is the length of the shortest binary program p\u2217 for a Universal Turing machineU that generates x , and then halts [10, 11].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "1 Kolmogorov Complexity \u008ce Kolmogorov complexity of a \u0080nite binary string x is the length of the shortest binary program p\u2217 for a Universal Turing machineU that generates x , and then halts [10, 11].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "I (y : x) + = I (x : y), where + = denotes equality up to an additive constant, and therefore also called algorithmic mutual information [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "\u008ce Kolmogorov complexity of a probability distribution P , K(P), is the length of the shortest program that outputs P(x) to precision q on input \u3008x ,q\u3009 [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 10, "context": "We refer the interested reader to Li & Vit\u00e1nyi [11] for more details on Kolmogorov complexity.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "Postulate 1 (independence of input and mechanism [25]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "\u008cis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25].", "startOffset": 119, "endOffset": 133}, {"referenceID": 8, "context": "\u008cis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25].", "startOffset": 119, "endOffset": 133}, {"referenceID": 23, "context": "\u008cis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25].", "startOffset": 119, "endOffset": 133}, {"referenceID": 6, "context": "[7] de\u0080ne independence in terms of information geometry.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Liu & Chan [12] formulate independence in terms of the distance correlation between marginal and conditional empirical distribution.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "Janzing & Sch\u00f6lkopf [8] formalise independence using algorithmic information theory, and postulate algorithmic independence of P(X ) and P(Y | X ).", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Postulate 2 (algorithmic independence of Markov kernels [8]).", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Postulate 2 is equivalent to saying that if X \u2192 Y , factorizing the joint distribution over X and Y into P(X ) and P(Y | X ), will lead to simpler \u2014 in terms of Kolmogorov complexity \u2014 models than factorizing it into P(Y ) and P(X | Y ) [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We can, for instance, approximate Kolmogorov complexity from above through lossless compression [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 63, "endOffset": 70}, {"referenceID": 20, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 63, "endOffset": 70}, {"referenceID": 2, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 163, "endOffset": 170}, {"referenceID": 26, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 163, "endOffset": 170}, {"referenceID": 20, "context": "1 Minimum Description Length Principle \u008ce Minimum Description Length (MDL) [21] principle is a practical version of Kolmogorov complexity.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "\u008ce MDL principle has its root in the two-part decomposition of the Kolmogorov complexity [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "It can be roughly described as follows [3].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "Unlike crude MDL, re\u0080ned MDL encodes D with the (entire) model classM, resulting in single one-part code L\u0304(D | M) [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Although the coding schemes are di\u0082erent across those codes, the resulting code lengths L\u0304(D | M) are almost the same [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 25, "context": "First, it gives a unique solution to the minimax problem posed by Shtarkov [27],", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "In other words, the NML distribution is the mini-max optimal universal model with respect to the model class [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Second, it also provides solution to another mini-max problem formulated by Rissanen [22], which is given by", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "However, we can approximate the normalising sum up to a \u0080nite \u0083oating-point precision in sub-linear time with respect to the data size n given precomputed counts hi [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 7, "context": "However, it has also a\u008aracted quite a lot of a\u008aention over the years [8, 17, 26, 28].", "startOffset": 69, "endOffset": 84}, {"referenceID": 16, "context": "However, it has also a\u008aracted quite a lot of a\u008aention over the years [8, 17, 26, 28].", "startOffset": 69, "endOffset": 84}, {"referenceID": 24, "context": "However, it has also a\u008aracted quite a lot of a\u008aention over the years [8, 17, 26, 28].", "startOffset": 69, "endOffset": 84}, {"referenceID": 16, "context": "Constraint-based approaches like conditional independence test [17, 28] are one of the widely used causal inference frameworks.", "startOffset": 63, "endOffset": 71}, {"referenceID": 5, "context": "\u008ce linear trace method [6, 32] infers linear causal relations of the form Y = AX , where A is the structure matrix that maps the cause to the e\u0082ect, using the linear trace condition.", "startOffset": 23, "endOffset": 30}, {"referenceID": 29, "context": "\u008ce linear trace method [6, 32] infers linear causal relations of the form Y = AX , where A is the structure matrix that maps the cause to the e\u0082ect, using the linear trace condition.", "startOffset": 23, "endOffset": 30}, {"referenceID": 1, "context": "\u008ce kernelized trace method [2] can infer non-linear causal relations, but requires the causal relation to be deterministic, functional, and invertible.", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "One of the key frameworks for causal inference are the Additive Noise Models (ANMs) [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 19, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 24, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 28, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 7, "context": "Algorithmic information theory provides a sound general theoretical foundation for causal inference [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "of the distributions P(X ) and P(Y | X ) [8].", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "It has also been used in justifying the additive noise model based causal discovery [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "For instance, the information-geometric approach [7] de\u0080nes independence via orthogonality in information space.", "startOffset": 49, "endOffset": 52}, {"referenceID": 23, "context": "Cure [25] de\u0080nes independence in terms of the accuracy of the estimations of P(Y | X ) and P(X | Y ).", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "Using algorithmic information theory, Vreeken [30] proposes a causal framework based on relative conditional complexity and instantiates it with cumulative entropy to infer the causal direction in continuous real-valued data.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "Budhathoki & Vreeken [1] propose a decision tree based approach for causal inference on univariate and multivariate binary data.", "startOffset": 21, "endOffset": 24}, {"referenceID": 18, "context": "[19] (dr) extend additive noise models to discrete data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Liu & Chan [12] (dc) de\u0080ne independence in terms of the distance correlation between empirical distributions P(X ) and P(Y | X ) to infer the causal direction from categorical data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "We compare cisc against Discrete Regression (dr) [19], and dc [12].", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "We compare cisc against Discrete Regression (dr) [19], and dc [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "[19], we sample X from the following distributions, using independently generated uniform noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "However, this happens in only few trivial instances [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "2 Benchmark Data Next we evaluate cisc on benchmark cause-e\u0082ect pairs with known ground truth [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 6, "context": "When we consider only those pairs where cisc is most decisive\u2014with a very high value of |S(X \u2192 Y ) \u2212 S(Y \u2192 X )|, it is 100% accurate on top 22% of the pairs, 80% accurate on top 45% of the pairs, which is on-par with the top-performing causal inference frameworks for continuous real-valued data [7, 25].", "startOffset": 296, "endOffset": 303}, {"referenceID": 23, "context": "When we consider only those pairs where cisc is most decisive\u2014with a very high value of |S(X \u2192 Y ) \u2212 S(Y \u2192 X )|, it is 100% accurate on top 22% of the pairs, 80% accurate on top 45% of the pairs, which is on-par with the top-performing causal inference frameworks for continuous real-valued data [7, 25].", "startOffset": 296, "endOffset": 303}, {"referenceID": 18, "context": "[19], we regard the data as discrete, and consider X \u2192 Y1, X \u2192 Y2, and X \u2192 Y3 as the ground truth as sex causes the size of the abalone and not the other way around.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "\u008ce proposed framework lays a clear computable foundation for algorithmic causal inference principle postulated by Janzing & Sch\u00f6lkopf [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 22, "context": "It would also be interesting to explore factorized normalized maximum likelihood models [23] to instantiate the framework for multivariate data [1].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "It would also be interesting to explore factorized normalized maximum likelihood models [23] to instantiate the framework for multivariate data [1].", "startOffset": 144, "endOffset": 147}], "year": 2017, "abstractText": "\u008ce algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identi\u0080ed as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable. We hence propose to do causal inference by stochastic complexity. \u008cat is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. \u008cis means that even in an adversarial se\u008aing, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class. We instantiate this framework, which we call cisc, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that cisc is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes.", "creator": "LaTeX with hyperref package"}}}