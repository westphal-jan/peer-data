{"id": "1702.06794", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing", "abstract": "error propagation is a common problem in nlp. reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes available are independently made early in learning a process. in this paper, we apply reinforcement learning mechanics to greedy dependency parsing procedures which is known to suffer from error propagation. reinforcement error learning improves accuracy of both labeled and unlabeled dependencies of the stanford neural dependency parser, a remarkably high performance greedy parser, while maintaining its efficiency. we investigate the portion of naive errors which are the original result of experimental error propagation and confirm that reinforcement learning reduces the previous occurrence of error propagation.", "histories": [["v1", "Wed, 22 Feb 2017 13:49:18 GMT  (620kb,D)", "http://arxiv.org/abs/1702.06794v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["minh le", "antske fokkens"], "accepted": false, "id": "1702.06794"}, "pdf": {"name": "1702.06794.pdf", "metadata": {"source": "CRF", "title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing", "authors": ["Minh L\u00ea", "Antske Fokkens"], "emails": ["m.n.le@vu.nl", "antske.fokkens@vu.nl"], "sections": [{"heading": "1 Introduction", "text": "Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013). It can occur when NLP tools applied early on in a pipeline make mistakes that have negative impact on higher-level tasks further down the pipeline. It can also occur within the application of a specific task, when sequential decisions are taken and errors made early in the process affect decisions made later on.\nWhen reinforcement learning is applied, a system actively tries out different sequences of actions. Most of these sequences will contain some errors. We hypothesize that a system trained in this manner will be more robust and less susceptible to error propagation.\nWe test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004),\nwhich have been popular because of superior efficiency and accuracy nearing state-of-the-art. They are also known to suffer from error propagation. Because they work by carrying out a sequence of actions without reconsideration, an erroneous action can exert a negative effect on all subsequent decisions. By rendering correct parses unreachable or promoting incorrect features, the first error induces the second error and so on. McDonald and Nivre (2007) argue that the observed negative correlation between parsing accuracy and sentence length indicates error propagation is at work.\nWe compare reinforcement learning to supervised learning on Chen and Manning (2014)\u2019s parser. This high performance parser is available as open source. It does not make use of alternative strategies for tackling error propagation and thus provides a clean experimental setup to test our hypothesis. Reinforcement learning increased both unlabeled and labeled accuracy on the Penn TreeBank and German part of SPMRL (Seddah et al., 2014). This outcome shows that reinforcement learning has a positive effect, but does not yet prove that this is indeed the result of reduced error propagation. We therefore designed an experiment which identified which errors are the result of error propagation. We found that around 50% of avoided errors were cases of error propagation in our best arc-standard system. Considering that 27% of the original errors were caused by error propagation, this result confirms our hypothesis.\nThis paper provides the following contributions:\n1. We introduce Approximate Policy Gradient (APG), a new algorithm that is suited for dependency parsing and other structured prediction problems.\n2. We show that this algorithm improves the accuracy of a high-performance greedy parser.\nar X\niv :1\n70 2.\n06 79\n4v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\n3. We design an experiment for analyzing error propagation in parsing.\n4. We confirm our hypothesis that reinforcement learning reduces error propagation.\nTo our knowledge, this paper is the first to experimentally show that reinforcement learning can reduce error propagation in NLP.\nThe rest of this paper is structured as follows. We discuss related work in Section 2. This is followed by a description of the parsers used in our experiments in Section 3. Section 4 outlines our experimental setup and presents our results. The error propagation experiment and its outcome are described in Section 5. Finally, we conclude and discuss future research in Section 6."}, {"heading": "2 Related Work", "text": "In this section, we address related work on dependency parsing, including alternative approaches for reducing error propagation, and reinforcement learning."}, {"heading": "2.1 Dependency Parsing", "text": "We use Chen and Manning (2014)\u2019s parser as a basis for our experiments. Their parser is opensource and has served as a reference point for many recent publications (Dyer et al., 2015; Weiss et al., 2015; Alberti et al., 2015; Honnibal and Johnson, 2015, among others). They provide an efficient neural network that learns dense vector representations of words, PoS-tags and dependency labels. This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy. They acknowledge the error propagation problem of greedy parsers, but leave addressing this through (e.g.) beam search for future work.\nDyer et al. (2015) introduce an approach that uses Long Short-Term Memory (LSTM). Their parser still works incrementally and the number of required operations grows linearly with the length of the sentence, but it uses the complete buffer, stack and history of parsing decisions, giving the model access to global information. Weiss et al. (2015) introduce several improvements on Chen and Manning (2014)\u2019s parser. Most importantly, they put a globally-trained perceptron layer instead of a softmax output layer. Their model uses\nsmaller embeddings, rectified linear instead of cubic activation function, and two hidden layers instead of one. They furthermore apply an averaged stochastic gradient descent (ASGD) learning scheme. In addition, they apply beam search and increase training data by using unlabeled data through the tri-training approach introduced by Li et al. (2014), which leads to further improvements.\nKiperwasser and Goldberg (2016) introduce a new way to represent features using a bidirectional LSTM and improve the results of a greedy parser. Andor et al. (2016) present a mathematical proof that globally normalized models are more expressive than locally normalized counterparts and propose to use global normalization with beam search at both training and testing.\nOur approach differs from all of the work mentioned above, in that it manages to improve results of Chen and Manning (2014) without changing the architecture of the model nor the input representation. The only substantial difference lies in the way the model is trained. In this respect, our research is most similar to training approaches using dynamic oracles (Goldberg and Nivre, 2012). Traditional static oracles can generate only one sequence of actions per sentence. A dynamic oracle gives all trajectories leading to the best possible result from every valid parse configuration. They can therefore be used to generate more training sequences including those containing errors. A drawback of this approach is that dynamic oracles have to be developed specifically for individual transition systems (e.g. arc-standard, arceager). Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Bj\u00f6rkelund and Nivre, 2015). In contrast, the reinforcement learning approach proposed in this paper is more general and can be applied to a variety of systems.\nZhang and Chan (2009) present the only study we are aware of that also uses reinforcement learning for dependency parsing. They compare their results to Nivre et al. (2006b) using the same features, but they also change the model and apply beam search. It is thus unclear to what extend their improvements are due to reinforcement learning.\nEven though most approaches mentioned above improve the results reported by Chen and Manning (2014) and even more impressive results on\ndependency parsing have been achieved since (notably, Andor et al. (2016)), Chen and Manning\u2019s parser provides a better baseline for our purposes. We aim at investigating the influence of reinforcement learning on error propagation and want to test this in a clean environment, where reinforcement learning does not interfere with other methods that address the same problem."}, {"heading": "2.2 Reinforcement Learning", "text": "Reinforcement learning has been applied to several NLP tasks with success, e.g. agenda-based parsing (Jiang et al., 2012), semantic parsing (Berant and Liang, 2015) and simultaneous machine translation (Grissom II et al., 2014). To our knowledge, however, none of these studies investigated the influence of reinforcement learning on error propagation.\nLearning to Search (L2S) is probably the most prominent line of research that applies reinforcement learning (more precisely, imitation learning) to NLP. Various algorithms, e.g. SEARN (Daum\u00e9 III et al., 2009) and DAgger (Ross et al., 2011), have been developed sharing common high-level steps: a roll-in policy is executed to generate training states from which a roll-out policy is used to estimate the loss of certain actions. The concrete instantiation differs from one algorithm to another with choices including a referent policy (static or dynamic oracle), learned policy, or a mixture of the two. Early work in L2S focused on reducing reinforcement learning into binary classification (Daum\u00e9 III et al., 2009), but newer systems favored regressors for efficiency (Chang et al., 2015, Supplementary material, Section B). Our algorithm APG is simpler than L2S in that it uses only one policy (pre-trained with standard supervised learning) and applies the existing classifier directly without reduction (the only requirement is that it is probabilistic). Nevertheless, our results demonstrate its effectiveness.\nAPG belongs to the family of policy gradient algorithms (Sutton et al., 1999), i.e. it maximizes the expected reward directly by following its gradient w.r.t. the parameters. The advantage of using a policy gradient algorithm in NLP is that gradientbased optimization is already widely used. REINFORCE (Williams, 1992; Ranzato et al., 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al., 1999).\nWe directly compare our approach to REINFORCE, whereas we leave a direct comparison to L2S for future work. Our experiments show that our algorithm results in lower variance and achieves better performance than REINFORCE.\nRecent work addresses the approximation of reinforcement learning gradient in the context of machine translation. Shen et al. (2016)\u2019s algorithm is roughly equivalent to the combination of an oracle and random sampling. Their approach differs from ours, because it does not retain memory across iteration as in our best-performing model (see Section 3.4)."}, {"heading": "2.3 Reinforcement and error propagation", "text": "As mentioned above, previous work that applied reinforcement learning to NLP has, to our knowledge, not shown that it improved results by reducing error propagation.\nWork on identifying the impact of error propagation in parsing is rare, Ng and Curran (2015) being a notable exception. They provide a detailed error analysis for parsing and classify which kind of parsing errors are involved with error propagation. There are four main differences between their approaches and ours. First, Ng and Curran correct arcs in the tree and our algorithm corrects decisions of the parsing algorithm. Second, our approach distinguishes between cases where one erroneous action deterministically leads to multiple erroneous arcs and cases where an erroneous action leads to conditions that indirectly result in further errors (see Section 5.1 for a detailed explanation). Third, Ng and Curran\u2019s algorithm corrects all erroneous arcs that are the same type of parsing error and point out that they cannot examine the interaction between multiple errors of the same type in a sentence. Our algorithm corrects errors incrementally and therefore avoids this issue. Finally, the classification and analysis presented in Ng and Curran (2015) are more extensive and detailed than ours. Our algorithm can, however, easily be extended to perform similar analysis. Overall, Ng and Curran\u2019s approach for error analysis and ours are complementary. Combining them and applying them to various systems would form an interesting direction for future work."}, {"heading": "3 A Reinforced Greedy Parser", "text": "This section describes the systems used in our experiments. We first describe the arc-standard al-\ngorithm, because familiarity with it helps to understand our error propagation analysis. Next, we briefly point out the main differences between the arc-standard algorithm and the alternative algorithms we experimented with (arc-eager and swapstandard). We then outline the traditional and our novel machine learning approaches. The features we used are identical to those described in Chen and Manning (2014). We are not aware of research identifying the best feature for a neural parser with arc-eager or swap-standard so we use the same features for all transition systems."}, {"heading": "3.1 Transition-Based Dependency Parsing", "text": "In an arc-standard system (Nivre, 2004), a parsing configuration consists of a triple \u3008\u03a3, \u03b2, A\u3009, where \u03a3 is a stack, \u03b2 is a buffer containing the remaining input tokens and A are the dependency arcs that are created during parsing process. At initiation, the stack contains only the root symbol (\u03a3 = [ROOT]), the buffer contains the tokens of the sentence (\u03b2 = [w1, ..., wn]) and the set of arcs is empty (A = \u2205).\nThe arc-standard system supports three transitions. When \u03c31 is the top element and \u03c32 the second element on the stack, and \u03b21 the first element of the buffer:1\nLEFTl adds an arc \u03c31 l\u2212\u2192 \u03c32 toA and removes \u03c32 from the stack. RIGHTl adds an arc \u03c32\nl\u2212\u2192 \u03c31 to A and removes \u03c31 from the stack.\nSHIFT moves \u03b21 to the stack.\nWhen the buffer is empty, the stack contains only the root symbol and A contains a parse tree, the configuration is completed. For a sentence of\n1Naturally, the transitions LEFTl and RIGHTl can only take place if the stack contains at least two elements and SHIFT can only occur when there is at least one element on the buffer.\nNw tokens, a full parse takes 2Nw + 1 transitions to complete (including the initiation). Figure 1 provides the gold parse tree for a (simplified) example from the Penn Treebank. The steps taken to create the dependencies between the sentence\u2019s head word hit and its subject and direct object are provided in Table 1.\nTo demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009). Arc-eager is designed for incremental parsing and included in the popular MaltParser (Nivre et al., 2006a). Swap-standard is a simple and effective solution to unprojective dependency trees. Because arc-eager does not guarantee complete parse trees, we used a variation that employs an action called UNSHIFT to resume processing of tokens that would otherwise not be attached to a head (Nivre and Fern\u00e1ndezGonz\u00e1lez, 2014)."}, {"heading": "3.2 Training with a Static Oracle", "text": "In transition-based dependency parsing, it is common to convert a dependency treebank D 3 (x, y) into a collection of input features s \u2208 S and corresponding gold-standard actions a \u2208 A for training, using a static oracle O. In Chen and Manning (2014), a neural network works as a function mapping input features to probabilities of actions: fNN : S \u00d7 A \u2192 [0, 1]. The neural network is trained to minimize negative log-likelihood loss\non the converted treebank:\nL = \u2211\n(x,y)\u2208D \u2211 (s,a)\u2208O(x,y) \u2212 log fNN (s, a; \u03b8) (1)"}, {"heading": "3.3 Reinforcement Learning", "text": "Following Maes et al. (2009), we view transitionbased dependency parsing as a deterministic Markov Decision Process. The problem is summarized by a tuple \u3008S,A, T , r\u3009 where S is the set of all possible states, A contains all possible actions, T is a mapping S\u00d7A \u2192 S called transition function and r : S \u00d7A \u2192 R is a reward function.\nA state corresponds to a configuration and is summarized into input features. Possible actions are defined for each transition system described in Section 3.1. We keep the training approach simple by using only one reward r(y\u0304) at the end of each parse.\nGiven this framework, a stochastic policy guides our parser by mapping each state to a probabilistic distribution of actions. During training, we use function fNN described in Section 3.2 as a stochastic policy. At test time, actions are chosen in a greedy fashion following existing literature. We aim at finding the policy that maximizes the expected reward (or, equivalently, minimizes the expected loss) on the training dataset: maximize \u03b7 = \u2211\n(x,y)\u2208D \u2211 a1:m\u223cf r(y\u0304) m\u220f i=1 fNN (si, ai; \u03b8)\n(2) where a1:m is a sequence of actions obtained by following policy fNN until termination and s1:m are corresponding states (with sm+1 being the termination state)."}, {"heading": "3.4 Approximate Policy Gradient", "text": "Gradient ascent can be used to maximize the expected reward in Equation 2. The gradient of expected reward w.r.t. parameters is (note that dz = zd(log z)):\n\u2202\u03b7 \u2202\u03b8 = \u2211 (x,y)\u2208D \u2211 a1:m\u223cfNN r(y\u0304) m\u220f i=1 fNN (si, ai)\nm\u2211 i=1 \u2202 \u2202\u03b8 log fNN (si, ai; \u03b8)\n(3)\nBecause of the exponential number of possible trajectories, calculating the gradient exactly is not\npossible. We propose to replace it by an approximation (hence the name Approximate Policy Gradient) by summing over a small subset U of trajectories. Following common practice, we also use a baseline b(y) that only depends on the correct dependency tree. The parameter is then updated by following the approximate gradient: \u2206\u03b8 \u221d \u2211\n(x,y)\u2208D \u2211 a1:m\u2208U (r(y\u0304)\u2212 b(y)) m\u220f i=1 fNN (si, ai)\nm\u2211 i=1 \u2202 \u2202\u03b8 log fNN (si, ai; \u03b8)\n(4)\nInstead of sampling one trajectory at a time as in REINFORCE, Equation 4 has the advantage that sampling over multiple trajectories could lead to more stable training and higher performance. To achieve that goal, the choice of U is critical. We empirically evaluate three strategies:\nRL-ORACLE: only includes the oracle transition sequence.\nRL-RANDOM: randomly samples k distinct trajectories at each iteration. Every action is sampled according to fNN , i.e. preferring trajectories for which the current policy assigns higher probability.\nRL-MEMORY: samples randomly as the previous method but retains k trajectories with highest rewards across iterations in a separate memory. Trajectories are \u201cforgotten\u201d (removed) randomly with probability \u03c1 before each iteration.2\nIntuitively, trajectories that are more likely and produce higher rewards are better training examples. It follows from Equation 3 that they also bear bigger weight on the true gradient. This is the rationale behind RL-RANDOM and RL-ORACLE. For a suboptimal parser, however, these objectives sometimes work against each other. RLMEMORY was designed to find the right balance between them. It is furthermore important that the parser is pretrained to ensure good samples. Algorithm 1 illustrates the procedure of training a dependency parser using the proposed algorithms.\n2We assign a random number (drawn uniformly from [0, 1]) to each trajectory in memory and remove those assigned a number less than \u03c1.\nMemorySeqs\u2190 \u2205; foreach training batch b do\nforeach sentence s \u2208 b do OracleSeq \u2190 Oracle(s); SystemSeqs\u2190 (sample k parsing transition sequences for s);\nif RL-Oracle then ComputeGradients(OracleSeq); else if RL-Random then ComputeGradients(SystemSeqs);\nelse if RL-Memory then m\u2190MemorySeqs[s]; foreach q \u2208 m do\nif RandomNumber() < \u03c1 then Remove q from m;\nend end foreach q \u2208 SystemSeqs do\nif |m| < k then Insert q into m; else p\u2190 (sequence with smallest reward in m);\nif reward(q) > reward(p) then\nReplace p by q in m; end\nend ComputeGradients(m);\nend Perform one gradient descent step;\nend Algorithm 1: Training a dependency parser with approximate policy gradient."}, {"heading": "4 Reinforcement Learning Experiments", "text": "We first train a parser using a supervised learning procedure and then improve its performance using APG. We empirically tested that training a second time with supervised learning has little to no effect on performance."}, {"heading": "4.1 Experimental Setup", "text": "We use PENN Treebank 3 with standard split (training, development and test set) for our experiments with arg-standard and arg-eager. Because the swap-standard parser is mainly suited for nonprojective structures, which are rare in the PENN Treebank, we evaluate this parser on the German\nsection of the SPMRL dataset. For PENN Treebank, we follow Chen and Manning\u2019s preprocessing steps. We also use their pretrained model3 for arc-standard and train our own models in similar settings for other transition systems.\nFor reinforcement learning, we use AdaGrad for optimization. We do not use dropout because we observed that it destablized the training process. The reward r(y\u0304) is the number of correct labeled arcs (i.e. LAS multiplied by number of tokens).4 The baseline is fixed to half the number of tokens (corresponding to a 0.5 LAS score). As training takes a lot of time, we tried only few values of hyperparameters on the development set and picked k = 8 and \u03c1 = 0.01. 1,000 updates were performed (except for REINFORCE which was trained for 8,000 updates) with each training batch contains 512 randomly selected sentences. The Stanford dependency scorer5 was used for evaluation."}, {"heading": "4.2 Effectiveness of Reinforcement Learning", "text": "Table 2 displays the performance of different approaches to training dependency parsers. Although we used Chen and Manning (2014)\u2019s pretrained model and Stanford open-source software, the results of our baseline are slightly worse than what is reported in their paper. This could be due to minor differences in settings and does not affect our conclusions.\nAcross transition systems and two languages, APG outperforms supervised learning, verifying our hypothesis. Moreover, it is not simply because the learners are exposed to more examples than their supervised counterparts. RL-ORACLE\n3We use PTB_Stanford_params.txt.gz downloaded from http://nlp.stanford.edu/software/ nndep.shtml on December 30th, 2015.\n4Punctuation is not taken into account, following Chen and Manning (2014).\n5Downloaded from http://nlp.stanford.edu/ software/lex-parser.shtml.\nis trained on exactly the same examples as the standard supervised learning system (SL), yet it is consistently superior. This can only be explained by the superiority of the reinforcement learning objective function compared to negative log-likelihood.\nThe results support our hypothesis that APG is better than REINFORCE (abbreviated as RE in Table 2) as RL-MEMORY always outperforms the classical algorithm and the other two heuristics do in two out of three cases. The usefulness of training examples that contain errors is evident through the better performance of RL-RANDOM and RLMEMORY in comparison to RL-ORACLE.\nTable 3 shows the importance of samples for RL-RANDOM. The algorithm hurts performance when only one sample is used whereas training with two or more samples improves the results. The difference cannot be explained by the total number of observed samples because one-sample training is still worse after 8,000 iterations compared to a sample size of 8 after 1,000 iterations. The benefit of added samples is twofold: increased performance and decreased variance. Because these benefits saturate quickly, we did not test sample sizes beyond 32."}, {"heading": "5 Error Propagation Experiment", "text": "We hypothesized that reinforcement learning avoids error propagation. In this section, we describe our algorithm and the experiment that identifies error propagation in the arc-standard parsers."}, {"heading": "5.1 Error Propagation", "text": "Section 3.1 explained that a transition-based parser goes through the sentence incrementally and must select a transition from [SHIFT, LEFTl,\nRIGHTl] at each step. We use the term arc error to refer to an erroneous arc in the resulting tree. The term decision error refers to a transition that leads to a loss in parsing accuracy. Decision errors in the parsing process lead to one or more arc errors in the resulting tree. There are two ways in which a single decision error may lead to multiple arc errors. First, the decision can deterministically lead to more than one arc error, because (e.g.) an erroneously formed arc also blocks other correct arcs. Second, an erroneous parse decision changes some of the features that the model uses for future decisions and these changes can lead to further (decision) errors down the road.\nWe illustrate both cases using two incorrect derivations presented in Figure 2. The original gold tree is repeated in (A). The dependency graph in Figure 2 (B) contains three erroneous dependency arcs (indicated by dashed arrows). The first error must have occurred when the parser executed RIGHTamod creating the arc Big \u2192 Board. After this error, it is impossible to create the correct relations on\u2192 Board and Board\u2192 the. The wrong arcs Big\u2192 the and on\u2192 Big are thus all the result of a single decision error.\nFigure 2 (C) represents the dependency graph that is actually produced by our parser.6 It contains two erroneous arcs: hit \u2192 themselves and themselves\u2192 on. Table 4 provides a possible sequence of steps that led to this derivation, starting from the moment stocks was added to the stack (Step 4). The first error is introduced in Step 5\u2019, where hit combines with stocks before stocks has picked up its dependent themselves. At that point, themselves can no longer be combined with the right head. The proposition on, on the other hand, can\n6The example is a fragment of a more complex sentence consisting of 33 tokens. The parser does provide the correct output when is analyzes this sequence in isolation.\nstill be combined with the correct head. This error is introduced in Step 7\u2019, where the parser moves on to the stack rather than creating an arc from hit to themselves.7 There are thus two decision errors that lead to the arc errors in Figure 2 (C). The second decision error can, however, be caused indirectly by the first error. If a decision error causes additional decision errors later in the parsing process, we talk of error propagation. This cannot be known just by looking at the derivation."}, {"heading": "5.2 Examining the impact of decision errors", "text": "We examine the impact of individual decision errors on the overall parse results in our test set by combining a dynamic oracle and a recursive function. We use a dynamic oracle based on Goldberg et al. (2014) which gives us the overall loss at any point during the derivation. The loss is equal to the minimal number of arc errors that will have been made once the parse is complete. We can thus deduce how many arc errors are deterministically caused by a given decision error.\nThe propagation of decision errors cannot be determined by simply examining the increase in loss during the parsing process. We use a recursive function to identify whether a particular parse suffered from this. While parsing the sentence, we register which decisions lead to an increase in loss. We then recursively reparse the sentence correcting one additional decision error during each run until the parser produces the gold. If each erroneous decision has to be corrected in order to arrive at the gold, we assume the decision errors are\n7Note that technically, on can still become a dependent of hit, but this can only happen if on becomes the head of themselves which would also be an error.\nindependent of each other. If, on the other hand, the correction of a specific decision also fixes other decisions down the road, the original parse suffers from error propagation.\nThe results are presented in Table 5. Total Loss indicates the number of arc errors in the corpus, Dec. Errors the number of decision errors and Err. Prop. the number of decision errors that were the result of error propagation. This number was obtained by comparing the number of decision errors in the original parse to the number of decision errors that needed to be fixed to obtain the gold parse. If less errors had to be fixed than originally present, we counted the difference as error propagation. Note that fixing errors sometimes leads to new decision errors during the derivation. We also counted the cases where more decision errors needed to be fixed than were originally present and report them in Table 5.8\n8We ran an alternative analysis where we counted all cases where fixing one decision error in the derivation reduced the overall number of decision errors in the parse by more than one. Under this alternative analysis, similar reductions in the proportion of error propagation were observed for reinforcement learning.\nOn average, decision errors deterministically lead to more than one arc error in the resulting parse tree. This remains stable across systems (around 1.4 arc errors per decision error). We furthermore observe that the proportion of decision errors that are the result of error propagation has indeed reduced for all reinforcement learning models. Among the errors avoided by APG, 35.9% were propagated errors for RL-ORACLE, 48.9% for RL-RANDOM, and 51.9% for RLMEMORY. These percentages are all higher than the proportion of propagated errors occurring in the corpus parsed by SL (27%). This outcome confirms our hypothesis that reinforcement learning is indeed more robust for making decisions in imperfect environments and therefore reduces error propagation."}, {"heading": "6 Conclusion", "text": "This paper introduced Approximate Policy Gradient (APG), an efficient reinforcement learning algorithm for NLP, and applied it to a highperformance greedy dependency parser. We hypothesized that reinforcement learning would be more robust against error propagation and would hence improve parsing accuracy.\nTo verify our hypothesis, we ran experiments applying APG to three transition systems and two languages. We furthermore introduced an experiment to investigate which portion of errors were the result of error propagation and compared the output of standard supervised machine learning to reinforcement learning. Our results showed that: (a) reinforcement learning indeed improved parsing accuracy and (b) propagated errors were overrepresented in the set of avoided errors, confirming our hypothesis.\nTo our knowledge, this paper is the first to show experimentally that reinforcement learning can reduce error propagation in an NLP task. This result was obtained by a straight-forward implementation of reinforcement learning. Furthermore, we only applied reinforcement learning in the training phase, leaving the original efficiency of the model intact. Overall, we see the outcome of our experiments as an important first step in exploring the possibilities of reinforcement learning for tackling error propagation.\nRecent research on parsing has seen impressive improvement during the last year achieving UAS around 94% (Andor et al., 2016). This improve-\nment is partially due to other approaches that, at least in theory, address error propagation, such as beam search. Both the reinforcement learning algorithm and the error propagation study we developed can be applied to other parsing approaches. There are two (related) main questions to be addressed in future work in the domain of parsing. The first addresses whether our method is complementary to alternative approaches and could also improve the current state-of-the-art. The second question would address the impact of various approaches on error propagation and the kind of errors they manage to avoid (following Ng and Curran (2015)).\nAPG is general enough for other structured prediction problems. We therefore plan to investigate whether we can apply our approach to other NLP tasks such as coreference resolution or semantic role labeling and investigate if it can also reduce error propagation for these tasks.\nThe source code of all experiments is publicly available at https://bitbucket.org/ cltl/redep-java."}, {"heading": "Acknowledgments", "text": "The research for this paper was supported by the Netherlands Organisation for Scientific Research (NWO) via the Spinoza-prize Vossen projects (SPI 30-673, 2014-2019) and the VENI project Reading between the lines (VENI 275-89-029). Experiments were carried out on the Dutch national e-infrastructure with the support of SURF Cooperative. We would like to thank our friends and colleagues Piek Vossen, Roser Morante, Tommaso Caselli, Emiel van Miltenburg, and Ngoc Do for many useful comments and discussions. We would like to extend our thanks the anonymous reviewers for their feedback which helped improving this paper. All remaining errors are our own."}], "references": [{"title": "Improved Transition-Based Parsing and Tagging with Neural Networks", "author": ["David Weiss", "Greg Coppola", "Slav Petrov"], "venue": "EMNLP", "citeRegEx": "Alberti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Globally Normalized Transition-Based Neural Networks. arXiv.org, cs.CL", "author": ["Andor et al.2016] Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": null, "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Imitation Learning of Agenda-based Semantic Parsers", "author": ["Berant", "Liang2015] Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2015}, {"title": "Non-Deterministic Oracles for Unrestricted Non-Projective Transition-Based Dependency Parsing", "author": ["Bj\u00f6rkelund", "Nivre2015] Anders Bj\u00f6rkelund", "Joakim Nivre"], "venue": "IWPT", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2015}, {"title": "Learning to search better than your teacher", "author": ["Chang et al.2015] Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Search-based Structured Prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "The Necessity of Parsing for Predicate Argument Recognition", "author": ["Gildea", "Palmer2002] Daniel Gildea", "Martha Palmer"], "venue": "ACL", "citeRegEx": "Gildea et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 2002}, {"title": "A Dynamic Oracle for Arc-Eager Dependency Parsing", "author": ["Goldberg", "Nivre2012] Yoav Goldberg", "Joakim Nivre"], "venue": "COLING", "citeRegEx": "Goldberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2012}, {"title": "Training Deterministic Parsers with Non-Deterministic Oracles", "author": ["Goldberg", "Nivre2013] Yoav Goldberg", "Joakim Nivre"], "venue": "In TACL 2013,", "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "A tabular method for dynamic oracles in transition-based parsing", "author": ["Francesco Sartorio", "Giorgio Satta"], "venue": "In TACL 2014,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "A Polynomial-Time Dynamic Oracle for Non-Projective Dependency Parsing", "author": ["GomezRodriguez", "Francesco Sartorio", "Giorgio Satta."], "venue": "EMNLP 2014, pages 917\u2013927. ACL.", "citeRegEx": "GomezRodriguez et al\\.,? 2014", "shortCiteRegEx": "GomezRodriguez et al\\.", "year": 2014}, {"title": "Don\u2019t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation", "author": ["Jordan Boyd-Graber", "He He", "John Morgan", "Hal Daume III"], "venue": null, "citeRegEx": "II et al\\.,? \\Q2014\\E", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Effects of parsing errors on pre-reordering performance for Chinese-to-Japanese", "author": ["Han et al.2013] Dan Han", "Pascual Mart\u00ednez-G\u00f3mez", "Yusuke Miyao", "Katsuhito Sudoh", "Masaaki Nagata"], "venue": "SMT. PACLIC", "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "An Improved Non-monotonic Transition System for Dependency Parsing", "author": ["Honnibal", "Johnson2015] Matthew Honnibal", "Mark Johnson"], "venue": "EMNLP", "citeRegEx": "Honnibal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Honnibal et al\\.", "year": 2015}, {"title": "Learned Prioritization for Trading Off Accuracy and Speed", "author": ["Jiang et al.2012] Jiarong Jiang", "Adam Teichert", "Hal Daum\u00e9 III", "Jason Eisner"], "venue": "ICML workshop on Inferning: Interactions between Inference and Learning,", "citeRegEx": "Jiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2012}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. CoRR, abs/1603.0", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing", "author": ["Li et al.2014] Zhenghua Li", "Min Zhang", "Wenliang Chen"], "venue": "ACL", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Structured prediction with reinforcement learning", "author": ["Maes et al.2009] Francis Maes", "Ludovic Denoyer", "Patrick Gallinari"], "venue": "Machine Learning,", "citeRegEx": "Maes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maes et al\\.", "year": 2009}, {"title": "Characterizing the Errors of Data-Driven Dependency Parsing Models", "author": ["McDonald", "Nivre2007] Ryan McDonald", "Joakim Nivre"], "venue": "EMNLP-CoNLL", "citeRegEx": "McDonald et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2007}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "HLT-EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Identifying Cascading Errors using Constraints in Dependency Parsing", "author": ["Ng", "Curran2015] Dominick Ng", "James R Curran"], "venue": "In ACL-IJCNLP,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Arc-eager Parsing with the Tree Constraint", "author": ["Nivre", "Daniel Fern\u00e1ndez-Gonz\u00e1lez"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2014}, {"title": "MaltParser: A data-driven parsergenerator for dependency parsing", "author": ["Nivre et al.2006a] Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In LREC 2006,", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Labeled pseudo-projective dependency parsing with support vector machines", "author": ["Nivre et al.2006b] Joakim Nivre", "Johan Hall", "Jens Nilsson", "G\u00fcl\u015fen Eryi\u011fit", "Svetoslav Marinov"], "venue": "In CoNLL", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "MaltParser: A language-independent system for datadriven dependency parsing", "author": ["Nivre et al.2007] Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "Eryi\u01e7it G\u00fcl\u015fen", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi"], "venue": "Natural Language En-", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "An Efficient Algorithm for Projective Dependency Parsing", "author": ["Joakim Nivre"], "venue": "IWPT", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in Deterministic Dependency Parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Non-projective Dependency Parsing in Expected Linear Time", "author": ["Joakim Nivre"], "venue": "ACLIJCNLP", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "The impact of parse quality on syntactically-informed statistical machine translation", "author": ["Quirk", "Corston-Oliver2006] Chris Quirk", "Simon Corston-Oliver"], "venue": "EMNLP", "citeRegEx": "Quirk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2006}, {"title": "Sequence Level Training with Recurrent Neural Networks", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "A Reduction of Imitation Learning and Structured Prediction to NoRegret Online Learning", "author": ["Ross et al.2011] Stephane Ross", "Geoffrey J Gordon", "J Andrew Bagnell"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Introducing the SPMRL 2014 Shared Task on Parsing Morphologically-Rich Languages", "author": ["Seddah et al.2014] Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty"], "venue": "In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Lan-", "citeRegEx": "Seddah et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["Shen et al.2016] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "ACL", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors", "author": ["Song et al.2012] Hyun-Je Song", "Jeong-Woo Son", "TaeGil Noh", "Seong-Bae Park", "Sang-Jo Lee"], "venue": "ACL", "citeRegEx": "Song et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["David Mcallester", "Satinder Singh", "Yishay Mansour"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "ACL-IJCNLP", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Statistical Dependency Analysis with Support Vector Machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proceedings of IWPT,", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "Joint Inference for Fine-grained Opinion Extraction", "author": ["Yang", "Cardie2013] Bishan Yang", "Claire Cardie"], "venue": "ACL", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Dependency Parsing with Energybased Reinforcement Learning", "author": ["Zhang", "Chan2009] Lidan Zhang", "Kwok Ping Chan"], "venue": "IWPT", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 35, "context": "Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013).", "startOffset": 57, "endOffset": 173}, {"referenceID": 14, "context": "Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013).", "startOffset": 57, "endOffset": 173}, {"referenceID": 28, "context": "We test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004), which have been popular because of superior efficiency and accuracy nearing state-of-the-art.", "startOffset": 93, "endOffset": 134}, {"referenceID": 27, "context": "We test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004), which have been popular because of superior efficiency and accuracy nearing state-of-the-art. They are also known to suffer from error propagation. Because they work by carrying out a sequence of actions without reconsideration, an erroneous action can exert a negative effect on all subsequent decisions. By rendering correct parses unreachable or promoting incorrect features, the first error induces the second error and so on. McDonald and Nivre (2007) argue that the observed negative correlation between parsing accuracy and sentence length indicates error propagation is at work.", "startOffset": 122, "endOffset": 593}, {"referenceID": 33, "context": "Reinforcement learning increased both unlabeled and labeled accuracy on the Penn TreeBank and German part of SPMRL (Seddah et al., 2014).", "startOffset": 115, "endOffset": 136}, {"referenceID": 26, "context": "This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 21, "context": ", 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy.", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "Andor et al. (2016) present a mathematical proof that globally normalized models are more expressive than locally normalized counterparts and propose to use global normalization with beam search at both training and testing.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Bj\u00f6rkelund and Nivre, 2015).", "startOffset": 81, "endOffset": 214}, {"referenceID": 23, "context": "They compare their results to Nivre et al. (2006b) using the same features, but they also change the model and apply beam search.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": "dependency parsing have been achieved since (notably, Andor et al. (2016)), Chen and Manning\u2019s parser provides a better baseline for our purposes.", "startOffset": 54, "endOffset": 74}, {"referenceID": 16, "context": "agenda-based parsing (Jiang et al., 2012), semantic parsing (Berant and Liang, 2015) and simultaneous machine translation (Grissom II et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 32, "context": ", 2009) and DAgger (Ross et al., 2011), have been developed sharing common high-level steps: a roll-in policy is executed to generate training states from which a roll-out policy is used to estimate the loss of certain actions.", "startOffset": 19, "endOffset": 38}, {"referenceID": 36, "context": "APG belongs to the family of policy gradient algorithms (Sutton et al., 1999), i.", "startOffset": 56, "endOffset": 77}, {"referenceID": 38, "context": "REINFORCE (Williams, 1992; Ranzato et al., 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al.", "startOffset": 10, "endOffset": 48}, {"referenceID": 31, "context": "REINFORCE (Williams, 1992; Ranzato et al., 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al.", "startOffset": 10, "endOffset": 48}, {"referenceID": 36, "context": ", 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al., 1999).", "startOffset": 110, "endOffset": 131}, {"referenceID": 34, "context": "Shen et al. (2016)\u2019s algorithm is roughly equivalent to the combination of an oracle and random sampling.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "In an arc-standard system (Nivre, 2004), a parsing configuration consists of a triple \u3008\u03a3, \u03b2, A\u3009, where \u03a3 is a stack, \u03b2 is a buffer containing the remaining input tokens and A are the dependency arcs that are created during parsing process.", "startOffset": 26, "endOffset": 39}, {"referenceID": 27, "context": "To demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009).", "startOffset": 119, "endOffset": 132}, {"referenceID": 29, "context": "To demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009).", "startOffset": 150, "endOffset": 163}, {"referenceID": 19, "context": "Following Maes et al. (2009), we view transitionbased dependency parsing as a deterministic Markov Decision Process.", "startOffset": 10, "endOffset": 29}, {"referenceID": 9, "context": "We use a dynamic oracle based on Goldberg et al. (2014) which gives us the overall loss at any point during the derivation.", "startOffset": 33, "endOffset": 56}, {"referenceID": 1, "context": "Recent research on parsing has seen impressive improvement during the last year achieving UAS around 94% (Andor et al., 2016).", "startOffset": 105, "endOffset": 125}, {"referenceID": 1, "context": "Recent research on parsing has seen impressive improvement during the last year achieving UAS around 94% (Andor et al., 2016). This improvement is partially due to other approaches that, at least in theory, address error propagation, such as beam search. Both the reinforcement learning algorithm and the error propagation study we developed can be applied to other parsing approaches. There are two (related) main questions to be addressed in future work in the domain of parsing. The first addresses whether our method is complementary to alternative approaches and could also improve the current state-of-the-art. The second question would address the impact of various approaches on error propagation and the kind of errors they manage to avoid (following Ng and Curran (2015)).", "startOffset": 106, "endOffset": 781}], "year": 2017, "abstractText": "Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.", "creator": "LaTeX with hyperref package"}}}