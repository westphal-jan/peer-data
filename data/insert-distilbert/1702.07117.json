{"id": "1702.07117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations", "abstract": "topic models have been widely used in discovering latent topics which are shared across documents in text mining. vector representations, contain word embeddings and topic size embeddings, map words and reference topics into a low - dimensional and dense partially real - value vector space, which have obtained overall high performance in nlp tasks. however, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. though some naive other models use the information trained from external large corpus to help improving smaller context corpus. in this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. an em - style algorithm framework is employed to iteratively optimize possible both topic model and vector representations. experimental results show that our model outperforms state - of - art methods required on various nlp tasks.", "histories": [["v1", "Thu, 23 Feb 2017 07:16:03 GMT  (96kb,D)", "http://arxiv.org/abs/1702.07117v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jarvan law", "hankz hankui zhuo", "junhua he", "erhu rong"], "accepted": false, "id": "1702.07117"}, "pdf": {"name": "1702.07117.pdf", "metadata": {"source": "CRF", "title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations", "authors": ["Jarvan Law", "Hankz Hankui Zhuo", "Junhua He", "Erhu Rong"], "emails": ["JarvanLaw@gmail.com,", "zhuohank@mail.sysu.edu.cn", "hejunh@mail2.sysu.edu.cn", "rongerhu@mail2.sysu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings, e.g., distributed word representations [Mikolov et al., 2013], represent words with low dimensional and dense real-value vectors, which capture useful semantic and syntactic features of words. Distributed word embeddings can be used to measure word similarities by computing distances between vectors, which have been widely used in various IR and NLP tasks, such as entity recognition [Turian et al., 2010], disambiguation [Collobert et al., 2011] and parsing [Socher et al., 2011; Socher et al., 2013]. Despite the success of previous approaches on word embeddings, they all assume each word has a specific meaning and represent each word with a single vector, which restricts their applications in fields with polysemous words, e.g., \u201cbank\u201d can be either \u201ca financial institution\u201d or \u201ca raised area of ground along a river\u201d.\nTo overcome this limitation, [Liu et al., 2015] propose a topic embedding approach, namely Topical Word Embeddings (TWE), to learn topic embeddings to characterize various meanings of polysemous words by concatenating\ntopic embeddings with word embeddings. Despite the success of TWE, compared to previous multi-prototype models [Reisinger and Mooney, 2010; Huang et al., 2012], it assumes that word distributions over topics are provided by off-theshelf topic models such as LDA, which would limit the applications of TWE once topic models do not perform well in some domains [Petterson et al., 2010; Phan et al., 2011]. As a matter of fact, pervasive polysemous words in documents would harm the performance of topic models that are based on co-occurrence of words in documents. Thus, a more realistic solution is to build both topic models with regard to polysemous words and polysemous word embeddings simultaneously, instead of using off-the-shelf topic models.\nIn this work, we propose a novel learning framework, called Latent Topical Skip-Gram (LTSG) model, to mutually learn polysemous-word models and topic models. To the best of our knowledge, this is the first work that considers learning polysemous-word models and topic models simultaneously. Although there have been approaches that aim to improve topic models based on word embeddings MRF-LDA [Xie et al., 2015], they fail to improve word embeddings provided words are polysemous; although there have been approaches that aim to improve polysemous-word models TWE [Liu et al., 2015] based on topic models, they fail to improve topic models considering words are polysemous. Different from previous approaches, we introduce a new node Tw, called global topic, to capture all of the topics regarding polysemous word w based on topic-word distribution \u03d5, and use the global topic to estimate the context of polysemous word w. Then we characterize polysemous word embeddings by concatenating word embeddings with topic embeddings. We illustrate our new model in Figure 1, where Figure 1(A) is the skip-gram model [Mikolov et al., 2013], which aims to maximize the probability of context c given word w, Figure 1(B) is the TWE model, which extends the skip-gram model to maximize the probability of context c given both word w and topic t, and Figure 1(C) is our LTSG model which aims to maximize the probability of context c given word w and global topic Tw. Tw is generated based on topic-word distribution \u03d5 (i.e., the joint distribution of topic embedding t and word embedding w) and topic embedding t (which is based on topic assignment z). Through our LTSG model, we can simultaneously learn word embeddingsw and global topic embeddings Tw for representing polysemous word embeddings, ar X iv :1 70 2. 07 11 7v 1\n[ cs\n.C L\n] 2\n3 Fe\nb 20\n17\nand topic word distribution \u03d5 for mining topics with regard to polysemous words. We will exhibit the effectiveness of our LTSG model in text classification and topic mining tasks with regard to polysemous words in documents.\nIn the remainder of the paper, we first introduce preliminaries of our LTSG model, and then present our LTSG algorithm in detail. After that, we evaluate our LTSG model by comparing our LTSG algorithm to state-of-the-art models in various datasets. Finally we review previous work related to our LTSG approach and conclude the paper with future work."}, {"heading": "2 Preliminaries", "text": "In this section, we briefly review preliminaries of Latent Dirichlet Allocation (LDA), Skip-Gram, and Topical Word Embeddings (TWE), respectively. We show some notations and their corresponding meanings in Table 1, which will be used in describing the details of LDA, Skip-Gram, and TWE."}, {"heading": "2.1 Latent Dirichlet Allocation", "text": "Latent Dirichlet Allocation (LDA) [Blei et al., 2003], a threelevel hierarchical Bayesian model, is a well-developed and widely used probabilistic topic model. Extending Probabilistic Latent Semantic Indexing (PLSI) [Hofmann, 1999], LDA adds Dirichlet priors to document-specific topic mixtures to overcome the overfitting problem in PLSI. LDA aims at modeling each document as a mixture over sets of topics, each associated with a multinomial word distribution. Given a docu-\nment corpus D, each document wm \u2208 D is assumed to have a distribution over K topics. The generative process of LDA is shown as follows,\n1. For each topic k = 1 \u2192 K, draw a distribution over words \u03d5k \u223c Dir(\u03b2)\n2. For each document wm \u2208 D,m \u2208 {1, 2, . . . ,M} (a) Draw a topic distribution \u03b8m \u223c Dir(\u03b1) (b) For each word wm,n \u2208 wm, n = 1, . . . , Nm\ni. Draw a topic assignment zm,n \u223c Mult(\u03b8m), zm,n \u2208 {1, . . . ,K}.\nii. Draw a word wm,n \u223cMult(\u03d5zm,n) where \u03b1 and \u03b2 are Dirichlet hyperparameters, specifying the nature of priors on \u03b8 and \u03d5. Variational inference and Gibbs sampling are the common ways to learn the parameters of LDA."}, {"heading": "2.2 The Skip-Gram Model", "text": "The Skip-Gram model is a well-known framework for learning word vectors [Mikolov et al., 2013]. Skip-Gram aims to predict context words given a target word in a sliding window, as shown in Figure 1(A).\nGiven a document corpus D defined in Table 1, the objective of Skip-Gram is to maximize the average log-probability\nL(D) = 1\u2211M m=1Nm M\u2211 m=1 Nm\u2211 n=1 \u2211 \u2212c\u2264j\u2264c,j 6=0\nlog Pr(wm,n+j |wm,n), (1)\nwhere c is the context window size of the target word. The basic Skip-Gram formulation defines Pr(wm,n+j |wm,n) using the softmax function:\nPr(wm,n+j |wm,n) = exp(vwm,n+j \u00b7 vwm,n)\u2211W w=1 exp(vw \u00b7 vwm,n) , (2)\nwhere vwm,n and vwm,n+j are the vector representations of target word wm,n and its context word wm,n+j , and W is the number of words in the vocabulary V . Hierarchical softmax and negative sampling are two efficient approximation methods used to learn Skip-Gram."}, {"heading": "2.3 Topical Word Embeddings", "text": "Topical word embeddings (TWE) is a more flexible and powerful framework for multi-prototype word embeddings, where topical word refers to a word taking a specific topic as context [Liu et al., 2015], as shown in Figure 1(B). TWE model employs LDA to obtain the topic distributions of document corpora and topic assginment for each word token. TWE model uses topic zm,n of target word to predict context word compared with only using the target word wm,n to predict context word in Skip-Gram. TWE is defined to maximize the following average log probability\nL(D) = 1\u2211M m=1Nm M\u2211 m=1 Nm\u2211 n=1 \u2211 \u2212c\u2264j\u2264c,j 6=0\nlog Pr(wm,n+j |wm,n) + log Pr(wm,n+j |zm,n).\n(3)\nTWE regards each topic as a pseudo word that appears in all positions of words assigned with this topic. When training TWE, Skip-Gram is being used for learning word embeddings. Afterwards, each topic embedding is initialized with the average over all words assigned to this topic and learned by keeping word embeddings unchanged.\nDespite the improvement over Skip-Gram, the parameters of LDA, word embeddings and topic embeddings are learned separately. In other word, TWE just uses LDA and Skip-Gram to obtain external knowledge for learning better topic embeddings.\n3 Our LTSG Algorithm Extending from the TWE model, the proposed Latent Topical Skip-Gram model (LTSG) directly integrates LDA and SkipGram by using topic-word distribution \u03d5 mentioned in topic models like LDA, as shown in Figure 1(C). We take three steps to learn topic modeling, word embeddings and topic embeddings simultaneously, as shown below.\nStep 1 Sample topic assignment for each word token. Given a specific word token wm,n, we sample its latent topic zm,n by performing Gibbs updating rule similar to LDA.\nStep 2 Compute topic embeddings. We average all words assigned to each topic to get the embedding of each topic.\nStep 3 Train word embeddings. We train word embeddings similar to Skip-Gram and TWE. Meanwhile, topicword distribution \u03d5 is updated based on Equation (10). The objective of this step is to maximize the following function\nL(D) = 1\u2211M m=1Nm M\u2211 m=1 Nm\u2211 n=1 \u2211 \u2212c\u2264j\u2264c,j 6=0\nlog Pr(wm,n+j |wm,n) + log Pr(wm,n+j |Twm,n), (4)\nwhere Twm,n = K\u2211\nk=1\ntk \u00b7\u03d5k,wm,n . tk indicates the k-th\ntopic embedding. Twm,n can be seen as a distributed representation of global topical word of wm,n.\nWe will address the above three steps in detail below."}, {"heading": "3.1 Topic Assignment via Gibbs Sampling", "text": "To perform Gibbs sampling, the main target is to sample topic assingments zm,n for each word token wm,n. Given all topic assignments to all of the other words, the full conditional distribution Pr(zm,n = k|z\u2212(m,n),w) is given below when applying collapsed Gibbs sampling [Griffiths and Steyvers, 2004],\nPr(zm,n = k|z\u2212(m,n),w) \u221d n \u2212(m,n) k,wm,n + \u03b2\u2211w w=1 n \u2212(m,n) k,w +W\u03b2\n\u00b7 n \u2212(m,n) m,k + \u03b1\u2211K\nk\u2032=1 n \u2212(m,n) m,k\u2032 +K\u03b1\n,\n(5)\nwhere \u2212(m,n) indicates that the current assignment of zm,n is excluded. nk,w and nm,k denote the number of word tokens w assigned to topic k and the count of word tokens in document m assinged to topic k, respectively. After sampling all the topic assignments for words in corpus D, we can estimate each component of \u03d5 and \u03b8 by Equations (6) and (7).\n\u03d5\u0302k,w = nk,w + \u03b2\u2211W\nw\u2032=1 nk,w\u2032 +W\u03b2 (6)\n\u03b8\u0302d,k = nm,k + \u03b1\u2211K\nk\u2032=1 nm,k\u2032 +K\u03b1 (7)\nUnlike standard LDA, the topic-word distribution \u03d5 is used directly for constructing the modified Gibbs updating rule in LTSG. Following the idea of DRS [Du et al., 2015], with the conjugacy property of Dirichlet and multinomial distributions, the Gibbs updating rule of our model LTSG can be approximately represented by\nPr(zm,n =k|w, z\u2212(m,n),\u03d5,\u03b1) \u221d\n\u03d5k,wm,n \u00b7 n \u2212(m,n) m,k + \u03b1\u2211K\nk\u2032=1 n \u2212(m,n) m,k\u2032 +K\u03b1\n. (8)\nIn different corpus or applications, Equation (8) can be replaced with other Gibbs updating rules or topic models, eg. LFLDA [Nguyen et al., 2015]."}, {"heading": "3.2 Topic Embeddings Computing", "text": "Topic embeddings aim to approximate the latent semantic centroids in vector space rather than a multinomial distribution. TWE trains topic embeddings after word embeddings\nhave been learned by Skip-Gram. In LTSG, we use a straightforward way to compute topic embedding for each topic. For the kth topic, its topic embedding is computed by averaging all words with their topic assignment z equivalent to k, i.e.,\ntk =\nM\u2211 m=1 Nm\u2211 n=1\nI(zm,n = k) \u00b7 vwm,n\u2211W w=1 nk,w\n(9)\nwhere I(x) is indicator function defined as 1 if x is true and 0 otherwise.\nSimilarly, you can design your own more complex training rule to train topic embedding like TopicVec [Li et al., 2016] and Latent Topic Embedding (LTE) [Jiang et al., 2016]."}, {"heading": "3.3 Word Embeddings Training", "text": "LTSG aims to update \u03d5 during word embeddings training. Following the similar optimization as Skip-Gram, hierarchical softmax and negative sampling are used for training the word embeddings approximately due to the computationally expensive cost of the full softmax function which is proportional to vocabulary size W . LTSG uses stochastic gradient descent to optimize the objective function given in Equation (4).\nThe hierarchical softmax uses a binary tree (eg. a Huffman tree) representation of the output layer with theW words as its leaves and, for each node, explicitly represents the relative probabilities of its child nodes. There is a unique path from root to each word w and node(w, i) is the i-th node of the path. Let L(w) be the length of this path, then node(w, 1) = root and node(w,L(w)) = w. Let child(u) be an arbitrary child of node u, e.g. left child. By applying hierarchical softmax on Pr(wm,n+j |Twm,n) similar to Pr(wm,n+j |wm,n) desciebed in Skip-gram [Mikolov et al., 2013], we can compute the log gradient of \u03d5 as follows,\n\u2202 log Pr(wm,n+j |Twm,n) \u2202\u03d5k=zm,n,w=wm,n = 1 L(wm,n \u2212 1) L(wm,n)\u22121\u2211 i=1[\n1\u2212 hwm,n+ji+1 \u2212 \u03c3(Twm,n \u00b7 v wm,n+j i ) ] tk \u00b7 v wm,n+j i ,\n(10)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)). Given a path from root to word wm,n+j constructed by Huffman tree, v wm,n+j i is the vector representation of i-th node. And hwm,n+ji+1 is the Huffman coding on the path defined as hwm,n+ji+1 = I ( node(wm,n+j , i+ 1) = child(node(wm,n+j , i) ) .\nFollow this idea, we can compute the gradients for updating the word w and non-leaf node. From Equation (10), we can see that \u03d5 is updated by using topic embeddings vk directly and word embeddings indirectly via the non-leaf nodes in Huffman tree, which is used for training the word embeddings.\n3.4 An overview of our LTSG algorithm In this section we provide an overview of our LTSG algorithm, as shown in Algorithm 1. In line 1 in Algorithm 1, we run the standard LDA with certain iterations and initialize \u03d5 based on Equation (6). From lines 4 to 6, there are the\nthree steps mentioned in section 3. From lines 7 to 13, \u03d5 will be updated after training the whole corpus D rather than per word, which is more suitable for multi-thread training. Function f(\u03be, nk,w) is a dynamic learning rate, defined by f(\u03be, nk,w) = \u03be \u00b7 log(nk,w)/nk,w. In line 16, document-topic distribution \u03b8m,k is computed to model documents.\nAlgorithm 1 Latent Topical Skip-Gram on Iterative Interactive Learning Framework Input: corpus D, # topics K, size of vocabulary W , Dirich-\nlet hyperparameters \u03b1, \u03b2, # iterations of LDA for initialization I , # iterations of framework IILF nItrs, # Gibbs sampling iterations nGS. Output: \u03b8m,k, \u03d5k,w, vw, tk, m = 1, 2, . . . ,M ; k = 1, 2, . . . ,K;w = 1, 2, . . . ,W\n1: Initialization. Initialize \u03d5k,w as in Equation (6) with I iterations in standard LDA as in Equation (5) 2: i\u2190 0 3: while (i < nItrs) do 4: Step 1. Sample zm,n as in Equation (8) with nGS\niterations 5: Step 2. Compute each topic embedding tk as in\nEquation (9) 6: Step 3. Train word embeddings with objective func-\ntion as in Equation (4) 7: Compute the first-order partial derivatives L\u2032(D) 8: Set the learning rate \u03be 9: for (k = 1\u2192 K) do\n10: for (w = 1\u2192W ) do 11: \u03d5(i+1)k,w \u2190 \u03d5 (i) k,w + f(\u03be, nk,w) \u2202L\u2032(D) \u2202\u03d5k,w 12: end for 13: end for 14: i\u2190 i+ 1 15: end while 16: Compute each \u03b8m,k as in Equation (7)"}, {"heading": "4 Experiments", "text": "In this section, we evaluate our LTSG model in three aspects, i.e., contextual word similarity, text classification, and topic coherence.\nWe use the dataset 20NewsGroup, which consists of about 20,000 documents from 20 different newsgroups. For the baseline, we use the default settings of parameters unless otherwise specified. Similar to TWE, we set the number of topic K = 80 and the dimensionality of both word embeddings and topic embeddings d = 400 for all the relative models. In LTSG, we initialize \u03d5 with I = 2500. We perform nItrs = 5 runs on our framework. We perform nGS = 200 Gibbs sampling iterations to update topic assignment with \u03b1 = 0.01, \u03b2 = 0.1."}, {"heading": "4.1 Contextual Word Similarity", "text": "To evaluate contextual word similarity, we use Stanford\u2019s Word Contextual Word Similarities (SCWS) dataset introduced by [Huang et al., 2012], which has been also used for\nevaluating state-of-art model [Liu et al., 2015]. There are totally 2,003 word pairs and their sential contexts. For comparison, we conpute the Spearman correlation similarity scores of different models and human judgments.\nFollowing the TWE model, we use two scores AvgSimC and MaxSimC to evaluate the multi-prototype model for contextual word similarity. The topic distribution Pr(z|w, c) will be infered by regarding c as a document using Pr(z|w, c) \u221d Pr(w|z) Pr(z|c). Given a pair of words with their contexts, namely (wi, ci) and (wj , cj), AvgSimC aims to measure the averaged similarity between the two words all over the topics:\nAvgSimC = \u2211\nz,z\u2032\u2208K Pr(z|wi, ci) Pr(z\u2032|wj , cj)S(vzwi ,v\nz\u2032 wj )\n(11) where vzw is the embedding of word w under its topic z by concatenating word and topic embeddings vzw = vw \u2295 tz . S(vzwi ,v z\u2032 wj ) is the cosine similarity between v z wi and v z\u2032\nwj . MaxSimC selects the corresponding topical word embedding vzw of the most probable topic z inffered using w in context c as the contextual word embedding, defined as\nMaxSimc = S(vzwi ,v z\u2032 wj ) (12)\nwhere z = argmaxz Pr(z|wi, ci), z\u2032 = argmaxz Pr(z|wj , cj). We consider the two baselines Skip-Gram and TWE. SkipGram is a well-known single prototype model and TWE is the state-of-the-art multi-prototype model. We use all the default settings in these two model to train the 20NewsGroup corpus.\nFrom Table 2, we can see that LTSG achieves better performance compared to the two competitive baseline. It shows that topic model can actually help improving polysemousword model, including word embeddings and topic embeddings."}, {"heading": "4.2 Text Classification", "text": "In this sub-section, we investigates the effectiveness of LTSG for document modeling using multi-class text classification. The 20NewsGroup corpus has been divided into training set and test set with ratio 60% to 40% for each category. We calculate macro-averaging precision, recall and F1-measure to measure the performance of LTSG.\nWe learn word and topic embeddings on the training set and then model document embeddings for both training set and testing set. Afterwards, we consider document embeddings as document features and train a linear classifier using Liblinear [Fan et al., 2008]. We use vm, tk, vw to represent\ndocument embeddings, topic embeddings, word embeddings, respectively, and model documents on both topic-based and embedding-based methods as shown below. \u2022 LTSG-theta. Document-topic distribution \u03b8m esti-\nmated by Equation (7). \u2022 LTSG-topic. vm = \u2211K\nk=1 \u03b8m,k \u00b7 tk. \u2022 LTSG-word. vm = (1/Nm) \u2211Nm n=1 vwm,n .\n\u2022 LTSG. vm = (1/Nm) \u2211Nm n=1 v zm,n wm,n , where contextual\nword is simply constructed by vzm,nwm,n = vwm,n \u2295 tzm,n . We consider the following baselines, bag-of-word (BOW) model, LDA, Skip-Gram and TWE. The BOW model represents each document as a bag of words and use TFIDF as the weighting measure. For the TFIDF model, we select top 50,000 words as features according to TFIDF score. LDA represents each document as its inferred topic distribution. In Skip-Gram, we build the embedding vector of a document by simply averaging over all word embeddings in the document. The experimental results are shown in Table 3.\nFrom Table 3, we can see that, for topic modeling, LTSGtheta and LTSG-topic perform better than LDA slightly. For word embeddings, LTSG-word significantly outperforms Skip-Gram. For topic embeddings using for multi-prototype word embeddings, LTSG also outperforms state-of-the-art baseline TWE. This verifies that topic modeling, word embeddings and topic embeddings can indeed impact each other in LTSG, which lead to the best result over all the other baselines."}, {"heading": "4.3 Topic Coherence", "text": "In this section, we evaluate the topics generated by LTSG on both quantitative and qualitative analysis. Here we follow the same corpus and parameters setting in section 4.2 for LSTG model.\nQuantitative Analysis Although perplexity (held-out likehood) has been widely used to evaluate topic models, [Chang et al., 2009] found that perplexity can be hardly to reflect the semantic coherence of individual topics. Topic Coherence metric [Mimno et al., 2011] was found to produce higher correlation with human judegments in assessing topic quality, which has become popular to evaluate topic models [Arora et al., 2013; Chen and Liu, 2014]. A higher topic coherence score indicates a more coherent topic.\nWe compute the score of the top 10 words for each topic. We present the score for some of topics in the last line of\nTable 4. By averaging the score of the total 80 topics, LTSG gets -92.23 compared with -108.72 of LDA. We can conclude that LTSG performs better than LDA in finding higher quality topics.\nQualitative Analysis Table 4 shows top 10 words of topics from LTSG and LDA model on 20NewsGroup. The words in this two models are ranked based on the probability distribution \u03d5 for each topic. As shown, LTSG is able to capture more concrete topics compared with general topics in LDA. For the topic about \u201cimage\u201d, LTSG shows about image convertion on different format, while LDA shows the image quality of different format. In topic \u201cprinter\u201d, LTSG emphasizes the different technique of printer in detail and LDA generally focus on \u201cgood quality\u201d of printing. In topic about \u201cmail\u201d, LTSG gives a way to build a mail server while LDA generally concerns about attribute of email."}, {"heading": "5 Releated Work", "text": "Rencently, researches on cooperating topic models and vector representations have made great advances in NLP community. [Xie et al., 2015] proposed a Markov Random Field regularized LDA model (MRF-LDA) to incorporate word similarities into topic modeling. The MRF-LDA model encourages similar words to share the same topic so as to learn more coherent topics. [Das et al., 2015] proposed Gaussian LDA to use pre-trained word embeddings in Gibbs sampler based on multivariate Gaussian distributions. [Nguyen et al., 2015] proposed LFLDA which is modeled as a mixture of the conventional categorical dirtribution and an embedding link function. These works have given the faith that vector representations are capable of helping improving topic models. On the contrary, vector representations, especially topic embeddings, have been promoted for modeling documents or polysemy with great help of topic models. For examples, [Liu et al., 2015] used topic model to globally cluster the words into different topics accroding to their context for learning better multi-prototype word embeddings. [Li et al., 2016] proposed generative topic embedding (TopicVec) model that replaces categorical distribution in LDA with embedding link function. However, these models do not show close interactions among topic models, word embeddings and topic em-\nbeddings. Besides, these researches lack of investigation on the influence of topic model on word embeddings."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we introduce a general framework to make topic models and vector representations mutually help each other. We propose a basic model Latent Topical Skip-Gram (LTSG) which shows that LDA and Skip-Gram can mutually help improve performance on different task. The experimental results show that LTSG achieves the competitive results compaired with the state-of-art models. Especially, we can make a conclusion that topic model helps promoting word embeddings in LTSG model.\nWe consider the following future research directions: I) The number of topics must be pre-defined and Gibbs sampling is time-consuming for training large-scale data with using single thread. we will investigate non-parametric topic models [Teh et al., 2006] and parallel topic models [Liu et al., 2011]. II) There are many topic models and word embeddings models have been proposed to use in various tasks and specific domains. We will construct a package which can be convenient to extend with other models to our framework by using the interfaces. III) LTSG could not deal with unseen words in new documents, we may explore techniques to train word embeddings and topic assigments for the unseen words like Gaussian LDA [Das et al., 2015]. IV) We wish to evaluate topic embeddings directly similar to topic coherence task."}], "references": [{"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Arora et al", "2013] Sanjeev Arora", "Rong Ge", "Yonatan Halpern", "David M. Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["Blei et al", "2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Chang et al", "2009] Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L Boyd-Graber", "David M Blei"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Topic modeling using topics from many domains, lifelong learning and big data", "author": ["Chen", "Liu", "2014] Zhiyuan Chen", "Bing Liu"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert et al", "2011] Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Gaussian LDA for topic models with word embeddings", "author": ["Das et al", "2015] Rajarshi Das", "Manzil Zaheer", "Chris Dyer"], "venue": "In Proceedings of ACL,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Topic modeling with document relative similarities", "author": ["Du et al", "2015] Jianguang Du", "Jing Jiang", "Dandan Song", "Lejian Liao"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan et al", "2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Finding scientific topics", "author": ["Griffiths", "Steyvers", "2004] Thomas L Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "[Hofmann,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al", "2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In ACL,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Latent topic embedding", "author": ["Jiang et al", "2016] Di Jiang", "Lei Shi", "Rongzhong Lian", "Hua Wu"], "venue": "In COLING,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Generative topic embedding: a continuous representation of documents", "author": ["Li et al", "2016] Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "Chunyan Miao"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "PLDA+: parallel latent dirichlet allocation with data placement and pipeline processing", "author": ["Liu et al", "2011] Zhiyuan Liu", "Yuzhou Zhang", "Edward Y. Chang", "Maosong Sun"], "venue": "ACM TIST,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Topical word embeddings", "author": ["Liu et al", "2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In AAAI,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Optimizing semantic coherence in topic models", "author": ["Mimno et al", "2011] David M. Mimno", "Hanna M. Wallach", "Edmund M. Talley", "Miriam Leenders", "Andrew McCallum"], "venue": "In EMNLP,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Improving topic models with latent feature word representations", "author": ["Nguyen et al", "2015] Dat Quoc Nguyen", "Richard Billingsley", "Lan Du", "Mark Johnson"], "venue": "TACL,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Word features for latent dirichlet allocation", "author": ["Petterson et al", "2010] James Petterson", "Alexander J. Smola", "Tib\u00e9rio S. Caetano", "Wray L. Buntine", "Shravan M. Narayanamurthy"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1921\\E", "shortCiteRegEx": "al. et al\\.", "year": 1921}, {"title": "A hidden topic-based framework toward building applications with short web documents", "author": ["Phan et al", "2011] Xuan Hieu Phan", "Cam-Tu Nguyen", "DieuThu Le", "Minh Le Nguyen", "Susumu Horiguchi", "QuangThuy Ha"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney", "2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher et al", "2011] Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["Socher et al", "2013] Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In ACL,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Hierarchical dirichlet processes", "author": ["Teh et al", "2006] Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al", "2010] Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Incorporating word correlation knowledge into topic modeling", "author": ["Xie et al", "2015] Pengtao Xie", "Diyi Yang", "Eric P. Xing"], "venue": "In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Extending Probabilistic Latent Semantic Indexing (PLSI) [Hofmann, 1999], LDA adds Dirichlet priors to document-specific topic mixtures to overcome the overfitting problem in PLSI.", "startOffset": 56, "endOffset": 71}], "year": 2017, "abstractText": "Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.", "creator": "LaTeX with hyperref package"}}}