{"id": "1702.07495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Dirichlet-vMF Mixture Model", "abstract": "this document is about expanding the multi - document von - mises - fisher mixture model with a dirichlet prior, referred to as vmfmix. vmfmix algorithm is analogous to latent posterior dirichlet allocation ( lda ) in that they can capture the co - occurrence patterns acorss multiple documents. the significant difference is that in vmfmix, the topic - important word distribution is clearly defined on applying a continuous n - dimensional hypersphere. hence vmfmix is used uniquely to derive topic embeddings, i. e., representative vectors, from multiple connected sets of data embedding vectors. an efficient variational expectation - maximization inference algorithm is derived. the performance of vmfmix on two document classification tasks is reported, with some preliminary analysis.", "histories": [["v1", "Fri, 24 Feb 2017 08:35:10 GMT  (4kb)", "http://arxiv.org/abs/1702.07495v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shaohua li"], "accepted": false, "id": "1702.07495"}, "pdf": {"name": "1702.07495.pdf", "metadata": {"source": "CRF", "title": "Dirichlet-vMF Mixture Model", "authors": ["Shaohua Li"], "emails": ["shaohua@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 49\n5v 1\n[ cs\n.C L\n] 2\nThis document is about the multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to derive topic embeddings, i.e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis.\nWe present a simplification of the Bayesian vMF mixture model proposed in [2]1. For computational efficiency, the priors on the vMF mean {\u00b5k} and on the vMF concentration {\u03bak} are removed. This model is referred to as VMFMix.\nA Python implementation of VMFMix is available at https://github.com/askerlee/vmfmix."}, {"heading": "1 Model Specification", "text": "The generative process is as follows:\n1. \u03b8i \u223c Dir(\u03b1);\n2. zij \u223c Cat(\u03b8i);\n3. xij \u223c vMF(\u00b5zij , \u03bazij ).\nHere \u03b1 is a hyperparameter, {\u00b5k, \u03bak} are parameters of mixture components to be learned.\n1This model reappears in [4] under the name \u201cmix-vMF topic model\u201d. But [4] only offers a sampling-\nbased inference scheme, which is usually less accurate than the EM algorithm presented in this document."}, {"heading": "2 Model Likelihood and Inference", "text": "Given parameters {\u00b5k, \u03bak}, the complete-data likelihood of a dataset {X,Z,\u0398} = {xij , zij , \u03b8i} is:\np(X,Z,\u0398|\u03b1, {\u00b5k, \u03bak}) = \u220f\ni\nDir(\u03b8i|\u03b1) \u220f\nj\n\u03b8i,zijvMF(xij |\u00b5zij , \u03bazij ). (1)\nThe incomplete-data likelihood of {X,\u0398} = {xij , \u03b8i} is obtained by integrating out the latent variablesZ,\u0398:\np(X|\u03b1, {\u00b5k, \u03bak}) =\n\u222b\nd\u0398 \u00b7 \u220f\ni\nDir(\u03b8i|\u03b1) \u220f\nj\n\u2211\nk\n\u03b8ikvMF(xij |\u00b5k, \u03bak). (2)\n(2) is apparently intractable, and instead we seek its variational lower bound:\nlog p(X|\u03b1, {\u00b5k, \u03bak}) \u2265 Eq(Z,\u0398)[log p(X,Z,\u0398|\u03b1, {\u00b5k, \u03bak})\u2212 log q(Z,\u0398)].\n= L(q, {\u00b5k, \u03bak}) (3)\nIt is natural to use the following variational distribution to approximate the posterior\ndistribution of Z,\u0398:\nq(Z,\u0398) = \u220f\ni\n{\nDir(\u03b8i|\u03c6i) \u220f\nj\nCat(zij |\u03c0ij) } . (4)\nThen the variational lower bound is\nL(q, {\u00b5k, \u03bak})\n=C0 +H(q) + Eq(Z,\u0398)\n[\n(\u03b1\u2212 1) \u2211\ni,k\nlog \u03b8ik\n+ \u2211\ni,j,k\n\u03b4(zij = k)(log \u03b8ik + log cd(\u03bak) + \u03bak\u00b5 \u22a4 kxij) ]\n=C0 +H(q) + \u2211\ni,k\n(\u03b1\u2212 1 + ni\u00b7k) ( \u03c8(\u03c6ik)\u2212 \u03c8(\u03c6i0) )\n+ \u2211\nk\n(\nn \u00b7\u00b7k \u00b7 log cd(\u03bak) + \u03bak\u00b5\n\u22a4 k rk\n)\n, (5)\nwhere\nni\u00b7k = \u2211\nj\n\u03c0ijk, n\u00b7\u00b7k = \u2211\ni,j\n\u03c0ijk, (6)\nrk = \u2211\ni,j\n\u03c0ijk \u00b7 xij , (7)\nandH(q) is the entropy of q(Z ,\u0398):\nH(q) =\u2212 Eq[log q(Z,\u0398)]\n= \u2211\ni\nEq\n[\n\u2211\nk\nlog \u0393(\u03c6ik)\u2212 log \u0393(\u03c6i0)\u2212 \u2211\nk\n(\u03c6ik \u2212 1) log \u03b8ik\n\u2212 \u2211\nj,k\n\u03b4(zij = k) log \u03c0ijk\n]\n= \u2211\ni\n(\n\u2211\nk\nlog \u0393(\u03c6ik)\u2212 log \u0393(\u03c6i0)\u2212 \u2211\nk\n(\u03c6ik \u2212 1)\u03c8(\u03c6ik) )\n+ (\u03c6i0 \u2212K)\u03c8(\u03c6i0)\u2212 \u2211\nj,k\n\u03c0ijk log \u03c0ijk . (8)\nBy taking the partial derivative of (5) w.r.t. \u03c0ijk , \u03c6ik,\u00b5k, \u03bak, respectively, we can\nobtain the following variational EM update equations [1, 2, 4].\n2.1 E-Step\n\u03c0ijk \u223c e \u03c8(\u03c6ik) \u00b7 vMF(xij |\u00b5k, \u03bak),\n\u03c6ik = ni\u00b7k + \u03b1. (9)\n2.2 M-Step\n\u00b5k = rk\n\u2016rk\u2016 ,\nr\u0304k = \u2016rk\u2016\nn..k ,\n\u03bak \u2248 r\u0304kD \u2212 r\u0304\n3 k\n1\u2212 r\u03042k . (10)\nThe update equation of \u03bak adopts the approximation proposed in [1]."}, {"heading": "3 Evaluation", "text": "The performance of this model was evaluated on two text classification tasks that are on 20 Newsgroups (20News) and Reuters, respectively. The experimental setup for the compared methods were identical to that in [3]. Similar to TopicVec, VMFMix learns an individual set of K topic embeddings from each category of documents, and all these sets are combined to form a bigger set of topic embeddings for the whole corpus. This set of topic embeddings are used to derive the topic proportions of each document, which are taken as features for the SVM classifier. TheK for 20News and Reuters are chosen as 15 and 12, respectively, which are identical to TopicVec.\nThe macro-averaged precision, recall and F1 scores of all methods are presented in\nTable 1.\nWe can see from Table 1 that, VMFMix achieves better performance than Doc2Vec, LDA, sLDA and LFTM. However, its performance is still inferior to BOW, Mean word embeddings (MeanWV), TWE and TopicVec. The reason might be that by limiting the embeddings in the unit hypersphere (effectively normalizing them as unit vectors), certain representational flexibility is lost.\nAn empirical observation we have is that, VMFMix approaches convergence very quickly. The variational lower bound increases only slightly after 10~20 iterations. By manually checking the intermediate parameter values, we see that after so many iterations, the parameters change very little too. It suggests that VMFMix might easily get stuck in local optima.\nNonetheless, VMFMix might still be relevant when the considered embedding vectors are infinite and continuously distributed in the embedding space, as opposed to the finite vocabulary of word embeddings2. Such scenarios include the neural encodings of images from a convolutional neural network (CNN)."}], "references": [{"title": "Clustering on the unit hypersphere using von mises-fisher distributions", "author": ["Arindam Banerjee", "Inderjit S Dhillon", "Joydeep Ghosh", "Suvrit Sra"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Von mises-fisher clustering models", "author": ["Siddharth Gopal", "Yiming Yang"], "venue": "In ICML, pages 154\u2013162,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Generative topic embedding: a continuous representation of documents. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016. Each set of word embeddings can be viewed as a finite and discrete sample from a continuous embedding space", "author": ["Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "ChunyanMiao"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Integrating topic modeling with word embeddings by mixtures of vmfs", "author": ["Ximing Li", "Jinjin Chi", "Changchun Li", "Jihong OuYang", "Bo Fu"], "venue": "In COLING,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "We present a simplification of the Bayesian vMF mixture model proposed in [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "This model reappears in [4] under the name \u201cmix-vMF topic model\u201d.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "But [4] only offers a samplingbased inference scheme, which is usually less accurate than the EM algorithm presented in this document.", "startOffset": 4, "endOffset": 7}], "year": 2017, "abstractText": "This document is about the multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to derive topic embeddings, i.e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis.", "creator": "LaTeX with hyperref package"}}}