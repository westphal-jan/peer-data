{"id": "1702.07752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "A supervised approach to time scale detection in dynamic networks", "abstract": "for any stream of time - stamped edges that form a dynamic network, an important choice is the aggregation granularity that an analyst uses to bin the data. picking such a windowing of the incomplete data is often done by hand, or wires left up connected to the technology that is collecting the data. however, the choice can make a big difference in resolving the interface properties of the dynamic network. this is the time scale detection problem. in previous work, this problem is often solved concurrently with a heuristic as an unsupervised measurement task. looking as an aggregation unsupervised problem, it is difficult to measure how well a given algorithm performs. in addition, we show that tracking the quality of the whole windowing is dependent on which task an analyst wants to perform on the network after windowing. doing therefore the random time scale detection problem should not be handled, independently from the program rest consists of capturing the analysis of the network.", "histories": [["v1", "Fri, 24 Feb 2017 20:45:02 GMT  (64kb,D)", "http://arxiv.org/abs/1702.07752v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["benjamin fish", "rajmonda s caceres"], "accepted": false, "id": "1702.07752"}, "pdf": {"name": "1702.07752.pdf", "metadata": {"source": "CRF", "title": "A supervised approach to time scale detection in dynamic networks", "authors": ["Benjamin Fish", "Rajmonda S. Caceres"], "emails": ["bfish3@uic.edu"], "sections": [{"heading": null, "text": "For any stream of time-stamped edges that form a dynamic network, an important choice is the aggregation granularity that an analyst uses to bin the data. Picking such a windowing of the data is often done by hand, or left up to the technology that is collecting the data. However, the choice can make a big difference in the properties of the dynamic network. This is the time scale detection problem. In previous work, this problem is often solved with a heuristic as an unsupervised task. As an unsupervised problem, it is difficult to measure how well a given algorithm performs. In addition, we show that the quality of the windowing is dependent on which task an analyst wants to perform on the network after windowing. Therefore the time scale detection problem should not be handled independently from the rest of the analysis of the network.\nWe introduce a framework that tackles both of these issues: By measuring the performance of the time scale detection algorithm based on how well a given task is accomplished on the resulting network, we are for the first time able to directly compare different time scale detection algorithms to each other. Using this framework, we introduce time scale detection algorithms that take a supervised approach: they leverage ground truth on training data to find a good windowing of the test data. We compare the supervised approach to previous approaches and several baselines on real data."}, {"heading": "1 Introduction", "text": "Much big data mining on social and other types of networks either requires information about dynamics or is improved by such information. Incorporating temporal information can improve the efficacy and lead to more detailed analyses.\nAs data collection becomes cheaper and easier, the rate at which the data is being collected is often orders of magnitude more frequent than the underlying system. The rate of the data collection process is typically a function of the technology used and not necessarily related to the evolution or dynamics of the network itself. Thus the data collection\n\u2217University of Illinois at Chicago, Department of Mathematics, Statistics, and Computer Science, bfish3@uic.edu \u2020MIT Lincoln Laboratory, Lexington, Massachussetts\nprocess makes a choice about the bin size - also called the resolution, aggregation granularity, or time scale of the dynamic network - that may not be the correct choice. Binning the data at courser resolutions may make it possible to distinguish between noisy local temporal orderings and critical temporal orderings in the network. Indeed, the time scale of the network strongly impacts what structures and dynamics may be observed in the network [3, 12, 17]. Moreover, the choice of time scale impacts the efficacy of data mining on networks [8]. Thus for any data mining task over dynamic networks, choosing the bin size is not only important, but a necessary choice the data scientist must make, and leaving it up to the data collection process is not ideal. This is the problem we take up in this paper, variously called time scale detection, generating graph snapshots, oversampling correction, temporal scale inference, aggregation granularity detection, or windowing detection. We will refer to it as windowing to emphasize the fact that the bin sizes - or windows - may not necessarily all be the same size.\nThis problem is marginally related to change point detection, which is the problem of detecting when the network changes drastically, inducing a segmentation of the sequence, and is only a matter of degrees away from windowing detection, which also asks for a segmentation. Implicitly, however, change point detection assumes the network is observed at the right time scale.\nTypically, the windowing problem is framed as an unsupervised task. If the goal is data exploration and it is unclear how we will use the data in the future, then unsupervised windowing may be sufficient. On the other end of the spectrum, if sufficient knowledge of the data is at hand, the time scale may be chosen manually to represent natural scales, such as the diurnal or weekly scales natural for dynamics among people. However, frequently, getting such domainknowledge through a data exploration phase or otherwise may be prohibitively difficult or expensive. Moreover, we will demonstrate that the best windowing often cannot be defined independently of the analytical task.\nIn this paper, we show that the time scale for a dynamic network depends on the task - the most appropriate choice of scale for predicting new links appearing in the network may be different than the appropriate choice for detecting change points in the network, for example. The intuition is that the information needed to predict new links is different than the\nar X\niv :1\n70 2.\n07 75\n2v 1\n[ cs\n.S I]\n2 4\nFe b\n20 17\ninformation needed to detect change points. This insight may appear quite intuitive, but it runs contrary to many approaches for problems typically framed as unsupervised.\nThis informs our approach towards windowing: we set up the windowing problem as a supervised machine learning task. For example, for detecting change points, we set aside earlier training data from the network with labeled change points. We then find the right time scale for this labeled data, and apply the time scale to new data to find the change points there. In other words, the windowing algorithm takes as a parameter the desired task, such as change point detection, and finds the right time scale for that task on the input data. Of course, the downside to this approach is that it requires training data, such as labeled change points on past data. However, we regard this as both a natural assumption and a necessary assumption: Many popular prediction and classification problems have some notion of ground truth. In addition, since the best time scale depends on the task anyway, we might as well tailor our time scale detection towards a particular goal.\nIn this paper, we consider link prediction, attribute prediction, and change point detection. We use these three tasks as popular examples of tasks analysts perform on dynamic networks that have some notion of ground truth.\nOur contributions are as follows: In addition to giving evidence that the best windowing of the data is task-dependent, we, for the first time, describe a simple framework for directly comparing the performance of windowing algorithms that leverages task-dependency. We also introduce windowing algorithms that takes a supervised-machine-learning approach. We compare our approach against several baselines and previous work. We demonstrate that this supervised approach is often superior to other approaches, but that like all supervised approaches in machine learning, its quality may be dependent on the quality of the training data."}, {"heading": "1.1 Previous Work", "text": "In numeric time series, this problem is often termed \u2018segmentation,\u2019 and segmentation of numeric times series has a long history which is outside the scope of this work; see [10] for an overview.\nFor dynamic networks, there has been some work related to our problem in the area of change point detection, which seeks to find points in time where the dynamic network has changed abruptly. Typical methods include using generative models of dynamic networks [16] or clustering similar time slices [2]. This literature is marginally different from the problem addressed in this paper because - while finding change points do implicitly segment the dynamic network - the goal of change point detection is not necessarily to find a good representation of the network for the purpose of binning each segment but rather just to find the points when the dynamic network undergoes significant change.\nAlso related is graph compression, which tries to find a representation of the dynamic network that minimizes the bits needed to store it while simultaneously retaining sufficient information about the network, under the general heading of graph summarization algorithms. See [14] for a survey of graph summarization techniques. This approach has been applied, more generally, to multi-layer networks, dynamic or not [6]. This problem is also slightly different from ours because we do not necessarily seek a small representation, merely an accurate representation for the task at hand.\nMeanwhile, previous work on our problem has focused either on task-independent heuristics or methods that attempt to optimize for a specific metric on graphs. Caceres et al. maps the dynamic network to a time series using a metric on graphs (such as the number of triangles in each graph) and then determines time scale by finding the scale at which the time series\u2019 compression ratio and variance is balanced [21]. Soundarajan et al. determines a windowing of the data with respect to some metric (such as the exponent of the degree distribution) by measuring when that metric has converged [20]. In our view, these approaches have the downside that they require a specific metric. For a given data set or task to accomplish on a data set, it is not necessarily clear what metric to choose. Darst et al. use a parameterfree approach that seeks to find an appropriate time scale by measuring the similarity between graphs using the Jaccard index [5]. Most closely related to the approach that we take in this paper, Fish and Caceres use the quality of the performance of link prediction algorithms to determine the best time scale [8]. These are parameter-free methods or, relatedly, methods that assume that there is some \u2018ground-truth\u2019 time scale via a generative model or the like, as in [3, 5, 8]. In this paper, we do not take this tactic because as we demonstrate, choice of time scale may be dependent on the task at hand, which functions as a parameter for the problem."}, {"heading": "1.2 Background", "text": "Consider a dynamic network over a fixed set of vertices V , represented as a stream of time-stamped edges. The goal of windowing is to segment this input stream of edges into (possibly overlapping) intervals to form a sequence of graphs H1, . . . , Hm, each graph representing all of the edges that occurred within each interval. Representing the input edge stream as a sequence of graphs G1,. . . ,GT , a window of size k is a sequence Gi, Gi+1, . . . , Gi+k\u22121. A windowing is a sequence of windows {G1, . . . , Gk1}, {Gk1 + 1, Gk1 + 2, . . . , Gk2}, . . . , {Gkm\u22121 , . . . , GT }. In general, it is possible to also consider windows that overlap, but in this paper we focus on non-overlapping windows. The resulting sequence is H1, . . . , Hm, where Hi = \u222akij=ki\u22121+1Gj . It is possible to consider other functions mapping a window to the resulting graph Hi, but we only consider the simple union in this paper. As a slight abuse of notation, we will refer to both the segmentation of the input graph sequence and the resulting\nsequence H1, . . . , Hm as a windowing.\nIf all windows have the same size w (except possibly the last window if the length of the sequence does not divide w), we refer to this as a uniform windowing and w the bin size, window size, or time scale. A window size of w = 1 represents the time scale of the collection process and a window size of w = T , where T is the duration of the observed network, means that all temporal information is ignored. The goal of this paper is to describe and evaluate windowing algorithms, i.e. algorithms for finding a windowing given an input sequence of graphs. As pointed out in [8, 20, 21], too small a window size may introduce noise and lack the structure necessary for analysis but too large a window size may lose important temporal information.\nOnce we have found a windowing H1, . . . , Hm, it can now be the input for any task that operates over a dynamic network. We consider three popular tasks: link prediction, attribute prediction, and change point detection.\nIn this paper, we consider a supervised approach: the best windowing is the windowing that maximizes the performance of the algorithm for a given task, which we will refer to as the task algorithm. This gives us a way to compare different windowing algorithms: we evaluate the performance of the windowing algorithm by evaluating the performance of the task algorithm on a test set. In general, we are given the edges of a dynamic network up to some time t as a training set, and performance is evaluated on the dynamic network from time t+ 1 onwards. The windowing algorithm gets ground truth on the training set, e.g. the change points that occurred, and the task algorithm uses only the windowed graph sequence to conduct its analysis, say finding the change points in the test set."}, {"heading": "2 Tasks", "text": "Given a candidate windowing, we evaluate its quality by performing a learning task algorithm on that windowing. We then evaluate the windowing by the performance of the task algorithm when using that windowing. We consider three task algorithms: link prediction, attribute prediction, and change point detection. We treat link prediction as an online task, and attribute prediction and change point detection as offline tasks. We do this to demonstrate windowing algorithms on both kinds of tasks. In what follows, we describe the algorithms we use for each task and how we judge their performances."}, {"heading": "2.1 Link prediction", "text": "In link prediction, the goal is to predict the edges that are most likely to appear in the future. In the online setting, at every time step, our goal is to predict the edges that will appear in the next time step (the next step in the initial input sequence before windowing).\nWhile there are many methods for link prediction (see [1] for a survey), the method we use is a simple scoring function that scores every pair of vertices by how likely an edge is to appear between them. In this paper, we use the Katz\u03b2 score, an efficient and well-performing score [13]. \u03b2 is a damping parameter that weights shorter paths exponentially higher than longer paths in the most recent graph in the input sequence. For our experiment results, we use \u03b2 = 0.005, which has been used before [8, 13].\nThe performance of the link prediction algorithm is evaluated using the AUC of the precision-recall curve, as recommended by [23], averaged over all predictions made, one set of predictions for each graph."}, {"heading": "2.2 Attribute prediction", "text": "We use the Time Varying Relational Classifier (TVRC) algorithm of Sharan and Neville [19] to determine the unknown value of a binary vertex attribute. As in their work, we assume attributes do not change over time. The goal is then to infer the missing attribute values by taking advantage of not only the known attribute values of the vertices but also by using the temporal information in the data. Their method is a Bayesian model that takes advantage of knowledge of attributes in a vertex\u2019s neighborhood. This model uses the time stamps to weight the influence each vertex has on its neighbors. In our implementation, we use add-one smoothing for categorical features and a Gaussian distribution for continuous features. The goal is then to find a windowing where TVRC builds the best performing model. TVRC requires a kernel for weighting the importance of edges - as they suggest, we use their exponential kernel (1 \u2212 \u03b8)t\u2212i\u03b8, where t is the total time, i the current time, and \u03b8 a hyperparameter controlling the rate of decay (for the sake of simplicity, we set \u03b8 = 1/2 without also trying to optimize this parameter).\nWe use a special form of leave-one-out testing to measure the performance of TVRC. In this setting, the target attribute of one of the vertices is removed from both training and testing, a model is trained with the training set, and gives a prediction for the value of the missing attribute using the test set. To make the problem harder and more realistic, instead of just removing one target attribute, we remove a whole batch of them at once, and use the trained model to predict the values of all of their target attributes. The vertex set is partitioned into batches using a batch size parameter b, and this is repeated for each batch. Once a prediction has been made for all batches, we measure the performance or TVRC as the standard AUC of the ROC curve."}, {"heading": "2.3 Change point detection", "text": "We use Graphscope, from Sun et al. [22]. Graphscope detects change points by estimating the times where segmenting the graph sequence at those times maximizes compressibility.\nSince our graphs are not bipartite, we make the necessary modifications to Graphscope so that it may be used in the non-bipartite case.\nIn order to evaluate a change point detection algorithm, we need a single score summing up how good a set of change points are, but to the best of our knowledge no such measure has been proposed in the literature, contrary to classification tasks, which frequently use AUC or other single values to summarize quality. We will need a single score rather than, say, a precision-recall curve, not only to evaluate, but also because our supervised windowing algorithm will need a single score to directly and automatically compare the quality of different window sizes.\nTo rectify this, we propose the following measure: Let t1, . . . , tk be the times of the ground-truth events, and s1, . . . , s` the times of the proposed events. Let n be the length of the sequence and \u03b4(x) the function that equals 1 if x is true and 0 otherwise. Peel and Clauset [16] propose the following notions of precision and recall:\nPrecision(d) = 1\n` \u2211 i \u03b4(inf j |si \u2212 tj | \u2264 d)\nRecall(d) = 1\nk \u2211 j \u03b4(inf i |si \u2212 tj | \u2264 d).\nWe will define the AUC of the precision-recall curve as the normalized volume under the curve given by Precision(d) and Recall(d) as d goes from 0 to n. Consider the set of distances {|si \u2212 tj | : i, j} \u222a {0, n}, ordered from smallest to largest as d1 < . . . < dm, so d1 = 0 and dm = n. Then PR-AUC is defined as\n1\nn m\u22121\u2211 i=1 (di+1 \u2212 di) \u00b7 Precision(di) \u00b7 Recall(di).\nFor the sake of completeness, if k = 0 or ` = 0, we define the PR-AUC to be 0. Note this is a [0, 1]-valued measure."}, {"heading": "3 Experimental Setup", "text": "We now describe how we test the performance of windowing algorithms. In the offline setting, we set aside a previous interval of the dynamic network for training, and the next interval for testing. The training interval includes groundtruth information. For example for change point detection, it includes any change points that occurred in that interval. A windowing algorithm may use this information to decide on a window size to use in the test set. A windowing algorithm, uniform or otherwise, is also allowed to see the edges in the test set to determine the windowing for the test set, but of course no ground truth information about the task.\nOnce the test set is windowed, we perform our task on the windowed data and measure its performance, as describe in Section 2. The windowing algorithm\u2019s score is then just the score that the task algorithm received. For each data set, we\nsplit it up into six consecutive intervals, and then do training and testing on consecutive intervals, where the previous test set becomes the next training set, so there are five total tests1. We do this in order to promote generalizability of our results. For change point detection, we merely average the scores over each of the tests. For attribute prediction, we use pooling: we take the AUC as described in Section 2 over all vertices in all test sets, instead of averaging the individual AUC\u2019s of each test set, because the population, i.e. the vertices, is the same in each test set.\nIn the online setting, a new graph from the initial input sequence is given to the windowing algorithm, and the windowing algorithm must make a decision as to how to incorporate the new graph into the windowing so far. After windowing, a prediction is made, and then the process is repeated. Since the link prediction algorithm we use only ranks the likelihood of an edge appearing rather than determining the number of new links, we only do this process for those time steps where at least one new edge appears. The score for the windowing algorithm is the average over all scores received for each prediction. As in the offline setting, we split each data set into six consecutive intervals and then do training and testing on each pair of intervals, where the testing phase is online."}, {"heading": "4 Data sets", "text": "We use five data sets: Enron, MIT Reality Mining, Badge, Hypertext09, and Haggle. We treat all of these as undirected dynamic networks. For both convenience and uniformity, we bin each of these data sets at a initial window size at a \u2018natural\u2019 size, i.e. a choice that a data analyst might make, such as an hour or a day. This is so that a window size of w = 1 represents a baseline representing how well a handchosen windowing would perform. We only consider window sizes at least as large as this baseline.\nEach of these are suitable for link prediction. Of these, Enron, Reality Mining, and Badge are equipped with vertex attributes in order to test attribute prediction, and of these, Enron and Reality Mining also have established change point information."}, {"heading": "4.1 Enron", "text": "This is an email network between employees of Enron Inc. from January 1999 to July 2002, during the period of Enron\u2019s market manipulation scandal and subsequent collapse [11]. We use the emails of 151 employees, where each edge is an email sent from one of those employees to another. Each vertex, representing an employee, have a binary attribute indicating whether the employee was a manager or not, and\n1For Reality Mining, on change point detection, we split it into five instead of six intervals, in order to have every training set contain at least one change point.\nfifty integer-valued attributes, which are the number of occurrences in each employee\u2019s outgoing emails of the top fifty words used in all emails (a list of stop words were excluded from the top fifty words). The attribute we test on for attribute learning is whether the employee was a manager or not, which we took from [4]. We use the change points from [16], which represent times when the emails undergo substantial shifts, such as the launch of Enron online and changes in the CEO position. The initial bin size is one day."}, {"heading": "4.2 Reality Mining", "text": "This is a proximity network of 90 MIT students and faculty using data taken from cell phones from September 2004 to May 2005 (we only use data up until the end of the academic year) [7]. We use as edges both phone calls between participants and whenever two participants are close to each other, detected using Bluetooth. Each of the participants filled out a survey about their cell phone usage, such as how much they use their cell phone, where they live etc., which we use as the categorical attributes for each vertex. The attribute we use for testing attribute prediction is whether they are part of the business school or the MIT Media Lab. Change points are taken from [16], which are the start and ends of vacations, semesters, etc. The initial bin size is one day."}, {"heading": "4.3 Badge", "text": "This is a proximity network of 23 employees at a data server configuration firm for a month (the name of the data set comes from the badges the employees wore to track their location at the workplace) [15]. Each edge represents when two employees are in close proximity to each other, representing an interaction. Each employee was assigned a certain number of tasks, and data about these tasks was recorded,\ne.g. average completion time, whether they took on a difficult task or not, etc. For attribute prediction, we predict whether or not they made an error in one of their tasks. The initial bin size is one hour."}, {"heading": "4.4 Haggle Infocomm", "text": "This is a proximity network consisting of interactions, recorded using Bluetooth, among attendees at an IEEE Infocomm conference over four days [18]. 41 attendees participated in this network. The initial bin size is 10 minutes."}, {"heading": "4.5 Hypertext09", "text": "This is another proximity network of attendees at the ACM Hypertext 2009 conference, held over three days [9]. Each vertex is one of the 113 attendees, and an edge represents a interaction between two attendees that was active for at least 20 seconds. The initial bin size is 10 minutes."}, {"heading": "5 Task dependence", "text": "Ideally, it would be nice to have just one windowing algorithm that performs well regardless of whether your goal is link prediction, attribute prediction, change point detection, or any other task. However, we demonstrate that this does not appear to be feasible while still maximizing the performance of the task algorithm.\nTo do this, we score each window size by the performance of each of the three task algorithms when the data set is windowed at that size, so we have a score representing the quality of the window size for each of the three tasks. Table 1 show the score for the ith task, if you chose the window size with the highest score for the jth task. (Scores are computed for each of the intervals as described in Section 3 and\nthe values in Table 1 are the average of the results for each interval). For example, choose the best window size for link prediction. The table shows that at that window size the other two tasks do not achieve as high a score as if you had chosen the best window size for those two tasks. This means that the windowing algorithm should choose a different window size for each of the tasks in order to maximize score.\nThis effect is not limited to just the top-scoring window size. More generally, the scores between two tasks do not positively correlate with each other, so that a higher score for a given window size on the first task does not necessarily mean a higher score for that window size on the second task. In order to demonstrate this, we use Spearman\u2019s correlation coefficient, which tests the monotonicity between two variables. A negative score indicates anti-correlation, a score of 0 indicates no correlation, and a positive score indicates positive correlation. Table 2 show the correlation coefficients and their associated p-values for each pair of tasks: most of them are either negative or close to zero, indicating little correlation between the tasks. This is further evidence that the quality of window sizes depend on the task, and hence that we should use the task as supervision for windowing."}, {"heading": "6 Windowing algorithms and baselines", "text": "We now describe the windowing algorithms that we will compare in both the offline and online settings. So that the supervised algorithms we introduce remain useful for not just the three tasks we consider, but any task, our algorithms do not attempt to take advantage of the particular nature of the task and corresponding algorithm at hand. In other words, we treat the task algorithms as black boxes. However, as we show in Section 7, the supervised approaches are still able to perform well despite this self-imposed constraint."}, {"heading": "6.1 Offline supervised algorithms", "text": "Our intuition is very simple: since we know what task we want to accomplish on the test set, we use the same task algorithm to learn the window size on the training set. In the offline setting, in order to try to prevent overfitting and to make the search space smaller, we only consider uniform windowings. This allows the algorithm to be very simple: For each window size, up to the length of the training set, window the training set at that size and use that as input for the task algorithm. Measure the performance of the task algorithm (remember we assume we have ground truth for the training set) and use that as the score for the window size. Window the test set with the window size that received the highest score. Of course, this means running the task algorithm O(T ) times (where T is the length of the training set), which is not particularly efficient. However, we make the assumption that since this is an offline setting, this blowup in running time in the training phase is not prohibitively large.\nWe will refer to this as the offline supervised method. Among the three tasks, the attribute prediction algorithm has an important difference, in so much as that it requires training data to build a model. We therefore need to make sure to decouple the training data for the model and the training data used to find the best window size. To do this, we split the training data into two, use the first half as the data for the model, and the second half of the training data to test out how well the model does when windowed at each window size. We do this by taking the vertices that still have the value of the target attribute and using the same process we use to test the quality of the attribute prediction on the test set: remove them in batches, build the model at each window size, and then test which value TVRC predicts on the second half of the training data. As described above, we then use the AUC as the quality of that window size.\nAlgorithm 1 Online windowing for link prediction\nParameters: Integers M and B, Link predictor L Initialize scoresw as the empty list for each new graph Gi do\nLet new window sizes include w for 1 \u2264 w < i if length(scoresw) < M Let best window sizes include the top B window sizes by average(scoresw) for w in new window sizes, best window sizes do\nLet Hw = H1, . . . , Hd i\u22121 w e be the windowing of G1, . . . , Gi\u22121 at window size w predicted links = L(Hw) Append AUC(new links in Gi, predicted links) to scoresw\nend for w\u2217 = arg maxw average(scoresw) Let Hw\u2217 = H1, . . . , Hd i\nw\u2217 e be the windowing of\nG1, . . . , Gi at window size w \u2217\nreturn L(Hw\u2217) end for"}, {"heading": "6.2 Online supervised algorithms", "text": "In the online setting, we could at each time step perform a similar procedure as in the offline case, leading to O(i) runs of the task algorithm at the ith step, for a total of O(T 2) times, where T is the total length of the sequence. This will often be prohibitively expensive. We introduce an approximate online windowing algorithm to deal with this issue. We illustrate with link prediction, as a natural example of an online task.\nEvery time we receive a new graph Gi, representing the edges that occurred in the next time step, we can test each window size w by binning the sequence so far at size w and then use the last graph in the windowed sequence to predict the edges that will appear in Gi. We then compare the\npredicted edges to the actual edges in Gi, producing an AUC score for that window size2. The window size w\u2217 chosen next to bin the sequence seen so far including Gi is the window size that maximizes the average of all scores for that window size so far. However, this still means testing O(i) window sizes at each time step, for a total of O(T 2) tests.\nTo decrease the number of tests, we instead use an approximate version of this where only some of the window sizes are tested, as described in Algorithm 1. This online algorithm is described explicitly for link prediction, but it is worth nothing that the same algorithm may in principle be used for any online task. Given hyperparameters to the algorithm M and B, we test a window size if it has either been tested fewer than M times, or if the average score so far ranks it amongst the top B window sizes. The intuition behind this approach is that a window size that has been performing badly will not suddenly become the best performing window size, and thus doesn\u2019t need to be tested. This requires only O(M \u00b7B \u00b7T ) total runs of the link prediction algorithm instead of O(T 2), where we think of as M and B as constants. In our experiments, we set M = B = 10 (We also demonstrate the effect of changing these hyperparameters in Section 7). We also test a weighted variant of Algorithm 1 (Weighted Algorithm 1) by instead using a weighted average of the scores for each window size, in order to privilege scores\n2For the sake of computational efficiency, we only score pairs of vertices with non-zero degree, since the Katz score will produce a score of 0 for all other pairs.\ncloser to the present than the past. We use an exponential weighting scheme, where the weight for the score tested on the jth graph where t graphs have been seen already is \u03b1t\u2212j . For the purpose of our experiments, we use \u03b1 = 1/2.\nWe also consider a version of this that stops generating new scores after the training period is over, and sticks with the best window size for the training data for all future time steps (Training only)."}, {"heading": "6.3 Other windowing algorithms and baselines", "text": "We compare against ADAGE, the method of Soundarajan et al. [20]. ADAGE needs a metric as a parameter, so we use the exponent of the degree distribution, which is the metric they use. We also compare against the Jaccard-indexbased method (Jaccard) of Darst et al. [5] and the entropybased method (Entropy) of De Domenico et al. [6]. Since this method allows for graphs with any types of layers, we treat each time step as a layer and modify their method to only allow adjacent time steps to be merged.\nWe also compare against several baselines: the first is always using a window size of w = 1, which as mentioned above, represents a \u2018hand-chosen\u2019 value (which we will refer to as the hand-chosen algorithm). This represents the window size that an expert might have chosen. The second baseline is the random algorithm (Random), which chooses a random windowing of the test set. In addition, for attribute prediction, we also consider the windowing that removes all temporal information, i.e. the window size that is always\nthe length of the test sequence (No time). The final baseline we consider is slightly more sophisticated: Consider a time series that assigns a real value to every graph in the sequence. For any such time series, we may compute its discrete Fourier transform (DFT). For frequency f , denote by xf the amplitude of that frequency. The score we assign w is the maximum magnitude |xf | of any frequency in the transform such that f rounds to w. Only windows where there is such a frequency are assigned scores, and we choose the window size with the maximum score (Fourier). In this paper, we consider the DFT under the commonly-used Hanning window where the metric is the number of edges in each graph. This serves as a proxy for the amount of activity at any given time. The DFT of this particular time series for dynamic networks has been used before, e.g. to study the Reality Mining data set [7].\nAll of these algorithms can be used in the offline setting, but only ADAGE, the hand-chosen baseline, and the random baseline can be used in the online setting."}, {"heading": "7 Results", "text": "Tables 3, 4, and 5 show our results. The supervised approach does do well, but there are certainly caveats. In link prediction, the \u2018hand-picked\u2019 window size is often the best performer, probably due to the fact that these are well-studied data sets with natural periodicities dictated by human activity, and those window sizes reflect that. However, in general, we will want to have windowing algorithms that do not rely on an analyst\u2019s knowledge of the data set: acquiring domain-specific knowledge like this can be time-consuming, expensive, or difficult. Outside of this approach, at least one of the supervised methods does the best for all of the data sets. The weighted version is the overall winner, with a caveat: like any supervised technique, overfitting is always going to be an issue. We surmise that this is why the unweighted version outperforms the weighted version on the Haggle data set.\nWe also test the effect of the hyperparameters B and M , shown in Figure 1. As would be expected, by increasing B or M (which increases the number of window sizes tested) performance is generally improved. This effect, however, is extremely weak on the Reality Mining data set. That and our ability to perform relatively well even for small values of B and M validates our choice to use small constant values of B and M to use for Algorithm 1 and forego any attempt to optimize their values in the course of the windowing algorithm.\nIn attribute prediction, the supervised approach is the best performing algorithm, although the absolute difference over the others is sometimes rather small. On the other hand, the supervised approach does not do well for change point prediction. We believe this is due to a limitation of supervised approaches: they depend on the quality of the training data. Given the sparsity of change points, each training set contains only a very small number of change points, making it difficult to distinguish between different window sizes. This issue separates change point detection from our attribute prediction problem, which has a much greater amount of training data to work with.\nOur approach also reveals other differences between the three tasks. Figure 2 shows the quality of every window size on each of the first four intervals of Reality Mining (we don\u2019t use the last interval because each interval needs a subsequent interval for testing attribute prediction). The scores for change point detection are extremely sensitive to window size, with small differences in size making a large difference, indicating Graphscope\u2019s sensitivity to window size. TVRC, on the other hand, is much more stable under changes to window size, with link prediction falling somewhere in the middle. Such sensitivity makes it much more difficult to speed up the process by only testing some of the windows sizes instead of all of them, as we do here. We leave for future work determining if there is a way to test a fewer number of window sizes and if the choice of algorithm for a given task impacts sensitivity to window size.\nOur supervised approach makes the basic assumption that a window size that works well in the past is more likely to work well in the future. To measure this, Figure 3 shows the difference between the two scores a window size got on consecutive intervals, averaged over all windows and consecutive intervals. Smaller differences mean that past data reflects future data more precisely. Figure 3 shows the difference was significantly larger for change point detection than the other two tasks, indicating that this assumption (at least when using Graphscope) is violated more under this task. This serves as evidence for why the supervised approach does not perform as well on change point detection."}, {"heading": "8 Conclusion and future work", "text": "In this paper, we have given evidence that windowing is taskdependent. Recognizing this, we have provided a simple and easy-to-use framework for directly comparing the quality of windowing algorithms and moreover, introduced supervised windowing algorithms that leverage our ability to test a windowing. Nonetheless, we leave for future work several challenges: Like any supervised machine learning, the quality of the learner depends on the quantity and quality of training data. Improving windowing algorithms in the face of environments with little training data remains an issue. Even with training data, this can be a computationally-expensive procedure if the windowing algorithm has to repeatedly invoke an expensive task algorithm. We leave for future work finding heuristics that approximate well how a windowing will perform for a given task. We also leave for future work if performance may be improved by considering other kinds of windowings besides the non-overlapping windowings focused on in this paper."}], "references": [{"title": "A survey of link prediction in social networks", "author": ["Mohammad Al Hasan", "Mohammed J. Zaki"], "venue": "In Social Network Data Analytics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "As time goes by: Discovering eras in evolving social networks", "author": ["Michele Berlingerio", "Michele Coscia", "Fosca Giannotti", "Anna Monreale", "Dino Pedreschi"], "venue": "In Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Persistence and periodicity in a dynamic proximity network", "author": ["Aaron Clauset", "Nathan Eagle"], "venue": "DIMACS Workshop on Computational Methods for Dynamic Interaction Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Segmentation and automated so- 9  cial hierarchy detection through email network analysis", "author": ["Germ\u00e1n Creamer", "Ryan Rowe", "Shlomo Hershkop", "Salvatore J Stolfo"], "venue": "In Advances in Web Mining and Web Usage Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Detection of timescales in evolving complex systems", "author": ["Richard K. Darst", "Clara Granell", "Alex Arenas", "Sergio G\u00f3mez", "Jari Saram\u00e4ki", "Santo Fortunato"], "venue": "arXiv preprint arXiv:1604.00758,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Structural reducibility of multilayer networks", "author": ["Manlio De Domenico", "Vincenzo Nicosia", "Alexandre Arenas", "Vito Latora"], "venue": "Nature Communications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Reality mining: sensing complex social systems", "author": ["Nathan Eagle", "Alex Sandy Pentland"], "venue": "Personal and Ubiquitous Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Handling oversampling in dynamic networks using link prediction", "author": ["Benjamin Fish", "Rajmonda S. Caceres"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "What\u2019s in a crowd? analysis of face-to-face behavioral networks", "author": ["Lorenzo Isella", "Juliette Stehl\u00e9", "Alain Barrat", "Ciro Cattuto", "Jean-Fran\u00e7ois Pinton", "Wouter Van den Broeck"], "venue": "Journal of Theoretical Biology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Segmenting time series: A survey and novel approach", "author": ["Eamonn Keogh", "Selina Chu", "David Hart", "Michael Pazzani"], "venue": "Data mining in Time Series Databases,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "The enron corpus: A new dataset for email classification research", "author": ["Bryan Klimt", "Yiming Yang"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Effects of time window size and placement on the structure of an aggregated communication network", "author": ["Gautier Krings", "M\u00e1rton Karsai", "Sebastian Bernhardsson", "Vincent D Blondel", "Jari Saram\u00e4ki"], "venue": "EPJ Data Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "The link prediction problem for social networks", "author": ["David Liben-Nowell", "Jon Kleinberg"], "venue": "In Proceedings of the Twelfth International Conference on Information and Knowledge Management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "A graph summarization: A survey", "author": ["Yike Liu", "Abhilash Dighe", "Tara Safavi", "Danai Koutra"], "venue": "arXiv preprint arXiv:1612.04883,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Sensible organizations: Technology and methodology  for automatically measuring organizational behavior", "author": ["Daniel Olg\u00fa\u0131n Olg\u00fa\u0131n", "Benjamin N. Waber", "Taemie Kim", "Akshay Mohan", "Koji Ara", "Alex Pentland"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Detecting change points in the large-scale structure of evolving networks", "author": ["Leto Peel", "Aaron Clauset"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Quantifying the effect of temporal resolution on timevarying networks", "author": ["Bruno Ribeiro", "Nicola Perra", "Andrea Baronchelli"], "venue": "Scientific Reports,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "CRAW- DAD dataset cambridge/haggle (v. 2009-05-29)", "author": ["James Scott", "Richard Gass", "Jon Crowcroft", "Pan Hui", "Christophe Diot", "Augustin Chaintreau"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Temporalrelational classifiers for prediction in evolving domains", "author": ["Umang Sharan", "Jennifer Neville"], "venue": "In Eighth IEEE International Conference on Data Mining, 2008", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Generating graph snapshots from streaming edge data", "author": ["Sucheta Soundarajan", "Acar Tamersoy", "Elias B. Khalil", "Tina Eliassi-Rad", "Duen Horng Chau", "Brian Gallagher", "Kevin Roundy"], "venue": "In Proceedings of the 25th International Conference Companion on World Wide Web,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Meaningful selection of temporal resolution for dynamic networks", "author": ["Rajmonda Sulo", "Tanya Berger-Wolf", "Robert Grossman"], "venue": "In Proceedings of the Eighth Workshop on Mining and Learning with Graphs,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Graphscope: parameter-free mining of large time-evolving graphs", "author": ["Jimeng Sun", "Christos Faloutsos", "Spiros Papadimitriou", "Philip S. Yu"], "venue": "In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Evaluating link prediction methods", "author": ["Yang Yang", "Ryan N. Lichtenwalter", "Nitesh V. Chawla"], "venue": "Knowledge and Information Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Indeed, the time scale of the network strongly impacts what structures and dynamics may be observed in the network [3, 12, 17].", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "Indeed, the time scale of the network strongly impacts what structures and dynamics may be observed in the network [3, 12, 17].", "startOffset": 115, "endOffset": 126}, {"referenceID": 16, "context": "Indeed, the time scale of the network strongly impacts what structures and dynamics may be observed in the network [3, 12, 17].", "startOffset": 115, "endOffset": 126}, {"referenceID": 7, "context": "Moreover, the choice of time scale impacts the efficacy of data mining on networks [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "In numeric time series, this problem is often termed \u2018segmentation,\u2019 and segmentation of numeric times series has a long history which is outside the scope of this work; see [10]", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "Typical methods include using generative models of dynamic networks [16] or clustering similar time slices [2].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "Typical methods include using generative models of dynamic networks [16] or clustering similar time slices [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 13, "context": "See [14] for a survey of graph summarization techniques.", "startOffset": 4, "endOffset": 8}, {"referenceID": 5, "context": "This approach has been applied, more generally, to multi-layer networks, dynamic or not [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 20, "context": "maps the dynamic network to a time series using a metric on graphs (such as the number of triangles in each graph) and then determines time scale by finding the scale at which the time series\u2019 compression ratio and variance is balanced [21].", "startOffset": 236, "endOffset": 240}, {"referenceID": 19, "context": "determines a windowing of the data with respect to some metric (such as the exponent of the degree distribution) by measuring when that metric has converged [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 4, "context": "use a parameterfree approach that seeks to find an appropriate time scale by measuring the similarity between graphs using the Jaccard index [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 7, "context": "Most closely related to the approach that we take in this paper, Fish and Caceres use the quality of the performance of link prediction algorithms to determine the best time scale [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "These are parameter-free methods or, relatedly, methods that assume that there is some \u2018ground-truth\u2019 time scale via a generative model or the like, as in [3, 5, 8].", "startOffset": 155, "endOffset": 164}, {"referenceID": 4, "context": "These are parameter-free methods or, relatedly, methods that assume that there is some \u2018ground-truth\u2019 time scale via a generative model or the like, as in [3, 5, 8].", "startOffset": 155, "endOffset": 164}, {"referenceID": 7, "context": "These are parameter-free methods or, relatedly, methods that assume that there is some \u2018ground-truth\u2019 time scale via a generative model or the like, as in [3, 5, 8].", "startOffset": 155, "endOffset": 164}, {"referenceID": 7, "context": "As pointed out in [8, 20, 21], too small a window size may introduce noise and lack the structure necessary for analysis but too large a window size may lose important temporal information.", "startOffset": 18, "endOffset": 29}, {"referenceID": 19, "context": "As pointed out in [8, 20, 21], too small a window size may introduce noise and lack the structure necessary for analysis but too large a window size may lose important temporal information.", "startOffset": 18, "endOffset": 29}, {"referenceID": 20, "context": "As pointed out in [8, 20, 21], too small a window size may introduce noise and lack the structure necessary for analysis but too large a window size may lose important temporal information.", "startOffset": 18, "endOffset": 29}, {"referenceID": 0, "context": "While there are many methods for link prediction (see [1] for a survey), the method we use is a simple scoring function that scores every pair of vertices by how likely an edge is to appear between them.", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "In this paper, we use the Katz\u03b2 score, an efficient and well-performing score [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "005, which has been used before [8, 13].", "startOffset": 32, "endOffset": 39}, {"referenceID": 12, "context": "005, which has been used before [8, 13].", "startOffset": 32, "endOffset": 39}, {"referenceID": 22, "context": "The performance of the link prediction algorithm is evaluated using the AUC of the precision-recall curve, as recommended by [23], averaged over all predictions made, one set of predictions for each graph.", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "We use the Time Varying Relational Classifier (TVRC) algorithm of Sharan and Neville [19] to determine the unknown value of a binary vertex attribute.", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Peel and Clauset [16] propose the following notions of precision and recall:", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "Note this is a [0, 1]-valued measure.", "startOffset": 15, "endOffset": 21}, {"referenceID": 10, "context": "from January 1999 to July 2002, during the period of Enron\u2019s market manipulation scandal and subsequent collapse [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "The attribute we test on for attribute learning is whether the employee was a manager or not, which we took from [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 15, "context": "We use the change points from [16], which represent times when the emails undergo substantial shifts, such as the launch of Enron online and changes in the CEO position.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "This is a proximity network of 90 MIT students and faculty using data taken from cell phones from September 2004 to May 2005 (we only use data up until the end of the academic year) [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 15, "context": "points are taken from [16], which are the start and ends of vacations, semesters, etc.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "This is a proximity network of 23 employees at a data server configuration firm for a month (the name of the data set comes from the badges the employees wore to track their location at the workplace) [15].", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "This is a proximity network consisting of interactions, recorded using Bluetooth, among attendees at an IEEE Infocomm conference over four days [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "This is another proximity network of attendees at the ACM Hypertext 2009 conference, held over three days [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] and the entropybased method (Entropy) of De Domenico et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "to study the Reality Mining data set [7].", "startOffset": 37, "endOffset": 40}], "year": 2017, "abstractText": "For any stream of time-stamped edges that form a dynamic network, an important choice is the aggregation granularity that an analyst uses to bin the data. Picking such a windowing of the data is often done by hand, or left up to the technology that is collecting the data. However, the choice can make a big difference in the properties of the dynamic network. This is the time scale detection problem. In previous work, this problem is often solved with a heuristic as an unsupervised task. As an unsupervised problem, it is difficult to measure how well a given algorithm performs. In addition, we show that the quality of the windowing is dependent on which task an analyst wants to perform on the network after windowing. Therefore the time scale detection problem should not be handled independently from the rest of the analysis of the network. We introduce a framework that tackles both of these issues: By measuring the performance of the time scale detection algorithm based on how well a given task is accomplished on the resulting network, we are for the first time able to directly compare different time scale detection algorithms to each other. Using this framework, we introduce time scale detection algorithms that take a supervised approach: they leverage ground truth on training data to find a good windowing of the test data. We compare the supervised approach to previous approaches and several baselines on real data.", "creator": "LaTeX with hyperref package"}}}