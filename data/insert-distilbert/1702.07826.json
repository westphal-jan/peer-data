{"id": "1702.07826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations", "abstract": "we introduce ai rationalization, an approach established for generating explanations of autonomous system behavior as if a genuine human had historically done the behavior. we describe a rationalization technique that uses neural machine knowledge translation to appropriately translate internal state - organized action representations of the autonomous agent into natural language. secondly we will evaluate our technique in the frogger game environment. the natural language is collected from human players - thinking out loud as they play the active game. we motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization using technique, and describe future research agenda.", "histories": [["v1", "Sat, 25 Feb 2017 03:20:49 GMT  (183kb,D)", "http://arxiv.org/abs/1702.07826v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.HC cs.LG", "authors": ["brent harrison", "upol ehsan", "mark o riedl"], "accepted": false, "id": "1702.07826"}, "pdf": {"name": "1702.07826.pdf", "metadata": {"source": "META", "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations", "authors": ["Brent Harrison", "Upol Ehsan", "Mark O. Riedl"], "emails": ["<brent.harrison@cc.gatech.edu>."], "sections": [{"heading": "1. Introduction", "text": "Autonomous systems must make complex sequential decisions in the face of uncertainty. Explainable AI refers to artificial intelligence and machine learning techniques that enable human users to understand, appropriately trust, and effectively operate semi-autonomous systems. Explainability becomes important in human-agent (or robot) teams wherein a human operator works alongside an autonomous or semi-autonomous agent. In the event of failure\u2014or if the agent performs unexpected behaviors\u2014it is natural for the human operator to want to know why. Explanations help the human operator understand why an agent failed to achieve a goal or the circumstances whereby the behavior of the agent deviated from the expectations of the human operator. They may then take appropriate remedial action: trying again, providing more training to machine learning algorithms controlling the agent, reporting bugs to the manufacturer, repairing the sensors or effectors, etc.\nExplanation differs from interpretability, which is a feature of an algorithm or representation that affords inspec-\n1Georgia Institute of Technology, Atlanta, Georgia, USA. Correspondence to: Brent Harrison <brent.harrison@cc.gatech.edu>.\ntion for the purposes of understanding behavior or results. For example, decision trees are considered to be relatively interpretable and neural networks are generally considered to be uninterpretable without additional processes to visualize patterns of neuron activation (Zeiler & Fergus, 2014; Yosinski et al., 2015). Interpretability is a feature desirable for AI/ML system developers who can use it to debug and improve their systems. Explanation, on the other hand, is grounded in natural language communication and is theorized to be more useful for non-AI-experts who need to operate autonomous or semi-autonomous systems.\nIn this paper we introduce a new approach to explainable AI: AI rationalization. AI rationalization is a process of producing an explanation for agent behavior as if a human had done the behavior. AI rationalization is based on the observation that humans do not generally understand how their own brains work and consequently do not give explanations that literally reveal how a decision was made. Rather it is more likely that humans invent plausible explanations on the spot. However, we accept human-generated rationalizations as providing some lay insight into the mind of the other.\nAI rationalization has a number of theoretical benefits: (1) by communicating like humans, rationalizations are naturally accessible and intuitive to humans without AI or computer science training; (2) human-likeness of communication between autonomous system and human operator may afford higher degrees of trust, rapport, and willingness to use autonomous systems; (3) rationalization is fast, sacrificing absolute accuracy for real-time response, appropriate for real-time human-agent collaboration. Note that AI rationalizations, like human-produced rationalizations, are not required to be accurate, but only to give some insight into the agent\u2019s decision-making process. Should deeper, more accurate explanations or interpretations be necessary, rationalizations may need to be supplemented by other explanation, interpretation, or visualization techniques.\nWe propose a technique for AI rationalization that treats the generation of explanations as a problem of translation between state-action-state sequences that are produced during execution of an autonomous system and natural language. Language translation technology using encoder-\nar X\niv :1\n70 2.\n07 82\n6v 1\n[ cs\n.A I]\n2 5\nFe b\n20 17\ndecoder neural networks (e.g., (Luong et al., 2015)) has reached the level of maturity such that it is used in commercial language-to-language translation. This is the first time, to our knowledge, that it has been used to translate the internal data structures of a reinforcement learning (RL) agent into natural language. To translate state-action representations into natural language, we collect a corpus of natural language utterances from people performing the task that we will later ask the autonomous system to perform. We treat the string serializations of state-action pairs and natural language utterances as equivalent and train an encoderdecoder neural network that must find a neural embedding that produces a rational natural language output when presented with state-action input.\nSpecifically, we apply AI rationalization to an agent that plays the game, Frogger (Figure 1). We do this for two reasons. First, we aim to show the technique works on an autonomous system without adaptation\u2014we use an arbitrary state representation designed for the game without consideration for rationalization. This environment is also notable because conventional learning algorithms do not learn to play Frogger like human players, and our target audience would not be expected to understand the specific information about how an agent learns to play this game or why it makes the decisions that it does. Most people will probably also not have knowledge on the specific state representation used in these environments. Second, we aim to establish a baseline that we can compare to later investigations of AI rationalization applied to state abstractions learned from a deep learning techniques such as a convolutional neural network.\nThe contributions of our paper are as follows:\n\u2022 We introduce the concept of AI rationalization as an approach to explainable AI.\n\u2022 We describe a technique for generating rationalizations that treats explanation generation as a language translation problem.\n\u2022 We report on an experiment using synthetic data to assess the accuracy of the translation technique.\n\u2022 We identify a number of subsequent hypotheses about AI rationalization.\nThe remainder of the paper is organized as follows. In Sections 2 we overview related work on explainable AI. In Section 3, we describe our AI rationalization technique. Section 4 describes specific details of how we used rationalization in experiments designed to assess the accuracy of the technique. In Section 5 we propose a number of next steps in the investigation of rationalization as an approach to explainable AI."}, {"heading": "2. Related Work", "text": "Interpretability has received an increasing amount of attention recently. For a model to be interpretable it must be possible to explain why it generates certain outputs or behaves in a certain way. Inherently, some machine learning techniques produce models that are more interpretable than others. Models such as decision trees (Letham et al., 2015), generalized additive models (Caruana et al., 2015), and attention-based approaches (Xu et al., 2015) have the benefit of being naturally interpretable. This is because one need only examine the output to understand the model\u2019s decision making process (in the case of rule based approaches such as decision trees) or examine some internal features which reveal which areas of the model\u2019s feature space have the greatest influence on prediction (in the case of attention and regression-based approaches). While these techniques are effective, relying solely on them for interpretable models greatly inhibits machine learning researchers by limiting the types of models they can use for learning.\nAn alternate approach to creating interpretable machine learning models involves creating separate models of explainability that are often built on top of black box techniques such as neural networks. These approaches, sometimes called model-agnostic (Ribeiro et al., 2016) approaches, allow greater flexibility in model selection since they enable black-box models to become interpretable; however, the added layer of complexity incurred by creating a model specifically for explainability can be costly in terms of training time and explanation quality when compared to naturally interpretable models. A common ap-\nUse\"these\"bullets\"for\"each\"stage\"if\"you\"don\u2019t\"like\"the\"icons\"placed\"next\"to\"the\" current\"labels\"such\"as\"training\"the\"model,\"corpus\"genera=on,\"etc\"\nproach to creating model-agnostic is to visualize the internal state of the model, as has been done with convolutional neural networks (Zeiler & Fergus, 2014; Yosinski et al., 2015). Other approaches seek to learn a naturally interpretable model which describes predictions that were made (Krause et al., 2016) or by intelligently modifying model inputs so that resulting models can describe how outputs are affected.\nExplainable AI has been explored in the context of ad-hoc techniques for transforming simulation logs to explanations (van Lent et al., 2004), intelligent tutoring systems (Core et al., 2006), and transforming AI plans into natural language (van Lent et al., 2005). Our work differs in that we explore the application of explainable AI techniques to sequential decision-making in stochastic domains focus on generating rationalizations, not explanations.\nOur work differs from this previous work in that we specifically explore rationalization. By focusing on rationalization we relax the requirement that explanations need to be faithful to the original machine learning algorithm\u2019s decision making process. This allows us more flexibility in creating explanations; however, situations may arise where rationalizations would be detrimental to a human seeking to understand an algorithm\u2019s motives."}, {"heading": "3. AI Rationalization", "text": "Rationalization is a form of explanation that attempts to justify or explain an action or behavior. Whereas explanation implies an accurate account of a decision-making process, AI rationalization suggests that the explanation is the one that a human would most likely give were he or she in full control of an agent or robot. There is no clear guidance on what makes a good explanation; for an agent using Q-learning (Watkins & Dayan, 1992), explanations of decisions could range from \u201cthe action had the highest Q value given this state\u201d to \u201cI have explored numerous possible future state-action trajectories from this point\nand deemed this action to be the most likely to achieve the highest expected reward according to iterative application of the Bellman update equation.\u201d The closer the explanation to the actual implementation of the decision-making or machine learning algorithm, the more background knowledge the human operator will require to make sense of the explanation.\nRationalization is geared toward human operators who may not have significant background knowledge in computer science or artificial intelligence (including machine learning). Rationalizations generated by an autonomous or semi-autonomous system do not need to accurately reflect the true decision-making process underlying the agent system, but must still give the operator some insight into what the agent is doing and trying to achieve. Human rationalizations are often produced quickly and thus we propose that AI rationalizations should also be generated quickly; the ideal use case is in real-time critical scenarios in which it may be inconvenient or impossible to quickly produce an explanation that can be understood and acted upon by a human operator. If more accurate explanations are necessary, rationalization can be coupled with other explanations, interpretations, or visualization techniques that require greater time and greater cognitive effort to understand.\nThis paper, which is the first to explore the question of rationalization as an approach to explanation, looks at the accuracy of rationalizations generated by neural machine translation techniques. Our approach for translating stateaction-state sequences to natural language consists of two general steps. First, we must create a training corpus of natural language and state-action pairs. Second, we use this corpus to train an encoder-decoder network to translate the state and action information to natural language. Our system\u2019s workflow is shown in Figure 2."}, {"heading": "3.1. Training Corpus", "text": "In order to learn translation from state-action pairs to language, we must first create a corpus of natural language descriptions of behavior and their corresponding state and action information. To do this, we ask people to complete the agent task\u2019s in a virtual environment and are prompted to \u201cthink aloud\u201d as they perform the task. We record the visited states and performed actions along with the natural language utterances of critical states and actions. This method of corpus creation ensures that the annotations gathered are associated with specific states and actions. In essence we create parallel corpora, one of which contains state representations and actions, the other containing natural language utterances. The state representation and actions constitute a \u201cmachine language\u201d.\nThe general approach is agnostic to the details of how the parallel corpora are obtained. We describe the specific methodology we used to create the parallel corpora for the purpose of experimentation in Section 4.1.\nThe precise representation of states and actions in the autonomous system does not matter as long as they can be converted to strings in a consistent fashion. Autonomous agent may have a state representation tailored to a specific environment or may use a neural network to learn a representation of the environment. Our approach emphasizes that it should not matter how the state representation is structured and the human operator should not need to know how to interpret it."}, {"heading": "3.2. Translation from Internal Representation to Natural Language", "text": "In this work, we use encoder-decoder networks to translate between complex state and action information and natural language rationalizations. Encoder-decoder networks, which have primarily been used in machine translation and dialogue systems, are a generative architecture comprised of two component networks that learn how to translate an input sequence X = (x1, ..., xT ) into an output sequence Y = (y1, ..., y \u2032 T ). The first component network, the encoder, is a recurrent neural network (RNN) that learns to encode the input vector X into a fixed length context vector v. This vector is then used as input into the second component network, the decoder, which is a RNN that learns how to iteratively decode this vector into the target output Y .\nSince the input data that we work with in this paper can be quite long it is possible that the context vector may lose information about the input sequence. To address this, we specifically use an encoder-decoder network with an added attention mechanism (Luong et al., 2015). The attention mechanism allows the decoder network to selectively peek at the input data and use this information along with the\ncontext vector to iteratively predict outputs."}, {"heading": "4. Experiments", "text": "The objective of the experiments we describe below is to assess the accuracy of the neural machine translation approach to rationalization. Evaluating natural language generation is challenging; utterances can be \u201ccorrect\u201d even if they do not exactly match known utterances from a testing dataset. To facilitate the assessment of rationalizations generated by our technique, we devised a technique whereby semi-synthetic natural language was paired against stateaction representations internal to an autonomous system. The semi-synthetic language was produced by observing humans \u201cthinking out loud\u201d while performing a task and then creating a grammar that reproduced and generalized the utterances (described below). Our evaluation methodology enables us to identify the rules that fire during autonomous system execution and compare rationalizations generated by the neural machine translation network to a ground-truth. Later, we can replace the grammar with a corpus entirely trained on humans performing a task in the wild.\nWe conducted the experiments by generating rationalizations for states and actions in the game Frogger. In this environment, the agent is a frog that must navigate from the bottom of the screen to the top. Scattered throughout the environment are obstacles that the agent must avoid as well as platforms that the agent must jump on in order to successfully reach its goal. The actions available to the agent in this environment are movement actions in the four cardinal directions and action for standing still. We chose Frogger as an experimental domain because computer games have been demonstrated to be good stepping stones toward real-world stochastic environments (Laird & van Lent, 2001; Mnih et al., 2015) and because Frogger is fast-paced, stochastic, has a reasonably rich state space, and yet can be learned optimally without too much trou-\nble. Furthermore, unlike other games, the maps can be easily modified to produce playable variations and Frogger\u2019s starting point can changed.\nWe evaluate our rationalization technique two two baselines. The first baseline, randomly selects any sentence that can be generated by the testing grammar. The second baseline will always select sentences associated with the rule that is most commonly used to generate rationalizations on a given map."}, {"heading": "4.1. Methodology", "text": ""}, {"heading": "4.1.1. GRAMMAR CREATION", "text": "In order to translate between state information and natural language, we first need ground truth language containing rationalizations that can be associated explicitly with this state and action information. To generate this information, we used crowdsourcing to gather a set of gameplay videos of human participants playing Froggerwhile engaging in a think-aloud protocol, following the work by (Dorst & Cross, 2001; Fonteyn et al., 1993). In total, we gathered recordings of 12 participants from 3 continents (4 from Asia, 3 from Europe, and 5 from North America) who were recruited online. In these recordings, we specifically asked participants to verbalize an their internal thoughts while playing Frogger as if they were speaking to another person. Each person was allowed to play one level of the game, and each person played the same game level as each other participant.\nAfter players completed the game, they uploaded their gameplay video to an online speech transcription service. This service generated transcripts of each players speech from their video recordings. This transcript was then presented to each player and they were asked to map important utterances to specific actions available in the game. They were also asked to assign utterances to pre-assigned stages of the game (e.g. being in the starting row, being in the region of the game with cars, etc.). This process produced 225 action-rationalization trace pairs of gameplay aligned with speech.\nTypically, think-aloud protocol studies lack self-validation that gives the participant the voice to select important aspects of their transcript. This method of running a thinkaloud study, however, gives players some amount of autonomy in specifying important utterances that may not be discovered otherwise. This layer of self-validation facilitates the robustness of the data.\nWe then use these action-rationalization annotations generate a grammar for generating sentences. The role of this grammar is to act as a representation of the reasoning that humans go through when they generate rationalizations or explanations for their actions. Thus, a grammar allows us\nto associate rules about the environment with the rationalizations that are generated. One benefit of using a grammar is that it allows us to explicitly evaluate how accurate our system is at producing the right kind of rationalizations. Since the grammar contains the rules that govern when certain rationalizations are generated, it allows us to compare automatically generated rationalizations against a groundtruth that one would not normally have if the entire corpus was crowdsourced in the wild.\nTo generate the grammar, we used qualitative coding schemes and clustered the user annotation data that we gathered previously. Our technique was inspired by Grounded Theory (Strauss & Corbin, 1994), which provides an analytical tool and facilitates discovery of emerging patterns in data (Walsh et al., 2015). Once clustered, we used Tracery (Compton et al., 2014), an authoring tool based on a standard context-free grammar (CFG), to build the grammar which we used to generate ground truth rationalizations."}, {"heading": "4.1.2. TRAINING AND TEST SET GENERATION", "text": "Since we are using a grammar produce ground truth rationalizations, one can interpret the role of the encoderdecoder network as learning to reproduce this grammar. In order to train the network to do this, we use the grammar to generate rationalizations for each state in the environment. The rules that the grammar uses to generate rationalizations are based on a combination of the state of the world and the action taken. Specifically, the grammar uses the following triple to determine which rationalizations to generate: (s1, a, s2). Here, s1 is the starting state of the world, a is the action that was performed in s1, and s2 is the resulting state of the world after action a is executed. States s1 and s2 consist of the (x,y) coordinates of the agent and the current state of grid environment. Our grammar would generate a rationalization for each possible (s1, a, s2) triple in the environment. These triples are then clustered based on the rules that were used to generate these rationalizations. For evaluation, we take 20% of the triples in each of these clusters and set them aside as a testing set. This ensures that the testing set is a representative sample of the parent population while still containing example triples associated with each rule in the grammar.\nTo aid in training we duplicate training examples until the training set contains 1000 examples per grammar rule. We then inject noise into these training samples in order to help avoid overfitting and help the network possibly generalize to other map types. Recall that the input to the encoderdecoder network is a triple of the form (s1, a, s2) where s1 and s2 are states. Also recall that each state does include a full representation of the Frogger map at the current time. To inject noise, we randomly select 30% of the rows in this\nmap representation for both s1 and s2 and redact them by replacing them with a dummy value. This set of examples becomes our training set.\nTo evaluate the extent to which rationalization generalizes to different environments other than the one the encoderdecoder network was trained on, we developed three different maps. The first map was randomly generated by filling 25% of the bottom with car obstacles and filling 25% of the top with log platforms. The second map was 50% cars/logs and the third map was 75% cars/logs. See Figure 3. For the remainder of the paper, we refer to these maps as the 25% map, the 50% map, and the 75% map respectively. We also ensured that it was possible to complete each of these maps to act as a loose control on map quality."}, {"heading": "4.1.3. TRAINING AND TESTING THE NETWORK", "text": "The parallel corpus of state-action representations and natural language are used to train an encoder-decoder neural translation algorithm based on (Luong et al., 2015). Specifically, we use a 2-layered encoder-decoder network with an embedding layer and an attention mechanism. The encoder and decoder networks are long short-term memory (LSTM) recurrent neural networks where each LSTM node has a hidden size of 300. We train the network for 50 epochs, after which the trained model is used to generate rationalizations for each triple in the testing set. To further test the generalization of this technique, we also have each model produce rationalizations for each triple in the testing set for each other map used during evaluation. Consider the 25% map as an example. After training, the model trained on the 25% map will then generate rationalizations for states sampled from the 25% map. To test generalization, this model will also be asked to generate rationalizations for states sampled from the 50% map as well as the 75% map.\nThe goal of this evaluation is to determine how appropriate a rationalization is by determining how well it matches when the grammar would produce a similar rationalization. To evaluate accuracy, we need to have a way to associate the sentence predicted by our model with a rule that exists in our grammar. Because encoder-decoder networks are generative, we cannot explicitly match the generated sentences to sentences that are generated by the grammar. To determine the set of rules that are associated with a predicted sentence, we use BLEU (Papineni et al., 2002) score to calculate sentence similarity. The BLEU score is a common metric used in machine translation to determine the quality of translated sentences with respect to a reference sentence.\nWe calculate the BLEU score between the sentence generated by our predictive model with each sentence that can be generated by the grammar and record the sentence that achieves the highest score. We can then identify which rule\nin the grammar would have been used to generate this sentence and use that to calculate accuracy. If this rule matches the rule that was used to produce the test sentence then we say that it was a match. Total accuracy is defined as the percentage of the predictions that matched their associated test example. If the predicted sentence does not achieve a BLEU score of greater than 0.7 when compared to any reference sentence, then we say that this predicted sentence does not match any sentence in the grammar and this automatically gets counted as a mismatch. This threshold is put in place to ensure that low quality rationalizations in terms of grammar and syntax do not get erroneously matched to rules in the grammar.\nIt is possible for a generated sentence to be associated with more than one rule in the grammar. If the rule that generated the testing sentence matches at least one of the rules associated with the generated sentence, then we count this as a match as well."}, {"heading": "4.2. Results", "text": "A screenshot of an agent trained to play Frogger and \u201cthinking out loud\u201d after every action using our system is shown in Figure 4. The results of our experiments can be found in Table 1. As you can see in the table, the encoderdecoder network was able to consistently outperform both the random baseline and majority baseline models. Comparing the maps to each other, the encoder-decoder network produced the highest accuracy when generating rationalizations for the 75% map, followed by the 25% map and the 50% map respectively. To evaluate the significance of the observed differences between these models, we ran a chisquared test between the models produced by the encoderdecoder network and random predictor as well as between the encoder-decoder network models and the majority classifier. Each difference was deemed to be statistically significant (p < 0.05) across all three maps.\nTo test how well this technique generalizes to other types of maps, we performed another set of experiments where models built on specific maps were used to predict test sentences generated by the grammar for different maps. The results of these experiments can be found in Table 2. These results show that the model built off of the 25% map had difficulty generating the correct rationalization for states drawn from th 50% map as well as the 75% map, achieving accuracy values of 0.124 and 0.159 respectively. The models trained on the 50% map and the 75% had similar success generating rationalizations for states generated from the 25% map achieving accuracy values of 0.453 and 0.442 respectively. The model trained on the 50% map did reasonably well generating rationalizations for states drawn from the 75% (accuracy = 0.674). This value is actually comparable to how this model performed generating ratio-\nnalizations for states sampled from the 50% map (accuracy = 0.687). The accuracy value that the model trained on the 75% also performed comparably on states sampled from the 50% map (0.732) and those sampled from the 75% map (0.80)."}, {"heading": "4.3. Discussion", "text": "The first thing to note about our results is that the models produced by the encoder-decoder network significantly outperformed the baseline models in terms of accuracy per-\ncentage. This means that this network was able to better learn when it was appropriate to generate certain rationalizations when compared to the random and majority baseline models. Given the nature of our test set as well, this gives evidence to the claim that these models can generalize to unseen states as well. It is important to note, however, that claims about the possibility of these maps generalizing using the results in Table 1 only extend to states drawn from maps with similar obstacle densities. While it is not surprising that encoder-decoder networks were able to outperform these baselines, the margin of difference between these models is worth noting. The performances of both the random and majority classifiers are a testament to the complexity of this problem.\nThe results of our second set of experiments warrant further discussion as well. The goal of these experiments was to see how well models would generalize to states sampled from maps with different obstacle densities. What we found is that in general, the model trained on the 25% map did not generalize well to the other maps. This is likely due to the fact that this map contained significantly fewer obstacles than the other two maps. When learning, the encoderdecoder network how to associate certain obstacle configurations with natural language rationalizations. Since there were very few obstacles in the 25% map, the network did not have enough information to determine how to translate between states with high-obstacle densities and natural language. In other words, this model did not generalize well because there were no samples that were suitable for learning about states with high obstacle density.\nThe models trained on the 50% and 75% maps fared much better generalizing to different maps. Both models achieved similar performance on the test set sampled from the 25% map. These accuracy values, while still well the highest accuracy values achieved, support the idea that the presence of more obstacles in this environment makes it more likely that the model will be able to generalize. This is likely due to the nature of the rules associated with the grammar used to generate the training sentences in these environment. These grammar rules often used the presence of the different obstacles in frogger to determine when to generate certain sentences. Since these environments contained a high density of obstacles, it was more likely that the network could learn state abstractions useful in predicting when certain natural language should be generated.\nIt is also interesting to note that the 75% model achieved similar accuracy values on the 75% map test set as well as the 50% test set. We also observe similar behavior with the 50% maps. This most likely occurs because both these maps ended up with similar densities due to variance in the map generation algorithm. This result still helps further support the claim that these models will generalize to\ndifferent maps of similar densities."}, {"heading": "5. Future Work", "text": "Having determined that neural machine translation can accurately produce rationalizations for state-action pairs, the next step is to investigate hypotheses about how rationalizations impact human trust, rapport, and willingness to use autonomous and semi-autonomous systems. To address these questions, it will be necessary to conduct experiments in which non-expert human operators work with autonomous systems to achieve a particular task.\nQuestionnaires can be devised to assess human operators\u2019 subjective feelings of trust and rapport. Treating trust and rapport as dependent variables, between-subject studies can be devised so that different populations work with autonomous systems with and without rationalization abilities. One can look at the effects of \u201cthinking out loud\u201d during execution versus rationalization when something unexpected occurs. Further, one can devise interventions to ensure that a failure happens or that the agent is trained to optimize a reward function that does not match operator expectations.\nSince rationalizations do not truly reflect the autonomous systems\u2019 underlying decision-making process, the question arises of how inaccurate can rationalizations be before subjective feelings of trust and rapport are significantly affected. The experimental methodology we describe above can be adapted to inject increasingly more error into the rationalizations."}, {"heading": "6. Conclusions", "text": "AI rationalization provides a new lens through which we can explore the question of what makes a good explanation of the decisions of an autonomous or semi-autonomous agent. In the future, we envision the increasing need for human operators of autonomous or semi-autonomous who do not have computer science or AI backgrounds. These operators will naturally want to know why an agent fails to achieve an objective or why its behavior deviates from expectations. Further, operators may need to quickly and effortlessly model the reasoning behind agent decisions in real-time scenarios. Since AI rationalization is the process of generating explanations as if a human had done the behavior, we hypothesize that rationalization will increase human operators\u2019 feelings of trust, rapport, and comfort using autonomous and semi-autonomous systems.\nWe have shown that our approach to AI rationalization, using neural machine translation from the autonomous agent\u2019s internal state and action representations to natural language produces rationalizations with accuracies above\nbaselines. Testing hypotheses about trust and rapport are discussed as next steps in determining the applicability of rationalization as an approach to explainable AI.\nRationalization allows autonomous systems to appear more natural and human-like in their decision-making when their decision-making processes can be very complicated and different than human reasoning. We believe that AI rationalization can be an important step toward deployment of real-world commercial robotic systems including in healthcare, disability support, personal services, and military teamwork."}], "references": [{"title": "Tracery: Approachable story grammar authoring for casual users", "author": ["Compton", "Kate", "Filstrup", "Benjamin"], "venue": "In Seventh Intelligent Narrative Technologies Workshop,", "citeRegEx": "Compton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Compton et al\\.", "year": 2014}, {"title": "Building Explainable Artificial Intelligence Systems", "author": ["Core", "Mark", "Lane", "H. Chad", "van Lent", "Michael", "Gomboc", "Dave", "Solomon", "Steve", "Rosenberg", "Milton"], "venue": "In Proceedings of the 18th Innovative Applications of Artificial Intelligence Conference,", "citeRegEx": "Core et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Core et al\\.", "year": 2006}, {"title": "Creativity in the design process: co-evolution of problem\u2013solution", "author": ["Dorst", "Kees", "Cross", "Nigel"], "venue": "Design studies,", "citeRegEx": "Dorst et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dorst et al\\.", "year": 2001}, {"title": "A description of think aloud method and protocol analysis", "author": ["Fonteyn", "Marsha E", "Kuipers", "Benjamin", "Grobe", "Susan J"], "venue": "Qualitative Health Research,", "citeRegEx": "Fonteyn et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Fonteyn et al\\.", "year": 1993}, {"title": "Interacting with predictions: Visual inspection of black-box machine learning models", "author": ["Krause", "Josua", "Perer", "Adam", "Ng", "Kenney"], "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Krause et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2016}, {"title": "Human-level ais killer application: Interactive computer", "author": ["Laird", "John", "van Lent", "Michael"], "venue": "games. AI Magazine,", "citeRegEx": "Laird et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Laird et al\\.", "year": 2001}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["Letham", "Benjamin", "Rudin", "Cynthia", "McCormick", "Tyler H", "Madigan", "David"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Letham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Letham et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Wierstra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2015}, {"title": "Why should i trust you?: Explaining the predictions of any classifier", "author": ["Ribeiro", "Marco Tulio", "Singh", "Sameer", "Guestrin", "Carlos"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Grounded theory methodology", "author": ["Strauss", "Anselm", "Corbin", "Juliet"], "venue": "Handbook of qualitative research,", "citeRegEx": "Strauss et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Strauss et al\\.", "year": 1994}, {"title": "An explainable artificial intelligence system for small-unit tactical behavior", "author": ["van Lent", "Michael", "Fisher", "William", "Mancuso"], "venue": "In Proceedings of the 16th conference on Innovative Applications of Artifical Intelligence,", "citeRegEx": "Lent et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lent et al\\.", "year": 2004}, {"title": "What grounded theory is a critically reflective conversation among scholars", "author": ["Walsh", "Isabelle", "Holton", "Judith A", "Bailyn", "Lotte", "Fernandez", "Walter", "Levina", "Natalia", "Glaser", "Barney"], "venue": "Organizational Research Methods,", "citeRegEx": "Walsh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Walsh et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Bengio", "Yoshua"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Bengio and Yoshua.,? \\Q2015\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Fuchs", "Thomas", "Lipson", "Hod"], "venue": "ICML Workshop on Deep Learning. Citeseer,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "For example, decision trees are considered to be relatively interpretable and neural networks are generally considered to be uninterpretable without additional processes to visualize patterns of neuron activation (Zeiler & Fergus, 2014; Yosinski et al., 2015).", "startOffset": 213, "endOffset": 259}, {"referenceID": 7, "context": ", (Luong et al., 2015)) has", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "Models such as decision trees (Letham et al., 2015), generalized additive models (Caruana et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 9, "context": "These approaches, sometimes called model-agnostic (Ribeiro et al., 2016) approaches, allow greater flexibility in model selection since they enable black-box models to become interpretable; however, the added layer of complexity incurred by creat-", "startOffset": 50, "endOffset": 72}, {"referenceID": 14, "context": "proach to creating model-agnostic is to visualize the internal state of the model, as has been done with convolutional neural networks (Zeiler & Fergus, 2014; Yosinski et al., 2015).", "startOffset": 135, "endOffset": 181}, {"referenceID": 4, "context": "Other approaches seek to learn a naturally interpretable model which describes predictions that were made (Krause et al., 2016) or by intelligently modifying model inputs so that resulting models can describe how outputs are affected.", "startOffset": 106, "endOffset": 127}, {"referenceID": 1, "context": ", 2004), intelligent tutoring systems (Core et al., 2006), and transforming AI plans into natural language (van Lent et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 7, "context": "attention mechanism (Luong et al., 2015).", "startOffset": 20, "endOffset": 40}, {"referenceID": 3, "context": "To generate this information, we used crowdsourcing to gather a set of gameplay videos of human participants playing Froggerwhile engaging in a think-aloud protocol, following the work by (Dorst & Cross, 2001; Fonteyn et al., 1993).", "startOffset": 188, "endOffset": 231}, {"referenceID": 12, "context": "Our technique was inspired by Grounded Theory (Strauss & Corbin, 1994), which provides an analytical tool and facilitates discovery of emerging patterns in data (Walsh et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 0, "context": "Once clustered, we used Tracery (Compton et al., 2014), an authoring tool based on a standard context-free grammar (CFG), to build the grammar which we used to generate ground truth rationalizations.", "startOffset": 32, "endOffset": 54}, {"referenceID": 7, "context": "The parallel corpus of state-action representations and natural language are used to train an encoder-decoder neural translation algorithm based on (Luong et al., 2015).", "startOffset": 148, "endOffset": 168}], "year": 2017, "abstractText": "We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.", "creator": "LaTeX with hyperref package"}}}