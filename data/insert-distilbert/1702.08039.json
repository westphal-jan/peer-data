{"id": "1702.08039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Criticality & Deep Learning I: Generally Weighted Nets", "abstract": "motivated accordingly by the idea that criticality and universality of phase transitions might play a crucial role in successively achieving and sustaining learning and therefore intelligent behaviour in biological and artificial networks, we analyse a theoretical curriculum and a pragmatic conceptual experimental set up guidelines for critical phenomena in deep learning. on the theoretical side, we use results from statistical physics to carry out critical point calculations in feed - forward / fully connected communication networks, while on the experimental side principles we set out to find traces of criticality in deep neural networks. this is our first step in issuing a series critique of upcoming investigations to map ruling out the relationship between criticality and learning in these deep networks.", "histories": [["v1", "Sun, 26 Feb 2017 14:43:38 GMT  (78kb,D)", "http://arxiv.org/abs/1702.08039v1", null], ["v2", "Wed, 31 May 2017 09:38:30 GMT  (78kb,D)", "http://arxiv.org/abs/1702.08039v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["dan oprisa", "peter toth"], "accepted": false, "id": "1702.08039"}, "pdf": {"name": "1702.08039.pdf", "metadata": {"source": "CRF", "title": "Criticality and Deep Learning, Part I: Theory vs. Empirics", "authors": ["Dan Oprisa", "Peter Toth"], "emails": ["dan.oprisa@critical.ai", "peter.toth@critical.ai"], "sections": [{"heading": null, "text": "Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks."}, {"heading": "1 Introduction", "text": "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term \u201duniversality\u201d, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3]. Furthermore, because of universality, it turns out that the most simplistic mathematical models exhibit the same statistical properties when their parameters are tuned correctly. As such it suffices to study N-particle systems with simple, \u201datomistic\u201d components and interactions since they already exhibit many non-trivial emergent properties in the large N limit. Certain \u201dorder\u201d parameters change behavior in a non-classical fashion, for specific noise levels. Using the rich and deep knowledge gained in statistical physics about those systems, we map the mathematical properties and learn about novel behaviors in deep learning set ups. Specifically we look at a collection of N units on a lattice with various pair interactions; when the units are binary spins with values (\u00b11), the model is known\nas a Curie-Weiss model. From a physical point of view, this is one of the basic, analytically solvable models, which still possesses the rich emergent properties of critical phenomena. However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10]. All those systems, with a rich and diverse origination, posses almost identical behavior at criticality. In the latter case of machine learning, the Curie-Weiss model encodes information about fully connected and feed-forward architectures to first order. Similar work was done in [11, 12], where insights from Ising models and fully connected layers are drawn and applied to net architectures; in [13] a natural link between the energy function and an autoencoder is established. We will address the generalisation of fully connected system and understand its properties, before moving to the deep learning network and applying there the same techniques and intuition.\nThe article is organised as follows: section 2 gives a short introduction of critical systems and appropriate examples from physics; in section 3 we map a concrete, non-linear, feed forward net to its physical counterpart and discuss other architectures as well; then we turn to investigating the practical question whether we can spot traces of criticality in current deep learning nets in 4. Finally we summarise our findings in 5 and hint at future directions for the rich map between statistical systems and deep learning."}, {"heading": "2 Brief summary of critical phenomena", "text": "Critical phenomena were first thoroughly explained and analysed in the field of statistical mechanics, although they were observed in various other systems, but lacking a theoretical understanding. The study of criticality belongs to statistical physics and is an incredibly rich and wide field, hence we can only briefly summarise some few results of interest for the present article; definitely a much more comprehensive coverage can be found, see e.g.[14, 15, 16, 17]. In a nutshell, the subject is concerned with the behavior of\nar X\niv :1\n70 2.\n08 03\n9v 1\n[ cs\n.A I]\n2 6\nFe b\n20 17\nsystems in the neighbourhood of their critical points, [18]. One thus looks at systems composed of (families of) many, identical particles, trying to derive properties for macroscopic parameters, such as density or polarisation from the microscopic properties and interactions of the particles; statistical mechanics can hence be understood as a bridge between macroscopic phenomenology (e.g. thermodynamics) and microscopic dynamics (e.g. molecular or quantum-mechanical interacting collections of particles). In a nutshell, criticality is achieved when macroscopic parameters show anomalous, divergent behavior at a phase transition. Depending on the system at hand, the parameters might be magnetisation, polarisation, correlation, density, etc. Specifically it is the correlation function of the \u201dcomponents\u201d which then displays divergent behavior, and signals strong coordinated group behavior over a wide range of magnitudes. Usually it is the noise (temperature) which at certain values will induce the phase transition accompanied by the critical anomalous behavior. Given its relevance in physics and also its mathematical analogy to our deep learning networks, we will briefly review here the CurieWeiss model with non-constant coupling and examine its behavior at criticality."}, {"heading": "2.1 Curie-Weiss model", "text": "A simplistic, fully solvable model for a magnet is the Curie-Weiss model (CW), [19]. It possesses many interesting features, exhibits critical behavior and correctly predicts some of the experimental findings. As its mathematics is later on used in our deep learning setup, we will briefly present main properties and solutions for the sake of self-consistency.\nThe Hamiltonian of the CW model is given by\nH = \u2212 J 2N N\u2211 ij sisj \u2212 b N\u2211 i si (1)\nHere the si are a collection of interacting \u201dparticles\u201d, in our physical case, spins, that interact with each other via the coupling J ; they take values (\u00b11) and interact pairwise with each other, at long distances; the inclusion of a factor of 1N multiplying the quadratic spin term makes this long-range interaction tractable in the large N limit. Furthermore, there is a directed external magnetic field which couples to every spin via b. Since the coupling between spins is a constant and since every spin interacts with every other spin (except self-interactions, which is accounted by a factor of 12 ) the Hamiltonian can be rewritten to\nH = \u2212 J 2N ( N\u2211 i si )2 \u2212 b N\u2211 i si (2)\nWith \u03b2 = 1/kT being the inverse temperature the par-\ntition function can be formulated\nZ = \u2211\nsi\u2208{\u00b11}\ne\u2212\u03b2H(s) (3)\n= \u2211\nsi\u2208{\u00b11}\nexp\u03b2\n[ J\n2N\n(\u2211N i si )2 + b \u2211N i si ] (4)\nwhich can be fully solved, [19], summing over each of the 2N states; given an explicit partition Z, the free energy can be computed via\nF = \u2212kT lnZ (5)\nOnce we have F various macroscopic values of interest can be inferred such as the magnetisation of the system, aka first derivative of F wrt. b. This is a so called \u201dorder parameter\u201d, which carries various other denominations, such as polarisation, density, opinion imbalance, etc. depending on the system at hand. It basically measures how arranged or homogeneous the system is under the influence of the outside field which couples to the spins via b. A full treatment and derivation of the model including all its critical behavior can be found in [20], from where we get the equation of state for the magnetisation\nm = b tanh (K b m+ b T ) (6)\nwith K = ( JT ) 1/2. The analysis of this equation for various temperatures T and couplings J , b reveals a phase transition at the critical temperature Tc = J . Introducing the dimensionless parameter t = (T/Tc\u22121) and expanding (6) in small couplings the famous power law dependence on temperature for the magnetisation emerges:\nm ' \u221a 3 (K \u2212 1)1/2\nK3/2 \u223c |t|1/2 (7)\nHere we recognise one of the very typical power laws which are ubiquitous to critical systems. The quantity we are most interested in though is the second derivative of the free energy F wrt. b, which is basically the 2-point correlation function of the spins si. Again, expanding the second derivative of the free energy in small couplings and looking in the neighbourhood of the critical temperature Tc yields\n\u3008si, sj\u3009 \u223c b2\nTc |t|\u22121 (8)\nagain displaying power law behavior with a power coefficient \u03b3 = 1. The innocent looking equation 8 has actually tremendous consequences, as it implies that correlation does not simply restrict to nearest neighbours but goes over very long distances only slowly decaying; further, because of the power law behavior, there will be self-similar, fractal patterns in the system: islands of equal magnetisation will form within other islands and so on, all the way through all scales. Also, the correlation diverges at the criticality point Tc. We will carry out the explicit calculations for our case of interest - non-constant matrix couplings - later one, in section 3.1."}, {"heading": "2.2 Criticality in real-world networks", "text": "Two of the main motivations why we look for criticality and exploit on it in artificial networks, are the universal arising of this phenomenon as well as various hints of its occurrence in biological [4] and neural systems [25, 22]; once systems get \u201dsizable\u201d enough, gaining complexity, critical behavior emerges, which also applies to man-made nets [23]. Various measures can be formulated to detect criticality, and they all show power law distribution behavior. In the world wide web, e.g. the number of links to a source, and the number of links away from a source, both exhibit power law distribution\nP (k) \u223c k\u2212\u03b3 (9)\nfor some power coefficient \u03b3 6= 0. Similar behavior can be uncovered in various other networks, if sizable enough, such as citation behavior of scientific articles, social networks, etc. A simple, generic metric to detect criticality in networks is the degree distribution, defined as the number of (weighted) links connecting to one node.\nFurther, also the correlation between nodes is nontrivial, such that nodes with similar degree have higher probability of being connected than nodes of different degree [23], chapter VII. We will follow a similar path as proposed above and grow an experimental network with new nodes having the simplest preferential and directed attachment towards existing nodes, as a function of their degree:\n\u03a0(k) \u223c k\u03b1 (10)\nHere, \u03a0(k) denotes the probability that some node will grow a link to another node of degree k. Hence, every new node, will prefer nodes with higher degrees, leading to the overall power distribution observed in the real world systems. Additional metrics we look at are single neuron activity as well as layer activity and pattern behavior; more details on that in section 4."}, {"heading": "3 Criticality in deep learning nets", "text": ""}, {"heading": "3.1 From feed-forward to fully connected architecture", "text": "We will focus now on a feed-forward network, with two layers, ai and bj connected via a weight matrix wij ; In order to probe our system for criticality, we write down its Hamiltonian\nH = \u2212 1 2N N\u2211 ij wijaibj \u2212 h N\u2211 i bi (11)\nwhich has been first formulated in the seminal paper [9]. Here, the values of the a and b are {0, 1}. Further, by absorbing the biases bi in the usual way we can assume our weight matrix has the form:\nW =  2Nh 0 \u00b7 \u00b7 \u00b7 0 2Nh w11 \u00b7 \u00b7 \u00b7 w1n\n... ...\n. . . ...\n\u22122Nh wn1 \u00b7 \u00b7 \u00b7 wnn  (12) while the Vi read (1, V1, \u00b7 \u00b7 \u00b7 , VN ). This Hamiltonian describes a two layer net containing rectified linear units (ReLU) in the b-layer with a common bias term h. The weight matrix wij sums the binary inputs coming from the ai and those are fed into bi; depending whether the ReLU threshold has been reached, ai is activated, hence the binary values allowed for both, inputs and b-layer.\nFurther, we show in appendix A, that the partition function is up to a constant the same for the units taking values in {\u00b11} or {0, 1}. By redefining N+1\u2192 N We can then formulate the partition function as\nZ = \u2211\na,b\u2208{\u00b11}\ne\u2212 \u03b2 2N \u2211 ijWijaibj (13)\nwhere \u03b2 is the inverse temperature 1/T . This is the partition function of a bipartite graph with non-constant connection matrix w.\nHowever, it turns out, that the partition function of the fully connected layer is the highest contribution (1st order) of our feed forward network (see appendix B), hence further simplifying the expression to\nZ = \u2211\nsi\u2208{\u00b11}\ne\u2212 \u03b2 2N \u2211 ijWijsisj (14)\nWe will now proceed and compute the free energy F , defined as F = \u2212T lnZ, using the procedure presented in [10]. From the free energy we then find all quantities of interest, especially the 2-point correlation function of the neurons."}, {"heading": "3.2 Fully connected architecture with non-constant weights", "text": "In order to solve the CW model analytically, one has to perform the sum over spins, which is hindered by the quadratic term sisj . The standard way to overcome this problem is the gaussian linearisation trick which replaces quadratic term by its square root - linear in si and one additional continuous variable - the \u201dmean\u201d field, which is being integrated over entire R:\nea 2 = 1\u221a 2\u03c0 \u222b \u221e \u2212\u221e dxe\u2212x 2/2+ \u221a 2ax (15)\nwhich in physics, is known as the HubbardStratonovich transform.\nUnfortunately our coupling is not scalar, and hence we will linearise the sum term by term to keep track of all the weight matrix entries. First we will insert N identities via the Dirac delta function into our Hamiltonian as used in (14):\nH(s) = \u2212 1 2N N\u2211 ij siWijsj (16)\n= \u2212 1 2N \u220f k \u222b \u221e \u2212\u221e dVk\u03b4(sk \u2212 Vk) N\u2211 ij ViWijVj\n= \u220f k \u222b \u221e \u2212\u221e dVk\u03b4(sk \u2212 Vk)H(V )\nWith the definition of the delta function \u03b4(x) = 1 2\u03c0i \u222b i\u221e \u2212i\u221e dye xy the partition function (14) reads now\nZ(s) = \u220f k \u222b \u221e \u2212\u221e dVk\u03b4(sk \u2212 Vk) \u2211\nsi\u2208{\u00b11}\ne\u2212\u03b2H(V ) (17)\n\u223c \u220f k \u222b \u221e \u2212\u221e dVk \u222b i\u221e \u2212i\u221e dUk \u2211\nsi\u2208{\u00b11}\neUk(sk\u2212Vk)e\u2212\u03b2H(V )\n= \u220f k \u222b \u221e \u2212\u221e dVk \u222b i\u221e \u2212i\u221e dUke \u2212UkVk+ln(coshUk)e\u2212\u03b2H(V )\nAs already stated, we could perform the sum over the binary units si, since they show up linearly in the exponential after the change of variables via delta identity1; we effectively converted the sum over binary values {\u00b11} into integrals over R, leading to\n1In general we\u2019re not interested in numerical multiplicative constants, as later on, when logging the partition and computing the free energy, those terms will be simple additive constants without any contribution after differentiating the expression\nZ = c \u220f i \u222b \u221e \u2212\u221e dVi \u222b i\u221e \u2212i\u221e dUi e \u2212Hg(V,U,T ) (18)\nwith a generalised Hamiltonian\nHg =\u2212 \u03b2 2N \u2211 ij WijViVj + \u2211 i [ UiVi \u2212 ln (coshUi) ] =\u2212 \u03b2\n2N\n\u2211 ij wijViVj \u2212 \u03b2h \u2211 i Vi\n+ \u2211 i [ UiVi \u2212 ln (coshUi) ] (19)\nUltimately we are interested in the free energy per unit, which contains the partition function, via\nF = lim N\u2192\u221e\n(\u2212T lnZ)/N (20)\nFrom F we can now obtain all quantities of interest via derivatives, in our case with respect to h. The partition function Z still contains a product of double integrals, which can be solved via the saddle point approximation; we recall here the one-dimensional case\u222b \u221e\n\u2212\u221e dxe\u2212f(x) \u2248 ( 2\u03c0 f \u2032\u2032(x0) )1/2 e\u2212f(x0) (21)\nwhere x0 is the stationary value of f and f \u2032\u2032(x0) is in our case the Hessian evaluated at the stationary point:\nHgViVj = \u2212 \u03b2\nN wij (22)\nHgUiUj = \u2212\u03b4ij(1\u2212 tanh 2 Ui) HgViUj = \u03b4ij\n(23)\nwhile Hg is given in (19).\nThe expression 18 can now be computed by applying simultaneously the saddle point conditions for both integrals. The stationarity conditions2 for Vi and Ui give\n\u2202Hg\n\u2202Vi = \u2212 \u03b2 N \u2211 j WijVj + Ui = 0 (24)\n= \u2212\u03b2( \u2211 j wijVj/N + h) + Ui = 0\n\u2202Hg \u2202Ui = Vi \u2212 tanhUi = 0\n2We keep in mind that we enlarged W to contain h as well, hence the explicit equations are h dependent\nwhich combined deliver the self consistency mean field equation of the fully connected layer (29). Further, denoting Hg0 the the Hamiltonian satisfying the stationarity conditions, it reads\nHg0 = \u03b2\n2N\n\u2211 ij wijViVj (25)\n\u2212 \u2211 i ln cosh\u03b2( \u2211 j wijVj/N + h)\nEquation (25) already displays manifestly the consistency equation for the mean field, as taking the first derivative wrt. Vi leaves exactly the consistency equation over per its construction;\nNow we can rewrite the free energy (20) as\nF = lim N\u2192\u221e\nT\nN\n[ Hg0 + ln detH g hh ] \u223c lim N\u2192\u221e\n(26)[ 1 2N2 \u2211 ij wijViVj + 1 N2 \u2211 ij ln[wij(1\u2212 V 2i )\u2212 1]\n\u2212 T N \u2211 i ln cosh\u03b2( \u2211 j wijVj/N + h) ] We need to address now the large N limit; obviously the second term coming from the determinant clearly vanishes in the large-N limit, as the logarithm is slowly increasing, while we divide through N2; the first term - a double sum over Vi is of order N\n2 and hence a well defined average in the limit; the last term - ln cosh, when expanded, is again linear in the sum3, and hence a well defined average after dividing through N , hence we\u2019re left with the free energy\nF = T\nN Hg0 (27)\n= 1\n2N2 \u2211 ij wijViVj\n\u2212 T N \u2211 i ln cosh\u03b2( \u2211 j wijVj/N + h) ] We\u2019re at the point now, where all quantities of interest can be derived from the free energy F ; the order parameter (aka magnetisation when dealing with spins) per unit is defined as\nm \u2261 dF dh = \u2202F \u2202h \u2223\u2223\u2223\u2223 V st + 7 0 \u2202F \u2202Vi \u2202Vi \u2202h \u2223\u2223\u2223\u2223 V st\n(28)\n3The interior sum over j is an average, hence well defined in the limit; after expansion, we\u2019re left with the outer sum (over i), which is again a well defined average when divided by N\nThe second term on the right vanishes identically, as we recognize it being evaluated at the stationarity condition V st for the Hamiltonian. The contribution of the first term is:\n\u2211 ik wikVk/N = 1 N \u2211 i tanh\u03b2( \u2211 k wikVk + h) (29)\nm Vi = tanh\u03b2( \u2211 k wikVk/N + h)\nwhich is (the weighted sum version of) the iconic selfconsistency mean field equation of the CW magnet (6).\nThe critical point, Pc is located where the correlation function diverges for h \u2192 0; the 2-point correlation function (aka susceptibility when dealing with spins) is the second derivative of F, i.e. the derivative of (29) wrt. h:\nPc \u2261 d2F dh2 = dm dh (30)\nm \u2202Vi \u2202h = \u03b2(1\u2212 V 2i )(1 + \u2211 k wik \u2202Vk \u2202h /N)\nwhere we used the original equation (29) for taking the derivatives. It is worth contemplating first equations (29) and (30). They both capture the essence of the criticality of our system, including it\u2019s power law behavior. When the weight matrix reduces to a scalar coupling, both equations reduce to the classical CW system and display the behavior shown in (7) and (8). Furthermore, eq. (30) encodes all the information needed for finding the critical point of matrix system at hand; we recall that all V s (and their derivatives) are already implicitly \u201dsolved\u201d in terms of h and wij via the stationarity equation (29) and hence the Vi are just place holders for functions of w and h; we\u2019re thus left with a non-linear system of first order differential equations in N variables, which will produce poles for specific values of the couplings and temperature at criticality."}, {"heading": "4 Experimental results", "text": "After investigating criticality through the partition function in our theoretical setup, now we turn to a practical question: do current deep learning networks exhibit critical behaviour, or put it differently, can we spot traces of critical phenomena in them? Instead of directly attacking the partition function of real world deep neural nets, we start with the practical observation, that systems at around criticality show off power law distributions in certain internal attributes.\nConcretely for networks [23, 24] we look for traces of power laws in weight distributions, layer activation pattern frequencies, single node activation frequencies and average layer activations. In the following we will present experimental results for multilayer feed-\nforward networks, convolutional neural nets and autoencoders.\nFor all networks we ran experiments on the CIFAR10 dataset, training each models for 200 epochs using ReLU activations and Adam Optimizer without gradient clipping and run inferences for 100 epochs. The feed forward network had 3 layers with 500, 400 and 200 nodes, the CNN had 3 convolutional layers followed by 3 fully connected layers and the autoencoder had one layer with 500 nodes.\nFor weight distributions we looked at sums of absolute values of the outgoing weights at each node, as a weighted order of the node. In fig. 1 we have a log-log plot of counts versus the node order as defined above, and detect no linear behavior.\nFor layer activation patterns we counted the frequency of each layer activations through the inference epochs. Figures 2 and 3 are log-log plots of layer activation frequencies versus their respective counts for the feedforward layer the autoencoder. As we see, the hidden layer activation pattern frequencies of the Autoencoder resembles a truncated straight line, indicating that learning hidden features in unsupervised manner can give rise for scale free, power law phenomena in accordance with the findings of [24], but no other architectures show traces of any power law.\nFor single node activation frequencies we counted the frequency of each node activations through the inference epochs.\nFigures 6 and 7 depict the behavior of feed-forward and CN network. The flat, nearly horizontal line in the latter architecture is again a sign of missing exponent whatsoever.\nAs a last measure we employed the sum of activations defined as the average activations on each layer throughout the inference epochs.\nSpontaneous and detectable criticality did not arise in classical architectures so the next step will be to create and experiment with systems that have induced criticality and learning rules that take into account criticality. Our first approach was to grow a fully connected net using the preferential attachment algorithm to induce at least some power law in node weights, and use the fully connected net as a hidden to hidden module. We further experimented with different solutions, regarding input and read out of activations from this hidden to hidden module, without changing the power law distribution. (This would simulate a system located at a critical state, with power law weight distribution). Our findings so far show that learning in these systems is very unstable without any advancement in learning and inference. The fundamental missing part is how to naturally induce a critical state in a network, which is equipped with learning rules that inherently take into account the critical state. For that we need new architectures and new learning rules, derived from the critical point equations (30)."}, {"heading": "5 Summary and outlook", "text": "Summary: In this article we make our first steps in investigating the relationship between criticality and deep learning networks. After a short introduction of criticality in statistical physics and real world networks we started with the theoretical setup of a fully connected layer. We used continuous mean field approximation techniques to tackle the partition function of the system ending up with a system of differential equations that determine the critical behaviour of the system. These equations can be the starting point for a possible network architecture with induced criticality and learning rules exploiting criticality. After that we presented results of experiments aiming to find traces of power law distributions in current deep learning networks such as multilayer feed-forward nets, convolutional networks and autoencoders. The results - except for the autoencoder - were affirmative in the negative sense, setting up as next the necessity to create networks with induced criticality and learning rules that exploit the critical state.\nOutlook: Obviously the fully connected layer, which can be solved analytically on the theoretical side is of limited importance, as it translates into a rather simplistic architecture; more realistic, widely used setups, e.g. convolutional or recurrent nets, do very well contain the feed-forward mechanism, but are strongly deviating and hence only partially mapped to our theoretical treatment; it would definitely be essential to address theoretically the convolution mechanism of deep\nnets and establish a link between the theoretical and experimental side; also inducing criticality into the net via eq. (30) could prove beneficial and might very well affect learning behavior and flow on the surface on the loss function.\nAppendix"}, {"heading": "A Different unit values", "text": "We here show that the partition function with Hamiltonian H{0,1} = \u2211 ij aiwijaj + hiai (31)\nwho\u2019s units are taking values in {0, 1} has the same qualities as encoded in the partition function with Hamiltonian H{\u00b11}, who\u2019s units take values in {\u00b11}.\nWe rewrite the Hamiltonian in (31) with units taking values in {\u00b11} (using Einstein\u2019s summation convention over double indices) :\nH = 1\n4 (1\u2212 ui)wij(1\u2212 uj) +\n1 2 hi(1\u2212 ui) (32)\nwhere the ui and vi take values in {\u00b11}. Carrying now the multiplications in (32) yields\nH = 1\n4 \u2211 ij wij + 1 4 uiwijuj\n\u2212 2 4 uiwij1j \u2212 1 2 hiui + 1 2 hi (33) = 1\n2 (c+ uiwijvj + h\n\u2032 iui)\nwith h\u2032i = \u2212wij1j \u2212 hi. Hence when computing the partition Z with (32) we obtain\nZ = \u2211\nu\u2208{\u00b11}\neH = ec \u2211\na\u2208{0,1}\neaiwijaj+h \u2032 iai (34)\nwhere the right hand side is the original Hamiltonian with a shifted coupling h\u2032. The additional constant c factors out completely and hence when taking the logarithm and the second derivative it won\u2019t change the outcome. Also we note that the second derivative wrt. h\u2032 is \u2202h\u2032h\u2032 = \u2202hh."}, {"heading": "B First order contribution", "text": "We consider here the Hamiltonian of the bi-partite graph connected via weight matrix w (with Einstein summation convention):\nHb = uiwijvj (35)\nwith the free energy\nFb = \u2212 ln \u2211\nu,v\u2208{\u00b11}\nexp(uiwijvj) (36)\nWithout any loss of generality we set the temperature T = 1, and we won\u2019t keep track of it. Carrying the partial sum over vi yields\nFb = \u2212 ln \u2211 u \u220f j [ exp(uiwij) + exp(\u2212uiwij) ] (37)\n= \u2212 ln \u2211 u \u220f j [ 2 cosh(uiwij) ] The sum over the vi is understood as a collection of 2N terms, each corresponding to a unique combination of 0\u2019s and 1\u2019s in the vector of length N representing that specific state of the spins; however, the sum can be conveniently written as a product of N binary summands, where each contains exactly the two possible states of the ith spin - this is where the product over j comes from in upper formula. Expanding now to lowest order in w we obtain\nFb \u223c \u2212 ln \u2211 u \u220f j (1 + (uiwij) 2 2 ) (38)\n\u223c \u2212 ln \u2211 u exp \u2211 j (uiwij) 2 2\n= \u2212 ln \u2211 u eH(ui)\nwhere H(ui) is the Hamiltonian of the fully connected graph, defined as (Einstein summation convention)\nH(ui) = \u2211 j (uiwij) 2 2 (39)\n= 1\n2 \u2211 ik ( \u2211 j wijwjk)\ufe38 \ufe37\ufe37 \ufe38 w\u2032ik uiuk\n= 1\n2 \u2211 ik uiw \u2032 ikuk\nA few notes are in place regarding eq. (39): the matrix w\u2032ik is now symmetric by construction and hence mediates between equally sized (actually identical) layers; further, all higher terms of the cosh function are even, hence all contributions are higher order, symmetric interactions of the layer ui with itself."}], "references": [{"title": "How Nature Works: the science of selforganized criticality", "author": ["Per Bak"], "venue": "Copernicus Springer-Verlag,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Scale Invariance, From Phase Transitions to Turbulence", "author": ["Lesne Annick", "Lagues Michel"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Self-organised criticality", "author": ["Gunnar Pruessner"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Neocognitron: A Selforganizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position", "author": ["Kunihiko Fukushima"], "venue": "www.cs.princeton.edu/courses/archive/", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Neural Networks and physical systems with emergent collective computational abilities,", "author": ["J.J. Hopfield"], "venue": "Proceedings of the National Academy of Science, USA,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "A Mean Field Theory Learning Algorithm for Neural Networks", "author": ["Carsten Peterson", "James R . Anderson"], "venue": "Complex Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1987}, {"title": "Complexity and Criticality (Advanced Physics Texts)", "author": ["Kim Christensen", "Nicholas R. Moloney"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "One rule of life: Are we poised on the border of order?", "author": ["Philip Ball"], "venue": "New Scientist,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Renormalization Group: Applications in Statistical Physics", "author": ["Uwe C. Tauber"], "venue": "Nuclear Physics B,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The Origins of Order: Self- Organization and Selection in Evolution", "author": ["Stuart A. Kauffman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1993}, {"title": "Introduction to statistical physics", "author": ["Silvio Salinas"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Weak pairwise correlations imply strongly correlated network states in a neural population", "author": ["Elad Schneidman", "Michael J. Berry II", "Ronen Segev", "William Bialek"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term \u201duniversality\u201d, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3].", "startOffset": 310, "endOffset": 313}, {"referenceID": 1, "context": "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term \u201duniversality\u201d, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3].", "startOffset": 345, "endOffset": 348}, {"referenceID": 2, "context": "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term \u201duniversality\u201d, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3].", "startOffset": 418, "endOffset": 421}, {"referenceID": 3, "context": "However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10].", "startOffset": 181, "endOffset": 190}, {"referenceID": 4, "context": "However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10].", "startOffset": 207, "endOffset": 214}, {"referenceID": 5, "context": "However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10].", "startOffset": 207, "endOffset": 214}, {"referenceID": 6, "context": "[14, 15, 16, 17].", "startOffset": 0, "endOffset": 16}, {"referenceID": 7, "context": "[14, 15, 16, 17].", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "[14, 15, 16, 17].", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "[14, 15, 16, 17].", "startOffset": 0, "endOffset": 16}, {"referenceID": 10, "context": "1 Curie-Weiss model A simplistic, fully solvable model for a magnet is the Curie-Weiss model (CW), [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "which can be fully solved, [19], summing over each of the 2 states; given an explicit partition Z, the free energy can be computed via", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "Two of the main motivations why we look for criticality and exploit on it in artificial networks, are the universal arising of this phenomenon as well as various hints of its occurrence in biological [4] and neural systems [25, 22]; once systems get \u201dsizable\u201d enough, gaining complexity, critical behavior emerges, which also applies to man-made nets [23].", "startOffset": 223, "endOffset": 231}, {"referenceID": 4, "context": "which has been first formulated in the seminal paper [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "We will now proceed and compute the free energy F , defined as F = \u2212T lnZ, using the procedure presented in [10].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks.", "creator": "LaTeX with hyperref package"}}}