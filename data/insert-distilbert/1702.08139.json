{"id": "1702.08139", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "abstract": "extremely recent work on a generative modeling of text networks has found suggest that variational auto - encoders ( vae ) incorporating lstm decoders perform worse than simpler lstm language reconstruction models ( bowman smith et al., 2015 ). this negative result is so far poorly understood, but has been attributed to the propensity of lstm decoders to ignore conditioning information accessible from the encoder. in this paper, we experiment with a new type of decoder for vae : a dilated expression cnn. by changing the decoder's dilation language architecture, we control the effective context from previously generated words. in modern experiments, instead we find that theoretically there is a trade off between the contextual capacity of the decoder and the amount intensity of encoding quality information used. we show that although with being the right decoder, vae can outperform lstm language models. we demonstrate perplexity gains demonstrated on two datasets, representing finding the first positive data experimental result on the use vae for generative modeling of speech text. further, we conduct an in - depth investigation of the use of vae ( with our new decoding architecture ) for complex semi - supervised and sequential unsupervised labeling tasks, systematically demonstrating gains over several slightly strong baselines.", "histories": [["v1", "Mon, 27 Feb 2017 04:16:01 GMT  (1084kb,D)", "http://arxiv.org/abs/1702.08139v1", "12 pages"], ["v2", "Sun, 18 Jun 2017 00:31:34 GMT  (6060kb,D)", "http://arxiv.org/abs/1702.08139v2", "camera ready"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["zichao yang", "zhiting hu", "ruslan salakhutdinov", "taylor berg-kirkpatrick"], "accepted": true, "id": "1702.08139"}, "pdf": {"name": "1702.08139.pdf", "metadata": {"source": "META", "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "authors": ["Zichao Yang", "Zhiting Hu", "Ruslan Salakhutdinov", "Taylor Berg-Kirkpatrick"], "emails": ["<zichaoy@cs.cmu.edu>,", "<zhitingh@cs.cmu.edu>,", "<rsalakhu@cs.cmu.edu>,", "<tbergkir@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "Generative modeling techniques play an important role in many machine learning application areas. Generative models allow for principled and effective use of unlabeled data and therefore facilitate unsupervised and semi-supervised learning. Recent use of deep neural networks inside of generative models has lead to model classes that are particularly flexible and can potentially model a wide range of\n1Carnegie Mellon University. Correspondence to: Zichao Yang <zichaoy@cs.cmu.edu>, Zhiting Hu <zhitingh@cs.cmu.edu>, Ruslan Salakhutdinov <rsalakhu@cs.cmu.edu>, Taylor Berg-Kirkpatrick <tbergkir@cs.cmu.edu>.\ndata and modalities, including both images and text. We focus on a specific instance of this class: the variational autoencoder1 (VAE) (Kingma & Welling, 2013).\nThe generative story behind the VAE (to be described in detail in the next section) is simple: First, a continuous latent representation is sampled from a Gaussian. Then, an observed sample is generated from a neural decoder, conditioned on the latent representation. The latent representation (which must be marginalized out) is intended to give the model more expressive capacity when compared with simpler neural generative models\u2013for example, conditional language models. Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).\nHowever, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016). The obvious choice for decoding architecture for a textual VAE is an LSTM, a typical workhorse in the language processing community. Bowman et al. (2015) demonstrated negative results using VAEs for text modeling, finding that they perform worse than LSTM language models. In particular, they observe that the LSTM decoder does not make effective use of the latent representation (even when combined with more sophisticated training techniques) and as a result VAE collapses to a simple language model. Related work (Miao et al., 2016; Larochelle & Lauly, 2012; Mnih & Gregor, 2014) has used simpler decoders that model text as a bag of words. Their results indicate better use of latent representations, but their decoders are too simple to effectively model longer-range dependencies in text.\nMotivated by these observations, we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data. We propose the use of a dilated CNN as a decoder in VAE, inspired by the recent success of using CNN for audio, image and lan-\n1The name VAE is typically used to refer to both a model class and an inference procedure. Here we use it to refer to the model class.\nar X\niv :1\n70 2.\n08 13\n9v 1\n[ cs\n.N E\n] 2\n7 Fe\nb 20\n17\nguage modeling (van den Oord et al., 2016a; Kalchbrenner et al., 2016a; van den Oord et al., 2016b). In contrast with this prior work where extremely large CNNs are used, we exploit the dilated CNN for its flexibility in varying the amount of conditioning context. In the two extremes, depending on the choice of dilation, the CNN decoder can reproduce a simple MLP using a bags of words representation of text, or can reproduce the long-range dependence of recurrent architectures (like an LSTM) by conditioning on the entire history. Thus, by choosing a dilated CNN as the decoder, we are able to conduct experiments where we vary contextual capacity, finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation produced by the encoder. We demonstrate that when this trade off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders (Chen et al., 2016). We go on to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering, outperforming several strong baselines on both text categorization and sentiment analysis.\nOur contributions are as follows: First, we propose the use of a dilated CNN as a new decoder for VAE. We then empirically evaluate several dilation architectures with different capacities, finding that reduced contextual capacity leads to stronger reliance on latent representations. By picking a decoder with suitable contextual capacity, we find our VAE performs better than LSTM language models on two data sets. We explore the use of dilated CNN VAEs for semisupervised classification and find they perform better than strong baselines from (Dai & Le, 2015). Finally, we verify that the same framework can be used effectively for unsupervised clustering."}, {"heading": "2. Model", "text": "In this section, we begin by providing background on the use of variational autoencoders for language modeling. Then we introduce the dilated CNN architecture that we will use in experiments as a new decoder for VAE. Finally, we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification and unsupervised clustering."}, {"heading": "2.1. Variational Autoencoder for Language Modeling", "text": "Language models (Mikolov et al., 2010) typically generate each token xt conditioned on the entire history of previously generated tokens:\np(x) = \u220f t p(xt|x1, x2, ..., xt\u22121). (1)\nState-of-the-art language models generally parametrize these conditional probabilities using RNNs, which compute an evolving hidden state over the sequence and predicts xt based on the hidden state. Such models, though effective in modeling text, do not learn a vector that represents the full sequence (Bowman et al., 2015).\nBowman et al. (2015) proposes a different approach to generative text modeling. Instead of modeling the joint probability p(x) directly as in Equation 1, we specify a generative process for which p(x) is a marginal distribution. Specifically, we first generate a continuous latent vector representation z from a Gaussian prior p(z), and then generate the sequence x from a conditional distribution (the decoder) p(x|z). To estimate parameters for this model we would like to maximize the marginal probability p(x) = \u222b p(z)p(x|z)dz. The marginal probability is intractable, but the following variational lower bound is often used as an objective:\n\u2212 log p\u03b8(x) = \u2212 log \u222b p\u03b8(z)p\u03b8(x|z)dz\n\u2264 Eq\u03c6(z|x)[\u2212 log p\u03b8(x|z)\u2212 log p\u03b8(z) + log q\u03c6(z|x)] = \u2212Eq\u03c6(z|x)[log p\u03b8(x|z)] + KL(q\u03c6(z|x)||p\u03b8(z)).\nWe optimize the lower bound w.r.t. the model parameters \u03b8 and the parameters of our approximation to posterior, \u03c6 (often called the recognition model or encoder.) In order for the bound to be tight, the posterior probability p\u03c6(z|x) needs to be close to the true posterior. p\u03c6(z|x) is typically assumed to be Gaussian so that the re-parametrization trick from (Kingma & Welling, 2013) can be used.\nThis model and inference procedure are often referred to as a VAE. In contrast with Equation 1, this distribution conditions on a latent representation z:\np(x|z) = \u220f t p(xt|x1, x2, ..., xt\u22121, z). (2)\nThe desired result is that learned representations z contains some high level information such as topic, which is helpful in predicting tokens xt.\nWe can also view the VAE as a regularized version of the autoencoder. If only the first part of the lower bound objective Eq\u03c6(z|x)[log p\u03b8(x|z)] is used as the objective function, the variance of the posterior probability q\u03c6(z|x) will be very small and it collapses to an autoencoder. With the regularization from the KL-divergence term KL(q\u03c6(z|x)||p\u03b8(z)), the variational autoencoder not just learns to encode x as a single point z, it instead learns a distribution over the latent space.\nThe encoder (recognition model) and decoder (generative model) are typically parametrized with neural networks. For images, the encoder and decoder can be MLPs or\nCNNs. For text, a RNN such as a LSTM is used as in (Bowman et al., 2015). However, the authors find the decoder depends too much on context information and the latent representation from the encoder is ignored. We suspect that it is the decoder model that plays an important role. If the decoder relies too much on context, the VAE tends to ignore the latent representation, turning into a standard RNN language model. Hence, we propose to use a dilated CNN as the decoder. The architecture flexibility of CNNs allows us to change the contextual capacity, hence control the context information and latent representation trade-off. In two extreme cases, when the effective contextual width of a CNN is very large, it resembles the behavior of LSTM and when it is very small, it behaves like a bag of words model."}, {"heading": "2.2. Dilated Convolutional Decoder", "text": "The CNN used for text modeling (Kalchbrenner et al., 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.\nOne Dimensional Convolution: Note that xt can only condition on past tokens x<t, applying the traditional convolution will break this and use tokens x\u2265t as inputs to predict xt. We can avoid this either by applying a mask on the convolution filter or shift the input by several slots (van den Oord et al., 2016b). Here we adopt the second approach. The overall model architecture is shown in Figure 1.\nSuppose we use convolution with filter size k and use n layers, then the effective filter size (the number of past tokens to condition to in predicting xt) is (k \u2212 1) \u00d7 n + 1. The filter size grows linearly with the depth of the network.\nDilation: Dilated convolution (Yu & Koltun, 2015) was introduced to greatly increase the effective receptive field size without increasing the computational cost. With dilation d, the convolution is applied so that the inputs are skipped d \u2212 1 values. Casual convolution can be seen a special case with d = 1. With dilation, the effective receptive size grows exponentially with network depth. In Figure 1, we\nuse dilation of size 2, 4 in the second and third layer. Suppose the dilation size in the i-th layer is di and we use the same filter size k in all layers, then the effective filter size is (k\u2212 1) \u2211 i di+1. The dilations are typically set to double every layer di+1 = 2di, hence the effective receptive field size can grow exponentially. Hence, the contextual capacity of a CNN can be controlled by manipulating the filter size, dilation size and network depth.\nResidual Connection: Residual connection (He et al., 2016) is used in the decoder to speed up convergence and enable us to train deep models. Our residual block is similar to that of (Kalchbrenner et al., 2016a) and is shown in Figure 3. We use three convolutional layers with filter size 1\u00d71, 1\u00d7k, 1\u00d71 respectively. ReLU activation function is used between the convolutional layers. The residual block can be more powerful by adding batch normalization and gating mechanism (van den Oord et al., 2016b; Kalchbrenner et al., 2016a).\nOverall architecture: Our VAE architecture is shown in Figure 2. We use LSTM as the encoder to get the posterior probability q(z|x), which we assume to be diagonal Gaussian. We parametrize the mean \u00b5 and variance \u03c3 with LSTM output. We sample z from q(z|x), the decoder is conditioned on the sample by concatenating z with every word embedding of the decoder input."}, {"heading": "2.3. Semi-supervised VAE", "text": "In this section, we briefly review semi-supervised VAEs of (Kingma et al., 2014) that can incorporate labels. Given the labeled set (x, y) \u223c DL and the unlabeled set x \u223c DU ,\n(Kingma et al., 2014) proposed a semi-supervised VAE model whose latent representation contains both continuous variable z and discrete label y:\np(x,y, z) = p(y)p(z)p(x|y, z). (3)\nThe semi-supervised VAE trains a discriminative network q(y|x), an inference network q(z|x,y) and a generative network p(x|y, z) jointly by minimizing the variational lower bound. For labeled data (x,y), the variational lower bound is\n\u2212 log p(x,y) \u2264\u2212 Eq(z|x,y)[log p(x|y, z)] + KL(q(z|x,y)||p(z))\u2212 log p(y)\n=L(x,y)\u2212 log p(y).\nFor unlabeled data x, the label y is treated as a latent variable and marginalized out in the training objective:\n\u2212 log p(x) \u2264U(x) =\u2212 Eq(y|x)[Eq(z|x,y)[log p(x|y, z)]\n+ KL(q(z|x,y)||p(z))\u2212 log p(y) + log q(y|x)] = \u2211 y q(y|x)L(x,y) + KL(q(y|x)||p(y)).\nCombining the labeled and unlabeled data loss, we have the overall objective as:\nJ =E(x,y)\u223cDL [L(x,y)] + Ex\u223cDU [U(x)] + \u03b1E(x,y)\u223cDL [log q(y|x)],\nwhere \u03b1 controls the trade off between generative loss and discriminative loss.\nSince y is a discrete variable, we have to compute the marginal probability by iterating all classes. The computational cost scales linearly with the number of classes.\nGumbel-Softmax: (Jang et al., 2016; Maddison et al., 2016) propose a continuous approximation to the samples of categorical distribution. Let u be a categorical distribution with probabilities \u03c01, \u03c02, ..., \u03c0c, the samples from categorical distribution can be approximated using:\nyi = exp((log(\u03c0i) + gi)/\u03c4)\u2211c j=1 exp((log(\u03c0j) + gj)/\u03c4) , (4)\nwhere gi follows Gumbel(0, 1). We can obtain the samples from Gumbel distribution by first sample u \u223c Uniform(0, 1) and then compute g = \u2212 log(\u2212 log(u)). The approximation is accurate when \u03c4 \u2192 0 and is smooth when \u03c4 > 0. In experiments, we anneal \u03c4 so that it is large and sample variance is small at beginning and then gradually decrease \u03c4 .\nWe use Gumbel-Softmax to approximate the samples from p(y|x) to reduce the computational cost. We can directly\nback propagate the gradients of U(x) to the discriminator network.\nUnsupervised clustering: In this section we adapt the same framework for unsupervised clustering. We directly minimize the objective U(x), which is consisted of two parts: reconstruction loss and KL regularization on q(y|x). The first part encourages the model to assign x to label y such that the reconstruction loss is low. We find that the model can easily get stuck in two local optimum: the KL term is very small and q(y|x) is close to uniform distribution or the KL term is very large and all samples collapse to one class. In order to make the model more robust, we modify the KL term by:\nKLy = max(\u03b3,KL(q(y|x)|p(y)). (5)\nThat is, we only minimize the KL term when it is large enough."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Data sets", "text": "Since we would like to investigate VAEs for language modeling and semi-supervised classification, the data sets should be suitable for both purposes. We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015). The original data sets contain millions of samples, of which we sample 100k as training and 10k as validation and test from the respective partitions. The detailed statistics of both data sets are in Table 1. Yahoo Answer contains 10 topics including Society & Culture, Science & Mathematics etc. Yelp15 contains 5 level of rating, with higher rating better."}, {"heading": "3.2. Model configurations and Training details", "text": "We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders. For CNNs, we explore several different configurations. We set the convolution filter size to be 3 and gradually increase the depth and dilation from [1, 2, 4], [1, 2, 4, 8, 16] to [1, 2, 4, 8, 16, 1, 2, 4, 8, 16]. They represent small, medium and large model and we name them as SCNN, MCNN and LCNN. We also explore a very large model with dilations [1, 2, 4, 8, 16, 1, 2, 4, 8, 16, 1, 2, 4, 8, 16] and name it as VLCNN. The effective filter size are 15, 63, 125 and 187 respectively. We use the\nlast hidden state of the encoder LSTM and feed it though an MLP to get the mean and variance of q(z|x), from which we sample z and then feed it through an MLP to get the starting state of decoder. For the LSTM decoder, we follow (Bowman et al., 2015) to use it as the initial state of LSTM and feed it to every step of LSTM. For the CNN decoder, we concatenate it with the word embedding of every decoder input.\nThe architecture of the Semi-supervised VAE basically follows that of the VAE. We feed the last hidden state of the encoder LSTM through a two layer MLP then a softmax to get q(y|x). We use Gumbel-softmax to sample y from q(y|x). We then concatenate y with the last hidden state of encoder LSTM and feed them throught an MLP to get the mean and variance of q(z|y,x). y and z together are used as the starting state of the decoder.\nWe use a vocabulary size of 20k for both data sets and set the word embedding dimension to be 512. The LSTM dimension is 1024. The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally, as shown in Figure 3. We select the dimension of z from [32, 64]. We find our model is not sensitive to this parameter.\nWe use Adam (Kingma & Ba, 2014) to optimize all models and the learning rate is selected from [2e-3, 1e-3, 7.5e-4] and \u03b21 is selected from [0.5, 0.9]. Empirically, we find learning rate 1e-3 and \u03b21 = 0.5 to perform the best. We select drop out ratio of LSTMs (both encoder and decoder)\nfrom [0.3, 0.5]. Following (Bowman et al., 2015), we also use drop word for the LSTM decoder, the drop word ratio is selected from [0, 0.1, 0.3, 0.5, 0.7]. For the CNN decoder, we use a drop out ratio of 0.1 at each layer. We do not use drop word for CNN decoders. We use batch size of 32 and all model are trained for 40 epochs. We start to half the learning rate every 2 epochs after epoch 30. Following (Bowman et al., 2015), we use KL cost annealing strategy. We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T . We treat T as a hyper parameter and select it from [10k, 40k, 80k]."}, {"heading": "3.3. Language modeling results", "text": "The results for language modeling are shown in Table 2. We report the negative log likelihood (NLL) and perplexity (PPL) of the test set. For the NLL of VAEs, we decompose it into reconstruction loss and KL divergence and report the KL divergence in the parenthesis. To better visualize these results, we plot the results of Yahoo data set (Table 2a) in Figure 4.\nWe first look at the LM results for Yahoo data set. As we gradually increase the effective filter size of CNN from SCNN, MCNN to LCNN, the NLL decreases from 345.3, 338.3 to 335.4. The NLL of LCNN-LM is very close to the NLL of LSTM-LM 334.9. But VLCNN-LM is a little bit worse than LCNN-LM, this indicates a little bit of over-fitting.\nThe cases are different when we the CNNs as decoders for VAEs. We can see that LSTM-VAE is worse than LSTMLM in terms of NLL and the KL term is nearly zero, which verifies the finding of (Bowman et al., 2015). When we use CNNs as the decoders for VAEs, we can see improvement over pure CNN LMs. For SCNN, MCNN and LCNN, the VAE results improve over LM results from 345.3 to 337.8, 338.3 to 336.2, and 335.4 to 333.9 respectively. The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity. When the model is as large as VLCNN, the improvement diminishes and the VAE result is almost the same with LM result. This is also reflected in the KL term, SCNNVAE has the largest KL of 13.3 and VLCNN-VAE has the smallest KL of 0.7. When LCNN is used as the decoder, we obtain an optimal trade off between using contextual information and latent representation. LCNN-VAE achieves a NLL of 333.9, which improves over LSTM-LM with NLL of 334.9.\nWe find that if we initialize the parameters of LSTM encoder with parameters of LSTM language model, we can improve the VAE results further. This indicates better encoder model is also a key factor for VAEs to work well. Combined with encoder initialization, LCNN-VAE improves over LSTM-LM from 334.9 to 332.1 in NLL and from 66.2 to 63.9 in PPL.\nSimilar observation is found for the sentiment data set Yelp in Table 2b. LCNN-VAE improves over LSTM-LM from 362.7 to 359.1 in NLL and from 42.6 to 41.1 in PPL.\nLatent representation visualization: In order to visualize the latent representation, we set the dimension of z to be 2 and plot the mean of posterior probability q(z|x), as shown in Figure 5. We can see distinct different characteristics of topic and sentiment representation. In Figure 5a, we can see that documents of different topics fall into dif-\nferent clusters, while in Figure 5b, documents of different ratings form a continuum, they lie continuously on the xaxis as the review rating increases. This is consistent with sentiment actually being real-valued."}, {"heading": "3.4. Semi-supervised VAE results", "text": "Motivated by the success of VAEs for language modeling, we continue to explore VAEs for semi-supervised learning. Following that of (Kingma et al., 2014), we set the number of labeled samples to be 100, 500, 1000 and 2000 respectively.\nAblation Study: At first, we would like to explore the effect of different decoders for semi-supervised classification. We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in Table. 5. We can see that SCNN-VAESemi has the best classification accuracy of 65.5. The accuracy decreases as we gradually increase the decoder contextual capacity. On the other hand, LCNN-VAE-Semi has the best NLL result. This classification accuracy and NLL trade off once again verifies our conjecture: with small contextual window size, the decoder is forced to use the encoder information, hence the latent representation is better learned.\nComparing the NLL results of Table 5 with that of Ta-\nble 2a, we can see the NLL improves. The NLL of semisupervised VAE improves over simple VAE from 337.8 to 335.7 for SCNN, from 336.2 to 332.8 for MCNN, and from 333.9 to 332.8 for LCNN. The improvement mainly comes from the KL divergence part, this indicates with better latent representation, we can decrease the KL divergence, hence further improving the VAE results.\nCompare with Existing Methods: We compare Semisupervised VAE with the methods from (Dai & Le, 2015), which represent the previous state of the art methods for semi-supervised sequence learning. Dai & Le (2015) pretrains a classifier by initializing the parameters of a classifier with that of a language model or a sequence autoencoder. They find it improves the classification accuracy significantly. Since SCNN-VAE-Semi performs the best according to Table 5, we fix the decoder to be SCNN in this part. The detailed comparison is in Table 4. We can see that semi-supervised VAE performs better than LM-LSTM and LA-LSTM from (Dai & Le, 2015). We also initialize the encoder of the VAE with parameters from LM and find classification accuracy further improves. We also see that the advantage of SCNN-VAE-Semi over LM-LSTM is greater when the number of labeled samples is smaller. The advantage decreases as we increase the number of labeled samples. When we set the number of labeled samples to be 25k, the SCNN-VAE-Semi achieves an accuracy of 70.4, which is similar to LM-LSTM with an accuracy of 70.5. Also, SCNN-VAE-Semi performs better on Yahoo data set than Yelp data set. For Yelp, SCNN-VAE-Semi is a little bit worse than LM-LSTM if the number of labeled samples is greater than 100, but becomes better when we initialize the encoder. Figure 5b explains this observation. It shows the documents are coupled together and are harder to classify. Also, the latent representation contains information other than sentiment, which may not be useful for classification."}, {"heading": "3.5. Unsupervised clustering results", "text": "We also explored using the same framework for unsupervised clustering. We compare with the baselines that extract the feature with existing models and then run Gaussian Mixture Model (GMM) on these features. We find empir-\nically that simply using the features does not perform well since the features are high dimensional. We run a PCA on these features, the dimension of PCA is selected from [8, 16, 32]. Since GMM can easily get stuck in poor local optimum, we run each model ten times and report the best result.\nWe find directly optimizingU(x) does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model. The model only works well for Yahoo data set. This is potentially because Figure 5b shows that sentiment latent representations does not fall into clusters. \u03b3 in Equation 5 is a sensitive parameter, we select it from the range between 0.5 and 1.5 with an interval of 0.1.\nWe use the following evaluation protocol (Makhzani et al., 2015): after we finish training, for cluster i, we find out the validation sample xn from cluster i that has the best q(yi|x) and assign the label of xn to all samples in cluster i. We then compute the test accuracy based on this assignment. The detailed results are in Table 5. We can see SCNN-VAE-Unsup + init performs better than other baselines. LSTM+GMM performs very bad probably because the feature dimension is 1024 and is too high for GMM, even though we already used PCA to reduce the dimension."}, {"heading": "3.6. Conditional text generation", "text": "With the semi-supervised VAE, we are able to generate text conditional on the label. Due to space limitation, we only\nshow one example of generated reviews conditioning on review rating in Table 6. More examples of text generated conditioning on topic and rating are shown in the Appendix. For each group of generated text, we fix z and vary the label y. We use beam search of size 10 in the generation process."}, {"heading": "4. Related work", "text": "Variational inference through re-parameterization trick was initially proposed by (Kingma & Welling, 2013; Rezende et al., 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).\nOur work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016). (Bowman et al., 2015) is the first work to combine VAE with language model and they use LSTM as the decoder and find some negative results. On the other hand, (Miao et al., 2016) models text as bag of words, though improvement has been found, the model can not be used to generate text. Our work fills the gaps between them. (Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported. (Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.\nOur use of CNN as decoder is inspired by recent success of PixelCNN model for images (van den Oord et al., 2016b), WaveNet for audios (van den Oord et al., 2016a), Video Pixel Network for video modeling (Kalchbrenner et al., 2016b) and ByteNet for machine translation (Kalchbrenner et al., 2016a). But in contrast to those works showing using a very deep architecture leads to better performance, CNN as decoder is used in our model to control the contextual capacity. We find a suitable CNN with VAE can have the best performance.\nOur work is closed related the recently proposed variational lossy autoencoder (Chen et al., 2016) which is used to predict image pixels. They find that conditioning on a smaller window of a pixels leads to better results with VAE, which is similar to our finding. Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows. We treat this as one of our future works. This work is largely orthogonal and could be potentially combined with a more effective choice of decoder to yield additional gains.\nThere are many previous works that explore unsupervised sentence encoding such as skip-thought vectors (Kiros et al., 2015), paragraph vector (Le & Mikolov, 2014) and sequence autoencoder (Dai & Le, 2015). (Dai & Le, 2015) applies the pre-trained model to semi-supervised classification and find significant gains, we use this as the baseline for our semi-supervised VAE."}, {"heading": "5. Conclusion", "text": "We propose to use dilated CNNs as decoders for VAEs for text modeling. We studied the contextual information and latent representation trade off by varying the decoder contextual capacity through changing CNN architectures. We find with a decoder with a small context window, the VAE is forced to use information from the latent representation. By selecting a suitable decoder, the VAE can perform better than simple LSTM language models. We find a similar trade off between classification accuracy and NLL for semi-supervsied VAEs. We show our semi-supervised VAEs perform better than strong baselines with proper decoders are selected. There are several future directions to explore based on our work. The first is to use more sophisticated prior/posterior probability representations such as inverse autoregressive flow to further improve the VAE results. Anther direction is to come up with better models for sentiment analysis with VAE since it has shown rather different code structure with topic."}], "references": [{"title": "Learning stochastic recurrent networks", "author": ["References Bayer", "Justin", "Osendorfer", "Christian"], "venue": "arXiv preprint arXiv:1411.7610,", "citeRegEx": "Bayer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "Jozefowicz", "Rafal", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Variational lossy autoencoder", "author": ["Chen", "Xi", "Kingma", "Diederik P", "Salimans", "Tim", "Duan", "Yan", "Dhariwal", "Prafulla", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A recurrent latent variable model for sequential data", "author": ["Chung", "Junyoung", "Kastner", "Kyle", "Dinh", "Laurent", "Goel", "Kratarth", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Sequential neural models with stochastic layers", "author": ["Fraccaro", "Marco", "S\u00f8nderby", "S\u00f8ren Kaae", "Paquet", "Ulrich", "Winther", "Ole"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fraccaro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fraccaro et al\\.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Towards conceptual compression", "author": ["Gregor", "Karol", "Besse", "Frederic", "Rezende", "Danilo Jimenez", "Danihelka", "Ivo", "Wierstra", "Daan"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben"], "venue": "arXiv preprint arXiv:1611.01144,", "citeRegEx": "Jang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "Neural machine translation in linear time", "author": ["Kalchbrenner", "Nal", "Espeholt", "Lasse", "Simonyan", "Karen", "Oord", "Aaron van den", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Video pixel networks", "author": ["Kalchbrenner", "Nal", "Oord", "Aaron van den", "Simonyan", "Karen", "Danihelka", "Ivo", "Vinyals", "Oriol", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1610.00527,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "arXiv preprint arXiv:1611.00712,", "citeRegEx": "Maddison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": "In Proc. ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "A hierarchical latent variable encoderdecoder model for generating dialogues", "author": ["Serban", "Iulian Vlad", "Sordoni", "Alessandro", "Lowe", "Ryan", "Charlin", "Laurent", "Pineau", "Joelle", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang", "Duyu", "Qin", "Bing", "Liu", "Ting"], "venue": "In EMNLP,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["van den Oord", "A\u00e4ron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Yan", "Xinchen", "Yang", "Jimei", "Sohn", "Kihyuk", "Lee", "Honglak"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang", "Zichao", "Diyi", "Dyer", "Chris", "He", "Xiaodong", "Smola", "Alex", "Hovy", "Eduard"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Yu", "Fisher", "Koltun", "Vladlen"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Variational neural machine translation", "author": ["Zhang", "Biao", "Xiong", "Deyi", "Su", "Jinsong", "Duan", "Hong", "Min"], "venue": "arXiv preprint arXiv:1605.07869,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Characterlevel convolutional networks for text classification", "author": ["Zhang", "Xiang", "Zhao", "Junbo", "LeCun", "Yann"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Recent work on generative modeling of text has found that variational autoencoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015).", "startOffset": 165, "endOffset": 186}, {"referenceID": 6, "context": "Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).", "startOffset": 197, "endOffset": 259}, {"referenceID": 26, "context": "Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).", "startOffset": 197, "endOffset": 259}, {"referenceID": 31, "context": "Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).", "startOffset": 197, "endOffset": 259}, {"referenceID": 1, "context": "However, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).", "startOffset": 75, "endOffset": 115}, {"referenceID": 21, "context": "However, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).", "startOffset": 75, "endOffset": 115}, {"referenceID": 21, "context": "Related work (Miao et al., 2016; Larochelle & Lauly, 2012; Mnih & Gregor, 2014) has used simpler decoders that model text as a bag of words.", "startOffset": 13, "endOffset": 79}, {"referenceID": 1, "context": "However, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016). The obvious choice for decoding architecture for a textual VAE is an LSTM, a typical workhorse in the language processing community. Bowman et al. (2015) demonstrated negative results using VAEs for text modeling, finding that they perform worse than LSTM language models.", "startOffset": 76, "endOffset": 271}, {"referenceID": 2, "context": "We demonstrate that when this trade off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders (Chen et al., 2016).", "startOffset": 239, "endOffset": 258}, {"referenceID": 22, "context": "Language models (Mikolov et al., 2010) typically generate each token xt conditioned on the entire history of previously generated tokens:", "startOffset": 16, "endOffset": 38}, {"referenceID": 1, "context": "Such models, though effective in modeling text, do not learn a vector that represents the full sequence (Bowman et al., 2015).", "startOffset": 104, "endOffset": 125}, {"referenceID": 1, "context": "For text, a RNN such as a LSTM is used as in (Bowman et al., 2015).", "startOffset": 45, "endOffset": 66}, {"referenceID": 17, "context": ", 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.", "startOffset": 44, "endOffset": 86}, {"referenceID": 8, "context": ", 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.", "startOffset": 44, "endOffset": 86}, {"referenceID": 8, "context": "Residual Connection: Residual connection (He et al., 2016) is used in the decoder to speed up convergence and enable us to train deep models.", "startOffset": 41, "endOffset": 58}, {"referenceID": 12, "context": "In this section, we briefly review semi-supervised VAEs of (Kingma et al., 2014) that can incorporate labels.", "startOffset": 59, "endOffset": 80}, {"referenceID": 12, "context": "(Kingma et al., 2014) proposed a semi-supervised VAE model whose latent representation contains both continuous variable z and discrete label y:", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Gumbel-Softmax: (Jang et al., 2016; Maddison et al., 2016) propose a continuous approximation to the samples of categorical distribution.", "startOffset": 16, "endOffset": 58}, {"referenceID": 20, "context": "Gumbel-Softmax: (Jang et al., 2016; Maddison et al., 2016) propose a continuous approximation to the samples of categorical distribution.", "startOffset": 16, "endOffset": 58}, {"referenceID": 28, "context": "We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).", "startOffset": 176, "endOffset": 234}, {"referenceID": 32, "context": "We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).", "startOffset": 176, "endOffset": 234}, {"referenceID": 35, "context": "We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).", "startOffset": 176, "endOffset": 234}, {"referenceID": 1, "context": "\u2217\u2217 is from (Bowman et al., 2015).", "startOffset": 11, "endOffset": 32}, {"referenceID": 1, "context": "For the LSTM decoder, we follow (Bowman et al., 2015) to use it as the initial state of LSTM and feed it to every step of LSTM.", "startOffset": 32, "endOffset": 53}, {"referenceID": 1, "context": "Following (Bowman et al., 2015), we also use drop word for the LSTM decoder, the drop word ratio is selected from [0, 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "Following (Bowman et al., 2015), we use KL cost annealing strategy.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "We can see that LSTM-VAE is worse than LSTMLM in terms of NLL and the KL term is nearly zero, which verifies the finding of (Bowman et al., 2015).", "startOffset": 124, "endOffset": 145}, {"referenceID": 12, "context": "Following that of (Kingma et al., 2014), we set the number of labeled samples to be 100, 500, 1000 and 2000 respectively.", "startOffset": 18, "endOffset": 39}, {"referenceID": 25, "context": "Variational inference through re-parameterization trick was initially proposed by (Kingma & Welling, 2013; Rezende et al., 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al.", "startOffset": 82, "endOffset": 128}, {"referenceID": 6, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 31, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 26, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 7, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 1, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 21, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 27, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 34, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 1, "context": "(Bowman et al., 2015) is the first work to combine VAE with language model and they use LSTM as the decoder and find some negative results.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "On the other hand, (Miao et al., 2016) models text as bag of words, though improvement has been found, the model can not be used to generate text.", "startOffset": 19, "endOffset": 38}, {"referenceID": 27, "context": "(Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported.", "startOffset": 0, "endOffset": 41}, {"referenceID": 34, "context": "(Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported.", "startOffset": 0, "endOffset": 41}, {"referenceID": 3, "context": "(Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.", "startOffset": 0, "endOffset": 69}, {"referenceID": 5, "context": "(Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.", "startOffset": 0, "endOffset": 69}, {"referenceID": 2, "context": "Our work is closed related the recently proposed variational lossy autoencoder (Chen et al., 2016) which is used to predict image pixels.", "startOffset": 79, "endOffset": 98}, {"referenceID": 15, "context": "Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows.", "startOffset": 5, "endOffset": 70}, {"referenceID": 2, "context": "Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows.", "startOffset": 5, "endOffset": 70}, {"referenceID": 16, "context": "There are many previous works that explore unsupervised sentence encoding such as skip-thought vectors (Kiros et al., 2015), paragraph vector (Le & Mikolov, 2014) and sequence autoencoder (Dai & Le, 2015).", "startOffset": 103, "endOffset": 123}], "year": 2017, "abstractText": "Recent work on generative modeling of text has found that variational autoencoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder\u2019s dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.", "creator": "LaTeX with hyperref package"}}}