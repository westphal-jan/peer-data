{"id": "1702.08165", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Reinforcement Learning with Deep Energy-Based Policies", "abstract": "we propose a method for learning expressive threshold energy - based policies for continuous states and actions, which has been feasible presented only earlier in tabular domains before. we apply our optimization method to learning maximum entropy policies, resulting into a new algorithm, potentially called soft q - learning, typically that comfortably expresses the optimal policy via a boltzmann distribution. therefore we use the recently proposed amortized stein variational gradient descent to possibly learn a stochastic sampling network that approximates samples from this expected distribution. the benefits of the loosely proposed algorithm has include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. we tentatively also draw a connection to actor - critic methods, which can be viewed performing our approximate inference on the corresponding energy - based model.", "histories": [["v1", "Mon, 27 Feb 2017 07:16:41 GMT  (4137kb,D)", "http://arxiv.org/abs/1702.08165v1", null], ["v2", "Fri, 21 Jul 2017 20:25:54 GMT  (4966kb,D)", "http://arxiv.org/abs/1702.08165v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["tuomas haarnoja", "haoran tang", "pieter abbeel", "sergey levine"], "accepted": true, "id": "1702.08165"}, "pdf": {"name": "1702.08165.pdf", "metadata": {"source": "META", "title": "Reinforcement Learning with Deep Energy-Based Policies", "authors": ["Tuomas Haarnoja", "Haoran Tang", "Pieter Abbeel", "Sergey Levine"], "emails": ["<haarnoja@berkeley.edu>,", "<hrtang@berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "Deep reinforcement learning (deep RL) has emerged as a promising direction for autonomous acquisition of complex behaviors (Mnih et al., 2015; Silver et al., 2016), due to its ability to process complex sensory input (Jaderberg et al., 2016) and to acquire elaborate behavior skills using general-purpose neural network representations (Levine et al., 2016). Deep reinforcement learning methods can be used to optimize deterministic (Lillicrap et al., 2015) and stochastic (Schulman et al., 2015a; Mnih et al., 2016) policies. However, most deep RL methods operate on the conventional deterministic notion of optimality, where the optimal solution, at least under full observability, is always a deterministic policy (Sutton & Barto, 1998). Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with\n*Equal contribution 1UC Berkeley, Department of Electrical Engineering and Computer Sciences 2OpenAI 3International Computer Science Institute. Correspondence to: Tuomas Haarnoja <haarnoja@berkeley.edu>, Haoran Tang <hrtang@berkeley.edu>.\nhigh entropy (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016).\nIn some cases, we might actually prefer to learn stochastic behaviors. In this paper, we explore two potential reasons for this: exploration in the presence of multimodal objectives, and compositionality attained via pretraining. Other benefits include robustness in the face of uncertain dynamics (Ziebart, 2010), imitation learning (Ziebart et al., 2008), and improved convergence and computational properties (Gu et al., 2016a). However, in order to learn such policies, we must define an objective that promotes stochasticity. In which cases is a stochastic policy actually the optimal solution?\nAs discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference (Todorov, 2008). While there are multiple instantiations of this framework, they typically include the cost or reward function as an additional factor in a factor graph, and infer the optimal conditional distribution over actions conditioned on states. The solution can be shown to optimize an entropy-augmented reinforcement learning objective or to correspond to the solution to a maximum entropy learning problem (Toussaint, 2009). Intuitively, framing control as inference produces policies that aim to capture not only the single deterministic behavior that has the lowest cost, but the entire range of low-cost behaviors, explicitly maximizing the entropy of the corresponding policy. Instead of learning the best way to perform the task, the resulting policies try to learn all of the ways of performing the task. It should now be apparent why such policies might be preferred: if we can learn all of the ways that a given task might be performed, the resulting policy can serve as a good initialization for finetuning to a more specific behavior (e.g. first learning all the ways that a robot could move forward, and then using this as an initialization to learn separate running and bounding skills); a better exploration mechanism for seeking out the best mode in a multimodal reward landscape; and a more robust behavior in the face of adversarial perturbations, where the ability to perform the same task in multiple different ways can provide the agent with more options to recover from perturbations.\nUnfortunately, solving such maximum entropy stochastic\nar X\niv :1\n70 2.\n08 16\n5v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20\n17\npolicy learning problems in the general case is challenging. A number of methods have been proposed, including Z-learning (Todorov, 2007), maximum entropy inverse RL (Ziebart et al., 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al., 2012), and G-learning (Fox et al., 2016), as well as more recent proposals in deep RL such as PGQ (O\u2019Donoghue et al., 2016), but these generally operate either on simple tabular representations, which are difficult to apply to continuous or high-dimensional domains, or employ a simple parametric representation of the policy distribution, such as a conditional Gaussian. Therefore, although the policy is optimized to perform the desired skill in many different ways and, in the case of deep RL methods (O\u2019Donoghue et al., 2016), might even use an expressive high-capacity model for representing the mean action, the resulting distribution is typically very limited in terms of its representational power, even if the parameters of that distribution are represented by an expressive function approximator, such as a neural network.\nHow can we extend the framework of maximum entropy policy search to arbitrary policy distributions? In this paper, we borrow an idea from energy-based models, which in turn reveals an intriguing connection between Q-learning, actor-critic algorithms, and probabilistic inference. In our method, we formulate a stochastic policy as a (conditional) energy-based model (EBM), with the energy function corresponding to the \u201csoft\u201d Q-function obtained when optimizing the maximum entropy objective. In highdimensional continuous spaces, sampling from this policy, just as with any general EBM, becomes intractable. We borrow from the recent literature on EBMs to devise an approximate sampling procedure based on training a separate sampling network, which is optimized to produce unbiased samples from the policy EBM. This sampling network can then be used both for updating the EBM and for action selection. In the parlance of reinforcement learning, the sampling network is the actor in an actor-critic algorithm. This reveals an intriguing connection: entropy regularized actorcritic algorithms can be viewed as approximate Q-learning methods, with the actor serving the role of an approximate sampler from an intractable posterior. We explore this connection further in the paper, and in the course of this discuss connections to popular deep RL methods such as deterministic policy gradient (DPG) (Silver et al., 2014; Lillicrap et al., 2015), normalized advantage functions (NAF) (Gu et al., 2016b), and PGQ (O\u2019Donoghue et al., 2016).\nThe principal contribution of this work is a tractable, efficient algorithm for optimizing arbitrary multimodal stochastic policies represented by energy-based models, as well as a discussion that relates this method to other recent algorithms in RL and probabilistic inference. In our experimental evaluation, we explore two potential applications of\nour approach. First, we demonstrate improved exploration performance in tasks with multi-modal reward landscapes, where conventional deterministic or unimodal methods are at high risk of falling into suboptimal local optima. Second, we explore how our method can be used to provide a degree of compositionality in reinforcement learning by showing that stochastic energy-based policies can serve as a much better initialization for learning new skills than either random policies or policies pretrained with conventional maximum reward objectives."}, {"heading": "2. Preliminaries", "text": "In this section, we will define the reinforcement learning problem that we are addressing and briefly summarize the maximum entropy policy search objective. We will also present a few useful identities that we will build on in our algorithm, which will be presented in Section 3."}, {"heading": "2.1. Maximum Entropy Reinforcement Learning", "text": "We will address learning of maximum entropy policies with approximate inference for reinforcement learning in continuous action spaces. Our reinforcement learning problem can be defined as policy search in an infinite-horizon Markov decision process (MDP), which consists of the tuple (S,A, ps, r), The state space S and action space A are assumed to be continuous, and the state transition probability ps : S \u00d7 S \u00d7 A \u2192 [0, 1] represents the probability density of the next state st+1 \u2208 S given the current state st \u2208 S and action at \u2208 A. The environment emits a reward r : S \u00d7 A \u2192 [rmin, rmax] on each transition, which we will abbreviate as rt , r(st,at) to simplify notation. We will also use \u03c1\u03c0(st) and \u03c1\u03c0(st,at) to denote the state and state-action marginals of the trajectory distribution induced by a policy \u03c0(at|st).\nOur goal is to learn a policy \u03c0(at|st). We can define the standard reinforcement learning objective in terms of the above quantities as\n\u03c0\u2217std = arg max \u03c0 \u2211 t E(st,at)\u223c\u03c1\u03c0 [r(st,at)] . (1)\nMaximum entropy RL augments the reward with an entropy term, such that the optimal policy aims to maximize its entropy at each visited state: \u03c0\u2217MaxEnt =arg max\u03c0 \u2211 t E(st,at)\u223c\u03c1\u03c0 [r(st,at)+\u03b1H(\u03c0(\u00b7|st))] , (2) where \u03b1 is an optional but convenient parameter that can be used to determine the relative importance of entropy and reward.1 Optimization problems of this type have been explored in a number of prior works (Kappen, 2005; Todorov,\n1In principle, 1/\u03b1 can be folded into the reward function, eliminating the need for an explicit multiplier, but in practice, it is often convenient to keep \u03b1 as a hyperparameter.\n2007; Ziebart et al., 2008), which are covered in more detail in Section 4. Note that this objective differs qualitatively from the behavior of Boltzmann exploration (Sallans & Hinton, 2004) and PGQ (O\u2019Donoghue et al., 2016), which greedily maximize entropy at the current time step, but do not explicitly optimize for policies that aim to reach states where they will have high entropy in the future. This distinction is crucial, since the maximum entropy objective can be shown to maximize the entropy of the entire trajectory distribution for the policy \u03c0, while the greedy Boltzmann exploration approach does not (Ziebart et al., 2008; Levine & Abbeel, 2014). As we will discuss in Section 5, this maximum entropy formulation has a number of benefits, such as improved exploration in multimodal problems and better pretraining for later adaptation.\nIf we wish to extend either the conventional or the maximum entropy RL objective to infinite horizon problems, it is convenient to also introduce a discount factor \u03b3 to ensure that the sum of expected rewards (and entropies) is finite. In the context of policy search algorithms, the use of a discount factor is actually a somewhat nuanced choice, and writing down the precise objective that is optimized when using the discount factor is non-trivial (Thomas, 2014). We defer the full derivation of the discounted objective to Appendix A, since it is unwieldy to write out explicitly, but we will use the discount \u03b3 in the following derivations and in our final algorithm."}, {"heading": "2.2. Soft Value Functions and Energy-Based Models", "text": "Optimizing the maximum entropy objective in Equation 2 provides us with a framework for training stochastic policies, but we must still choose a representation for these policies. The choices in prior work include discrete multinomial distributions (O\u2019Donoghue et al., 2016) and Gaussian distributions (Rawlik et al., 2012). However, if we want to use a very general class of distributions that can represent complex, multimodal behaviors, we can instead opt for using general energy-based policies of the form\n\u03c0(at|st) \u221d exp (\u2212E(st,at)) , (3)\nwhere E(st,at) is an energy function that could be represented, for example, by a deep network. If we use a universal function approximator for E , we can represent any distribution \u03c0(at|st). There is a close connection between such energy-based models and soft versions of value functions and Q-functions, where we set E(st,at) = \u2212 1\u03b1Qsoft(st,at) and use the following theorem: Theorem 1. Let the soft Q-function be defined by\nQ\u2217soft(st,at) = rt+ (4)\nE(st+1,... )\u223c\u03c1\u03c0 [ \u221e\u2211 l=1 \u03b3l (rt+l+\u03b1H (\u03c0\u2217MaxEnt(\u00b7|st+l))) ] ,\nand soft value function by\nV \u2217soft(st) = \u03b1 log \u222b A exp ( 1 \u03b1 Q\u2217soft(st,a \u2032) ) da\u2032. (5)\nThen the optimal policy for Equation 2 is given by\n\u03c0\u2217MaxEnt(at|st)=exp ( 1 \u03b1 (Q \u2217 soft(st,at)\u2212V \u2217soft(st)) ) . (6)\nProof. See Appendix A.1 as well as (Ziebart, 2010).\nTheorem 1 connects the maximum entropy objective in Equation 2 and energy-based models, where 1\u03b1Qsoft(st,at) acts as the negative energy, and 1\u03b1Vsoft(st) serves as the log-partition function. As with the standard Q-function and value function, we can relate the Q-function to the value function at a future state via a soft Bellman equation:\nTheorem 2. The soft Q-function in Equation 4 satisfies the soft Bellman equation\nQ\u2217soft(st,at) = rt + \u03b3 Est+1\u223cps [V \u2217soft(st+1)] , (7)\nwhere the soft value function V \u2217soft is given by Equation 5.\nProof. See Appendix A.2, as well as (Ziebart, 2010).\nThe soft Bellman equation is a generalization of the conventional (hard) equation, where we can recover the more standard equation as \u03b1 \u2192 0, which causes Equation 5 to approach a hard maximum over the actions. In the next section, we will discuss how we can use these identities to derive a Q-learning style algorithm for learning maximum entropy policies, and how we can make this practical for arbitrary Q-function representations via an approximate inference procedure."}, {"heading": "3. Training Expressive Energy-Based Models via Soft Q-Learning", "text": "In this section, we will present our proposed reinforcement learning algorithm, which is based on the soft Q-function described in the previous section, but can be implemented via a tractable stochastic gradient descent procedure with approximate sampling. We will first describe the general case of soft Q-learning, and then present the inference procedure that makes it tractable to use with deep neural network representations in high-dimensional continuous state and action spaces. In the process, we will relate this Qlearning procedure to inference in energy-based models and actor-critic algorithms."}, {"heading": "3.1. Soft Q-Iteration", "text": "We can obtain a solution to Equation 7 by iteratively updating estimates of V \u2217soft and Q \u2217 soft. This leads to a fixed-point iteration that resembles Q-iteration:\nTheorem 3. Soft Q-iteration. LetQsoft(\u00b7, \u00b7) and Vsoft(\u00b7) be bounded and assume that \u222b A exp ( 1 \u03b1Qsoft(\u00b7,a \u2032) ) da\u2032 <\u221e\nand that Q\u2217soft <\u221e exists. Then the fixed-point iteration\nQsoft(st,at)\u2190rt+\u03b3 Est+1\u223cps [Vsoft(st+1)] , \u2200st,at (8) Vsoft(st)\u2190\u03b1 log \u222b A exp ( 1 \u03b1 Qsoft(st,a \u2032) ) da\u2032, \u2200st (9)\nconverges to Q\u2217soft and V \u2217 soft, respectively.\nProof. See Appendix A.2.\nWe refer to the updates in Equation 8 and (9) as the soft Bellman backup operator that acts on the soft value function, and denote it by T . The maximum entropy policy in Equation 6 can then be recovered by iteratively applying this operator until convergence. However, there are several practicalities that need to be considered in order to make use of the algorithm. First, the soft Bellman backup cannot be performed exactly in continuous or large state and action spaces, and second, sampling from the energy-based model in Equation 6 is intractable in general. We will address these challenges in the following sections."}, {"heading": "3.2. Soft Q-Learning", "text": "This section discusses how the Bellman backup in Theorem 3 can be implemented in a practical algorithm that uses a finite set of samples from the environment, resulting in a method similar to Q-learning. Since the soft Bellman backup is a contraction, the optimal value function is the fixed point of the Bellman backup, and we can find it by optimizing for a Q-function for which the soft Bellman error |T Q\u2212Q| is minimized at all states and actions. While this procedure is still intractable due to the integral in Equation 9 and the infinite set of all states and actions, we can express it as a stochastic optimization, which leads to a stochastic gradient descent update procedure. We will model the soft Q-function with a function approximator with parameters \u03b8 and denote it as Q\u03b8soft(st,at).\nTo convert Theorem 3 into a stochastic optimization problem, we first express the soft value function in terms of an expectation via importance sampling:\nV \u03b8soft(st) = \u03b1 logEqa\u2032\n[ exp ( 1 \u03b1Q \u03b8 soft(st,a \u2032) )\nqa\u2032(a\u2032)\n] , (10)\nwhere qa\u2032 can be an arbitrary distribution over the action space. Second, by noting the identity xi = yi,\u2200i \u2208 X \u21d4 Ei\u223cqi [ (xi \u2212 yi)2 ] = 0, where qi can be any distribution with support in X, we can express the soft Q-iteration in an equivalent form as minimizing\nJQ(\u03b8)=Est\u223cqst ,at\u223cqat [ 1 2 ( Q\u0302\u03b8\u0304soft(st,at)\u2212Q\u03b8soft(st,at) )2] , (11)\nwhere Q\u0302\u03b8\u0304soft(st,at) = rt+\u03b3Est+1\u223cps [V \u03b8\u0304soft(st+1)] is a target Q-value, with V \u03b8\u0304soft(st+1) given by Equation 10.\nThis stochastic optimization problem can be solved approximately using stochastic gradient descent using sampled\nstates and actions. While the sampling distributions qst and qat can be arbitrary, we typically use real samples from rollouts of the current policy \u03c0(at|st) \u221d 1\u03b1Q \u03b8 soft(st,at). For qa\u2032 we have more options. A convenient choice is a uniform distribution. However, this choice can scale poorly to high dimensions. A better choice is to use the current policy, which produces an unbiased estimate of the soft value as can be confirmed by substitution. This overall procedure yields an iterative approach that optimizes over the Q-values, which we summarize in Section 3.4.\nHowever, in continuous spaces, we still need a tractable way to sample from the policy \u03c0(at|st) \u221d 1\u03b1Q \u03b8 soft(st,at), both to take on-policy actions and, if so desired, to generate action samples for estimating the soft value function. Since the form of the policy is so general, sampling from it is intractable. We will therefore use an approximate sampling procedure, as discussed in the following section."}, {"heading": "3.3. Approximate Sampling and Stein Variational Gradient Descent (SVGD)", "text": "In this section we describe how we can approximately sample from the soft Q-function. Existing approaches that sample from energy-based distributions generally fall into two categories: methods that use Markov chain Monte Carlo (MCMC) based sampling (Sallans & Hinton, 2004), and methods that learn a stochastic sampling network trained to output approximate samples from the target distribution (Zhao et al., 2016; Kim & Bengio, 2016). Since sampling via MCMC is not tractable when the inference must be performed online (e.g. when executing a policy), we will use a sampling network based on Stein variational gradient descent (SVGD) (Liu & Wang, 2016) and amortized SVGD (Wang & Liu, 2016). Amortized SVGD has several intriguing properties: First, it provides us with a stochastic sampling network that we can query for extremely fast sample generation. Second, it can be shown to converge to an accurate estimate of the posterior distribution of an EBM. Third, the resulting algorithm, as we will show later, strongly resembles actor-critic algorithm, which provides for a simple and computationally efficient implementation and sheds light on the connection between our algorithm and prior actor-critic methods.\nFormally, we want to learn a state-conditioned stochastic neural network at = f\u03c6(\u03be; st), parametrized by \u03c6, that maps noise samples \u03be drawn from a normal Gaussian, or other arbitrary distribution, into unbiased action samples from the target EBM corresponding to Q\u03b8soft. We denote the induced distribution of the actions as \u03c0\u03c6(at|st), and we want to find parameters \u03c6 so that the induced distribution approximates the energy-based distribution in terms of the KL divergence:\nJ\u03c0(\u03c6; st) = DKL ( \u03c0\u03c6(\u00b7|st) \u2225\u2225 exp (Q\u03b8soft(st, \u00b7))) , (12)\nwhere we have slightly abused the notation by allowing the second argument of KL-divergence to be unnormalized for simplicity. While minimizing this loss is still intractable, the Stein variational gradient (Liu & Wang, 2016) gives the direction \u2206f\u03c6(\u00b7; st) that greedily minimizes the induced KL-divergence up to an arbitrary precision in closed form. The greedy direction is given by\n\u2206f\u03c6(\u00b7; st) = (13) Eat\u223c\u03c0\u03c6 [ \u03ba(at, \u00b7)\u2207a\u2032Q\u03b8soft(st,a\u2032) \u2223\u2223 a\u2032=at +\u2207a\u2032\u03ba(a\u2032, \u00b7) \u2223\u2223 a\u2032=at ] where \u03ba is a kernel function with support in A that we can choose. To be precise, \u2206f\u03c6 is the optimal direction in the reproducing kernel Hilbert space of \u03ba, and is thus not strictly speaking the gradient, but it turns out that we can set \u2202J\u03c0\u2202at \u221d \u2206f\n\u03c6 as explained in (Wang & Liu, 2016). With this assumption, we can use the chain rule and backpropagate the Stein variational gradient into the policy network according to\n\u2202J\u03c0(\u03c6; st)\n\u2202\u03c6 \u221d \u2206f\u03c6(at; st)\n\u2202f\u03c6(at; st)\n\u2202\u03c6 , (14)\nand use any gradient-based optimization method to learn the optimal sampling network parameters. The sampling network f\u03b8 can be viewed as an actor in an actor-critic algorithm. We will discuss this connection in Section 4, but first we will summarize our complete maximum entropy policy learning algorithm."}, {"heading": "3.4. Algorithm Summary", "text": "To summarize, we propose the soft Q-learning algorithm for learning maximum entropy policies in continuous domains. The algorithm proceeds by alternating between collecting new experience from the environment, and updating the soft Q-function and sampling network parameters. The experience is stored in a replay memory buffer D as standard in deep Q-learning (Mnih et al., 2013), and the parameters are updated using random minibatches from this memory. The soft Q-function updates use a delayed version of the target values, characterized by a smoothing factor \u03c4 , to improve stability (Mnih et al., 2013). For optimization, we use the ADAM (Kingma & Ba, 2015) optimizer and empirical estimates of the gradients, which we denote by \u2207\u0302. The exact formulae used to compute the gradient estimates is deferred to Appendix C, which also discusses other implementation details, but we summarize an overview of soft Q-learning in Algorithm 1."}, {"heading": "4. Related Work", "text": "Maximum entropy policies emerge as the solution when we cast optimal control as probabilistic inference. In the case of linear-quadratic systems, the mean of the maximum entropy policy is exactly the optimal deterministic\nAlgorithm 1 Soft Q-learning \u03b8, \u03c6 \u223c some initialization distributions Assign target parameters \u03b8\u0304 \u2190 \u03b8, \u03c6\u0304\u2190 \u03c6 D \u2190 empty replay memory\nfor each epoch do for each t do\nCollect experience Sample an action for st using f\u03c6: at \u2190 f\u03c6(\u03be; st) where \u03be \u223c N (0, I).\nSample next state from the environment: st+1 \u223c ps(st+1|st,at). Save the new experience in the replay memory: D \u2190 D \u222a {(st,at, r(st,at), st+1)} .\nSample a minibatch from the replay memory {(s(i)t ,a (i) t , r (i) t , s (i) t+1)}Ni=0 \u223c D. Update the soft Q-function parameters Sample {a(i,j)}Mj=0 \u223c qa\u2032 for each s (i) t+1.\nCompute empirical soft values V\u0302 \u03b8soft(s (i) t+1) in (10). Compute empirical gradient \u2207\u0302\u03b8JQ of (11). Update \u03b8 according to \u2207\u0302\u03b8JQ using ADAM.\nUpdate policy Sample {\u03be(i,j)}Mj=0 \u223c N (0, I) for each s (i) t .\nCompute actions a(i,j)t = f \u03c6(\u03be(i,j), s (i) t ). Compute \u2206f\u03c6 using empirical estimate of (13). Compute empiricial estimate of (14): \u2207\u0302\u03c6J\u03c0 . Update \u03c6 according to \u2207\u0302\u03c6J\u03c0 and using ADAM.\nend for Update target parameters \u03b8\u0304 \u2190 (1\u2212 \u03c4)\u03b8\u0304 + \u03c4\u03b8 \u03c6\u0304\u2190 (1\u2212 \u03c4)\u03c6\u0304+ \u03c4\u03c6\nend for\npolicy (Todorov, 2008), which has been exploited to construct practical path planning methods based on iterative linearization and probabilistic inference techniques (Toussaint, 2009). In discrete state spaces, the maximum entropy policy can be obtained exactly. This has been explored in the context of linearly solvable MDPs (Todorov, 2007) and, in the case of inverse reinforcement learning, MaxEnt IRL (Ziebart et al., 2008). In continuous systems and continuous time, path integral control studies maximum entropy policies and maximum entropy planning (Kappen, 2005). In contrast to these prior methods, our work is focused on extending the maximum entropy policy search framework to high-dimensional continuous spaces and highly multimodal objectives, via expressive general-purpose energy functions represented by deep neural networks. A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016). Among these, the work of Rawlik et al. (2012) re-\nsembles ours in that it also makes use of a temporal difference style update to a soft Q-function. However, unlike this prior work, we focus on general-purpose energy functions with approximate sampling, rather than analytically normalizable distributions. The form of our sampler resembles the stochastic networks proposed in recent work on hierarchical learning (Florensa et al., 2017). However this prior work uses a task-specific reward bonus system to encourage stochastic behavior, while our approach is derived from optimizing a general maximum entropy objective.\nA closely related concept to maximum entropy policies is Boltzmann exploration, which uses the exponential of the standard Q-function as the probability of an action (Kaelbling et al., 1996). A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012). Although these methods are closely related, they have not, to our knowledge, been extended to the case of deep network models, have not made extensive use of approximate inference techniques, and have not been demonstrated on the complex continuous tasks. More recently, O\u2019Donoghue et al. (2016) drew a connection between Boltzmann exploration and entropy-regularized policy gradient, though in a theoretical framework that differs from maximum entropy policy search: unlike the full maximum entropy framework, the approach of O\u2019Donoghue et al. (2016) only optimizes for maximizing entropy at the current time step, rather than planning for visiting future states where entropy will be further maximized. This prior method also does not demonstrate learning complex multimodal policies in continuous action spaces.\nAlthough we motivate our method as Q-learning, its structure resembles an actor-critic algorithm. It is particularly instructive to observe the connection between our approach and the deep deterministic policy gradient method (DDPG) (Lillicrap et al., 2015), which updates a Qfunction critic according to (hard) Bellman updates, and then backpropagates the Q-value gradient into the actor, similarly to NFQCA (Hafner & Riedmiller, 2011). Our actor update differs only in the addition of the \u03ba term. Indeed, without this term, our actor would estimate a maximum a posteriori (MAP) action, rather than capturing the entire EBM distribution. This suggests an intriguing connection between our method and DDPG: if we simply modify the DDPG critic updates to estimate soft Q-values, we recover the MAP variant of our method. Furthermore, this connection allows us to cast DDPG as simply an approximate Q-learning method, where the actor serves the role of an approximate maximizer. This helps explain the good performance of DDPG on off-policy data. We can also make a connection between our method and policy gradients. In\nAppendix B, we show that the policy gradient for a policy represented as an energy-based model closely corresponds to the update in soft Q-learning."}, {"heading": "5. Experiments", "text": "Our experiments aim to answer the following questions: (1) Does our soft Q-learning method accurately capture a multi-modal policy distribution? (2) Can soft Q-learning with energy-based policies aid exploration for complex tasks that require tracking multiple modes? (3) Can a maximum entropy policy serve as a good initialization for finetuning on different tasks, when compared to pretraining with a standard deterministic objective? We compare our algorithm to DDPG (Lillicrap et al., 2015), which has been shown to achieve better sample efficiency on the continuous control problems that we consider than other recent techniques such as REINFORCE (Williams, 1992), TRPO (Schulman et al., 2015a), and A3C (Mnih et al., 2016). This comparison is particularly interesting since, as discussed in Section 4, DDPG closely corresponds to a deterministic maximum a posteriori variant of our method. The detailed experimental setup can be found in Appendix D. Videos of all experiments are available online2."}, {"heading": "5.1. Didactic Example: Multi-Goal Environment", "text": "In order to verify that amortized SVGD can correctly draw samples from energy-based policies of the form exp ( Q\u03b8soft(s, a) ) , and that our complete algorithm can successful learn to represent multi-modal behavior, we designed a simple \u201cmulti-goal\u201d environment, in which the agent is a 2D point mass trying to reach one of four symmetrically placed goals. The reward is defined as a mixture of Gaussians, with means placed at the goal positions. An optimal strategy is to go to an arbitrary goal. However, the optimal maximum entropy policy should be able to choose each of the four goals at random. The final policy obtained with our method is illustrated in Figure 1. The Q-values indeed have complicated shapes, being unimodal at s = (\u22122, 0), convex at s = (0, 0), and bimodal at s = (2.5, 2.5). The stochastic policy samples actions closely following the energy landscape, hence learning diverse trajectories that lead to all four goals. In comparison, a policy trained with DDPG only converges to a randomly chosen goal."}, {"heading": "5.2. Learning Multi-Modal Policies for Exploration", "text": "Though not all environments have a clear multi-modal reward landscape as in the \u201cmulti-goal\u201d example, multimodality is prevalent in a variety of tasks. For example, a chess player might try various strategies before settling on one that seems most effective, and an agent navigating a maze may need to try various paths before finding the exit.\n2https://sites.google.com/view/softqlearning/home\nDuring the learning process, it is often best to keep trying multiple available options until the agent is confident that one of them is the best (similar to a bandit problem (Lai & Robbins, 1985)). However, deep RL algorithms for continuous control typically use uni-modal action distributions, which are not well suited to capture such multi-modality. As a consequence, such algorithms may prematurely commit to one mode and converge to suboptimal behavior.\nTo evaluate how maximum entropy policies might aid exploration, we constructed simulated continuous control environments where tracking multiple modes is important for success. The first experiment uses a simulated swimming snake (see Figure 2), which receives a reward equal to its speed along the x-axis, either forward or backward. However, once the swimmer swims far enough forward, it crosses a \u201cfinish line\u201d and receives a larger reward. Therefore, the best learning strategy is to explore in both directions until the bonus reward is discovered, and then commit to swimming forward. As illustrated in Figure 6 in the supplement, our method is able to recover this strategy, keeping track of both modes until the finish line is discovered. All stochastic policies eventually commit to swimming forward. The deterministic DDPG method shown in the comparison commits to a mode prematurely, with only 80% of the policies converging on a forward motion, and 20% choosing the suboptimal backward mode.\nThe second experiment studies a more complex task with a continuous range of equally good options prior to discovery of a sparse reward goal. In this task, a quadrupedal\n3D robot (adapted from Schulman et al. (2015b)) needs to find a path through a maze to a target position (see Figure 2). The reward function is a Gaussian centered at the target. The agent may choose either the upper or lower passage, which appear identical at first, but the upper passage is blocked by a barrier. Similar to the swimmer experiment, the optimal strategy requires exploring both directions and choosing the better one. Figure 3(b) compares the performance of DDPG and our method. The curves show the minimum distance to the target achieved so far and the threshold equals the minimum possible distance if the robot chooses the upper passage. Therefore, successful exploration means reaching below the threshold. All policies trained with our method manage to succeed, while only 60% policies trained with DDPG converge to choosing the lower passage."}, {"heading": "5.3. Accelerating Training on Complex Tasks with Pretrained Maximum Entropy Policies", "text": "A standard way to accelerate deep neural network training is task-specific initialization (Goodfellow et al., 2016), where a network trained for one task is used as initialization for another task. The first task might be something highly general, such as classifying a large image dataset, while the second task might be more specific, such as fine-\ngrained classification with a small dataset. Pretraining has also been explored in the context of RL (Shelhamer et al., 2016). However, in RL, near-optimal policies are often near-deterministic, which makes them poor initializers for new tasks. In this section, we explore how our energybased policies can be trained with fairly broad objectives to produce an initializer for more quickly learning more specific tasks.\n10 5 0 5 10 10\n5\n0\n5 10We demonstrate this on a variant of the quadrupedal robot task. The pretraining phase involves learning to locomote in an arbitrary way, with a reward that simply equals the speed of the center of mass. The resulting policy moves quickly, but in random directions. An overhead plot of the center of mass traces is shown above to illustrate this. This pretraining is similar in some ways to recent work on modulated controllers (Heess et al., 2016) and hierarchical models (Florensa et al., 2017). However, in contrast to these prior works, we do not require any task-specific highlevel goal generator or reward, the reward is simply to run, and the quadruped learns a variety of running strategies.\nFigure 4 also shows a variety of test environments that we used to finetune the running policy for a specific task. In the hallway environments, the agent receives the same reward, but the walls block sideways motion, so the optimal solution requires learning to run in a particular direction. Narrow hallways require choosing a more specific direction, but also allow the agent to use the walls to funnel itself. The U-shaped maze requires the agent to learn a curved trajectory in order to arrive at the target, with the reward given by a Gaussian bump at the target location.\nAs illustrated in Figure 7 in the supplement, the pretrained policy extensively explores the space extensively and in all directions. This gives a good initialization for the policy, allowing it to learn the behaviors in the test environments more quickly than training a policy with DDPG from a random initialization, as shown in Figure 5.3 We also evalu-\n3We use DDPG for the random initialization, because we found that it tends to learn faster when trained from scratch, making for a stronger baseline. Since DDPG is deterministic, we can-\nated an alternative pretraining method based on deterministic policies learned with DDPG. However, deterministic pretraining chooses an arbitrary but consistent direction in the training environment, providing a poor initialization for finetuning to a specific task, as shown in the results plots."}, {"heading": "6. Discussion and Future Work", "text": "We presented a method for learning stochastic energybased policies with approximate inference via Stein variational gradient descent (SVGD). Our approach can be viewed as a type of soft Q-learning method, with the additional contribution of using approximate inference to obtain complex multimodal policies. The sampling network trained as part of SVGD can also be viewed as tking the role of an actor in an actor-critic algorithm. Our experimental results show that our method can effectively capture complex multi-modal behavior on problems ranging from toy point mass tasks to complex torque control of simulated walking and swimming robots. The applications of training such stochastic policies include improved exploration in the case of multimodal objectives and compositionality via pretraining general-purpose stochastic policies that can then be efficiently finetuned into task-specific behaviors.\nWhile our work explores some potential applications of energy-based policies with approximate inference, an exciting avenue for future work would be to further study their capability to represent complex behavioral repertoires and their potential for composability. In the context of linearly solvable MDPs, several prior works have shown that policies trained for different tasks can be composed to create new optimal policies (Da Silva et al., 2009; Todorov, 2009). While these prior works have only explored simple, tractable representations, our method could be used to extend these results to complex and highly multi-modal deep neural network models, making them suitable for composable control of complex high-dimensional systems, such as humanoid robots. This composability could be used in future work to create a huge variety of near-optimal skills from a collection of energy-based policy building blocks.\nnot initialize it with a maximum entropy policy."}, {"heading": "A. Policy Improvement Proofs", "text": "In this appendix, we present proofs for the theorems that allow us to show that soft Q-learning leads to policy improvement with respect to the maximum entropy objective. First, we define a slightly more nuanced version of the maximum entropy objective that allows us to incorporate a discount factor. This definition is complicated by the fact that, when using a discount factor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense, discounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward, with the discount serving to reduce variance, as discussed by Thomas (2014). However, for the purposes of the derivation, we can define the objective that is optimized under a discount factor as\n\u03c0\u2217MaxEnt = arg max \u03c0 \u2211 t E(st,at)\u223c\u03c1\u03c0 [ \u221e\u2211 l=t \u03b3l\u2212tE(sl,al) [r(st,at) + \u03b1H(\u03c0(\u00b7|st))|st,at] ] .\nThis objective corresponds to maximizing the discounted expected reward and entropy for future states originating from every state-action tuple (st,at) weighted by its probability \u03c1\u03c0 under the current policy. Note that this objective still takes into account the entropy of the policy at future states, in contrast to greedy objectives such as Boltzmann exploration or the approach proposed by O\u2019Donoghue et al. (2016).\nWe can now derive policy improvement results for soft Q-learning. We start with the definition of the soft Q-value Q\u03c0soft for any policy \u03c0 as the expectation under \u03c0 of the discounted sum of rewards and entropy :\nQ\u03c0soft(s,a) , r0 + E\u03c4\u223c\u03c0,s0=s,a0=a [ \u221e\u2211 t=1 \u03b3t(rt +H(\u03c0(\u00b7|st))) ] . (15)\nHere, \u03c4 = (s0,a0, s1,a1, . . .) denotes the trajectory originating at (s,a). Notice that for convenience, we set the entropy parameter \u03b1 to 1. The theory can be easily adapted by dividing rewards by \u03b1.\nThe discounted maximum entropy policy objective can now be defined as\nJ(\u03c0) , \u2211 t E(st,at)\u223c\u03c1\u03c0 [Q \u03c0 soft(st,at) + \u03b1H(\u03c0(\u00b7|st))] . (16)\nA.1. The Maximum Entropy Policy\nIf the objective function is the expected discounted sum of rewards, the policy improvement theorem (Sutton & Barto, 1998) describes how policies can be improved monotonically. There is a similar theorem we can derive for the maximum entropy objective:\nTheorem 4. (Policy improvement theorem) Given a policy \u03c0, define a new policy \u03c0\u0303 as\n\u03c0\u0303(\u00b7|s) \u221d exp ( Q\u03c0soft(s, \u00b7) ) , \u2200s. (17)\nAssume that throughout our computation, Q is bounded and \u222b\nexp(Q(s,a)) da is bounded for any s (for both \u03c0 and \u03c0\u0303). Then Q\u03c0\u0303soft(s,a) \u2265 Q\u03c0soft(s,a) \u2200s,a.\nThe proof relies on the following observation: if one greedily maximize the sum of entropy and value with one-step look-ahead, then one obtains \u03c0\u0303 from \u03c0:\nH(\u03c0(\u00b7|s)) + Es\u223c\u03c1\u03c0 [Q\u03c0soft(s,a)] \u2264 H(\u03c0\u0303(\u00b7|s)) + Ea\u223c\u03c0\u0303 [Q\u03c0soft(s,a)] . (18)\nThe proof is straight-forward by noticing that H(\u03c0(\u00b7|s)) + Ea\u223c\u03c0 [Q\u03c0soft(s,a)] = \u2212DKL (\u03c0(\u00b7|s) \u2016 \u03c0\u0303(\u00b7|s)) + log \u222b exp (Q\u03c0soft(s,a)) da (19)\nWe write E\u03c4\u223c\u03c0\u0303,s0=s,a0=a with a shorthand E. Then we can show that\nQ\u03c0soft(s,a) = E [r0 + \u03b3(H(\u03c0(\u00b7|s1)) + Ea1\u223c\u03c0 [Q\u03c0soft(s1,a1)])] \u2264 E [r0 + \u03b3(H(\u03c0\u0303(\u00b7|s1)) + Ea1\u223c\u03c0\u0303 [Q\u03c0soft(s1,a1)])] = E [r0 + \u03b3(H(\u03c0\u0303(\u00b7|s1)) + r1)] + \u03b32 E [H(\u03c0(\u00b7|s2)) + Ea2\u223c\u03c0 [Q\u03c0soft(s2,a2)]] \u2264 E [r0 + \u03b3(H(\u03c0\u0303(\u00b7|s1)) + r1] + \u03b32E [H(\u03c0\u0303(\u00b7|s2)) + Ea2\u223c\u03c0\u0303 [Q\u03c0soft(s2,a2)]] = E [ r0 + \u03b3(H(\u03c0\u0303(\u00b7|s1)) + r1) + \u03b32(H(\u03c0\u0303(\u00b7|s2)) + r2) ] + \u03b33 E [H(\u03c0\u0303(\u00b7|s3)) + Ea3\u223c\u03c0\u0303 [Q\u03c0soft(s3,a3)]]\n...\n\u2264 E [ r0 +\n\u221e\u2211 t=1 \u03b3t(H(\u03c0\u0303(\u00b7|st)) + rt) ] = Q\u03c0\u0303soft(s,a). (20)\nWith Theorem 4, we start from an arbitrary policy \u03c00 and define the policy iteration as\n\u03c0i+1(\u00b7|s) \u221d exp (Q\u03c0isoft(s, \u00b7)) . (21)\nThen Q\u03c0isoft(s,a) improves monotonically. Under certain regularity conditions, \u03c0i converges to \u03c0\u221e. Obviously, we have \u03c0\u221e(a|s) \u221da exp (Q\u03c0\u221e(s,a)). Since any non-optimal policy can be improved this way, the optimal policy must satisfy this energy-based form. Therefore we have proven Theorem 1.\nA.2. Soft Bellman Equation and Soft Value Iteration\nRecall the definition of the soft value function: V \u03c0soft(s) , log \u222b exp (Q\u03c0soft(s,a)) da. (22)\nSuppose \u03c0(a|s) = exp (Q\u03c0soft(s,a)\u2212 V \u03c0soft(s)). Then we can show that Q\u03c0soft(s,a) = r(s,a) + \u03b3 Es\u2032\u223cps [ H(\u03c0(\u00b7|s\u2032)) + Ea\u2032\u223c\u03c0(\u00b7|s\u2032) [Q\u03c0soft(s\u2032,a\u2032)] ] = r(s,a) + \u03b3 Es\u2032\u223cps [V \u03c0soft(s\u2032)] . (23)\nThis completes the proof of Theorem 2.\nFinally, we show that the soft value iteration operator T , defined as T Q(s,a) , r(s,a) + \u03b3 Es\u2032\u223cps [ log \u222b expQ(s\u2032,a\u2032) da\u2032 ] , (24)\nis a contraction. Then Theorem 3 follows immediately.\nThe following proof has also been presented by Fox et al. (2016). Define a norm on Q values as \u2016Q1 \u2212 Q2\u2016 , maxs,a |Q1(s,a)\u2212Q2(s,a)|. Suppose \u03b5 = \u2016Q1 \u2212Q2\u2016. Then\nlog \u222b exp(Q1(s \u2032,a\u2032)) da\u2032 \u2264 log \u222b exp(Q2(s \u2032,a\u2032) + \u03b5) da\u2032\n= log ( exp(\u03b5) \u222b expQ2(s \u2032,a\u2032) da\u2032 )\n= \u03b5+ log \u222b expQ2(a \u2032,a\u2032) da\u2032. (25)\nSimilarly, log \u222b expQ1(s \u2032,a\u2032) da\u2032 \u2265 \u2212\u03b5+ log \u222b expQ2(s\n\u2032,a\u2032) da\u2032. Therefore \u2016T Q1 \u2212 T Q2\u2016 \u2264 \u03b3\u03b5 = \u03b3\u2016Q1 \u2212Q2\u2016. So T is a contraction. As a consequence, only one Q value satisfies the soft Bellman equation, and thus the optimal policy presented in Theorem 1 is unique."}, {"heading": "B. Connection between Policy Gradient and Q-Learning", "text": "We show that entropy-regularized policy gradient can be viewed as performing soft Q-learning on the maximum-entropy objective. First, suppose that we parametrize a stochastic policy as\n\u03c0\u03c6(at|st) , exp ( E\u03c6(st,at)\u2212 E\u0304\u03c6(st) ) , (26)\nwhere E\u03c6(st,at) is an energy function with parameters \u03c6, and E\u0304\u03c6(st) = log \u222b A exp E\n\u03c6(st,at)dat is the corresponding partition function. This is the most general class of policies, as we can trivially transform any given distribution p into exponential form by defining the energy as log p. We can write an entropy-regularized policy gradient as follows:\n\u2207\u03c6JPG(\u03c6) = E(st,at)\u223c\u03c1\u03c0\u03c6 [ \u2207\u03c6 log \u03c0\u03c6(at|st) ( Q\u0302\u03c0\u03c6(st,at) + b \u03c6(st) )] +\u2207\u03c6 Est\u223c\u03c1\u03c0\u03c6 [ H(\u03c0\u03c6(\u00b7|st)) ] , (27)\nwhere \u03c1\u03c0\u03c6(st,at) is the distribution induced by the policy, Q\u0302\u03c0\u03c6(st,at) is an empirical estimate of the Q-value of the policy, and b\u03c6(st) is a state-dependent baseline that we get to choose. The gradient of the entropy term is given by\n\u2207\u03c6H(\u03c0\u03c6) =\u2212\u2207\u03c6 Est\u223c\u03c1\u03c0\u03c6 [ Eat\u223c\u03c0\u03c6(at|st) [ log \u03c0\u03c6(at|st) ]] =\u2212 E(st,at)\u223c\u03c1\u03c0\u03c6 [ \u2207\u03c6 log \u03c0\u03c6(at|st) log \u03c0\u03c6(at|st) +\u2207\u03c6 log \u03c0\u03c6(at|st)\n] =\u2212 E(st,at)\u223c\u03c1\u03c0\u03c6 [ \u2207\u03c6 log \u03c0\u03c6(at|st) ( 1 + log \u03c0\u03c6(at|st) )] , (28)\nand after substituting this back into Equation 27, noting Equation 26 and choosing b\u03c6(st) = E\u0304\u03c6(st) + 1, we arrive at a simple form for the policy gradient:\n= E(st,at)\u223c\u03c1\u03c0\u03c6 [( \u2207\u03c6E\u03c6(st,at)\u2212\u2207\u03c6E\u0304\u03c6(st) ) ( Q\u0302\u03c0\u03c6(st,at)\u2212 E\u03c6(st,at) )] . (29)\nTo show that Equation 29 indeed correponds to soft Q-learning update, we consider the Bellman error\nJQ(\u03b8) = Est\u223cqst ,at\u223cqat\n[ 1\n2\n( Q\u0302\u03b8soft(st,at)\u2212Q\u03b8soft(st,at) )2] , (30)\nwhere Q\u0302\u03b8soft is an empirical estimate of the soft Q-function. There are several valid alternatives for this estimate, but in order to show a connection to policy gradient, we choose a specific form\nQ\u0302\u03b8soft(st,at) = A\u0302 \u03b8\u0304 soft(st,at) + V \u03b8 soft(st), (31)\nwhere A\u0302\u03b8\u0304soft is an empirical soft advantage function that is assumed not to contribute the gradient computation. With this choice, the gradient of the Bellman error becomes\n\u2207\u03b8JQ(\u03b8) = Est\u223cqst ,at\u223cqat [( \u2207\u03b8Q\u03b8soft(st,at)\u2212\u2207\u03b8V \u03b8soft(st) ) ( A\u0302\u03b8\u0304soft(st,at) + V \u03b8 soft(st,at)\u2212Q\u03b8soft(st,at) )] = Est\u223cqst ,at\u223cqat [( \u2207\u03b8Q\u03b8soft(st,at)\u2212\u2207\u03b8V \u03b8soft(st) ) ( Q\u0302\u03b8soft(st,at)\u2212Q\u03b8soft(st,at) )] . (32)\nNow, if we choose E\u03c6(st,at) , Q\u03b8soft(st,at) and qst(st)qat(at) , \u03c1\u03c0\u03c6(st,at), we recover the policy gradient in Equation 29. Note that the choice of using an empirical estimate of the soft advantage rather than soft Q-value makes the target independent of the soft value, and at convergence, Q\u03b8soft approximates the soft Q-value up to an additive constant. The resulting policy is still correct, since the Boltzmann distribution in Equation 26 is independent of constant shift in the energy function.\nC. Implementation C.1. Computing the Actor Update\nHere we explain in full detail how the actor update direction \u2207\u0302\u03c6J\u03c0 in Algorithm 1 is computed. We reuse the indices i, j in this section with a different meaning than in the body of the paper for the sake of providing a cleaner presentation.\nExpectations appear in amortized SVGD in two places. First, SVGD approximates the optimal descent direction \u03c6(\u00b7) in Equation (13) with an empirical average over the samples a(i)t = f\n\u03c6(\u03be(i)). Similarly, SVGD approximates the expectation in Equation (14) with samples a\u0303(j)t = f\n\u03c6(\u03be\u0303(j)), which can be the same or different from a(i)t . Substituting (13) into (14) and taking the gradient gives the empirical estimate\n\u2207\u0302\u03c6J\u03c0(\u03c6; st) = 1\nKM K\u2211 j=1 M\u2211 i=1 ( \u03ba(a (i) t , a\u0303 (j) t )\u2207a\u2032Qsoft(st,a\u2032) \u2223\u2223 a\u2032=a (i) t +\u2207a\u2032\u03ba(a\u2032, a\u0303(j)t ) \u2223\u2223 a\u2032=a (i) t ) \u2207\u03c6f\u03c6(\u03be\u0303(j); st),\nFinally, the update direction \u2207\u0302\u03c6J\u03c0 is the average of \u2207\u0302\u03c6J\u03c0(\u03c6; st), where st is drawn from a mini-batch.\nC.2. SVGD for bounded domains\nIt is noteworthy that the original SVGD method only applies when the distributions are defined on Rd. However, bounded action spaces appear in many practical situations and also throughout the simulation experiments in this paper. Applying SVGD directly on the actions will cause them to saturate at the boundary of the action space instead of approximating the distribution exp(Q(s, \u00b7)). A simple trick to avoid this problem is to let the sampling network f\u03c6(\u03be; st) output unbounded values, while the Q network applies a squashing operator \u03c3(\u00b7) to those values before feeding them into a standard neural network. For example, if the action at each dimension is bounded by \u22121 and 1, then we can choose \u03c3(x) = tanh(x).\nC.3. Computing the density of sampled actions\nEquation 10 states that the soft value can be computed by sampling from a distribution qa\u2032 and that qa\u2032(\u00b7) \u221d exp ( 1 \u03b1Q \u03c6 soft(s, \u00b7) ) is optimal. A direct solution is to obtain actions from the sampling network: a\u2032 = f\u03c6(\u03be\u2032; s). If the samples \u03be\u2032 and actions a\u2032 have the same dimension, and if the jacobian matrix \u2202a \u2032\n\u2202\u03be\u2032 is non-singular, then the probability density is\nqa\u2032(a \u2032) = p\u03be(\u03be \u2032) 1\u2223\u2223\u2223det(\u2202a\u2032\u2202\u03be\u2032 )\u2223\u2223\u2223 . (33)\nIn practice, the Jacobian is usually singular at the beginning of training, when the sampler f\u03c6 is not fully trained. A simple solution is to begin with uniform action sampling and then switch to f\u03c6 later, which is reasonable, since an untrained sampler is unlikely to produce better samples for estimating the partition function anyway."}, {"heading": "D. Experiments", "text": "D.1. Hyperparameters\nThroughout all experiments, we use the following parameters for both DDPG and soft Q-learning. The Q values are updated using ADAM with learning rate 0.001. The DDPG policy and soft Q-learning sampling network use ADAM with a learning rate of 0.0001. The algorithm uses a replay pool of size one million. Training does not start until the replay pool has at least 10,000 samples. Every mini-batch has size 64. Each training iteration consists of 10000 time steps, and both the Q values and policy / sampling network are trained at every time step. All experiments are run for 500 epochs, except that the multi-goal task uses 100 epochs and the fine-tuning tasks are trained for 200 epochs. Both the Q value and policy / sampling network are neural networks of two hidden layers, with 200 hidden units at each layer and ReLU nonlinearity. Both DDPG and soft Q-learning use additional OU Noise (Uhlenbeck & Ornstein, 1930; Lillicrap et al., 2015) to improve exploration. The parameters are \u03b8 = 0.15 and \u03c3 = 0.3. In addition, we found that updating the target parameters too frequently can destabilize training. Therefore we freeze target parameters for every 1000 time steps (except for the swimming snake experiment, which freezes for 5000 epochs), and then copy the current network parameters to the target networks directly (\u03c4 = 1).\nSoft Q-learning uses K = M = 32 action samples (see Appendix C.1) to compute the actor update, except that the multigoal experiment uses K = M = 100. The number of additional action samples to compute the soft value is KV = 50. The kernel \u03ba is a radial basis function, written as \u03ba(a,a\u2032) = exp(\u2212 1h\u2016a \u2212 a\n\u2032\u201622), where h = d2 log(M+1) , with d equal to the median of pairwise distance of sampled actions a(i)t . Note that the step size h changes dynamically depending on the state s, as suggested in (Liu & Wang, 2016).\nThe entropy coefficient \u03b1 is 10 for multi-goal environment, and 0.1 for the swimming snake, maze, hallway (pretraining) and U-shaped maze (pretraining) experiments.\nAll fine-tuning tasks anneal the entropy coefficient \u03b1 quickly in order to improve performance, since the goal during finetuning is to recover a near-deterministic policy on the fine-tuning task. In particular, \u03b1 is annealed log-linearly to 0.001 within 20 epochs of fine-tuning. Moreover, the samples \u03be are fixed to a set {\u03bei} K\u03be i=1 and K\u03be is reduced linearly to 1 within 20 epochs.\nD.2. Task description\nAll tasks have a horizon of T = 500, except the multi-goal task, which uses T = 20. We add an additional termination condition to the quadrupedal 3D robot to discourage it from flipping over.\nD.3. Additional Results"}], "references": [{"title": "Linear Bellman combination for control of character animation", "author": ["M. Da Silva", "F. Durand", "J. Popovi\u0107"], "venue": "ACM Trans. on Graphs,", "citeRegEx": "Silva et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2009}, {"title": "Freeenergy based reinforcement learning for vision-based navigation with high-dimensional sensory inputs", "author": ["S. Elfwing", "M. Otsuka", "E. Uchibe", "K. Doya"], "venue": "In Int. Conf. on Neural Information Processing,", "citeRegEx": "Elfwing et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Elfwing et al\\.", "year": 2010}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["C. Florensa", "Y. Duan", "Abbeel"], "venue": "In Int. Conf. on Learning Representations,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Taming the noise in reinforcement learning via soft updates", "author": ["R. Fox", "A. Pakman", "N. Tishby"], "venue": "In Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Fox et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2016}, {"title": "Deep learning. chapter 8.7.4", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["S. Gu", "T. Lillicrap", "Z. Ghahramani", "R.E. Turner", "S. Levine"], "venue": "arXiv preprint arXiv:1611.02247,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Continuous deep Q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Reinforcement learning in feedback control", "author": ["R. Hafner", "M. Riedmiller"], "venue": "Machine Learning,", "citeRegEx": "Hafner and Riedmiller,? \\Q2011\\E", "shortCiteRegEx": "Hafner and Riedmiller", "year": 2011}, {"title": "Actor-critic reinforcement learning with energy-based policies", "author": ["N. Heess", "D. Silver", "Y.W. Teh"], "venue": "In Workshop on Reinforcement Learning,", "citeRegEx": "Heess et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2012}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["N. Heess", "G. Wayne", "Y. Tassa", "T. Lillicrap", "M. Riedmiller", "D. Silver"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade,? \\Q2002\\E", "shortCiteRegEx": "Kakade", "year": 2002}, {"title": "Path integrals and symmetry breaking for optimal control theory", "author": ["H.J. Kappen"], "venue": "Journal of Statistical Mechanics: Theory And Experiment,", "citeRegEx": "Kappen,? \\Q2005\\E", "shortCiteRegEx": "Kappen", "year": 2005}, {"title": "Deep directed generative models with energy-based probability estimation", "author": ["T. Kim", "Y. Bengio"], "venue": "arXiv preprint arXiv:1606.03439,", "citeRegEx": "Kim and Bengio,? \\Q2016\\E", "shortCiteRegEx": "Kim and Bengio", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine and Abbeel,? \\Q2014\\E", "shortCiteRegEx": "Levine and Abbeel", "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Q. Liu", "D. Wang"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Liu and Wang,? \\Q2016\\E", "shortCiteRegEx": "Liu and Wang", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "Rusu", "A. A", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Variational inference for policy search in changing situations", "author": ["G. Neumann"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Neumann,? \\Q2011\\E", "shortCiteRegEx": "Neumann", "year": 2011}, {"title": "PGQ: Combining policy gradient and Q-learning", "author": ["B. O\u2019Donoghue", "R. Munos", "K. Kavukcuoglu", "V. Mnih"], "venue": "arXiv preprint arXiv:1611.01626,", "citeRegEx": "O.Donoghue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2016}, {"title": "Free-energybased reinforcement learning in a partially observable environment", "author": ["M. Otsuka", "J. Yoshimoto", "K. Doya"], "venue": "In ESANN,", "citeRegEx": "Otsuka et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Otsuka et al\\.", "year": 2010}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "In AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "On stochastic optimal control and reinforcement learning by approximate inference", "author": ["K. Rawlik", "M. Toussaint", "S. Vijayakumar"], "venue": "Proceedings of Robotics: Science and Systems VIII,", "citeRegEx": "Rawlik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rawlik et al\\.", "year": 2012}, {"title": "Reinforcement learning with factored states and actions", "author": ["B. Sallans", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sallans and Hinton,? \\Q2004\\E", "shortCiteRegEx": "Sallans and Hinton", "year": 2004}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "In Int. Conf on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Loss is its own reward: Self-supervision for reinforcement learning", "author": ["E. Shelhamer", "P. Mahmoudieh", "M. Argus", "T. Darrell"], "venue": "arXiv preprint arXiv:1612.07307,", "citeRegEx": "Shelhamer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shelhamer et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "In Int. Conf on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. sabis"], "venue": "search. Nature,", "citeRegEx": "sabis,? \\Q2016\\E", "shortCiteRegEx": "sabis", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Bias in natural actor-critic algorithms", "author": ["P. Thomas"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Thomas,? \\Q2014\\E", "shortCiteRegEx": "Thomas", "year": 2014}, {"title": "Linearly-solvable Markov decision problems", "author": ["E. Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Todorov,? \\Q2007\\E", "shortCiteRegEx": "Todorov", "year": 2007}, {"title": "General duality between optimal control and estimation", "author": ["E. Todorov"], "venue": "In IEEE Conf. on Decision and Control,", "citeRegEx": "Todorov,? \\Q2008\\E", "shortCiteRegEx": "Todorov", "year": 2008}, {"title": "Compositionality of optimal control laws", "author": ["E. Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Todorov,? \\Q2009\\E", "shortCiteRegEx": "Todorov", "year": 2009}, {"title": "Robot trajectory optimization using approximate inference", "author": ["M. Toussaint"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Toussaint,? \\Q2009\\E", "shortCiteRegEx": "Toussaint", "year": 2009}, {"title": "On the theory of the brownian motion", "author": ["Uhlenbeck", "George E", "Ornstein", "Leonard S"], "venue": "Physical review,", "citeRegEx": "Uhlenbeck et al\\.,? \\Q1930\\E", "shortCiteRegEx": "Uhlenbeck et al\\.", "year": 1930}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["D. Wang", "Q. Liu"], "venue": "arXiv preprint arXiv:1611.01722,", "citeRegEx": "Wang and Liu,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Energybased generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["B.D. Ziebart"], "venue": "PhD thesis,", "citeRegEx": "Ziebart,? \\Q2010\\E", "shortCiteRegEx": "Ziebart", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from every state-action tuple (st,at) weighted by its probability \u03c1\u03c0 under the current policy. Note that this objective still takes into account the entropy of the policy at future states, in contrast to greedy objectives", "author": ["O\u2019Donoghue"], "venue": null, "citeRegEx": "O.Donoghue,? \\Q2016\\E", "shortCiteRegEx": "O.Donoghue", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Deep reinforcement learning (deep RL) has emerged as a promising direction for autonomous acquisition of complex behaviors (Mnih et al., 2015; Silver et al., 2016), due to its ability to process complex sensory input (Jaderberg et al.", "startOffset": 123, "endOffset": 163}, {"referenceID": 10, "context": ", 2016), due to its ability to process complex sensory input (Jaderberg et al., 2016) and to acquire elaborate behavior skills using general-purpose neural network representations (Levine et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 18, "context": ", 2016) and to acquire elaborate behavior skills using general-purpose neural network representations (Levine et al., 2016).", "startOffset": 102, "endOffset": 123}, {"referenceID": 19, "context": "Deep reinforcement learning methods can be used to optimize deterministic (Lillicrap et al., 2015) and stochastic (Schulman et al.", "startOffset": 74, "endOffset": 98}, {"referenceID": 23, "context": ", 2015) and stochastic (Schulman et al., 2015a; Mnih et al., 2016) policies.", "startOffset": 23, "endOffset": 66}, {"referenceID": 33, "context": "Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with", "startOffset": 145, "endOffset": 209}, {"referenceID": 19, "context": "Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with", "startOffset": 145, "endOffset": 209}, {"referenceID": 22, "context": "Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with", "startOffset": 145, "endOffset": 209}, {"referenceID": 12, "context": "high entropy (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016).", "startOffset": 13, "endOffset": 70}, {"referenceID": 23, "context": "high entropy (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016).", "startOffset": 13, "endOffset": 70}, {"referenceID": 45, "context": "Other benefits include robustness in the face of uncertain dynamics (Ziebart, 2010), imitation learning (Ziebart et al.", "startOffset": 68, "endOffset": 83}, {"referenceID": 46, "context": "Other benefits include robustness in the face of uncertain dynamics (Ziebart, 2010), imitation learning (Ziebart et al., 2008), and improved convergence and computational properties (Gu et al.", "startOffset": 104, "endOffset": 126}, {"referenceID": 38, "context": "As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference (Todorov, 2008).", "startOffset": 162, "endOffset": 177}, {"referenceID": 40, "context": "The solution can be shown to optimize an entropy-augmented reinforcement learning objective or to correspond to the solution to a maximum entropy learning problem (Toussaint, 2009).", "startOffset": 163, "endOffset": 180}, {"referenceID": 37, "context": "A number of methods have been proposed, including Z-learning (Todorov, 2007), maximum entropy inverse RL (Ziebart et al.", "startOffset": 61, "endOffset": 76}, {"referenceID": 46, "context": "A number of methods have been proposed, including Z-learning (Todorov, 2007), maximum entropy inverse RL (Ziebart et al., 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 40, "context": ", 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al.", "startOffset": 53, "endOffset": 70}, {"referenceID": 28, "context": ", 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al., 2012), and G-learning (Fox et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 3, "context": ", 2012), and G-learning (Fox et al., 2016), as well as more recent proposals in deep RL such as PGQ (O\u2019Donoghue et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 25, "context": ", 2016), as well as more recent proposals in deep RL such as PGQ (O\u2019Donoghue et al., 2016), but these generally operate either on simple tabular representations, which are difficult to apply to continuous or high-dimensional domains, or employ a simple parametric representation of the policy distribution, such as a conditional Gaussian.", "startOffset": 65, "endOffset": 90}, {"referenceID": 25, "context": "Therefore, although the policy is optimized to perform the desired skill in many different ways and, in the case of deep RL methods (O\u2019Donoghue et al., 2016), might even use an expressive high-capacity model for representing the mean action, the resulting distribution is typically very limited in terms of its representational power, even if the parameters of that distribution are represented by an expressive function approximator, such as a neural network.", "startOffset": 132, "endOffset": 157}, {"referenceID": 33, "context": "We explore this connection further in the paper, and in the course of this discuss connections to popular deep RL methods such as deterministic policy gradient (DPG) (Silver et al., 2014; Lillicrap et al., 2015), normalized advantage functions (NAF) (Gu et al.", "startOffset": 166, "endOffset": 211}, {"referenceID": 19, "context": "We explore this connection further in the paper, and in the course of this discuss connections to popular deep RL methods such as deterministic policy gradient (DPG) (Silver et al., 2014; Lillicrap et al., 2015), normalized advantage functions (NAF) (Gu et al.", "startOffset": 166, "endOffset": 211}, {"referenceID": 25, "context": ", 2016b), and PGQ (O\u2019Donoghue et al., 2016).", "startOffset": 18, "endOffset": 43}, {"referenceID": 25, "context": "Note that this objective differs qualitatively from the behavior of Boltzmann exploration (Sallans & Hinton, 2004) and PGQ (O\u2019Donoghue et al., 2016), which greedily maximize entropy at the current time step, but do not explicitly optimize for policies that aim to reach states where they will have high entropy in the future.", "startOffset": 123, "endOffset": 148}, {"referenceID": 46, "context": "This distinction is crucial, since the maximum entropy objective can be shown to maximize the entropy of the entire trajectory distribution for the policy \u03c0, while the greedy Boltzmann exploration approach does not (Ziebart et al., 2008; Levine & Abbeel, 2014).", "startOffset": 215, "endOffset": 260}, {"referenceID": 36, "context": "In the context of policy search algorithms, the use of a discount factor is actually a somewhat nuanced choice, and writing down the precise objective that is optimized when using the discount factor is non-trivial (Thomas, 2014).", "startOffset": 215, "endOffset": 229}, {"referenceID": 25, "context": "The choices in prior work include discrete multinomial distributions (O\u2019Donoghue et al., 2016) and Gaussian distributions (Rawlik et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 28, "context": ", 2016) and Gaussian distributions (Rawlik et al., 2012).", "startOffset": 35, "endOffset": 56}, {"referenceID": 45, "context": "1 as well as (Ziebart, 2010).", "startOffset": 13, "endOffset": 28}, {"referenceID": 45, "context": "2, as well as (Ziebart, 2010).", "startOffset": 14, "endOffset": 29}, {"referenceID": 44, "context": "Existing approaches that sample from energy-based distributions generally fall into two categories: methods that use Markov chain Monte Carlo (MCMC) based sampling (Sallans & Hinton, 2004), and methods that learn a stochastic sampling network trained to output approximate samples from the target distribution (Zhao et al., 2016; Kim & Bengio, 2016).", "startOffset": 310, "endOffset": 349}, {"referenceID": 21, "context": "The experience is stored in a replay memory buffer D as standard in deep Q-learning (Mnih et al., 2013), and the parameters are updated using random minibatches from this memory.", "startOffset": 84, "endOffset": 103}, {"referenceID": 21, "context": "The soft Q-function updates use a delayed version of the target values, characterized by a smoothing factor \u03c4 , to improve stability (Mnih et al., 2013).", "startOffset": 133, "endOffset": 152}, {"referenceID": 38, "context": "policy (Todorov, 2008), which has been exploited to construct practical path planning methods based on iterative linearization and probabilistic inference techniques (Toussaint, 2009).", "startOffset": 7, "endOffset": 22}, {"referenceID": 40, "context": "policy (Todorov, 2008), which has been exploited to construct practical path planning methods based on iterative linearization and probabilistic inference techniques (Toussaint, 2009).", "startOffset": 166, "endOffset": 183}, {"referenceID": 37, "context": "This has been explored in the context of linearly solvable MDPs (Todorov, 2007) and, in the case of inverse reinforcement learning, MaxEnt IRL (Ziebart et al.", "startOffset": 64, "endOffset": 79}, {"referenceID": 46, "context": "This has been explored in the context of linearly solvable MDPs (Todorov, 2007) and, in the case of inverse reinforcement learning, MaxEnt IRL (Ziebart et al., 2008).", "startOffset": 143, "endOffset": 165}, {"referenceID": 13, "context": "In continuous systems and continuous time, path integral control studies maximum entropy policies and maximum entropy planning (Kappen, 2005).", "startOffset": 127, "endOffset": 141}, {"referenceID": 27, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 24, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 28, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 3, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 3, "context": ", 2012; Fox et al., 2016). Among these, the work of Rawlik et al. (2012) re-", "startOffset": 8, "endOffset": 73}, {"referenceID": 2, "context": "The form of our sampler resembles the stochastic networks proposed in recent work on hierarchical learning (Florensa et al., 2017).", "startOffset": 107, "endOffset": 130}, {"referenceID": 11, "context": "A closely related concept to maximum entropy policies is Boltzmann exploration, which uses the exponential of the standard Q-function as the probability of an action (Kaelbling et al., 1996).", "startOffset": 166, "endOffset": 190}, {"referenceID": 1, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012).", "startOffset": 181, "endOffset": 268}, {"referenceID": 26, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012).", "startOffset": 181, "endOffset": 268}, {"referenceID": 8, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012).", "startOffset": 181, "endOffset": 268}, {"referenceID": 1, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012). Although these methods are closely related, they have not, to our knowledge, been extended to the case of deep network models, have not made extensive use of approximate inference techniques, and have not been demonstrated on the complex continuous tasks. More recently, O\u2019Donoghue et al. (2016) drew a connection between Boltzmann exploration and entropy-regularized policy gradient, though in a theoretical framework that differs from maximum entropy policy search: unlike the full maximum entropy framework, the approach of O\u2019Donoghue et al.", "startOffset": 206, "endOffset": 566}, {"referenceID": 1, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012). Although these methods are closely related, they have not, to our knowledge, been extended to the case of deep network models, have not made extensive use of approximate inference techniques, and have not been demonstrated on the complex continuous tasks. More recently, O\u2019Donoghue et al. (2016) drew a connection between Boltzmann exploration and entropy-regularized policy gradient, though in a theoretical framework that differs from maximum entropy policy search: unlike the full maximum entropy framework, the approach of O\u2019Donoghue et al. (2016) only optimizes for maximizing entropy at the current time step, rather than planning for visiting future states where entropy will be further maximized.", "startOffset": 206, "endOffset": 822}, {"referenceID": 19, "context": "It is particularly instructive to observe the connection between our approach and the deep deterministic policy gradient method (DDPG) (Lillicrap et al., 2015), which updates a Qfunction critic according to (hard) Bellman updates, and then backpropagates the Q-value gradient into the actor, similarly to NFQCA (Hafner & Riedmiller, 2011).", "startOffset": 135, "endOffset": 159}, {"referenceID": 19, "context": "Our experiments aim to answer the following questions: (1) Does our soft Q-learning method accurately capture a multi-modal policy distribution? (2) Can soft Q-learning with energy-based policies aid exploration for complex tasks that require tracking multiple modes? (3) Can a maximum entropy policy serve as a good initialization for finetuning on different tasks, when compared to pretraining with a standard deterministic objective? We compare our algorithm to DDPG (Lillicrap et al., 2015), which has been shown to achieve better sample efficiency on the continuous control problems that we consider than other recent techniques such as REINFORCE (Williams, 1992), TRPO (Schulman et al.", "startOffset": 470, "endOffset": 494}, {"referenceID": 23, "context": ", 2015a), and A3C (Mnih et al., 2016).", "startOffset": 18, "endOffset": 37}, {"referenceID": 30, "context": "3D robot (adapted from Schulman et al. (2015b)) needs to find a path through a maze to a target position (see Figure 2).", "startOffset": 23, "endOffset": 47}, {"referenceID": 4, "context": "A standard way to accelerate deep neural network training is task-specific initialization (Goodfellow et al., 2016), where a network trained for one task is used as initialization for another task.", "startOffset": 90, "endOffset": 115}, {"referenceID": 32, "context": "Pretraining has also been explored in the context of RL (Shelhamer et al., 2016).", "startOffset": 56, "endOffset": 80}, {"referenceID": 9, "context": "This pretraining is similar in some ways to recent work on modulated controllers (Heess et al., 2016) and hierarchical models (Florensa et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 2, "context": ", 2016) and hierarchical models (Florensa et al., 2017).", "startOffset": 32, "endOffset": 55}, {"referenceID": 39, "context": "In the context of linearly solvable MDPs, several prior works have shown that policies trained for different tasks can be composed to create new optimal policies (Da Silva et al., 2009; Todorov, 2009).", "startOffset": 162, "endOffset": 200}], "year": 2017, "abstractText": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actorcritic methods, which can be viewed performing approximate inference on the corresponding energy-based model.", "creator": "LaTeX with hyperref package"}}}