{"id": "1702.08169", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis", "abstract": "we study the fundamental problem of principal component analysis in a statistical constrained distributed setting in which when each machine out of $ b m $ stores a sample of $ n $ points sampled i. i. d. from a single unknown digital distribution. we study algorithms for estimating the leading principal component of the population covariance matrix that are both communication - efficient sets and achieve estimation error of the order of the centralized database erm solution that uses all $ mn $ samples. on the negative side, mostly we show that in sharp contrast to results mostly obtained for distributed estimation under convexity assumptions, for the pca implementation objective, simply averaging the local erm solutions naturally cannot guarantee error that is consistent with the centralized numerical erm. we show that this unfortunate phenomena can be simply remedied by performing rather a simple correction step which correlates between the individual solutions, and provides an arithmetic estimator that is consistent regularly with the centralized erm for accepting sufficiently - large $ n $. we also introduce an intermediate iterative distributed algorithm that is applicable best in any regime of $ n $, which is strictly based on distributed matrix - vector products. the algorithm gives significant acceleration in terms of communication rounds done over previous distributed query algorithms, in a sufficient wide regime of parameters.", "histories": [["v1", "Mon, 27 Feb 2017 07:45:58 GMT  (36kb)", "http://arxiv.org/abs/1702.08169v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dan garber", "ohad shamir", "nathan srebro"], "accepted": true, "id": "1702.08169"}, "pdf": {"name": "1702.08169.pdf", "metadata": {"source": "CRF", "title": "Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis", "authors": ["Dan Garber"], "emails": ["dangar@technion.ac.il", "ohad.shamir@weizmann.ac.il", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n08 16\n9v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20"}, {"heading": "1 Introduction", "text": "Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning. For data that consists of N vectors in Rd, x1, ...,xN , with normalized covariance matrix X\u0302 = 1 N \u2211N i=1 xix \u22a4 i , The PCA method finds the k-dimensional subspace (which corresponds to the span of the top k principal components) such that the projection of the data onto the subspace has largest variance, i.e., it is the solution to the optimization problem:\nmax W\u2208Rd\u00d7k ,WTW=I\n\u2016X\u0302W\u20162F . (1)\nPCA is often considered in a statistical setting in which the assumption is that the input vectors are not arbitrary but sampled i.i.d. from some fixed but unknown distribution with certain general characteristics D. Then, it is often of interest to use the observed sample to estimate the top k principal components of the population covariance matrix, rather then that of the sample, which leads to the modified optimization problem:\nmax W\u2208Rd\u00d7k ,WTW=I\n\u2016Ex\u223cD [ xx\u22a4 ] W\u20162F . (2)\nOf course the empirical estimation problem (1) and the population estimation problem (2) are well connected, and it is well-known that under mild assumptions on the distribution D and\ngiven a sufficiently large sample, we can guarantee small estimation error in (2) by solving optimization problem (1).\nIn this work we consider the problem of estimating the first principal component (i.e., k = 1) in a statistical and distributed setting. We assume the availability of m machines, each of which stores a sample of n vectors sampled i.i.d from a fixed distribution D over Rd, and we are interested in algorithms that can be applied efficiently to solve Problem (2) for k = 1, with estimation error that approaches that of a centralized algorithm, which has access to all mn samples and does not pay for communication between machines. Indeed, when considering the efficiency of algorithms, we will mainly focus on the amount of communication between machines they require, since this is often the most expensive resource in distributed computing. We note that the i.i.d. assumption is standard in many applications of PCA, and can be leveraged to get more efficient algorithms than when the data partition is arbitrary. Also, we will make a standard assumption that the population covariance matrix has a non-zero additive gap between the first and second eigenvalues, which makes the problem of estimating the leading principal component meaningful.\nA main challenge that often arises in many computational settings of principal components is that it leads to inherently non-convex optimization problems. While many times these problems turn out to admit efficient algorithms, the rich toolbox of optimization and statistical estimation procedures developed for convex problems often cannot be directly applied to problems such as (1) and (2). Instead, one often needs to consider a specialized and more involved analysis, to get analogous convergence results for the PCA problem. This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem. This is also the case in our distributed setting. For instance, [26] proposed communication-efficient algorithms for a distributed statistical estimation settings, similar to ours, but under convexity assumptions. The authors show that under their assumptions, in a wide regime of parameters (namely when the per-machine sample size n is large enough), then a simple averaging of the empirical risk minimizers (ERM), computed locally on each machine, leads to estimation error of the population parameters of the order the centralized ERM solution. While averaging makes perfect sense in a convex setting, it is clear that it can completely fail in a non-convex setting. Indeed, we show that already for the PCA problem with k = 1, simply averaging the local ERM solutions (and normalizing to obtain a unit vector as required), cannot improve significantly over the estimation error of any single machine. We then show that a simple fix to the above scheme, namely correlating the directions of individual ERM solutions, remedies this phenomena and results in estimation error similar to that of the centralized ERM solution. Much like the results of [26], this result only holds in the regime when the per-machine sample size n is sufficiently large. As discussed, due to the inherent non-convexity of the PCA objective, this approach requires a novel analysis tailored to the PCA problem. In this context, we view this work as an initiation of a research effort to understand how to efficiently aggregate statistical estimators in a distributed non-convex setting.\nA second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]). The benefit of these methods is that (a) they provide meaningful estimation error guarantees in a much wider regime of parameters than the \u201cone-shot\u201d aggregation methods (namely in terms of the number of samples per machine), and (b), due to their iterative nature, they allow to approximate the centralized ERM solution arbitrary well. Unfortunately, these methods, all of which rely heavily on convexity assumption, cannot be directly applied to the PCA problem. Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7]. The S&I method reduces the problem of computing the leading eigenvector of a real positive semidefinite\nmatrix to that of approximately solving a small number (i.e. poly-logarithmic in the problem parameters) of systems of linear equations. These in turn, could be efficiently solved by arbitrary distributed convex solvers. We show that coupling the S&I method with the stochastic pre-conditioning technique for linear systems proposed in [27] and well known fast gradient methods such as the conjugate gradient method, gives state-of-the-art guarantees in terms of communication costs, and provides a significant improvement over distributed variants of classical fast eigenvector algorithms such as power iterations and the faster Lanczos algorithm. Much like its convex counterparts, which only rely on distributed gradient computations and simple vector aggregations, our iterative method only relies on distributed matrix-vector products, i.e., it requires each machine to only send products of its local empirical covariance matrix with some input vector.\nBeyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits. The approximation guarantees provided in these works are in terms of the projection of the data onto the leading principal components (instead of alignment between the estimate and the optimal solution, studied in this paper). Applying these results to our setting will give a number of communication rounds that scales like poly(\u01eb\u22121\u03b4\u22121), where \u01eb is the desired error and \u03b4 is the population eigengap. In our setting, \u01eb will scale with the inverse of the size of the sample, i.e., \u01eb \u2248 (mn)\u22121, which for these algorithms will result in amount of communication that is polynomial in the size of the data. In contrast, we will be interested in algorithms whose communication costs does not scale with n at all. In this context we note that, by focusing on algorithms that either perform simple aggregation of local ERM solutions, or perform only distributed matrix-vector products with the empirical covariance matrix, we can circumvent the need to measure communication explicitly in terms of the number of bits transmitted, which often burdens the analysis of natural algorithms, such as those proposed here."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Notation and problem setting", "text": "We write vectors in Rd in boldface lower-case letters (e.g., v), matrices in boldface upper-case letters (e.g., X), and scalars are written as lightface letters (e.g., c). We let \u2016 \u00b7 \u2016 denote the standard Euclidean norm for vectors and the spectral norm for matrices.\nWe consider the following statistical distributed setting. Let D be a distribution over vectors in Rd with squared \u21132 norm at most b, for some b > 0. We consider a setting in which m machines, numbered 1...m, are each given a dataset of n samples drawn i.i.d. from D. We let v1 denote a leading eigenvector of the population covariance matrix X = Ex\u223cD[xx\u22a4]. Our goal is to efficiently (mainly in terms of communication) find an estimate w for v1, i.e., a unit vector that maximizes the product (v\u22a41 w)\n2 with high probability. Towards this end, we assume that the population covariance matrix X has a non-zero eigengap \u03b4, i.e., \u03b4 := \u03bb1(X) \u2212 \u03bb2(X) > 0, where \u03bbi(\u00b7) denotes the ith largest eigenvalue of a symmetric real matrix. Note that \u03b4 > 0 is necessary for v1 to be uniquely defined (up to sign).\nIn addition, we let X\u0302i denote the empirical covariance matrix of the sample stored on\nmachine i for every i \u2208 [m], i.e., X\u0302i = 1n \u2211n j=1 x (i) j x (i)\u22a4 j , where x (i) 1 ...x (i) n are the samples stored on machine i. We let X\u0302 denote the empirical covariance matrix of the union of points across all machines i.e., X\u0302 = 1m \u2211m i=1 X\u0302i.\nOur model of communication assumes that the m machines work in rounds during which a central machine (w.l.o.g. machine 1) can send a single vector in Rd to all other machines, or every machine can send either the leading eigenvector of its local empirical covariance matrix, or the product of a single input vector with its local covariance, to machine 1. We will measure\ncommunication complexity in terms of number of such rounds required to achieve a certain estimation error."}, {"heading": "2.1.1 The centralized solution", "text": "Our primary benchmark for measuring performance will be the centralized empirical risk minimizer which is the leading eigenvector of the aggregated empirical covariance matrix X\u0302.\nThe following standard result bounds the error of the centralized ERM.\nLemma 1 (Risk of centralized ERM). Fix p \u2208 (0, 1). Suppose that \u03b4 > 0 and let v\u03021 denote the leading eigenvector of X\u0302, i.e., v\u03021 \u2208 argmaxv:\u2016v\u2016=1 v\u22a4X\u0302v. Then it holds w.p. at least 1\u2212 p that\n1\u2212 (v\u22a41 v\u03021)2 \u2264 \u01ebERM(p) := 32b2 ln(d/p)\nmn\u03b42 . (3)\nLemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the Davis-Kahan sin(\u03b8) theorem (whose proof is given in the appendix for completeness):\nTheorem 1 (Matrix Hoeffding, see [24]). Let D be a distribution over vectors with squared \u21132 norm at most b, and let X = Ex\u223cD[xx\u22a4]. Let X\u0302 = 1 n \u2211n i=1 xix \u22a4 i , where x1, ...,xn are sampled i.i.d. from D. Then, it holds that\n\u2200\u01eb > 0 : Pr ( \u2016X\u0302\u2212X\u2016 \u2265 \u01eb ) \u2264 d \u00b7 exp ( \u2212 \u01eb 2n\n16b2\n)\n.\nTheorem 2 (Davis-Kahan sin(\u03b8) theorem). Let X,Y be symmetric real d \u00d7 d matrices with leading eigenvectors vX and vY respetively. Also, suppose that \u03b4(X) := \u03bb1(X) \u2212 \u03bb2(X) > 0. Then it holds that\n1\u2212 ( v\u22a4XvY )2 \u2264 2\u2016X\u2212Y\u2016 2\n\u03b4(X)2 ."}, {"heading": "2.2 Informal statement of main results and previous algorithms", "text": "We now informally describe our main results, followed by a detailed description of previous approaches that are directly applicable to our setting. The algorithmic results (both new and old) are summarized in Table 1."}, {"heading": "2.2.1 Main results", "text": "Failure of simple averaging of local ERM solutions We show that a natural approach of simply averaging the individual leading eigenvectors of the empirical covariance matrices X\u0302i (and normalizing the obtain a unit vector) cannot significantly improve (beyond logarithmic factors) over the performance of any of the individual eigenvectors. More concretely, if we let v\u0302 (i) 1 denote the leading eigenvector of X\u0302i for any i \u2208 [m], and we denote their average by v\u03041 = 1 m \u2211m i=1 v\u0302 (i) 1 , then there exists a distribution D over vectors with magnitude O(1) and covariance eigengap \u03b4 = 1, such that\n\u2200m,n : ED [ 1\u2212 ( v\u0304\u22a41 v1 \u2016v\u03041\u2016 )2 ] = \u2126 ( 1 n ) ,\nSee Theorem 3 in Section 3 for the complete and formal argument."}, {"heading": "A successful single communication round algorithm via correlation of individual", "text": "ERM solutions We show that if prior to averaging the local ERM solutions, as suggested above, we correlate their directions by aligning them according to any single machine (say machine number 1), i.e., we let v\u03041 = 1 m \u2211m i=1 sign(v\u0302 (i)\u22a4 1 v\u0302 (1) 1 )v\u0302 (i) 1 , then this guarantees that for any p \u2208 (0, 1), w.p. at least 1\u2212 p,\n1\u2212 ( v\u0304\u22a41 v1 \u2016v\u03041\u2016 )2 = O\n\n\nb2 ln (\ndm p\n)\n\u03b42mn +\nb4 ln2 (\ndm p\n)\n\u03b44n2\n\n . (4)\nSee Theorem 4 in Section 3 for the complete and formal result. In particular, in the likely scenario when m = O(d/p) we have that w.p. at least 1 \u2212 p,\n1 \u2212 ( v\u0304\u22a41 v1/\u2016v\u03041\u2016 )2 = \u01ebERM(p)) \u00b7 O ( 1 +m2 \u00b7 \u01ebERM(p) )\n, where \u01ebERM(p)) is defined in Eq. (3). Another related interpretation of the results is that the bound in Eq. (4) is comparable with \u01ebERM (up to poly-log factors) when n = \u2126 ( \u03b4\u22122b2m ln(dm/p) )\n. We also show a matching lower bound that the bound in Eq. (4) is tight (up to poly-log\nfactors) for this aggregation method, see Theorem 5.\nA multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem. We show that for any p \u2208 (0, 1), when mn = \u2126(b2 ln(d/p)/\u03b42) (i.e., when Lemma 3 is meaningful), the algorithm produces a solution w such that w.p. at least 1\u2212 p,\n1\u2212 (v\u22a41 w)2 \u2264 \u01ebERM(p)) \u00b7 (1 + o(1)) , (5)\nwhere \u01ebERM(p)) is defined in Eq. (3). The algorithm performs overall O\u0303( \u221a b\u03b4\u22121/2n\u22121/4) distributed matrix-vector products with the centralized empirical covariance matrix X\u0302 1. The O\u0303(\u00b7) notation hides poly-logarithmic factors in 1/p, 1/\u03b4, d, 1/\u01ebERM(p). See Theorem 6 in Section 4 for the complete and formal result.\nWe note that in particular, under our assumption that mn = \u2126\u0303(b2/\u03b42), it holds that the number of distributed matrix-vector products is upper bounded by O\u0303(m1/4). Moreover, in the regime n = \u2126(b2\u03b4\u22122), we can see that the number of distributed matrix-vector products depends only poly-logarithmically on the problem parameters.\nIn general, the sub-constant o(1) factor in (5) could be made arbitrarily small by trading the approximation error with the number of distributed matrix-vector products.\n1i.e., on each round, each machine i sends the product of an input vector in Rd with its local covariance matrix X\u0302i."}, {"heading": "2.2.2 Previous algorithms", "text": "Distributed versions of classical iterative algorithms: Classical fast iterative algorithms for computing the leading eigenvector of a positive semidefinite matrix, such as the well-known Power Method and the Lanczos Algorithm, require iterative multiplications of the input matrix (X\u0302 in our case) with the current estimate. It is thus straightforward to implement these algorithms in our distributed setting, by multiplying the same vector with the covariance matrices at each machine, and averaging the result. Thus, by well-known convergence guarantees of these two methods, we will have that for a fixed \u01eb > 0, these methods produce a unit vector w such that, for any p \u2208 (0, 1), 1 \u2212 (w\u22a4v\u03021)2 \u2264 \u01eb w.p. at least 1 \u2212 p, after O(\u03bb\u03021\u03b4\u0302\u22121 ln(d/p\u01eb)) rounds for the Power Method and O( \u221a\n\u03bb\u03021\u03b4\u0302\u22121 ln(d/p\u01eb)) for the Lanczos Algorithm, where \u03bb\u03021, \u03b4\u0302 denote the leading eigenvalue and eigengap of X\u0302, respectively. Moreover, in the regime of mn in which Lemma 1 is meaningful, we can replace \u03bb\u03021, \u03b4\u0302 with \u03bb1, \u03b4 in the above bounds, and the result will still hold with high probability.\nSimple calculations show that in the regime of mn in which Lemma 1 is meaningful, it holds that our Shift-and-Invert-based algorithm outperforms distributed Lanczos (in terms of worst-case guarantees) whenever n = \u2126\u0303(b2/\u03bb21).\n\u201cHot potato\u201d SGD: Another straightforward approach is to apply a sequential algorithm for direct risk minimization that can process the data-points one by one, such as stochastic gradient descent (SGD), by passing its state from one machine to the next, after completing a full pass over the machine\u2019s data. Clearly, this process of making a full pass over the data of a certain machine before sending the final estimate to the next one, requires overall m communication rounds in order to make a full pass over all mn points. SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3]. For instance applying the result of [12] in this way will result in a final estimate w satisfying\n1\u2212 (w\u22a4v1)2 = O ( b2 ln d\n\u03b42mn\n)\nw.p. at least 3/4. (6)\nWe note that in the regime in which the bound in (6) is meaningful it holds that the number of communication rounds of our Shift-and-Invert-based algorithm is upper-bounded by O\u0303(m1/4) which for sufficiently large m dominates the communication complexity of SGD."}, {"heading": "3 Single Communication Round Algorithms via ERM on Each", "text": "Machine\nIn this section we consider distributed algorithms that require only a single round of communication. Naturally for this regime, all algorithms will be based on aggregating the ERM solutions of the individual machines, i.e., each machine i only sends the leading eigenvector of its empirical covariance matrix X\u0302i to a centralized machine (without loss of generality, machine 1) which it turn combines them to a single unit vector in some manner."}, {"heading": "3.1 Simple averaging of eigenvectors fail", "text": "Perhaps the simplest method to aggregate the individual eigenvectors of each machine is to average them, and then normalize to obtain a unit vector. For instance, in the distributed statistical setting considered in [26], in which the objective is strongly convex, it was shown that simply averaging the individual ERM solutions leads, in a meaningful regime of parameters, to estimation error of the order of the centralized ERM solution. However, here we show that for PCA, in which the objective is certainly not convex, this approach fails practically in any\nregime, in the sense that the error of the returned aggregated solution can be no better than that returned by any single machine.\nTheorem 3. There exists a distribution over vectors in R2 with \u21132 norm bounded by a universal constant for which the eigengap in the covariance matrix is 1 (i.e., \u03b4 = 1), such that if each machine i returns an estimate v\u0302 (i) 1 which is an unbiased leading eigenvector of X\u0302i (i.e., both outcomes \u2212v\u0302(i)1 ,+v\u0302 (i) 1 are equally likely), then the aggregated vector v\u03041 = 1 m \u2211m i=1 v\u0302 (i) 1 satisfies\n\u2200m,n : E [ 1\u2212 \u2329 v\u03041\n\u2016v\u03041\u2016 ,v1\n\u232a2 ]\n= \u2126(1/n).\nThe proof is given in the appendix."}, {"heading": "3.2 Averaging with Sign Fixing", "text": "As evident from the statement of Theorem 3, an important assumption is that each machine produces an unbiased estimate, in the sense that the sign of the outcome is uniform and independent of the other machines. This hints that correlating the signs of the different estimates can circumvent the lower bound result in Theorem 3. It turns out that this is indeed the case, as captured by the following theorem:\nTheorem 4. Let w\u0303i be the leading eigenvector of X\u0302i for any i \u2208 [m], and consider the unit vector\nw =\n\u2211m i=1 sign(w\u0303 \u22a4 i w\u03031)w\u0303i\n\u2016\u2211mi=1 sign(w\u0303\u22a4i w\u03031)w\u0303i\u2016 . (7)\nThen, for any p \u2208 (0, 1), it holds w.p. at least 1\u2212 p that\n1\u2212 (v\u22a41 w)2 = O\n\n\nb2 log (\ndm p\n)\n\u03b42mn +\nb4 log2 (\ndm p\n)\n\u03b44n2\n\n .\nFor ease of presentation, throughout the rest of this section we denote the correlated vector w\u0302i = sign(w\u0303 \u22a4 i w\u03031)w\u0303i for any i \u2208 [m].\nThe main step towards proving Theorem 4 is to consider each w\u0302i as an approximately unbiased perturbation of the true leading eigenvector v1 and to upper bound the magnitude of this perturbation. This is carried out in the following much more general and self-contained lemma, which might be of independent interest.\nLemma 2. Let A be a positive semidefinite matrix with some fixed leading eigenvector v1, a leading eigenvalue \u03bb1 and an eigengap \u03b4 := \u03bb1(A) \u2212 \u03bb2(A) > 0. Let A\u0302 be some positive semidefinite matrix such that \u2016A\u0302\u2212A\u2016 \u2264 \u03b4/4. Then there is a unique leading eigenvector v\u03021 of A\u0302 such that \u3008v\u03021,v\u3009 \u2265 0, and\n\u2225 \u2225\n\u2225 v\u03021 \u2212 v1 \u2212 (\u03bb1I\u2212A)\u2020(A\u0302\u2212A)v1\n\u2225 \u2225 \u2225 \u2264 c\u2016A\u0302\u2212A\u2016 2\n\u03b42 ,\nwhere \u2020 denotes the pseudo-inverse, and c is a positive numerical constant.\nProof. The proof is based on viewing A\u0302 as an unbiased perturbation of the matrix A, and computing a Taylor expansion of v\u03021 around v1. For notational convenience, let E = A\u0302 \u2212 A, and define A(t) = A+ tE for t \u2208 [0, 1]. Also, define \u03bb(t) to be the leading eigenvalue of A(t).\nFirst, we note that for any t \u2208 [0, 1], A(t) has an eigengap of at least \u03b4/2 between its first two eigenvalues (since by Weyl\u2019s inequality, its eigenvalues are at most \u2016tE\u2016 \u2264 \u2016E\u2016 \u2264 \u03b4/4 different\nthan A, and we know that A has an eigengap of \u03b4). Therefore, the leading eigenvalue of A(t) is simple. This means that the function v(t), which equals the leading eigenvector of A(t), is uniquely defined up to a sign. This sign will be chosen so that \u3008v(t),v1\u3009 \u2265 0, which makes v(t) unique and well-defined2. By Theorem 1 in [16], we have that both \u03bb(t) and v(t) are infinitely differentiable at any t \u2208 [0, 1], and satisfy3\n\u03bb\u2032(t) = v(t)\u22a4Ev(t) , v\u2032(t) = (\u03bb(t)I \u2212A(t))\u2020Ev(t) .\nWe will also need to bound the second derivative of v(t). By the product rule and the equations above, this derivative equals\nv\u2032\u2032(t) = \u2202\n\u2202t\n( (\u03bb(t)I\u2212A(t))\u2020 ) Ev(t) + (\u03bb(t)I \u2212A(t))\u2020E \u2202 \u2202t v(t)\n= \u2202\n\u2202t\n( (\u03bb(t)I\u2212A(t))\u2020 ) Ev(t) + (\u03bb(t)I \u2212A(t))\u2020E(\u03bb(t)I \u2212A(t))\u2020Ev(t). (8)\nTo compute the derivative above, we apply the chain rule. The derivative of a pseudo-inverse B\u2020 of a matrix-valued function B = B(t) with respect to t (assumingB and hence its pseudo-inverse is symmetric for all t) is given by (see Theorem 4.3 in [8])\n\u2212B\u2020 ( \u2202\n\u2202t B\n)\nB\u2020 + ( B\u2020 )2\n(\n\u2202 \u2202t B\n) (I \u2212BB\u2020) + (I \u2212B\u2020B) ( \u2202\n\u2202t B\n)\n( B\u2020 )2 .\nThis formula is true assuming the rank of B is constant in some open neighborhood of t. Applying this for B = \u03bb(t)I \u2212 A(t) (which indeed has a fixed rank of d \u2212 1 by the eigengap assumption), noting that \u2225 \u2225\n\u2202 \u2202t(\u03bb(t)I \u2212A(t))\n\u2225 \u2225 = \u2225 \u2225v(t)\u22a4Ev(t)I \u2212E \u2225 \u2225 \u2264 2\u2016E\u2016, and using the facts that \u2016v(t)\u2016 = 1, \u2016I\u2212B\u2020B\u2016 \u2264 1,\u2016I\u2212BB\u2020\u2016 \u2264 1 and \u2016(\u03bb(t)I\u2212A(t))\u2020\u2016 \u2264 2/\u03b4 (since the smallest non-zero eigenvalue of \u03bb(t)I\u2212A(t) is at least \u03b4/2), we have that\n\u2225 \u2225 \u2225 \u2225 \u2202\n\u2202t\n( (\u03bb(t)I\u2212A(t))\u2020 )\n\u2225 \u2225 \u2225 \u2225 \u2264 24 \u00b7 \u2016E\u2016 \u03b42 .\nPlugging this into (8), and again using the fact that \u2016(\u03bb(t)I \u2212A(t))\u2020\u2016 \u2264 2/\u03b4, we get that\n\u2225 \u2225v\u2032\u2032(t) \u2225 \u2225 \u2264 c\u2016E\u2016 2\n\u03b42\nfor some numerical constant c. By a first-order Taylor expansion of v(t) with an explicit remainder term4,\nv(1) = v(0) + v\u2032(0) + 1\n2\n\u222b 1\nt=0 (1\u2212 t)2v\u2032\u2032(t)dt ,\nwhich by the equations above and the definition of v(t) implies that\nv\u03021 = v1 + (\u03bb1I\u2212A)\u2020Ev1 + 1\n2\n\u222b 1\nt=0 (1\u2212 t)2v\u2032\u2032(t)dt .\n2Note that ties are impossible, since that can only happen if \u3008v(t),v1\u3009 = 0, yet by applying the Davis-Kahan\nsin(\u03b8) theorem (Theorem 2), \u221a 1\u2212 \u3008v(t),v1\u30092 \u2264 2\u2016A(t)\u2212A\u2016 \u03b4 \u2264 2\u2016E\u2016 \u03b4 \u2264 1 2 .\n3Formally speaking, the theorem only ensures v(t), \u03bb(t) exist and are infinitely differentiable in some open neighborhood of t. However, since the result holds for any t \u2208 [0, 1], and the proof implies that these functions are unique in each such neighborhood (where the uniqueness of v(t) holds once we fixed the sign as above), it follows that the same holds in all of t \u2208 [0, 1].\n4Since v(t),v\u2032(t),v\u2032\u2032(t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t 7\u2192 v(t)j , mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in [16], and in particular twice continuously differentiable.\nThis implies\n\u2225 \u2225 \u2225v\u03021 \u2212 v1 \u2212 (\u03bb1I\u2212A)\u2020Ev1 \u2225 \u2225 \u2225 \u2264 1 2\n\u222b 1 t=0 (1\u2212 t)2\u2016v\u2032\u2032(t)\u2016dt \u2264 c\u2016E\u2016 2 2\u03bb2 \u222b 1 t=0 (1\u2212 t)2dt,\nwhich is at most c\u2032\u2016E\u20162/\u03bb2 for some appropriate numerical constant c\u2032. Plugging back E = A\u0302\u2212A, the result follows.\nLemma 2 is central to the proof of the following Lemma, of which the proof of Theorem 4 is an easy consequence.\nLemma 3. The following two conditions hold with probability at least 1\u2212 p\u2212 d exp(\u2212\u03b42n/cb2), for some numerical constants c, c\u2032 > 0:\n\u2022 The leading eigenvalue of every X\u0302i is simple, i.e., \u03bb1(X\u0302i)\u2212 \u03bb2(X\u0302i) > 0.\n\u2022 Fixing v1, there exist unique leading eigenvectors v\u0302i1, . . . , v\u0302im of X\u03021, . . . , X\u0302m, such that maxi \u2016v\u0302i1 \u2212 v1\u2016 \u2264 14 , and\n\u2225 \u2225 \u2225 1\nm\nm \u2211\ni=1\nv\u0302i1 \u2212 v1 \u2225 \u2225 \u2225 \u2264 c\u2032\n(b2 log(2dm/p)\n\u03b42n +\n\u221a\nb2 log(2dm/p)\n\u03b42mn\n)\n.\nProof. Using the matrix Hoeffding inequality (Theorem 1) and a union bound, we that\nPr\n(\n\u2203i, \u2016X\u0302i \u2212X\u2016 > \u03b4\n12\n) \u2264 md exp ( \u2212 \u03b4 2n\nc\u2032b2\n)\n(9)\nfor some constant c\u2032 > 0. Thus, with high probability, maxi \u2016X\u0302i \u2212 X\u2016 \u2264 \u03b4/12. By Weyl\u2019s inequality, it follows that the eigenvalues of X and X\u0302i are at most \u03b4/12 apart, and since X has an eigengap of \u03b4 between its two leading eigenvalues, it follows that X\u0302i has an eigengap of at least \u03b4 \u2212 \u03b4/12 \u2212 \u03b4/12 > 0, which proves the first part of the lemma. To handle the second part, note that by a variant of the Davis-Kahan sin\u03b8 theorem (see Corollary 1 in [25]), if maxi \u2016X\u0302i \u2212 X\u2016 \u2264 \u03b4/12, then the leading eigenvectors v\u0302i1 of X\u0302i (after choosing the sign appropriately, i.e. \u3008v\u0302i1,v1\u3009 \u2265 0) are all at a distance of at most 1/4 from v1. Moreover, by Lemma 2,\n1\nm\nm \u2211\ni=1\n\u2225 \u2225\n\u2225v\u0302 i 1 \u2212 v1 \u2212 (\u03bb1I\u2212X)\u2020(X\u0302i \u2212X)v1\n\u2225 \u2225 \u2225 \u2264 c \u03b42 \u00b7 1 m\nm \u2211\ni=1\n\u2016X\u0302i \u2212X\u20162.\nBy the triangle inequality, this implies\n\u2225 \u2225 \u2225 \u2225 \u2225 1 m m \u2211\ni=1\nv\u0302i1 \u2212 v1 \u2212 (\u03bb1I\u2212X)\u2020 ( 1\nm\nm \u2211\ni=1\n(X\u0302i \u2212X) ) v1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 c \u03b42 \u00b7 1 m m \u2211\ni=1\n\u2016X\u0302i \u2212X\u20162,\nand therefore (as \u2016v1\u2016 = 1), \u2225\n\u2225 \u2225 \u2225 \u2225 1 m\nm \u2211\ni=1\nv\u0302i1 \u2212 v1 \u2225 \u2225 \u2225 \u2225\n\u2225 \u2264 c \u03b42 \u00b7 1 m\nm \u2211\ni=1\n\u2016X\u0302i \u2212X\u20162 + \u2225 \u2225 \u2225 (\u03bb1I\u2212X)\u2020 \u2225 \u2225 \u2225 \u00b7 \u2225 \u2225 \u2225 \u2225\n\u2225\n1\nm\nm \u2211\ni=1\nX\u0302i \u2212X \u2225 \u2225 \u2225 \u2225\n\u2225\n. (10)\nSince X has an eigengap of \u03b4, it follows that the minimal non-zero eigenvalue of \u03bb1I\u2212X is at least \u03b4, and therefore \u2225 \u2225(\u03bb1I\u2212X)\u2020 \u2225\n\u2225 \u2264 1/\u03b4. As to the other terms, recall that X\u0302i is the average of n i.i.d. matrices with mean X, and 1m \u2211m i=1 X\u0302i is the average of mn such i.i.d. matrices. Thus,\nby a matrix Hoeffding inequality (Theorem 1) and a union bound, it holds with probability at least 1\u2212 p that\n\u2200i, \u2016X\u0302i \u2212X\u2016 \u2264 c1 \u221a b2 log(2dm/p)\nn as well as\n\u2225 \u2225 \u2225 \u2225 \u2225 1 m m \u2211\ni=1\nX\u0302i \u2212X \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2264 c1 \u221a b2 log(2dm/p)\nmn\nfor some constant c1. Combining this with (9) using a union bound, and plugging into (10), it follows that with probability at least 1\u2212 p\u2212 d exp (\n\u2212 \u03b42nc\u2032b2 ) ,\n\u2225 \u2225 \u2225 \u2225 \u2225 1 m m \u2211\ni=1\nv\u0302i1 \u2212 v1 \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2264 cc 2 1b 2 log(2dm/p)\n\u03b42n + c1\n\u221a\nb2 log(2dm/p)\n\u03b42mn .\nSlightly simplifying, the result follows.\nWe can now complete the proof of Theorem 4.\nProof of Thm. 4. The proof is an easy consequence of Lemma 3. Assuming the events in the lemma occur, we have that the leading eigenvalues of X as well as X\u0302i for all i are simple, hence the leading eigenvectors are all unique up to a sign. In particular, let v1 be the eigenvector closest to w\u03031 = w\u03021, with ties broken arbitrarily, so that \u2016w\u03021 \u2212 v1\u2016 \u2264 \u2016w\u03021 + v1\u2016. This implies that w\u03021 = v\u0302 1 1 (where v\u0302 1 1 is as defined in Lemma 3), since otherwise, by the inequality above, we would get \u2016 \u2212 v\u030211 \u2212 v1\u2016 \u2264 \u2016 \u2212 v\u030211 + v1\u2016, which implies in turn \u3008v\u030211,v1\u3009 \u2264 0, contradicting the fact that \u2016v\u030211 \u2212 v1\u2016 = \u221a\n2\u2212 2\u3008v\u03021,v1\u3009 is at most 1/4 by Lemma 3. Having established that w\u03021 = v\u0302 1 1, we note that by Lemma 3 and the triangle inequality, for\nany i > 1,\n\u2016v\u0302i1 \u2212 v\u030211\u2016 \u2264 1\n2 and therefore \u2016v\u0302i1 \u2212 w\u03021\u2016 \u2264\n1 2 .\nAs v\u0302i1, w\u03021 are unit vectors, this implies that \u2016v\u0302i1 \u2212 w\u03021\u2016 < \u2016 \u2212 v\u0302i1 \u2212 w\u03021\u2016. Since for any i, we have w\u0302i \u2208 {\u2212v\u0302i1, v\u0302i1}, with the sign chosen based on which vector is closest to w\u03021, it follows that w\u0302i = v\u0302 i 1 for all i. Applying Lemma 3 with w\u0302i = v\u0302 i 1, we get that with probability at least 1\u2212 p\u2212 d exp ( \u2212\u03b42n/cb2 ) ,\n\u2225 \u2225 \u2225 1\nm\nm \u2211\ni=1\nw\u0302i \u2212 v1 \u2225 \u2225 \u2225 \u2264 c\u2032\n(b2 log(2dm/p)\n\u03b42n +\n\u221a\nb2 log(2dm/p)\n\u03b42mn\n)\n.\nSquaring both sides and using the fact that (x+ y)2 \u2264 2x2 + 2y2, we get that \u2225\n\u2225 \u2225\n1\nm\nm \u2211\ni=1\nw\u0302i \u2212 v1 \u2225 \u2225 \u2225 2 \u2264 2(c\u2032)2\n(b4 log2(2dm/p)\n\u03b44n2 +\nb2 log(2dm/p)\n\u03b42mn\n)\n. (11)\nThis holds with probability at least 1 \u2212 p \u2212 d exp ( \u2212\u03b42n/cb2 )\n. To simplify things a bit, note that we can assume d exp(\u2212\u03b42n/cb2) \u2264 p without loss of generality, since otherwise the bound in the displayed equation above is at least a constant and therefore trivially true (holds with probability 1) if we make the constant c\u2032 sufficiently large. Therefore, we can argue that (11) (with an appropriate c\u2032) holds with probability at least 1\u2212 2p. Absorbing the 2 factor into the p term, slightly increasing c\u2032 appropriately, and simplifying a bit, the result finally follows from the simple observation that\n(v\u22a41 w) 2 =\n1\n2\n( 2\u2212 \u2016w \u2212 v1\u20162 ) \u2265 1 2 ( 2\u2212 2 \u2225 \u2225 \u2225w \u2212 1 m\nm \u2211\ni=1\nw\u0302i\n\u2225 \u2225 \u2225 2 \u2212 2 \u2225 \u2225 \u2225 1\nm\nm \u2211\ni=1\nw\u0302i \u2212 v1 \u2225 \u2225 \u2225 2)\n\u2265 1\u2212 2 \u2225 \u2225\n\u2225\n1\nm\nm \u2211\ni=1\nw\u0302i \u2212 v1 \u2225 \u2225 \u2225 2 ,\nwhere the first inequality follows from the triangle inequality and the inequality (a + b)2 \u2264 2a2 + 2b2, and the second inequality follows since v1 is a unit vector, and by definition, w is the unit vector closest to 1m \u2211m i=1 w\u0302i."}, {"heading": "3.3 Lower Bound for Sign Fixing", "text": "We now show that the result of Theorem 4 is tight up to poly-logarithmic factors and cannot be improved in general:\nTheorem 5. For any \u03b4 \u2208 (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most a universal constant) with eigengap \u03b4 in the covariance matrix, such that for any number of machines m and for per-machine sample size any n sufficiently larger than 1/\u03b42, the aggregated vector v\u03041 = 1 m \u2211m i=1 v\u0302 (i) 1 (even after sign fixing with the population eigenvector v1) satisfies\nE\n[\n1\u2212 \u2329 v\u03041\n\u2016v\u03041\u2016 , e1\n\u232a2 ]\n= \u2126\n(\n1\n\u03b42mn +\n1\n\u03b44n2\n)\nThe proof is given in the appendix."}, {"heading": "4 A Multi-round Algorithm based on Shift-and-Invert Itera-", "text": "tions\nIn this section we move on to consider distributed algorithms that perform multiple communication rounds. The main motivation, beyond improving some poly-logarithmic factors in the estimation error, is to obtain a result that does not require the per-machine sample size n to grow with the number of machines m, as in the result of Theorem 4.\nTowards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.e., find a unit vector that is an approximate solution to maxv:\u2016v\u2016=1 v \u22a4X\u0302v.\nThroughout this section we let \u03bb\u03021, \u03b4\u0302 denote the leading eigenvalue and eigengap of X\u0302, respectively. Also, we assume without loss of generality that b = 1 (i.e., all data points lie in the unit Euclidean ball).\nSince our approach is to approximate the population risk by approximating the empirical risk, we state the following simple lemma for completeness (a proof is given in the appendix).\nLemma 4 (Risk of approximated-ERM for PCA). Let w be a unit vector such that (w\u22a4v\u03021)2 \u2265 1 \u2212 \u01eb, for some fixed \u01eb > 0, where v\u03021 is the leading eigenvector of X\u0302. Then it holds that 1\u2212 (w\u22a4v1)2 \u2264 1\u2212 (w\u22a4v\u03021)2 + \u221a 2\u01eb."}, {"heading": "4.1 The Shift-and-Invert meta-algorithm", "text": "The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X\u0302 to that of approximately-solving a polylogarithmic number of linear systems, i.e., finding approximate minimizers of convex quadratic optimization problems of the form\nmin z\u2208Rd\n{F\u03bb,w(z) := 1 2 z\u22a4(\u03bbI \u2212 X\u0302)z\u2212 z\u22a4w}, (12)\nwhere \u03bb > \u03bb1(X\u0302) is a shifting parameter. The algorithm is essentially based on applying power iterations to a shifted and inverted matrix (\u03bbI\u2212X\u0302)\u22121, where the shifting parameter \u03bb is carefully chosen. The algorithm that implements this reduction, originally described in [6], is given below (see Algorithm 1).\nAlgorithm 1 Shift-and-Invert Power Method\n1: Input: estimate \u03b4\u0303 for the gap \u03b4\u0302, accuracy \u01eb \u2208 (0, 1), failure probability p 2: Set: m1 \u2190 \u23088 ln ( 144d/p2 ) \u2309,m2 \u2190 \u230832 ln ( 18d p2\u01eb ) \u2309 3: Set: \u01eb\u0303 \u2190 min {\n1 16\n( \u03b4\u0303/8 )m1+1\n, \u01eb4\n( \u03b4\u0303/8 )m2+1 }\n4: Set: \u03bb(0) \u2190 1 + \u03b4\u0303 , w\u03020 \u2190 random unit vector, s \u2190 0 5: repeat 6: s \u2190 s+ 1 , Ms \u2190 (\u03bb(s\u22121)I\u2212 X\u0302) 7: for t = 1...m1 do 8: Find an approximate minimizer - w\u0302t of F\u03bb(s\u22121),w\u0302t\u22121(z) such that \u2016w\u0302t \u2212M\u22121s w\u0302t\u22121\u2016 \u2264 \u01eb\u0303 9: end for\n10: ws \u2190 w\u0302m1/\u2016w\u0302m1\u2016 11: Find an approximate minimizer - vs of F\u03bb(s\u22121),ws(z) such that \u2016vs \u2212M\u22121s ws\u2016 \u2264 \u01eb\u0303 12: \u2206s \u2190 12 \u00b7 1w\u22a4s vs\u2212\u01eb\u0303 , \u03bb(s) \u2190 \u03bb(s\u22121) \u2212 \u2206s 2 13: until \u2206s \u2264 \u03b4\u0303 14: \u03bb(f) \u2190 \u03bb(s) , Mf \u2190 (\u03bb(f)I\u2212 X\u0302) 15: for t = 1...m2 do 16: Find an approximate minimizer - w\u0302t of F\u03bb(f),w\u0302t\u22121(z) such that \u2016w\u0302t \u2212M\u22121f w\u0302t\u22121\u2016 \u2264 \u01eb\u0303 17: end for 18: Return: wf \u2190 w\u0302m2/\u2016w\u0302m2\u2016\nLemma 5 (Efficient reduction of top eigenvector to convex optimization; originally Theorem 4.2 in [6]). Suppose that \u03b4\u0302 := \u03bb1(X\u0302)\u2212 \u03bb2(X\u0302) > 0 and suppose that the estimate \u03b4\u0303 in Algorithm 1 satisfies \u03b4\u0303 \u2208 [\u03b4\u0302/2, 3\u03b4\u0302/4]. Then, with probability at least 1\u2212 p, Algorithm 1 finds a unit vector wf such that (w \u22a4 f v\u03021)\n2 \u2265 1 \u2212 \u01eb, and the total number of optimization problems of the form (12) solved during the run of the algorithm, is upper bounded by O ( ln(d/p) ln(\u03b4\u0302\u22121) + ln (\nd p\u01eb\n))\n.\nMoreover, throughout the run of the algorithm it holds that 1 + \u03b4\u0302 \u2265 \u03bb(s) \u2212 \u03bb\u03021 = \u2126(\u03b4\u0302).\nRemark: the purpose of the repeat-until loop in Algorithm 1 is to efficiently find a shifting parameter \u03bb(f) such that c1\u03b4\u0302 \u2264 \u03bb(f)\u2212 \u03bb\u03021 \u2264 c2\u03b4\u0302 for some universal constants c2 > c1 > 0. When n satisfies n = \u2126(\u03b4\u22122 ln(d/p)), it follows that we can directly find (with high probability) such a shifting parameter, by simply estimating \u03bb\u03021, \u03b4\u0302 from the data of a single machine, without any communication overhead. Also, in this regime, instead of taking the vector w\u03020 to be arbitrary, we can take it to be the leading eigenvector of any single machine, since this will already have a constant correlation with v\u03021 (with high probability). Thus, for such n, the total number of optimization problems can be reduced to O(ln(p\u22121\u01eb\u22121)).\nAlgorithm 1 is a meta-algorithm in the sense that the choice of solver for the optimization problems minF\u03bb,w is unspecified, and any solver will do. A simple calculation shows that a naive application of either the conjugate gradient method or Nesterov\u2019s accelerated gradient method to solve these optimization problems in a distributed manner, i.e., the computation of the gradient vector is distributed across machines, will require overall O\u0303 ( \u221a \u03bb\u03021/\u03b4\u0302 )\ncommunication rounds, which does not give any improvement over the distributed Lanczos approach, described in Subsection 2.2.2. However, this can be substantially improved by taking advantage of the fact that the data on all machines is sampled i.i.d. from the same distribution. In particular, we present below an approach based on applying a pre-conditioner to the optimization Problem (12), in the spirit of the one described in [27]."}, {"heading": "4.2 Faster Distributed Approximation of Linear Systems via Local Preconditioning", "text": "Let M = \u03bbI \u2212 X\u0302, for some shift parameter \u03bb > \u03bb\u03021, and define the pre-conditioning matrix C = (\u03bb + \u00b5)I \u2212 X\u03021, where \u00b5 is required so C is invertible. Consider now solving the following modified quadratic problem:\nF\u0303\u03bb,w(y) := 1 2 y\u22a4C\u22121/2MC\u22121/2y \u2212 y\u22a4C\u22121/2w. (13)\nNote that if y\u2217 is the optimal solution to Problem (13), i.e.,\ny\u2217 = C1/2M\u22121C1/2C\u22121/2w = C1/2M\u22121w,\nthen z\u2217 := C\u22121/2y\u2217 is the optimal solution to Problem (12). The idea behind choosing C this way is very intuitive. Ideally we could have chosen C = M, making the condition number of F\u0303\u03bb,w equal to \u03ba(F\u0303\u03bb,w) = 1, which is the best we can hope for. The problem of course is that this requires us to explicitly compute M\u22121/2, which is more challenging then just computing the leading eigenvector of X\u0302. The next best thing is thus to choose C based only on the data available on any single machine, which allows computing C\u22121/2 without additional communication overhead, and leads to the choice described above. The following lemma, rephrased from [27], quantifies exactly how such a choice of C helps in improving the condition number of the new optimization problem, Problem (13). The proof is given in the appendix.\nLemma 6. Suppose that \u00b5 \u2265 \u2016X\u0302\u2212 X\u03021\u2016. Then, F\u0303\u03bb,w(y) is 1-smooth and ( \u03bb\u2212\u03bb\u03021 (\u03bb\u2212\u03bb\u03021)+2\u00b5 ) -strongly convex. In particular, The condition number5 \u03ba (\nF\u0303\u03bb,w\n)\nsatisfies\n\u03ba (\nF\u0303\u03bb,w\n) \u2264 1 + 2\u00b5 \u03bb\u2212 \u03bb1(X\u0302) .\nMoreover, fixing y\u0303 \u2208 Rd, if we let z\u0303 := C\u22121/2y\u0303, then it holds that\n\u2016z\u0303\u2212M\u22121w\u2016 \u2264 (\u03bb\u2212 \u03bb\u03021)\u22121/2\u2016y\u0303 \u2212C1/2M\u22121w\u2016.\nFinally, for any p \u2208 (0, 1), if we set \u00b5 = 4 \u221a ln(d/p)/n, then the above holds with probability at least 1\u2212 p, where this probability depends only on the randomness in X\u03021."}, {"heading": "4.2.1 Solving the pre-conditioned linear systems", "text": "We now discuss the application of gradient-based algorithms for finding an approximate minimizer of the pre-conditioned problem, Problem (13), in our distributed setting. Towards this end we require a distributed implementation for the first-order oracle of F\u0303\u03bb,w(y) (i.e., computation of the value and gradient vector at a queried point).\nA straight-forward implementation of the first-order oracle in our distributed setting is given in Algorithm 2.\nWe have the following lemma, the proof of which is deferred to the appendix.\nLemma 7. Fix some \u03bb > \u03bb1(X\u0302) and w \u2208 Rd, and let 1 \u2265 \u00b5 > 0 be as in Lemma 6. Fix \u01eb > 0. Consider the following two-step algorithm:\n1. Apply either the conjugate gradient method or Nesterov\u2019s accelerated method with the distributed first-order oracle described in Algorithm 2 to find y\u0303 \u2208 Rd such that F\u0303\u03bb,w(y\u0303)\u2212 miny\u2208Rd F\u0303\u03bb,w(y) \u2264 \u01eb\u2032\n5defined as the smoothness parameter divided by the strong-convexity parameter.\nAlgorithm 2 Distributed First-Order Oracle for F\u0303\u03bb,w(y)\n1: Input: shift parameter \u03bb > 0, regularization parameter \u00b5 > 0, vector w \u2208 Rd, query vector y \u2208 Rd 2: send y\u0303 := C\u22121/2y to machines {2, . . . ,m} for C := (\u03bb+ \u00b5)I\u2212 X\u03021 {executed on machine 1} 3: for i = 1...m do 4: send \u2207\u0303i := X\u0302iy\u0303 to machine 1 {executed on each machine i} 5: end for 6: aggregate \u2207\u0303 := 1m \u2211m\ni=1 \u2207\u0303i {executed on machine 1} 7: compute F\u0303\u03bb,w(y) = 1 2(\u03bby\n\u22a4C\u22121y \u2212 y\u22a4C\u22121/2\u2207\u0303)\u2212 y\u22a4C\u22121/2w {executed on machine 1} 8: compute \u2207F\u0303\u03bb,w(y) = \u03bbC\u22121y\u2212C\u22121/2\u2207\u0303 \u2212C\u22121/2w {executed on machine 1} 9: return: (F\u0303\u03bb,w(y),\u2207F\u0303\u03bb,w(y))\n2. Return z\u0303 = C\u22121/2y\u0303.\nThen, for \u01eb\u2032 = \u01eb2\n(\n1 + 2\u00b5 \u03bb\u2212\u03bb\u03021\n)\u22121 (\u03bb \u2212 \u03bb\u03021) it holds that \u2016z\u0303\u2212 (\u03bbI\u2212 X\u03021)\u22121w\u2016 \u2264 \u01eb, and the total\nnumber distributed matrix-vector products with the empirical covariance matrix X\u0302 required to compute z\u0303 is upper-bounded by\nO\n( \u221a\n1 + 2\u00b5(\u03bb\u2212 \u03bb\u03021)\u22121 ln (( 1 + 2\u00b5\n\u03bb\u2212 \u03bb\u03021\n) \u2016w\u2016/[(\u03bb \u2212 \u03bb\u03021)\u01eb] )\n)\n."}, {"heading": "4.3 Putting it all together", "text": "We now state our main result for this section, which is a simple consequence of the previous lemmas. The full proof is given in the appendix.\nTheorem 6. Fix \u01eb \u2208 (0, 1) and p \u2208 (0, 1). Suppose that mn = \u2126(\u03b4\u22122 ln(d/p)). Set \u00b5 = 4 \u221a\nln(3d/p)/n. Applying the Shift-and-Invert algorithm, Algorithm 1, with the parameters \u01eb, p/3, and applying the algorithm in Lemma 7 with the parameter \u00b5, to approximately solve the linear systems, yields with probability at least 1\u2212 p a unit vector wf such that (w\u22a4f v\u03021)2 \u2265 1\u2212 \u01eb, after executing at most\nO\n\n\n\u221a\n\u221a\nln(d/p) \u03b4 \u221a n\n[\nln\n(\nd\np\u01eb2\n)\nln\n(\n\u221a\nln(d/p) \u03b42 \u221a n\n)\n+ ln2 ( d\np\u01eb2\n)\nln\n(\n1\n\u03b4\n)\n]\n\n = O\u0303\n(\u221a\n1\n\u03b4 \u221a n\n)\ndistributed matrix-vector products with the empirical covariance matrix X\u0302.\nRemark: Our approach of using Shift-and-Invert with the preconditioning technique for linear systems is applicable in a much more general setting. Namely, all that is required for the method to obtain accelerated rates over standard algorithms, is (1) a non-zero gap in the aggregated empirical matrix, i.e., \u03b4(X\u0302) > 0, and (2) that the distance \u2016X\u0302\u2212 X\u03021\u2016 admits a non-trivial upper-bound."}, {"heading": "5 Experiments", "text": "To validate some of our theoretical findings we conducted experiments with single-round algorithms on synthetic data. We generated synthetic datasets using two distributions. For both distributions we used the covariance matrix X = U\u03a3U\u22a4 with U being a random d \u00d7 d orthonormal matrix and \u03a3 is diagonal satisfying: \u03a3(1, 1) = 1, \u03a3(2, 2) = 0.8, \u2200j \u2265 3 : \u03a3(j, j) =\n0.9 \u00b7 \u03a3(j \u2212 1, j \u2212 1), i.e., \u03b4 = 0.2. One dataset was generated according to the normal distributions N (0,X), and for the second datasets we generated samples by taking x = \u221a\n3/2X1/2y where y \u223c U [\u22121, 1]. In both cases we set d = 300.\nBeyond the single-round algorithms that are based on aggregating the individual ERM solutions described so far, we propose an additional natural aggregation approach, based on aggregating the individual projection matrices. More concretely, letting {v\u0302(i)1 }mi=1 denote the leading eigenvectors of the individual machines, let P\u03041 := 1 m \u2211m i=1 v\u0302 (i) 1 v\u0302 (i)\u22a4 1 . We then take the final estimate w to be the leading eigenvector of the aggregated matrix P\u03041. Note that as with the sign-fixing based aggregation, this approach also resolves the sign-ambiguity in the estimates produced by the different machines, which circumvents the lower bound result of Theorem 3.\nFor both datasets we fixed the number of machines to m = 25. We tested the estimation error (i.e., the value 1\u2212(w\u22a4v1)2 where v1 is the leading eigenvector of X andw is the estimator) of five benchmarks vs. the per-machine sample size n: the centralized solution v\u03021, the average of the individual (unbiased) ERM solutions (normalized to unit norm),the average of ERM solutions with sign-fixing, and the leading eigenvector of the averaged projection matrix. We also plotted the average loss of the individual ERM solutions. Results are averaged over 400 independent runs.\nThe results appear in Figure 1. It is observable that the results for both distributions are very similar. We can see that, as our lower bound in Theorem 3 suggests, simply averaging and normalizing the individual ERM solutions has significantly worse performance than the centralized ERM solution. Perhaps surprisingly, the performance of this estimator is even worse than the average error of an estimate computed using only a single machine. We see that both aggregation methods that are based on correlating the individual ERM solutions, namely the sign-fixing-based estimator, and the proposed averaging-of-projections heuristic, are asymptotically consistent with the centralized ERM. In particular, the averaging-of-projections scheme, at least empirically, significantly outperforms the sign-fixing approach, which justifies further theoretical investigation of this heuristic. For the sign fixing approach, we can see that as suggested by our bounds, the estimator is not consistent with the centralized ERM solution for small values of n."}, {"heading": "A Proofs Omitted from Section 3", "text": "A.1 Proof of Theorem 3\nProof. Consider the following distribution over R2.\nx = e1 +\n(\n\u01eb1 \u01eb2\n)\n, \u01eb1, \u01eb2 \u223c U{\u22121,+1},\nwhere e1 is the first standard basis vector in R 2.\nThe population covariance matrix and the empirical covariance matrix of a sample of size n are clearly given by\nX =\n(\n2 0 0 1\n)\n, X\u0302(n) =\n(\n2 yn yn 1\n)\n,\nwhere yn is a random variable which is the average of n U{\u22121,+1} random variables. By elementary calculations we have that the leading eigenvector of X\u0302(n) is given by\nv\u03021 = \u03c3 \u00b7 C(yn) \u00b7 ( 1, 2yn\n1 + \u221a\n1 + 4y2n\n)\n,\nwhere\nC(yn) :=\n\n1 +\n(\n2yn\n1 + \u221a\n1 + 4y2n\n)2 \n\n\u22121/2\nis the normalization factor that guarantees that v\u03021 is a unit vector. In particular, it holds that 1/ \u221a 2 \u2264 C(yn) \u2264 1. The random variable \u03c3 \u223c U{\u22121,+1} is independent of yn and determines the sign of v\u03021, which follows from our assumption that v\u03021 is generated by unbiased ERM.\nConsider now the average of m such unit vectors v\u0302 (1) 1 ..v\u0302 (m) 1 given by v\u0304 = 1 m \u2211m i=1 v\u0302 (i) 1 and the normalized estimate v\u03041/\u2016v\u03041\u2016, and recall that the leading eigenvector of the population covariance matrix is e1. It holds that\n\u3008 v\u03041\u2016v\u03041\u2016 , e1\u30092 =\nv\u03041(1) 2\nv\u03041(1)2 + v\u03041(2)2 = 1\u2212 v\u03041(2)\n2\nv\u03041(1)2 + v\u03041(2)2 . (14)\nTowards upper-bounding the RHS of (14) in expectation, the main step is to lower bound the random variable |v\u03041(2)| using Chebyshev\u2019s inequality.\nIt holds that\nE[|v\u03041(2)|] = E [\u2223 \u2223 \u2223\n\u2223\n1\nm v\u0302 (i) 1 (2)\n\u2223 \u2223 \u2223 \u2223 ] = E\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 m m \u2211\ni=1\n\u03c3(i) 2C(y\n(i) n )y (i) n\n1 +\n\u221a\n1 + 4y (i)2 n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223  \n= (a) E\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 m m \u2211\ni=1\n\u03c3(i) 2C(y\n(i) n )|y(i)n |\n1 +\n\u221a\n1 + 4y (i)2 n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223  \n= E{\u03c3(i)}\n\nE{y(i)n }\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 m m \u2211\ni=1\n\u03c3(i) 2C(y\n(i) n )|y(i)n |\n1 +\n\u221a\n1 + 4y (i)2 n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 | {\u03c3(i)}    \n\u2265 (b) E{\u03c3(i)}\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 E{y(i)n }   1 m m \u2211\ni=1\n\u03c3(i) 2C(y\n(i) n )|y(i)n |\n1 +\n\u221a\n1 + 4y (i)2 n\n| {\u03c3(i)}\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223  \n= (c) E{\u03c3(i)}\n[\u2223\n\u2223 \u2223 \u2223 \u2223 1 m\nm \u2211\ni=1\n\u03c3(i)\n\u2223 \u2223 \u2223 \u2223 \u2223 ] \u00b7 Eyn [ 2C(yn)|yn| 1 + \u221a\n1 + 4y2n\n]\n= (d) \u0398\n(\n1\u221a mn\n)\n, (15)\nwhere (a) follows since \u03c3(i)y (i) n \u223c \u03c3(i)|y(i)n | and C(y(i)n )/(1 +\n\u221a\n1 + 4y (i)2 n ) depends only on |y(i)n |,\n(b) follows from the triangle inequality, and (c) follows since {\u03c3(i)}i\u2208[m] and {y(i)n }i\u2208[m] are independent random variables. Finally, it is easy to verify that (d) follows since\n\u2211m i=1 \u03c3 (i)/m is the average of m U{\u22121,+1} random variables and hence its expected absolute value is \u0398(1/ \u221a m). Similarly the expected absolute value of yn is \u0398(1/ \u221a n) and C(yn)/(1 + \u221a\n1 + 4y2n) is lower bounded by a positive constant.\nAlso, observe that\nE[v\u03041(2) 2] = E\n[\n(\n1\nm v\u0302 (i) 1 (2)\n)2 ]\n= 1\nm E[v\u03021(2)\n2] = 1\nm E\n\n\n(\n2C(yn)yn\n1 + \u221a\n1 + 4y2n\n)2 \n\n\u2265 1 2m E[y2n] = \u0398\n(\n1\nmn\n)\n, (16)\nwhere the inequality follows since |yn| \u2264 1 and 1/ \u221a 2 \u2264 C(yn) \u2264 1.\nCombining Eq. (15) and Eq. (16), we have by an application of Chebyshev\u2019s inequality to the random variable |v\u03041(2)| that there exists universal constants c1 > 0 such that\nPr\n(\n|v\u03041(2)| \u2264 1\nc1 \u221a mn\n)\n\u2264 1 4 . (17)\nAlso, it is easy to verify that\nE[v\u03041(1) 2] = O(1/m), E[v\u03041(2) 2] = O(1/m).\nThus, by a simple application of Markov\u2019s inequality we have that there exists a universal constant c2 > 0 such that\nPr\n(\nmax{v\u03041(1)2, v\u03041(2)2} \u2265 1\nc3m\n)\n\u2264 1 4 . (18)\nUsing Eq. (14), (17) and (18) we finally have that\nE\n[\n\u3008 v\u03041\u2016v\u03041\u2016 , e1\u30092\n] = 1\u2212 E [\nv\u03041(2) 2\nv\u03041(1)2 + v\u03041(2)2\n] = 1\u2212\u2126 ( 1\nn\n)\n.\nA.2 Proof of Theorem 5\nThe proof is a combination of the following two lemmas, each proves one of the lower bounds. We first state the two lemmas and then prove them.\nLemma 8. For any \u03b4 \u2208 (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) such that the covariance matrix has eigengap \u03b4, and for any number of machines m and per-machine sample size n, the aggregated vector v\u03041 = 1 m \u2211m i=1 v\u0302 (i) 1 (even after sign fixing) satisfies\nE\n[\n1\u2212 \u3008 v\u03041\u2016v\u03041\u2016 , e1\u30092\n]\n= \u2126\n(\nmin\n{\n1\nm ,\n1\n\u03b42mn\n})\n.\nLemma 9. For any \u03b4 \u2208 (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) with eigengap \u03b4 in the covariance matrix, such that for any number of machines m and for per-machine sample size any n sufficiently larger than 1/\u03b42, the aggregated vector v\u03041 = 1 m \u2211m i=1 v\u0302 (i) 1 (even after sign fixing with the population eigenvector v1) satisfies\nE\n[\n1\u2212 \u3008 v\u03041\u2016v\u03041\u2016 , e1\u30092\n]\n= \u2126\n(\n1\n\u03b44n2\n)\n.\nproof of Lemma 8. We will prove the result for d = 2 (i.e. a distribution in R2). This is without loss of generality, since we can always embed the distribution below in Rd for any d > 2 (say, by having all coordinates other than the first two identically zero).\nConsider the distribution defined by the random vector x = \u221a 1 + \u03b4e1 + \u03c3e2, where \u03c3 is uniformly distributed on {\u22121,+1}, and e1 = (1, 0), e2 = (0, 1) are the standard basis vectors. Clearly, the population covariance matrix is\nX := E[xx\u22a4] =\n(\n1 + \u03b4 0 0 1\n)\n,\nwith a leading eigenvector (1, 0). Let us now consider the distribution of the output of a machine i. Given n samples, the empirical covariance matrix is\nX\u0302(n) =\n(\n1 + \u03b4 yn yn 1\n) , yn := \u221a 1 + \u03b4 \u00b7 1\nn\nn \u2211\ni=1\n\u01ebi,\nwhere \u01ebi are i.i.d. and uniformly distributed on {\u22121,+1}. Using a standard formula for the leading eigenvector of a 2 \u00d7 2 matrix [1], we have that the leading eigenvector (and hence the output of any machine i) is of the form\nv\u03021 = 1 \u2016u\u0302\u2016 u\u0302 where u\u0302 := ( \u03b4 2 + \u221a \u03b42 4 + y2n , yn ) . (19)\nNote that with this formula, the leading eigenvector is always closer to (1, 0) than (\u22121, 0), and converges to (1, 0) as n \u2192 \u221e. Thus, we can view the random variable v\u0302(i) as the output of any machine i, given n samples and after fixing the sign.\nConsider now the average of m such vectors given by v\u0304 = 1m \u2211m i=1 v\u0302 (i) 1 . Using (19), we have\nthat\nE[v\u03041(2) 2] = E\n\n\n(\n1\nm\nm \u2211\ni=1\nv\u0302 (i) 2\n)2 \n = 1\nm2\nm \u2211\ni=1\nE[(v\u0302 (i) 2 )\n2] = 1\nm E[(v\u0302(2))2]\n= 1\nm E\n\n y2n \u03b42\n2 + 2y 2 n + \u03b4\n\u221a\n\u03b42 4 + y 2 n\n\n . (20)\nBy definition of yn and recalling that \u03b4 \u2208 [0, 1], we have that there exist universal constants c1, c2 > 0 such that with constant probability it holds that c1/n \u2265 y2n \u2265 c2/n. Using this fact and considering the two cases 1/n \u2265 \u03b42 and 1/n < \u03b42 in the RHS of Eq. (20) separately, we can see that\nE[v\u03041(2) 2] = \u2126\n(\n1 m min{1, 1 \u03b42n } ) . (21)\nUsing Eq. (21) we have that\nE\n[\n\u3008 v\u03041\u2016v\u03041\u2016 , e1\u30092\n]\n= E\n[\nv\u03041(1) 2\nv\u03041(1)2 + v\u03041(2)2\n] = 1\u2212 E [\nv\u03041(2) 2\nv\u03041(1)2 + v\u03041(2)2\n]\n\u2264 1\u2212 E [ v\u03041(2) 2 ] = 1\u2212 \u2126 ( min { 1\nm ,\n1\n\u03b42mn\n})\n,\nwhere the inequality follows since \u2016v\u03041\u2016 \u2264 1.\nproof of Lemma 9. As in Lemma 8, we prove the result for d = 2, however, using a different construction. Consider the defined by the random vector\nx = \u221a 1 + \u03b4 \u00b7 e1 + \u03be \u00b7 e2,\nwhere \u03be is an independent random variable defined as:\n\u03be =\n{ \u221a 2 w.p. 1/3\n\u22121/ \u221a 2 w.p. 2/3\nIt is easy to verify that E[\u03be] = 0, E[\u03be2] = 1, E[\u03be3] = 1/ \u221a 2. As we shall see, choosing \u03be to be asymmetric (as opposed to \u01eb in the proof of Lemma 9) will be key to our construction. Clearly, the population covariance and the empirical covariance of a sample of size n are given by we have\nX = E[xx\u22a4] =\n(\n1 + \u03b4 0 0 1\n)\n, X\u0302(n) =\n(\n1 + \u03b4 yn yn zn\n)\n,\nwhere\nyn := \u221a 1 + \u03b4 \u00b7 1\nn\nn \u2211\ni=1\n\u03bei , zn := 1\nn\nn \u2211\ni=1\n\u03be2i ,\nwith \u03be1, . . . , \u03ben being i.i.d. copies of the random variable \u03be.\nClearly the leading eigenvector of X is e1 = (1, 0). Consider now v\u0302 (1) 1 , . . . , v\u0302 (m) 1 to be the\nleading eigenvectors of m i.i.d. empirical covariance matrices of n samples, X\u0302 (1) (n), . . . , X\u0302 m) (n), and let v\u03041 denote their average after sign-fixings according to the leading eigenvector of the population covariance e1. In the following, we let v\u0302 i j denote the jth coordinate in the eigenvector v\u0302 (i) 1 . It holds that\nE\n[\n\u3008 v\u03041\u2016v\u03041\u2016 , e1\u30092\n]\n= E\n[\nv\u03041(1) 2\nv\u03041(1)2 + v\u03041(2)2\n] = 1\u2212 E [\nv\u03041(2) 2\nv\u03041(1)2 + v\u03041(2)2\n]\n\u2264 1\u2212 E [ v\u03041(2) 2 ] = 1\u2212 E\n\n\n(\n1\nm\nm \u2211\ni=1\nsign(v\u0302i1)v\u0302 i 2\n)2 \n\n\u2264 1\u2212 ( 1\nm\nm \u2211\ni=1\nE [ sign(v\u0302i1)v\u0302 i 2 ]\n)2\n= 1\u2212 ( E[sign(v\u030211)v\u0302 1 2 ] )2 , (22)\nwhere the first inequality follows since \u2016v\u03041\u2016 \u2264 1, the second inequality follows from Jensen\u2019s inequality, and the last equality follows from the fact that v\u0302\n(1) 1 , . . . , v\u0302 (m) 1 are i.i.d. random vari-\nables. From this chain of inequalities, it follows that it is enough to lower bound ( E[sign(v\u030211)v\u0302 1 2 ] )2 , where v\u03021 is the leading eigenvector computed by machine 1. Let us now consider the distribution of the leading eigenvector of the empirical covariance matrix X\u0302(n). Using a standard formula for the leading eigenvector of a 2\u00d72 matrix [1], we have that this leading eigenvector v\u03021 is proportional to\n\n \u03b4 + 1\u2212 zn 2 +\n\u221a\n( \u03b4 + 1\u2212 zn 2\n)2\n+ y2n , yn\n\n (23)\nAssume for now that zn \u2264 1+c\u03b4 for some positive constant c to be fixed later (note this happens with arbitrarily high probability as n \u2192 \u221e, as zn converges to 1 in probability). In that case, the sign of the first coordinate in the formula above is positive, and has the same sign as the first coordinate of the leading eigenvector v1 = (1, 0). Moreover, we know that v\u0302 (1) 1 must have unit norm, from which follows that\nsign(v\u030211) \u00b7 v\u0302(1)1 =\n(\n\u03b4+1\u2212zn 2 +\n\u221a\n( \u03b4+1\u2212zn 2 )2 + y2n , yn\n)\n\u221a\ny2n +\n(\n\u03b4+1\u2212zn 2 +\n\u221a\n( \u03b4+1\u2212zn 2 )2 + y2n\n)2 . (24)\nIn particular, letting rn = 1\u2212 zn, we have that if rn \u2265 \u2212c\u03b4, then\nsign(v\u030211) \u00b7 v\u030212 = yn \u221a\ny2n +\n(\n\u03b4+rn 2 +\n\u221a\n(\n\u03b4+rn 2 )2 + y2n\n)2\n= yn \u221a\n\u221a \u221a \u221ay2n + ( \u03b4+rn 2 )2\n(\n1 +\n\u221a\n1 + (\n2yn \u03b4+rn\n)2 )2\n. (25)\nTowards using Eq. (22) to derive the lower bound, the main step is to bound the expectation of the RHS of Eq.(25) away from zero. To get an intuition why this is possible, observe that when n \u2192 \u221e (in particular, when it is significantly larger than 1/\u03b42), it holds that\nRHS of (25) \u2248 yn\u221a y2n +\u0398(\u03b4 2) ,\nsince in this regime, with high probability, rn << \u03b4 and yn << 1. Now comes to play our choice of \u03be to be an asymmetric random variable. If, just for sake of intuition, we set n = 1, it is easy to verify that despite the fact that E[yn] = 0, it holds that\nE\n[\nyn \u221a\ny2n +\u0398(\u03b4 2)\n]\n= E\n[\n\u03be \u221a\n\u03be2 +\u0398(\u03b42)\n]\n< 0.\nNote in particular that taking \u03be to be uniformly distributed on {\u22121,+1}, as in Lemma 8, will still give zero expectation, and hence will not work. We now formalize this intuition. We will use a Taylor expansion of the formula above, in order to bound its expectation (over yn, rn), from which a lower bound on ( E [\nsign(v\u030211) \u00b7 v\u030212 ])2 would follow. To that end, define the function\ng(t) = tyn \u221a\n\u221a \u221a \u221a(tyn)2 + ( \u03b4+trn 2 )2\n(\n1 +\n\u221a\n1 + (\n2tyn \u03b4+trn\n)2 )2\n, t \u2208 [0, 1],\nand note that g(1) equals sign(v\u030211) \u00b7 v\u030212 as defined above. By a Taylor expansion, we have\nsign(v\u030211) \u00b7 v\u030212 = g(1) = g(0) + g\u2032(0) + 1 2 g\u2032\u2032(0) +\ns3\n6 g\u2032\u2032\u2032(s)\nfor some s \u2208 [0, 1]. A tedious calculation of g\u2019s derivatives6 reveals that this implies\nsign(v\u030211) \u00b7 v\u030212 = yn \u03b4 \u2212 rnyn \u03b42 \u00b1O ( |yn|3 + |rn|3 \u03b43 ) , (26)\nassuming max{|yn|, |rn|} \u2264 c\u03b4 for some constant c (hence fixing c we used in our earlier assumptions on rn, zn). To simplify notation, let qn = sign(v\u0302 1 1) \u00b7 v\u030212 , let bn = yn\u03b4 \u2212 rnyn \u03b42 \u00b1O ( |yn|3+|rn|3 \u03b43 ) be the expression on the right-hand side of the equation above, and let A be the event that max{|yn|, |rn|} \u2264 c\u03b4 indeed holds. Also, note that with probability 1, |qn| \u2264 1 and |bn| = O(1/\u03b43). Thus, by Eq. (26), we have that E[qn|A] = E[bn|A], and therefore\nE[qn] = Pr(\u00acA) \u00b7 E[qn|\u00acA] + Pr(A) \u00b7 E[qn|A] = Pr(\u00acA) \u00b7 E[qn|\u00acA] + Pr(A) \u00b7 E[bn|A] = Pr(\u00acA) \u00b7 E[qn|\u00acA] + E[bn]\u2212 Pr(\u00acA) \u00b7 E[bn|\u00acA] = E[bn]\u00b1O ( Pr(\u00acA)/\u03b43 ) .\n6Using MATLAB\u2019s symbolic math toolbox together with some straightforward manual calculations\nPlugging back the definitions of qn, bn, A, we get that\nE [ sign(v\u030211) \u00b7 v\u030212 ] = E\n[\nyn \u03b4 \u2212 rnyn \u03b42 \u00b1O ( |yn|3 + |rn|3 \u03b43 )] \u00b1O ( 1 \u03b43 Pr(max{|yn|, |rn|} > c\u03b4) ) .\nRecalling that yn = \u221a 1 + \u03b4 \u00b7 1n \u2211n i=1 \u03bei and rn = 1 \u2212 zn = 1 \u2212 1n \u2211n i=1 \u03be 2 i , where \u03bei are i.i.d.\ncopies of a zero-mean, bounded random variable satisfying E[\u03be3] = 1/ \u221a 2, and using Hoeffding\u2019s inequality, it is easily verified that the above equals\n0 + \u221a 1 + \u03b4\n1\u221a 2\u03b42n \u00b1O (\n1\n(\u03b42n)3/2\n) \u00b1O ( 1\n\u03b43 exp(\u2212\u2126(n\u03b42))\n)\n,\nwhich is \u2126 ( 1 \u03b42n ) assuming n is sufficiently larger than 1/\u03b42. As a result, we get that ( E [ sign(v\u030211) \u00b7 v\u030212 ])2\n= \u2126 (\n1 \u03b44n2\n)\nas required."}, {"heading": "B Proofs Omitted from Section 4", "text": "B.1 Proof of Lemma 4\nProof. Let \u2022 denote the standard inner product for matrices, i.e., A \u2022B = Tr(AB\u22a4). It holds that\n(w\u22a4v1) 2 = ww\u22a4 \u2022 v1v\u22a41 \u2265 v\u03021v\u0302\u22a41 \u2022 v1v\u22a41 \u2212 \u2016ww\u22a4 \u2212 v\u03021v\u0302\u22a41 \u2016F \u00b7 \u2016v1v\u22a41 \u2016\n= (w\u22a4v1) 2 \u2212\n\u221a 2(1\u2212 1(w\u22a4v\u03021)2) \u2265 (w\u22a4v1)2 \u2212 \u221a 2\u01eb.\nB.2 Proof of Lemma 6\nProof. Observe that C = M+ (X\u0302\u2212 X\u03021) + \u00b5I. Thus, by our assumption on \u00b5 it follows that\nM+ 2\u00b5I C M. (27)\nSince F\u0303\u03bb,w(y) is twice differentiable, in order to bound its smoothness and strong-convexity parameters, it suffices to upper bound the largest eigenvalue and lower bound the smallest eigenvalue of its Hessian, respectively.\nThe Hessian of F\u0303\u03bb,w(y) is given by \u22072F\u0303\u03bb,w(y) = C\u22121/2MC\u22121/2. From Eq. (27) it follows that we can write M = C\u2212\u2206 where \u2206 0. Thus we have that\n\u03bb1(C \u22121/2MC\u22121/2) = \u03bb1(C \u22121/2(C\u2212\u2206)C\u22121/2) \u2264 \u03bb1(I) = 1, (28)\nwhere the inequality follows since C\u22121/2\u2206C\u22121/2 is positive semidefinite. Since M,C are invertible and positive definite, Eq. (27) implies that\nM\u22121 C\u22121 (M+ 2\u00b5I)\u22121. (29)\nThus we have that\n\u03bbd(C \u22121/2MC\u22121/2) = \u03bbd(M 1/2C\u22121/2C\u22121/2MC\u22121/2C1/2M\u22121/2) = \u03bbd(M 1/2C\u22121M1/2)\n\u2265 \u03bbd(M1/2(M+ 2\u00b5I)\u22121M1/2) = min i\u2208[d] { \u03bbi(M) \u03bbi(M) + 2\u00b5 }\n= \u03bbd(M)\n\u03bbd(M) + 2\u00b5 = \u03bb\u2212 \u03bb\u03021 (\u03bb\u2212 \u03bb\u03021) + 2\u00b5 , (30)\nwhere the first equality follows from matrix similarity and the fact that M,C are invertible, and the first inequality follows from Eq. (29).\nTo prove the second part of the lemma we observe that\n\u2016z\u0303\u2212M\u22121w\u2016 = \u2016C\u22121/2y\u0303 \u2212C\u22121/2C1/2M\u22121w\u2016 \u2264 \u2016C\u22121/2\u2016 \u00b7 \u2016y\u0303 \u2212C1/2M\u22121w\u2016 \u2264 1\u221a\n\u03bb\u2212 \u03bb1(X\u0302) \u2016y\u0303 \u2212C1/2M\u22121w\u2016,\nwhere the second inequality follows from Eq. (29). Finally, the last part of the lemma follows from a direct application of Theorem 1 to upper bound \u2016X\u2212 X\u03021\u2016.\nB.3 Proof of Lemma 7\nProof. Let z\u2217 := (\u03bbI\u2212 X\u0302)\u22121w,y\u2217 := C1/2(\u03bbI\u2212 X\u0302)\u22121w, and recall that z\u2217 and y\u2217 are the global minimizers of F\u03bb,w(z) and F\u0303\u03bb,w(y), respectively. Using the results of Lemma 6 we have that\n\u2016z\u0303\u2212 z\u2217\u2016 \u2264 (\u03bb\u2212 \u03bb\u03021)\u22121/2\u2016y\u0303 \u2212 y\u2217\u2016 \u2264 (\u03bb\u2212 \u03bb\u03021)\u22121/2 \u221a 2 ( 1 + 2\u00b5\n\u03bb\u2212 \u03bb\u03021\n)\n\u01eb\u2032,\nwhere the second inequality follows from the strong-convexity of F\u0303\u03bb,w(y). Thus, it suffices to set \u01eb\u2032 as stated in the lemma in order to obtain the approximation guarantee for z\u0303.\nTo upper-bound the total number of communication rounds required to obtain y\u0303 with the guarantee prescribed in the lemma, we note that both the conjugate gradient method and Nesterov\u2019s accelerated gradient method require\nO\n( \u221a\n\u03b2 \u03b1 ln ( \u2016y\u2217\u2016/\u01eb\u2032 )\n)\n(31)\ncalls to the first-order oracle of F\u0303\u03bb,w(y) to obtain y\u0303 satisfying F\u0303\u03bb,w(y\u0303)\u2212miny\u2208Rd F\u0303\u03bb,w(y) \u2264 \u01eb\u2032, where \u03b1 and \u03b2 are the strong-convexity and smoothness parameters of F\u0303\u03bb,w, respectively, and assuming w.l.o.g. that the initial iterate is y0 = ~0. Thus, by our construction of a distributed first-order oracle given in Algorithm 2, we have that the total number of communication rounds is upper bounded by (31). The lemma now follows from noticing that by Lemma 6 we have that \u03b2/\u03b1 = 1 + 2\u00b5\n\u03bb\u2212\u03bb\u03021 and that\n\u2016y\u2217\u2016 = \u2016C1/2(\u03bbI\u2212 X\u03021)w\u2016 \u2264 \u03bb1(C1/2)(\u03bb\u2212 \u03bb\u03021)\u22121\u2016w\u2016 = O ( \u2016w\u2016/(\u03bb \u2212 \u03bb\u03021) ) .\nB.4 Proof of Theorem 6\nProof. Under our assumption that mn = \u2126(\u03b4\u22122 ln(d/p)), the following three events all hold with probability at least 1\u2212 p (each of which holds w.p. at least 1\u2212 p/3):\n1. the output wf satisfies (w \u22a4 f v\u03021) 2 \u2265 1 \u2212 \u01eb (holds w.p. 1\u2212 p/3 by applying Lemma 5 with our choice of parameters)\n2. \u03b4\u0302 = \u0398(\u03b4) (by applying Theorem 1)\n3. \u2016X\u0302\u2212 X\u03021\u2016 \u2264 \u00b5, where \u00b5 is as prescribed in the Theorem (by applying Theorem 1)\nThe approximation guarantee of wf follows directly from Lemma 5. It thus remains to upperbound the number of matrix-vector products. Thus, combining Lemmas 5 and 7 we have that when using either the conjugate gradient method or Nesterov\u2019s accelerated method to approximately solve the linear systems in Algorithm 1, as prescribed in Lemma 7, the total number of distributed matrix-vector products with X\u0302 is:\nO\n(\nln\n(\nd\np\u01eb\n) \u00b7 ( \u221a 1 + 2\u00b5\n\u03b4\n(\nln \u03b4\u22121 ln\n(\nd\np\u01eb\n)\n+ ln\n(\n(1 + 2\u00b5/\u03b4)\n\u03b4\u01eb\u0303\n))\n))\n=\nO\n(\n\u221a\n1 + 2\u00b5\n\u03b4\n( ln \u03b4\u22121 ln2 ( d\np\u01eb\n)\n+ ln\n(\nd\np\u01eb\n)(\nln\n(\n(1 + 2\u00b5/\u03b4)\n\u03b4\n)\n+ ln\n(\n1\n\u01eb\u0303\n)))\n)\n=\nO\n(\n\u221a\n1 + 2\u00b5\n\u03b4\n( ln \u03b4\u22121 ln2 ( d\np\u01eb\n)\n+ ln\n(\nd\np\u01eb\n)\nln\n(\n(1 + 2\u00b5/\u03b4)\n\u03b4\n) + ln2 ( d\np\u01eb\n)\nln\n(\n1\n\u03b4\n))\n)\n,\nwhere the first term in the O(\u00b7) in the first row accounts for the total number of instances of F\u03bb,w(z) needs to be solved, given by the bound in Lemma 5, and the second term in the first row accounts for the communication-complexity of solving each such instance according to Lemma 7. Additionally, we have used Lemma 5 to lower bound \u03bb \u2212 \u03bb\u03021 = \u2126(\u03b4\u0302), and \u01eb\u0303(\u01eb) is as prescribed in Algorithm 1. Finally, we have upper-bounded ln(\u2016w\u2016), in all instances of F\u03bb,w(z) solved throughout the run of the algorithm, by noticing that in all of them it holds that\nln(\u2016w\u2016) = O ( ln ( \u03bb(s) \u2212 \u03bb\u03021)\u2212max{m1,m2} )) = O\n(\nln \u03b4\u22121 ln\n(\nd\np\u01eb\n))\n,\nwhere m1,m2 are as prescribed in Algorithm 1, and we have used Lemma 5 again to lower bound \u03bb(s) \u2212 \u03bb\u03021 = \u2126(\u03b4).\nFinally, using Lemma 6, we can set \u00b5 = 4 \u221a\nln(3d/p)\u221a n . Thus, the overall number of communi-\ncation rounds is upper-bound by\nO\n\n\n\u221a\n\u221a\nln(d/p) \u03b4 \u221a n\n(\nln\n(\nd\np\u01eb2\n)\nln\n(\n\u221a\nln(d/p) \u03b42 \u221a n\n)\n+ ln2 ( d\np\u01eb2\n)\nln\n(\n1\n\u03b4\n)\n)\n\n ."}, {"heading": "C Proof of the Davis-Kahan sin\u03b8 Theorem", "text": "We prove Theorem 2 in greater generality. In particular, Theorem 2 follows from setting k = 1 in the next theorem.\nTheorem 7 (Davis-Kahan sin\u03b8 theorem). Let X,Y be symmetric real d \u00d7 d matrices and fix k \u2208 [d]. Let VX and VY denote d \u00d7 k matrix whose columns are the top k eigenvectors of X and the matrix whose columns are the top k eigenvectors of Y, respectively. Also, suppose that \u03b4k(X) := \u03bbk(X)\u2212 \u03bbk+1(X) > 0. Then it holds that\n\u2016VXV\u22a4X \u2212VYV\u22a4Y\u2016F \u2264 2 \u2016X \u2212Y\u2016 \u03b4k(X) .\nProof. Throughout the proof we denote the projection matrices:\nPX := VXV \u22a4 X, P \u22a5 X := I\u2212VXV\u22a4X, PY := VYV\u22a4Y, P\u22a5Y := I\u2212VYV\u22a4Y,\ni.e., PX is the projection matrix onto the top k eigenvectors of X and P \u22a5 X is the projection matrix onto the lower d\u2212 k eigenvectors, and same goes for PY,P\u22a5Y. We also let A \u2022B denote the standard inner products between matrices A,B.\nWe can write PY as\nPY = PXPYPX +P \u22a5 XPYPX +PXPYP \u22a5 X +P \u22a5 XPYP \u22a5 X. (32)\nObserve that\nPXPYP \u22a5 X \u2022X = Tr\n(\nPXPYP \u22a5 XX\n) = Tr ( PYP \u22a5 XXPX ) = 0, (33)\nwhere the second equality follows from the cyclic property of the trace, and the last equality follows since P\u22a5XXPX = 0d\u00d7d. Using Eq. (32) and (33) we have that\nPY \u2022X = PXPYPX \u2022X+P\u22a5XPYP\u22a5X \u2022X = Tr (PXPYPXX) + Tr ( P\u22a5XPYP \u22a5 XX )\n= Tr (PYPXX) + Tr ( P\u22a5XPYP \u22a5 XP \u22a5 XX ) \u2264 Tr (PYPXX) + Tr ( P\u22a5XPYP \u22a5 X ) \u00b7 \u03bb1(P\u22a5XX) = Tr (PYPXX) + \u03bbk+1(X) \u00b7 Tr ( P\u22a5XPY ) , (34)\nwhere the inequality follows since for any two positive semidefinite matrices A,B it holds that Tr(AB) \u2264 Tr(A) \u00b7 \u03bb1(B) and the fact that P\u22a5XX is positive semidefinite. The last equality follows since \u03bb1(P \u22a5 XX) = \u03bbk+1(X). It further holds that\nPY \u2022Y \u2265 PX \u2022Y = Tr(PXX) +PX \u2022 (Y \u2212X). (35)\nSubtracting Eq. (35) from Eq. (34) we have that\nTr (PYPXX) + \u03bbk+1(X) \u00b7 Tr ( P\u22a5XPY ) \u2212 Tr(PXX)\u2212PX \u2022 (Y \u2212X) \u2265 PY \u2022X\u2212PY \u2022Y.\nRearranging we have that\nTr ((I \u2212PY)PXX)\u2212 \u03bbk+1(X) \u00b7 Tr ( P\u22a5XPY )\n\u2264 (Y \u2212X) \u2022 (PY \u2212PX) \u2264 \u2016X\u2212Y\u2016 \u00b7 \u2016PX \u2212PY\u2016F . (36)\nIt holds that\nTr ((I \u2212PY)PXX) = Tr (PX(I\u2212PY)PXPXX) \u2265 Tr (PX(I\u2212PY)PX) \u00b7 \u03bbk(PXX) = Tr (PX \u2212PYPX)) \u00b7 \u03bbk(X) = (k \u2212PX \u2022PY) \u00b7 \u03bbk(X)\n= \u03bbk(X)\n2 \u2016PX \u2212PY\u20162F . (37)\nFurthermore, it holds that\nTr ( P\u22a5XPY ) = Tr ((I\u2212PX)PY) = k \u2212PX \u2022PY = 1\n2 \u2016PX \u2212PY\u20162F . (38)\nPlugging Eq. (37) and (38) into Eq. (36), we have that\n1 2 \u2016PX \u2212PY\u20162F \u00b7 (\u03bbk(X)\u2212 \u03bbk+1(X)) \u2264 \u2016X\u2212Y\u2016 \u00b7 \u2016PX \u2212PY\u2016F , (39)\nwhich completes the proof."}], "references": [{"title": "Even faster SVD decomposition yet without agonizing pain", "author": ["Zeyuan Allen Zhu", "Yuanzhi Li"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Fast global convergence of online", "author": ["Zeyuan Allen Zhu", "Yuanzhi Li"], "venue": "PCA. CoRR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The fast convergence of incremental PCA", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Optimal principal component analysis in distributed and streaming models", "author": ["Christos Boutsidis", "David P Woodruff", "Peilin Zhong"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Fast and simple pca via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "arXiv preprint arXiv:1509.05647,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Faster eigenvector computation via shift-and-invert", "author": ["Dan Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "preconditioning. CoRR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate", "author": ["Gene H Golub", "Victor Pereyra"], "venue": "SIAM Journal on numerical analysis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1973}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "J. Educ. Psych., 24", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1933}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Tak\u00e1c", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1602.06929,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1602.06929,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Principal component analysis", "author": ["IT Jolliffe"], "venue": "2002. Spring-verlag, New York", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Distributed stochastic variance reduced gradient methods", "author": ["Jason D. Lee", "Tengyu Ma", "Qihang Lin"], "venue": "CoRR, abs/1507.07595,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Improved distributed principal component analysis", "author": ["Yingyu Liang", "Maria-Florina F Balcan", "Vandana Kanchanapally", "David Woodruff"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "On differentiating eigenvalues and eigenvectors", "author": ["Jan R Magnus"], "venue": "Econometric Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine, 2(6):559\u2013572", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1901}, {"title": "AIDE: fast and communication efficient distributed optimization", "author": ["Sashank J. Reddi", "Jakub Konecn\u00fd", "Peter Richt\u00e1rik", "Barnab\u00e1s P\u00f3czos", "Alexander J. Smola"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Convergence of stochastic gradient descent for PCA", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Fast stochastic algorithms for svd and pca: Convergence properties and convexity", "author": ["Ohad Shamir"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Without-replacement sampling for stochastic gradient methods", "author": ["Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Samworth. A useful variant of the davis\u2013kahan theorem for statisticians", "author": ["Yi Yu", "Tengyao Wang", "Richard J"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Yuchen Zhang", "John C Duchi", "Martin J Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.", "startOffset": 50, "endOffset": 61}, {"referenceID": 7, "context": "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.", "startOffset": 50, "endOffset": 61}, {"referenceID": 11, "context": "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.", "startOffset": 50, "endOffset": 61}, {"referenceID": 2, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 18, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 9, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 1, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 17, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 19, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 4, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 5, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 0, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 24, "context": "For instance, [26] proposed communication-efficient algorithms for a distributed statistical estimation settings, similar to ours, but under convexity assumptions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Much like the results of [26], this result only holds in the regime when the per-machine sample size n is sufficiently large.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 12, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 20, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 8, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 16, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 4, "context": "Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7].", "startOffset": 190, "endOffset": 196}, {"referenceID": 5, "context": "Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7].", "startOffset": 190, "endOffset": 196}, {"referenceID": 13, "context": "Beyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits.", "startOffset": 37, "endOffset": 44}, {"referenceID": 3, "context": "Beyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits.", "startOffset": 37, "endOffset": 44}, {"referenceID": 22, "context": "(3) Lemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the Davis-Kahan sin(\u03b8) theorem (whose proof is given in the appendix for completeness): Theorem 1 (Matrix Hoeffding, see [24]).", "startOffset": 231, "endOffset": 235}, {"referenceID": 4, "context": "A multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem.", "startOffset": 149, "endOffset": 155}, {"referenceID": 5, "context": "A multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem.", "startOffset": 149, "endOffset": 155}, {"referenceID": 2, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 18, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 19, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 10, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 1, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 10, "context": "For instance applying the result of [12] in this way will result in a final estimate w satisfying 1\u2212 (wv1) = O ( b2 ln d \u03b42mn )", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "For instance, in the distributed statistical setting considered in [26], in which the objective is strongly convex, it was shown that simply averaging the individual ERM solutions leads, in a meaningful regime of parameters, to estimation error of the order of the centralized ERM solution.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "By Theorem 1 in [16], we have that both \u03bb(t) and v(t) are infinitely differentiable at any t \u2208 [0, 1], and satisfy3 \u03bb\u2032(t) = v(t)\u22a4Ev(t) , v\u2032(t) = (\u03bb(t)I \u2212A(t))\u2020Ev(t) .", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "3 in [8]) \u2212B\u2020 ( \u2202 \u2202t B )", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "Since v(t),v(t),v(t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t 7\u2192 v(t)j , mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in [16], and in particular twice continuously differentiable.", "startOffset": 260, "endOffset": 264}, {"referenceID": 23, "context": "To handle the second part, note that by a variant of the Davis-Kahan sin\u03b8 theorem (see Corollary 1 in [25]), if maxi \u2016X\u0302i \u2212 X\u2016 \u2264 \u03b4/12, then the leading eigenvectors v\u0302i 1 of X\u0302i (after choosing the sign appropriately, i.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Towards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.", "startOffset": 101, "endOffset": 107}, {"referenceID": 5, "context": "Towards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "1 The Shift-and-Invert meta-algorithm The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X\u0302 to that of approximately-solving a polylogarithmic number of linear systems, i.", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "1 The Shift-and-Invert meta-algorithm The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X\u0302 to that of approximately-solving a polylogarithmic number of linear systems, i.", "startOffset": 69, "endOffset": 75}, {"referenceID": 4, "context": "The algorithm that implements this reduction, originally described in [6], is given below (see Algorithm 1).", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "2 in [6]).", "startOffset": 5, "endOffset": 8}], "year": 2017, "abstractText": "We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of m stores a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large n. We also introduce an iterative distributed algorithm that is applicable in any regime of n, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}