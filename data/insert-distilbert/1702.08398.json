{"id": "1702.08398", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "McGan: Mean and Covariance Feature Matching GAN", "abstract": "we introduce new families of integral probability metrics ( ipm ) for training generative adversarial sensor networks ( gan ). our ipms are based together on matching statistics estimates of distributions embedded in a hypothetical finite dimensional feature space. mean and error covariance proportional feature matching ipms allow for stable training of gans, which we will call mcgan. mcgan minimizes a few meaningful loss between distributions.", "histories": [["v1", "Mon, 27 Feb 2017 17:46:30 GMT  (3643kb,D)", "http://arxiv.org/abs/1702.08398v1", "15 pages; under review"], ["v2", "Thu, 8 Jun 2017 23:45:25 GMT  (3887kb,D)", "http://arxiv.org/abs/1702.08398v2", "15 pages; published at ICML 2017"]], "COMMENTS": "15 pages; under review", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["youssef mroueh", "tom sercu", "vaibhava goel"], "accepted": true, "id": "1702.08398"}, "pdf": {"name": "1702.08398.pdf", "metadata": {"source": "META", "title": "McGan: Mean and Covariance Feature Matching GAN", "authors": ["Youssef Mroueh", "Tom Sercu", "Vaibhava Goel"], "emails": ["<mroueh@us.ibm.com>."], "sections": [{"heading": "1. Introduction", "text": "Unsupervised learning of distributions is an important problem, in which we aim to learn underlying features that unveil the hidden the structure in the data. The classic approach to learning distributions is by explicitly parametrizing the data likelihood and fitting this model by maximizing the likelihood of the real data. An alternative recent approach is to learn a generative model of the data without explicit parametrization of the likelihood. Variational AutoEncoders (VAE) (Kingma & Welling, 2013) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) fall under this category.\nWe focus on the GAN approach. In a nutshell GANs learn a generator of the data via a min-max game between the generator and a discriminator, which learns to distinguish between \u201creal\u201d and \u201cfake\u201d samples. In this work we focus on the objective function that is being minimized between the learned generator distribution P\u03b8 and the real data distribution Pr.\nThe original work of (Goodfellow et al., 2014) showed that in GAN this objective is the Jensen-Shannon divergence. (Nowozin et al., 2016) showed that other\u03d5-divergences can be successfully used. The Maximum Mean Discrepancy objective (MMD) for GAN training was proposed in (Li et al., 2015; Dziugaite et al., 2015). As shown empirically\n*Equal contribution 1Watson Multimodal Algorithms and Engines Group IBM T.J. Watson Research Center, NY, USA. Correspondence to: Youssef Mroueh <mroueh@us.ibm.com>.\nSubmitted to the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\nin (Salimans et al., 2016), one can train the GAN discriminator using the objective of (Goodfellow et al., 2014) while training the generator using mean feature matching. An energy based objective for GANs was also developed recently (Zhao et al., 2017). Finally, closely related to our paper, the recent work Wasserstein GAN (WGAN) of (Arjovsky et al., 2017) proposed to use the Earth Moving distance (EM) as an objective for training GANs. Furthermore (Arjovsky et al., 2017) show that the EM objective has many advantages as the loss function correlates with the quality of the generated samples and the mode dropping problem is reduced in WGAN.\nIn this paper, inspired by the MMD distance and the kernel mean embedding of distributions (Muandet et al., 2017) we propose to embed distributions in a finite dimensional feature space and to match them based on their mean and covariance feature statistics. Incorporating first and second order statistics has a better chance to capture the various modes of the distribution. While mean matching was empirically used in (Salimans et al., 2016), we show in this work that it is theoretically grounded: similarly to the EM distance in (Arjovsky et al., 2017), mean and covariance feature matching of two distributions can be written as a distance in the framework of Integral Probability Metrics (IPM) (Muller, 1997). To match the means, we can use any `q norm, hence we refer to mean matching IPM, as IPM\u00b5,q . For matching covariances, in this paper we consider the nuclear norm and refer to the corresponding IPM as IPM\u03a3.\nOur technical contributions can be summarized as follows:\na) We show in Section 3 that the `q mean feature matching IPM\u00b5,q has two equivalent primal and dual formulations and can be used as an objective for GAN training in both formulations.\nb) We show in Section 3.3 that the parametrization used in Wasserstein GAN corresponds to `1 mean feature matching GAN (IPM\u00b5,1 GAN in our framework).\nc) We show in Section 4.2 that the covariance feature matching IPM\u03a3 admits also two dual formulations, and can be used as an objective for GAN training.\nd) Similar to Wasserstein GAN, we show that mean feature matching and covariance matching GANs (McGan) are stable to train, have a reduced mode dropping and the IPM loss\nar X\niv :1\n70 2.\n08 39\n8v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20\n17\ncorrelates with the quality of the generated samples."}, {"heading": "2. Integral Probability Metrics", "text": "We define in this Section IPMs as a distance between distribution. Intuitively each IPM finds a \u201ccritic\u201d f (Arjovsky et al., 2017) which maximally discriminates between the distributions."}, {"heading": "2.1. IPM Definition", "text": "Consider a compact space X in Rd. Let F be a set of measurable and bounded real valued functions on X . Let P(X ) be the set of measurable probability distributions on X . Given two probability distributions P,Q \u2208 P(X ), the Integral probability metric (IPM) indexed by the function space F is defined as follows (Muller, 1997):\ndF (P,Q) = sup f\u2208F \u2223\u2223\u2223 E x\u223cP f(x)\u2212 E x\u223cQ f(x) \u2223\u2223\u2223.\nIn this paper we are interested in symmetric function spaces F , i.e \u2200f \u2208 F ,\u2212f \u2208 F , hence we can write the IPM in that case without the absolute value:\ndF (P,Q) = sup f\u2208F { E x\u223cP f(x)\u2212 E x\u223cQ f(x) } . (1)\nIt is easy to see that dF defines a pseudo-metric over P(X). (dF non-negative, symmetric and satisfies the triangle inequality. A pseudo metric means that dF (P,P) = 0 but dF (P,Q) = 0 does not necessarily imply P = Q).\nBy choosing F appropriately (Sriperumbudur et al., 2012; 2009), various distances between probability measures can be defined. In the next subsection following (Arjovsky et al., 2017; Li et al., 2015; Dziugaite et al., 2015) we show how to use IPM to learn generative models of distributions, we then specify a special set of functions F that makes the learning tractable."}, {"heading": "2.2. Learning Generative Models with IPM", "text": "In order to learn a generative model of a distribution Pr \u2208 P(X ), we learn a function\ng\u03b8 : Z \u2282 Rnz \u2192 X , such that for z \u223c pz , the distribution of g\u03b8(z) is close to the real data distribution Pr, where pz is a fixed distribution on Z (for instance z \u223c N (0, Ik)). Let P\u03b8 be the distribution of g\u03b8(z), z \u223c pz . Using an IPM indexed by a function class F we shall solve therefore the following problem:\nmin g\u03b8\ndF (Pr,P\u03b8) (2)\nHence this amounts to solving the following min-max problem:\nmin g\u03b8 sup f\u2208F E x\u223cPr f(x)\u2212 E z\u223cpz f(g\u03b8(z))\nGiven samples {xi, 1 . . . N} from Pr and samples {zi, 1 . . .M} from pz we shall solve the following empirical problem:\nmin g\u03b8 sup f\u2208F\n1\nN\nN\u2211\ni=1\nf(xi)\u2212 1\nM\nM\u2211\nj=1\nf(g\u03b8(zj)),\nin the following we consider for simplicity M = N ."}, {"heading": "3. Mean Feature Matching GAN", "text": "In this Section we introduce a class of functions F having the form \u3008v,\u03a6\u03c9(x)\u3009, where vector v \u2208 Rm and \u03a6\u03c9 : X \u2192 Rm a non linear feature map (typically parametrized by a neural network). We show in this Section that the IPM defined by this function class corresponds to the distance between the mean of the distribution in the \u03a6\u03c9 space."}, {"heading": "3.1. IPM\u00b5,q: Mean Matching IPM", "text": "More formally consider the following finite dimensional Hilbert space:\nFv,\u03c9,p = {f(x) = \u3008v,\u03a6\u03c9(x)\u3009 \u2223\u2223\u2223v \u2208 Rm, \u2016v\u2016p \u2264 1,\n\u03a6\u03c9 : X \u2192 Rm, \u03c9 \u2208 \u2126},\nwhere \u2016.\u2016p is the `p norm. Fv,\u03c9,p is the space of bounded linear functions defined in the non linear feature space induced by the parametric feature map \u03a6\u03c9 . \u03a6\u03c9 is typically a multi-layer neural network. The parameter space \u2126 is chosen so that the function space F is bounded.\nWe recall here simple definitions on dual norms that will be necessary for the analysis in this Section. Let p, q \u2208 [1,\u221e], such that 1p + 1 q = 1. By duality of norms we have: \u2016x\u2016q =\nmaxv,\u2016v\u2016p\u22641 \u3008v, x\u3009 and the Holder inequality: \u2223\u2223\u2223 \u3008x, y\u3009 \u2223\u2223\u2223 \u2264 \u2016x\u2016p \u2016y\u2016q . From Holder inequality we obtain the following bound:\n\u2223\u2223\u2223f(x) \u2223\u2223\u2223 = \u2223\u2223\u2223 \u3008v,\u03a6\u03c9x\u3009 \u2223\u2223\u2223 \u2264 \u2016v\u2016p \u2016\u03a6\u03c9(x)\u2016q \u2264 \u2016\u03a6\u03c9(x)\u2016q .\nTo ensure that f is bounded, it is enough to consider \u2126 such that \u2016\u03a6\u03c9(x)\u2016q \u2264 B, \u2200 x \u2208 X . Given that the space X is bounded it is sufficient to control the norm of the weights and biases of the neural network \u03a6\u03c9 by regularizing the `\u221e (clamping) or `2 norms (weight decay) to ensure the boundedness of Fv,\u03c9,p.\nNow that we ensured the boundedness of Fv,\u03c9,p , we look\nat its corresponding IPM:\ndFv,\u03c9,p(P,Q) = sup f\u2208Fv,\u03c9,p E x\u223cP f(x)\u2212 E x\u223cQ f(x)\n= max \u03c9\u2208\u2126,v,||v||p\u22641 \u2329 v, E x\u223cP \u03a6\u03c9(x)\u2212 E x\u223cQ \u03a6\u03c9(x) \u232a\n= max \u03c9\u2208\u2126\n[ max\nv,||v||p\u22641\n\u2329 v, E x\u223cP \u03a6\u03c9(x)\u2212 E x\u223cQ \u03a6\u03c9(x) \u232a]\n= max \u03c9\u2208\u2126 \u2016\u00b5\u03c9(P)\u2212 \u00b5\u03c9(Q)\u2016q ,\nwhere we used the linearity of the function class and expectation in the first equality and the definition of the dual norm \u2016.\u2016q in the last equality and our definition of the mean feature embedding of a distribution P \u2208P(X ):\n\u00b5\u03c9(P) = E x\u223cP\n[ \u03a6\u03c9(x) ] \u2208 Rm.\nWe see that the IPM indexed by Fv,\u03c9,p, corresponds to the Maximum mean feature Discrepancy between the two distributions. Where the maximum is taken over the parameter set \u2126, and the discrepancy is measured in the `q sense between the mean feature embedding of P and Q. In other words this IPM is equal to the worst case `q distance between mean feature embeddings of distributions. We refer in what follows to dFv,\u03c9,p as IPM\u00b5,q ."}, {"heading": "3.2. Mean Feature Matching GAN", "text": "We turn now to the problem of learning generative models with IPM\u00b5,q . Setting F to Fv,\u03c9,p in Equation (2) yields to the following min-max problem for learning generative models:\nmin g\u03b8 max \u03c9\u2208\u2126 max v,||v||p\u22641 L\u00b5(v, \u03c9, \u03b8), (3)\nwhere\nL\u00b5(v, \u03c9, \u03b8) = \u2329 v, E x\u223cPr \u03a6\u03c9(x)\u2212 E z\u223cpz \u03a6\u03c9(g\u03b8(z)) \u232a ,\nor equivalently using the dual norm:\nmin g\u03b8 max \u03c9\u2208\u2126 \u2016\u00b5\u03c9(Pr)\u2212 \u00b5\u03c9(P\u03b8)\u2016q , (4)\nwhere \u00b5\u03c9(P\u03b8) = E z\u223cpz \u03a6\u03c9(g\u03b8(z)).\nWe refer to formulations (3) and (4) as primal and dual formulation respectively.\nThe dual formulation in Equation (4) has a simple interpretation as an adversarial learning game: while the feature space \u03a6\u03c9 tries to map the mean feature embeddings of the real distribution Pr and the fake distribution P\u03b8 to be far apart (maximize the `q distance between the mean embeddings), the generator g\u03b8 tries to put them close one to another. Hence we refer to this IPM as mean matching IPM.\nWe devise empirical estimates of both formulations in Equations (3) and (4), given samples {xi, i = 1 . . . N} from Pr, and {zi, i = 1 . . . N} from pz . The primal formulation (3) is more amenable to stochastic gradient descent since the expectation operation appears in a linear way in the cost function of Equation (3), while it is non linear in the cost function of the dual formulation (4) (inside the norm). We give here the empirical estimate of the primal formulation by giving empirical estimates L\u0302\u00b5(v, \u03c9, \u03b8) of the primal cost function:\n(P\u00b5) : min g\u03b8 max \u03c9\u2208\u2126\nv,||v||p\u22641\n\u2329 v, 1\nN\nN\u2211\ni=1\n\u03a6\u03c9(xi)\u2212 1\nN\nN\u2211\ni=1\n\u03a6\u03c9(g\u03b8(zi))\n\u232a\nAn empirical estimate of the dual formulation can be also given as follows:\n(D\u00b5) : min g\u03b8 max \u03c9\u2208\u2126 \u2225\u2225\u2225\u2225\u2225 1 N N\u2211\ni=1\n\u03a6\u03c9(xi)\u2212 1\nN\nN\u2211\ni=1\n\u03a6\u03c9(g\u03b8(zi)) \u2225\u2225\u2225\u2225\u2225 q\nIn what follows we refer to the problem given in (P\u00b5) and (D\u00b5) as `q Mean Feature Matching GAN. Note that while (P\u00b5) does not need real samples for optimizing the generator, (D\u00b5) does need samples from real and fake. Furthermore we will need a large minibatch of real data in order to get a good estimate of the expectation. This makes the primal formulation more appealing computationally."}, {"heading": "3.3. Related Work", "text": "We show in this Section that several previous works on GAN, can be written within the `q mean feature matching IPM (IPM\u00b5,q) minimization framework:\na) Wasserstein GAN (WGAN): (Arjovsky et al., 2017) recently introduced Wasserstein GAN. While the main motivation of this paper is to consider the IPM indexed by Lipchitz functions on X , we show that the particular parametrization considered in (Arjovsky et al., 2017) corresponds to a mean feature matching IPM. Indeed (Arjovsky et al., 2017) consider the function set parametrized by a convolutional neural network with a linear output layer and weight clipping. Written in our notation, the last linear layer corresponds to v, and the convolutional neural network below corresponds to \u03a6\u03c9 . Since v and \u03c9 are simultaneously clamped, this corresponds to restricting v to be in the `\u221e unit ball, and to define in \u2126 constraints on the `\u221e norms of \u03c9. In other words (Arjovsky et al., 2017) consider functions in Fv,\u03c9,p, where p = \u221e . Setting p = \u221e in Equation (3), and q = 1 in Equation (4), we see that in WGAN we are minimizing dFv,\u03c9,\u221e , that corresponds to `1 mean feature matching GAN.\nb) MMD GAN: Let H be a Reproducing Kernel Hilbert Space (RKHS) with k its reproducing kernel. For any valid\nPSD kernel k there exists an infinite dimensional feature map \u03a6 : X \u2192 H such that: k(x, y) = \u3008\u03a6(x),\u03a6(y)\u3009H . For an RKHS \u03a6 is noted usually k(x, .) and satisfies the reproducing proprety:\nf(x) = \u3008f,\u03a6(x)\u3009H , for all f \u2208H .\nSetting F = { f \u2223\u2223 ||f ||H \u2264 1 } in Equation (1) the IPM dF has a simple expression:\ndF (P,Q) = sup f,||f ||H \u22641\n{\u2329 f, E x\u223cP \u03a6(x)\u2212 E x\u223cQ \u03a6(x) \u232a}\n= \u2223\u2223\u2223 \u2223\u2223\u2223\u00b5(P)\u2212 \u00b5(Q) \u2223\u2223\u2223 \u2223\u2223\u2223 H , (5)\nwhere \u00b5(P) = E x\u223cP \u03a6(x) \u2208H is the so called kernel mean embedding (Muandet et al., 2017). dF in this case is the so called Maximum kernel Mean Discrepancy (MMD) (Gretton et al., 2012) . Using the reproducing property MMD has a closed form in term of the kernel k. Note that IPM\u00b5,2 is a special case of MMD when the feature map is finite dimensional, with the main difference that the feature map is fixed in case of MMD and learned in the case of IPM\u00b5,2. (Li et al., 2015; Dziugaite et al., 2015) showed that GANs can be learned using MMD with a fixed gaussian kernel.\nc) Improved GAN: Building on the pioneering work of (Goodfellow et al., 2014), (Salimans et al., 2016) suggested to learn the discriminator with the binary cross entropy criterium of GAN while learning the generator with `2 mean feature matching. The main difference of our IPM\u00b5,2 GAN is that both \u201cdiscriminator\u201d and \u201cgenerator\u201d are learned using the mean feature matching criterium, with additional constraints on \u03a6\u03c9 ."}, {"heading": "4. Covariance Feature Matching GAN", "text": ""}, {"heading": "4.1. IPM\u03a3: Covariance Matching IPM", "text": "As follows from our discussion of mean matching IPM comparing two distributions amounts to comparing a first order statistics, the mean of their feature embeddings. Here we ask the question how to incorporate second order statistics, i.e covariance information of feature embeddings.\nIn this Section we will provide a function space F such that the IPM in Equation (1) captures second order information. Intuitively a distribution of points represented in a feature space can be approximately captured by its mean and its covariance. Commonly in unsupervised learning, this covariance is approximated by its first k principal components (PCA directions), which capture the directions of maximal variance in the data. Similarly, the metric we define in this Section will find k directions that maximize the discrimination between the two covariances. Adding second order information would enrich the discrimination power of the feature space (See Figure 1).\nThis intuition motivates the following function space of bilinear functions in \u03a6\u03c9 :\nFU,V,\u03c9 = {f(x) = k\u2211\nj=1\n\u3008uj ,\u03a6\u03c9(x)\u3009 \u3008vj ,\u03a6\u03c9(x)\u3009\n{uj}, {vj} \u2208 Rm orthonormal j = 1 . . . k, \u03c9 \u2208 \u2126}.\nNote that the set FU,V,\u03c9 is symmetric and hence the IPM indexed by this set (Equation (1)) is well defined. It is easy to see that FU,V,\u03c9 can be written as:\nFU,V,\u03c9 = {f(x) = \u2329 U>\u03a6\u03c9(x), V >\u03a6\u03c9(x) \u232a ) \u2223\u2223\u2223\nU, V \u2208 Rm\u00d7k, U>U = Ik, V >V = Ik, \u03c9 \u2208 \u2126}\nthe parameter set \u2126 is such that the function space remains bounded. Let\n\u03a3\u03c9(P) = E x\u223cP\n\u03a6\u03c9(x)\u03a6\u03c9(x) >,\nbe the uncentered feature covariance embedding of P. It is easy to see that E\nx\u223cP f(x) can be written in terms of U, V,\nand \u03a3\u03c9(P):\nE x\u223cP f(x) = E x\u223cP\n\u2329 U>\u03a6(x), V >\u03a6(x) \u232a = Trace(U>\u03a3\u03c9(P)V ).\nFor a matrix A \u2208 Rm\u00d7m, we note by \u03c3j(A) the singular value of A, j = 1 . . .m in descending order. The 1- schatten norm or the nuclear norm is defined as the sum of singular values, \u2016A\u2016\u2217 = \u2211m j=1 \u03c3j . We note by [A]k the k-th rank approximation of A. We note Om,k = {M \u2208 Rm\u00d7k|M>M = Ik}. Consider the IPM induced by this function set. Let P,Q \u2208P(X ) we have:\ndFU,V,\u03c9 (P,Q) = sup f\u2208FU,V,\u03c9 E x\u223cP f(x)\u2212 E x\u223cQ f(x)\n= max \u03c9\u2208\u2126 U,V \u2208Om,k E x\u223cP f(x)\u2212 E x\u223cQ f(x)\n= max \u03c9\u2208\u2126 max U,V \u2208Om,k\nTrace [ U>(\u03a3\u03c9(P)\u2212 \u03a3\u03c9(Q))V ]\n= max \u03c9\u2208\u2126\nk\u2211\nj=1\n\u03c3j (\u03a3\u03c9(P)\u2212 \u03a3\u03c9(Q))\n= max \u03c9\u2208\u2126 \u2016[\u03a3\u03c9(P)\u2212 \u03a3\u03c9(Q)]k\u2016\u2217 ,\nwhere we used the variational definition of singular values and the definition of the nuclear norm. Note that U, V are the left and right singuular vectors of \u03a3\u03c9(P)\u2212\u03a3\u03c9(Q). Hence dFU,V,\u03c9 measures the worst case distance between the covariance feature embeddings of the two distributions, this distance is measured with the Ky Fan k-norm (nuclear norm of truncated covariance difference). Hence we call this IPM covariance matching IPM, IPM\u03a3."}, {"heading": "4.2. Covariance Matching GAN", "text": "Turning now to the problem of learning a generative model g\u03b8 of Pr \u2208P(X ) using IPM\u03a3 we shall solve:\nmin g\u03b8 dFU,V,\u03c9 (Pr,P\u03b8),\nthis has the following primal formulation:\nmin g\u03b8 max \u03c9\u2208\u2126,U,V \u2208Om,k L\u03c3(U, V, \u03c9, \u03b8), (6)\nwhere L\u03c3(U, V, \u03c9, \u03b8) = E x\u223cPr\n\u2329 U>\u03a6\u03c9(x), V >\u03a6\u03c9(x)) \u232a\n\u2212 E z\u223cpz\n\u2329 U>\u03a6\u03c9(g\u03b8(z)), V >\u03a6\u03c9(g\u03b8(z)) \u232a ,\nor equivalently the following dual formulation:\nmin g\u03b8 max \u03c9\u2208\u2126 \u2016[\u03a3\u03c9(Pr)\u2212 \u03a3\u03c9(P\u03b8)]k\u2016\u2217 , (7)\nwhere \u03a3\u03c9(P\u03b8) = Ez\u223cpz\u03a6\u03c9(g\u03b8(z))\u03a6\u03c9(g\u03b8(z))>.\nThe dual formulation in Equation (7) shows that learning generative models with IPM\u03a3, consists in an adversarial game between the feature map and the generator, when the feature maps tries to maximize the distance between the feature covariance embeddings of the distributions, the generator tries to minimize this distance. Hence we call learning with IPM\u03a3, covariance matching GAN.\nWe give here an empirical estimate of the primal formulation in Equation (6) which is amenable to stochastic gradient. The dual requires nuclear norm minimization and is more involved. Given {xi, xi \u223c Pr}, and {zj , zj \u223c pz}, the covariance matching GAN can be written as follows:\nmin g\u03b8 max \u03c9\u2208\u2126,U,V \u2208Om,k L\u0302\u03c3(U, V, \u03c9, \u03b8), (8)\nwhere L\u0302\u03c3(U, V, \u03c9, \u03b8) = 1\nN\nN\u2211\ni=1\n\u2329 U>\u03a6\u03c9(xi), V >\u03a6\u03c9(xi) \u232a\n\u2212 1 N\nN\u2211\nj=1\n\u2329 U>\u03a6\u03c9(g\u03b8(zj)), V >\u03a6\u03c9(g\u03b8(zj)) \u232a ."}, {"heading": "4.3. Mean and Covariance Matching GAN", "text": "In order to match first and second order statistics we propose the following simple extension:\nmin g\u03b8 max \u03c9\u2208\u2126,v,||v||p\u22641 U,V \u2208Om,k L\u00b5(v, \u03c9, \u03b8) + L\u03c3(U, V, \u03c9, \u03b8),\nthat has a simple dual adversarial game interpretation\nmin g\u03b8 max \u03c9\u2208\u2126 \u2016\u00b5\u03c9(P)\u2212 \u00b5\u03c9(P\u03b8)\u2016q+\u2016[\u03a3\u03c9(Pr)\u2212 \u03a3\u03c9(P\u03b8)]k\u2016\u2217 ,\nwhere the discriminator finds a feature space that discriminates between means and variances of real and fake, and the generator tries to match the real statistics. We can also give empirical estimates of the primal formulation similar to expressions given in the paper."}, {"heading": "5. Algorithms", "text": "We present in this Section our algorithms for mean and covariance feature matching GAN (McGan) with IPM\u00b5,q and IPM\u03a3.\nMean Matching GAN. Primal P\u00b5: We give in Algorithm 1 an algorithm for solving the primal IPM\u00b5,q GAN (P\u00b5). Algorithm 1 is adapted from (Arjovsky et al., 2017) and corresponds to their algorithm for p =\u221e. The main difference is that we allow projection of v on different `p balls, and we maintain the clipping of \u03c9 to ensure boundedness of \u03a6\u03c9 . For example for p = 2, projB`2 (v) = min(1, 1 \u2016v\u20162\n)v. For p =\u221e we obtain the same clipping in (Arjovsky et al., 2017) projB`\u221e (v) = clip(v,\u2212c, c) for c = 1. Dual D\u00b5: We give in Algorithm 2 an algorithm for solving the dual formulation IPM\u00b5,q GAN (D\u00b5). As mentioned earlier we need samples from \u201creal\u201d and \u201cfake\u201d for training both generator and the \u201ccritic\u201d feature space.\nCovariance Matching GAN. Primal P\u03a3: We give in Algorithm 3 an algorithm for solving the primal of IPM\u03a3 GAN (Equation (8)). The algorithm performs a stochastic gradient ascent on (\u03c9,U, V ) and a descent on \u03b8. We maintain clipping on \u03c9 to ensure boundedness of \u03a6\u03c9 , and perform a QR retraction on the Stiefel manifoldOm,k (Absil et al., 2007), maintaining orthonormality of U and V .\nAlgorithm 1 Mean Matching GAN - Primal (P\u00b5) Input: p to define the ball of v ,\u03b7 Learning rate, nc number of iterations for training the critic, c clipping or weight decay parameter, N batch size Initialize v, \u03c9, \u03b8 repeat\nfor j = 1 to nc do Sample a minibatch xi, i = 1 . . . N, xi \u223c Pr Sample a minibatch zi, i = 1 . . . N, zi \u223c pz (gv, g\u03c9)\u2190 (\u2207vL\u0302\u00b5(v, \u03c9, \u03b8),\u2207\u03c9L\u0302\u00b5(v, \u03c9, \u03b8)) (v, \u03c9)\u2190 (v, \u03c9) + \u03b7 RMSProp ((v, \u03c9), (gv, g\u03c9)) {Project v on `p ball, B`p = {x, \u2016x\u2016p \u2264 1}} v \u2190 projB`p (v) \u03c9 \u2190 clip(\u03c9,\u2212c, c) {Ensure \u03a6\u03c9 is bounded} end for Sample zi, i = 1 . . . N, zi \u223c pz d\u03b8 \u2190 \u2212\u2207\u03b8 \u2329 v, 1N \u2211N i=1 \u03a6\u03c9(g\u03b8(zi)) \u232a\n\u03b8 \u2190 \u03b8 \u2212 \u03b7 RMSProp (\u03b8, d\u03b8) until \u03b8 converges"}, {"heading": "6. Experiments", "text": "We train McGan for image generation with both Mean Matching and Covariance Matching objectives. We show generated images on the labeled faces in the wild (lfw)\nAlgorithm 2 Mean Matching GAN - Dual (D\u00b5) Input: q the matching `q norm ,\u03b7 Learning rate, nc number of iterations for training the critic, c clipping or weight decay parameter, N batch size Initialize v, \u03c9, \u03b8 repeat\nfor j = 1 to nc do Sample a minibatch xi, i = 1 . . . N, xi \u223c Pr Sample a minibatch zi, i = 1 . . . N, zi \u223c pz \u2206\u03c9,\u03b8 \u2190 1N \u2211N i=1 \u03a6\u03c9(xi)\u2212 1N \u2211N i=1 \u03a6\u03c9(g\u03b8(zi))\ng\u03c9 \u2190 \u2207\u03c9 \u2016\u2206\u03c9,\u03b8\u2016q \u03c9 \u2190 \u03c9 + \u03b7 RMSProp (\u03c9, g\u03c9) \u03c9 \u2190 clip(\u03c9,\u2212c, c) {Ensure \u03a6\u03c9 is bounded}\nend for Sample zi, i = 1 . . . N, zi \u223c pz Sample xi, i = 1 . . .M, xi \u223c Pr (M > N) \u2206\u03c9,\u03b8 \u2190 1M \u2211M i=1 \u03a6\u03c9(xi)\u2212 1N \u2211N i=1 \u03a6\u03c9(g\u03b8(zi)) d\u03b8 \u2190 \u2207\u03b8 \u2016\u2206\u03c9,\u03b8\u2016q \u03b8 \u2190 \u03b8 \u2212 \u03b7 RMSProp (\u03b8, d\u03b8)\nuntil \u03b8 converges\nAlgorithm 3 Covariance Matching GAN - Primal (P\u03a3) Input: k the number of components ,\u03b7 Learning rate, nc number of iterations for training the critic, c clipping or weight decay parameter, N batch size Initialize U, V, \u03c9, \u03b8 repeat\nfor j = 1 to nc do Sample a minibatch xi, i = 1 . . . N, xi \u223c Pr Sample a minibatch zi, i = 1 . . . N, zi \u223c pz G\u2190 (\u2207U ,\u2207V ,\u2207\u03c9)L\u0302\u03c3(U, V, \u03c9, \u03b8) (U, V, \u03c9)\u2190 (U, V, \u03c9)+\u03b7 RMSProp ((U, V, \u03c9), G) { Project U and V on the Stiefel manifold Om,k} Qu, Ru \u2190 QR(U) su \u2190 sign(diag(Ru)) Qv, Rv \u2190 QR(V ) sv \u2190 sign(diag(Rv)) U \u2190 QuDiag(su) V \u2190 QvDiag(sv) \u03c9 \u2190 clip(\u03c9,\u2212c, c) {Ensure \u03a6\u03c9 is bounded} end for Sample zi, i = 1 . . . N, zi \u223c pz d\u03b8 \u2190 \u2212\u2207\u03b8 1N \u2211N j=1 \u3008U\u03a6\u03c9(g\u03b8(zj)), V \u03a6\u03c9(g\u03b8(zj))\u3009\n\u03b8 \u2190 \u03b8 \u2212 \u03b7 RMSProp (\u03b8, d\u03b8) until \u03b8 converges\n(Huang et al., 2007), LSUN bedrooms (Yu et al., 2015), and cifar-10 (Krizhevsky & Hinton, 2009) datasets.\nIt is well-established that evaluating generative models is hard (Theis et al., 2016). Many GAN papers rely on a combination of samples for quality evaluation, supplemented by a number of heuristic quantitative measures. We will mostly focus on training stability by showing plots of the\nloss function, and will provide generated samples to claim comparable sample quality between methods, but we will avoid claiming better sample quality. These samples are all generated at random and are not cherry-picked.\nThe design of g\u03b8 and \u03a6\u03c9 are following DCGAN principles (Radford et al., 2015), with both g\u03b8 and \u03a6\u03c9 being a convolutional network with batch normalization (Ioffe & Szegedy, 2015) and ReLU activations. \u03a6\u03c9 has output size bs \u00d7 F \u00d7 4 \u00d7 4. The inner product can then equivalently be implemented as conv(4x4, F->1) or flatten + Linear(4*4*F -> 1). We generate 64\u00d7 64 images for lfw and LSUN and 32\u00d7 32 images on cifar, and train with minibatches of size 64. We follow the experimental framework and implementation of (Arjovsky et al., 2017), where we ensure the boundedness of \u03a6\u03c9 by clipping the weights pointwise to the range [\u22120.01, 0.01]. Primal versus dual form of mean matching. We trained mean matching GANs both in the primal and dual form, see respectively Algorithm 1 and 2. Samples are shown in Figure 2. The primal formulation of IPM\u00b5,1 GAN corresponds to clipping v, i.e. the original WGAN, while for IPM\u00b5,2 we divide v by its `2 norm if it becomes larger than 1. In the dual formulation, for q = 2 we noticed little difference between maximizing the `2 norm or its square.\nWe observed that the default learning rates from WGAN (5e-5) are optimal for both primal and dual formulation. Figure 3 shows the loss (i.e. IPM estimate) dropping steadily for both the primal and dual formulation independently of the choice of the `q norm. We also observed that during the whole training process, samples generated from the same noise vector across iterations, remain similar in nature (face identity, bedroom style), while details and background will evolve. This qualitative observation indicates valuable stability of the training process.\nFor the dual formulation (Algorithm 2), we confirmed the hypothesis that we need a good estimate of \u00b5\u03c9(Pr) in order to compute the gradient of the generator \u2207\u03b8: we needed to increase the minibatch size of real threefold to 3\u00d7 64. Covariance GAN. We now experimentally investigate the IPM defined by covariance matching. For this section and the following, we use only the primal formulation, i.e. with explicit uj and vj orthonormal (Algorithm 3). Figure 4 and 5 show samples and loss from lfw and LSUN training respectively. We use Algorithm 3 with k = 16 components. We obtain samples of comparable quality to the mean matching formulations (Figure 2), and we found training to be stable independent of hyperparameters like number of components k varying between 4 and 64.\nCovariance GAN with labels and conditioning. Finally, we conduct experiments on the cifar-10 dataset, where we will leverage the additional label information by training\na conditional GAN (Mirza & Osindero, 2014). The way we incorporate label information is following InfoGAN (Chen et al., 2016) and AC-GAN (Odena et al., 2016), where both real and generated data are labeled, but the label is not given as input to the discriminator. The generator sees the label by appending a one-hot encoding along the feature map dimension, i.e. the generator receives an input vector of size (nz + 10) \u00d7 1 \u00d7 1. The classifier is a linear output layer S on top of \u03a6\u03c9 followed by softmax. We now optimize a combination of the original IPM loss in the primal formulation with the cross-entropy loss CE= \u2212Ex,ylogsoftmax(\u3008Sy,\u03a6\u03c9(x)\u3009).\nDuring the nc critic training iterations, we alternate between mazimizing\n\u2022 the IPM L\u0302\u03c3 alone, where g\u03b8 gets conditioning labels at random, and\n\u2022 L\u0302\u03c3\u2212\u03bbDCE, where g\u03b8 gets conditioning labels matching the real labels.\nThe generator is trained by sampling with random labels y and always minimizes L\u0302\u03c3 + \u03bbGCE.\nResults are shown in figure 6. Notice rows corresponding to recognizable classes, while the noise z (shared within each column) clearly determines other elements of the visual style like dominant color, across label conditioning. Additional experimental results, with combinations of Mean and Covariance Matching are presented in the supplementary material."}, {"heading": "7. Discussion", "text": "We noticed the influence of clipping on the capacity of the critic: a higher number of feature maps was needed to compensate for clipping. The question remains what alternatives to clipping of \u03a6\u03c9 can ensure the boundedness. For example, we succesfully used an `2 penalty on the weights of \u03a6\u03c9 . Other directions are to explore geodesic distances between the covariances (Arsigny et al., 2006), and extensions of the IPM framework to the multimodal setting (Isola et al., 2016)."}, {"heading": "A. Subspace Matching Interpretation of Covariance Matching GAN", "text": "Let \u2206\u03c9 = \u03a3\u03c9(P) \u2212 \u03a3\u03c9(Q). \u2206\u03c9 is a symmetric matrix but not PSD, which has the property that its eigenvalues \u03bbj are related to its singular values as given by: \u03c3j = |\u03bbj | and its left and right singular vectors coincides with its eigenvectors and satisfy the following equality uj = sign(\u03bbj)vj . One can ask here if we can avoid having both U, V in the definition of IPM\u03a3 since at the optimum uj = \u00b1vj . One could consider \u03b4E\u03c9(Pr,P\u03b8) defined as follows:\nmax \u03c9\u2208\u2126,U\u2208Om,k E x\u223cPr \u2016U\u03a6\u03c9(x)\u20162\nEnergy in the subspace of real data\n\u2212 E z\u223cpz \u2016U\u03a6\u03c9(g\u03b8(z))\u20162\nEnergy in the subspace of fake data\n,\nand then solve for ming\u03b8 \u03b4E\u03c9(Pr,P\u03b8). Note that:\n\u03b4E\u03c9(Pr,P\u03b8) = max \u03c9\u2208\u2126,U\u2208Om,k\nTrace(U>(\u03a3\u03c9(Pr)\u2212 \u03a3\u03c9(P\u03b8))U)\n= max \u03c9\u2208\u2126\nk\u2211\ni=1\n\u03bbi(\u2206\u03c9)\n\u03b4E\u03c9 is not symmetric furthermore the sum of those eigenvalues is not guaranteed to be positive and hence \u03b4E\u03c9 is not guaranteed to be non negative, and hence does not define an IPM. Noting that \u03c3i(\u2206\u03c9) = |\u03bbi(\u2206\u03c9)|,we have that:\nIPM\u03a3(Pr,P\u03b8) = k\u2211\ni=1\n\u03c3i(\u2206\u03c9) \u2265 k\u2211\ni=1\n\u03bbi(\u2206\u03c9) = \u03b4E\u03c9(Pr,P\u03b8).\nHence \u03b4E is not an IPM but can be optimized as a lower bound of the IPM\u03a3. This would have an energy interpretation as in the energy based GAN introduced recently (Zhao et al., 2017): the discriminator defines a subspace that has higher energy on real data than fake data, and the generator maximizes his energy in this subspace."}, {"heading": "B. Mean and Covariance Matching Loss Combinations", "text": "We report below samples for McGan, with different IPM\u00b5,q and IPM\u03a3 combinations. All results are reported for the same architecture choice for generator and discriminator, which produced qualitatively good samples with IPM\u03a3 (Same one reported in Section 6 in the main paper). Note that in Figure 7 with the same hyper-parameters and architecture choice, WGAN failed to produce good sample. In other configurations training converged."}], "references": [{"title": "Optimization Algorithms on Matrix Manifolds", "author": ["Absil", "P.-A", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2007}, {"title": "Log-euclidean metrics for fast and simple calculus on diffusion tensors", "author": ["Arsigny", "Vincent", "Fillard", "Pierre", "Pennec", "Xavier", "Ayache", "Nicholas"], "venue": "In Magnetic Resonance in Medicine,", "citeRegEx": "Arsigny et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2006}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Dziugaite", "Gintare Karolina", "Roy", "Daniel M", "Ghahramani", "Zoubin"], "venue": "In UAI,", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A kernel two-sample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "Learned-Miller", "Erik"], "venue": "Technical report,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "Proc. ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Isola", "Phillip", "Zhu", "Jun-Yan", "Zhou", "Tinghui", "Efros", "Alexei A"], "venue": "arxiv,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": "Arxiv,", "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Kernel mean embedding of distributions: A review and beyond", "author": ["Muandet", "Krikamol", "Fukumizu", "Kenji", "Sriperumbudur", "Bharath", "Schlkopf", "Bernhard"], "venue": null, "citeRegEx": "Muandet et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2017}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["Muller", "Alfred"], "venue": "Advances in Applied Probability,", "citeRegEx": "Muller and Alfred.,? \\Q1997\\E", "shortCiteRegEx": "Muller and Alfred.", "year": 1997}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Nowozin", "Sebastian", "Cseke", "Botond", "Tomioka", "Ryota"], "venue": "In NIPS,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["Odena", "Augustus", "Olah", "Christopher", "Shlens", "Jonathon"], "venue": null, "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "Arxiv,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS", "citeRegEx": "Rahimi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2008}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": null, "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "On integral probability metrics, phi -divergences and binary classification", "author": ["Sriperumbudur", "Bharath K", "Fukumizu", "Kenji", "Gretton", "Arthur", "Schlkopf", "Bernhard", "Lanckriet", "Gert R. G"], "venue": null, "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "On the empirical estimation of integral probability metrics", "author": ["Sriperumbudur", "Bharath K", "Fukumizu", "Kenji", "Gretton", "Arthur", "Schlkopf", "Bernhard", "Lanckriet", "Gert R. G"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2012}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["Yu", "Fisher", "Zhang", "Yinda", "Song", "Shuran", "Seff", "Ari", "Xiao", "Jianxiong"], "venue": "arXiv preprint arXiv:1506.03365,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Energy based generative adversarial networks", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Lecun", "Yann"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "Variational AutoEncoders (VAE) (Kingma & Welling, 2013) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) fall under this category.", "startOffset": 98, "endOffset": 123}, {"referenceID": 4, "context": "The original work of (Goodfellow et al., 2014) showed that in GAN this objective is the Jensen-Shannon divergence.", "startOffset": 21, "endOffset": 46}, {"referenceID": 15, "context": "(Nowozin et al., 2016) showed that other\u03c6-divergences can be successfully used.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "The Maximum Mean Discrepancy objective (MMD) for GAN training was proposed in (Li et al., 2015; Dziugaite et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 3, "context": "The Maximum Mean Discrepancy objective (MMD) for GAN training was proposed in (Li et al., 2015; Dziugaite et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 19, "context": "in (Salimans et al., 2016), one can train the GAN discriminator using the objective of (Goodfellow et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": ", 2016), one can train the GAN discriminator using the objective of (Goodfellow et al., 2014) while training the generator using mean feature matching.", "startOffset": 68, "endOffset": 93}, {"referenceID": 24, "context": "An energy based objective for GANs was also developed recently (Zhao et al., 2017).", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "In this paper, inspired by the MMD distance and the kernel mean embedding of distributions (Muandet et al., 2017) we propose to embed distributions in a finite dimensional feature space and to match them based on their mean and covariance feature statistics.", "startOffset": 91, "endOffset": 113}, {"referenceID": 19, "context": "While mean matching was empirically used in (Salimans et al., 2016), we show in this work that it is theoretically grounded: similarly to the EM distance in (Arjovsky et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 21, "context": "By choosing F appropriately (Sriperumbudur et al., 2012; 2009), various distances between probability measures can be defined.", "startOffset": 28, "endOffset": 62}, {"referenceID": 11, "context": "In the next subsection following (Arjovsky et al., 2017; Li et al., 2015; Dziugaite et al., 2015) we show how to use IPM to learn generative models of distributions, we then specify a special set of functions F that makes the learning tractable.", "startOffset": 33, "endOffset": 97}, {"referenceID": 3, "context": "In the next subsection following (Arjovsky et al., 2017; Li et al., 2015; Dziugaite et al., 2015) we show how to use IPM to learn generative models of distributions, we then specify a special set of functions F that makes the learning tractable.", "startOffset": 33, "endOffset": 97}, {"referenceID": 13, "context": "where \u03bc(P) = E x\u223cP \u03a6(x) \u2208H is the so called kernel mean embedding (Muandet et al., 2017).", "startOffset": 66, "endOffset": 88}, {"referenceID": 5, "context": "dF in this case is the so called Maximum kernel Mean Discrepancy (MMD) (Gretton et al., 2012) .", "startOffset": 71, "endOffset": 93}, {"referenceID": 11, "context": "(Li et al., 2015; Dziugaite et al., 2015) showed that GANs can be learned using MMD with a fixed gaussian kernel.", "startOffset": 0, "endOffset": 41}, {"referenceID": 3, "context": "(Li et al., 2015; Dziugaite et al., 2015) showed that GANs can be learned using MMD with a fixed gaussian kernel.", "startOffset": 0, "endOffset": 41}, {"referenceID": 4, "context": "c) Improved GAN: Building on the pioneering work of (Goodfellow et al., 2014), (Salimans et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 19, "context": ", 2014), (Salimans et al., 2016) suggested to learn the discriminator with the binary cross entropy criterium of GAN while learning the generator with `2 mean feature matching.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "We maintain clipping on \u03c9 to ensure boundedness of \u03a6\u03c9 , and perform a QR retraction on the Stiefel manifoldOm,k (Absil et al., 2007), maintaining orthonormality of U and V .", "startOffset": 112, "endOffset": 132}, {"referenceID": 6, "context": "(Huang et al., 2007), LSUN bedrooms (Yu et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": ", 2007), LSUN bedrooms (Yu et al., 2015), and cifar-10 (Krizhevsky & Hinton, 2009) datasets.", "startOffset": 23, "endOffset": 40}, {"referenceID": 22, "context": "It is well-established that evaluating generative models is hard (Theis et al., 2016).", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "The design of g\u03b8 and \u03a6\u03c9 are following DCGAN principles (Radford et al., 2015), with both g\u03b8 and \u03a6\u03c9 being a convolutional network with batch normalization (Ioffe & Szegedy, 2015) and ReLU activations.", "startOffset": 55, "endOffset": 77}, {"referenceID": 2, "context": "The way we incorporate label information is following InfoGAN (Chen et al., 2016) and AC-GAN (Odena et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 16, "context": ", 2016) and AC-GAN (Odena et al., 2016), where both real and generated data are labeled, but the label is not given as input to the discriminator.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "Other directions are to explore geodesic distances between the covariances (Arsigny et al., 2006), and extensions of the IPM framework to the multimodal setting (Isola et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 8, "context": ", 2006), and extensions of the IPM framework to the multimodal setting (Isola et al., 2016).", "startOffset": 71, "endOffset": 91}], "year": 2017, "abstractText": "We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.", "creator": "LaTeX with hyperref package"}}}