{"id": "1702.08568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys", "abstract": "for seventy years traditional security machine learning research has promised to obviate the need for signature based detection by automatically learning to detect indicators of attack. unfortunately, this vision hasn't come to fruition : in fact, developing and jointly maintaining today's security machine learning systems can require engineering resources that are comparable to that of signature - based detection systems, due in part to the need to develop and continuously tune the \" features \" these machine learning systems look at as attacks naturally evolve. deep learning, a subfield fork of machine learning, promises to change this by operating on raw input signals and automating the process of feature design and extraction. in this paper we propose the expose partial neural network, which uses a deep learning approach we have developed to take generic, raw short character strings as input ( a common case for security inputs, which include alias artifacts like potentially malicious urls, file paths, named alias pipes, named mutexes, and registry keys ), and learns to simultaneously extract features and classify using character - level embeddings and convolutional neural network. furthermore in addition to completely automating the feature design and extraction process, expose outperforms manual feature extraction based baselines on all of the intrusion problem detection problems we tested it on, yielding a 5 % - 10 % detection rate gain at 0. 1 % false positive rate compared to these baselines.", "histories": [["v1", "Mon, 27 Feb 2017 22:32:13 GMT  (1041kb)", "http://arxiv.org/abs/1702.08568v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["joshua saxe", "konstantin berlin"], "accepted": false, "id": "1702.08568"}, "pdf": {"name": "1702.08568.pdf", "metadata": {"source": "CRF", "title": "eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys", "authors": ["Joshua Saxe"], "emails": ["josh.saxe@invincea.com", "kberlin@invincea.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n08 56\n8v 1\n[ cs\n.C R\n] 2"}, {"heading": "1 Introduction", "text": "While for over a decade researchers have proposed systems that apply machine learning methods to computer security detection problems, this research has gained only limited prevalence in real-world security systems, in part, we believe, because machine learning systems require significant expert effort to develop and maintain.\nFor example, development of machine learning based security detection systems requires an in-depth exploration of the feature representation of a given security artifact type (e.g. Windows PE binaries, URLs, or behavioral traces), and an exploration of what machine learning detection approaches yield the best\naccuracy given those representations. As cyber-attacks evolve, machine learning feature representations must be updated to keep pace with the latest cyber threats. The calculation of many computer security products companies is often that signature based systems are thus a less risky investment.\nWhile many technical problems stand in the way of effective deployment of machine learning systems (e.g. the collection of large volumes of labeled training data, the problem of evaluating these systems when attacker behavior is constantly changing, and the problem of deploying complex models of low-resource endpoints), one way to reduce the cost of creating and maintaining machine learning approaches is to move beyond manual feature engineering, given that feature engineering is often recognized as the most time consuming aspect of machine learning system development. Deep learning, a subfield of machine learning that utilizes neural networks operating directly on raw inputs, promises to allow us to do this.\nIn line with this vision, we present eXpose, a deep learning approach to a number of security detection problems, that directly works on raw inputs to detect maliciousness. Specifically, eXpose takes generic short character strings as its input and learns to detect whether they are indicators of malicious behavior based on their lexical semantics. In this paper, we demonstrate eXpose\u2019s ability to detect malicious URLs, malicious file paths, and malicious registry keys.\nTo make our research objectives clear, below are examples of these data, starting with malicious URLs (we\u2019ve substituted URL forward slashes for backslashes to avoid accidental clicks):\nhttp:\\\\0fx8o.841240.cc\\201610\\18\\content_23312\\svchost.exe http:\\\\31.14.136.202\\secure.apple.id.login\\Apple\\login.php http:\\\\1stopmoney.com\\paypal-login-secure\\websc.php\nNext, a few examples of malicious file paths:\nC:\\Temp\\702D97503A79B0EC69\\JUEGOS/Call of Duty 4+Keygen C:\\Temp\\svchost.vbs C:\\DOCUME~1\\BASANT~1\\LOCALS~1\\Temp\\WzEC.tmp\\fax.doc.exe\nFinally, a few examples of malicious registry keys:\nHKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run Alpha Antivirus HKCR\\Applications\\WEBCAM HACKER 1.0.0.4.EXE HKCR\\AppID\\bccicabecccag.exe\nAll of these examples appear malicious, or at least suspicious, to the expert eye, leading us to hypothesize that a machine learning system could also infer their maliciousness. It might even be possible to exceed human expert\u2019s ability to guess whether these artifacts are malicious, by learning to recognize generalized deceptive patterns observed over tens of millions of malicious artifacts. And indeed, on all of the intrusion detection problems we tested, eXpose outperformed manual feature extraction based machine learning baselines, yielding a 5%-10% higher detection rate at deployment relevant false positive rates. Our\nresearch demonstrates the potential deep learning methods hold solving hard security detection problems.\nThe rest of this paper is structured as follows. In Section 2 we describe related work. In Section 3 we motivate and describe our eXpose approach, including an exact and reproducible description of our system. Section 4 describes our evaluation methodology and results. Finally, in section 5 we sum up the paper and discuss directions for future work."}, {"heading": "2 Previous Work", "text": ""}, {"heading": "2.1 Related work in computer security", "text": "We designed eXpose as a fairly generic detection tool that simultaneously addresses a number of cybersecurity problems. This is somewhat different from previous work, which tends to focus on individual security detection problems, such as identifying malicious URLs or malicious host-based behavior individually. In this section we place our work in conversation with current cybersecurity literature and describe its relationship to the broader deep learning literature.\nA number of previous works in machine learned based behavioral detection of malware is related to automatic classification of individual file paths or registry keys. In general, previous behavioral malware detection methods have focused on making detections on the basis of sequences of observed process or operating system-level events. For example, [3] proposes a logistic regression-based method for detecting malware infections based on n-grams of audit log event observations. Relatedly, [1] proposes to use an anomaly detection approach on sequences of registry accesses to infer whether a host has been compromised. [6] surveys a wide variety of behavioral malware detection techniques, all of which perform manual feature engineering on collections of events to infer whether or not dynamically executed binaries are malicious or benign.\nUnlike the work summarized above, which operates on groups of dynamic host-based observations to detect malware, eXpose operates on individual events, but rather than modeling individual host-based events using manually defined feature representations, eXpose learns representations of input strings (e.g. file paths and registry keys) as part of its overall process of learning to make accurate detections of malicious behavior. We thus think of eXpose as providing a complementary and orthogonal detection capability relative to these research efforts.\nUnlike individual file and registry writes, identifying malicious URLs is a more studied problem in the security detection literature. Proposed malicious URL detection approaches have tended to either exclusively use URL strings as their input or utilize both URL strings and supplementary information like website registration services, website content, and network reputation [14]. In contrast to work that uses both input URLs and auxiliary information to detect malicious URLs, our work relies solely on URL input strings, making it easier to deploy.\nWith respect to the detection mechanism used in previous URL detection work, the simplest proposed approaches have involved blacklists, which can be collected using manual labeling, user feedback, client honeypots, and other heuristics [10]. While blacklists have a very low false positive rate, they are also very brittle and thus cannot generalize to previously unseen URL strings [26]. To address these limitations, statistical approaches, such as machine-learning or similarity based URL detection have been proposed [14]. Unfortunately, manually discovering potentially useful features is time consuming and requires constant adaptation to evolving obfuscation techniques, which limits the achievable accuracy of the detectors. In contrast to work that requires manual feature extraction from URLs to make detections, our work automates this feature extraction process."}, {"heading": "2.2 Machine Learning", "text": "Convolutional Neural Networks eXpose uses neural network convolutional kernels as part of its approach to automating feature engineering and extraction, and so in addition to computer security literature focused on detecting cyber attacks, our work is also related to the convolutional neural network and recurrent neural network literature in machine learning.\nConvolutional Neural Networks (CNN) [18,19] have been applied to image recognition problems for a long time, but only fairly recently have they demonstrated breakthrough results in image recognition [17]. The advantage of CNNs over previous approaches is that they work directly on the raw pixel data, thus eliminating tedious and fairly limited hand designing of features. What makes CNNs particularly powerful for images is that they are able to efficiently exploit information locality by applying convolutional operations on a raw data using a set of different kernels. These kernels are learned jointly with the entire network, and thus are better able to adapt to learning objectives than hand designed designed kernels. Since the same kernel is applied to every pixel of the image, there is a tremendous reduction in parameters that need to be learned, as compared to a fully dense neural network.\nIn addition to targeting image inputs, CNNs have also found rich applications within natural language processing, where they are typically applied to subsequences of words or patterns such that they perform pattern matching on sequential patterns within input texts. For example, [24] proposes to combine a word embedding approach with a convolutional neural network to perform sentiment analysis on Twitter data.\nOther authors propose machine learning approaches that operate on character level embeddings [23,15,30]. The advantage of such approaches is that they do not require syntactical understanding of the language, such as word boundaries or punctuation. This work is closely to related to our own, since we also model text strings at the character level, embedding them in an embedding space and then extracting features using convolutions. To our knowledge, our approach is the first computer security detector to take this approach or any approach that extracts features from raw inputs.\nRecurrent Neural Networks RNNs are commonly used to process sequential information, with LSTM based approaches being among the more popular [9]. While theoretically they are able to learn long terms dependencies in a sequences, RNNs are problematic to train due to the vanishing gradient problem as well as large computational cost because of the need to sequentially back and forward propagate information during training and prediction phases [8]. Recently CNN have been shown to be just (if not more) effective on sequential data oriented modeling problems, while allowing a significantly faster model training and prediction evaluation [13].\nMulti-task Learning Multi-task learning using neural networks is a somewhat related idea to our generalized CNN model. There, a set of network layers is shared between various related learning tasks, and an individualized set of final layers is used to make the final prediction on the specific task [4]. Sharing internal weights potentially allows the network to learn a richer representation of the data, which is especially useful when individual datasets are very small [2]. Another advantage of multi-task learning is if a set of related classifiers needs to be deployed to an endpoint, weight-sharing provides a smaller deployment footprint.\nMulti-task learning has also been suggested for malware detection by simultaneously trying to predict binary classification as well as malware family [11]. However, multi-task learning does not directly map to our set of detection problems, where the semantic meaning of the characters and substrings significantly changes between problems. Furthermore, our labeled dataset is of virtually unlimited size, making training on combined datasets less useful."}, {"heading": "3 Method", "text": "eXpose is built on the premise that applying a neural network directly to the raw input of short character strings provides better classification accuracy than previous approaches that rely on hand-designed features. In this section we describe how we architected our neural network to operate directly on raw character input, and the intuition behind our decisions. Our network was implemented in Python 2.7 using Keras v1.1 [5]."}, {"heading": "3.1 Architecture", "text": "Fig. 1 gives an intuitive overview of our approach, showing that our neural network is divided into three notional components 3: character embedding, feature detection, and a classifier.\nThe character embedding component embeds the alphabet of printable Englishlanguage characters into a multi-dimensional feature space, thus encoding an input string\u2019s sequence of raw characters as a two-dimensional tensor.\nUsing this tensor, the feature detection component detects important local sequence patterns within the full character sequence, and then aggregates this information into a fixed-length feature vector.\nFinally, the classification component classifies the detected features using a dense neural network. All of these components are optimized jointly using stochastic gradient descent. Fig. 2 gives a formal diagram of our neural network architecture, which we also describe step by step in the text below.\nCharacter Embedding Our model starts its computational flow with the raw length s sequence of characters and embeds them into an s \u00d7 m floating point matrix. This operation is a simply dictionary lookup, where each character, irrespective of the characters that came before it or after it, is mapped to its corresponding vector, and then these vectors are concatenated into this matrix. The matrix\u2019s rows represent the sequence of characters in the original string, and the matrix\u2019s columns represent the dimensions of the embedding space.\nEmbedding layer is optimized jointly with the rest of the model through backpropagation, optimizing the individual characters\u2019 embedding vectors to be more reflective of their semantic meaning, resulting in pairs of semantically similar characters being embedded closer to each other if they have similar attributes (e.g. they are both uppercase, both control control characters, etc.) [15]. This clustering of semantically similar characters makes it similar for the lower layers to identify semantically similar patterns in the string.\nIn our implementation we set s = 200 and m = 32. For our URL-based experiments we use an input vocabulary of the 87 URL-valid characters, and for our file path and registry key experiments we use an input vocabulary of the 100 valid printable characters. Any unicode we encounter in our experiments we wildcard with a lower-case \u2018x\u2019, reserving more sophisticated handling of international characters for future work. We set maximum string length on all artifacts to 200 since this is around the 95th percentile or greater of all strings in our experimental URL, file path, and registry datasets. If the string is shorter than\n3 What we mean here is that our overall model is most easily understood as containing three separate components, each focused on a somewhat different task. We used this notional hierarchy when developing our networks architecture. It is important to note, however, that the entire model is simultaneously optimized, end-to-end, and thus all components are optimized for the singular classification task. We can think of the entire model as some complex classifier, or alternatively, a deep feature extractor, followed by a logistic regression.\n200, we pad it with a special null symbol in the front. If the string is longer than 200, we cutoff the beginning of the string. We empirically determined that m = 32 provides a good tradeoff between accuracy and computational complexity. Note that 32 is much smaller then the potential 87-sized or 100-sized one-hot-encoding of these same characters, which is a common way to represent categorical input in machine learning models.\nWe visually demonstrate that our trained model does indeed learn semantically related embedding in Fig. 3, where we show a two-dimensional MDS projection of our learned embeddings. As the Fig. 3 shows, letters with similar semantics tend to cluster together, with upper case letters appearing near other upper case letters, lowercase letters near other lowercase letters, tilde, a highly important character in the URL case falling into its own cluster, etc. The fact that our character vectors cluster in this way suggests that our embedding representation is capturing their semantic meaning.\nFeature Detection Once we embed our input into an s\u00d7m matrix, the next step is extracting and aggregating locally detected features. This is done in two stages: in the first stage we detect local features by applying multiple kernel convolutions, Conv(t, k, n), and in the second stage we aggregate the results across the entire sequence by summing the kernels\u2019 activations using SumPool. We define both Conv and SumPool formally in our appendix. These steps are done separately for each k \u2208 {2, 3, 4, 5}, and we empirically set t = 256. The four results for each k tower are then concatenated together into a 1024 length vector.\nThe t filters in our CNN spans the entire length of the character embedding m, and can be thought of as sliding of convolution kernels (or masks) over the sequence of character embeddings. The motivation for using convolutions as our feature extraction component flows from similar approaches in natural language processing (NLP) [30].\nComputing convolutions on raw character embedding matrix is conceptually similar to traditional bag-of-words approaches. The main conceptional difference is that rather than directly detecting n-grams, we allow for \u201capproximate\u201d matches on semantically similar substrings.\nIn this manner of thinking, each convolutional filter is responsible for detecting a distinct set of similar sequential patterns, and by summing up its activations over a text string, we obtain the degree to which these patterns occurs in the full string, similar to a bag-of-words aggregating all the n-grams. Just like the n-gram approach, our approach is robust to insertions and deletions within the character-string, as subsequences can occur anywhere in the string and still be detected by the convolution.\nNormalization and Regularization To speed up model training and prevent overfitting, we use layer-wise BatchNorm and Dropout(0.5) (0.2 for registry keys) between layers (see Fig. 2 for details). We define both BatchNorm and Dropout in our appendix below. We found that layer-wise normalization gave better results\nthan the more popular batch normalization [12], and that putting the normalization after the activation gave equivalent results to putting it before each units activation function. We also found that without regularization our model can easily overfit our training data, even when training on millions of samples.\nClassification Once we extract the features, we use a standard dense neural network to classify the string as malicious or benign. The dense neural network has two layers, a Dense(l) unit, followed by the DenseSigmoid(l) layer with l = 1024 units. We define both Dense and DenseSigmoid in our appendix below. The dense layers learn a non-linear kernel given the convolution based features, and the sigmoid layer output provides the probability that the input string is malicious given the output of the final dense layer. We measure our detector\u2019s prediction loss using binary-cross entropy,\nL(y\u0302,y) = \u2212 1\nN\nN \u2211\ni\n[yi log y\u0302i + (1 \u2212 yi) log(1 \u2212 y\u0302i)] (1)\nwhere y\u0302 is our model\u2019s prediction probability vector for all the URL samples and y is the vector of true label (0 for benign, and 1 for malicious). We use Adam [16] method to minimize eq. (1).\nTypically, it is much easier to collect benign than malicious data, resulting in highly imbalanced dataset. Rather than simply reweighing individual values to equalize the overall contribution of the benign and malware in eq. (1), we adjusted the benign to malware ratio directly during batch streaming. We generate 256-sized batch by first randomly sampling the full dataset of benign samples and randomly selected 128 samples, and then repeating the same approach for 128 malware samples. This effectively created an even class balance between malicious and benign data in our training dataset, with a more diverse representation of each class than simple reweighting of individual samples. We count one epoch as having processed 4096 batches, and train for 100 epochs.\nFor our final solution we select the best overall model, determined by largest area under the ROC (AUC) for the time-split validation.\nAlternatives Attempted\nDuring the development of our current architecture we have also tried several alternative architectures that did not yield better performance, and/or where computationally too expensive:\n\u2013 Replacing regular convolutions with a series of dilated convolutions [29,13]. \u2013 Stacking embeddings, convolutions, and a long short term memory recurrent layer [22].\n\u2013 Stacked convolutions to learn non-linear convolutional activations [28]. \u2013 ResNet type stacking of convolutions [7]"}, {"heading": "4 Results", "text": "We evaluate our model against two baseline approaches, described below, on three different problems. The first problem involves identifying malicious URLs directly from the URL string. For this problem we downloaded 19067879 unique URLs, randomly sampled over a roughly two month period, from VirusTotal.\nFor the second and third problem, identifying malicious registry key and file paths, we extracted over 18 million Cuckoo sandbox runs, as recorded on VirusTotal, and utilized all the observed file and registry writes and creations. This gave us 5590614 unique file paths, and 1661716 registry key paths. We give a detailed breakdown of these data in Table 1."}, {"heading": "4.1 Labeling", "text": "Training and evaluating eXpose\u2019s performance requirs assigning a binary label to every artifact in our experimental datasets indicating whether it is malicious or benign. We label URL artifacts we used a voting approach, in which we assigned a label to the URL based on the score given by 59 anti-virus engines. If 5 or more of these engines assigned a \u201cmalicious\u201d label to a URL we considered it malicious, and if no engines assigned a \u201cmalicious\u201d label to a URL we labeled it benign. We discarded URLs with 1 to 4 anti-virus engine detections. Our motivation is that URLs may or may not be benign or malicious, and this uncertainty would introduce label noise into both the training of our model and our validation of its accuracy.\nSince registry keys and file paths can be ambiguous (e.g., both malware and benignware can write to same path), we took a different approach for labeling file and registry key paths. First, we labeled our corpus of binaries as either malicious or benign using a voting technique over an ensemble of 60 anti-virus engines, where binaries that had 5 or more anti-virus based detections were labeled as malicious and binaries that had 0 detections were labeled as benign. We then discarded binaries with between 1 and 4 detections from our dataset, as we regarded the question of whether these binaries were malicious or benign as ambiguous. Next we inspected the behavioral traces of the resulting 18 million binaries, counting how often each unique file path or registry key was created or\nwritten to in both benign and malicious sandbox runs. Finally, we labeled any file paths or registry keys that only occurred in malicious contexts as malicious, and labeled the other artifacts (benign or partially cases) as benign."}, {"heading": "4.2 Baseline Models", "text": "We implemented two baseline models, one is standard general feature n-gram extractor, where we pool out a set of all possible 1-5 sized n-grams. The second model, used only for URLs, is based on manually extracted features described in [20]. These features include common sense statistics like: URL length, the number of \u2018.\u2019 separators in a URL, and categorical lexical features like domain name and URL suffix tokens. Combined together, they form a very large, but sparse feature vector.\nTo make training tractable at the scale of millions of examples, we use the feature hashing trick to randomly hash these features into 1024-dimensional vectors. Our motivation for picking a dimensionality of 1024 was two-fold. First, in order to tractably train our baseline models on millions of URL examples, a small feature vector is optimal. Second, given that the output of our novel models feature extraction is 1024 dimensional, we were interested in comparing a conventional 1024-dimensional representation of URLs with our deep learning representation, thereby answering the question, which representation is the richest representation for performing malicious URL detection?\nThe above hashed features are fed directly into a deep MLP model. This MLP model is identical to our novel neural network model, except that we\u2019ve stripped off the deep learning feature extraction layers, and replaced the input they provide with our manually constructed 1024-dimensional feature vector. This design is intended to highlight the potential contribution of our convolutional feature extractor in improving detection accuracy."}, {"heading": "4.3 Evaluation", "text": "We present our results using a ROC curve between true positive and false positives rates. This measure is independent of the ratio of benign to malware in our dataset, and so is simplest to interpret. We focus on the low FPR rates 10\u22124 and 10\u22123, which from our experience represent a reasonable deployment threshold. The ROC curves for all three problems are shown in Fig. 3, and the specific values are given in Table 2.\nIn addition to the ROC curves, we also present the two dimensional PCA projection of the normalized embedding vectors for the all individual characters in Fig. 3. The capital letters, lowercase letters, numbers tend to cluster together, while important special symbols like \u201c/\u201d and \u201d?\u201d are fairly separated from the rest of the character. The results support our intuition behind inserting the embedding layer to provide a richer n-gram like detection, by clustering semantically similar characters together.\nOur results show that across the board convolution feature extraction outperforms other approaches. For example, at FPR 10\u22123 eXpose has 6% higher\ndetection rate than n-gram or expert derived features, with even larger improvement on file paths and registry keys problems. The fact that our tuned expert features are not able to outperform n-grams is potentially explain by our large dataset size. This is consistent with the well observed fact in the fields of NLP [25] and bioinformatics [21], that a bag of n-grams is a highly effective representation in itself. The fact that the convolutional network is able to exceed n-gram performance in this large dataset setting suggest that embeddings with convolutional networks can be used as powerful automatic feature extractor.\nThe overall results for the file paths and registry keys problem are worse than for URLs. This is not surprising because of the difficulty of properly labeling samples. Recall that our approach was to label any sample that had 0 occurrences in malware data as benign, and everything else was labeled malware. However, estimating if the probability sample being 0 is difficult because typically there is only one observation of the string in the datasets. Furthermore, file paths and registry keys have significantly less training data, which in our experience can significantly decrease our model\u2019s generalizability.\nWhile our CNN model outperform the n-gram model, one potential explanation is that there are too many collisions in the 1024-sized vector caused by feature hashing. Conversely, the vector is too large for the neural network to capture good relationships between the features. Therefore, we have also done the n-gram experiment with 512 and 2048-sized feature vector. These n-gram\nexperiments yielded worse results than the 1024-sized result, and so are not shown.\nWe note that, potentially, extensive re-architecture of the n-gram\u2019s neural network or switching to a different ML approach could yield better results than we presented. Furthermore, feature importance values, as computed using mutual information between label and individual feature vectors, L1 logisticregression, random forest, or related methods can be used to significantly reduce the number of n-gram features that are hashed into the vector, thus also potentially improving results. The downside is that these methods in themselves require significant amount of tuning and multiple passes through a very large dataset. The advantage of our end-to-end learning is that we work directly with raw data, and so can simply utilize the same loading of samples in small batches no matter the architecture. With this approach is no a priori loss of information that is inherent in features engineering, enabling very rapid prototyping."}, {"heading": "5 Conclusions", "text": "We developed and demonstrated the first, to our knowledge, convolutional neural network for extracting automatic features from short string in the context of cybersecurity problems. Using embeddings with convolutions as top layers in our neural network coupled with supervised training, allows us to implicitly extract a feature set that is directly optimized for classification. While similar approaches have been suggested for NLP, eXpose is the first approach that demonstrates how top to bottom deep-learning method can be adapted to several important cybersecurity problems in an adversarial environment, where strings are purposely obfuscated to prevent obvious feature extraction.\nOne of the major issues during our experimentation was the computational cost of training on longer strings, that prevented us from trying more complex architectures. With current advances in hardware and distributed training modules added to modern frameworks, our results can potentially be further improved with some of the more computationally expensive architecture that we were unable to try.\nLooking forward, we hope that ideas integrated into eXpose will help guide the security industry into moving away from expensive feature engineering to directly utilizing already existing labeled datasets for end-to-end learning. As hardware and available datasets improve, the difference between automatically extracted features and traditional feature extraction approaches will only get starker.\nAppendix"}, {"heading": "5.1 Components", "text": "Our convolutional neural network is implemented in Python 2.7 using Keras v1.1 [5]. Below we describe our model\u2019s pre-defined set of components (or layers), which are described in terms of Keras build-in layers documented online:\nEmbedding(s,m) An embedding layer that takes in a list of s integers representing the URL character list (each unique character is mapped to an associated unique integer in the range [1, |\u03a3|]), and outputs a matrix of floating points, where each original scalar integer value is represented by an m dimensional embedding vector. |\u03a3| is the size of the alphabet used to express the URLs. This operation is defined in Keras as Embedding(input dim=s)\nConv(t, k, n) A filter bank of t k-length one dimensional convolution kernels that convolve n adjacent m dimensional vectors, and are immediately followed by a non-linear ReLU activation. Defined in Keras as Convolution1D(t, k, input shape=(s,m)), followed by Activation(relu). Note, we drop m in our notation, since it can be inferred from the previous layer, or otherwise defined in the text.\nBatchNorm Layer-wise batch normalization. Defined in Keras as BatchNormalization(mode=1).\nSumPool Sum of the input along the input length s, such that the output size is k, given the input size (s, k). The operation is defined in Keras as Lambda(f, output shape=(k, )), where f(X) = K.sum(X, axis=1).\nDropout(p) Dropout with probability p [27]. Defined in Keras as Dropout(p).\nMerge A merge operation that takes output from a previous set of layers, {k1, k2, k3, k4}, and concatenates them into a single matrix, [ k1, k2, k3, k4 ]T\n. Defined in Keras as Merge(...,mode = \u201dconcat\u201d).\nDense(l) A fully connected linear unit with output size l, followed by a ReLU non-linear activation. Defined in Keras as Dense(l), followed by Activation(relu).\nDenseSigmoid Last layer used to generate a binary decisions. Same as Dense(1), but followed by a sigmoid (instead of ReLU) activation, defined in Keras as Activation(sigmoid)."}, {"heading": "Acknowledgment", "text": "We would like to thank Richard Harang and Joe Levy for their valuable feedback on early drafts of the manuscript and Hillary Sanders for in-depth discussion of our URL results."}], "references": [{"title": "Detecting malicious software by monitoring anomalous windows registry accesses", "author": ["F. Apap", "A. Honig", "S. Hershkop", "E. Eskin", "S. Stolfo"], "venue": "International Workshop on Recent Advances in Intrusion Detection. pp. 36\u201353. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning internal representations", "author": ["J. Baxter"], "venue": "Proceedings of the eighth annual conference on Computational learning theory. pp. 311\u2013320. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Malicious behavior detection using windows audit logs", "author": ["K. Berlin", "D. Slater", "J. Saxe"], "venue": "Proceedings of the 8th ACM Workshop on Artificial Intelligence and Security. pp. 35\u201344. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask connectionist learning", "author": ["R.A. Caruana"], "venue": "In Proceedings of the 1993 Connectionist Models Summer School. Citeseer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of machine learning techniques used in behavior-based malware detection", "author": ["I. Firdausi", "A. Erwin", "Nugroho", "A.S"], "venue": "Advances in Computing, Control and Telecommunication Technologies (ACT), 2010 Second International Conference on. pp. 201\u2013203. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "European Conference on Computer Vision. pp. 630\u2013645. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Malicious url detection by dynamically mining patterns without pre-defined elements", "author": ["D. Huang", "K. Xu", "J. Pei"], "venue": "World Wide Web 17(6), 1375\u20131394", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Mtnet: a multi-task neural network for dynamic malware classification", "author": ["W. Huang", "J.W. Stokes"], "venue": "Detection of Intrusions and Malware, and Vulnerability Assessment, pp. 399\u2013418. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15). pp. 448\u2013456", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation in linear time", "author": ["N. Kalchbrenner", "L. Espeholt", "K. Simonyan", "Oord", "A.v.d.", "A. Graves", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Phishing detection: a literature survey", "author": ["M. Khonji", "Y. Iraqi", "A. Jones"], "venue": "IEEE Communications Surveys & Tutorials 15(4), 2091\u20132121", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1746\u20131751. Association for Computational Linguistics, Doha, Qatar", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "The International Conference on Learning Representations", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems. pp. 1097\u20131105", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation 1(4), 541\u2013551", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning algorithms for classification: A comparison on handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "C. Cortes", "J.S. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger", "P Simard"], "venue": "Neural networks: the statistical mechanics perspective 261, 276", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Beyond blacklists: learning to detect malicious web sites from suspicious urls", "author": ["J. Ma", "L.K. Saul", "S. Savage", "G.M. Voelker"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 1245\u20131254. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Mash: fast genome and metagenome distance estimation using minhash", "author": ["B.D. Ondov", "T.J. Treangen", "P. Melsted", "A.B. Mallonee", "N.H. Bergman", "S. Koren", "A.M. Phillippy"], "venue": "Genome Biology 17(1), 132", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. pp. 4580\u20134584. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C. dos Santos", "M. Gatti"], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. pp. 69\u201378", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Unitn: Training deep convolutional neural network for twitter sentiment classification", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Association for Computational Linguistics, Denver, Colorado. pp. 464\u2013469", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse non-negative matrix language modeling for skip-grams", "author": ["N. Shazeer", "J. Pelemans", "C. Chelba"], "venue": "Interspeech. pp. 1428\u20131432", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "An empirical analysis of phishing blacklists", "author": ["S. Sheng", "B. Wardman", "G. Warner", "L.F. Cranor", "J. Hong", "C. Zhang"], "venue": "Proceedings of Sixth Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1), 1929\u20131958", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1\u20139", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems. pp. 649\u2013657", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "For example, [3] proposes a logistic regression-based method for detecting malware infections based on n-grams of audit log event observations.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Relatedly, [1] proposes to use an anomaly detection approach on sequences of registry accesses to infer whether a host has been compromised.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "[6] surveys a wide variety of behavioral malware detection techniques, all of which perform manual feature engineering on collections of events to infer whether or not dynamically executed binaries are malicious or benign.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Proposed malicious URL detection approaches have tended to either exclusively use URL strings as their input or utilize both URL strings and supplementary information like website registration services, website content, and network reputation [14].", "startOffset": 243, "endOffset": 247}, {"referenceID": 9, "context": "With respect to the detection mechanism used in previous URL detection work, the simplest proposed approaches have involved blacklists, which can be collected using manual labeling, user feedback, client honeypots, and other heuristics [10].", "startOffset": 236, "endOffset": 240}, {"referenceID": 25, "context": "While blacklists have a very low false positive rate, they are also very brittle and thus cannot generalize to previously unseen URL strings [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "To address these limitations, statistical approaches, such as machine-learning or similarity based URL detection have been proposed [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "Convolutional Neural Networks (CNN) [18,19] have been applied to image recognition problems for a long time, but only fairly recently have they demonstrated breakthrough results in image recognition [17].", "startOffset": 36, "endOffset": 43}, {"referenceID": 18, "context": "Convolutional Neural Networks (CNN) [18,19] have been applied to image recognition problems for a long time, but only fairly recently have they demonstrated breakthrough results in image recognition [17].", "startOffset": 36, "endOffset": 43}, {"referenceID": 16, "context": "Convolutional Neural Networks (CNN) [18,19] have been applied to image recognition problems for a long time, but only fairly recently have they demonstrated breakthrough results in image recognition [17].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "For example, [24] proposes to combine a word embedding approach with a convolutional neural network to perform sentiment analysis on Twitter data.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Other authors propose machine learning approaches that operate on character level embeddings [23,15,30].", "startOffset": 93, "endOffset": 103}, {"referenceID": 14, "context": "Other authors propose machine learning approaches that operate on character level embeddings [23,15,30].", "startOffset": 93, "endOffset": 103}, {"referenceID": 29, "context": "Other authors propose machine learning approaches that operate on character level embeddings [23,15,30].", "startOffset": 93, "endOffset": 103}, {"referenceID": 8, "context": "Recurrent Neural Networks RNNs are commonly used to process sequential information, with LSTM based approaches being among the more popular [9].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "While theoretically they are able to learn long terms dependencies in a sequences, RNNs are problematic to train due to the vanishing gradient problem as well as large computational cost because of the need to sequentially back and forward propagate information during training and prediction phases [8].", "startOffset": 300, "endOffset": 303}, {"referenceID": 12, "context": "Recently CNN have been shown to be just (if not more) effective on sequential data oriented modeling problems, while allowing a significantly faster model training and prediction evaluation [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 3, "context": "There, a set of network layers is shared between various related learning tasks, and an individualized set of final layers is used to make the final prediction on the specific task [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 1, "context": "Sharing internal weights potentially allows the network to learn a richer representation of the data, which is especially useful when individual datasets are very small [2].", "startOffset": 169, "endOffset": 172}, {"referenceID": 10, "context": "Multi-task learning has also been suggested for malware detection by simultaneously trying to predict binary classification as well as malware family [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 4, "context": "1 [5].", "startOffset": 2, "endOffset": 5}, {"referenceID": 14, "context": ") [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "The motivation for using convolutions as our feature extraction component flows from similar approaches in natural language processing (NLP) [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": "than the more popular batch normalization [12], and that putting the normalization after the activation gave equivalent results to putting it before each units activation function.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "We use Adam [16] method to minimize eq.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Alternatives Attempted During the development of our current architecture we have also tried several alternative architectures that did not yield better performance, and/or where computationally too expensive: \u2013 Replacing regular convolutions with a series of dilated convolutions [29,13].", "startOffset": 281, "endOffset": 288}, {"referenceID": 12, "context": "Alternatives Attempted During the development of our current architecture we have also tried several alternative architectures that did not yield better performance, and/or where computationally too expensive: \u2013 Replacing regular convolutions with a series of dilated convolutions [29,13].", "startOffset": 281, "endOffset": 288}, {"referenceID": 21, "context": "\u2013 Stacking embeddings, convolutions, and a long short term memory recurrent layer [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "\u2013 Stacked convolutions to learn non-linear convolutional activations [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "\u2013 ResNet type stacking of convolutions [7]", "startOffset": 39, "endOffset": 42}, {"referenceID": 19, "context": "The second model, used only for URLs, is based on manually extracted features described in [20].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "This is consistent with the well observed fact in the fields of NLP [25] and bioinformatics [21], that a bag of n-grams is a highly effective representation in itself.", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "This is consistent with the well observed fact in the fields of NLP [25] and bioinformatics [21], that a bag of n-grams is a highly effective representation in itself.", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "1 [5].", "startOffset": 2, "endOffset": 5}, {"referenceID": 26, "context": "Dropout(p) Dropout with probability p [27].", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "For years security machine learning research has promised to obviate the need for signature based detection by automatically learning to detect indicators of attack. Unfortunately, this vision hasn\u2019t come to fruition: in fact, developing and maintaining today\u2019s security machine learning systems can require engineering resources that are comparable to that of signature-based detection systems, due in part to the need to develop and continuously tune the \u201cfeatures\u201d these machine learning systems look at as attacks evolve. Deep learning, a subfield of machine learning, promises to change this by operating on raw input signals and automating the process of feature design and extraction. In this paper we propose the eXpose neural network, which uses a deep learning approach we have developed to take generic, raw short character strings as input (a common case for security inputs, which include artifacts like potentially malicious URLs, file paths, named pipes, named mutexes, and registry keys), and learns to simultaneously extract features and classify using character-level embeddings and convolutional neural network. In addition to completely automating the feature design and extraction process, eXpose outperforms manual feature extraction based baselines on all of the intrusion detection problems we tested it on, yielding a 5%10% detection rate gain at 0.1% false positive rate compared to these baselines.", "creator": "LaTeX with hyperref package"}}}