{"id": "1702.08712", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Algorithmic Stability and Hypothesis Complexity", "abstract": "we introduce a notion of algorithmic stability of learning algorithms - - - that we term hypothesis stability - - - that captures stability of the hypothesis output by the constrained learning algorithm in the normed space of functions from which hypotheses are selected. the main result of proving the paper bounds the generalization error of defining any learning algorithm in these terms of its hypothesis stability. the bounds are based on martingale inequalities in defining the minimal banach space to which the hypotheses belong. we also apply the general bounds to bound the error performance of some learning algorithms accurately based on empirical risk minimization and stochastic density gradient descent.", "histories": [["v1", "Tue, 28 Feb 2017 09:39:03 GMT  (12kb)", "http://arxiv.org/abs/1702.08712v1", null], ["v2", "Thu, 3 Aug 2017 11:45:15 GMT  (38kb)", "http://arxiv.org/abs/1702.08712v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tongliang liu", "g\u00e1bor lugosi", "gergely neu", "dacheng tao"], "accepted": true, "id": "1702.08712"}, "pdf": {"name": "1702.08712.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Tongliang Liu", "G\u00e1bor Lugosi", "Gergely Neu", "Dacheng Tao"], "emails": ["tliang.liu@gmail.com,", "dacheng.tao@sydney.edu.au", "bor.lugosi@upf.edu", "gergely.neu@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n08 71\n2v 1\n[ st\nat .M\nL ]\n2 8\nWe introduce a notion of algorithmic stability of learning algorithms\u2014that we term hypothesis stability\u2014that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected.\ne main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. e bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent.\n\u2217Parts of the work were done when Tongliang Liu was a visiting PhD student at Pompeu Fabra University.\n\u2020School of Information Technologies, Faculty Engineering and Information Technologies, University of Sydney, Sydney, Australia, tliang.liu@gmail.com, dacheng.tao@sydney.edu.au\n\u2021Department of Economics and Business, Pompeu Fabra University, Barcelona, Spain, gabor.lugosi@upf.edu\n\u00a7ICREA, Pg. Llus Companys 23, 08010 Barcelona, Spain \u00b6Barcelona Graduate School of Economics \u2225AI group, DTIC, Universitat Pompeu Fabra, Barcelona, Spain, gergely.neu@gmail.com"}, {"heading": "1 Introduction", "text": "Many efforts have beenmade to analyze various notions of algorithmic stability and prove that a broad spectrum of learning algorithms are stable in some sense. Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that \u21132 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., \u2113p regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se ing will therefore be uniformly stable under mild assumptions. e notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se ing.\nIn this paper we study learning algorithms that select a hypothesis (i.e., a function used for prediction) from a certain fixed class of functions belonging to a separable Banach space. We introduce a notion of hypothesis stability which measures the impact of changing a single training example on the hypothesis selected by the learning algorithm. is notion of stability is stronger than uniform algorithmic stability of Bousquet and Elisseeff [2002] that is only concerned about the change in the loss but not the hypothesis itself. However, as we will show, the new notion is still quite natural and holds for a variety of learning algorithms. On the other hand, it allows one to exploit martingale inequalities Boucheron et al. [2013] in the Banach space of the hypotheses. Indeed, the performance bounds we derive for stable algorithms depend on characteristics related to themartingale type of the Banach space.\nGeneralization bounds typically depend on the complexity of a class of hypotheses that can be chosen by the learning algorithm. Exploiting the local estimates of the complexity of the predefined hypothesis class is a promising way to obtain sharp bounds. Building on martingale inequalities in the Banach space of the hypotheses, we define a subset of the predefined hypothesis class, whose elements will (or will have a high probability to) be output by a learning algorithm, as the algorithmic hypothesis class, and study the complex-\nity of the algorithmic hypothesis class of hypothesis-stable learning algorithms. We show that, if the hypotheses belong to a Hilbert space, the upper bound of the Rademacher complexity of the algorithmic hypothesis class will converge at a fast rate of order O(1/n), where n is the sample size. e rest of the paper is organized as follows. Section 2 introduces the mathematical framework and the proposed notion of algorithmic stability. Section 3 presents the main results of this study, namely the generalization bounds in terms of hypothesis stability. Section 4 specializes the results to some learning algorithms, including empirical risk minimization and stochastic gradient descent. Section 5 concludes the paper."}, {"heading": "2 Algorithmic Stability and Hypothesis Class", "text": "We consider the classical statistical learning problem, where the value of a real random variable Y is to be predicted based on the observation of an another random variable X . Let S be a training sample of n i.i.d. pairs of random variables Z1 = (X1, Y1), . . . , Zn = (Xn, Yn) drawn from a fixed distribution P on a setZ = X \u00d7Y , where X is the so-called feature space. A learning algorithm A : S \u2208 Zn 7\u2192 hS \u2208 H is a mapping from Z\nn to a hypothesis class H that we assume to be a subset of a separable Banach space (B, \u2016 \u00b7 \u2016). We focus on linear prediction problems, that is, when h(x) is a linear functional of x. We write h(x) = \u3008h, x\u3009. In other words, we assume that the feature space X is the algebraic dual of the Banach space B. We denote the norm in X by \u2016 \u00b7 \u2016\u2217. e output hS of the learning algorithm is a hypothesis used for predicting the value for Y .\nAn important special case is when B is a Hilbert space. In that case we may assume that X = B and \u3008h, x\u3009 is the inner product in B. e quality of the predictions made by any hypothesis will be measured by a loss function \u2113 : B \u00d7 Z \u2192 R+ (where R+ denotes the set of positive reals). Specifically, \u2113(h, Z) measures the loss of predicting an example Z using a hypothesis h.\ne risk of h \u2208 H is defined by\nR(h) = E\u2113(h, Z) ;\nwhile the empirical risk is\nRS(h) = 1\nn\nn \u2211\ni=1\n\u2113(h, Zi) .\nFor the output hS of a learning algorithm A, the generalization error is defined as\nR(hS)\u2212 RS(hS) . (1)\ne notion of algorithmic stability was proposed to measure the changes of outputs of a learning algorithm when the input is changed. In this paper, we assume that the learning algorithm is symmetric, that is, invariant to permutations of the examples in\nthe training sample. Various ways have been introduced to measure algorithmic stability. Here we recall the notion of uniform stability defined by Bousquet and Elisseeff [2002] for comparison purposes. is notion of stability relies on the altered sample Si = {Z1, . . . , Zi\u22121, Z \u2032 i, Zi+1, . . . , Zn}, the sample S with the i-th example being replaced by an independent copy of Zi.\nDefinition 1 (Uniform Stability). A learning algorithm A is \u03b2(n)-uniformly stable with respect to the loss function \u2113 if for all i \u2208 {1, . . . , n},\n|\u2113(hS, Z)\u2212 \u2113(hSi, Z)| \u2264 \u03b2(n) ,\nwith probability one, where \u03b2(n) \u2208 R+.\nWe propose the following, similar, notion that \u201cacts\u201d on the hypotheses directly, as opposed to the losses.\nDefinition 2 (Uniform Hypothesis Stability). A learning algorithm A is \u03b1(n)-uniformly hypothesis stable if for all i \u2208 {1, . . . , n},\n\u2016hS \u2212 hSi\u2016 \u2264 \u03b1(n) .\nwith probability one, where \u03b1(n) \u2208 R+.\ne two notions of stability are closely related. Indeed, since\n\u2016hS \u2212 hSi\u2016 = sup x\u2208X :\u2016x\u2016\u2217\u22641 (\u3008hS, x\u3009 \u2212 \u3008hSi, x\u3009) ,\nif the loss function is Lipschitz in the sense that |\u2113(h, z)\u2212 \u2113(h\u2032, z)| \u2264 L |\u3008h, x\u3009 \u2212 \u3008h\u2032, x\u3009| for all z \u2208 Z and h, h\u2032 \u2208 H and \u2016X\u2016\u2217 is bounded by some B > 0 with probability one, then an \u03b1(n)-uniformly hypothesis stable learning algorithm is uniformly stable with \u03b2(n) = LB\u03b1(n). However, the reverse implication need not necessarily hold and hence uniform hypothesis stability is a stronger notion. e relationship between hypothesis stability and generalization performance hinges on a property of the Banach space B that is closely related to the martingale type of the space\u2014see Pisier [2011] for a comprehensive account. For concreteness we assume that the Banach space B is (2, D)-smooth (or of martingale type 2) for some D > 0. is means that for all h, h\u2032 \u2208 B,\n\u2016h+ h\u2032\u20162 + \u2016h\u2212 h\u2032\u20162 \u2264 2\u2016h\u20162 + 2D2\u2016h\u2032\u20162 .\nNote that Hilbert spaces are (2, 1)-smooth. e property we need is described in the following result of [Pinelis, 1994]:\nProposition 1. LetD1, . . . , Dn be a martingale difference sequence taking values in a separable (2, D)-smooth Banach space B. en for any \u01eb > 0,\nP\n(\nsup n\u22651\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\nt=1\nDt\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2265 c\u01eb ) \u2264 2 exp ( \u2212 \u01eb2 2D2 ) ,\nwhere c is a constant satisfying that \u2211\u221e t=1 \u2016Dt\u2016 2 \u221e \u2264 c 2 (and \u2016Dt\u2016\u221e is the essential supremum of the random variable \u2016Dt\u2016).\nOur arguments extend, in a straightforward manner, to more general Banach spaces whenever exponential tail inequalities for boundedmartingale sequences similar to Proposition 1 are available. We stay with the assumption of (2, D)-smoothness for convenience and because it applies to the perhaps most important special case when B is a Hilbert space. We refer to Rakhlin and Sridharan [2015] for more information of martingale inequalities of this kind.\nA key property of stable algorithms, implied by the martingale inequality, is that the hypothesis hS output by the algorithm is concentrated\u2014in the Banach space B\u2014around its expectation EhS . is is established in the next simple lemma.\nLemma 1. Let the Banach space B be (2, D)-smooth. If a learning algorithm A is \u03b1(n)uniformly hypothesis stable, then, for any \u03b4 > 0,\nP\n(\n\u2016hS \u2212 EhS\u2016 \u2264 D\u03b1(n) \u221a 2n log(2/\u03b4) ) \u2265 1\u2212 \u03b4 .\nProof. Introduce the martingale differences\nDt = E(hS|Z1, . . . , Zt)\u2212 E(hS|Z1, . . . , Zt\u22121)\nso that\nhS \u2212 EhS =\nn \u2211\nt=1\nDt .\nWe have \u221e \u2211\nt=1\n\u2016Dt\u2016 2 \u221e\n=\nn \u2211\nt=1\n\u2016E(hS|Z1, . . . , Zt)\u2212 E(hS |Z1, . . . , Zt\u22121)\u2016 2 \u221e\n=\nn \u2211\nt=1\n\u2016E(hS \u2212 hSt |Z1, . . . , Zt)\u2016 2 \u221e\n\u2264\nn \u2211\nt=1\n(E(\u2016(hS \u2212 hSt\u2016\u221e|Z1, . . . , Zt)) 2\n\u2264 n\u03b1(n)2 .\nus, by Proposition 1, we have\nP\n(\n\u2016hS \u2212 EShS\u2016 \u2265 \u03b1(n)D \u221a 2n log(2/\u03b4) ) \u2264 \u03b4\nfor \u03b4 = 2 exp ( \u2212 \u01eb 2\n2D2\n)\n."}, {"heading": "3 Algorithmic Rademacher Complexity and General-", "text": "ization Bound\ne concentration result of Lemma 1 justifies the following definition of the \u201calgorithmic hypothesis class\u201d: sincewith high probability hS concentrates around its expectationEhS , what ma ers in the generalization performance of the algorithm is the complexity of the ball centered at EhS and not that of the entire hypothesis class H . is observation may lead to significantly improved performance guarantees.\nDefinition 3 (AlgorithmicHypothesis Class). For a sample size n and confidence parameter \u03b4 > 0, let r = r(n, \u03b4) = D\u03b1(n) \u221a\n2n log(2/\u03b4) and define the algorithmic hypothesis class of a stable learning algorithm by\nBr = {h \u2208 H| \u2016h\u2212 EhS\u2016 \u2264 r(n, \u03b4)} .\nNote that, by Lemma 1, hS \u2208 Br with probability at least 1\u2212 \u03b4. We bound the generalization error (1) in terms of the Rademacher complexity Bartle and Mendelson\n[2003] of the algorithmic hypothesis class. e Rademacher complexity of a hypothesis classH on the feature space X is defined as\nR(H) = E sup h\u2208H\n1\nn\nn \u2211\ni=1\n\u03c3i\u3008h,Xi\u3009 ,\nwhere\u03c31, . . . , \u03c3n are i.i.d. Rademacher variables that are uniformly distributed in {\u22121,+1}. e next theorem shows how the Rademacher complexity of the algorithmic hypothesis class can be bounded. e bound depends on the type of the feature space X . Recall that the Banach space (X , \u2016 \u00b7 \u2016\u2217) is of type p \u2265 1 if there exists a constant Cp such that for all x1, . . . , xn \u2208 X ,\nE\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\n\u03c3ixi\n\u2225 \u2225 \u2225 \u2225 \u2225\n\u2217\n\u2264 Cp\n(\nn \u2211\ni=1\n\u2016xi\u2016 p \u2217\n)1/p\n.\nIn the important special casewhenX is a Hilbert space, the space is of type 2with constant C2 = 1.\neorem 1. Assume thatB is a (2, D)-smooth Banach space and that its dual X is of type p. Suppose that the marginal distribution of theXi is such that \u2016Xi\u2016\u2217 \u2264 B with probability one, for some B > 0. If a learning algorithm is \u03b1(n)-uniformly hypothesis stable, then the Rademacher complexity of the algorithmic hypothesis class Br on the feature space satisfies\nR(Br) \u2264 DCpB \u221a 2 log(2/\u03b4)\u03b1(n)n\u22121/2+1/p .\nIn particular, whenB is a Hilbert space, the bound simplifies to\nR(Br) \u2264 B \u221a 2 log(2/\u03b4)\u03b1(n) .\nProof. We have\nR(Br)\n= E sup h\u2208Br\n1\nn\nn \u2211\ni=1\n\u03c3i\u3008h,Xi\u3009\n= E sup h\u2208Br\n1\nn\nn \u2211\ni=1\n(\u03c3i\u3008h,Xi\u3009\n\u2212\u03c3iE\u3008hS, Xi\u3009+ \u03c3iE\u3008hS , Xi\u3009)\n= E sup h\u2208Br\n1\nn\nn \u2211\ni=1\n\u03c3i(\u3008h,Xi\u3009 \u2212 E\u3008hS, Xi\u3009)\n= E sup h\u2208Br\n1\nn\nn \u2211\ni=1\n\u03c3i \u3008h\u2212 EhS , Xi\u3009\n\u2264 E sup h\u2208Br\n1 n \u2016h\u2212 EhS\u2016\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\n\u03c3iXi\n\u2225 \u2225 \u2225 \u2225 \u2225\n\u2217\n\u2264 r\nn E\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\n\u03c3iXi\n\u2225 \u2225 \u2225 \u2225 \u2225\n\u2217\n\u2264 1\nn \u03b1(n)D\n\u221a\n2n log(2/\u03b4)Cp\n(\nn \u2211\ni=1\n\u2016Xi\u2016 p \u2217\n)1/p\n\u2264 DCpB \u221a 2 log(2/\u03b4)\u03b1(n)n\u22121/2+1/p ,\nconcluding the proof.\ne theorem abovemay be easily used to bound the performance of an\u03b1(n)-uniformly hypothesis stable learning algorithm. For simplicity, we state the result for Hilbert spaces only. e extension to (2, D)-smooth Banach spaceswith a type-p dual is straightforward.\nCorollary 1. Assume that B is a separable Hilbert space. Suppose that the marginal distribution of the Xi is such that \u2016Xi\u2016\u2217 \u2264 B with probability one, for some B > 0 and that the loss function is bounded and Lipschitz, that is, \u2113(h, Z) \u2264 M with probability one for some M > 0 and |\u2113(h, z)\u2212 \u2113(h\u2032, z)| \u2264 L |\u3008h, x\u3009 \u2212 \u3008h\u2032, x\u3009| for all z \u2208 Z and h, h\u2032 \u2208 H . If a learning algorithm is \u03b1(n)-uniformly hypothesis stable, then its generalization error is bounded as follows. With probability at least 1\u2212 2\u03b4,\nR(hS)\u2212 RS(hS) \u2264 2LB \u221a 2 log(2/\u03b4)\u03b1(n) +M\n\u221a\nlog(1/\u03b4)\n2n .\nProof. Note first that, by Lemma 1, with probability at least 1\u2212 \u03b4,\nR(hS)\u2212 RS(hS) \u2264 sup h\u2208Br (R(h)\u2212 RS(h)) .\nOn the other hand, by the boundedness of the loss function, and the bounded differences inequality, with probability at least 1\u2212 \u03b4,\nsup h\u2208Br (R(h)\u2212RS(h))\n\u2264 E sup h\u2208Br (R(h)\u2212 RS(h)) +M\n\u221a\nlog(1/\u03b4)\n2n\n\u2264 2R(\u2113 \u25e6Br) +M\n\u221a\nlog(1/\u03b4)\n2n ,\nwhere \u2113 \u25e6H denotes the set of compositions of functions \u2113 and h \u2208 H . By the Lipschitz property of the loss function and a standard contraction argument, we have [Ledoux and Talagrand, 2013],\nR(\u2113 \u25e6Br) \u2264 L \u00b7R(Br) \u2264 LB \u221a 2 log(2/\u03b4)\u03b1(n) .\nNote that the order of magnitude of\u03b1(n) of many stable algorithms is of orderO(1/n). For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017]. As we will show in the examples below, many of these learning algorithms even have uniform hypothesis stability of order O(1/n). In such cases the bound of Corollary 1 is essentially equivalent of the earlier results cited above. e bound is dominated by the termM \u221a\nlog(1/\u03b4) 2n present by using the bounded differences inequality. Fluctuations of the\norder ofO(n\u22121/2) are o en inevitable, especiallywhenR(hS) is not typically small. When small risk is reasonable to expect, one may use more advanced concentration inequalities with second-moment information, at the price of replacing the generalization error by the so-called \u201cdeformed\u201d generalization error R(hS)\u2212 a a\u22121\nRS(hS) where a > 1. e next theorem derives such a bound, relying on techniques developed by Bartle et al. [2005]. is result improves essentially on earlier stability-based bounds.\neorem 2. Assume that B is a separable Hilbert space. Suppose that the marginal distribution of the Xi is such that \u2016Xi\u2016\u2217 \u2264 B with probability one, for some B > 0 and that the loss function is bounded and Lipschitz, that is, \u2113(h, Z) \u2264 M with probability one for some M > 0 and |\u2113(h, z)\u2212 \u2113(h\u2032, z)| \u2264 L |\u3008h, x\u3009 \u2212 \u3008h\u2032, x\u3009| for all z \u2208 Z and h, h\u2032 \u2208 H . Let a > 1. If a learning algorithm is \u03b1(n)-uniformly hypothesis stable, then, with probability at least 1\u2212 2\u03b4,\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2264 8LB\n\u221a 2 log(2/\u03b4)\u03b1(n) + (6a+ 8)M log(1/\u03b4)\n3n .\ne proof of eorem 2 relies on techniques developed by Bartle et al. [2005]. In particular, we make use of the following result.\nProposition 2. [Bartle et al., 2005, eorem 2.1]. Let F be a class of functions that map X into [0,M ]. Assume that there is some \u03c1 > 0 such that for every f \u2208 F , var(f(X)) \u2264 \u03c1. en, with probability at least 1\u2212 \u03b4, we have\nsup f\u2208F\n(\nEf(X)\u2212 1\nn\nn \u2211\ni=1\nf(Xi)\n)\n\u2264\n(\n4R(F ) +\n\u221a\n2\u03c1 log(1/\u03b4)\nn +\n4M\n3\nlog(1/\u03b4)\nn\n)\n.\nTo prove the theorem, we also need to introduce the following auxiliary lemma. Define\nGr(Z) =\n{\nr\nmax{r,E\u2113(h, Z)} \u2113(h, Z)|h \u2208 Br\n}\n.\nIt is evident that Gr \u2286 {\u03b1h|h \u2208 Br, \u03b1 \u2208 [0, 1]}. e following lemma is proven in [Bartle et al., 2005].\nLemma 2. Define\nVr = sup g\u2208Gr\n(\nEg(Z)\u2212 1\nn\nn \u2211\ni=1\ng(Zi)\n)\n.\nFor any r > 0 and a > 1, if Vr \u2264 r/a then every h \u2208 Br satisfies\nE\u2113(h, Z) \u2264 a\na\u2212 1\n1\nn\nn \u2211\ni=1\n\u2113(h, Zi) + Vr.\nNow, we are ready to prove eorem 2.\nProof of eorem 2. First, we introduce an inequality to build the connection between al-\ngorithmic stability and hypothesis complexity. For any a > 1, we have\nP\n(\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2265 r\n)\n= P(hS \u2208 Br)P\n(\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2265 r|hS \u2208 Br\n)\n+ P(hS 6\u2208 Br)P\n(\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2265 r|hS 6\u2208 Br\n)\n\u2264 P(hS \u2208 Br)P\n(\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2265 r|hS \u2208 Br\n)\n+ P(hS 6\u2208 Br)\n\u2264 P\n(\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2265 r, hS \u2208 Br\n)\n+ P(hS 6\u2208 Br)\n\u2264 P\n(\nsup h\u2208Br\n(R(h)\u2212 a\na\u2212 1 RS(h)) \u2265 r\n)\n+ \u03b4 . (2)\ne last inequality holds because of Lemma 1. Second, we are going to upper bound the term suph\u2208Br(R(h)\u2212 a a\u22121\nRS(h)) with high probability. It is easy to check that for any g \u2208 Gr , Eg(Z) \u2264 r and g(Z) \u2208 [0,M ]. en\nvar(g(Z)) \u2264 E(g(Z))2 \u2264 MEg(Z) \u2264 Mr.\nApplying Proposition 2,\nVr \u2264 4R(Gr) +\n\u221a\n2Mr log(1/\u03b4)\nn +\n4M\n3\nlog(1/\u03b4)\nn .\nLet\n4R(Gr) +\n\u221a\n2Mr log(1/\u03b4)\nn +\n4M\n3\nlog(1/\u03b4)\nn =\nr a .\nWe have\nr \u2264 2Ma2 log(1/\u03b4)\nn + 8aR(Gr) +\n4\n3\n2aM log(1/\u03b4)\nn ,\nwhich means that there exists an r\u2217 \u2264 2Ma 2 log(1/\u03b4) n + 8aR(Gr) + 4 3 2aM log(1/\u03b4) n such that Vr\u2217 \u2264 r \u2217/a holds. According to Lemma 2, for any h \u2208 Br, with probability at least 1\u2212 \u03b4,\nwe have\nE\u2113(h, Z) \u2264 a\na\u2212 1\n1\nn\nn \u2211\ni=1\n\u2113(h, Zi) + Vr\u2217\n\u2264 a\na\u2212 1\n1\nn\nn \u2211\ni=1\n\u2113(h, Zi) + r\u2217\na\n\u2264 a\na\u2212 1\n1\nn\nn \u2211\ni=1\n\u2113(h, Zi) + 2Ma log(1/\u03b4)\nn + 8R(Gr) +\n4\n3\n2M log(1/\u03b4)\nn .\nIt is easy to verify that Gr \u2286 {\u03b1h|h \u2208 Br, \u03b1 \u2208 [0, 1]} \u2286 convBr. By elementary properties of the Rademacher complexity (see, e.g., Bartle and Mendelson\n[2003]), H \u2032 \u2282 H impliesR(H \u2032) \u2264 R(H). en, with probability at least 1\u2212 \u03b4, we have\nsup h\u2208Br\n(\nE\u2113(h,X)\u2212 a\na\u2212 1\n1\nn\nn \u2211\ni=1\n\u2113(h,Xi)\n)\n\u2264 2Ma log(1/\u03b4)\nn + 8R(\u2113 \u25e6Br) +\n4\n3\n2M log(1/\u03b4)\nn .\ne proof of eorem 2 is complete by combining the above inequality with eorem 1 and inequality (2).\nIn the next section, we specialize the above results to some learning algorithms by proving their uniform hypothesis stability."}, {"heading": "4 Applications", "text": "Various learning algorithms have been proved to possess some kind of stability. We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017] for such examples, including stochastic gradient descent methods, empirical risk minimization, and non-parametric learning algorithms such as k-nearest neighbor rules and kernel regression."}, {"heading": "4.1 Empirical Risk Minimization", "text": "Regularized empirical riskminimization has beenknown to be uniformly stable [Bousquet and Elisseeff, 2002]. Here we consider regularized empirical risk minimization (RERM) algorithms of the following form. e empirical risk (or the objective function) of RERM is formulated as\nRS,\u03bb(h) = 1\nn\nn \u2211\ni=1\n\u2113(h,Xi) + \u03bbN(h),\nwhere N : h \u2208 H 7\u2192 N(h) \u2208 R+ is a convex function. Its corresponding expected counterpart is defined as\nR\u03bb(h) = E\u2113(h,X) + \u03bbN(h).\nBousquet and Elisseeff [2002] proved that \u21132-regularized learning algorithms are \u03b2(n)uniformly stable. Wibisono et al. [2009] extended the result and studied a sufficient condition of the penalty term N(h) to ensure uniform \u03b2(n)-stability. As we now show, both of their proof methods are applicable to the analysis of uniform hypothesis stability.\nBy exploiting their results, we show that stable RERM algorithms have strong generalization properties.\neorem 3. Assume that B is a separable Hilbert space. Suppose that the marginal distribution of the Xi is such that \u2016Xi\u2016\u2217 \u2264 B with probability one, for some B > 0 and that the loss function is convex in h, bounded byM and L-Lipschitz. Suppose that for some constants C and \u03be > 1, the penalty function N(h) satisfies\nN(hS) +N(hSi)\u2212 2N\n(\nhS + hSi\n2\n)\n\u2265 C\u2016hS \u2212 hSi\u2016 \u03be. (3)\nen, for any \u03b4 > 0, and a > 1, if hS is the output of RERM, with probability at least 1\u2212 2\u03b4, we have\nR(hS)\u2212 a\na\u2212 1 RS(hS) \u2264 8LB\n(\nLB\nC\u03bbn\n) 1\n\u03be\u22121 \u221a 2 log(2/\u03b4) + (6a+ 8)M log(1/\u03b4)\n3n .\nSpecifically, when N(h) = \u2016h\u20162, (3) holds with \u03be = 2 and C = 1 2\n(\nM \u03bb\n) 1\n2 .\nProof. e proof of eorem 3 relies on the following result implied by Wibisono et al. [2009].\nProposition 3. Assume the conditions of eorem 3. en the RERM learning algorithm is \u03b2(n)-uniformly stable with\n\u03b2(n) =\n(\nk\u03beL\u03be\nC\u03bbn\n)\n1\n\u03be\u22121\n,\nand is \u03b1(n)-uniformly hypothesis stable with\n\u03b1(n) =\n(\nkL\nC\u03bbn\n) 1\n\u03be\u22121\n.\nSpecifically, when N(h) = \u2016h\u2016pp and 1 < p \u2264 2, the condition 3 on the penalty function holds with \u03be = 2 and C = 1 4 p(p \u2212 1) ( M \u03bb ) p\u22121 p , where \u2016h\u2016pp = \u2211 r |hr| p and r is the index for the dimensionality.\neorem 3 follows by combining eorem 2 and Proposition 3."}, {"heading": "4.2 Stochastic Gradient Descent", "text": "Stochastic gradient descent (SGD) is one of the most widely used optimization methods in machine learning. Hardt et al. [2015] showed that parametric models trained by SGD methods are uniformly stable. eir results apply to both convex and non-convex learning problems and provide insights for why SGD performs well in practice, in particular, for deep learning algorithms. eir results are based on the assumptions that the loss function employed is both Lipschitz and smooth. In order to avoid technicalities of defining derivatives in general Hilbert spaces, in this section we assume thatB = X = Rd, the d-dimensional Euclidean space.\nDefinition 4. A differentiable loss function \u2113(h, \u00b7) is s-smooth if for all h, h\u2032 \u2208 H , we have\n\u2016\u2207h\u2113(h, \u00b7)\u2212\u2207h\u2032\u2113(h \u2032, \u00b7)\u2016 \u2264 s\u2016h\u2212 h\u2032\u2016,\nwhere\u2207xf(x) denotes the derivative of f(x) with respect to x and s > 0.\nDefinition 5. A differentiable loss function \u2113(h, \u00b7) is \u03b3-strongly convex with respect to \u2016 \u00b7 \u2016 if for all h, h\u2032 \u2208 H , we have\n(\u2207h\u2113(h, \u00b7)\u2212\u2207h\u2032\u2113(h \u2032, \u00b7))T (h\u2212 h\u2032) \u2265 \u03b3\u2016h\u2212 h\u2032\u20162,\nwhere \u03b3 > 0.\neorem 4. Let the stochastic gradient update rule be given by ht+1 = ht\u2212\u03b1t\u2207h\u2113(ht, Xit), where \u03b1t > 0 is the learning rate and it is the index for choosing one example for the t-th update. Let hT and h i T denote the outputs of SGD run on sample S and S i, respectively. Assume that \u2016X\u2016\u2217 \u2264 B with probability one. Suppose that the loss function is L-Lipschitz, s-smooth, and upper bounded by M . Let SGD is run with a monotonically non-increasing step size \u03b1t \u2264 c/t, where c is a universal constant, for T steps. en, for any \u03b4 > 0 and a > 1, with probability at least 1\u2212 2\u03b4, we have\nR(hT )\u2212 a\na\u2212 1 RS(hT )\n\u2264 8BL 1 + 1/sc\nn\u2212 1 (2cBL)\n1 sc+1T sc sc+1 \u221a 2 log(2/\u03b4) + (6a+ 8)M log(1/\u03b4)\n3n .\nWhen the loss function \u2113 is convex, L-admissible, s-smooth, and upper bounded by M , suppose that SGD is run with step sizes \u03b1t \u2264 2/s for T steps. en, for any \u03b4 > 0 and a > 1, with probability at least 1\u2212 2\u03b4,\nR(hT )\u2212 a\na\u2212 1 RS(hT ) \u2264\n16B2L2\nn\nT \u2211\nt=1\n\u03b1t \u221a 2 log(2/\u03b4) + (6a+ 8)M log(1/\u03b4)\n3n .\nMoreover, when the loss function \u2113 is \u03b3-strongly convex, s-smooth, and upper bounded by M , let the stochastic gradient update be given by ht+1 = \u03a0\u2126(ht\u2212\u03b1t\u2207h\u2113(ht, Xit)), where\u2126\nis a compact, convex set over which we wish to optimize and \u03a0\u2126(\u00b7) is a projection such that \u03a0\u2126(f) = argminh\u2208H \u2016h\u2212 f\u2016. If the loss function is further L-Lipschitz over the set \u2126 and the projected SGD is run with a constant step size \u03b1 \u2264 1/s for T steps. en, for any \u03b4 > 0 and a > 1, with probability at least 1\u2212 2\u03b4, the projected SGD satisfies that\nR(hT )\u2212 a\na\u2212 1 RS(hT ) \u2264\n16DB2L2\n\u03b3n\n\u221a 2 log(2/\u03b4) + (6a+ 8)M log(1/\u03b4)\n3n .\nNote that the \u21132 regularized convex loss function can be regarded as a strongly convex loss function. Bousquet and Elisseeff [2002] studied the stability of batch methods. When the loss function is strongly convex, the stability of SGD is consistent with the result in [Bousquet and Elisseeff, 2002]. eorem 4 implies that SGD generalizeswell with an early stop. is partially explains why deep learning algorithms employing SGD with a certain small number of iterations perform well. However, early stopped SGD may have a large empirical risk RS(hT ). e proof of eorem 4 follows immediately from eorem 2, combined with the following result implied by Hardt et al. [2015] (which is a collection of the results of eorems 3.8, 3.9, and 3.12 therein).\nProposition 4. Let the stochastic gradient update be given by ht+1 = ht \u2212 \u03b1t\u2207h\u2113(ht, Zit), where \u03b1t > 0 is the learning rate and it is the index for choosing one example for the t-th update. Let hT and h i T denote the outputs of SGD running on sample S and S i respectively. When the loss function is L-Lipschitz and s-smooth, suppose that SGD is run with monotonically non-increasing step size \u03b1t \u2264 c/t, where c is a universal constant, for T steps. en,\n\u2016hT \u2212 h i T\u2016 \u2264\n1 + 1/sc\nn\u2212 1 (2cBL)\n1 sc+1T sc sc+1 .\nWhen the loss function \u2113 is convex, L-Lipschitz, and s-smooth, suppose that SGD is run with step sizes \u03b1t \u2264 2/s for T steps. en,\n\u2016hT \u2212 h i T \u2016 \u2264\n2BL\nn\nT \u2211\nt=1\n\u03b1t.\nMoreover, when the loss function \u2113 is \u03b3-strongly convex and s-smooth, let the stochastic gradient update be given byht+1 = \u03a0\u2126(ht\u2212\u03b1t\u2207h\u2113(ht, Zit)), where\u2126 is a compact, convex set over which we wish to optimize and\u03a0\u2126(\u00b7) is a projection such that\u03a0\u2126(f) = argminh\u2208H \u2016h\u2212f\u2016. If the loss function is L-Lipschitz over the set \u2126 and the projected SGD is run with constant step size \u03b1 \u2264 1/s for T steps. en, e projected SGD satisfies algorithmic hypothesis stability with\n\u2016hT \u2212 h i T\u2016 \u2264\n2BL\n\u03b3n ."}, {"heading": "5 Conlusion", "text": "We introduced the concepts of uniform hypothesis stability and algorithmic hypothesis class, defined as the class of hypotheses that are likely to be output by the learning algorithm. We proposed a general probabilistic framework to exploit local estimates for the complexity of hypothesis class to obtain fast convergence rates for stable learning algorithms. Specifically, we defined the algorithmic hypothesis class by observing that the output of stable learning algorithms concentrates aroundEhS . e Rademacher complexity defined on the algorithmic hypothesis class then converges at the same rate as that of the uniform hypothesis stability in Hilbert space, which are of order O(1/n) for various learning algorithms, such as empirical risk minimization and stochastic gradient descent. We derived fast convergence rates of order O(1/n) for their deformed generalization errors. Unlike previously published guarantees of similar flavor, our bounds hold with high probability, rather than only in expectation.\nOur study leaves some open problems and allows several possible extensions. First, the algorithmic hypothesis class defined in this study depends mainly on the property of learning algorithms but li le on the data distribution. It would be interesting to investigate a way to define an algorithmic hypothesis class by considering both the algorithmic property and the data distribution. Second, it would be interesting to explore if there are some algorithmic properties other than stability that could result in a small algorithmic hypothesis class."}], "references": [{"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartle", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartle\u008a and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartle\u008a and Mendelson.", "year": 2003}, {"title": "Local rademacher complexities", "author": ["Peter L Bartle", "Olivier Bousquet", "ShaharMendelson"], "venue": "Annals of Statistics,", "citeRegEx": "Bartle\u008a et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartle\u008a et al\\.", "year": 2005}, {"title": "Perturbation analysis of optimization problems", "author": ["J Fr\u00e9d\u00e9ric Bonnans", "Alexander Shapiro"], "venue": "Springer Science & Business Media,", "citeRegEx": "Bonnans and Shapiro.,? \\Q2013\\E", "shortCiteRegEx": "Bonnans and Shapiro.", "year": 2013}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "OUP Oxford,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Distribution-free inequalities for the deleted and holdout error estimates. Information \u008aeory", "author": ["Luc Devroye", "Terry J Wagner"], "venue": "IEEE Transactions on,", "citeRegEx": "Devroye and Wagner.,? \\Q1979\\E", "shortCiteRegEx": "Devroye and Wagner.", "year": 1979}, {"title": "Train faster, generalize be\u008aer: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Probability in Banach Spaces: isoperimetry and processes", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ledoux and Talagrand.,? \\Q2013\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 2013}, {"title": "Algorithm-dependent generalization bounds for multi-task learning", "author": ["Tongliang Liu", "DachengTao", "Mingli Song", "Stephen J. Maybank"], "venue": "IEEE Transactions on Pa\u0088ern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "On the posterior-probability estimate of the error rate of nonparametric classification rules. Information \u008aeory", "author": ["G\u00e1bor Lugosi", "Miroslaw Pawlak"], "venue": "IEEE Transactions on,", "citeRegEx": "Lugosi and Pawlak.,? \\Q1994\\E", "shortCiteRegEx": "Lugosi and Pawlak.", "year": 1994}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Ri\u0088in"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Mukherjee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2006}, {"title": "Optimum bounds for the distributions of martingales in Banach spaces", "author": ["Iosif Pinelis"], "venue": "\u008ae Annals of Probability,", "citeRegEx": "Pinelis.,? \\Q1994\\E", "shortCiteRegEx": "Pinelis.", "year": 1994}, {"title": "Martingales in Banach spaces (in connection with type and cotype)", "author": ["Gilles Pisier"], "venue": "IHP course notes,", "citeRegEx": "Pisier.,? \\Q2011\\E", "shortCiteRegEx": "Pisier.", "year": 2011}, {"title": "On equivalence of martingale tail bounds and deterministic regret inequalities", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1510.03925,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2015\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2015}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "Journal ofMachine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Sufficient conditions for uniform stability of regularization algorithms", "author": ["Andre Wibisono", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "Techincal Report MIT-CSAIL-TR-2009-060,", "citeRegEx": "Wibisono et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wibisono et al\\.", "year": 2009}, {"title": "Leave-one-out bounds for kernel methods", "author": ["Tong Zhang"], "venue": "Neural Computation,", "citeRegEx": "Zhang.,? \\Q2003\\E", "shortCiteRegEx": "Zhang.", "year": 2003}], "referenceMentions": [{"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013].", "startOffset": 221, "endOffset": 248}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 289}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 315}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 333}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 430}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.", "startOffset": 222, "endOffset": 522}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al.", "startOffset": 222, "endOffset": 776}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions.", "startOffset": 222, "endOffset": 900}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al.", "startOffset": 222, "endOffset": 1381}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se\u008aing.", "startOffset": 222, "endOffset": 1599}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se\u008aing. In this paper we study learning algorithms that select a hypothesis (i.e., a function used for prediction) from a certain fixed class of functions belonging to a separable Banach space. We introduce a notion of hypothesis stability which measures the impact of changing a single training example on the hypothesis selected by the learning algorithm. \u008cis notion of stability is stronger than uniform algorithmic stability of Bousquet and Elisseeff [2002] that is only concerned about the change in the loss but not the hypothesis itself.", "startOffset": 222, "endOffset": 2270}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se\u008aing. In this paper we study learning algorithms that select a hypothesis (i.e., a function used for prediction) from a certain fixed class of functions belonging to a separable Banach space. We introduce a notion of hypothesis stability which measures the impact of changing a single training example on the hypothesis selected by the learning algorithm. \u008cis notion of stability is stronger than uniform algorithmic stability of Bousquet and Elisseeff [2002] that is only concerned about the change in the loss but not the hypothesis itself. However, as we will show, the new notion is still quite natural and holds for a variety of learning algorithms. On the other hand, it allows one to exploit martingale inequalities Boucheron et al. [2013] in the Banach space of the hypotheses.", "startOffset": 222, "endOffset": 2557}, {"referenceID": 11, "context": "\u008ce property we need is described in the following result of [Pinelis, 1994]:", "startOffset": 60, "endOffset": 75}, {"referenceID": 4, "context": "Here we recall the notion of uniform stability defined by Bousquet and Elisseeff [2002] for comparison purposes.", "startOffset": 58, "endOffset": 88}, {"referenceID": 4, "context": "Here we recall the notion of uniform stability defined by Bousquet and Elisseeff [2002] for comparison purposes. \u008cis notion of stability relies on the altered sample S = {Z1, . . . , Zi\u22121, Z \u2032 i, Zi+1, . . . , Zn}, the sample S with the i-th example being replaced by an independent copy of Zi. Definition 1 (Uniform Stability). A learning algorithm A is \u03b2(n)-uniformly stable with respect to the loss function l if for all i \u2208 {1, . . . , n}, |l(hS, Z)\u2212 l(hSi, Z)| \u2264 \u03b2(n) , with probability one, where \u03b2(n) \u2208 R+. We propose the following, similar, notion that \u201cacts\u201d on the hypotheses directly, as opposed to the losses. Definition 2 (Uniform Hypothesis Stability). A learning algorithm A is \u03b1(n)-uniformly hypothesis stable if for all i \u2208 {1, . . . , n}, \u2016hS \u2212 hSi\u2016 \u2264 \u03b1(n) . with probability one, where \u03b1(n) \u2208 R+. \u008ce two notions of stability are closely related. Indeed, since \u2016hS \u2212 hSi\u2016 = sup x\u2208X :\u2016x\u2016\u2217\u22641 (\u3008hS, x\u3009 \u2212 \u3008hSi, x\u3009) , if the loss function is Lipschitz in the sense that |l(h, z)\u2212 l(h, z)| \u2264 L |\u3008h, x\u3009 \u2212 \u3008h, x\u3009| for all z \u2208 Z and h, h \u2208 H and \u2016X\u2016\u2217 is bounded by some B > 0 with probability one, then an \u03b1(n)-uniformly hypothesis stable learning algorithm is uniformly stable with \u03b2(n) = LB\u03b1(n). However, the reverse implication need not necessarily hold and hence uniform hypothesis stability is a stronger notion. \u008ce relationship between hypothesis stability and generalization performance hinges on a property of the Banach space B that is closely related to the martingale type of the space\u2014see Pisier [2011] for a comprehensive account.", "startOffset": 58, "endOffset": 1524}, {"referenceID": 13, "context": "We refer to Rakhlin and Sridharan [2015] for more information of martingale inequalities of this kind.", "startOffset": 12, "endOffset": 41}, {"referenceID": 0, "context": "We bound the generalization error (1) in terms of the Rademacher complexity Bartle\u008a and Mendelson [2003] of the algorithmic hypothesis class.", "startOffset": 76, "endOffset": 105}, {"referenceID": 7, "context": "By the Lipschitz property of the loss function and a standard contraction argument, we have [Ledoux and Talagrand, 2013], R(l \u25e6Br) \u2264 L \u00b7R(Br) \u2264 LB \u221a", "startOffset": 92, "endOffset": 120}, {"referenceID": 6, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al.", "startOffset": 59, "endOffset": 84}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al.", "startOffset": 85, "endOffset": 115}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al.", "startOffset": 85, "endOffset": 139}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al. [2015], Liu et al.", "startOffset": 85, "endOffset": 160}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017]. As we will show in the examples below, many of these learning algorithms even have uniform hypothesis stability of order O(1/n).", "startOffset": 85, "endOffset": 179}, {"referenceID": 1, "context": "\u008ce next theorem derives such a bound, relying on techniques developed by Bartle\u008a et al. [2005]. \u008cis result improves essentially on earlier stability-based bounds.", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "\u008ce proof of \u008ceorem 2 relies on techniques developed by Bartle\u008a et al. [2005]. In particular, we make use of the following result.", "startOffset": 55, "endOffset": 77}, {"referenceID": 1, "context": "\u008ce following lemma is proven in [Bartle\u008a et al., 2005].", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": ", Bartle\u008a and Mendelson [2003]), H \u2032 \u2282 H impliesR(H ) \u2264 R(H).", "startOffset": 2, "endOffset": 31}, {"referenceID": 4, "context": "1 Empirical Risk Minimization Regularized empirical riskminimization has beenknown to be uniformly stable [Bousquet and Elisseeff, 2002].", "startOffset": 106, "endOffset": 136}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 22, "endOffset": 48}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 22, "endOffset": 74}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 75, "endOffset": 105}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 75, "endOffset": 119}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al. [2015], Liu et al.", "startOffset": 75, "endOffset": 164}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017] for such examples, including stochastic gradient descent methods, empirical risk minimization, and non-parametric learning algorithms such as k-nearest neighbor rules and kernel regression.", "startOffset": 75, "endOffset": 183}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] proved that l2-regularized learning algorithms are \u03b2(n)uniformly stable.", "startOffset": 0, "endOffset": 30}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] proved that l2-regularized learning algorithms are \u03b2(n)uniformly stable. Wibisono et al. [2009] extended the result and studied a sufficient condition of the penalty term N(h) to ensure uniform \u03b2(n)-stability.", "startOffset": 0, "endOffset": 126}, {"referenceID": 15, "context": "\u008ce proof of \u008ceorem 3 relies on the following result implied by Wibisono et al. [2009]. Proposition 3.", "startOffset": 63, "endOffset": 86}, {"referenceID": 6, "context": "Hardt et al. [2015] showed that parametric models trained by SGD methods are uniformly stable.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "When the loss function is strongly convex, the stability of SGD is consistent with the result in [Bousquet and Elisseeff, 2002].", "startOffset": 97, "endOffset": 127}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] studied the stability of batch methods.", "startOffset": 0, "endOffset": 30}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] studied the stability of batch methods. When the loss function is strongly convex, the stability of SGD is consistent with the result in [Bousquet and Elisseeff, 2002]. \u008ceorem 4 implies that SGD generalizeswell with an early stop. \u008cis partially explains why deep learning algorithms employing SGD with a certain small number of iterations perform well. However, early stopped SGD may have a large empirical risk RS(hT ). \u008ce proof of \u008ceorem 4 follows immediately from \u008ceorem 2, combined with the following result implied by Hardt et al. [2015] (which is a collection of the results of \u008ceorems 3.", "startOffset": 0, "endOffset": 573}], "year": 2017, "abstractText": "We introduce a notion of algorithmic stability of learning algorithms\u2014that we term hypothesis stability\u2014that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. \u008ce main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. \u008ce bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent. Parts of the work were done when Tongliang Liu was a visiting PhD student at Pompeu Fabra University. School of Information Technologies, Faculty Engineering and Information Technologies, University of Sydney, Sydney, Australia, tliang.liu@gmail.com, dacheng.tao@sydney.edu.au Department of Economics and Business, Pompeu Fabra University, Barcelona, Spain, gabor.lugosi@upf.edu ICREA, Pg. Llus Companys 23, 08010 Barcelona, Spain Barcelona Graduate School of Economics AI group, DTIC, Universitat Pompeu Fabra, Barcelona, Spain, gergely.neu@gmail.com 1", "creator": "LaTeX with hyperref package"}}}