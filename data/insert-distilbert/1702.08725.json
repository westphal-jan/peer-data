{"id": "1702.08725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Bayesian Verification under Model Uncertainty", "abstract": "machine learning enables systems to build and update domain effect models based on runtime observations. in this paper, we study statistical model checking and runtime verification assessments for systems with this ability. two standard challenges arise : ( 1 ) models built from limited numerical runtime data yield uncertainty parameters to be dealt with. ( 2 ) there is no definition of satisfaction w. r. t. uncertain hypotheses. we later propose such a definition assessment of subjective satisfaction based on recently introduced robust satisfaction functions. we also propose positioning the bv algorithm as encompassing a bayesian solution to runtime uncertainty verification of subjective satisfaction under model uncertainty. bv provides user - definable stochastic bounds for type i events and ii errors. first we discuss empirical reliable results from an example application to illustrate our ideas.", "histories": [["v1", "Tue, 28 Feb 2017 10:14:30 GMT  (408kb,D)", "http://arxiv.org/abs/1702.08725v1", "Accepted at SEsCPS @ ICSE 2017"]], "COMMENTS": "Accepted at SEsCPS @ ICSE 2017", "reviews": [], "SUBJECTS": "cs.SE cs.AI", "authors": ["lenz belzner", "thomas gabor"], "accepted": false, "id": "1702.08725"}, "pdf": {"name": "1702.08725.pdf", "metadata": {"source": "CRF", "title": "Bayesian Verification under Model Uncertainty", "authors": ["Lenz Belzner"], "emails": ["belzner@ifi.lmu.de", "belzner@ifi.lmu.de"], "sections": [{"heading": null, "text": "I. INTRODUCTION Statistical approaches to model checking and runtime verification exploit a domain model in order to evaluate system properties at design and runtime [1]. The system simulates potential traces based on the domain model in order to establish some statistical guarantees about properties of interest.\nStatistical verification is often based on a singular domain model [2], [3], [4]. Machine learning enables systems to build and adapt models about their application domains based on runtime observations (see e.g. [5], [6]). In particular, Bayesian statistics generally allow to infer and reason about an infinite amount of models [7]. Bayesian approaches allow to quantify the likelihood of a particular model, given prior beliefs and observed data. A system that verifies itself at runtime has to cope with model uncertainty to establish reliable verification results: Which hypothesis to assume when assessing system properties?\nModel uncertainty induced by learning from limited runtime information also raises another issue: What does it exactly mean for a system to satisfy a particular property, given many model hypotheses and their respective plausibilities?\nIn this paper, we study statistical verification for systems that are able to build and update their models based on runtime information. The paper\u2019s contributions are twofold:\n\u2022 We propose a definition of subjective satisfaction for systems that perform runtime verification based on limited information and possibly infinite hypothesis spaces. \u2022 We also propose a Bayesian verification algorithm, BV, that enables learning systems to decide on satisfaction or violation of required subjective satisfaction. BV provides user-definable stochastic bounds for type I and II errors (false negatives/positives). By construction, these error bounds are independent of the number of observations a system made about its environment.\n\u2022 We empirically establish the validity of BV\u2019s error bounds on a toy example.\nThe paper is structured as follows. Section II recaps Bayesian model checking and satisfaction functions for systems with parametrized models. In Section III we introduce our definition of subjective satisfaction. In Section IV we discuss the Bayesian treatment of verification under model uncertainty with the BV algorithm. In Section V, we describe setup and results of an empirical evaluation of BV. We discuss related work in Section VI. Section VII concludes and discusses venues for further research."}, {"heading": "II. PRELIMINARIES", "text": "This Section recalls Bayesian model checking and satisfaction functions for parametrized models."}, {"heading": "A. Bayesian Model Checking", "text": "Bayesian model checking (BMC) is based on Bayesian sequential hypothesis testing, and aims to infer the posterior distribution of the probability that a system satisfies its requirements [3], [4]. In contrast to point estimation (e.g. maximum likelihood), the Bayesian posterior captures the uncertainty about the true probability that arises from only performing a finite number of system assessments.\nRequirements may be formally specified in a suitable probabilistic temporal logic [8], [9].\nBMC treats a bounded simulation run of a system with a particular configuration as a Bernoulli experiment: The run may either satisfy or violate requirements. As the simulation captures probabilistic domain dynamics, the result of a simulation run is Bernoulli distributed with a probability p \u2208 [0, 1].\nBMC infers a posterior distribution over p based on the observed simulation results and a prior assumption about the distribution of p. In general, the posterior is proportional to the likelihood of observed data D (i.e. the results of simulation), multiplied by the prior distribution P (\u03b8) over the parameters of interest, \u03b8 = p in the case of BMC (Equation 1).\nP (\u03b8|D) \u221d P (D|\u03b8)P (\u03b8) (1)\nBMC models the uncertainty about p by a Beta distribution, the conjugate prior of the Bernoulli distribution. This approach ensures that the posterior is of the same form as the prior distribution, and thus enables efficient sequential updating of the distribution. The Beta distribution is parametrized by two parameters a, b \u2208 R+. In the case of BMC, a and b are given\nar X\niv :1\n70 2.\n08 72\n5v 1\n[ cs\n.S E\n] 2\n8 Fe\nb 20\n17\nby the successes and failures of the simulation runs. Given s successes, f failures, and assuming a uniform prior over p, the posterior (for \u03b8 = p) is determined by Equation 2.\nP (p|D) = Beta(s+ 1, f + 1) (2)\nTermination can be determined by assessing whether the probability mass above or below the required value of preq meets a particular confidence requirement creq \u2208 (0; 1). For alternative termination criteria, we refer to [4]."}, {"heading": "B. Satisfaction Functions", "text": "Many modern systems operate with models of the environment that are stochastic and parametrized, e.g. models build by machine learning. Classical statistical model checking algorithms, including classical BMC, enable to assess requirement satisfaction for a single parametrization of the model. Recently, the satisfaction function was introduced as a concept to allow for efficient, regressive assessment of requirement satisfaction for parametrizable models with potentially infinitely many parameters [10]. At its core, the satisfaction function is defined as follows.\nfsat(\u03b8) = P (sat|\u03b8) (3)\nHere, sat denotes a boolean variable indicating requirement satisfaction or violation, and \u03b8 are the model parameters. The satisfaction probability is depending on the particular parametrization of the model. However, note that the definition of the satisfaction function does not make any assumptions about the distribution of the parameters themselves. We will now turn to combine estimations about the parameters and the satisfaction function in order to define what we label subjective satisfaction."}, {"heading": "III. SUBJECTIVE SATISFACTION", "text": "Consider a system that was able to make a limited number of observations about the dynamics of its environment. For example, consider a mobile agent whose moves may fail to have an effect with Bernoulli probability pfail \u2208 [0; 1]. The agent may observe whether its moves are effective or not. Consider a situation where the agent observed its moves 10 times, out of which two had no effect. The following questions naturally arise:\n\u2022 What is pfail? \u2022 How confident can the agent be in its estimate of pfail? With these two questions in mind, consider now the situation that the agent finds itself in a grid world with obstacles at particular positions. Also, the agent has a sequence of movements to be executed in order to fulfill some given task, e.g. computed by a planning component. Consider that there is a requirement that the agent is only allowed to hit a limited number of obstacles (e.g. 2), with at most a specified probability preq \u2208 [0; 1]. Another question arises:\n\u2022 What is the probability psat that the sequence of movements will satisfy the requirements, given the limited observations about pfail?\nIn this setup, an agent has to cope with various uncertainties: 1) Domain uncertainty is inherent to the environment, in\nour example given by pfail. It is aleatoric, therefore irreducible and originates from the physical setup of the domain (e.g. sensory abstraction, laws of physics, etc.). Note that domain uncertainty in combination with requirements uniquely defines a satisfaction function (cf. Section II-B). 2) Model uncertainty is the epistemic uncertainty about the aleatoric domain uncertainty. It arises from the limited number of observations that the agent is able to collect from its environment. Note that model uncertainty not only arises from models learned at runtime. All empirically assessed models convey this kind of uncertainty, in particular all models built with machine learning approaches, regardless of the position in the a system\u2019s development lifecycle. 3) Subjective satisfaction, the uncertainty about a plan satisfying (or violating) a requirement in a particular situation, is also epistemic. It is a consequence of domain and model uncertainty, and the given system requirements.\nThe relation of domain and model uncertainty can be modeled in a Bayesian way. This is a widely adopted view, and a vast body of literature and techniques exists for estimating model uncertainty P (\u03b8|D) based on available domain observations D [5], [7]. For readability, we write P (\u03b8) for P (\u03b8|D) in the remainder of the paper.\nWe now combine model uncertainty with the satisfaction function to define subjective satisfaction psat.\npsat = \u222b \u03b8 P (sat|\u03b8)P (\u03b8)d\u03b8 (4)\nSubjective satisfaction psat can be interpreted as the parameter of a Bernoulli distribution that models uncertainty about satisfaction of the requirements. Intuitively, Eq. 4 weights the satisfaction probability for given parameters w.r.t. the the probability that these parameter represent the ground truth. Subjective satisfaction is considering all possible hypothetic domain parametrizations at once, and weights their respective satisfaction probabilities according to their plausibility (which is based on domain observations)."}, {"heading": "IV. BAYESIAN VERIFICATION UNDER MODEL UNCERTAINTY", "text": "We now define Bayesian Verification (BV), an algorithm for estimating subjective satisfaction by Monte Carlo simulation. By taking a Bayesian stance, we also get a confidence measure for this estimate. In fact, due to assessment of satisfaction with a limited number of simulations, an additional source of uncertainty arises: The uncertainty about the estimate of psat. BV establishes and updates a probability distribution P (sat) to quantify this uncertainty, and uses it to decide on termination. BV takes the following inputs.\n\u2022 The current system state s. \u2022 P (\u03b8), the system\u2019s model uncertainty.\n\u2022 A probabilistic simulation model of the domain dynamics M , parametrized by \u03b8. M takes a state, a plan, a requirement and a parametrization, and yields a boolean variable indicating requirement satisfaction. I.e. this model implicitly provides the satisfaction function P (sat|\u03b8). \u2022 The system\u2019s plan \u03c0 to be assessed. \u2022 A system requirement \u03c6, e.g. a temporal logic formula. \u2022 A required probability preq of satisfying \u03c6. \u2022 A required confidence creq in the estimate of psat. BV is shown in Algorithm 1. BV first initializes it estimate of psat. As satisfaction of a requirement in a stochastic domain can be interpreted as a Bernoulli random variable we use a uniform prior, which is a Beta(1, 1) distribution (line 2).\nWe define the confidence in the estimate of psat that is above the required satisfaction probability preq by determining the probability mass of P (psat) above preq.\ncsat = 1\u222b preq P (psat)dpsat (5)\nBV updates its estimate P (psat) and uses it in order to decide whether the estimate of satisfaction (or violation) can be done with at least required confidence (cf. Equation 5). To this end, it performs the following steps in repetition.\n1) A sample parametrization is drawn from the model uncertainty P (\u03b8) (line 4). 2) A simulation run is performed w.r.t. state, plan, requirement and parameters (line 5). Note that the simulation result is distributed accounting for both model uncertainty and satisfaction function, as the parameterization has been sampled from model uncertainty before. That is, sat \u223c P (sat|\u03b8)P (\u03b8) at this point. 3) The simulation result is used to update the belief distribution about psat (line 6). 4) The probability mass of the belief distribution is used to determine whether satisfaction or violation have been assessed with at least required confidence (Eq. 5). If so, the algorithm terminates accordingly (lines 7 and 8).\nAlgorithm 1 The BV algorithm for Bayesian verification under model uncertainty.\n1: procedure BV(s, P (\u03b8),M, \u03c0, \u03c6, preq, creq) 2: P (psat)\u2190 Beta(1, 1) 3: loop 4: \u03b8 \u223c P (\u03b8) 5: sat \u223cM(s, \u03c0, \u03c6, \u03b8) . sat \u223c P (sat|\u03b8)P (\u03b8) 6: update P (sat) according to sat . Eq. 2 7: if csat \u2265 creq then return true 8: if 1\u2212 csat \u2265 creq then return false"}, {"heading": "V. EMPIRICAL RESULTS", "text": "We empirically assessed BV on a toy example. While we modeled a very simple example, it may be worth noting that in general the Bayesian approach to model uncertainty scales\nup to much larger models. There exist varied and powerful tools for sampling from complex, high-dimensional posteriors P (\u03b8), such as Markov Chain Monte Carlo (see e.g. [11] for a very interesting read), or variational inference (e.g. [12])."}, {"heading": "A. Setup", "text": "The state s is constituted by a 10 x 10 grid world, with the agent at position (0, 0). Obstacles are randomly positioned, at an obstacle to free position ratio of 0.2. The agent is presented a plan \u03c0 (an action sequence) of 10 movements (up, down, left, right, with obvious semantics). The agent has a Bernoulli action failure probability pfail uniformly sampled from [0; 1]. Action failure results in the inverse movement (e.g. failing up yields down). The agent is presented a number of observations about its failure probability before running BV. We build model uncertainty P (\u03b8) about \u03b8 = pfail with a Beta distribution (cf. Eq. 2 and Section IV).\nIn our setting, \u03c6 is the requirement to hit less than three obstacles while executing the plan. We set preq = 0.9. This means we allow the agent to classify a plan as satisfying the requirement if it hits less than three obstacles in ninety percent of executions. We use a confidence requirement of creq = 0.95.\nWe approximate the ground truth satisfaction probability of a plan \u03c0 by taking the maximum likelihood estimate of satisfaction probability based on 10000 simulation runs. We assessed two error types.\n\u2022 A type I error is an incorrect rejection. This occurs if a plan \u03c0 satisfies \u03c6 with at least probability preq and is falsely rejected. \u2022 A type II error is a false accept. This occurs if a plan \u03c0 violates the requirements (i.e. \u03c6 is not satisfied with at least probability preq) and is falsely accepted.\nWe also assessed a variant of BV that does not explicitly build model uncertainty from observations, but rather builds a corresponding maximum likelihood estimate (p\u0302fail = observed failures / number of observations). Line 4 is correspondingly changed to \u03b8 \u2190 p\u0302fail in Algorithm 1.\nOur implementation of the setup and BV is available at https://github.com/jazzbob/bv."}, {"heading": "B. Results", "text": "Results are recorded for 10 randomly sampled observations of action failure probability. An exemplary result of our experiments is shown in Figure 1. The former shows accumulated type I errors over the course of different setups (i.e. randomly generated environments paired with random plans), the latter type II errors respectively. The dashed line shows the required statistical error bound (0.05 for creq = 0.95).\nIn particular, BV is able to establish the required statistical error bounds for both error types, while the MLE approach that is not explicitly using model uncertainty for inference fails to do so for type II errors. We observed this behavior for various numbers of observations presented to the system."}, {"heading": "VI. RELATED WORK", "text": "BV is an instance of statistical model checking in general [2], and Bayesian statistical model checking in particular [3], [4]. Typically, these approaches are assuming a perfect available model, and do not deal with explicitly quantified epistemic model uncertainty. One of the starting points of the current article is the work on smoothed model checking [10]. SMC approximates a satisfaction function w.r.t. uncertain model parameters by Gaussian process regression. However, SMC does not incorporate distributions over model parameters for system assessment. Our definition of subjective satisfaction is a direct consequence of combining quantified model uncertainty with SMC\u2019s satisfaction function. Parametrized Bayesian model checking for DBNs [13] does deal with quantified model uncertainty. However, the author does not exploit the posterior for bounding or estimating errors. The algorithm terminates when the posterior variance \u201cis less than some user-specified threshold\u201d. This approach does not yield statistical error estimates or bounds. We argue that in the context of software engineering, quantifiable error guarantees or estimates play a key role for system assessment. A quite different approach to quantitative system assessment under model uncertainty is formal verification with confidence intervals (FACT) [14]. It is based on (exhaustive) probabilistic model checking, and therefore allows to perform more thorough analysis than BV, which is approximate and (temporally) bounded. However, for the same reason, FACT suffers from the state space explosion. FACT models uncertainty in terms of frequentist confidence intervals, in contrast to BV\u2019s Bayesian modeling approach."}, {"heading": "VII. CONCLUSION", "text": "We have presented a Bayesian approach to statistical model checking under model uncertainty. We introduced the notion of subjective satisfaction as a result of combining recently introduced satisfaction functions with model uncertainty. We also presented Bayesian Verification (BV), an approximate Monte Carlo style algorithm for assessing subjective system satisfaction based on a simulation. BV allows for userspecified confidence bounds, and thus enables to statistically bound verification errors. We empirically evaluated BV on a toy example with positive results.\nThere are some limitations to the BV algorithm. When psat is close to preq, BV may take a many iterations to establish\nthe required confidence. Note that this property is independent from the absolute value of psat. Similar to Bayesian model checking based on a fixed model, BV scales well with required satisfaction probabilities close to one (see e.g. [4]). BV\u2019s obtained error bounds are statistical: They do not provide a hard upper bound. I.e. this bound may be surpassed temporarily when operating BV (e.g. an error may occur even when running BV only once, yielding an error rate of one). Also, while we could empirically observe that the error bound was not severely violated for our toy problem, there may be an intimate connection to the choice of prior for P (\u03b8). To study the connection of prior and error bound would probably yield interesting directions for further research. Another limitation of BV is its boundedness in terms of search depth. To this end, it would be interesting to increase the quality of satisfaction estimates, for example by adding global, previously trained satisfaction estimators to BV."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Martin Wirsing and Matthias Ho\u0308lzl for many inspiring discussions that led us into the direction of research presented in this paper."}], "references": [{"title": "and D", "author": ["M. Kwiatkowska", "G. Norman"], "venue": "Parker, \u201cPRISM 4.0: Verification of probabilistic real-time systems,\u201d in Proc. 23rd International Conference on Computer Aided Verification (CAV\u201911), ser. LNCS, G. Gopalakrishnan and S. Qadeer, Eds., vol. 6806. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "and S", "author": ["A. Legay", "B. Delahaye"], "venue": "Bensalem, \u201cStatistical model checking: An overview,\u201d in International Conference on Runtime Verification. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "and P", "author": ["S.K. Jha", "E.M. Clarke", "C.J. Langmead", "A. Legay", "A. Platzer"], "venue": "Zuliani, \u201cA bayesian approach to model checking biological systems,\u201d in International Conference on Computational Methods in Systems Biology. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "and E", "author": ["P. Zuliani", "A. Platzer"], "venue": "M. Clarke, \u201cBayesian statistical model checking with application to simulink/stateflow verification,\u201d in Proceedings of the 13th ACM international conference on Hybrid systems: computation and control. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Information theory", "author": ["D.J. MacKay"], "venue": "inference and learning algorithms. Cambridge university press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Probability theory: The logic of science", "author": ["E.T. Jaynes"], "venue": "Cambridge university press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "The temporal logic of programs,", "author": ["A. Pnueli"], "venue": "in Foundations of Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1977}, {"title": "Principles of model checking", "author": ["C. Baier", "J.-P. Katoen", "K.G. Larsen"], "venue": "MIT press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "and G", "author": ["L. Bortolussi", "D. Milios"], "venue": "Sanguinetti, \u201cSmoothed model checking for uncertain continuous-time Markov chains,\u201d Information and Computation, vol. 247, pp. 235\u2013253", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "The Markov chain Monte Carlo revolution,", "author": ["P. Diaconis"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "M", "author": ["M.J. Wainwright"], "venue": "I. Jordan et al., \u201cGraphical models, exponential families, and variational inference,\u201d Foundations and Trends R  \u00a9 in Machine Learning, vol. 1, no. 1\u20132, pp. 1\u2013305", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalized queries and bayesian statistical model checking in dynamic bayesian networks: Application to personalized medicine,", "author": ["C.J. Langmead"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "and G", "author": ["R. Calinescu", "C. Ghezzi", "K. Johnson", "M. Pezz\u00e9", "Y. Rafiq"], "venue": "Tamburrelli, \u201cFormal verification with confidence intervals to establish quality of service properties of software systems,\u201d IEEE Transactions on Reliability, vol. 65, no. 1, pp. 107\u2013125", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Statistical approaches to model checking and runtime verification exploit a domain model in order to evaluate system properties at design and runtime [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "Statistical verification is often based on a singular domain model [2], [3], [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Statistical verification is often based on a singular domain model [2], [3], [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Statistical verification is often based on a singular domain model [2], [3], [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "[5], [6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], [6]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "In particular, Bayesian statistics generally allow to infer and reason about an infinite amount of models [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "Bayesian model checking (BMC) is based on Bayesian sequential hypothesis testing, and aims to infer the posterior distribution of the probability that a system satisfies its requirements [3], [4].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "Bayesian model checking (BMC) is based on Bayesian sequential hypothesis testing, and aims to infer the posterior distribution of the probability that a system satisfies its requirements [3], [4].", "startOffset": 192, "endOffset": 195}, {"referenceID": 7, "context": "Requirements may be formally specified in a suitable probabilistic temporal logic [8], [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 8, "context": "Requirements may be formally specified in a suitable probabilistic temporal logic [8], [9].", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "As the simulation captures probabilistic domain dynamics, the result of a simulation run is Bernoulli distributed with a probability p \u2208 [0, 1].", "startOffset": 137, "endOffset": 143}, {"referenceID": 3, "context": "For alternative termination criteria, we refer to [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "Recently, the satisfaction function was introduced as a concept to allow for efficient, regressive assessment of requirement satisfaction for parametrizable models with potentially infinitely many parameters [10].", "startOffset": 208, "endOffset": 212}, {"referenceID": 4, "context": "This is a widely adopted view, and a vast body of literature and techniques exists for estimating model uncertainty P (\u03b8|D) based on available domain observations D [5], [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "This is a widely adopted view, and a vast body of literature and techniques exists for estimating model uncertainty P (\u03b8|D) based on available domain observations D [5], [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 10, "context": "[11] for a very interesting read), or variational inference (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "BV is an instance of statistical model checking in general [2], and Bayesian statistical model checking in particular [3], [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "BV is an instance of statistical model checking in general [2], and Bayesian statistical model checking in particular [3], [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "BV is an instance of statistical model checking in general [2], and Bayesian statistical model checking in particular [3], [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 9, "context": "One of the starting points of the current article is the work on smoothed model checking [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Parametrized Bayesian model checking for DBNs [13] does deal with quantified model uncertainty.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "A quite different approach to quantitative system assessment under model uncertainty is formal verification with confidence intervals (FACT) [14].", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Machine learning enables systems to build and update domain models based on runtime observations. In this paper, we study statistical model checking and runtime verification for systems with this ability. Two challenges arise: (1) Models built from limited runtime data yield uncertainty to be dealt with. (2) There is no definition of satisfaction w.r.t. uncertain hypotheses. We propose such a definition of subjective satisfaction based on recently introduced satisfaction functions. We also propose the BV algorithm as a Bayesian solution to runtime verification of subjective satisfaction under model uncertainty. BV provides user-definable stochastic bounds for type I and II errors. We discuss empirical results of a toy experiment.", "creator": "LaTeX with hyperref package"}}}