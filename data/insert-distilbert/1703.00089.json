{"id": "1703.00089", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "A Joint Identification Approach for Argumentative Writing Revisions", "abstract": "prior work on revision identification typically uses a pipeline method : revision resource extraction is first conducted to identify the locations of revisions and performing revision key classification is then similarly conducted on the identified revisions. performing such a setting propagates the errors of modifying the revision extraction step to reduce the revision stage classification step. this consensus paper proposes taking an approach that identifies the revision location and the revision type jointly to solve the issue of error propagation. it utilizes a sequence representation of revisions and conducts sequence labeling for revision identification. a mutation - based response approach is utilized to update identification sequences. results demonstrate that our proposed approach yields better performance on both revision location extraction and revision type classification compared to a reference pipeline baseline.", "histories": [["v1", "Tue, 28 Feb 2017 23:54:57 GMT  (2633kb,D)", "http://arxiv.org/abs/1703.00089v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fan zhang", "diane litman"], "accepted": false, "id": "1703.00089"}, "pdf": {"name": "1703.00089.pdf", "metadata": {"source": "CRF", "title": "A Joint Identification Approach for Argumentative Writing Revisions", "authors": ["Fan Zhang", "Diane Litman"], "emails": ["zhangfan@cs.pitt.edu", "litman@cs.pitt.edu"], "sections": [{"heading": "1 Introduction", "text": "Rewriting is considered as an important writing skill and researchers have demonstrated that experienced versus novice writers have different rewriting behaviors (Faigley and Witte, 1981). Automatic revision identification allows the building of advanced writing tutoring systems that aim at improving students\u2019 writing skills (Roscoe et al., 2015; Zhang et al., 2016). Revision identification typically involves two tasks: revision extraction where the locations of the revisions are identified and revision classification where the types of revisions are identified. Existing works typically follow a pipelined approach where the revision extraction step is first conducted (manually or automatically) and revision classification is conducted on the extracted revisions (Adler et al., 2011; Dax-\nenberger and Gurevych, 2013; Bronner and Monz, 2012; Zhang and Litman, 2015). One problem of the pipelined approach is that the errors of the revision extraction step are propagated to the revision classification step. To solve this problem, an approach that can conduct revision extraction and revision classification at the same time is needed.\nIn this paper we choose to conduct our study on argumentative revision detection (Zhang and Litman, 2015; Zhang and Litman, 2016). In (Zhang and Litman, 2015), revision locations are identified according to the result of sentence alignment and revision types are categorized to five categories according to their argumentation purpose: Claim, Reasoning, Evidence, General and Surface1. Their experiment on pipeline revision identification demonstrates significant performance drop when compared to revision classification on gold-standard alignments. Table 1 demonstrates an example of error propagation in argumentative revision classification. According to human annotation, (D1-2) should be aligned to (D22), (D1-3) should be aligned to (D2-3). Based on alignment, their revision types should be Surface2. However, when the automatic sentence alignment misses the alignment, the revision classification step considers the sentences as deleted and categorizes them as Reasoning.\nWe propose a sequence labeling-based joint identification approach by incorporating the output of both tasks into one sequence. The approach is designed based on two hypotheses. First, the classification of a revision can be improved by considering its nearby revisions. For example, a Claim revision is likely to be followed by a Rea-\n1The categories are defined according to Toulmin\u2019s argumentation model (Toulmin, 2003).\n2Surface include changes such as spelling correction and sentence reorderings that do not change a paper\u2019s content.\nar X\niv :1\n70 3.\n00 08\n9v 1\n[ cs\n.C L\n] 2\n8 Fe\nb 20\n17\nsoning revision3. Zhang and Litman (2016) used the types of revisions as labels and transformed the revision classification task to a sequence labeling problem. Their approach demonstrated significantly better performance than SVM-based classification approaches. In this paper, we extend their ideas by introducing EditSequence to also utilize alignment information for revision type prediction. An EditSequence describes a consecutive sequence of edits where not only the revision type but also the alignment information are incorporated into the labels of the edits. We hypothesize that adding alignment information can further improve revision type prediction. Second, the alignment of sentences can be corrected according to the types of labeled revisions. For example, the predicted types in Table 1 as a whole are rare, as there are 2 deleted Reasoning sentences and 2 added Reasoning sentences without any Claim change. Such a sequence is likely to have a small likelihood in sequence labeling and thus a possible alignment error is detected. We introduce the idea of \u201cmutation\u201d from genetic algorithms to generate possible corrections of sentence alignments. The alignment of sentences after correction allows us to conduct a new round of revision type labeling. Our approach iteratively mutate and label sequences until we cannot find sequences with larger likelihood. Two approaches are utilized to generate seed sequences for mutation: (1) Direct transformation from predicted sentence alignment (Zhang and Litman, 2014) (2) Automatic sequence generation using a Recurrent Neural Network (RNN). These settings together allow us to achieve better performance for both revision extraction and revision classification."}, {"heading": "2 Related Work", "text": "The idea of using sequence labeling for revision identification derives from the work in (Zhang and Litman, 2016), where they focused on the revision classification step with the types of revisions used as labels. Revisions are transformed to a sequence of labels according to the gold-standard alignment information. In our work, the sentence alignment step is also included as a target of our identification4. We extend their work by grouping sentence\n3If you changed the thesis/claim of your essay, you have to change the way you reason for it.\n4As the models in this paper are trained at the paragraph level, we assume the paragraphs were aligned and leave the discussion of paragraph alignment to the future work.\nalignment and revision type together into one label for joint identification.\nAs our tasks involve alignment, the problem in this paper can look similar to a labeled alignment problem, which can be solved with approaches such as CRFs (Blunsom and Cohn, 2006) or structured perceptrons/SVMs (Moore et al., 2006). For example, Blunsom and Cohn (2006) utilized CRFs to induce word alignment between bilingual sentence pairs. In their work, each sentence in the source document is treated as a sequence. Sequence labeling is conducted on the source sentence and the index of the aligned word in the target sentence is used as the label. Features such as translation scores between words are used and the Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences. Our problem is more complicated as our labels cover both the alignment and the revision type information. In labeled alignment, labels are used to represent the alignment information itself in one sequence. In revision identification, labels are used to represent the interaction between two sequences (the difference between sentences). Thus, our work utilized the revision operation (add/delete/modify) instead of the sentence index to mark the alignment information. Such design allows us to have the location information better coupled with the revision type information, and meanwhile allows us to update the alignment prediction by simply mutating the revision operation part of the labels.\nThe idea of sequence mutation is introduced from genetic algorithms to generate possible sentence alignment corrections. There are works on tagging problems (Araujo, 2002; Alba et al., 2006; Silva et al., 2013) where genetic algorithms are applied to learn a best labeling or rules for labeling. However, our approach does not follow the standard genetic algorithm in that we do not have crossover operations and we stop mutating when the current generation is worse than last. The idea behind our seed generation approach is similar to Sequential Monte-Carlo (Particle-filter) (Khan et al., 2004), where the sequence samples are generated by sampling labels according to their previous labels. In the paper we utilize a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNN to generate sample sequences as seeds. The advantage of LSTM is that it can utilize long distance label information instead of\njust the label before."}, {"heading": "3 Joint Revision Identification", "text": ""}, {"heading": "3.1 Problem and Approach Description", "text": "As demonstrated in Table 1, our task aims at the identification of the author\u2019s modifications from one draft to the other draft. Given the sentences from two drafts as the input, the system provides output in the format as (D1-1, D2-1, Modify, Surface), where the sentence alignment is used to record the revision locations and the revision type is used to record the author\u2019s revision purpose.\nFigure 1 demonstrates the workflow of our approach. The sentence alignment approach in (Zhang and Litman, 2014) is first utilized to segment the essays into sentences and generate a sentence alignment prediction. Afterwards seed EditSequences are generated either using a LSTM network or by transforming directly from the predicted sentence alignment. The seed EditSequences are then labeled by the trained sequence labeling model. The candidate EditSequences are mutated according to the output of the sequence model. Finally the best EditSequence is chosen and transformed to the list of revisions."}, {"heading": "3.2 Transformation between Revision and EditSequence Representation", "text": "Instead of using the sentence indices as the alignment information as in other works (Blunsom and\nCohn, 2006), this paper proposes EditSequence as a sequence representation of revisions. It incorporates both the alignment information and the revision type information in one sequence5.\nEditStep is defined as the basic unit of an EditSequence. An EditSequence contains a consecutive sequence of EditSteps. An EditStep unit contains 3 elements (Op1, Op2, RevType). For a pair of revised essays (Draft1, Draft2), a cursor is created for each draft separately and we define D1Pos,D2Pos to record cursor locations. Op1 and Op2 record the actions of the cursors. There are two cursor actions: Move (M) and Keep (K). Move indicates that the corresponding cursor is going to move to the position of the next sentence while Keep indicates that the cursor remains at the same location. RevType records the revision type information. In this paper we follow (Zhang and Litman, 2016), where revision types include five types6 for sentences changed7 and one type Nochange when aligned sentences are identical.\nRevisions to EditSequence. Figure 2 demonstrates how we transform from the revision representation used in prior works to our sequential representation EditSequence. In Figure 2(a), the cur-\n5Following (Zhang and Litman, 2016), we treat a revision that reorders two sentences as a Delete and an Add revisions.\n6Claim/Ideas (Claim), Warrant/Reasoning/Backing (Reasoning), Evidence, General Content (General) and Surface\n7Added/Deleted/Modified\nsors of the two drafts start at the beginning of the segment with D1Pos and D2Pos set to 1. Given that sentence 1 in Draft1 is the same as sentence 1 in Draft 2, both cursors move to the next sentence and we generate an EditStep (M, M, Nochange). In Figure 2(b), D1Pos and D2Pos are set to 2 according to the action of the previous step. In the example, sentence 2 in Draft 2 is an added Reasoning sentence, thus we generate a new EditStep (K-M-Reasoning) by keeping the cursor of Draft 1 in its current position (for comparison at the next step) and moving the cursor of Draft 2. Similarly, we move the cursor of Draft 1 in Figure 2(c). In Figure 2(d), D1Pos and D2Pos are set to 3. Sentences at the position are aligned to each other and both cursors are thus moved. Each EditStep is assigned a label as Op1-Op2-RevType and thus we generate a labeled sequence representation of revisions. As there are only three possible Op combinations (M-M, K-M, M-K)8, the total number of possible labels is 3\u00d7RevisionClassNum.\nEditSequence to Revisions. The sequence transformation step is reversible and we can infer all the revisions according to the sequence of edits. Head of the EditStep label indicates the re-\n8At least one of the cursors has to move.\nvision location: a label starting with \u201cM-M\u201d indicates that two sentences are aligned, \u201cM-K\u201d indicates that a sentence is deleted while \u201cK-M\u201d indicates that a sentence is added. Tail of the label corresponds to the revision type."}, {"heading": "3.3 EditSequence Labeling and EditSequence Mutation", "text": "For our first hypothesis, we conduct sequence labeling on EditSequence and use RevType of the labeled sequence as the results of revision classification. For our second hypothesis, we utilize both the likelihood provided by the sequence labeler and the (Op1,Op2) information of labels to correct sentence alignments.\nEditSequence Labeling Given a candidate EditSequence, sequence labeling is conducted to assign labels to each EditStep in the sequence. The RevType part of the assigned label is used as the revision type. Conditional Random Fields (CRFs) (Lafferty et al., 2001) is utilized for labeling9. Features used in (Zhang and Litman, 2015) are reused, which include unigrams and three feature groups.\nLocation group. For each EditStep, we record\n9CRFSuite (Okazaki, 2007) is used in implementation.\nits corresponding D1Pos and D2Pos as features, We also record whether the D1Pos and D2Pos are at the beginning/end of the paragraph/essay.\nTextual group. For each EditStep, we extract features for the aligned sentences pair (D1Pos, D2Pos). Features include sentence length (in both drafts), edit distance between aligned sentences and the difference in sentence length and punctuation. We not only calculate the edit distance between sentence pair (D1Pos, D2Pos) but also for pairs (D1Pos, D2Pos+1)10 and (D1Pos+1, D2Pos).\nLanguage group. Part of speech (POS) unigrams and difference in POS counts are encoded. Again features are extracted for pairs (D1Pos, D2Pos+1) and (D1Pos+1, D2Pos) besides (D1Pos, D2Pos).\nEditSequence mutation Besides assigning labels to the sequence, the CRFs model also provides us the likelihood of each label and the likelihood of the whole sequence. We compare the likelihood between sequences to decide which sequence is a better labeling. Within one sequence, we compare the likelihood between EditSteps to decide which EditStep is most likely to be corrected. Besides using the likelihood of each EditStep, we also compare the (Op1, Op2) information with the (Op1, Op2) information of the prior candidate EditSequence. We call it collision when such information does not match, which indicates that the candidate\u2019s alignment does not follow a typical sequence pattern and suggests correction.\nWe borrow the idea of \u201cmutation\u201d from genetic algorithms to generate possible corrections of sentence alignment. There are three possible kinds of \u201cmutation\u201d operations. (1) \u201cM-M\u201d to \u201cMK\u201d or \u201cM-M\u201d to \u201cK-M\u201d. This indicates breaking an alignment of sentences to one Delete revision and one Add revision. Thus for a EditStep with tag \u201cM-M-Type\u201d, we would remove the step from the sequence and add two new steps \u201cMK-Nochange\u201d and \u201cK-M-Nochange\u201d. Notice that here Nochange is a dummy label and will be replaced in the next round of labeling. (2) \u201cM-K\u201d to \u201cM-M\u201d or \u201cK-M\u201d to \u201cM-M\u201d. This indicates aligning a deleted/added sentence to another sentence. Depending on the labeling of the following EditStep, the operation can be different. \u201cMK\u201d followed by \u201cK-M\u201d11 indicates that the aligned\n10If D2Pos+1 does not exceed paragraph boundary 11Or \u201cK-M\u201d is followed by \u201cM-K\u201d.\nsentence in Draft 2 is not aligned to other sentences. For example in Figure 3, the second EditStep (M-K-Nochange) is followed by EditStep (K-M-Nochange), which indicates that Sentence 2 (Draft 2) has not been aligned to other sentences and aligning sentence 2 (Draft 1) will not impact the alignment of Sentence 2 (Draft 2). In that case, we remove the two steps and add a step \u201cM-MNochange\u201d. \u201cM-K\u201d followed by \u201cM-M\u201d indicates that the aligned sentence has been aligned to other sentences. For that cases, we need to remove the \u201cM-K\u201d and \u201cM-M\u201d step and add two steps \u201cMM-Nochange\u201d and \u201cM-K-Nochange\u201d for the misaligned sentence. (3) \u201cM-K\u201d to \u201cK-M\u201d or \u201cKM\u201d to \u201cM-K\u201d. This means changing from Delete to Add. This is similar to the previous case, where the mutation operation depends on the labeling of the following EditStep. If the following EditStep starts with \u201cM-M\u201d, it indicates that the sentence in the Add revision is aligned and we need to break the existing alignments and add a \u201cM-K\u201d EditStep besides changing \u201cM-K\u201d to \u201cK-M\u201d.\nFigure 3 provides an example of the EditSequence update process. The process starts with seed candidate sequences as the first generation, the first generation will always be mutated. For a seed EditSequence Sseed and its labeled sequence Slabeled, the alignment part of their EditStep labels are compared to check for collision. For every collision detected, we mutate Sseed to generate one new candidate sequence Snew as a member of the next generation. After the mutation of the first generation is complete, all Snew in the new generation are labeled with CRFs again. The new labels provide us new revision types within the new alignments. If the likelihood of the labeled sequence SnewLabeled is larger than Slabeled, it indicates that the sentence alignment in Snew is more trustworthy than the alignment in Sseed, thus Snew should be further mutated to see if the alignment can be further improved. Otherwise we do not further mutate Snew. We keep mutating the EditSequences until we cannot conduct any further mutation. For the labeled EditSequences in all generations, we first select sequences with minimum number of collisions and then select the sequence with the maximum sequence likelihood. The (Op1, Op2) of labels are used as results of revision extraction and RevTypes are used for revision classification. Through the process, sequence labeling provides likelihood for both alignments\nand revision types, while sequence mutation provides new possible sequences for labeling."}, {"heading": "3.4 Seed Candidate EditSequence Generation", "text": "For a paragraph with m sentences in the first draft and n sentences in the second draft, there is a total of ( m+n n ) = (m+n)!m!n! possible sequences\n12. While theoretically we can first generate a sequence without sentence alignment (all sentences in Draft 1 treated as deleted and all sentences in Draft 2 treated as added) as the seed sequence and keep mutating until the best sequence is found, such process is too computationally expensive and is likely to fall into local optima during mutation. Thus an approach is needed for the generation of high-likelihood seed EditSequences. We propose two approaches for sequence generation, one based on the revision extraction method proposed\n12With m sentences of Draft 1 set, there are m+n slots to put in the n sentences of Draft 2.\nin (Zhang and Litman, 2014), the other based on automatic sequence generation with LSTM.\n1-Best EditSequence generation based on alignment prediction During preprocessing, the essays are segmented into sentences and sentences are aligned following (Zhang and Litman, 2014). A logistic regression classifier is first trained on the training data with Levenshtein distance as the feature and alignment is conducted using Nelken\u2019s global alignment approach (Nelken and Shieber, 2006) based on the likelihood provided by the classifier. As the number of essays in the dataset is limited, we construct sequences at the paragraph level. We trained our models on paired paragraphs assuming paragraphs have been aligned. For each paragraph pair, an EditSequence is generated following the sequence transformation method with RevType of all EditSteps set to Nochange13.\nN-Candidate EditSequence generation with LSTM network The 1-best approach can provide a good sequence to start with, however, it is more likely to fall into local optima in the labeling step with only one seed candidate. Thus we also trained LSTM to generate multiple possible candidates. As demonstrated in Figure 4, we construct the neural network with LSTM units. Due to the size limit of our current training data, we only include one layer of LSTM units to reduce the number of parameters in the network. Each EditStep is treated as a time step in the neural network. According to the D1Pos and D2Pos property of the EditStep, we extract features X as the input to\n13Nochange is a just a placeholder as the real RevType are to be labeled in the labeling step.\nthe neural network. The same set of features used in the sequence labeling step is used. The model transforms the input to hidden state S, where hidden state St\u22121 at time (t-1) is used together with input Xt to predict the hidden state St at time t. A softmax layer is added on the top of the hidden state to predict Ot, which describes the probability distribution of the sequence labels. At the training step, we fit the model with EditSequences transformed from revisions between the paragraphs. At the generation step we start with both D1Pos and D2Pos set to 1 and extract features for X1. In each time step, a label is sampled according to the probability distribution Ot. According to the sampled label, we change the positions of D1Pos and D2Pos to extract the features for X2. In the example, the sampled label at Xt\u22121 is M-M-Nochange, this label moves D1Pos and D2Pos to (2,2) and the Xt is extracted and used together with St\u22121 to predict St. According to the probability distribution Ot, a new label is sampled and the result is used to move the cursors for the next EditStep. The process is repeated until an EditSequence is generated for the whole paragraph pair. We repeat the algorithm until N candidates are collected."}, {"heading": "4 Experiments and Results", "text": "Data As in Table 2, our experiments use the data used in (Zhang and Litman, 2016), which consists of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A contains 47 paper draft pairs about placing contemporaries in Dante\u2019s Inferno. Corpus B contains 63 paper draft pairs explaining the\nrhetorical strategies used by the speaker/author of a previously read lecture/essay. Both corpora were double coded (Kappa for A: 0.75, B: 0.69) and gold standard labels were created upon agreement.\nExperiments We conducted experiments using different revision type settings. In (Zhang and Litman, 2015), the annotated Claim, Reasoning, Evidence and General Content were grouped together as one Content revision category14. In our work we in addition group the last three categories together as one Support category15. We first evaluate the performance of sentence alignment and Content vs. Surface vs. Nochange revision classification (3-class). Then we experimented with Claim vs. Support vs. Surface vs. Nochange (4-class). Finally we used all revision categories (6-class). For each experiment, three approaches are compared as in Table 3: Baseline, 1Best and +NCandidate. 10 draft pairs from Corpus B were used as the development set for setting up parameters of LSTM16 and choosing N. N is set to 10 for all our experiments. Afterwards 10-fold (student) crossvalidation were conducted on both corpora A and B. The same set of data folds and features were used for all three approaches. The training folds in each round will be used for training both CRFs and LSTM. For evaluation we used alignment accuracy17 to measure the accuracy of revision ex-\n14In contrast to the Surface revision type, Content represent revisions that change the information of the essay.\n15Content revisions that support the claim of the essay. 16LSTM implemented with deeplearning4j (http:// deeplearning4j.org) with epoch set to 1, iteration numbers to 100 and output dimension of the first layer to 100\n17 2\u00d7AgreedAlignment #Draft1Sentences+#Draft2Sentences\n, adapted from (Zhang and Litman, 2014).\ntraction and precision/recall to measure the result of revision identification. Precision is calculated as #CorrectRevisions#PredictedRevisions and Recall is calculated as\n#CorrectRevisions #GoldStandardRevisions .\nResults Table 4 demonstrates our experimental results. We first compare the pipeline baseline with our joint model using 1Best seed EditSequence. With 3 revision types (3-class), the joint model achieves significantly better performance than the baseline on Corpus A for both revision extraction (sentence alignment) and revision classification. It also shows better performance on Corpus B (while not significant). The improvement on the precision/recall of revision classification supports our first hypothesis that alignment information can improve the accuracy of revision classification. The improvement on sentence alignment supports our second hypothesis that the patterns of predicted revisions can be used to correct the false alignments. We notice that the number of revision types impacts the performance of the model. On corpus A, the model shows significantly better performance than the baseline in almost all experiments. While on corpus B, the model yields significantly better performance in 4-class experiment. The impact of revision types on our model can be two-fold. On the one hand, more revi-\nsion types indicates more detailed sequence information, which improves the chance of recognizing problems in sentence alignment. On the other hand, the increase of revision types increases the difficulty of sequence labeling, which in return can hurt the performance of joint identification. We leave the error analysis of performance difference between different revision types to the future work.\nNext, we compare results using 1Best and +NCandidate EditSequences. We observe that using generated sequences improves the 1Best performance, yielding the best result on almost all experiments (except on Corpus B with 6 revision types). We counted the number of generations in EditSequence mutation for both 1Best and +NCandidate on 3-class experiment. Results show that the 1Best approach will stop mutating after an average of 1.2 generations while +NCandidate stops mutating after an average of 2.3. This suggests that our approach prevents the model from easily falling into local optima."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper we propose a joint identification approach for argumentative writing revisions. For the two different sub tasks of revision identification (revision location extraction and revision type classification), we transform the location representation to a revision operation format and incorporate it together with the revision type into one label. The two different tasks are thus transformed to one joint sequence labeling task. With this design, the likelihood of a labeled sequence indicates not only the likelihood of sentence alignments but also the likelihood of the revision types. We utilize the mutation idea from genetic algorithms to iteratively update the labeling of sequences. LSTM is utilized to generate seed candidate EditSequences for mutation. Results demonstrate that our approach improves the performance of both tasks.\nIn this paper the effect of neural networks is limited by the data size. For the future work we would like to explore our approach on other larger writing revision datasets (Lee et al., 2015) to fully take advantage of LSTM. Another problem that has not been addressed in this paper is that the paragraphs are assumed to be aligned. To fully automatize our model, we plan to construct an accurate automatic paragraph alignment model (Barzilay and Elhadad, 2003) based on topic information (Blei\net al., 2003) as the preprocessing step."}], "references": [{"title": "Wikipedia vandalism detection: Combining natural language, metadata, and reputation features", "author": ["Luca De Alfaro", "Santiago M. Mola-Velasco", "Paolo Rosso", "Andrew G. West"], "venue": null, "citeRegEx": "Adler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2011}, {"title": "Natural language tagging with genetic algorithms", "author": ["Alba et al.2006] Enrique Alba", "Gabriel Luque", "Lourdes Araujo"], "venue": "Information Processing Letters,", "citeRegEx": "Alba et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Alba et al\\.", "year": 2006}, {"title": "Part-of-speech tagging with evolutionary algorithms", "author": ["Lourdes Araujo"], "venue": "In International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "Araujo.,? \\Q2002\\E", "shortCiteRegEx": "Araujo.", "year": 2002}, {"title": "Sentence alignment for monolingual comparable corpora", "author": ["Barzilay", "Elhadad2003] Regina Barzilay", "Noemie Elhadad"], "venue": "In Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "Barzilay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Discriminative word alignment with conditional random fields", "author": ["Blunsom", "Cohn2006] Phil Blunsom", "Trevor Cohn"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Asso-", "citeRegEx": "Blunsom et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2006}, {"title": "User edits classification using document revision histories", "author": ["Bronner", "Monz2012] Amit Bronner", "Christof Monz"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Bronner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bronner et al\\.", "year": 2012}, {"title": "Automatically classifying edit categories in Wikipedia revisions", "author": ["Daxenberger", "Iryna Gurevych"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Daxenberger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daxenberger et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An mcmc-based particle filter for tracking multiple interacting targets", "author": ["Khan et al.2004] Zia Khan", "Tucker Balch", "Frank Dellaert"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Khan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learn-", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Cityu corpus of essay drafts of english language learners: a corpus of textual revision in second language writing", "author": ["Lee et al.2015] John Lee", "Chak Yan Yeung", "Amir Zeldes", "Marc Reznicek", "Anke L\u00fcdeling", "Jonathan Webster"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Improved discriminative bilingual word alignment", "author": ["Moore et al.2006] Robert C Moore", "Wen-tau Yih", "Andreas Bode"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the As-", "citeRegEx": "Moore et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2006}, {"title": "Towards robust context-sensitive sentence alignment for monolingual corpora", "author": ["Nelken", "Shieber2006] Rani Nelken", "Stuart M Shieber"], "venue": "In EACL", "citeRegEx": "Nelken et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nelken et al\\.", "year": 2006}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Automated detection of essay revising patterns: applications for intelligent feedback in a writing tutor", "author": ["Roscoe et al.2015] Rod D Roscoe", "Erica L Snow", "Laura K Allen", "Danielle S McNamara"], "venue": null, "citeRegEx": "Roscoe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roscoe et al\\.", "year": 2015}, {"title": "A new approach to the pos tagging problem using evolutionary computation", "author": ["Arlindo Silva", "Irene Rodrigues"], "venue": "In RANLP,", "citeRegEx": "Silva et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2013}, {"title": "Sentence-level rewriting detection", "author": ["Zhang", "Litman2014] Fan Zhang", "Diane Litman"], "venue": "In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Annotation and classification of argumentative writing revisions", "author": ["Zhang", "Litman2015] Fan Zhang", "Diane Litman"], "venue": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Using context to predict the purpose of argumentative writing revisions", "author": ["Zhang", "Litman2016] Fan Zhang", "Diane Litman"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Argrewrite: A web-based revision assistant for argumentative writings", "author": ["Zhang et al.2016] Fan Zhang", "Rebecca Hwa", "Diane Litman", "Homa B. Hashemi"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Automatic revision identification allows the building of advanced writing tutoring systems that aim at improving students\u2019 writing skills (Roscoe et al., 2015; Zhang et al., 2016).", "startOffset": 138, "endOffset": 179}, {"referenceID": 19, "context": "Automatic revision identification allows the building of advanced writing tutoring systems that aim at improving students\u2019 writing skills (Roscoe et al., 2015; Zhang et al., 2016).", "startOffset": 138, "endOffset": 179}, {"referenceID": 0, "context": "Existing works typically follow a pipelined approach where the revision extraction step is first conducted (manually or automatically) and revision classification is conducted on the extracted revisions (Adler et al., 2011; Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Zhang and Litman, 2015).", "startOffset": 203, "endOffset": 303}, {"referenceID": 12, "context": "As our tasks involve alignment, the problem in this paper can look similar to a labeled alignment problem, which can be solved with approaches such as CRFs (Blunsom and Cohn, 2006) or structured perceptrons/SVMs (Moore et al., 2006).", "startOffset": 212, "endOffset": 232}, {"referenceID": 12, "context": "As our tasks involve alignment, the problem in this paper can look similar to a labeled alignment problem, which can be solved with approaches such as CRFs (Blunsom and Cohn, 2006) or structured perceptrons/SVMs (Moore et al., 2006). For example, Blunsom and Cohn (2006) utilized CRFs to induce word alignment between bilingual sentence pairs.", "startOffset": 213, "endOffset": 271}, {"referenceID": 2, "context": "There are works on tagging problems (Araujo, 2002; Alba et al., 2006; Silva et al., 2013) where genetic algorithms are applied to learn a best labeling or rules for labeling.", "startOffset": 36, "endOffset": 89}, {"referenceID": 1, "context": "There are works on tagging problems (Araujo, 2002; Alba et al., 2006; Silva et al., 2013) where genetic algorithms are applied to learn a best labeling or rules for labeling.", "startOffset": 36, "endOffset": 89}, {"referenceID": 16, "context": "There are works on tagging problems (Araujo, 2002; Alba et al., 2006; Silva et al., 2013) where genetic algorithms are applied to learn a best labeling or rules for labeling.", "startOffset": 36, "endOffset": 89}, {"referenceID": 9, "context": "The idea behind our seed generation approach is similar to Sequential Monte-Carlo (Particle-filter) (Khan et al., 2004), where the sequence samples are generated by sampling labels according to their previous labels.", "startOffset": 100, "endOffset": 119}, {"referenceID": 10, "context": "Conditional Random Fields (CRFs) (Lafferty et al., 2001) is utilized for labeling9.", "startOffset": 33, "endOffset": 56}, {"referenceID": 14, "context": "CRFSuite (Okazaki, 2007) is used in implementation.", "startOffset": 9, "endOffset": 24}, {"referenceID": 11, "context": "For the future work we would like to explore our approach on other larger writing revision datasets (Lee et al., 2015) to fully take advantage of LSTM.", "startOffset": 100, "endOffset": 118}], "year": 2017, "abstractText": "Prior work on revision identification typically uses a pipeline method: revision extraction is first conducted to identify the locations of revisions and revision classification is then conducted on the identified revisions. Such a setting propagates the errors of the revision extraction step to the revision classification step. This paper proposes an approach that identifies the revision location and the revision type jointly to solve the issue of error propagation. It utilizes a sequence representation of revisions and conducts sequence labeling for revision identification. A mutation-based approach is utilized to update identification sequences. Results demonstrate that our proposed approach yields better performance on both revision location extraction and revision type classification compared to a pipeline baseline.", "creator": "LaTeX with hyperref package"}}}