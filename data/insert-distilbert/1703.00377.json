{"id": "1703.00377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Gradient Boosting on Stochastic Data Streams", "abstract": "boosting is a popular ensemble algorithm that generates more powerful learners by linearly combining base models from a simpler hypothesis class. in this work, we investigate the problem of adapting batch gradient boosting for minimizing convex loss functions to online setting where the loss at each iteration is i. i. d sampled from an already unknown distribution. to generalize from batch to online, we first introduce the definition of online weak learning edge with which for strongly convex and smooth loss functions, we present an empirical algorithm, streaming simple gradient boosting ( sgb ) with exponential numerical shrinkage guarantees in the number of weak learners. we further present an adaptation function of sgb to appropriately optimize non - smooth loss functions, for which we derive a o ( ln n / n ) convergence rate. we also show that our analysis can extend to adversarial online learning setting under a stronger assumption that the default online weak learning edge will hold in adversarial population setting. we finally demonstrate new experimental ensemble results showing that in practice our algorithms can achieve competitive results as classic gradient boosting while using less computation.", "histories": [["v1", "Wed, 1 Mar 2017 16:46:54 GMT  (331kb,D)", "http://arxiv.org/abs/1703.00377v1", "To appear in AISTATS 2017"]], "COMMENTS": "To appear in AISTATS 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanzhang hu", "wen sun", "arun venkatraman", "martial hebert", "j", "rew bagnell"], "accepted": false, "id": "1703.00377"}, "pdf": {"name": "1703.00377.pdf", "metadata": {"source": "CRF", "title": "Gradient Boosting on Stochastic Data Streams", "authors": ["Hanzhang Hu", "Wen Sun", "Arun Venkatraman", "Martial Hebert", "J. Andrew Bagnell"], "emails": ["dbagnell}@cs.cmu.edu"], "sections": [{"heading": null, "text": "Boosting is a popular ensemble algorithm that generates more powerful learners by linearly combining base models from a simpler hypothesis class. In this work, we investigate the problem of adapting batch gradient boosting for minimizing convex loss functions to online setting where the loss at each iteration is i.i.d sampled from an unknown distribution. To generalize from batch to online, we first introduce the definition of online weak learning edge with which for strongly convex and smooth loss functions, we present an algorithm, Streaming Gradient Boosting (SGB) with exponential shrinkage guarantees in the number of weak learners. We further present an adaptation of SGB to optimize nonsmooth loss functions, for which we derive a O(lnN/N) convergence rate. We also show that our analysis can extend to adversarial online learning setting under a stronger assumption that the online weak learning edge will hold in adversarial setting. We finally demonstrate experimental results showing that in practice our algorithms can achieve competitive results as classic gradient boosting while using less computation."}, {"heading": "1 INTRODUCTION", "text": "Boosting (Freund and Schapire, 1995) is a popular method that leverages simple learning models (e.g., decision stumps) to generate powerful learners. Boosting has been used to great effect and trump other learning algorithms in a variety of applications. In computer vision, boosting was made popular by the seminal ViolaJones Cascade (Viola and Jones, 2001) and is still used\nProceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. JMLR: W&CP volume 54. Copyright 2017 by the author(s).\nto generate state-of-the-art results in pedestrian detection (Nam et al., 2014; Yang et al., 2015; Zhu and Peng, 2016). Boosting has also found success in domains ranging from document relevance ranking (Chapelle et al., 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al., 2012). Finally, boosting yields an anytime property at test time, which allows it to work with varying computation budgets (Grubb and Bagnell, 2012) for use in real-time applications such as controls and robotics.\nThe advent of large-scale data-sets has driven the need for adapting boosting from the traditional batch setting, where the optimization is done over the whole dataset, to the online setting where the weak learners (models) can be updated with streaming data. In fact, online boosting has received tremendous attention so far. For classification, (Chen et al., 2012; Oza and Russell, 2001; Beygelzimer et al., 2015b) proposed online boosting algorithms along with theoretical justifications. Recent work by Beygelzimer et al. (2015a), addressed the regression task through the introduction of Online Gradient Boosting (OGB). We build upon on the developments in (Beygelzimer et al., 2015a) to devise a new set of algorithms presented below.\nIn this work, we develop streaming boosting algorithms for regression with strong theoretical guarantees under stochastic setting, where at each round the data are i.i.d sampled from some unknown fixed distribution. In particular, our algorithms are streaming extension to the classic gradient boosting (Friedman, 2001), where weak predictors are trained in a stage-wise fashion to approximate the functional gradient of the loss with respect to the previous ensemble prediction, a procedure that is shown by Mason et al. (2000) to be functional gradient descent of the loss in the space of predictors. Since the weak learners cannot match the gradients of the loss exactly, we measure the error of approximation by redefining of edge of online weak learners (Beygelzimer et al., 2015b) for online regression setting.\nAssuming a non-trivial edge can be achieved by each deployed weak online learner, we develop algorithms to handle smooth or non-smooth loss functions, and theo-\nar X\niv :1\n70 3.\n00 37\n7v 1\n[ cs\n.L G\n] 1\nM ar\n2 01\n7\nretically analyze the convergence rates of our streaming boosting algorithms. Our first algorithm targets strongly convex and smooth loss functions and achieves exponential decay on the average regret with respect to the number of weak learners. We show the ratio of the decay depends on the edge and also the condition number of the loss function. The second algorithm, designed for strongly convex but non-smooth loss functions, extends from the batch residual gradient boosting algorithm from (Grubb and Bagnell, 2011). We show that the algorithm achieves O(lnN/N) convergence rate with respect to the number of weak learners N , which matches the online gradient descent (OGD)\u2019s no-regret rate for strongly convex loss (Hazan et al., 2007). Both of our algorithms promise that as T (the number of samples) and N go to infinity, the average regret converges to zero. Our analysis leverages Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014), hence our results naturally extends to adversarial online learning setting as long as the weak online learning edge holds in adversarial setting, a harsher setting than stochastic setting. We conclude with some proof-of-concept experiments to support our analysis. We demonstrate that our algorithm significantly boosts the performance of weak learners and converges to the performance of classic gradient boosting with less computation."}, {"heading": "2 RELATED WORK", "text": "Online boosting algorithms have been evolving since their batch counterparts are introduced. Oza and Russell (2001) developed some of the first online boosting algorithm, and their work are applied to online feature selection (Grabner and Bischof, 2006) and online semisupervised learning (Grabner et al., 2008). Leistner et al. (2009) introduced online gradient boosting for the classification setting albeit without a theoretical analysis. Chen et al. (2012) developed the first convergence guarantees of online boosting for classification. Then Beygelzimer et al. (2015b) presented two online classification boosting algorithms that are proved to be respectively optimal and adaptive.\nOur work is most related to (Beygelzimer et al., 2015a), which extends gradient boosting for regression to the online setting under a smooth loss: each weak online learner is trained by minimizing a linear loss, and weak learners are combined using Frank-Wolfe (Frank and Wolfe, 1956) fashioned updates. Their analysis generalizes those of batch boosting for regression (Zhang and Yu, 2005). In particular, these proofs forgo edge assumptions of the weak learners. Though Frank-Wolfe is a nice projection-free algorithm, it has relatively slow convergence and usually is restricted to smooth loss functions. In our work, each weak learner instead\nminimizes the squared loss between its prediction and the gradient, which allows us to treat weak learners as approximations of the gradients thanks to the weak learner edge assumption. Hence we can mimic classic gradient boosting and use a gradient descent approach to combine the weak learners\u2019 predictions. These differences enable our algorithms to handle non-smooth convex losses, such as hinge and L1-losses, and result in convergence bounds that is more analogous to the bounds of classic batch boosting algorithms. This work also differs from (Beygelzimer et al., 2015a) in that we assume an online weak learner edge exists, a common assumption in the classic boosting literature (Freund and Schapire, 1995, 1999) that is extended to the online boosting for classification by (Chen et al., 2012; Beygelzimer et al., 2015b). With this assumption, we analyze online gradient boosting using techniques from gradient descent for convex losses (Hazan et al., 2007)."}, {"heading": "3 PRELIMINARIES", "text": "In the classic online learning setting, at every time step t, the learner A first makes a prediction (i.e., picks a predictor ft \u2208 F , where F is a pre-defined class of predictors) on the input xt \u2208 Rd, then receives a loss `t(ft(xt)). The learner then updates ft to ft+1. The samples (`t, xt) could be generated by an adversary, but this work mainly focuses on the setting where (`t, xt) \u223c D are i.i.d sampled from a distribution D. The regret RA(T ) of the learner is defined as the difference between the total loss from the learner and the total loss from the best hypothesis in hindsight under the sequence of samples {(`t, xt)}t:\nRA(T ) = T\u2211 t=1 `t(ft(xt))\u2212 min f\u2217\u2208F T\u2211 t=1 `t(f \u2217(xt)). (1)\nWe say the online learner is no-regret if and only if RA(T ) is o(T ). That is, time averaged, the online learner predictor ft is doing as well as the best hypothesis f\u2217 in hindsight. We define risk of a hypothesis f as E(`,x)\u223cD[`(f(x))]. Our analysis of the risk leverages the classic Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014). The online-to-batch reduction first analyzes regret without the stochastic assumption on the sequence of loss `, and it then relates regret to risk using concentration of measure.\nThroughout the paper we will use the concepts of strong convexity and smoothness. A function `(x) is said to be \u03bb-strongly convex and \u03b2-smooth with respect to norm \u2016 \u00b7 \u2016 if and only if for any pair x1 and x2:\n\u03bb 2 \u2016x1 \u2212 x2\u20162 \u2264 `(x1)\u2212 `(x2)\u2212\u2207`(x2)(x1 \u2212 x2) \u2264 \u03b2 2 \u2016x1 \u2212 x2\u20162, (2)\nwhere \u2207`(x) denotes the gradient of function ` with respect to x."}, {"heading": "3.1 Online Boosting Setup", "text": "Our online boosting setup is similar to (Beygelzimer et al., 2015b) and (Beygelzimer et al., 2015a). At each time step t = 1, .., T , the environment picks loss `t : Rm \u2192 R. The online boosting learner makes a prediction yt \u2208 Rm without knowing `t. Then the learner suffers loss `t(yt). Throughout the paper we assume the loss is bounded as |`t(y)| \u2264 B,B \u2208 R+,\u2200t, y. We also assume that the gradient of the loss \u2207`t(y) is also bounded as \u2016\u2207`t(y)\u2016 \u2264 G,G \u2208 R+,\u2200t, y.1 The online boosting learner maintains a sequence of weak online learning algorithms A1, ...,AN . Each weak learner Ai can only use hypothesis from a restricted hypothesis class H to produce its prediction y\u0302it = hit(xt) (h : Rd \u2192 Rm,\u2200h \u2208 H), where hit \u2208 H. To make a prediction yt at each iteration, each Ai will first make a prediction y\u0302it \u2208 Rm where y\u0302it = hit(xt). The online boosting learner combines all the weak learners\u2019 predictions to produce the final prediction yt for sample xt. The online learner then suffers loss `t(yt) after the loss `t is revealed. As we will show later, with the loss `t, the online learner will pass a square loss to each weak learner. Each weak learner will then use its internal no-regret online update procedure to update its own weak hypothesis from hit to h i t+1. In stochastic setting where `t and xt are i.i.d samples from a fixed distribution, the online boosting learner will output a combination of the hypothesises that were generated by weak learners as the final boosted hypothesis for future testing.\nBy leveraging linear combination of weak learners, the goal of the online boosting learner is to boost the performance of a single online learner Ai. Additionally, we ideally want the prediction error to decrease exponentially fast in the number N of weak learners, as is the result from classic batch gradient boosting (Grubb and Bagnell, 2011)."}, {"heading": "4 WEAK ONLINE LEARNING", "text": "We specifically consider the setting where each weak learner minimizes a square loss \u2016y \u2212 h(x)\u20162, where y is the regression target, and h is in the weak-learner hypothesis classH. At each step t, a weak online learner A chooses a predictor ht \u2208 H to predict ht(xt), receives the target yt\n2 and then suffers loss \u2016yt\u2212ht(xt)\u20162. With 1Throughout the paper, the notation \u2016x\u2016 for any finite dimension vector x stands for the classic L2 norm. 2Abuse of notation: in Sec 4, yt \u2208 Rm simply stands for a regression target for the weak learner at step t, not the final prediction of the boosted learner defined in Sec. 3.1.\nthis, we now introduce the definition of Weak Online Learning Edge.\nDefinition 4.1. (Weak Online Learning Edge) Given a restricted hypothesis class H and a sequence of square losses {\u2016yt \u2212 h(xt)\u20162}t, the weak online learner predicts a sequence {ht} that has edge \u03b3 \u2208 (0, 1], such that with high probability 1\u2212 \u03b4:\nT\u2211 t=1 \u2016yt \u2212 ht(xt)\u20162 \u2264 (1\u2212 \u03b3) T\u2211 t=1 \u2016yt\u20162 +R(T ), (3)\nwhere R(T ) \u2208 o(T ) is usually known as the excess loss.\nThe high probability 1\u2212 \u03b4 comes from the possible randomness of the weak online learner and the sequence of examples. Usually the dependence of the high probability bound on \u03b4 is poly-logarithmic in 1/\u03b4 that is included in the term R(T ). We will give a concrete example on this edge definition in next section where we will show what R(T ) consists of. Intuitively, a larger edge implies that the hypothesis is able to better explain the variance of the learning targets y. Our online weak learning definition is closely related to the one from (Beygelzimer et al., 2015b) in that our definition is an result of the following two assumptions: (1) the online learning problem is agnostic-learnable (i.e., the weak learner has o(T )T \u2192 0 time-averaged regret against the best hypothesis h \u2208 H) with high probability:\nT\u2211 t=1 \u2016yt \u2212 ht(xt)\u20162 \u2264 min h\u2208H T\u2211 t=1 \u2016yt \u2212 h(xt)\u20162 + o(T ),\n(4)\nand (2) the restricted hypothesis class H is rich enough such that for any sequence of {yt, xt} with high probability:\nmin h\u2208H T\u2211 t=1 \u2016yt \u2212 h(xt)\u20162 \u2264 (1\u2212 \u03b3) T\u2211 t=1 \u2016yt\u20162 + o(T ).\n(5)\nOur definition of online weak learning directly generalizes the batch weak learning definition in (Grubb and Bagnell, 2011) to the online setting by the additional agnostic learnability assumption as shown in Eqn. 4.\nNote that we pick square losses (Eqn. 5) in our weak online learning definition. As we will show later, the goal is to enforce that the weak learners to accurately predict gradients, as was also originally used in the batch gradient boosting algorithm (Friedman, 2001). Least-squares losses are also shown to be important in streaming tasks by (Gao et al., 2016) for their superior computational and theoretical properties.\nThe above online weak learning edge definition immediately implies the following result, which is used in later proofs:\nLemma 4.2. Given the sequence of losses \u2016yt \u2212 h(xt)\u20162, 1 \u2264 t \u2264 T , the online weak learner generates a sequence of predictors {ht}t, such that:\nT\u2211 t=1 2yTt ht(xt) \u2265 \u03b3 T\u2211 t=1 \u2016yt\u20162 \u2212R(T ), \u03b3 \u2208 (0, 1]. (6)\nThe above lemma can be proved by expanding the square on the LHS of Eqn. 3, cancelling common terms and rearranging terms."}, {"heading": "4.1 Why Weak Learner Edge is Reasonable?", "text": "We demonstrate here that the weak online learning edge assumption is reasonable. Let us consider the case that the hypothesis class H is closed under scaling (meaning if h \u2208 H, then for all \u03b1 \u2208 R, \u03b1h \u2208 H) and let us assume x \u223c D, and y = f\u2217(x) for some unknown function f\u2217. We define the inner product \u3008h1, h2\u3009 of any two functions h1, h2 as Ex\u223cD[h1(x)Th2(x)] and the squared norm \u2016h\u20162 of any function h as \u3008h, h\u3009. We assume f\u2217 is bounded in a sense \u2016f\u2217(x)\u2016 \u2264 F \u2208 R+. The following proposition shows that as long as f\u2217 is not perpendicular to the span of H (f\u2217 6\u22a5 span(H)), i.e., \u2203h \u2208 span(H) such that \u3008h, f\u2217\u3009 6= 0, then we can achieve a non-zero edge:\nProposition 4.3. Consider any sequence of pairs {xt, yt}Tt=1, where xt is i.i.d sampled from D, yt = f\u2217(xt) and f\n\u2217 6\u22a5 span(H). Run any no-regret online algorithm A on sequence of losses {\u2016yt\u2212h(xt)\u20162}t and output a sequence of predictions {ht}t. With probability at least 1\u2212 \u03b4, there exists a weak online learning edge \u03b3 \u2208 (0, 1], such that:\nT\u2211 t=1 \u2016ht(xt)\u2212 yt\u20162 \u2264 (1\u2212 \u03b3) T\u2211 t=1 \u2016yt\u20162\n+RA(T ) + (2\u2212 \u03b3)O (\u221a T ln(1/\u03b4) ) ,\nwhere RA(T ) is the regret of online algorithm A.\nThe proof of the above proposition can be found in Appendix. Matching to Eq. 3, we have R(T ) = RA(T ) + (2\u2212 \u03b3)O (\u221a T ln(1/\u03b4) ) \u2208 o(T ). In addition,\nthe contrapositive of the proposition implies that without a positive edge, span(H) is orthogonal to f\u2217 so that no linear boosted ensemble can approximate f\u2217. Hence having a positive online weak learner edge is necessary for online boosted algorithms."}, {"heading": "5 ALGORITHM", "text": ""}, {"heading": "5.1 Smooth Loss Functions", "text": "We first present Streaming Gradient Boosting (SGB), an algorithm (Alg. 1) that is designed for loss func-\nAlgorithm 1 Streaming Gradient Boosting (SGB)\n1: Input: A restricted class H. N online weak learners {Ai}Ni=1. Learning rate \u03b7. 2: Each weak learner initlizes a hypothesis h1i \u2208 H,\u22001 \u2264 i \u2264 N . 3: for t = 1 to T do 4: Receive xt and initialize y 0 t = y0 (e.g., y0 = 0). 5: for i = 1 to N do 6: Set the partial sum yit = y i\u22121 t \u2212 \u03b7hti(xt). 7: end for 8: Predict yt = y N t .\n9: `t is revealed and learner suffers loss `t(yt). 10: for i = 1 to N do 11: Compute gradient w.r.t partial sum: \u2207ti = \u2207`t(yi\u22121t ). 12: Feed loss \u2016\u2207ti \u2212 hti(xt)\u20162 to Ai. 13: Weak learner Ai computes ht+1i using its noregret update procedure. 14: end for 15: end for 16: Set h\u0304i = 1 T \u2211T t=1 h t i,\u22001 \u2264 i \u2264 N .\n17: Return: { h\u03041, ..., h\u0304N } .\ntions {`t(y)} that are \u03bb-strongly convex and \u03b2-smooth. Alg. 1 is the online version of the classic batch gradient boosting algorithms (Friedman, 2001; Grubb and Bagnell, 2011). Alg. 1 maintains N weak learners. At each time step t, given example xt, the algorithm predicts yt by linearly combining the weak learners\u2019 predictions (Line 5). Then after receiving loss `t, for each weak learner, the algorithm computes the gradient of `t with respect to y evaluated at the partial sum yi\u22121t (Line 11) and feeds the square loss lt(h) with the computed gradient as the regression target to weak learnerAi (Line 12). The weak learner Ai then performs its own no-regret online update to compute ht+1i (Line 13).\nLine 16 and 17 are needed for stochastic setting. We compute the average h\u0304i for every weak learner Ai in Line 16. In testing time, given x \u223c D, we predict y as:\ny = y0 \u2212 \u03b7 N\u2211 i=1 h\u0304i(x). (7)\nSince we penalize the weak learners by the squared deviation of its own prediction and the gradient from the previous partial sum, we essentially force weak learners to produce predictions that are close to the gradients (in a no-regret perspective). With this perspective, SGB can be understood as using the weak learners\u2019 predictions as N gradient descent steps where the gradient of each step i is approximated by a weak learner\u2019s prediction (Line 5). Let us de-\nfine \u22060 = \u2211T t=1(`t(y 0 t ) \u2212 `t(f\u2217(xt))), for any f\u2217 \u2208 F .\nNamely \u22060 measures the performance of the initialization {y0t }t. Under our assumption that the loss is bounded, |`t(x)| \u2264 B, \u2200t, x, we can simply upper bound \u22060 as \u22060 \u2264 2BT . Alg. 1 has the following performance guarantee:\nTheorem 5.1. Assume weak learner Ai,\u2200i has weak online learning edge \u03b3 \u2208 (0, 1]. Let f\u2217 = arg minf\u2208F \u2211 t `t(f(xt)). There exists a \u03b7 = \u03b3 \u03b2(8\u22124\u03b3) , for \u03bb-strongly convex and \u03b2-smooth loss functions, `t, such that when T \u2192\u221e, Alg. 1 generates a sequence of predictions {yt}t where:\n1 T [ T\u2211 t=1 `t(yt)\u2212 T\u2211 t=1 `t(f \u2217(xt))] \u2264 2B(1\u2212 \u03b32\u03bb 16\u03b2 )N . (8)\nFor stochastic setting where (xt, `t) \u223c D independently, we have when T \u2192\u221e:\nE [ ` ( y0 \u2212 \u03b7 N\u2211 i=1 h\u0304i(x) ) \u2212 `(f\u2217(x)) ] \u2264 2B(1\u2212 \u03b3 2\u03bb 16\u03b2 )N .\n(9)\nThe expectation in Eqn. 9 of the above theorem is taken over the randomness of the sequence of pairs of loss and samples {`t, xt}Tt=1 (note that h\u0304i is dependent on `1, x1, ..., `T , xT ) and `, x. Theorem 5.1 shows that with infinite amount samples the average regret decreases exponentially as we increase the number of weak learners. This performance guarantee is very similar to classic batch boosting algorithms (Schapire and Freund, 2012; Grubb and Bagnell, 2011), where the empirical risk decreases exponentially with the number of algorithm iterations, i.e., the number of weak learners. Theorem 5.1 mirrors that of Theorem 1 in (Beygelzimer et al., 2015a), which bounds the regret of the Frank-Wolfe-based Online Gradient Boosting algorithm. Our results utilize the additional assumptions that the losses `t are strongly convex and that the weak learners have edge, allowing us to shrink the average regret exponentially with respect to N, while the average regret in (Beygelzimer et al., 2015a) shrinks in the order of 1/N (though this dependency on N is optimal under their setting).\nProof of Theorem 5.1, detailed in Appendix B, weaves our additional assumptions into the proof framework of gradient descent on smooth losses. In particular, using weak learner edge assumption, we derive Lemma 4.2 and the Lemma B.1 to relate parts of the strong smoothness expansion of the losses to the norm-squared of the gradients \u2016\u2207`t(yit)\u20162, which is an upper bound of 2\u03bb(`t(y i t)\u2212 `t(f\u2217(xt))) due to strong convexity. Using this observation, we can relate the total regret of the ensemble of the first i learners, \u2206i = \u2211T t=1(`t(y i t)\u2212 `t(f\u2217(xt))), with the regret from\nusing i+ 1 learners, \u2206i+1, and show that \u2206i+1 shrinks \u2206i by a constant fraction while only adding a small term O(R(T )) \u2208 o(T ). Solving the recursion on the sequence of \u2206i, we arrive at the final exponentially decaying regret bound in the number of learners.\nRemark Due to the weak online learning edge assumption, the regret bound shown in Eqn. 8 and the risk bound shown in Eqn. 9 are stronger than typical bounds in classic online learning, in a sense that we are competing against f\u2217 that could potentially be much more powerful than any hypothesis from H. For instance when the loss function is square loss `(f(x)) = \u2016f(x)\u2212 z\u20162, Theorem 5.1 essentially shows that the risk of the boosted hypothesis E[\u2016y0\u2212\u03b7 \u2211N i=1 h\u0304i(x)\u2212z\u20162] approaches to zero as N approaches to infinity, under the assumption that Ai,\u2200i have no-zero weak learning edge (e.g.,f\u2217 \u2208 span(H)). Note that this is analogous to the results of classification based batch boosting (Freund and Schapire, 1995; Grubb and Bagnell, 2011) and online boosting (Beygelzimer et al., 2015b): as number of weak learners increase, the average number of prediction mistakes approaches to zero. In other words, with the corresponding edge assumptions, these batch/online boosting classification algorithms can compete against any arbitrarily powerful classifier that always makes zero mistakes on any given training data."}, {"heading": "5.2 Non-smooth Loss Functions", "text": "The regret bound shown in Theorem 5.1 only applies for strongly convex and smooth loss functions. In fact, one can show that Alg. 1 will fail for general non-smooth loss functions. We can construct a sequence of nonsmooth loss functions and a special weak hypothesis class H, which together show that the regret of Alg. 1 grows linearly in the number of samples, regardless of the number of weak learners. We refer readers to Appendix D for more details.\nOur next algorithm, Alg. 2, extends SGB (Alg. 1) to handle strongly convex but non-smooth losses. Instead of training each weak learner to fit the subgradients of non-smooth loss with respect to current prediction, we instead keep track of a residual \u2206i\n3 that accumulates the difference between the subgradients, \u2207k, and the fitted prediction hk(xt), from k = 1 up to i\u22121. Instead of fitting the predictor hi+1 to match the subgradient \u2207i+1, we fit it to match the sum of the subgradient and the residuals, \u2207i+1 + \u2206i. More specifically, in Line 13 of Alg. 2, for each weak learner Ai, we feed a\n3Note the abusive notation. For the non-smooth loss setting (Alg. 2), \u2206i does not refer to the regret of the ensemble\u2019s regret with the i-th as used in the analysis of Alg. 1\nAlgorithm 2 Streaming Gradient Boosting (SGB) for non-smooth loss (Residual Projection)\n1: Input: A restricted class H. N online weak learners {Ai}Ni=1. Learning rate schedule {\u03b7i}Ni=1. 2: \u2200i,Ai initializes a hypothesis h1i \u2208 H. 3: for t = 1 to T do 4: Receive xt and initialize y 0 t = y0 (e.g., y0 = 0). 5: for i = 1 to N do 6: Set the projected partial sum yit = \u03a0Y(y i\u22121 t \u2212\n\u03b7ih t i(xt)).\n7: end for 8: Predict yt = 1 N \u2211N i=0 y i t\n9: The loss `t is revealed and compute loss `t(yt). 10: Set initial residual \u2206t0 = 0. 11: for i = 1 to N do 12: Compute subgradient w.r.t. partial sum: \u2207ti = \u2207`t(yi\u22121t ). 13: Feed loss\n\u2225\u2225(\u2206ti\u22121 +\u2207ti)\u2212 h(x)\u2225\u22252 to Ai. 14: Update residual: \u2206ti = \u2206 t i\u22121 +\u2207ti \u2212 hti(xt). 15: Weak learner Ai computes ht+1i using its noregret update procedure. 16: end for 17: end for 18: Return: hit, 1 \u2264 i \u2264 N, 1 \u2264 t \u2264 T .\nAlgorithm 3 SGB (Residual Projection) for testing\n1: Input: Test sample x and hit, 1 \u2264 i \u2264 N, 1 \u2264 t \u2264 T from the output of Alg. 2. 2: for t = 1 to T do 3: for i = 1 to N do 4: yit = \u03a0Y(y i\u22121 t \u2212 \u03b7ihti(x)). 5: end for 6: yt = 1 N \u2211N i=0 y i t. 7: end for 8: Predict: y = T (x) = 1T \u2211T t=1 yt.\nsquare loss with the sum of residual and the gradient as the regression target. Then Line 14 sets the new the residual \u2206ti as the difference between the target (\u2206ti\u22121+\u2207ti) and the weak learner Ai\u2019s prediction hti(xt).\nThe last line of Alg. 2 is needed for stochastic setting where (`t, xt) \u223c D i.i.d. In test, given sample x \u223c D, we predict y using hit,\u2200i, t in procedure shown in Alg. 3. For notation simplicity, we denote the testing procedure shown in Alg. 3 as T (x), which T explicitly depends on the returns hit, 1 \u2264 i \u2264 N, 1 \u2264 t \u2264 T from SGB (Residual Projection). Since it\u2019s impractical to store and apply all TN models, we follow a common stochastic learning technique which uses the final predictor at time T for testing (e.g., Johnson and Zhang (2013)) in the experiment section (i.e., simply set t = T in Line 3 in Alg. 3). In practice, if the learners converge and T\nis large, the average and final predictions are close.\nIntuitively, this approach prevents the weak learners from consistently failing to match a certain direction of the subgradient as the net error in the direction is stored in residual. By the assumption of weak learner edge, the directions will be approximated. We also note that if we assume the subgradients are bounded, then the residual magnitudes increase at most linearly in the number of weak learners. Simultaneously, each weak learner shrinks the residual by at least a constant factor due to the assumption of edge. Hence, we expect the residual to shrink exponentially in the number of learners. Utilizing this observation, we arrive at the following performance guarantee:\nTheorem 5.2. Assume the loss `t is \u03bb-strongly convex for all t with bounded gradients, \u2016\u2207`t(y)\u2016 \u2264 G for all y, and each weak learner Ai has edge \u03b3 \u2208 (0, 1]. Let F be a function space, and H \u2282 F be a restriction of F Let f\u2217 = arg minf\u2208F 1 T \u2211T t=1 `t(f(xt)) be the optimal predictor in F in hindsight. Let c = 2\u03b3 \u2212 1. Let step size be \u03b7i = 1 \u03bbi . When T \u2192\u221e, we have:\n1\nT T\u2211 t=1 (`t(yt)\u2212 `t(f\u2217(xt))) \u2264 4c2G2 \u03bbN (1 + lnN + 1 8N ).\n(10)\nFor stochastic setting where (xt, `t) \u223c D independently, when T \u2192\u221e we have:\nE [ `(T (x))\u2212 `(f\u2217(x)) ] \u2264 4c 2G2\n\u03bbN (1 + lnN +\n1\n8N ).\nThe above theorem shows that the average regret of Alg. 2 is O(lnN/N) with respect to the number N of weak learners, which matches the regret bounds of Online Gradient Descent for strongly convex loss. The key idea for proving Theorem 5.2 is to combine our online weak learning edge definition with the proof framework of Online Gradient Descent for strongly convex loss functions from (Hazan et al., 2007). The detailed proof can be found in Appendix C."}, {"heading": "6 EXPERIMENTS", "text": "We demonstrate the performance of our Streaming Gradient Boosting using the following UCI datasets (Lichman, 2013): YEAR, ABALONE, SLICE, and A9A (Kohavi and Becker) as well as the MNIST (LeCun et al., 1998) dataset. If available, we use the given train-test split of each data-set. Otherwise, we create a random 90%-10% train-test split."}, {"heading": "6.1 Experimental Analysis of Regret Bounds", "text": "We first demonstrate the relationships between the regret bounds shown in Eqn. 8 and the parameters\nincluding the number of weak learners, the number of samples and edge \u03b3. We compute the regret of SGB with respect to a deep regression tree (depth\u2265 15), which plays the f\u2217 in Eqn. 8. We use regression trees as the weak learners. We assume that deeper trees have higher edges \u03b3 because they empirically fit training data better. We show how the regret relates to the trees\u2019 depth, the number of weak learners N (Fig. 1a) and the number of samples T (Fig. 1b).\nFor the experimental results shown in Fig. 1, we used smooth loss functions with L2 regularization (see Appendix E for more details). We use logistic loss and square loss for binary classification (A9A) and regression task (SLICE), respectively. For each regression tree weak learner, Follow The Regularized Leader (FTRL) (Shalev-Shwartz, 2011) was used as the no-regret online update algorithm with regularization posed as the depth of the tree. Fig. 1a shows the relationship between the number of weak learners and the average regret given a fixed total number of samples. The average regret decreases as we increase the number of weak learners. We note that the curves are close to linear at the beginning, matching our theoretical analysis that the average regret decays exponentially (note the y-axis is log scale) with respect to the number of weak learners. This shows that SGB can significantly boost the performance of a single weak learner.\nTo investigate the effect of the edge parameter \u03b3, we additionally compute the average regret in Fig. 1 as the depth of the regression tree is increased. The tree depth increases the model complexity of the base learner and should relate to a larger \u03b3 edge parameter. From this experiment, we see that the average regret shrinks as the depth of the trees increases.\nFinally, Fig. 1b shows the convergence of the average regret with respect to the number of samples. We see that more powerful weak learners (deeper regression trees) results in faster convergence of our algorithm. We ran Alg. 2 on A9A with hinge loss and SLICE with L1 (least absolute deviation) loss and observed very similar results as shown in Fig. 1."}, {"heading": "6.2 Batch Boosting vs. Streaming Boosting", "text": "We next compare batch boosting to SGB using twolayer neural networks as weak learners4 and see that SGB reaches similar final performance as the batch boosting algorithm albeit with less training computation. As stated in Sec 5.2, we report hiT instead of h\u0304i for SGB, since at convergence the average prediction is close to the final prediction, and the latter is impractical to compute. We implement our baseline, the classic batch gradient boosting (GB) (Friedman, 2001), by optimizing each weak learner until convergence in order. In both GB and SGB, we train weak learners using ADAM (Kingma and Ba, 2015) optimization and use the default random parameter initialization for NN.\nWe analyze the complexity of training SGB and GB. We define the prediction complexity of one weak learner as the unit cost, since the training run-time complexity almost equates the total complexity of weak learner predictions and updates. Our choice of weak learner and update method (two-layer networks and ADAM) determines that updating a weak learner is about two units cost. In training using SGB, each of the T data samples triggers predictions and updates with all N of the weak learners. This results in a training computational complexity of 3TN = O(TN). For GB, let TB be the samples needed for each weak learner to converge. Then the complexity of training GB is TB \u2211N i=1 i + 2TBN ' 1 2TBN 2 = O(TBN 2), because when training weak learner i, all previous i\u2212 1 weak learners must also predict for each data point5. Hence, SGB and GB will have the same training complexity if TB ' 6TN = \u0398( T N ). In our experiments we observe weak learners typically converge less than TN samples, but our following experiment shows that SGB still can converge faster overall.\n4The number of hidden units by data-set: ABALONE, A9A: 1; YEAR, SLICE: 10; MNIST: 5x5 convolution with stride of 2 and 5 output channels. Sigmoid is used as the activation for all except SLICE, which uses leaky ReLU.\n5Saving previous predictions is disallowed, because data may not be revisited in an actual streaming setting.\nFig. 2 plots the test-time loss versus training computation, measured by the unit cost. Blue dots highlights when the weak learners are added in GB. We first note that SGB successfully converges to the results of GB in all cases, supporting that SGB is a truly a streaming conversion of GB. As it takes many weak learners to achieve good performance on ABALONE and YEAR, we observe that SGB converges with less computation than GB. On A9A, however, GB is more computationally efficient than SGB, because the first weak learner in GB already performs well and learning a single weak learner for GB is faster than simultaneously optimizing all N = 8 weak learners with SGB. This suggests that if we initially set N too big, SGB could be less computationally efficient. In fact Fig. 2f shows that very larger N causes slower convergence to the same final error plateau. On the other hand, small N (N = 3) results in worse performance. We specify the chosen N for SGB in Fig. 2, and they are around the number of weak learners that GB requires to converge and achieve good performance. We also note that SGB has slower initial progress compared to GB on SLICE in Fig. 2c and MNIST in Fig. 2e. This is an understandable result as SGB has a much larger pool of parameters to optimize. Despite this initial disadvantage, SGB surpasses GB and converges faster overall, suggesting the advantage of updating all the weak learners together. In practice, if we do not have a good guess of N , we can still use SGB to add multiple weak learners at a time in GB to speed up convergence. Table 1 records the test error (square error for regression and error ratio for classification) of the neural network base learner, GB, and SGB. We observe that SGB achieves test errors that are competitive with GB in all cases."}, {"heading": "7 CONCLUSION", "text": "In this paper, we present SGB for online convex programming. By introducing an online weak learning edge definition that naturally extends the edge definition from batch boosting to the online setting and by using square loss, we are able to boost the predictions from weak learners in a gradient descent fashion. Our SGB algorithm guarantees exponential regret shrinkage in the number N of weak learners for strongly convex and smooth loss functions. We additionally extend SGB for optimizing non-smooth loss function, which achieves O(lnN/N) no-regret rate. Finally, experimental results support the theoretical analysis.\nThough our SGB algorithm currently utilizes the procedure of gradient descent to combine the weak learners predictions, our online weak learning definition and the design of square loss for weak learners leave open the possibility to leverage other gradient-based update procedures such as accelerated gradient descent, mirror descent, and adaptive gradient descent for combining the weak learners\u2019 predictions."}, {"heading": "Acknowledgements", "text": "This material is based upon work supported in part by: Echo\u2019s Grant name, DARPA ALIAS contract number HR0011-15-C-0027, and National Science Foundation Graduate Research Fellowship Grant No. DGE1252522."}, {"heading": "E. J. Atkinson, T. M. Therneau, L. J. Melton, J. J. Camp,", "text": "S. J. Achenbach, S. Amin, and S. Khosla. Assessing fracture risk using gradient boosting machine (gbm) models. Journal of Bone and Mineral Research, 2012."}, {"heading": "A. Beygelzimer, E. Hazan, S. Kale, and H. Luo. Online", "text": "gradient boosting. In NIPS, pages 2449\u20132457, 2015a."}, {"heading": "A. Beygelzimer, S. Kale, and H. Luo. Optimal and adaptive", "text": "algorithms for online boosting. In ICML, pages 2323\u2013 2331, 2015b.\nN. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.\nO. Chapelle, Y. Chang, and T. Liu, editors. Proceedings of the Yahoo! Learning to Rank Challenge, held at ICML 2010, volume 14 of JMLR Proceedings, 2011.\nS.-T. Chen, H.-T. Lin, and C.-J. Lu. An online boosting algorithm with theoretical justifications. In ICML, 2012.\nM. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95\u2013110, 1956."}, {"heading": "Y. Freund and R. E. Schapire. A desicion-theoretic generalization of on-line learning and an application to boosting.", "text": "In European conference on computational learning theory, pages 23\u201337. Springer, 1995."}, {"heading": "Y. Freund and R. E. Schapire. A short introduction to", "text": "boosting. In Journal of Japanese Society for Artificial Intelligence, 1999."}, {"heading": "J. H. Friedman. Greedy function approximation: a gradient", "text": "boosting machine. Annals of statistics, pages 1189\u20131232, 2001.\nW. Gao, L. Wang, R. Jin, S. Zhu, and Z.-H. Zhou. Onepass auc optimization. In Artificial Intelligence Journal, volume 236, pages 1\u201329, 2016."}, {"heading": "H. Grabner and H. Bischof. On-line boosting and vision.", "text": "In CVPR, volume 1, pages 260\u2013267, 2006."}, {"heading": "H. Grabner, C. Leistner, and H. Bischof. Semisupervised", "text": "on-line boosting for robust tracking. In ECCV, page 234 247, 2008."}, {"heading": "A. Grubb and D. Bagnell. Generalized boosting algorithms", "text": "for convex optimization. In ICML, 2011."}, {"heading": "A. Grubb and D. Bagnell. Speedboost: Anytime prediction", "text": "with uniform near-optimality. In AISTATS, pages 458\u2013 466, 2012."}, {"heading": "E. Hazan and S. Kale. Beyond the regret minimization bar-", "text": "rier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15: 2489\u20132512, 2014.\nE. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169\u2013192, 2007."}, {"heading": "R. Johnson and T. Zhang. Accelerating stochastic gradient", "text": "descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, 2013."}, {"heading": "D. Kingma and J. Ba. Adam: A method for stochastic", "text": "optimization. ICLR, arXiv:1412.6980, 2015."}, {"heading": "R. Kohavi and B. Becker. Adult data set. UCI Machine", "text": "Learning Repository.\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 1998."}, {"heading": "C. Leistner, A. Saffari, P. M. Roth, and H. Bischof. On", "text": "robustness of on-line boosting - a competitive study. In ICCV Workshop on On-line Learning for Computer Vision, 2009.\nM. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml."}, {"heading": "L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting", "text": "algorithms as gradient descent. In NIPS, 2000."}, {"heading": "W. Nam, P. Dolla\u0301r, and J. H. Han. Local decorrelation for", "text": "improved pedestrian detection. In NIPS, pages 424\u2013432, 2014."}, {"heading": "N. C. Oza and S. Russell. Online bagging and boosting. In", "text": "AISTATS, pages 105\u2013112, 2001.\nR. E. Schapire and Y. Freund. Boosting: Foundations and algorithms. MIT press, 2012.\nS. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107\u2013194, 2011.\nP. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, volume 1. IEEE, 2001."}, {"heading": "B. Yang, J. Yan, Z. Lei, and S. Z. Li. Convolutional channel", "text": "features. In ICCV, pages 82\u201390, 2015.\nT. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. 33:15381579, 2005."}, {"heading": "Y. Zhang and A. Haghani. A gradient boosting method to", "text": "improve travel time prediction. Transportation Research Part C: Emerging Technologies, 58:308\u2013324, 2015."}, {"heading": "C. Zhu and Y. Peng. Group cost-sensitive boosting for", "text": "multi-resolution pedestrian detection. In AAAI, 2016.\nSupplementary Material for Gradient Boosting on Stochastic Data Streams"}, {"heading": "A Proof of Proposition 4.3", "text": "Proof. Given that a no-regret online learning algorithm A running on sequence of loss \u2016h(xt)\u2212 yt\u20162, we have can easily see that Eqn. 4 holds as:\nT\u2211 t=1 \u2016ht(xt)\u2212 yt\u20162 \u2264 min h\u2208H T\u2211 t=1 \u2016h(xt)\u2212 yt\u20162 +RA(T ), (11)\nwhere RA(T ) is the regret of A and is o(T ). To prove Proposition 4.3, we only need to show that Eqn. 5 holds for some \u03b3 \u2208 (0, 1]. This is equivalent to showing that there exist a hypothesis h\u0303 \u2208 H (\u2016h\u0303\u2016 = 1), such that \u3008h\u0303, f\u2217\u3009 > 0. To see this equivalence, let us assume that \u3008h\u0303, f\u2217/\u2016f\u2217\u2016\u3009 = > 0. Let us set h\u2217 = \u2016f\u2217\u2016h\u0303. Using Pythagorean theorem, we can see that \u2016h\u2217 \u2212 f\u2217\u20162 = (1\u2212 2)\u2016f\u2217\u20162. Hence we get \u03b3 is at least 2, which is in (0, 1].\nNow since we assume that f\u2217 6\u22a5 span(H), then there must exist h\u2032 \u2208 H, such that \u3008f\u2217, h\u2032\u3009 6= 0, otherwise f\u2217 \u22a5 H. Consider the hypothesis h\u2032/\u2016h\u2032\u2016 and \u2212h\u2032/\u2016h\u2032\u2016 (we assume H is closed under scale), we have that either \u3008h\u2032, f\u2217\u3009 > 0 or \u3008\u2212h\u2032, f\u2217\u3009 > 0. Namely, we find at least one hypothesis h such that \u3008h, f\u2217\u3009 > 0 and \u2016h\u2016 = 1. Hence if we pick h\u0303 = arg maxh\u2208H,\u2016h\u2016=1\u3008h, f\u2217/\u2016f\u2217\u2016\u3009, we must have \u3008h\u0303, f\u2217/\u2016f\u2217\u2016\u3009 = > 0. Namely we can find a hypothesis h\u2217 \u2208 H, which is \u2016f\u2217\u2016h\u0303, such that there is non-zero \u03b3 \u2208 (0, 1]:\n\u2016h\u2217 \u2212 f\u2217\u20162 \u2264 (1\u2212 \u03b3)\u2016f\u2217\u20162. (12)\nTo show that we can extend this \u03b3 to the finite sample case, we are going to use Hoeffding inequality to relate the norm \u2016 \u00b7 \u2016 to its finite sample approximation.\nApplying Hoeffding inequality, we get with probability at least 1\u2212 \u03b4/2,\n| 1 T T\u2211 t=1 \u2016yt\u20162 \u2212 \u3008f\u2217, f\u2217\u3009| \u2264 O (\u221aF 2 T ln(4/\u03b4) ) , (13)\nwhere based on assumption that f\u2217(\u00b7) is bounded as \u2016f\u2217(\u00b7)\u2016 \u2264 F . Similarly, we have with probability at least 1\u2212 \u03b4/2:\n| 1 T T\u2211 t=1 \u2016h\u2217(xt)\u2212 f\u2217(xt)\u20162 \u2212 \u2016h\u2217 \u2212 f\u2217\u20162| \u2264 O (\u221aF 2 T ln(4/\u03b4) ) , (14)\nApply union bound for the above two high probability statements, we get with probability at least 1\u2212 \u03b4,\n| 1 T T\u2211 t=1 y2t \u2212 \u3008f\u2217, f\u2217\u3009| \u2264 O (\u221aF 2 T ln(4/\u03b4) ) , and,\n| 1 T T\u2211 t=1 (h\u2217(xt)\u2212 f\u2217(xt))2 \u2212 \u2016h\u2217 \u2212 f\u2217\u2016| \u2264 O (\u221aF 2 T ln(4/\u03b4) ) . (15)\nNow to prove the theorem, we proceed as follows:\n1\nT T\u2211 t=1 \u2016h\u2217(xt)\u2212 f\u2217(xt)\u20162\n\u2264 \u2016h\u2217 \u2212 f\u2217\u2016+O (\u221aF 2\nT ln(4/\u03b4) ) \u2264 (1\u2212 \u03b3)\u2016f\u2217\u20162 +O (\u221aF 2 T ln(4/\u03b4) )\n\u2264 (1\u2212 \u03b3) 1 T T\u2211 t=1 y2t + (1\u2212 \u03b3)O (\u221aF 2 T ln(4/\u03b4) ) +O (\u221aF 2 T ln(4/\u03b4) ) . (16)\nHence we get with probability at least 1\u2212 \u03b4:\nT\u2211 t=1 \u2016h\u2217(xt)\u2212 f\u2217(xt)\u20162 \u2264 T\u2211 t=1 \u2016yt\u20162 + (2\u2212 \u03b3)O (\u221a T ln(1/\u03b4) ) . (17)\nSet R(T ) = RA(T ) + (2\u2212 \u03b3)O (\u221a T ln(1/\u03b4) ) , we prove the proposition."}, {"heading": "B Proof of Theorem 5.1", "text": "An important property of \u03bb-strong convexity that we will use later in the proof is that for any x and x\u2217 = arg minx l(x), we have:\n\u2016\u2207l(x)\u20162 \u2265 2\u03bb(l(x)\u2212 l(x\u2217)). (18)\nWe prove Eqn. 18 below.\nFrom the \u03bb-strong convexity of l(x), we have:\nl(y) \u2265 l(x) +\u2207l(x)(y \u2212 x) + \u03bb 2 \u2016y \u2212 x\u20162. (19)\nReplace y by x\u2217 in the above equation, we have:\nl(x\u2217) \u2265 l(x) +\u2207l(x)(x\u2217 \u2212 x) + \u03bb 2 \u2016x\u2217 \u2212 x\u20162\n\u21d22\u03bbl(x\u2217) \u2265 2\u03bbl(x) + 2\u03bb\u2207l(x)(x\u2217 \u2212 x) + \u03bb2\u2016x\u2217 \u2212 x\u20162 \u21d2\u2212 2\u03bb\u2207l(x)(x\u2217 \u2212 x)\u2212 \u03bb2\u2016x\u2217 \u2212 x\u20162 \u2265 2\u03bb(l(x)\u2212 l(x\u2217)) \u21d2\u2016\u2207l(x)\u20162 \u2212 \u2016\u2207l(x)\u20162 \u2212 2\u03bb\u2207l(x)(x\u2217 \u2212 x)\u2212 \u03bb2\u2016x\u2217 \u2212 x\u20162 \u2265 2\u03bb(l(x)\u2212 l(x\u2217)) \u21d2\u2016\u2207l(x)\u20162 \u2212 \u2016\u2207l(x) + \u03bb(x\u2217 \u2212 x)\u20162 \u2265 2\u03bb(l(x)\u2212 l(x\u2217)) \u21d2\u2016\u2207l(x)\u20162 \u2265 2\u03bb(l(x)\u2212 l(x\u2217)). (20)\nB.1 Proofs for Lemma 4.2\nProof. Complete the square on the left hand side (LHS) of Eqn. 3, we have:\u2211 \u2016yt\u20162 \u2212 2yTt ht(xt) + \u2016ht(xt)\u20162 \u2264 (1\u2212 \u03b3) \u2211 t \u2016yt\u20162 +R(T ). (21)\nNow let us cancel the \u2211 y2t from both side of the above inequality, we have:\u2211 \u22122yTt ht(xt) \u2264 \u2211 \u22122yTt ht(xt) + \u2016ht(xt)\u20162 \u2264 \u2212\u03b3 \u2211 \u2016yt\u20162 +R(T ). (22)\nRearrange, we have: \u2211 2yTt ht(xt) \u2265 \u03b3 \u2211 \u2016yt\u20162 \u2212R(T ). (23)\nB.2 Proof of Theorem 5.1\nWe need another lemma for proving theorem 5.1:\nLemma B.1. For each weak learner Ai, we have:\u2211 t \u2016hit(xt)\u20162 \u2264 (4\u2212 2\u03b3) \u2211 t \u2016\u2207`t(yi\u22121t )\u20162 + 2R(T ). (24)\nProof of Lemma B.1. For \u2211 t(h i t(xt))\n2, we have:\u2211 t \u2016hit(xt)\u20162 = \u2211 t \u2016hit(xt)\u2212\u2207`t(yi\u22121t ) +\u2207`t(yi\u22121t )\u20162\n\u2264 \u2211 t \u2016hit(xt)\u2212\u2207`t(yi\u22121t )\u20162 + \u2211 t \u2016\u2207`tyi\u22121t \u20162 + \u2211 t 2(hit(xt)\u2212\u2207`t(yt)i\u22121)T\u2207`t(yi\u22121t )\n\u2264 \u2211 t 2\u2016hit(xt)\u2212\u2207`t(yi\u22121t )\u20162 + \u2211 t 2\u2016\u2207`t(yi\u22121t \u20162\n\u2264 2(1\u2212 \u03b3) \u2211 t \u2016\u2207`t(yi\u22121t \u20162 + 2R(T ) + 2 \u2211 t \u2016\u2207`t(yi\u22121t \u20162\n(By Weak Onling Learning Definition) \u2264 (4\u2212 2\u03b3) \u2211 t \u2016\u2207`t(yi\u22121t \u20162 + 2R(T ). (25)\nProof of Theorem 5.1. For 1 \u2264 i \u2264 N , let us define \u2206i = \u2211T t=1(`t(y i t) \u2212 `t(f\u2217(xt))). Following similar proof strategy as shown in (Beygelzimer et al., 2015a), we will link \u2206i to \u2206i\u22121. For \u2206i, we have:\n\u2206i = T\u2211 t=1 (`t(y i t)\u2212 `t(f\u2217(xt))) = \u2211 t `t(y i\u22121 t \u2212 \u03b7hit(xt))\u2212 \u2211 t `t(f \u2217(xt))\n\u2264 \u2211 t [ `t(y i\u22121 t )\u2212 \u03b7\u2207`t(yi\u22121t )Thit(xt) + \u03b2\u03b72 2 \u2016hit(xt)\u20162 ] \u2212 \u2211 t `t(f \u2217(xt))\n(By \u03b2-smoothness of `t) \u2264 \u2211 t [ `t(y i\u22121 t )\u2212 \u03b7\u03b3 2 \u2016\u2207`t(yi\u22121t )\u20162 + \u03b7R(T ) 2 + \u03b2\u03b72 2 \u2016hit(xt)\u20162 ] \u2212 \u2211 t `t(f \u2217(xt))\n(By Lemma 4.2) \u2264 \u2211 t [ `t(y i\u22121 t )\u2212 \u03b7\u03b3 2 \u2016\u2207`t(yi\u22121t )\u20162 + \u03b7R(T ) 2 + \u03b2\u03b72(2\u2212 \u03b3)\u2016\u2207`t(yi\u22121t )\u20162 + \u03b2\u03b72R(T )\u2212 `t(f\u2217(xt)) ] (By Lemma B.1) = \u2206i\u22121 \u2212 ( \u03b7\u03b3\n2 \u2212 \u03b2\u03b72(2\u2212 \u03b3)) \u2211 t \u2016\u2207`t(yi\u22121t )\u20162 + ( \u03b7 2 + \u03b2\u03b72)R(T )\n\u2264 \u2206i\u22121 \u2212 (\u03b7\u03b3\u03bb\u2212 \u03b2\u03b72\u03bb(4\u2212 2\u03b3)) \u2211 t ( `t(y i\u22121 t )\u2212 `t(f\u2217(xt)) ) + ( \u03b7 2 + \u03b2\u03b72)R(T )\n(By Eqn. 18) = \u2206i\u22121 [ 1\u2212 (\u03b7\u03b3\u03bb\u2212 \u03b2\u03b72\u03bb(4\u2212 2\u03b3)) ] + ( \u03b7\n2 + \u03b2\u03b72)R(T ) (26)\nDue to the setting of \u03b7, we know that 0 < (1\u2212 (\u03b7\u03b3\u03bb\u2212 \u03b2\u03b72\u03bb(4\u2212 2\u03b3))) < 1. For notation simplicity, let us first define C = 1\u2212 (\u03b7\u03b3\u03bb\u2212 \u03b2\u03b72\u03bb(4\u2212 2\u03b3)). Starting from \u22060, keep applying the relationship between \u2206i and \u2206i\u22121 N times, we have:\n\u2206N = C N\u22060 + (\n\u03b7 2 + \u03b2\u03b72)R(T ) N\u2211 i=1 Ci\u22121\n= CN\u22060 + ( \u03b7\n2 + \u03b2\u03b72)R(T )\n1\u2212 CN\n1\u2212 C\n\u2264 CN\u22060 + ( \u03b7\n2 + \u03b2\u03b72)R(T )\n1\n1\u2212 C .\nNow divide both sides by T , and take T to infinity, we have:\n1 T \u2206N = C N 1 T \u22060 \u2264 CN2B, (27)\nwhere we simply assume that `t(y) \u2208 [\u2212B,B], B \u2208 R+ for any t and y. Now let us go back to C, to minimize C, we can take the derivative of C with respect to \u03b7, set it to zero and solve for \u03b7, we will have:\n\u03b7 = \u03b3\n\u03b2(8\u2212 4\u03b3) . (28)\nSubstitute this \u03b7 back to C, we have:\nC = 1\u2212 \u03b3 2\u03bb \u03b2(16\u2212 8\u03b3) \u2265 1\u2212 \u03bb 8\u03b2 \u2265 1\u2212 1 8 = 7 8 . (29)\nHence, we can see that there exist a \u03b7 = \u03b3\u03b2(8\u22124\u03b3) , such that:\n1 T \u2206N \u2264 2B(1\u2212\n\u03b32\u03bb\n\u03b2(16\u2212 8\u03b3) )N \u2264 2B(1\u2212 \u03b3\n2\u03bb\n16\u03b2 )N . (30)\nHence we prove the first part of the theorem regarding the regret. For the second part of the theorem where `t and xt are i.i.d sampled from a fixed distribution, we proceed as follows.\nLet us take expectation on both sides of the inequality 30. The left hand side of inequality 30 becomes:\n1 T E\u2206N = E 1 T [ T\u2211 t=1 (`t(y N t )\u2212 `t(f\u2217(xt))) ] = 1 T E [ T\u2211 t=1 `t(\u2212\u00b5 N\u2211 i=1 hit(xt)) ] \u2212 1 T E(`t,xt)\u223cD[`t(f \u2217(xt))]\n= 1\nT T\u2211 i=1 Et [ `t(\u2212\u00b5 N\u2211 i=1 hit(xt)) ] \u2212 E(`,x)\u223cD`(f\u2217(x)), (31)\nwhere the expectation is taken over the randomness of xt and `t. Note that h i t only depends on x1, `1, ..., xt\u22121, `t\u22121. We also define Et as the expectation over the randomness of xt and `t at step t conditioned on x1, `1, ..., xt\u22121, `t\u22121. Since `t, xt are sampled i.i.d from D, we can simply write Et[`t(\u2212\u00b5 \u2211N i=1 h i t(xt))] as Et[`(\u2212\u00b5 \u2211N i=1 h i t(x))]. Now the above inequality can be simplied as:\n1 T E\u2206N = 1 T T\u2211 t=1 Et[`(\u2212\u00b5 N\u2211 i=1 hit(x))]\u2212 E(`,x)\u223cD`(f\u2217(x)) \u2265 E [ `(\u2212\u00b5\nN\u2211 i=1 1 T T\u2211 t=1 hit(x)) ] \u2212 E(`,x)\u223cD`(f\u2217(x))\n= E [ `(\u2212\u00b5 N\u2211 i=1 h\u0304i(x)) ] \u2212 E(`,x)\u223cD`(f\u2217(x)) (32)\nNow use the fact that 1/TE\u2206N \u2264 2B(1\u2212 \u03b3 2\u03bb 16\u03b2 ) N , we prove the theorem."}, {"heading": "C Proof of Theorem 5.2", "text": "Lemma C.1. In Alg. 2, if we assume the 2-norm of gradients of the loss w.r.t. partial sums by G (i.e., \u2016\u2207it\u2016 = \u2016\u2207`t(yi\u22121t )\u2016 \u2264 G), and assume that each weak learner Ai has regret R(T ) = o(T ), then we there exists a constant c = 1\u2212\u03b3+ \u221a 1\u2212\u03b3(1\u2212R(T ) TG2 )\n\u03b3 < 2 \u03b3 \u2212 1 such that\nT\u2211 t=1 \u2016\u2206ti\u20162 \u2264 c2G2T and T\u2211 t=1 \u2016hti(xt)\u20162 \u2264 (4\u2212 2\u03b3)(1 + c)2G2T + 2R(T ) \u2264 4c2G2T. (33)\nProof. We prove the first inequality by induction on the weak learner index i. When i = 0, the claim is clearly true since \u2206t0 = 0 for all t. Now we assume the claim is true for some i \u2265 0, and prove it for i+ 1. We first note\nthat by the inequality 1T \u2211T t=1 at \u2264 \u221a\u2211 t a 2 t T for all sequence {at}t, we have\n1 T ( \u2211 t \u2016\u2206ti\u2016)2 \u2264 \u2211 t \u2016\u2206ti\u20162 \u2264 c2G2T (34)\n\u21d2( \u2211 t \u2016\u2206ti\u2016)2 \u2264 c2G2T 2 (35)\n\u21d2 \u2211 t \u2016\u2206ti\u2016 \u2264 cGT (36)\nThen by the assumption that weak learner Ai has an edge \u03b3 with regret R(T ), we have from step 14 of Alg. 2:\u2211 t \u2016\u2206ti+1\u20162 = \u2211 t \u2016\u2206ti +\u2207ti+1 \u2212 hti+1(xt)\u20162 \u2264 (1\u2212 \u03b3) \u2211 t \u2016\u2206ti +\u2207ti+1\u20162 +R(T ) (37)\n\u2264 (1\u2212 \u03b3) \u2211 t ( \u2016\u2206ti\u2016+G )2 +R(T ) (38)\n\u2264 (1\u2212 \u03b3) (\u2211 t \u2016\u2206ti\u20162 + 2G \u2211 t \u2016\u2206ti\u2016+G2T ) +R(T ) (39) \u2264 (1\u2212 \u03b3)(1 + c)2G2T +R(T ) (40) = c2G2T (41)\nWe have the last equality because c is chosen as the positive root of the quadratic equation: \u03b3c2 + (2\u03b3 \u2212 2)c+ (\u03b3 \u2212 1\u2212 R(T )TG2 ) = 0, which is equivalent to c 2G2T = (1\u2212 \u03b3)(c+ 1)2G2T +R(T ). The second inequality of the lemma can be derived from a similar argument of Lemma B.1 by expanding \u2016 ( \u2206ti\u22121 +\u2207ti \u2212 hti(xt) ) \u2212 ( \u2206ti\u22121 +\u2207ti ) \u20162 and then applying edge assumption.\nWe now use the above lemma to prove the performance guarantee of Alg. 2 as follows.\nProof of Theorem 5.2. We first define the intermediate predictors as: f t0(x) := h0(x), f\u0302 t i (x) := f t\u22121(x)\u2212 \u03b7ihti(x), and f ti (x) := P (f\u0302 t i (x)). Then for all i = 1, ..., N we have:\n\u2016f ti (xt)\u2212 f\u2217(xt)\u20162 \u2264 \u2016f\u0302 ti (xt)\u2212 f\u2217(xt)\u20162 = \u2016f ti\u22121(xt)\u2212 \u03b7ihti(xt)\u2212 f\u2217(xt)\u20162 (42) = \u2016f ti\u22121(xt)\u2212 f\u2217(xt)\u20162 + \u03b72i \u2016hti(xt)\u20162 \u2212 2\u03b7i \u2329 f ti\u22121(xt)\u2212 f\u2217(xt), hti(xt)\u2212\u2206ti\u22121 \u2212\u2207ti \u232a \u2212 2\u03b7i \u2329 f ti\u22121(xt)\u2212 f\u2217(xt),\u2206ti\u22121 +\u2207ti \u232a (43)\nRearanging terms we have:\u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2207ti \u232a (44)\n\u2265 1 2\u03b7i \u2016f ti (xt)\u2212 f\u2217(xt)\u20162 \u2212 1 2\u03b7i \u2016f ti\u22121(xt)\u2212 f\u2217(xt)\u20162 \u2212 \u03b7i 2 \u2016hti(xt)\u20162\n\u2212 \u2329 f\u2217(xt)\u2212 f ti\u22121(xt), hti(xt)\u2212\u2206ti\u22121 \u2212\u2207ti \u232a \u2212 \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2206ti\u22121 \u232a (45)\nUsing \u03bb-strongly convex of `t and applying the above equality and \u2206 t i = \u2206 t i\u22121 +\u2207ti \u2212 hti(xt), we have:\n`t(f \u2217(xt)) \u2265 `t(f ti\u22121(xt)) + \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2207ti \u232a + \u03bb\n2 \u2016f\u2217(xt)\u2212 f ti\u22121(xt)\u20162 (46)\n\u2265`t(f ti\u22121(xt)) + 1\n2\u03b7i \u2016f ti (xt)\u2212 f\u2217(xt)\u20162 \u2212\n1\n2\u03b7i \u2016f ti\u22121(xt)\u2212 f\u2217(xt)\u20162 \u2212 \u03b7i 2 \u2016hti(xt)\u20162\n+ \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2206ti \u232a \u2212 \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2206ti\u22121 \u232a + \u03bb\n2 \u2016f\u2217(xt)\u2212 f ti\u22121(xt)\u20162 (47)\nSumming over t = 1, ..., T and i = 1, ..., N we have:\nN T\u2211 t=1 `t(f \u2217(xt))\n\u2265 N\u2211 i=1 T\u2211 t=1 [ `t(f t i\u22121(xt)) + \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2207ti \u232a + \u03bb 2 \u2016f\u2217(xt)\u2212 f ti\u22121(xt)\u20162 ] (48)\n= N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 T\u2211 t=1 \u03b7i 2 \u2016hti(xt)\u20162\n+ N\u2211 i=1 T\u2211 t=1 1 2\u03b7i \u2016f ti (xt)\u2212 f\u2217(xt)\u20162 \u2212 N\u2211 i=1 T\u2211 t=1 ( 1 2\u03b7i \u2212 \u03bb 2 )\u2016f ti\u22121(xt)\u2212 f\u2217(xt)\u20162\n+ N\u2211 i=1 T\u2211 t=1 \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2206ti \u232a \u2212 N\u2211 i=1 T\u2211 t=1 \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2206ti\u22121 \u232a (49)\n= N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 T\u2211 t=1 \u03b7i 2 \u2016hti(xt)\u20162\n+ N\u2211 i=1 T\u2211 t=1 1 2\u03b7i \u2016f ti (xt)\u2212 f\u2217(xt)\u20162 \u2212 N\u22121\u2211 i=0 T\u2211 t=1 ( 1 2\u03b7i+1 \u2212 \u03bb 2 )\u2016f ti (xt)\u2212 f\u2217(xt)\u20162\n+ N\u2211 i=1 T\u2211 t=1 \u2329 f\u2217(xt)\u2212 f ti\u22121(xt),\u2206ti \u232a \u2212 N\u22121\u2211 i=1 T\u2211 t=1 \u2329 f\u2217(xt)\u2212 (f ti\u22121(xt)\u2212 \u03b7ihti(xt)),\u2206ti \u232a \u2212\nT\u2211 t=1 \u2329 f\u2217(xt)\u2212 f t0(xt),\u2206t0 \u232a (We switched index and apply \u2206t0 = 0 next.) (50)\n= N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 T\u2211 t=1 \u03b7i 2 \u2016hti(xt)\u20162 \u2212 N\u22121\u2211 i=1 T\u2211 t=1 \u2329 \u03b7ih t i(xt),\u2206 t i \u232a +\nN\u22121\u2211 i=1 T\u2211 t=1 1 2 \u2016f ti (xt)\u2212 f\u2217(xt)\u20162( 1 \u03b7i \u2212 1 \u03b7i+1 + \u03bb)\u2212 T\u2211 t=1 ( 1 2\u03b71 \u2212 \u03bb 2 )\u2016f t0(xt)\u2212 f\u2217(xt)\u20162\n+ T\u2211 t=1 [\u2329 f\u2217(xt)\u2212 f tN\u22121(xt),\u2206tN \u232a + 1 2\u03b7N \u2016f tN\u22121(xt)\u2212 \u03b7NhtN (xt)\u2212 f\u2217(xt)\u20162 ] (51)\n(We next apply \u03b7i = 1\n\u03bbi and complete the squares for the last sum.)\n= N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 T\u2211 t=1 \u03b7i 2 \u2016hti(xt)\u20162 \u2212 N\u22121\u2211 i=1 T\u2211 t=1 \u2329 \u03b7ih t i(xt),\u2206 t i \u232a + 1\n2\u03b7N T\u2211 t=1 \u2016 ( f tN\u22121(xt)\u2212 f\u2217(xt) ) + \u03b7N (\u2206 t N \u2212 htN (xt))\u20162\n\u2212 \u03b7N 2 T\u2211 t=1 ( \u2016\u2206tN \u2212 htN (xt)\u20162 \u2212 \u2016htN (xt)\u20162 ) (52)\n(We next drop the completed square, and apply Cauchy-Schwarz)\n\u2265 N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 T\u2211 t=1 \u03b7i 2 \u2016hti(xt)\u20162 \u2212 N\u2211 i=1 \u03b7i T\u2211 t=1 \u2016hti(xt)\u2016\u2016\u2206ti\u2016 \u2212 \u03b7N 2 T\u2211 t=1 \u2016\u2206tN\u20162 (53)\n(We next apply Cauchy-Schwarz again.)\n\u2265 N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 \u03b7i 2 T\u2211 t=1 \u2016hti(xt)\u20162 \u2212 \u03b7N 2 T\u2211 t=1 \u2016\u2206tN\u20162\n\u2212 N\u2211 i=1 \u03b7i \u221a\u221a\u221a\u221a T\u2211 t=1 \u2016hti(xt)\u20162 T\u2211 t=1 \u2016\u2206ti\u20162 (54)\nNow we apply Lemma C.1 and replace the remaining \u03b7i = 1 \u03bbi . Using \u2211N i=1 1 i \u2264 1 + lnN , we have:\nN T\u2211 t=1 `t(f \u2217(xt))\n\u2265 N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 N\u2211 i=1 1 2i\u03bb 4c2G2T \u2212 1 2N\u03bb c2G2T \u2212 N\u2211 i=1 1 i\u03bb 2c2G2T (55)\n\u2265 N\u2211 i=1 T\u2211 t=1 `t(f t i\u22121(xt))\u2212 4c2G2T \u03bb (1 + lnN)\u2212 c 2G2T 2N\u03bb (56)\nDividing both sides by NT and rearrange terms, we get:\n1\nTN N\u2211 i=1 T\u2211 t=1 [ `t(y i t)\u2212 `t(f\u2217(xt)) ] \u2264 4c 2G2 N\u03bb (1 + lnN) + c2G2 2N2\u03bb .\nUsing Jensen\u2019s inequality for the LHS of the above inequality, we get:\n1\nT T\u2211 t=1 `t( 1 N N\u2211 i=1 yit)\u2212 `t(f\u2217(xt)) \u2264 4c2G2 N\u03bb (1 + lnN) + c2G2 2N2\u03bb ,\nwhich proves the first part of the theorem.\nFor stochastic setting, we can prove it by using similar proof techniques (e.g., take expectation on both sides of Eqn. 57 and use Jensen inequality) that we used for proving theorem 5.1."}, {"heading": "D Counter Example for Alg. 1", "text": "In this section, we provide an counter example where we show that Alg. 1 cannot guarantee to work for non-smooth loss. We set y \u2208 R2, and design a loss function `t(y) = 2|y[1]| + |y[2]|, where y[i] stands for the i\u2019th entry of the vector y, for all time step t. The subgradient of this non-smooth loss is [2, 1]T , or [2,\u22121]T , or [\u22122, 1]T , or [\u22122,\u22121]T , depending on the position of y. We restricted the weak hypothesis class H to consist of only two types of hypothesis: hypothesis h(x) = [\u03b1, 0]T , or hypothesis h(x) = [0, \u03b1]T , where \u03b1 \u2208 [\u22122, 2]. We can show that given a sequence of training examples {(x\u03c4 , g\u03c4 )}t\u03c4=1, where gt is the one of the gradient from the total four possible subgradient of `t, the hypothesis that minimizes the accumulated square loss \u2211t \u03c4=1(h(x\u03c4 )\u2212 g\u03c4 )2 is going to be the type of h(x) = [\u03b1, 0]T .\nNow we consider using Follow the Leader (FTL) as a no-regret online learning algorithm for each weak learner. Based on the above analysis, we know that no matter what the sequence of training examples each weak learner has received as far, the weak leaners always choose the hypothesis with type h(x) = [\u03b1, 0]T from H. So, for every time step t, if we initialize y0t = [a, b]\nT , where a > 0 and b > 0, then the output yNt (computed from Line 8 in Alg.1) always have the form of yNt = [\u03b7, b], where \u03b7 \u2208 R. Namely, all weak learners\u2019 prediction only moves yt horizontally and it will never be moved vertically. But note that the optimal solution is located at [0, 0]T . Since for all t, yNt[2] is also b constant away from 0, the total regret accumulates linearly as bT , regardless of how many weak learners we have."}, {"heading": "E Details of Implementation", "text": "E.1 Binary Classification\nFor binary classification, following (Friedman, 2001), let us define feature x \u2208 Rn, label u \u2208 {\u22121, 1}. With xt and ut, the loss function `t is defined as:\n`t(y) = ln(1 + exp(\u2212uty)) + \u03bby2. (57)\nwhere y \u2208 R. In this setting, we have H : Rn \u2192 R. The regularization is to avoid overfitting: we can set y =\u221e\u2217 sign(ut) to make the loss close to zero.\nThe loss function `t(y) is twice differentiable with respect to y, and the second derivative is:\n\u22072`t(y) = exp(uty)\n(1 + exp(uty))2 (58)\nNote that we have:\n\u22072`t(y) \u2264 1 1/ exp(uty) + 2 + exp(uty) \u2264 1 4 . (59)\nHence, `t(y) is 1/4-smooth.\nUnder the assumption that the output from hypothesis from H is bounded as |y| \u2264 Y \u2208 R+, we also have:\n\u22072`t(y) \u2265 1\n2 + 2 exp(Y ) (60)\nHence, with boundness assumption, we can see that `t(y) is 1/(2 + 2 exp(Y ))-strongly convex and (1/4)-smooth.\nThe another loss we tried is the hinge loss:\n`t(y) = max(0, 1\u2212 uty) + \u03bby2. (61)\nWith the regularization, the loss `t(y) is still strongly convex, but no longer smooth.\nE.2 Multi-class Classification\nFollow the settings in (Friedman, 2001), for multi-class classification problem, let us define feature x \u2208 Rn, and label information u \u2208 Rk, as a one-hot representation, where u[i] = 1 (u[i] is the i-th element of u), if the example is labelled by i, and u[i] = 0 otherwise. The loss function `t is defined as:\n`t(y) = \u2212 k\u2211 i=1 ut[i] ln exp(y[i])\u2211k j=1 exp(y[j]) , (62)\nwhere y \u2208 Rk. In this setting, we let weak learner i pick hypothesis h from H that takes feature xt as input, and output y\u0302i \u2208 Rk. The online boosting algorithm then linearly combines the weak learners\u2019 prediction to predict y."}], "references": [{"title": "Assessing fracture risk using gradient boosting machine (gbm) models", "author": ["E.J. Atkinson", "T.M. Therneau", "L.J. Melton", "J.J. Camp", "S.J. Achenbach", "S. Amin", "S. Khosla"], "venue": "Journal of Bone and Mineral Research,", "citeRegEx": "Atkinson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Atkinson et al\\.", "year": 2012}, {"title": "Online gradient boosting", "author": ["A. Beygelzimer", "E. Hazan", "S. Kale", "H. Luo"], "venue": "In NIPS,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2015}, {"title": "Optimal and adaptive algorithms for online boosting", "author": ["A. Beygelzimer", "S. Kale", "H. Luo"], "venue": "In ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2015}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Proceedings of the Yahoo! Learning to Rank Challenge, held at ICML 2010, volume", "author": ["O. Chapelle", "Y. Chang", "T. Liu", "editors"], "venue": "JMLR Proceedings,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "An online boosting algorithm with theoretical justifications", "author": ["S.-T. Chen", "H.-T. Lin", "C.-J. Lu"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval research logistics quarterly,", "citeRegEx": "Frank and Wolfe.,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe.", "year": 1956}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In European conference on computational learning theory,", "citeRegEx": "Freund and Schapire.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1995}, {"title": "A short introduction to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Journal of Japanese Society for Artificial Intelligence,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of statistics,", "citeRegEx": "Friedman.,? \\Q2001\\E", "shortCiteRegEx": "Friedman.", "year": 2001}, {"title": "Onepass auc optimization", "author": ["W. Gao", "L. Wang", "R. Jin", "S. Zhu", "Z.-H. Zhou"], "venue": "In Artificial Intelligence Journal,", "citeRegEx": "Gao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "On-line boosting and vision", "author": ["H. Grabner", "H. Bischof"], "venue": "In CVPR,", "citeRegEx": "Grabner and Bischof.,? \\Q2006\\E", "shortCiteRegEx": "Grabner and Bischof.", "year": 2006}, {"title": "Semisupervised on-line boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "In ECCV,", "citeRegEx": "Grabner et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grabner et al\\.", "year": 2008}, {"title": "Generalized boosting algorithms for convex optimization", "author": ["A. Grubb", "D. Bagnell"], "venue": "In ICML,", "citeRegEx": "Grubb and Bagnell.,? \\Q2011\\E", "shortCiteRegEx": "Grubb and Bagnell.", "year": 2011}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["A. Grubb", "D. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Grubb and Bagnell.,? \\Q2012\\E", "shortCiteRegEx": "Grubb and Bagnell.", "year": 2012}, {"title": "Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale.,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR, arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "On robustness of on-line boosting - a competitive study", "author": ["C. Leistner", "A. Saffari", "P.M. Roth", "H. Bischof"], "venue": "In ICCV Workshop on On-line Learning for Computer Vision,", "citeRegEx": "Leistner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leistner et al\\.", "year": 2009}, {"title": "Boosting algorithms as gradient descent", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "In NIPS,", "citeRegEx": "Mason et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mason et al\\.", "year": 2000}, {"title": "Local decorrelation for improved pedestrian detection", "author": ["W. Nam", "P. Doll\u00e1r", "J.H. Han"], "venue": "In NIPS,", "citeRegEx": "Nam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2014}, {"title": "Online bagging and boosting", "author": ["N.C. Oza", "S. Russell"], "venue": "In AISTATS, pages 105\u2013112,", "citeRegEx": "Oza and Russell.,? \\Q2001\\E", "shortCiteRegEx": "Oza and Russell.", "year": 2001}, {"title": "Boosting: Foundations and algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": "MIT press,", "citeRegEx": "Schapire and Freund.,? \\Q2012\\E", "shortCiteRegEx": "Schapire and Freund.", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "In CVPR,", "citeRegEx": "Viola and Jones.,? \\Q2001\\E", "shortCiteRegEx": "Viola and Jones.", "year": 2001}, {"title": "Convolutional channel features", "author": ["B. Yang", "J. Yan", "Z. Lei", "S.Z. Li"], "venue": "In ICCV,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Boosting with early stopping", "author": ["T. Zhang", "B. Yu"], "venue": "Convergence and consistency", "citeRegEx": "Zhang and Yu.,? \\Q2005\\E", "shortCiteRegEx": "Zhang and Yu.", "year": 2005}, {"title": "A gradient boosting method to improve travel time prediction", "author": ["Y. Zhang", "A. Haghani"], "venue": "Transportation Research Part C: Emerging Technologies,", "citeRegEx": "Zhang and Haghani.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Haghani.", "year": 2015}, {"title": "Group cost-sensitive boosting for multi-resolution pedestrian detection", "author": ["C. Zhu", "Y. Peng"], "venue": "In AAAI,", "citeRegEx": "Zhu and Peng.,? \\Q2016\\E", "shortCiteRegEx": "Zhu and Peng.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Boosting (Freund and Schapire, 1995) is a popular method that leverages simple learning models (e.", "startOffset": 9, "endOffset": 36}, {"referenceID": 26, "context": "In computer vision, boosting was made popular by the seminal ViolaJones Cascade (Viola and Jones, 2001) and is still used Proceedings of the 20 International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA.", "startOffset": 80, "endOffset": 103}, {"referenceID": 22, "context": "to generate state-of-the-art results in pedestrian detection (Nam et al., 2014; Yang et al., 2015; Zhu and Peng, 2016).", "startOffset": 61, "endOffset": 118}, {"referenceID": 27, "context": "to generate state-of-the-art results in pedestrian detection (Nam et al., 2014; Yang et al., 2015; Zhu and Peng, 2016).", "startOffset": 61, "endOffset": 118}, {"referenceID": 30, "context": "to generate state-of-the-art results in pedestrian detection (Nam et al., 2014; Yang et al., 2015; Zhu and Peng, 2016).", "startOffset": 61, "endOffset": 118}, {"referenceID": 4, "context": "Boosting has also found success in domains ranging from document relevance ranking (Chapelle et al., 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 29, "context": ", 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 0, "context": ", 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al., 2012).", "startOffset": 74, "endOffset": 97}, {"referenceID": 14, "context": "Finally, boosting yields an anytime property at test time, which allows it to work with varying computation budgets (Grubb and Bagnell, 2012) for use in real-time applications such as controls and robotics.", "startOffset": 116, "endOffset": 141}, {"referenceID": 5, "context": "For classification, (Chen et al., 2012; Oza and Russell, 2001; Beygelzimer et al., 2015b) proposed online boosting algorithms along with theoretical justifications.", "startOffset": 20, "endOffset": 89}, {"referenceID": 23, "context": "For classification, (Chen et al., 2012; Oza and Russell, 2001; Beygelzimer et al., 2015b) proposed online boosting algorithms along with theoretical justifications.", "startOffset": 20, "endOffset": 89}, {"referenceID": 9, "context": "In particular, our algorithms are streaming extension to the classic gradient boosting (Friedman, 2001), where weak predictors are trained in a stage-wise fashion to approximate the functional gradient of the loss with respect to the previous ensemble prediction, a procedure that is shown by Mason et al.", "startOffset": 87, "endOffset": 103}, {"referenceID": 0, "context": ", 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al., 2012). Finally, boosting yields an anytime property at test time, which allows it to work with varying computation budgets (Grubb and Bagnell, 2012) for use in real-time applications such as controls and robotics. The advent of large-scale data-sets has driven the need for adapting boosting from the traditional batch setting, where the optimization is done over the whole dataset, to the online setting where the weak learners (models) can be updated with streaming data. In fact, online boosting has received tremendous attention so far. For classification, (Chen et al., 2012; Oza and Russell, 2001; Beygelzimer et al., 2015b) proposed online boosting algorithms along with theoretical justifications. Recent work by Beygelzimer et al. (2015a), addressed the regression task through the introduction of Online Gradient Boosting (OGB).", "startOffset": 75, "endOffset": 840}, {"referenceID": 0, "context": ", 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al., 2012). Finally, boosting yields an anytime property at test time, which allows it to work with varying computation budgets (Grubb and Bagnell, 2012) for use in real-time applications such as controls and robotics. The advent of large-scale data-sets has driven the need for adapting boosting from the traditional batch setting, where the optimization is done over the whole dataset, to the online setting where the weak learners (models) can be updated with streaming data. In fact, online boosting has received tremendous attention so far. For classification, (Chen et al., 2012; Oza and Russell, 2001; Beygelzimer et al., 2015b) proposed online boosting algorithms along with theoretical justifications. Recent work by Beygelzimer et al. (2015a), addressed the regression task through the introduction of Online Gradient Boosting (OGB). We build upon on the developments in (Beygelzimer et al., 2015a) to devise a new set of algorithms presented below. In this work, we develop streaming boosting algorithms for regression with strong theoretical guarantees under stochastic setting, where at each round the data are i.i.d sampled from some unknown fixed distribution. In particular, our algorithms are streaming extension to the classic gradient boosting (Friedman, 2001), where weak predictors are trained in a stage-wise fashion to approximate the functional gradient of the loss with respect to the previous ensemble prediction, a procedure that is shown by Mason et al. (2000) to be functional gradient descent of the loss in the space of predictors.", "startOffset": 75, "endOffset": 1576}, {"referenceID": 13, "context": "The second algorithm, designed for strongly convex but non-smooth loss functions, extends from the batch residual gradient boosting algorithm from (Grubb and Bagnell, 2011).", "startOffset": 147, "endOffset": 172}, {"referenceID": 16, "context": "We show that the algorithm achieves O(lnN/N) convergence rate with respect to the number of weak learners N , which matches the online gradient descent (OGD)\u2019s no-regret rate for strongly convex loss (Hazan et al., 2007).", "startOffset": 200, "endOffset": 220}, {"referenceID": 3, "context": "Our analysis leverages Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014), hence our results naturally extends to adversarial online learning setting as long as the weak online learning edge holds in adversarial setting, a harsher setting than stochastic setting.", "startOffset": 49, "endOffset": 98}, {"referenceID": 15, "context": "Our analysis leverages Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014), hence our results naturally extends to adversarial online learning setting as long as the weak online learning edge holds in adversarial setting, a harsher setting than stochastic setting.", "startOffset": 49, "endOffset": 98}, {"referenceID": 11, "context": "Oza and Russell (2001) developed some of the first online boosting algorithm, and their work are applied to online feature selection (Grabner and Bischof, 2006) and online semisupervised learning (Grabner et al.", "startOffset": 133, "endOffset": 160}, {"referenceID": 12, "context": "Oza and Russell (2001) developed some of the first online boosting algorithm, and their work are applied to online feature selection (Grabner and Bischof, 2006) and online semisupervised learning (Grabner et al., 2008).", "startOffset": 196, "endOffset": 218}, {"referenceID": 6, "context": ", 2015a), which extends gradient boosting for regression to the online setting under a smooth loss: each weak online learner is trained by minimizing a linear loss, and weak learners are combined using Frank-Wolfe (Frank and Wolfe, 1956) fashioned updates.", "startOffset": 214, "endOffset": 237}, {"referenceID": 28, "context": "Their analysis generalizes those of batch boosting for regression (Zhang and Yu, 2005).", "startOffset": 66, "endOffset": 86}, {"referenceID": 5, "context": ", 2015a) in that we assume an online weak learner edge exists, a common assumption in the classic boosting literature (Freund and Schapire, 1995, 1999) that is extended to the online boosting for classification by (Chen et al., 2012; Beygelzimer et al., 2015b).", "startOffset": 214, "endOffset": 260}, {"referenceID": 16, "context": "With this assumption, we analyze online gradient boosting using techniques from gradient descent for convex losses (Hazan et al., 2007).", "startOffset": 115, "endOffset": 135}, {"referenceID": 13, "context": "Oza and Russell (2001) developed some of the first online boosting algorithm, and their work are applied to online feature selection (Grabner and Bischof, 2006) and online semisupervised learning (Grabner et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Oza and Russell (2001) developed some of the first online boosting algorithm, and their work are applied to online feature selection (Grabner and Bischof, 2006) and online semisupervised learning (Grabner et al., 2008). Leistner et al. (2009) introduced online gradient boosting for the classification setting albeit without a theoretical analysis.", "startOffset": 134, "endOffset": 243}, {"referenceID": 3, "context": "Chen et al. (2012) developed the first convergence guarantees of online boosting for classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Then Beygelzimer et al. (2015b) presented two online classification boosting algorithms that are proved to be respectively optimal and adaptive.", "startOffset": 5, "endOffset": 32}, {"referenceID": 3, "context": "Our analysis of the risk leverages the classic Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014).", "startOffset": 73, "endOffset": 122}, {"referenceID": 15, "context": "Our analysis of the risk leverages the classic Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014).", "startOffset": 73, "endOffset": 122}, {"referenceID": 13, "context": "Additionally, we ideally want the prediction error to decrease exponentially fast in the number N of weak learners, as is the result from classic batch gradient boosting (Grubb and Bagnell, 2011).", "startOffset": 170, "endOffset": 195}, {"referenceID": 13, "context": "(5) Our definition of online weak learning directly generalizes the batch weak learning definition in (Grubb and Bagnell, 2011) to the online setting by the additional agnostic learnability assumption as shown in Eqn.", "startOffset": 102, "endOffset": 127}, {"referenceID": 9, "context": "As we will show later, the goal is to enforce that the weak learners to accurately predict gradients, as was also originally used in the batch gradient boosting algorithm (Friedman, 2001).", "startOffset": 171, "endOffset": 187}, {"referenceID": 10, "context": "Least-squares losses are also shown to be important in streaming tasks by (Gao et al., 2016) for their superior computational and theoretical properties.", "startOffset": 74, "endOffset": 92}, {"referenceID": 9, "context": "1 is the online version of the classic batch gradient boosting algorithms (Friedman, 2001; Grubb and Bagnell, 2011).", "startOffset": 74, "endOffset": 115}, {"referenceID": 13, "context": "1 is the online version of the classic batch gradient boosting algorithms (Friedman, 2001; Grubb and Bagnell, 2011).", "startOffset": 74, "endOffset": 115}, {"referenceID": 24, "context": "This performance guarantee is very similar to classic batch boosting algorithms (Schapire and Freund, 2012; Grubb and Bagnell, 2011), where the empirical risk decreases exponentially with the number of algorithm iterations, i.", "startOffset": 80, "endOffset": 132}, {"referenceID": 13, "context": "This performance guarantee is very similar to classic batch boosting algorithms (Schapire and Freund, 2012; Grubb and Bagnell, 2011), where the empirical risk decreases exponentially with the number of algorithm iterations, i.", "startOffset": 80, "endOffset": 132}, {"referenceID": 7, "context": "Note that this is analogous to the results of classification based batch boosting (Freund and Schapire, 1995; Grubb and Bagnell, 2011) and online boosting (Beygelzimer et al.", "startOffset": 82, "endOffset": 134}, {"referenceID": 13, "context": "Note that this is analogous to the results of classification based batch boosting (Freund and Schapire, 1995; Grubb and Bagnell, 2011) and online boosting (Beygelzimer et al.", "startOffset": 82, "endOffset": 134}, {"referenceID": 17, "context": ", Johnson and Zhang (2013)) in the experiment section (i.", "startOffset": 2, "endOffset": 27}, {"referenceID": 16, "context": "2 is to combine our online weak learning edge definition with the proof framework of Online Gradient Descent for strongly convex loss functions from (Hazan et al., 2007).", "startOffset": 149, "endOffset": 169}, {"referenceID": 19, "context": "We demonstrate the performance of our Streaming Gradient Boosting using the following UCI datasets (Lichman, 2013): YEAR, ABALONE, SLICE, and A9A (Kohavi and Becker) as well as the MNIST (LeCun et al., 1998) dataset.", "startOffset": 187, "endOffset": 207}, {"referenceID": 25, "context": "For each regression tree weak learner, Follow The Regularized Leader (FTRL) (Shalev-Shwartz, 2011) was used as the no-regret online update algorithm with regularization posed as the depth of the tree.", "startOffset": 76, "endOffset": 98}, {"referenceID": 9, "context": "We implement our baseline, the classic batch gradient boosting (GB) (Friedman, 2001), by optimizing each weak learner until convergence in order.", "startOffset": 68, "endOffset": 84}, {"referenceID": 18, "context": "In both GB and SGB, we train weak learners using ADAM (Kingma and Ba, 2015) optimization and use the default random parameter initialization for NN.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": "1 Binary Classification For binary classification, following (Friedman, 2001), let us define feature x \u2208 R, label u \u2208 {\u22121, 1}.", "startOffset": 61, "endOffset": 77}, {"referenceID": 9, "context": "2 Multi-class Classification Follow the settings in (Friedman, 2001), for multi-class classification problem, let us define feature x \u2208 R, and label information u \u2208 R, as a one-hot representation, where u[i] = 1 (u[i] is the i-th element of u), if the example is labelled by i, and u[i] = 0 otherwise.", "startOffset": 52, "endOffset": 68}], "year": 2017, "abstractText": "Boosting is a popular ensemble algorithm that generates more powerful learners by linearly combining base models from a simpler hypothesis class. In this work, we investigate the problem of adapting batch gradient boosting for minimizing convex loss functions to online setting where the loss at each iteration is i.i.d sampled from an unknown distribution. To generalize from batch to online, we first introduce the definition of online weak learning edge with which for strongly convex and smooth loss functions, we present an algorithm, Streaming Gradient Boosting (SGB) with exponential shrinkage guarantees in the number of weak learners. We further present an adaptation of SGB to optimize nonsmooth loss functions, for which we derive a O(lnN/N) convergence rate. We also show that our analysis can extend to adversarial online learning setting under a stronger assumption that the online weak learning edge will hold in adversarial setting. We finally demonstrate experimental results showing that in practice our algorithms can achieve competitive results as classic gradient boosting while using less computation.", "creator": "LaTeX with hyperref package"}}}