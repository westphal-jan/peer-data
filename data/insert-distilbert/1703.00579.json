{"id": "1703.00579", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Active Learning for Accurate Estimation of Linear Models", "abstract": "we explore the sequential decision making problem where the goal is to estimate uniformly well utilizing a number of linear models, unless given a shared budget of random contexts independently sampled from a known distribution. the decision maker must query one of the stationary linear models for each incoming context, and receives an observation corrupted partially by noise levels that are unknown, and depend on the model instance. we carefully present trace - ucb, an oriented adaptive allocation algorithm that learns the noise levels while repeatedly balancing contexts accordingly across the different linear functions, and and derive guarantees for simple regret mechanisms in both equal expectation and high - probability. finally, we extend the algorithm and its guarantees to compare high dimensional sampled settings, where the number of linear models times the dimension of the contextual space is higher than the total budget of samples. relational simulations with real data suggest realizing that trace - ucb is remarkably robust, outperforming a number enough of arbitrary baselines even optimal when its assumptions are violated.", "histories": [["v1", "Thu, 2 Mar 2017 01:29:57 GMT  (111kb,D)", "https://arxiv.org/abs/1703.00579v1", "37 pages, 8 figures"], ["v2", "Sat, 29 Jul 2017 19:41:29 GMT  (3585kb,D)", "http://arxiv.org/abs/1703.00579v2", "37 pages, 8 figures, International Conference on Machine Learning, ICML 2017"]], "COMMENTS": "37 pages, 8 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["carlos riquelme", "mohammad ghavamzadeh", "alessandro lazaric"], "accepted": true, "id": "1703.00579"}, "pdf": {"name": "1703.00579.pdf", "metadata": {"source": "META", "title": "Active Learning for Accurate Estimation of Linear Models", "authors": ["Carlos Riquelme", "Mohammad Ghavamzadeh", "Alessandro Lazaric"], "emails": ["<rikel@stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "We study the problem faced by a decision-maker whose goal is to estimate a number of regression problems equally well (i.e., with a small prediction error for each of them), and has to adaptively allocate a limited budget of samples to the problems in order to gather information and improve its estimates. Two aspects of the problem formulation are key and drive the algorithm design: 1) The observations Y collected from each regression problem depend on side information (i.e., contexts X \u2208 Rd) and we model the relationship between X and Y in each problem i as a linear function with unknown parameters \u03b2i \u2208 Rd, and 2) The \u201chardness\u201d of learning each parameter \u03b2i is unknown in advance and may vary across the problems. In particular, we\n1Stanford University, Stanford, CA, USA. 2DeepMind, Mountain View, CA, USA (The work was done when the author was with Adobe Research). 3Inria Lille, France. Correspondence to: Carlos Riquelme <rikel@stanford.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nassume that the observations are corrupted by noise levels that are problem-dependent and must be learned as well.\nThis scenario may arise in a number of different domains where a fixed experimentation budget (number of samples) should be allocated to different problems. Imagine a drug company that has developed several treatments for a particular form of disease. Now it is interested in having an accurate estimate of the performance of each of these treatments for a specific population of patients (e.g., at a particular geographical location). Given the budget allocated to this experiment, a number of patients n can participate in the clinical trial. Volunteered patients arrive sequentially over time and they are represented by a contextX \u2208 Rd summarizing their profile. We model the health status of patientX after being assigned to treatment i by scalar Yi \u2208 R, which depends on the specific drug through a linear function with parameter \u03b2i (i.e., Yi \u2248 XT\u03b2i). The goal is to assign each incoming patient to a treatment in such a way that at the end of the trial, we have an accurate estimate for all \u03b2i\u2019s. This will allow us to reliably predict the expected health status of each new patientX for any treatment i. Since the parameters \u03b2i and the noise levels are initially unknown, achieving this goal requires an adaptive allocation strategy for the n patients. Note that while n may be relatively small, as the ethical and financial costs of treating a patient are high, the distribution of the contexts X (e.g., the biomarkers of cancer patients) can be precisely estimated in advance.\nThis setting is clearly related to the problem of pure exploration and active learning in multi-armed bandits (Antos et al., 2008), where the learner wants to estimate the mean of a finite set of arms by allocating a finite budget of n pulls. Antos et al. (2008) first introduced this setting where the objective is to minimize the largest mean square error (MSE) in estimating the value of each arm. While the optimal solution is trivially to allocate the pulls proportionally to the variance of the arms, when the variances are unknown an exploration-exploitation dilemma arises, where variance and value of the arms must be estimated at the same time in order to allocate pulls where they are more needed (i.e., arms with high variance). Antos et al. (2008) proposed a forcing algorithm where all arms are pulled at least \u221a n times before allocating pulls proportionally to the estimated variances. They derived bounds on the regret, measuring the difference between the MSEs of the learnar X iv :1\n70 3.\n00 57\n9v 2\n[ st\nat .M\nL ]\n2 9\nJu l 2\n01 7\ning algorithm and an optimal allocation showing that the regret decreases as O(n\u22123/2). A similar result is obtained by Carpentier et al. (2011) that proposed two algorithms that use upper confidence bounds on the variance to estimate the MSE of each arm and select the arm with the larger MSE at each step. When the arms are embedded in Rd and their mean is a linear combination with an unknown parameter, then the problem becomes an optimal experimental design problem (Pukelsheim, 2006), where the objective is to estimate the linear parameter and minimize the prediction error over all arms (see e.g., Wiens & Li 2014; Sabato & Munos 2014). In this paper, we consider an orthogonal extension to the original problem where a finite number of linear regression problems is available (i.e., the arms) and random contexts are observed at each time step. Similarly to the setting of Antos et al. (2008), we assume each problem is characterized by a noise with different variance and the objective is to return regularized least-squares (RLS) estimates with small prediction error (i.e., MSE). While we leverage on the solution proposed by Carpentier et al. (2011) to deal with the unknown variances, in our setting the presence of random contexts make the estimation problem considerably more difficult. In fact, the MSE in one specific regression problem is not only determined by the variance of the noise and the number of samples used to compute the RLS estimate, but also by the contexts observed over time.\nContributions. We propose TRACE-UCB, an algorithm that simultaneously learns the \u201chardness\u201d of each problem, allocates observations proportionally to these estimates, and balances contexts across problems. We derive performance bounds for TRACE-UCB in expectation and highprobability, and compare the algorithm with several baselines. TRACE-UCB performs remarkably well in scenarios where the dimension of the contexts or the number of instances is large compared to the total budget, motivating the study of the high-dimensional setting, whose analysis and performance bounds are reported in App. F of Riquelme et al. (2017a). Finally, we provide simulations with synthetic data that support our theoretical results, and with real data that demonstrate the robustness of our approach even when some of the assumptions do not hold."}, {"heading": "2. Preliminaries", "text": "The problem. We consider m linear regression problems, where each instance i \u2208 [m] = {1, . . . ,m} is characterized by a parameter \u03b2i \u2208 Rd such that for any context X \u2208 Rd, a random observation Y \u2208 R is obtained as\nY = XT\u03b2i + i, (1)\nwhere the noise i is an i.i.d. realization of a Gaussian distribution N (0, \u03c32i ). We denote by \u03c32max = maxi \u03c32i and\nby \u03c32 = 1/m \u2211 i \u03c3 2 i , the largest and the average variance, respectively. We define a sequential decision-making problem over n rounds, where at each round t \u2208 [n], the learning algorithm A receives a context Xt drawn i.i.d. from N (0,\u03a3), selects an instance It, and observes a random sample YIt,t according to (1). By the end of the experiment, a training set Dn = {Xt, It, YIt,t}t\u2208[n] has been collected and all the m linear regression problems are solved, each problem i \u2208 [m] with its own training set Di,n (i.e., a subset of Dn containing samples with It = i), and estimates of the parameters {\u03b2\u0302i,n}i\u2208[m] are returned. For each \u03b2\u0302i,n, we measure its accuracy by the mean-squared error (MSE)\nLi,n(\u03b2\u0302i,n)=EX [ (XT\u03b2i\u2212XT\u03b2\u0302i,n)2 ] =\u2016\u03b2i\u2212\u03b2\u0302i,n\u20162\u03a3. (2)\nWe evaluate the overall accuracy of the estimates returned by the algorithm A as\nLn(A) = max i\u2208[m]\nEDn [ Li,n(\u03b2\u0302i,n) ] , (3)\nwhere the expectation is w.r.t. the randomness of the contexts Xt and observations Yi,t used to compute \u03b2\u0302i,n. The objective is to design an algorithm A that minimizes the loss (3). This requires defining an allocation rule to select the instance It at each step t and the algorithm to compute the estimates \u03b2\u0302i,n, e.g., ordinary least-squares (OLS), regularized least-squares (RLS), or Lasso. In designing a learning algorithm, we rely on the following assumption.\nAssumption 1. The covariance matrix \u03a3 of the Gaussian distribution generating the contexts {Xt}nt=1 is known.\nThis is a standard assumption in active learning, since in this setting the learner has access to the input distribution and the main question is for which context she should ask for a label (Sabato & Munos, 2014; Riquelme et al., 2017b). Often times, companies, like the drug company considered in the introduction, own enough data to have an accurate estimate of the distribution of their customers (patients).\nWhile in the rest of the paper we focus on Ln(A), our algorithm and analysis can be easily extended to similar objectives such as replacing the maximum in (3) with average across all instances, i.e., 1/m \u2211m i=1 EDn [ Li,n(\u03b2\u0302i,n) ] , and\nusing weighted errors, i.e., maxi wi EDn [ Li,n(\u03b2\u0302i,n) ] , by updating the score to focus on the estimated standard deviation and by including the weights in the score, respectively. Later in the paper, we also consider the case where the expectation in (3) is replaced by the high-probability error (see Eq. 17).\nOptimal static allocation with OLS estimates. While the distribution of the contexts is fixed and does not depend on the instance i, the errors Li,n(\u03b2\u0302i,n) directly depend on the variances \u03c32i of the noise i. We define an optimal baseline\nobtained when the noise variances {\u03c32i }mi=1 are known. In particular, we focus on a static allocation algorithm Astat that selects each instance i exactly ki,n times, independently of the context,1 and returns an estimate \u03b2\u0302i,n computed by OLS as\n\u03b2\u0302i,n = ( XTi,nXi,n )\u22121 XTi,nYi,n, (4)\nwhere Xi,n \u2208 Rki,n\u00d7d is the matrix of (random) samples obtained at the end of the experiment, and Yi,n \u2208 Rki,n is its corresponding vector of observations. It is simple to show that the global error corresponding to Astat is\nLn(Astat) = max i\u2208[m] \u03c32i ki,n\nTr ( \u03a3EDn [ \u03a3\u0302\u22121i,n ]) , (5)\nwhere \u03a3\u0302i,n = XTi,nXi,n/ki,n \u2208 Rd\u00d7d is the empirical covariance matrix of the contexts assigned to instance i. Since the algorithm does not change the allocation depending on the contexts and Xt \u223c N (0,\u03a3), \u03a3\u0302\u22121i,n is distributed as an inverse-Wishart and we may write (5) as\nLn(Astat) = max i\u2208[m] d\u03c32i ki,n \u2212 d\u2212 1 . (6)\nThus, we derive the following proposition for the optimal static allocation algorithm A\u2217stat. Proposition 1. Given m linear regression problems, each characterized by a parameter \u03b2i, Gaussian noise with variance \u03c32i , and Gaussian contexts with covariance \u03a3, let n > m(d + 1), then the optimal OLS static allocation algorithm A\u2217stat selects each instance\nk\u2217i,n = \u03c32i\u2211 j \u03c3 2 j n+ (d+ 1)\n( 1\u2212 \u03c3 2 i\n\u03c32\n) , (7)\ntimes (up to rounding effects), and incurs the global error\nL\u2217n = Ln(A\u2217stat) = \u03c32 md\nn +O\n( \u03c32 ( md\nn\n)2) . (8)\nProof. See Appendix A.1.2\nProposition 1 divides the problems into two types: those for which \u03c32i \u2265 \u03c3\u03042 (wild instances) and those for which \u03c32i < \u03c3\u0304\n2 (mild instances). We see that for the first type, the second term in (7) is negative and the instance should be selected less frequently than in the context-free case (where the optimal allocation is given just by the first term). On the other hand, instances whose variance is below the\n1This strategy can be obtained by simply selecting the first instance k1,n times, the second one k2,n times, and so on.\n2All the proofs can be found in the appendices of the extended version of the paper (Riquelme et al., 2017a).\nmean variance should be pulled more often. In any case, we see that the correction to the context-free allocation (i.e., the second term) is constant, as it does not depend on n. Nonetheless, it does depend on d and this suggests that in high-dimensional problems, it may significantly skew the optimal allocation.\nWhile A\u2217stat effectively minimizes the prediction loss Ln, it cannot be implemented in practice since the optimal allocation k\u2217i requires the variances \u03c3 2 i to be known at the beginning of the experiment. As a result, we need to devise a learning algorithm A whose performance approaches L\u2217n as n increases. More formally, we define the regret of A as\nRn(A) = Ln(A)\u2212 Ln(A\u2217stat) = Ln(A)\u2212 L\u2217n, (9)\nand we expect Rn(A) = o(1/n). In fact, any allocation strategy that selects each instance a linear number of times (e.g., uniform sampling) achieves a lossLn = O(1/n), and thus, a regret of orderO(1/n). However, we expect that the loss of an effective learning algorithm decreases not just at the same rate as L\u2217n but also with the very same constant, thus implying a regret that decreases faster than O(1/n)."}, {"heading": "3. The TRACE-UCB Algorithm", "text": "In this section, we present and analyze an algorithm of the form discussed at the end of Section 2, which we call TRACE-UCB, whose pseudocode is in Algorithm 1.\nAlgorithm 1 TRACE-UCB Algorithm 1: for i = 1, . . . ,m do 2: Select problem instance i exactly d+ 1 times 3: Compute its OLS estimates \u03b2\u0302i,m(d+1) and \u03c3\u03022i,m(d+1) 4: end for 5: for steps t = m(d+ 1) + 1, . . . , n do 6: for problem instance 1 \u2264 i \u2264 m do 7: Compute score (\u2206i,t\u22121 is defined in (11))\nsi,t\u22121 = \u03c3\u03022i,t\u22121 + \u2206i,t\u22121 ki,t\u22121 Tr ( \u03a3\u03a3\u0302\u22121i,t\u22121 ) 8: end for 9: Select problem instance It = arg maxi\u2208[m] si,t\u22121\n10: Observe Xt and YIt,t 11: Update its OLS estimators \u03b2\u0302It,t and \u03c3\u0302 2 It,t 12: end for 13: Return RLS estimates {\u03b2\u0302\u03bbi,n}mi=1 with regularization \u03bb\nThe regularization parameter \u03bb = O(1/n) is provided to the algorithm as input, while in practice one could set \u03bb independently for each arm using cross-validation.\nIntuition. Equation (6) suggests that while the parameters of the context distribution, particularly its covariance \u03a3, do\nnot impact the prediction error, the noise variances play the most important role in the loss of each problem instance. This is in fact confirmed by the optimal allocation k\u2217i,n in (7), where only the variances \u03c32i appear. This evidence suggests that an algorithm similar to GAFS-MAX (Antos et al., 2008) or CH-AS (Carpentier et al., 2011), which were designed for the context-free case (i.e., each instance i is associated to an expected value and not a linear function) would be effective in this setting as well. Nonetheless, (6) holds only for static allocation algorithms that completely ignore the context and the history to decide which instance It to choose at time t. On the other hand, adaptive learning algorithms create a strong correlation between the dataset Dt\u22121 collected so far, the current context Xt, and the decision It. As a result, the sample matrix Xi,t is no longer a random variable independent of A, and using (6) to design a learning algorithm is not convenient, since the impact of the contexts on the error is completely overlooked. Unfortunately, in general, it is very difficult to study the potential correlation between the contexts Xi,t, the intermediate estimates \u03b2\u0302i,t, and the most suitable choice It. However, in the next lemma, we show that if at each step t, we select It as a function of Dt\u22121, and not Xt, we may still recover an expression for the final loss that we can use as a basis for the construction of an effective learning algorithm.\nLemma 2. Let A be a learning algorithm that selects the instances It as a function of the previous history, i.e., Dt\u22121 = {X1, I1, YI1,1, . . . , Xt\u22121, It\u22121, YIt\u22121,t\u22121} and computes estimates \u03b2\u0302i,n using OLS. Then, its loss after n steps can be expressed as\nLn(A) = max i\u2208[m] EDn [ \u03c32i ki,n Tr ( \u03a3\u03a3\u0302\u22121i,n )] , (10)\nwhere ki,n = \u2211n t=1 I{It = i} and \u03a3\u0302i,n = XTi,nXi,n/ki,n.\nProof. See Appendix B.\nRemark 1 (assumptions). We assume noise and contexts are Gaussian. The noise Gaussianity is crucial for the estimates of the parameter \u03b2\u0302i,t and variance \u03c3\u03022i,t to be independent of each other, for each instance i and time t (we actually need and derive a stronger result in Lemma 9, see Appendix B). This is key in proving Lemma 2, as it allows us to derive a closed form expression for the loss function which holds under our algorithm, and is written in terms of the number of pulls and the trace of the inverse empirical covariance matrix. Note that \u03b2\u0302i,t drives our loss, while \u03c3\u03022i,t drives our decisions. One way to remove this assumption is by defining and directly optimizing a surrogate loss equal to (10) instead of (3). On the other hand, the Gaussianity of contexts leads to the whitened inverse covariance estimate \u03a3\u03a3\u0302\u22121i,n being distributed as an inverse Wishart. As there\nis a convenient closed formula for its mean, we can find the exact optimal static allocation k\u2217i,n in Proposition 1, see (7). In general, for sub-Gaussian contexts, no such closed formula for the trace is available. However, as long as the optimal allocation k\u2217i,n has no second order n\n\u03b1 terms for 1/2 \u2264 \u03b1 < 1, it is possible to derive the same regret rate results that we prove later on for TRACE-UCB.\nEquation (10) makes it explicit that the prediction error comes from two different sources. The first one is the noise in the measurements Y, whose impact is controlled by the unknown variances \u03c32i \u2019s. Clearly, the larger the \u03c3 2 i is, the more observations are required to achieve the desired accuracy. At the same time, the diversity of contexts across instances also impacts the overall prediction error. This is very intuitive, since it would be a terrible idea for the research center discussed in the introduction to estimate the parameters of a drug by providing the treatment only to a hundred almost identical patients. We say contexts are balanced when \u03a3\u0302i,n is well conditioned. Therefore, a good algorithm should take care of both aspects.\nThere are two extreme scenarios regarding the contributions of the two sources of error. 1) If the number of contexts n is relatively large, since the context distribution is fixed, one can expect that contexts allocated to each instance eventually become balanced (i.e., TRACE-UCB does not bias the distribution of the contexts). In this case, it is the difference in \u03c32i \u2019s that drives the number of times each instance is selected. 2) When the dimension d or the number of arms m is large w.r.t. n, balancing contexts becomes critical, and can play an important role in the final prediction error, whereas the \u03c32i \u2019s are less relevant in this scenario. While a learning algorithm cannot deliberately choose a specific context (i.e., Xt is a random variable), we may need to favor instances in which the contexts are poorly balanced and their prediction error is large, despite the fact that they might have small noise variances.\nAlgorithm. TRACE-UCB is designed as a combination of the upper-confidence-bound strategy used in CH-AS (Carpentier et al., 2011) and the loss in (10), so as to obtain a learning algorithm capable of allocating according to the estimated variances and at the same time balancing the error generated by context mismatch. We recall that all the quantities that are computed at every step of the algorithm are indexed at the beginning and end of a step t by i, t\u2212 1 (e.g., \u03c3\u03022i,t\u22121) and i, t (e.g., \u03b2\u0302i,t), respectively. At the end of each step t, TRACE-UCB first computes an OLS estimate \u03b2\u0302i,t, and then use it to estimate the variance \u03c3\u03022i,t as\n\u03c3\u03022i,t = 1 ki,t \u2212 d \u2225\u2225Yi,t \u2212XTi,t\u03b2\u0302i,t\u2225\u22252,\nwhich is the average squared deviation of the predictions based on \u03b2\u0302i,t. We rely on the following concentration in-\nequality for the variance estimate of linear regression with Gaussian noise, whose proof is reported in Appendix C.1.\nProposition 3. Let the number of pulls ki,t \u2265 d + 1 and R \u2265 maxi \u03c32i . If \u03b4 \u2208 (0, 3/4), then for any instance i and step t > m(d+ 1), with probability at least 1\u2212 \u03b42 , we have\n|\u03c3\u03022i,t \u2212 \u03c32i | \u2264 \u2206i,t \u2206 = R\n\u221a 64\nki,t \u2212 d\n( log 2mn\n\u03b4\n)2 . (11)\nGiven (11), we can construct an upper-bound on the prediction error of any instance i and time step t as\nsi,t\u22121 = \u03c3\u03022i,t\u22121 + \u2206i,t\u22121 ki,t\u22121 Tr ( \u03a3\u03a3\u0302\u22121i,t\u22121 ) , (12)\nand then simply select the instance which maximizes this score, i.e., It = arg maxi si,t\u22121. Intuitively, TRACE-UCB favors problems where the prediction error is potentially large, either because of a large noise variance or because of significant unbalance in the observed contexts w.r.t. the target distribution with covariance \u03a3. A subtle but critical aspect of TRACE-UCB is that by ignoring the current context Xt (but using all the past samples Xt\u22121) when choosing It, the distribution of the contexts allocated to each instance stays untouched and the second term in the score si,t\u22121, i.e., Tr(\u03a3\u03a3\u0302\u22121i,t\u22121), naturally tends to d as more and more (random) contexts are allocated to instance i. This is shown by Proposition 4 whose proof is in Appendix C.2.\nProposition 4. Force the number of samples ki,t \u2265 d+ 1. If \u03b4 \u2208 (0, 1), for any i \u2208 [m] and step t > m(d + 1) with probability at least 1\u2212 \u03b4/2, we have ( 1\u2212 CTr \u221a d\nki,t\n)2 \u2264\nTr (\n\u03a3\u03a3\u0302\u22121i,t ) d \u2264 ( 1 + 2CTr \u221a d\nki,t\n)2 ,\nwith CTr = 1 + \u221a 2 log(4nm/\u03b4)/d.\nWhile Proposition 4 shows that the error term due to context mismatch tends to the constant d for all instances i as the number of samples tends to infinity, when t is small w.r.t. d and m, correcting for the context mismatch may significantly improve the accuracy of the estimates \u03b2\u0302i,n returned by the algorithm. Finally, note that while TRACEUCB uses OLS to compute estimates \u03b2\u0302i,t, it computes its returned parameters \u03b2\u0302i,n by ridge regression (RLS) with regularization parameter \u03bb as\n\u03b2\u0302\u03bbi = (X T i,nXi,n + \u03bbI) \u22121XTi,nYi,n. (13)\nAs we will discuss later, using RLS makes the algorithm more robust and is crucial in obtaining regret bounds both in expectation and high probability.\nPerformance Analysis. Before proving a regret bound for TRACE-UCB, we report an intermediate result (proof in App. D.1) that shows that TRACE-UCB behaves similarly to the optimal static allocation.\nTheorem 5. Let \u03b4 > 0. With probability at least 1 \u2212 \u03b4, the total number of contexts that TRACE-UCB allocates to each problem instance i after n rounds satisfies\nki,n \u2265 k\u2217i,n \u2212 C\u2206 + 8CTr\n\u03c32min\n\u221a nd\n\u03bbmin \u2212 \u2126(n1/4) (14)\nwhereR \u2265 \u03c32max is known by the algorithm, and we defined C\u2206 = 16R log(2mn/\u03b4) and \u03bbmin = \u03c32min/ \u2211 j \u03c3 2 j .\nWe now report our regret bound for the TRACE-UCB algorithm. The proof of Theorem 6 is in Appendix D.2.\nTheorem 6. The regret of the Trace-UCB algorithm, i.e., the difference between its loss and the loss of optimal static allocation (see Eq. (8)), is upper-bounded by\nLn(A)\u2212 L\u2217n \u2264 O ( 1\n\u03c32min ( d \u03bbminn )3/2) . (15)\nEq. (15) shows that the regret decreases asO(n\u22123/2) as expected. This is consistent with the context-free results (Antos et al., 2008; Carpentier et al., 2011), where the regret decreases as n\u22123/2, which is conjectured to be optimal. However, it is important to note that in the contextual case, the numerator also includes the dimensionality d. Thus, when n d, the regret will be small, and it will be larger when n \u2248 d. This motivates studying the high-dimensional setting (App. F). Eq. (15) also indicates that the regret depends on a problem-dependent constant 1/\u03bbmin, which measures the complexity of the problem. Note that when \u03c32max \u2248 \u03c32min, we have 1/\u03bbmin \u2248 m, but 1/\u03bbmin could be much larger when \u03c32max \u03c32min.\nRemark 2. We introduce a baseline motivated by the context-free problem. At round t, let VAR-UCB selects the instance that maximizes the score3\ns\u2032i,t\u22121 = \u03c3\u03022i,t\u22121 + \u2206i,t\u22121\nki,t\u22121 . (16)\nThe only difference with the score used by TRACE-UCB is the lack of the trace term in (12). Moreover, the regret of this algorithm has similar rate in terms of n and d as that of TRACE-UCB reported in Theorem 6. However, the simulations of Sect. 4 show that the regret of VAR-UCB is actually much higher than that of TRACE-UCB, specially when dm is close to n. Intuitively, when n is close to dm, balancing contexts becomes critical, and VAR-UCB suffers because its score does not explicitly take them into account.\n3Note that VAR-UCB is similar to both the CH-AS and B-AS algorithms in Carpentier et al. (2011).\nSketch of the proof of Theorem 6. The proof is divided into three parts. 1) We show that the behavior of the ridge loss of TRACE-UCB is similar to that reported in Lemma 2 for algorithms that rely on OLS; see Lemma 19 in Appendix E. The independence of the \u03b2\u0302i,t and \u03c3\u03022i,t estimates is again essential (see Remark 1). Although the loss of TRACE-UCB depends on the ridge estimate of the parameters \u03b2\u0302\u03bbi,n, the decisions made by the algorithm at each round only depend on the variance estimates \u03c3\u03022i,t and observed contexts. 2) We follow the ideas in Carpentier et al. (2011) to lower-bound the total number of pulls ki,n for each i \u2208 [m] under a good event (see Theorem 5 and its proof in Appendix D.1). 3) We finally use the ridge regularization to bound the impact of those cases outside the good event, and combine everything in Appendix D.2.\nThe regret bound of Theorem 6 shows that the largest expected loss across the problem instances incurred by TRACE-UCB quickly approaches the loss of the optimal static allocation algorithm (which knows the true noise variances). WhileLn(A) measures the worst expected loss, at any specific realization of the algorithm, there may be one of the instances which is very poorly estimated. As a result, it would also be desirable to obtain guarantees for the (random) maximum loss\nL\u0303n(A) = max i\u2208[m] \u2016\u03b2i \u2212 \u03b2\u0302i,n\u20162\u03a3. (17)\nIn particular, we are able to prove the following highprobability bound on L\u0303n(A) for TRACE-UCB. Theorem 7. Let \u03b4 > 0, and assume \u2016\u03b2i\u20162 \u2264 Z for all i, for some Z > 0. With probability at least 1\u2212 \u03b4,\nL\u0303n \u2264\nm\u2211 j=1 \u03c32j\nn\n( d+ 2 log 3m\n\u03b4\n) +O ( 1\n\u03c32min ( d n\u03bbmin ) 3 2 ) . (18)\nNote that the first term in (18) corresponds to the first term of the loss for the optimal static allocation, and the second term is, again, a n\u22123/2 deviation. However, in this case, the guarantees hold simultaneously for all the instances.\nSketch of the proof of Theorem 7. In the proof we slightly modify the confidence ellipsoids for the \u03b2\u0302i,t\u2019s, based on self-normalized martingales, and derived in (AbbasiYadkori et al., 2011); see Thm. 13 in App. C. By means of the confidence ellipsoids we control the loss in (17). Their radiuses depend on the number of samples per instance, and we rely on a high-probability events to compute a lower bound on the number of samples. In addition, we need to make sure the mean norm of the contexts will not be too large (see Corollary 15 in App. C). Finally, we combine the lower bound on ki,n with the confidence ellipsoids to conclude the desired high-probability guarantees in Thm. 7.\nHigh-Dimensional Setting. High-dimensional linear models are quite common in practice, motivating the study of\nthe n < dm case, where the algorithms discussed so far break down. We propose SPARSE-TRACE-UCB in Appendix F, an extension of TRACE-UCB that assumes and takes advantage of joint sparsity across the linear functions. The algorithm has two-stages: first, an approximate support is recovered, and then, TRACE-UCB is applied to the induced lower dimensional space. We discuss and extend our high-probability guarantees to SPARSE-TRACE-UCB under suitable standard assumptions in Appendix F."}, {"heading": "4. Simulations", "text": "In this section, we provide empirical evidence to support our theoretical results. We consider both synthetic and realworld problems, and compare the performance (in terms of normalized MSE) of TRACE-UCB to uniform sampling, optimal static allocation (which requires the knowledge of noise variances), and the context-free algorithm VAR-UCB (see Remark 2). We do not compare to GFSP-MAX and GAFS-MAX (Antos et al., 2008) since they are outperformed by CH-AS Carpentier et al. (2011) and VAR-UCB is the same as CH-AS, except for the fact that we use the concentration inequality in Prop. 3, since we are estimating the variance from a regression problem using OLS.\nFirst, we use synthetic data to ensure that all the assumptions of our model are satisfied, namely we deal with linear regression models with Gaussian context and noise. We set the number of problem instances to m = 7 and consider two scenarios: one in which all the noise variances are equal to 1 and one where they are not equal, and \u03c32 = (0.01, 0.02, 0.75, 1, 2, 2, 3). In the latter case, \u03c32max/\u03c3 2 min = 300. We study the impact of (independently) increasing dimension d and horizon n on the performance, while keeping all other parameters fixed. Second, we consider real-world datasets in which the underlying model is non-linear and the contexts are not Gaussian, to observe how TRACE-UCB behaves (relative to the baselines) in settings where its main underlying assumptions are violated.\nSynthetic Data. In Figures 1(a,b), we display the results for fixed horizon n = 350 and increasing dimension d. For each value of d, we run 10, 000 simulations and report the median of the maximum error across the instances for each simulation. In Fig. 1(a), where \u03c32i \u2019s are equal, uniform sampling and optimal static allocation execute the same allocation since there is no difference in the expected losses of different instances. Nonetheless we notice that VARUCB suffers from poor estimation as soon as d increases, while TRACE-UCB is competitive with the optimal performance. This difference in performance can be explained by the fact that VAR-UCB does not control for contextual balance, which becomes a dominant factor in the loss of a learning strategy for problems of high dimensionality. In Fig. 1(b), in which \u03c32i \u2019s are different, uniform sampling is\nno longer optimal but even in this case VAR-UCB performs better than uniform sampling only for small d < 23, where it is more important to control for the \u03c32i \u2019s. For larger dimensions, balancing uniformly the contexts eventually becomes a better strategy, and uniform sampling outperforms VAR-UCB. In this case too, TRACE-UCB is competitive with the optimal static allocation even for large d, successfully balancing both noise variance and contextual error.\nNext, we study the performance of the algorithms w.r.t. n. We report two different losses, one in expectation (3) and one in high probability (17), corresponding to the results we proved in Theorems 6 and 7, respectively. In order to approximate the loss in (3) (Figures 1(c,d)) we run 30, 000 simulations, compute the average prediction error for each instance i \u2208 [m], and finally report the maximum mean error across the instances. On the other hand, we estimate the loss in (17) (Figures 1(e,f)) by running 30, 000 simulations, taking the maximum prediction error across the instances for each simulation, and finally reporting their median.\nIn Figures 1(c, d), we display the loss for fixed dimension d = 10 and horizon from n = 115 to 360. In Figure 1(c),\nTRACE-UCB performs similarly to the optimal static allocation, whereas VAR-UCB performs significantly worse, ranging from 25% to 50% higher errors than TRACE-UCB, due to some catastrophic errors arising from unlucky contextual realizations for an instance. In Fig. 1(d), as the number of contexts grows, uniform sampling\u2019s simple context balancing approach is enough to perform as well as VARUCB that again heavily suffers from large mistakes. In both figures, TRACE-UCB smoothly learns the \u03c32i \u2019s and outperforms uniform sampling and VAR-UCB. Its performance is comparable to that of the optimal static allocation, especially in the case of equal variances in Fig. 1(c).\nIn Figure 1(e), TRACE-UCB learns and properly balances observations extremely fast and obtains an almost optimal performance. Similarly to figures 1(a,c), VAR-UCB struggles when variances \u03c3\u03022i are almost equal, mainly because it gets confused by random deviations in variance estimates \u03c3\u03022i , while overlooking potential and harmful context imbalances. Note that even when n = 360 (rightmost point), its median error is still 25% higher than TRACE-UCB\u2019s. In Fig. 1(f), as expected, uniform sampling performs poorly,\ndue to mismatch in variances, and only outperforms VARUCB for small horizons in which uniform allocation pays off. On the other hand, TRACE-UCB is able to successfully handle the tradeoff between learning and allocating according to variance estimates \u03c3\u03022i , while accounting for the contextual trace \u03a3\u0302i, even for very low n. We observe that for large n, VAR-UCB eventually reaches the performance of the optimal static allocation and TRACE-UCB.\nIn practice the loss in (17) (figures 1(e,f)) is often more relevant than (3), since it is in high probability and not in expectation, and TRACE-UCB shows excellent performance and robustness, regardless of the underlying variances \u03c32i .\nReal Data. TRACE-UCB is based on assumptions such as linearity, and Gaussianity of noise and context that may not hold in practice, where data may show complex dependencies. Therefore, it is important to evaluate the algorithm with real-world data to see its robustness to the violation of its assumptions. We consider two collaborative filtering datasets in which users provide ratings for items. We choose a dense subset of k users and p items, where every user has rated every item. Thus, each user is represented by a p-dimensional vector of ratings. We define the user context by d out of her p ratings, and learn to predict her remaining m = p \u2212 d ratings (each one is a problem instance). All item ratings are first centered, so each item\u2019s mean is zero. In each simulation, n out of the k users are selected at random to be fed to the algorithm, also in random order. Algorithms can select any instance as the dataset contains the ratings of every instance for all the users. At the end of each simulation, we compute the prediction error for each instance by using the k \u2212 n users that did not participate in training for that simulation. Finally, we report the median error across all simulations.\nFig. 2(a) reports the results using the Jester Dataset by (Goldberg et al., 2001) that consists of joke ratings in a continuous scale from \u221210 to 10. We take d = 40 joke ratings as context and learn the ratings for another 9 jokes. In addition, we add another function that counts the total\nnumber of movies originally rated by the user. The latter is also centered, bounded to the same scale, and has higher variance (without conditioning on X). The number of total users is k = 3811, and m = 10. When the number of observations is limited, the advantage of TRACE-UCB is quite significant (the improvement w.r.t. uniform allocation goes from 45% to almost 20% for large n, while w.r.t. VAR-UCB it goes from almost 30% to roughly 5%), even though the model and context distribution are far from linear and Gaussian, respectively.\nFig. 2(b) shows the results for the MovieLens dataset (Maxwell Harper & Konstan, 2016) that consists of movie ratings between 0 and 5 with 0.5 increments. We select 30 popular movies rated by k = 1363 users, and randomly choose m = 5 of them to learn (so d = 25). In this case, all problems have similar variance (\u03c3\u03022max/\u03c3\u0302 2 min \u2248 1.3) so uniform allocation seems appropriate. Both TRACE-UCB and VAR-UCB modestly improve uniform allocation, while their performance is similar."}, {"heading": "5. Conclusions", "text": "We studied the problem of adaptive allocation of n contextual samples of dimension d to estimate m linear functions equally well, under heterogenous noise levels \u03c32i that depend on the linear instance and are unknown to the decision-maker. We proposed TRACE-UCB, an optimistic algorithm that successfully solves the explorationexploitation dilemma by simultaneously learning the \u03c32i \u2019s, allocating samples accordingly to their estimates, and balancing the contextual information across the instances. We also provide strong theoretical guarantees for two losses of interest: in expectation and high-probability. Simulations were conducted in several settings, with both synthetic and real data. The favorable results suggest that TRACE-UCB is reliable, and remarkably robust even in settings that fall outside its assumptions, thus, a useful and simple tool to implement in practice.\nAcknowledgements. A. Lazaric is supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and French National Research Agency projects ExTraLearn (n.ANR-14-CE24-0010-01)."}, {"heading": "A. Optimal Static Allocation", "text": ""}, {"heading": "A.1. Proof of Proposition 1", "text": "Proposition. Given m linear regression problems, each characterized by a parameter \u03b2i, Gaussian noise with variance \u03c32i , and Gaussian contexts with covariance \u03a3, let n > m(d + 1), then the optimal OLS static allocation algorithm A\u2217stat selects each instance\nk\u2217i,n = \u03c32i\u2211 j \u03c3 2 j n+ (d+ 1)\n( 1\u2212 \u03c3 2 i\n\u03c32\n) , (19)\ntimes (up to rounding effects), and incurs the global error\nL\u2217n = Ln(A\u2217stat) = \u03c32 md\nn +O\n( \u03c32 ( md\nn\n)2) . (20)\nProof. For the sake of readability in the following we drop the dependency on n.\nWe first derive the equality in Eq. 2\nLi(\u03b2\u0302i) = EX [ (XT\u03b2i \u2212XT\u03b2\u0302i)2 ] = EX [(\u03b2\u0302i \u2212 \u03b2i)TXXT(\u03b2\u0302i \u2212 \u03b2i)]\n= (\u03b2\u0302i \u2212 \u03b2i)TE[XXT](\u03b2\u0302i \u2212 \u03b2i)\n= (\u03b2\u0302i \u2212 \u03b2i)T\u03a3(\u03b2\u0302i \u2212 \u03b2i)\n= \u2016\u03b2i \u2212 \u03b2\u0302i\u20162\u03a3.\nAs a result, we can write the global error as\nLn(Astat) = max i\u2208[m]\nEDi,n [ \u2016\u03b2i \u2212 \u03b2\u0302i\u20162\u03a3 ] = max i\u2208[m] EDi,n [ Tr ( (\u03b2i \u2212 \u03b2\u0302i)T\u03a3(\u03b2i \u2212 \u03b2\u0302i) )]\n= max i\u2208[m]\nEDi,n [ Tr ( \u03a3(\u03b2i \u2212 \u03b2\u0302i)(\u03b2i \u2212 \u03b2\u0302i)T )]\n= max i\u2208[m] Tr\n( EDi,n [ \u03a3(\u03b2i \u2212 \u03b2\u0302i)(\u03b2i \u2212 \u03b2\u0302i)T ]) ,\nwhere Di,n is the training set extracted from Dn containing the samples for instance i. Since contexts and noise are independent random variables, we can decompose Di,n into the randomness related to the context matrix Xi \u2208 Rki\u00d7d and the noise vector i \u2208 Rki . We recall that for any fixed realization of Xi \u2208 Rki\u00d7d, the OLS estimates \u03b2\u0302i is distributed as\n\u03b2\u0302i | Xi \u223c N (\u03b2i, \u03c32i (XTi Xi)\u22121), (21)\nwhich means that \u03b2\u0302i conditioned on Xi is unbiased with covariance matrix given by \u03c32i (X T i Xi) \u22121. Thus, we can further develop Ln(Astat) as\nLn(Astat) = max i\u2208[m] Tr\n( EXi [ E i [ \u03a3(\u03b2i \u2212 \u03b2\u0302i)(\u03b2i \u2212 \u03b2\u0302i)T \u2223\u2223Xi]]), (22) = max i\u2208[m] \u03c32iTr ( \u03a3EXi [ (XTi Xi) \u22121 ])\n= max i\u2208[m]\n\u03c32iTr ( EXi [ (X T i Xi) \u22121 ]) ,\nwhere X = \u03a3\u22121/2X is a whitened context and Xi is its corresponding whitened matrix. Since whitened contexts X are distributed as N (0, I), we know that (XTi Xi)\u22121 is distributed as an inverse Wishart W\u22121(Id, ki), whose expectation is\nId/(ki \u2212 d\u2212 1), and thus,\nLn(Astat) = max i\u2208[m] \u03c32iTr\n[ 1\nki \u2212 d\u2212 1 Id ] = max i\u2208[m]\n\u03c32i d\nki \u2212 d\u2212 1 . (23)\nNote that this final expression requires that ki > d+ 1, since it is not possible to compute an OLS estimate with less than d + 1 samples. Therefore, we proceed by minimizing Eq. 23, subject to ki > d + 1. We write ki = k\u2032i + d + 1 for some k\u2032i > 0. Thus, equivalently, we minimize\nLn(Astat) = max i\n\u03c32i d\nk\u2032i . (24)\nSince \u2211 i k \u2032 i = n\u2212m(d+ 1), we may conclude that the optimal k\u2032i is given by\nk\u2032i = \u03c32i\u2211 j \u03c3 2 j\n( n\u2212m(d+ 1) ) ,\nso that all the terms in the RHS of Eq. 24 are equal. This gives us the optimal static allocation\nk\u2217i = \u03c32i\u2211 j \u03c3 2 j (n\u2212m(d+ 1)) + d+ 1\n= \u03c32i\u2211 j \u03c3 2 j n+ (d+ 1)\n( 1\u2212 \u03c3 2 i\n\u03c32\n) , (25)\nwhere \u03c32 = (1/m) \u2211 i \u03c3 2 i is the mean variance across the m problem instances.\nThus, for the optimal static allocation, the expected loss is given by\nL\u2217n = Ln(A\u2217stat) = dmax i \u03c32i \u03c32i\u2211 j \u03c3 2 j n\u2212 (d+ 1)\u03c3 2 i \u03c3\u03042\n=\n(\u2211 j \u03c3 2 j ) d\nn\u2212m(d+ 1)\n=\n(\u2211 j \u03c3 2 j ) d\nn +\n(\u2211 j \u03c3 2 j ) md(d+ 1)\nn ( n\u2212m(d+ 1) ) = (\u2211 j \u03c3 2 j ) d\nn +O\n (\u2211 j \u03c3 2 j ) md2\nn2  , which concludes the proof. Furthermore the following bounds trivially holds for any n \u2265 2m(d+ 1)\nmd\u03c32\nn \u2264 L\u2217n \u2264 2\nmd\u03c32\nn ."}, {"heading": "B. Loss of an OLS-based Learning Algorithm (Proof of Lemma 2)", "text": "Unlike in the proof of Proposition 1, when the number of pulls is random and depends on the value of the previous observations (through Dn), then in general, the OLS estimates \u03b2\u0302i,n are no longer distributed as Eq. 21 and the derivation for Astat no longer holds. In fact, for a learning algorithm, the value ki,t itself provides some information about the observations that have been obtained up until time t and were used by the algorithm to determine ki,t. In the following, we show that by ignoring the current context Xt when choosing instance It, we are still able to analyze the loss of TRACEUCB and obtain a result very similar to the static case.\nWe first need two auxiliary lemmas (Lemmas 8 and 9), one on the computation of an empirical estimate of the variance of the noise, and an independence result between the variance estimate and the linear regression estimate.\nLemma 8. In any linear regression problem with noise \u223c N (0, \u03c32), after t \u2265 d + 1 samples, given an OLS estimator \u03b2\u0302t, the noise variance estimator can be computed in a recurrent form as\n\u03c3\u03022t+1 = t\u2212 d\nt\u2212 d+ 1 \u03c3\u03022t +\n1 t\u2212 d+ 1 (XTt+1\u03b2\u0302t \u2212 Yt+1)2\n1 +XTt+1(X T tXt) \u22121Xt+1 , (26)\nwhere Xt \u2208 Rt\u00d7d is the sample matrix.\nProof. We first recall the \u201cbatch\u201d definition of the variance estimator\n\u03c3\u03022t = 1\nt\u2212 d t\u2211 s=1 (Ys \u2212XTs \u03b2\u0302t)2 = 1 t\u2212 d \u2016Yt \u2212XTt \u03b2\u0302t\u20162\nSince Yt = Xt\u03b2 + t and \u03b2\u0302t = \u03b2 + (XTtXt) \u22121XTt t, we have\n\u03c3\u03022t = 1\nt\u2212 d \u2016(XTtXt)\u22121XTt t \u2212 t\u20162 =\n1\nt\u2212 d\n( Tt t \u2212 TtXt(XTtXt)\u22121XTt t ) = 1\nt\u2212 d (Et+1 \u2212 Vt+1).\nWe now devise a recursive formulation for the two terms in the previous expression. We have\nEt+1 = T t+1 t+1 = T t t + 2 t+1 = Et + 2 t+1.\nIn order to analyze the second term we first introduce the design matrix St = XTtXt, which has the simple update rule St+1 = St +Xt+1X T t+1. Then we have\nVt+1 = T t+1Xt+1(X T t+1Xt+1) \u22121XTt+1 t+1 = ( TtXt + t+1X T t+1 )( St +Xt+1X T t+1 )\u22121( TtXt + t+1X T t+1 )T = ( TtXt + t+1X T t+1 )( S\u22121t \u2212 S\u22121t Xt+1X T t+1S \u22121 t\n1 +XTt+1S \u22121 t Xt+1\n)( TtXt + t+1X T t+1 )T ,\nwhere we used the Sherman-Morrison formula in the last equality. We further develop the previous expression as\nVt+1 = Vt + t+1X T t+1S \u22121 t Xt+1 t+1 + 2 t+1X T t+1S \u22121 t X T t t\n\u2212 TtXt S\u22121t Xt+1X T t+1S \u22121 t\n1 +XTt+1S \u22121 t Xt+1\nXTt t \u2212 t+1XTt+1 S\u22121t Xt+1X T t+1S \u22121 t\n1 +XTt+1S \u22121 t Xt+1\nXt+1 t+1 \u2212 2 TtXt S\u22121t Xt+1X T t+1S \u22121 t\n1 +XTt+1S \u22121 t Xt+1\nXt+1 t+1.\nWe define \u03b1t+1 = XTt+1S \u22121 t X T t t and \u03c8t+1 = X T t+1S \u22121 t Xt+1, and then obtain\nVt+1 = Vt + 2 t+1\u03c8t+1 + 2\u03b1t+1 t+1 \u2212 \u03b12t+1 1 + \u03c8t+1 \u2212 2t+1 \u03c82t+1 1\u2212 \u03c8t+1 \u2212 2 t+1 \u03b1t+1\u03c8t+1 1 + \u03c8t+1\n= Vt + 2 t+1 ( \u03c8t+1 +\n\u03c82t+1 1 + \u03c8t+1\n) + 2 t+1\n\u03b1t+1 1 + \u03c8t+1 \u2212 \u03b12t+1 1 + \u03c8t+1 .\nBringing everything together we obtain\nEt+1 \u2212 Vt+1 = Et \u2212 Vt + 2t+1 ( 1\u2212 \u03c8t+1 + \u03c82t+1\n1 + \u03c8t+1\n) \u2212 2 t+1\n\u03b1t+1 1 + \u03c8t+1 + \u03b12t+1 1 + \u03c8t+1\n= Et \u2212 Vt + 1\n1 + \u03c8t+1\n( 2t+1 \u2212 2 t+1\u03b1t+1 + \u03b1t+1 ) = Et \u2212 Vt + ( t+1 \u2212 \u03b1t+1 )2 1 + \u03c8t+1 .\nSince t+1 = Yt+1 \u2212XTt+1\u03b2, we may write\nEt+1 \u2212 Vt+1 = Et \u2212 Vt + ( Yt+1 \u2212XTt+1(\u03b2 + S\u22121t XTt t) )2 1 + \u03c8t+1 = Et \u2212 Vt + ( Yt+1 \u2212XTt+1\u03b2\u0302t )2 1 + \u03c8t+1 .\nRecalling the definition of the variance estimate, we finally obtain\n\u03c3\u03022t+1 = 1\nt\u2212 d+ 1 (Et+1 \u2212 Vt+1) =\n1\nt\u2212 d+ 1 (Et \u2212 Vt) +\n1\nt\u2212 d+ 1\n( Yt+1 \u2212XTt+1\u03b2\u0302t )2 1 +XTt+1S \u22121 t Xt+1\n= t\u2212 d\nt\u2212 d+ 1 \u03c3\u03022t +\n1\nt\u2212 d+ 1\n( Yt+1 \u2212XTt+1\u03b2\u0302t )2 1 +XTt+1S \u22121 t Xt+1 ,\nwhich concludes the proof.\nLemma 9. Let Fj be the \u03c3-algebra generated by X1, . . . , Xn and \u03c3\u030221 , . . . , \u03c3\u03022j . Then, for any j \u2265 d,\n\u03b2\u0302j | Fj \u223c N (\u03b2, \u03c32 (XT1:jX1:j)\u22121). (27)\nProof. We prove the lemma by induction. The statement is true for t = d. We want to prove the induction, that is if \u03b2\u0302t | Ft \u223c N (\u03b2, \u03c32 (XTtXt)\u22121), then\n\u03b2\u0302t+1 | Ft+1 \u223c N (\u03b2, \u03c32(XTt+1Xt+1)\u22121). (28)\nLet us first derive a recursive expression for \u03b2\u0302t+1. Let St = XTtXt, then\n\u03b2\u0302t+1 = \u03b2 + S \u22121 t+1X T t+1 t+1 = ( St +Xt+1X T t+1 )\u22121( XTt t + t+1Xt+1 ) = ( S\u22121t \u2212 S\u22121t Xt+1X T t+1S \u22121 t\n1 +XTt+1S \u22121 t Xt+1\n)( XTt t + t+1Xt+1 ) ,\nwhere we used Sherman-Morrison formula. By developing the previous expression we obtain\n\u03b2\u0302t+1 = ( \u03b2 + S\u22121t X T t t ) + t+1S \u22121 t Xt+1 ( 1\u2212 XTt+1S \u22121 t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n) \u2212 S\u22121t Xt+1X T t+1S \u22121 t X T t t\n1 +XTt+1S \u22121 t Xt+1\n= \u03b2\u0302t + t+1S\n\u22121 t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n\u2212 S\u22121t Xt+1X T t+1(\u03b2\u0302t \u2212 \u03b2)\n1 +XTt+1S \u22121 t Xt+1\n.\nWe can conveniently rewrite the previous expression as\n\u03b2\u0302t+1 \u2212 \u03b2 = ( I \u2212 S\u22121t Xt+1X T t+1\n1 +XTt+1S \u22121 t Xt+1\n) (\u03b2\u0302t \u2212 \u03b2) +\nt+1S \u22121 t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n= (I \u2212 \u03b1t)(\u03b2\u0302t \u2212 \u03b2) + \u03b3t t+1, (29)\nwhere \u03b1t \u2208 Rd\u00d7d and \u03b3t \u2208 Rd are defined implicitly. By Lemma 8, we notice that the sequence of empirical variances in Ft is equivalent to the sequence of squared deviations up to t. In order to make this equivalence more apparent we define the filtration\nGt = { {Xs}ns=1 \u222a \u03c3\u030222 \u222a {(XTs+1\u03b2\u0302s \u2212 s+1)2}t\u22121s=2 } ,\nso that \u03b2\u0302t+1 | Ft+1 \u223c \u03b2\u0302t+1 | Gt+1. We introduce two auxiliary random vectors conditioned on G\nU = t+1 \u2212XTt+1(\u03b2\u0302t \u2212 \u03b2) | Gt, V = \u03b2\u0302t+1 \u2212 \u03b2 | Gt.\nWe want to show that the random vectors U \u2208 R and V \u2208 Rd are independent. We first recall that the noise t+1 | Gt \u223c N (0, \u03c32), and it is independent of 1, . . . , t, and \u03b2\u0302t under Gt. Furthermore, by the induction assumption \u03b2\u0302t | Gt is also Gaussian, so we have that (\u03b2\u0302t, t+1) are jointly Gaussian given Gt. Then we can conveniently rewrite U as\nU = (\u2212Xt+1, 1)T(\u03b2\u0302t, t+1) +XTt+1\u03b2,\nwhich shows that it is a Gaussian vector. Using the recursive formulation in Eq. 29 we can also rewrite V as\nV = (Id\u2212 \u03b1t)(\u03b2\u0302t \u2212 \u03b2) + \u03b3t t+1 = [ I\u2212 \u03b1t \u03b3t ] [\u03b2\u0302t \u2212 \u03b2 t+1 ] ,\nwhich is also Gaussian. Furthermore, we notice that under the induction assumption, EGt [U ] = 0 and EGt [V ] = 0 and thus we need to show that E[UV | Gt] = 0 to prove that U and V are uncorrelated\nE[UV | Gt] = EGt [( t+1 \u2212XTt+1(\u03b2\u0302t \u2212 \u03b2) )( (Id\u2212 \u03b1t)(\u03b2\u0302t \u2212 \u03b2) + \u03b3t t+1 )] = \u03b3t EGt [ 2t+1 ] \u2212 EGt [ XTt+1(\u03b2\u0302t \u2212 \u03b2)(I\u2212 \u03b1t)(\u03b2\u0302t \u2212 \u03b2)\n] = \u03c32\u03b3t \u2212 EGt [ (I\u2212 \u03b1t)(\u03b2\u0302t \u2212 \u03b2)(\u03b2\u0302t \u2212 \u03b2)TXt+1\n] = \u03c32\u03b3t \u2212 (I\u2212 \u03b1t) EGt [ (\u03b2\u0302t \u2212 \u03b2)(\u03b2\u0302t \u2212 \u03b2)T ] Xt+1\n= \u03c32\u03b3t \u2212 \u03c32(I\u2212 \u03b1t)(XTtXt)\u22121Xt+1\n= \u03c32 S\u22121t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n\u2212 \u03c32 ( I\u2212 S\u22121t Xt+1X T t+1\n1 +XTt+1S \u22121 t Xt+1\n) S\u22121t Xt+1\n= \u03c32 S\u22121t Xt+1 \u2212 (1 +XTt+1S\u22121t Xt+1)S\u22121t Xt+1 + S\u22121t Xt+1XTt+1S\u22121t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n= 0.\nIt thus follows that, as U and V are uncorrelated, they are also independent. Combining the definition of Gt, U and its independence w.r.t V , we have\nV | Gj+1 = V | U,Gj = V | {X1, . . . , XT , \u03c3\u030222 , {(XTs+1\u03b2\u0302s \u2212 s+1)2}t\u22121s=2}\n= [ I\u2212 \u03b1t \u03b3t ] [\u03b2\u0302t \u2212 \u03b2 t+1 ] | Gt.\nBy the induction hypothesis the vector in the previous expression is distributed as[ \u03b2\u0302t \u2212 \u03b2 t+1 ] \u223c N ([ 0 0 ] , \u03c32 [ S\u22121t 0 0 1 ]) .\nTherefore, we conclude that V | Gt+1 \u223c N ( 0, \u03c32 [ I\u2212 \u03b1t \u03b3t ] [S\u22121t 0 0 1 ] [ I\u2212 \u03b1t \u03b3t ]T) = N (0, \u03c32 \u03a3\u2032),\nwhere the covariance matrix \u03a3\u2032 can be written as\n\u03a3\u2032 = [ I\u2212 \u03b1t \u03b3t ] [S\u22121t 0 0 1 ] [ I\u2212 \u03b1t \u03b3t ]T = [ I\u2212 \u03b1t \u03b3t ] [S\u22121t (I\u2212 \u03b1t)T \u03b3Tt\n] = (I\u2212 \u03b1t)S\u22121t (I\u2212 \u03b1t)T + \u03b3t\u03b3Tt .\nRecalling the definitions of \u03b1t and \u03b3t, and defining \u03c8t+1 = XTt+1S \u22121 t Xt+1\n\u03a3\u2032 = ( I\u2212 S\u22121t Xt+1X T t+1\n1 +XTt+1S \u22121 t Xt+1\n) S\u22121t ( I\u2212 S\u22121t Xt+1X T t+1\n1 +XTt+1S \u22121 t Xt+1\n)T\n+\n( S\u22121t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n)( S\u22121t Xt+1\n1 +XTt+1S \u22121 t Xt+1\n)T\n= S\u22121t \u2212 2 S\u22121t Xt+1X T t+1S \u22121 t\n1 + r + \u03c8t+1\nS\u22121t Xt+1X T t+1S \u22121 t\n(1 + \u03c8t+1)2 +\nS\u22121t Xt+1X T t+1S \u22121 t\n(1 + \u03c8t+1)2\n= S\u22121t \u2212 S\u22121t Xt+1X T t+1S \u22121 t\n1 + \u03c8t+1 = S\u22121t+1 = (X T t+1Xt+1) \u22121,\nwhere we applied the Woodbury matrix identity in the last step. Finally, it follows that\n\u03b2\u0302t+1 | Ft+1 \u223c N (\u03b2, \u03c32 (XTt+1Xt+1)\u22121),\nand the induction is complete.\nNow we can prove Lemma 2:\nLemma. Let A be a learning algorithm that selects instances It as a function of the previous history, that is, Dt\u22121 = {X1, I1, YI1,1, . . . , Xt\u22121, It\u22121, YIt\u22121,t\u22121} and computes estimates \u03b2\u0302i,n using OLS. Then, its loss after n steps can be expressed as\nLn(A) = max i\u2208[m] EDt [ \u03c32i ki,n Tr ( \u03a3\u03a3\u0302\u22121i,n )] , (30)\nwhere ki,n = \u2211n t=1 I{It = i} and \u03a3\u0302i,n = XTi,nXi,n/ki,n.\nProof. For any instance i, we can assume that the following random variables are sampled before TRACE-UCB starts collecting observations (we omit the i index in the table):\nt = 1 t = 2 . . . t = n X1 X2 . . . Xn 1 2 . . . n \u03b2\u03021 \u03b2\u03022 . . . \u03b2\u0302n \u03c3\u030221 \u03c3\u0302 2 2 . . . \u03c3\u0302 2 n\nAs a result, we can interpret TRACE-UCB as controlling the stopping time ti = ki,n for each problem i, that is, the total number of samples ki,n, leading to the final estimates \u03b2\u0302ti and \u03c3\u0302 2 ti . In the following we introduce the notation X1:j as the sample matrix constructed from exactly j samples, unlike Xi,n which is the sample matrix obtained with ki,n. So we have X1:ki,n = Xi,n. Crucially, when the errors j are Gaussian, then \u03b2\u0302j | X1:j and \u03c3\u03022j | X1:j are independent for any fixed j (note these random variables have nothing to do with the algorithm\u2019s decisions).\nLet Fj be the \u03c3-algebra generated by X1, . . . , Xn and \u03c3\u030221 , . . . , \u03c3\u03022j . We recall that from Lemma 9\n\u03b2\u0302j |X1:j = \u03b2\u0302j | Fj \u223c N (\u03b2j , \u03c32 (XT1:jX1:j)\u22121). (31)\nIntuitively, this results says that, given the data X1:n, if we are additionally given all the estimates for the variance {\u03c3\u03022s} j s=2 \u2014which obviously depend on 1, . . . , j\u2014, then the updated distribution for \u03b2\u0302j does not change at all. This is a crucial property since TRACE-UCB ignores the current context Xt and it makes decisions only based on previous contexts and the variance estimates {\u03c3\u03022s} j s=2, thus allowing us to proceed and do inference on \u03b2\u0302j as in the fixed allocation case.\nWe now need to take into consideration the filtration Fi,j for a specific instance i and the environment filtration E\u2212i containing all the contexts X and noise from all other instances (different from i). Since the environment filtration E\u2212i is independent from the samples from instance i, then we can still apply Lemma 9 and obtain\n\u03b2\u0302i,j | Fi,j , E\u2212i \u223c \u03b2\u0302i,j | Fi,j . (32)\nNow we can finally study the expected prediction error\nLi,n(\u03b2\u0302i,n) =E[(\u03b2\u0302i \u2212 \u03b2i)(\u03b2\u0302i \u2212 \u03b2i)T] = EX1:n,\u03b5\u2212i [ E[(\u03b2\u0302i \u2212 \u03b2i)(\u03b2\u0302i \u2212 \u03b2i)T | X1:n, \u03b5\u2212i] ] = EX1:n,\u03b5\u2212i  n\u2211 j=1 E[(\u03b2\u0302ki \u2212 \u03b2i)(\u03b2\u0302ki \u2212 \u03b2i)T | X1:n, \u03b5\u2212i, ki = j] P(ki = j)\n = EX1:n,\u03b5\u2212i  n\u2211 j=1 E [ EFj [(\u03b2\u0302j \u2212 \u03b2i)(\u03b2\u0302j \u2212 \u03b2i)T | Fj ,X1:n, \u03b5\u2212i, ki = j] | X1:n, \u03b5\u2212i, ki = j ] P(ki = j)\n = EX1:n,\u03b5\u2212i  n\u2211 j=1 E [ EFj [(\u03b2\u0302j \u2212 \u03b2i)(\u03b2\u0302j \u2212 \u03b2i)T | Fj ,X1:n] | X1:n, \u03b5\u2212i, ki = j ] P(ki = j)\n (33) = EX1:n,\u03b5\u2212i  n\u2211 j=1 E [ \u03c32i (X T 1:jX1:j) \u22121 | X1:n, ki = j ] P(ki = j)\n = EX1:n,\u03b5\u2212i  n\u2211 j=1 \u03c32i (X T 1:jX1:j) \u22121 P(ki = j)\n = \u03c32i EX1:n,\u03b5\u2212i [ Eki [(XT1:kiX1:ki) \u22121] ]\n= \u03c32i E [ (XT1:kiX1:ki) \u22121] , where in Eq. 33 we applied Lemma 9. Hence, going back to the definition of loss (see e.g., Eq. 22), we obtain an expression for the loss which applies under TRACE-UCB (while not in general for other algorithms)\nLn(A) = max i\nE [ \u03c32i Tr(\u03a3(X T i,nXi,n) \u22121) ]\n= max i E [ \u03c32i ki,n Tr ( \u03a3\u03a3\u0302\u22121i,n )] ."}, {"heading": "C. Concentration Inequalities (Proofs of Propositions 3 and 4)", "text": "In the next two subsections, we prove Propositions 3 and 4, respectively. In addition, we also show a confidence ellipsoid result for the \u03b2\u0302 estimates, and a concentration inequality for the norm of the observations Xt."}, {"heading": "C.1. Concentration Inequality for the Variance (Proof of Proposition 3)", "text": "We use the following concentration inequality for sub-exponential random variables.\nProposition 10. Let X be a mean-zero (\u03c42, b)-subexponential random variable. Then, for all \u03b7 > 0,\nP(|X| \u2265 \u03b7) \u2264 exp ( \u22121\n2 min\n{ \u03b72\n\u03c42 , \u03b7 b\n}) . (34)\nProof. See Proposition 2.2 in (Wainwright, 2015).\nWe first prove the concentration inequality for one single instance.\nProposition 11. Let t > d, Xt \u2208 Rt\u00d7d be a random matrix whose entries are independent standard normal random variables, Yt = XTt \u03b2 + t, where the noise t \u223c N (0, \u03c32t Id) is independent from Xt, and \u03b4 \u2208 (0, 3/4]. Then, with probability at least 1\u2212 \u03b4, we have\n|\u03c3\u03022t \u2212 \u03c32| \u2264 \u03c32 \u221a 64\nt\u2212 d\n( log 1\n\u03b4\n)2 , (35)\nwhere \u03c3\u03022t is the unbiased estimate \u03c3\u0302 2 t = 1 t\u2212d\u2016Yt \u2212Xt\u03b2\u0302t\u2016 2 and \u03b2\u0302t is the OLS estimator of \u03b2, given Xt and Yt.\nProof. First note that the distribution of \u03c3\u03022t conditioned on Xt follows the scaled chi-squared distribution, i.e.,\n\u03c3\u03022t | X \u223c \u03c32\nt\u2212 d \u03c72t\u2212d.\nAlso note that the distribution of the estimate does not depend on Xt and we can integrate out the randomness in Xt. In order to show concentration around the mean, we directly use the sub-exponential properties of \u03c3\u03022t . The \u03c7 2 k distribution is sub-exponential with parameters (4k, 4).4 Furthermore, we know that for any constant C > 0, C\u03c72k is (4C 2k, 4C)-subexponential. As a result, we have that \u03c3\u03022t is subexponential with parameters\n(\u03c42, b) =\n( 4\u03c34\nt\u2212 d ,\n4\u03c32\nt\u2212 d\n) .\nNow we use Proposition 10 as our concentration bound. In our case, \u03b72/\u03c42 < \u03b7/b, when \u03b7 < \u03c32. In such a case, if we denote the RHS of (34) by \u03b4, we conclude that\n\u03b7 = \u03c32 \u221a 8\nt\u2212 d log\n1 \u03b4 .\nThen, \u03b7 < \u03c32 holds when t \u2265 d+ 8 log(1/\u03b4). Otherwise, if \u03b72/\u03c42 > \u03b7/b, by Eq. 34, we have\n\u03b7 = 8\u03c32\nt\u2212 d log\n1 \u03b4 .\nIn this case, when t < d+ 8 log(1/\u03b4), we have that\n|\u03c3\u03022t \u2212 \u03c32| \u2264 \u03c32 8\nt\u2212 d log\n1 \u03b4 .\n4See Example 2.5 in (Wainwright, 2015).\nWe would like to derive a bound that is valid in both cases. Let x = 8 log(1/\u03b4)/(t\u2212 d), then we have\nP ( |\u03c3\u03022t \u2212 \u03c32| \u2265 \u03c32 max(x, \u221a x) ) \u2264 \u03b4. (36)\nSuppose x \u2265 \u221a x, so t < d+ log(1/\u03b4). Then, we would like to find C, such that x \u2264 C \u221a x. As t \u2265 d+ 1, we see that\n\u221a x =\n\u221a 8 log(1/\u03b4)\nt\u2212 d \u2264 \u221a 8 log(1/\u03b4) \u2206 = C.\nif C > 1, it does follow that max(x, \u221a x) < max(C \u221a x, \u221a x) < \u221a 8 log(1/\u03b4)x, which corresponds to \u03b4 < 0.88. By (36), we now conclude that\nP |\u03c3\u03022t \u2212 \u03c32| \u2265 \u03c32 \u221a 64\nt\u2212 d\n( log 1\n\u03b4 )2 \u2264 \u03b4, and the proof is complete.\nIn order to prove Proposition 3, we are just left to apply a union bound over steps t \u2208 {1, . . . , n} and instances i \u2208 {1, . . . ,m}. In order to avoid confusion, let \u03c3\u0302i,t be the estimate obtained by the algorithm after t steps and \u03c3\u0302i(j) the estimate obtained using j samples. Let j > d, then\nEi(j) = { |\u03c3\u03022i (j)\u2212 \u03c32i | \u2265 \u03c32i \u221a 64\nj \u2212 d\n( log 1\n\u03b4\n)2} ,\nis the high-probability event introduced in Proposition 11, which holds with probability 1\u2212 \u03b4. Then we have that the event\nE = m\u22c2 i=1 n\u22c2 j=1 Ei(j),\nholds with probability 1 \u2212 \u03b4\u2032, with \u03b4\u2032 = mn\u03b4. We complete the proof of Proposition 3 by properly tuning \u03b4 and taking R \u2265 maxi \u03c32i . Recall that Proposition 3 is as follows. Proposition. Let the number of pulls ki,t \u2265 d + 1 and R \u2265 maxi \u03c32i . If \u03b4 \u2208 (0, 3/4), then for any instance i and step t > m(d+ 1), with probability at least 1\u2212 \u03b42 , we have\n|\u03c3\u03022i,t \u2212 \u03c32i | \u2264 \u2206i,t \u2206 = R\n\u221a 64\nki,t \u2212 d\n( log 2mn\n\u03b4\n)2 . (37)"}, {"heading": "C.2. Concentration Inequality for the Trace (Proof of Proposition 4)", "text": "We first recall some basic definitions. For any matrix A \u2208 Rn\u00d7d, the i-th singular value si(A) is equivalent to si(A)2 = \u03bbi(A TA), where \u03bbi is the i-th eigenvalue. The smallest and largest singular values smin and smax satisfy\nsmin \u2016x\u20162 \u2264 \u2016Ax\u20162 \u2264 smax \u2016x\u20162 for all x \u2208 Rd.\nThe extreme singular values measure the maximum and minimum distortion of points and their distance when going from Rd to Rn via the linear operator A. We also recall that the spectral norm of A is given by\n\u2016A\u2016 = sup x\u2208Rd\\0 \u2016Ax\u20162 \u2016x\u20162 = sup x\u2208Sn\u22121 \u2016Ax\u20162,\nand thus, smax(A) = \u2016A\u2016 and smin(A) = 1/\u2016A\u22121\u2016, if A is invertible.\nWe report the following concentration inequality for the eigenvalues of random Gaussian matrices.\nProposition 12. Let n \u2265 d, X \u2208 Rn\u00d7d be a random matrix whose entries are independent standard normal random variables, and \u03a3 = X T X/n be the corresponding empirical covariance matrix. Let \u03b1 > 0, then with probability at least 1\u2212 2 exp(\u2212\u03b12d/2), we have\nTr ( \u03a3 \u22121) \u2265 d(1\u2212 2(1 + \u03b1)\u221ad+ (1 + \u03b1)2d/\u221an\u221a\nn+ 2(1 + \u03b1) \u221a d+ (1 + \u03b1)2d/ \u221a n\n) ,\nand\nTr ( \u03a3 \u22121) \u2264 d(1 + 2(1 + \u03b1)\u221ad\u2212 (1 + \u03b1)2d/\u221an\u221a\nn\u2212 2(1 + \u03b1) \u221a d+ (1 + \u03b1)2d/ \u221a n\n) .\nIn particular, we have\nd ( 1\u2212 (1 + \u03b1) \u221a d\nn\n)2 \u2264 Tr ( \u03a3 \u22121) \u2264 d(1 + 2(1 + \u03b1)\u221a d\nn\n)2 .\nProof. We first derive the concentration inequality for the eigenvalues of the empirical covariance matrix and then we invert it to obtain the guarantee for the inverse matrix. From Corollary 5.35 in (Vershynin, 2010), we have that for any t > 0 (\u221a\nn\u2212 \u221a d\u2212 t )2 \u2264 \u03bbmin(X T X) = smin(X) 2 \u2264 smax(X)2 = \u03bbmax(X T X) \u2264 (\u221a n+ \u221a d+ t )2 , (38)\nwith probability at least 1\u22122 exp(\u2212t2/2). Let \u03b1 > 0 and take t = \u03b1 \u221a d, then with probability at least 1\u22122 exp(\u2212\u03b12d/2), we obtain the desired statement( 1\u2212 (1 + \u03b1) \u221a d\nn\n)2 \u2264 \u03bbmin ( \u03a3 ) \u2264 \u03bbmax ( \u03a3 ) \u2264 ( 1 + (1 + \u03b1) \u221a d\nn\n)2 .\nWe now proceed by studying the eigenvalues of the inverse of the empirical covariance matrix \u03bbmin(\u03a3 \u22121 ) = 1/\u03bbmax(\u03a3) and \u03bbmax(\u03a3 \u22121 ) = 1/\u03bbmin(\u03a3). Combined with Eq. 38 we have\n\u03bbmin ( \u03a3 \u22121) \u2265 1(\n1 + (1 + \u03b1) \u221a\nd n )2 = 1\n1 + 2(1 + \u03b1) \u221a\nd n + (1 + \u03b1) 2 d n\n= 1\u2212 2(1 + \u03b1)\n\u221a d n + (1 + \u03b1) 2 d n\n1 + 2(1 + \u03b1) \u221a\nd n + (1 + \u03b1) 2 d n\n.\nSimilarly, we have that\n\u03bbmax ( \u03a3 \u22121) \u2264 1(\n1\u2212 (1 + \u03b1) \u221a\nd n )2 = 1\n1\u2212 2(1 + \u03b1) \u221a\nd n + (1 + \u03b1) 2 d n\n= 1 + 2(1 + \u03b1)\n\u221a d n \u2212 (1 + \u03b1) 2 d n\n1\u2212 2(1 + \u03b1) \u221a\nd n + (1 + \u03b1) 2 d n\n.\nUsing the fact that for any matrix A \u2208 Rd\u00d7d, we may write d \u03bbmin(A) \u2264 Tr(A) \u2264 d \u03bbmax(A), we obtain the final statement on the trace of \u03a3 \u22121 . The first of the two bounds can be further simplified by using 1/(1 + x) \u2265 1 \u2212 x for any\nx \u2265 0, thus obtaining\n\u03bbmin ( \u03a3 \u22121) \u2265 (1\u2212 (1 + \u03b1)\u221a d\nn\n)2 .\nWhile under the assumption that n \u2265 4(1 + \u03b1)2d we can use 1/(1\u2212 x) \u2264 1 + 2x (for any x \u2265 1/2) and obtain \u03bbmax ( \u03a3 \u22121) \u2264 (1 + 2(1 + \u03b1)\u221a d\nn\n)2 .\nThe statement of Proposition 4 (below) is obtained by recalling that \u03a3\u03a3\u0302\u22121i,n is the empirical covariance matrix of the whitened sample matrix Xi,n and by a union bound over the number of samples ki,n and the number of instances i.\nProposition. Force the number of samples ki,t \u2265 d + 1. If \u03b4 \u2208 (0, 1), for any i \u2208 [m] and step t > m(d + 1) with probability at least 1\u2212 \u03b4/2, we have\n( 1\u2212 CTr \u221a d\nn\n)2 \u2264\nTr (\n\u03a3\u03a3\u0302\u22121i,t ) d \u2264 ( 1 + 2CTr \u221a d n )2 ,\nwith CTr = 1 + \u221a 2 log(4nm/\u03b4)/d."}, {"heading": "C.3. Concentration Inequality for \u03b2\u0302 Estimates", "text": "We slightly modify Theorem 2 from (Abbasi-Yadkori et al., 2011) to obtain a confidence ellipsoid over the \u03b2\u0302i\u2019s. Theorem 13. Let {Ft}\u221et=0 be a filtration. Let {\u03b7t}\u221et=1 be a real-valued stochastic process such that \u03b7t is Ft measurable and \u03b7t is conditionally R-subgaussian for some R \u2265 0, i.e.\n\u2200\u03bb \u2208 R E[e\u03bb\u03b7t | Ft\u22121] \u2264 exp ( \u03bb2R2\n2\n) . (39)\nLet {Xt}\u221et=1 be an Rd-valued stochastic process such that Xt is Ft\u22121 measurable. Assume that V is a d \u00d7 d positive definite matrix. For any t \u2265 0, define\nV\u0304t = V + t\u2211 s=1 XsX T s , St = t\u2211 s=1 \u03b7sXs. (40)\nLet V = \u03bbId, \u03bb > 0, and define Yt = XTt \u03b2 \u2217 + \u03b7t. Assume that \u2016\u03b2\u2217\u20162 \u2264 S. Also, let \u03b2\u0302t = V\u0304 \u22121t XTt Yt be the ridge estimate for \u03b2 after t observations Xt,Yt. Then, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, for all t \u2265 0, \u03b2\u2217 lies in\nCt = \u03b2 \u2208 Rd : \u2016\u03b2\u0302t \u2212 \u03b2\u2016V\u0304t/t \u2264 R\u221at \u221a\u221a\u221a\u221a2 log(det (V\u0304t)1/2 det (\u03bbI)\u22121/2 \u03b4 ) + \u221a \u03bb t S  . (41) Proof. Take x = V\u0304tt (\u03b2\u0302t \u2212 \u03b2 \u2217) in equation 5 in the proof of Theorem 2 in (Abbasi-Yadkori et al., 2011).\nWe use the previous theorem by lower bounding the V\u0304t/t norm in \u03a3 norm."}, {"heading": "C.4. Bounded Norm Lemma", "text": "Lemma 14. Let X1, . . . , Xt \u2208 Rd be iid subgaussian random variables.\nIf \u2016X1\u20162 is subexponential with parameters (a2, b), then, for \u03b1 > 0\nP 1 t t\u2211 j=1 \u2016Xj\u20162 \u2264 E[\u2016X1\u20162] + \u03b1 t  \u2265 {1\u2212 exp(\u2212 \u03b122ta2) if 0 \u2264 \u03b1 \u2264 ta2/b, 1\u2212 exp ( \u2212 \u03b12b ) if \u03b1 > ta2/b.\n(42)\nProof. The proof directly follows by Proposition 10, by defining zero-mean subexponential random variable Z with parameters (a2/t, b/t)\nZ = 1\nt t\u2211 j=1 \u2016Xj\u20162 \u2212 E 1 t t\u2211 j=1 \u2016Xj\u20162  . (43)\nCorollary 15. Let X1, . . . , Xt \u2208 Rd be iid gaussian variables, X \u223c N (0, Id). Assume t \u2265 d+ 1. Let \u03b4 > 0. Then, with probability at least 1\u2212 \u03b4,\n1\nt t\u2211 j=1 \u2016Xj\u20162 \u2264 d+ 8 log ( 1 \u03b4 )\u221a d t , (44)\nProof. For standard Gaussian X \u223c N (0, Id), \u2016X\u20162 \u223c \u03c72d, and a2 = 4d and b = 4. Note that E[\u2016Xj\u20162] = d. By the proof of Lemma 14 and (44)\nP ( |Z| \u2265 a \u221a 2\nt log\n( 1\n\u03b4\n)) \u2264 \u03b4, when t \u2265 2 ( b\na\n)2 log ( 1\n\u03b4\n) . (45)\nP ( |Z| \u2265 2b\nt log\n( 1\n\u03b4\n)) \u2264 \u03b4, when t < 2 ( b\na\n)2 log ( 1\n\u03b4\n) . (46)\nSubstituting a = 2 \u221a d and b = 4 leads to\nP ( |Z| \u2265 \u221a 8d\nt log\n( 1\n\u03b4\n)) \u2264 \u03b4, when t \u2265 8\nd log\n( 1\n\u03b4\n) . (47)\nP ( |Z| \u2265 8\nt log\n( 1\n\u03b4\n)) \u2264 \u03b4, when t < 8\nd log\n( 1\n\u03b4\n) . (48)\nWe would like to upper bound 8 log (1/\u03b4) /t in (48). As t > d, we see\n8 t log\n( 1\n\u03b4\n) \u2264 8\u221a\ndt log\n( 1\n\u03b4\n) . (49)\nAs a consequence,\nP ( |Z| \u2265 8\u221a\ndt log\n( 1\n\u03b4\n)) \u2264 \u03b4, when t < 8\nd log\n( 1\n\u03b4\n) . (50)\nIt follows that for all t > d\nP ( |Z| \u2265 max ( 8\u221a dt log ( 1 \u03b4 ) , \u221a 8d t log ( 1 \u03b4 ))) \u2264 \u03b4. (51)\nAs \u03b4 < 1, we finally conclude that\nP ( |Z| \u2265 8 \u221a d\nt log\n( 1\n\u03b4\n)) \u2264 \u03b4. (52)\nTherefore, with probability at least 1\u2212 \u03b4,\n1\nt t\u2211 j=1 \u2016Xj\u20162 \u2264 d+ 8 log ( 1 \u03b4 )\u221a d t , (53)\nas stated in the corollary."}, {"heading": "D. Performance Guarantees for TRACE-UCB", "text": ""}, {"heading": "D.1. Lower Bound on Number of Samples (Proof of Theorem 5)", "text": "We derive the high-probability guarantee on the number of times each instance is selected.\nTheorem. Let \u03b4 > 0. With probability at least 1 \u2212 \u03b4, the total number of contexts that TRACE-UCB allocates to each problem instance i after n rounds satisfies\nki,n \u2265 k\u2217i,n \u2212 C\u2206 + 8CTr\n\u03c32min\n\u221a nd\n\u03bbmin \u2212 \u2126(n1/4) (54)\nwhere R \u2265 \u03c32max is known by the algorithm, and we defined C\u2206 = 16R log(2mn/\u03b4), CTr = 1 + \u221a\n2 log(4nm/\u03b4)/d, and \u03bbmin = \u03c3 2 min/ \u2211 j \u03c3 2 j .\nProof. We denote by E\u03b4 the joint event on which Proposition 3 and Proposition 4 hold at the same time with an overall probability 1\u2212 \u03b4. This immediately gives upper and lower confidence bounds on the score si,t used in TRACE-UCB as(\n1\u2212 CTr\n\u221a d\nki,t )2 \u03c32i ki,t \u2264 si,t d \u2264 ( 1 + 2CTr \u221a d ki,t )2 \u03c32i + 2\u2206i,t ki,t .\nRecalling the definition of \u2206i,t we can rewrite the last term as\n\u03c32i + 2\u2206i,t ki,t =\n( 1 + 16R log(2mn/\u03b4)\n\u03c32i \u221a ki,t \u2212 d ) \u03c32i ki,t = ( 1 +\nC\u2206 \u03c32i \u221a ki,t \u2212 d ) \u03c32i ki,t ,\nwhere C\u2206 = 16R log(2mn/\u03b4). We consider a step t + 1 \u2264 n at which It+1 = q. By algorithmic construction we have that sp,t \u2264 sq,t for every arm p \u2208 [m]. Using the inequalities above we obtain(\n1\u2212 CTr\n\u221a d\nkp,t )2 \u03c32p kp,t \u2264 sp,t d \u2264 sq,t d \u2264 ( 1 + 2CTr \u221a d kq,t )2 \u03c32q + 2\u2206q,t kq,t\nIf t + 1 is the last time step at which arm q is pulled, then kq,t = kq,t+1 \u2212 1 = kq,n \u2212 1 and kp,n \u2265 kp,t. Then we can rewrite the previous inequality as(\n1\u2212 CTr\n\u221a d\nkp,n )2 \u03c32p kp,n =: Ap,n \u2264 Bq,n := ( 1 + 2CTr \u221a d kq,n \u2212 1 )2( 1 +\nC\u2206 \u03c32q \u221a kq,n \u2212 d\u2212 1\n) \u03c32q\nkq,n \u2212 1 . (55)\nIf every arm is pulled exactly the optimal number of times, then for any i \u2208 [m], ki,n = k\u2217i,n and the statement of the theorem trivially holds. Otherwise, there exists at least one arm that is pulled more than k\u2217i,n. Let q be this arm, then kq,n > k \u2217 q,n. We recall that L \u2217 n = d\u03c3 2 q/(k \u2217 q,n \u2212 d\u2212 1) and we rewrite the RHS of Eq. 55 as\nBq,n \u2264 ( 1 + 2CTr \u221a d\nk\u2217q,n \u2212 d\u2212 1\n)2( 1 +\nC\u2206 \u03c32q \u221a k\u2217q,n \u2212 d\u2212 1\n) \u03c32q\nk\u2217q,n \u2212 d\u2212 1\n\u2264 ( 1 + 2CTr \u221a L\u2217n \u03c32q )2( 1 + C\u2206 \u221a L\u2217n d\u03c36q ) L\u2217n d .\nWe also simplify the LHS of Eq. 55 as\nAp,n = ( 1\u2212 2CTr \u221a d\nkp,n + C2Tr\nd\nkp,n ) \u03c32p kp,n \u2265 ( 1\u2212 2CTr \u221a d kp,n ) \u03c32p kp,n .\nAt this point we can solve Eq. 55 for kp,n and obtain a lower bound on it. We study the inequality 1/Ap,n \u2265 1/Bp,n.\nWe first notice that\n1 Ap,n \u2264 kp,n \u03c32p\n( 1 + 4CTr \u221a d\nkp,n ) \u2264 1 \u03c32p (\u221a kp,n + 2CTr \u221a d )2 ,\nwhere we used 1/(1\u2212 x) \u2264 1 + 2x for x \u2264 1/2 and we added a suitable positive term to obtain the final quadratic form. Similarly we have\n1\nBq,n \u2265\n( 1\u2212 2CTr \u221a L\u2217n \u03c32q )2( 1\u2212 C\u2206 \u221a L\u2217n d\u03c36q ) d L\u2217n = ( 1\u2212 2CTr \u221a L\u2217n \u03c32q )2( d L\u2217n \u2212 C\u2206 \u221a d L\u2217n\u03c3 6 q ) ,\nwhere we used 1/(1 + x) \u2265 1 \u2212 x for any x \u2265 0. In order to ease the derivation of an explicit lower-bound on kp,n, we further simplify the previous expression by replacing higher order terms with a big-\u2126 notation. We first recall that L\u2217n = \u0398\u0303(md\u03c3 2/n), then the terms of order (1/L\u2217n) and (1/ \u221a L\u2217n) clearly dominate the expression, while all other terms are asymptotically constant or decreasing in n and thus we can rewrite the previous bound as\n1 Bq,n \u2265 d L\u2217n \u2212 (C\u2206 + 4CTr\n\u221a d)\n\u221a d\nL\u2217n\u03c3 6 q\n\u2212 \u2126(1).\nBy setting C = C\u2206 + 4CTr \u221a d we can finally use the upper bound on 1/Ap,n and the lower bound on 1/Bq,n to obtain\n1\n\u03c32p\n(\u221a kp,n + 2CTr \u221a d )2 \u2265 d L\u2217n \u2212 C\n\u221a d\nL\u2217n\u03c3 6 q\n\u2212 \u2126(1).\nWe proceed with solving the previous inequality for kp,n and obtain\nkp,n \u2265 \u03c32p ( d L\u2217n \u2212 C \u221a d L\u2217n\u03c3 6 q \u2212 \u2126(1) )1/2 \u2212 2CTr \u221a d 2 . Taking the square on RHS and adding and subtracting d+ 1 we have\nkp,n \u2265 d+ 1 + \u03c32p  d L\u2217n \u2212 C \u221a d L\u2217n\u03c3 6 q \u2212 4CTr \u221a d ( d L\u2217n \u2212 C \u221a d L\u2217n\u03c3 6 q \u2212 \u2126(1) )1/2 + 4C2Trd \u2212 d\u2212 1\u2212 \u2126(1). We clearly notice that the first three terms in the RHS are dominant (they are higher order function of n through L\u2217n) and thus we can isolate them and replace all other terms by their asymptotic lower bound as\nkp,n \u2265 d+ 1 + d\u03c32p L\u2217n \u2212\n\u221a 1\nL\u2217n\n( C \u221a d\u03c34p \u03c36q + 4CTrd ) \u2212 \u2126(n1/4),\nwhere we used the fact that L\u2217n = \u0398\u0303(md\u03c3 2/n) to bound the higher order terms. Furthermore, we recall that k\u2217p,n = d\u03c32p/L \u2217 n + d+ 1 and thus we can finally write the previous bound as\nkp,n \u2265 k\u2217p,n \u2212\n\u221a 1\nL\u2217n\n( C \u221a d\u03c34p \u03c36q + 4CTrd ) \u2212 \u2126(n1/4).\nThe final bound is obtained by using \u03c32p/ \u2211 j \u03c3 2 j = \u03bbp \u2265 \u03bbmin and \u03c32q \u2265 \u03c32min with the final expression\nkp,n \u2265 k\u2217p,n \u2212 \u221a n ( C \u03c32min \u221a 1 \u03bbmin + 4CTr \u221a d ) \u2212 \u2126(n1/4).\nA quite loose bound based on the definition of C for the previous expression gives the final more readable result\nkp,n \u2265 k\u2217p,n \u2212 C\u2206 + 8CTr\n\u03c32min\n\u221a nd\n\u03bbmin \u2212 \u2126(n1/4)."}, {"heading": "D.2. Regret Bound (Proof of Theorem 6)", "text": "Theorem. The regret of the Trace-UCB algorithm, i.e., the difference between its loss and the loss of optimal static allocation (see Eq. 8), is upper-bounded by\nLn(A)\u2212 L\u2217n \u2264 O ( 1\n\u03c32min ( d \u03bbminn )3/2) , (56)\nwhere \u03bbmin = \u03c32min/ \u2211 j \u03c3 2 j .\nProof. We first simplify the expression of the loss for TRACE-UCB in Lemma 19. We invert trace operator and expectation and have\nLi,n(\u03b2\u0302 \u03bb i ) = E ( Tr [ \u03a3Wi,n ( \u03c32iX T i,nXi,n + \u03bb 2\u03b2i\u03b2 T i ) WTi,n ]) .\nWe notice that Wi,n = (XTi,nXi,n + \u03bbI) \u22121 (XTi,nXi,n)\u22121, where is the Lower ordering between positive-definite matrices. We focus on the two additive terms in the trace separately. We have\nTr ( \u03a3Wi,nX T i,nXi,nW T i,n ) = Tr ( Wi,nX T i,nXi,nW T i,n\u03a3 ) \u2264 Tr ( (XTi,nXi,n) \u22121XTi,nXi,nW T i,n\u03a3 ) = Tr ( \u03a3WTi,n ) (57)\n\u2264 Tr ( \u03a3(XTi,nXi,n) \u22121) = 1 ki,n Tr ( \u03a3\u03a3\u0302\u22121i,n ) ,\nwhere we used the fact that Tr(AB) = Tr(BA), Tr(AB) \u2264 Tr(CB) if A C and the definition of \u03a3\u0302i,n.\nSimilarly, we have Tr ( \u03a3Wi,n\u03b2i\u03b2 T i W T i,n ) = \u2016\u03b2i\u20162Tr ( \u03a3Wi,nW T i,n ) \u2264 \u2016\u03b2i\u20162Tr ( (XTi,nXi,n) \u22121\u03a3Wi,n ) \u2264 \u2016\u03b2i\u20162 \u03bbmax(\u03a3\u0302 \u22121 i,n) ki,n Tr ( \u03a3Wi,n\n) \u2264 \u2016\u03b2i\u20162 \u03bbmax(\u03a3\u0302 \u22121 i,n) ki,n Tr ( \u03a3(XTi,nXi,n) \u22121) = \u2016\u03b2i\u20162\u03bbmax(\u03a3\u0302\u22121i,n) k2i,n Tr ( \u03a3\u03a3\u0302\u22121i,n ) .\nGoing back to the loss expression we have\nLi,n(\u03b2\u0302 \u03bb i ) \u2264 E\n[ Tr ( \u03a3\u03a3\u0302\u22121i,n ) ki,n ( \u03c32i + \u2016\u03b2i\u20162 \u03bbmax(\u03a3\u0302 \u22121 i,n) ki,n )] .\nWe decompose the loss in two terms depending on the high-probability event E\u03b4 under which the concentration inequalities Proposition 3 and Proposition 4 hold at the same time\nLi,n(\u03b2\u0302 \u03bb i ) \u2264 E\n[ Tr ( \u03a3\u03a3\u0302\u22121i,n ) ki,n ( \u03c32i + \u2016\u03b2i\u20162 \u03bbmax(\u03a3\u0302 \u22121 i,n) ki,n )\u2223\u2223\u2223E\u03b4]+ \u03b4E (Tr [\u03a3Wi,n (\u03c32iXTi,nXi,n + \u03bb2\u03b2i\u03b2Ti )WTi,n] \u2223\u2223Ec\u03b4) , where we used P(Ec\u03b4 \u2264 \u03b4). If we denote the second expectation in the previous expression by Lci,n(\u03b2\u0302\u03bbi ), then we can use Eq. 57 and obtain\nLci,n(\u03b2\u0302 \u03bb i ) \u2264 \u03c32i E ( Tr ( \u03a3WTi,n )\u2223\u2223Ec\u03b4)+ \u2016\u03b2i\u2016\u03bb2E (Tr(\u03a3Wi,nWTi,n)\u2223\u2223Ec\u03b4) Using the fact that Tr(AB) \u2264 \u03bbmax(A)Tr(B), we can upper bound the previous equation as\nLci,n(\u03b2\u0302 \u03bb i ) \u2264 \u03c32iTr(\u03a3)E ( \u03bbmax(Wi,n) \u2223\u2223Ec\u03b4)+ \u2016\u03b2i\u2016Tr(\u03a3)\u03bb2E (\u03bbmax(Wi,n)2\u2223\u2223Ec\u03b4) Recalling that thanks to the regularization \u03bbmax(Wi,n) \u2264 1/\u03bb, we finally obtain\nLci,n(\u03b2\u0302 \u03bb i ) \u2264 Tr(\u03a3) (\u03c32i \u03bb + \u2016\u03b2i\u2016 ) . (58)\nThe analysis of the high-probability part of the bound relies on the concentration inequalities for the trace and \u03bbmax and the lower bound on the number of samples ki,n from Thm. 5. We recall the three main inequalities we are going to use to bound the loss\nki,n \u2265 k\u2217i,n \u2212 C \u221a nd\u2212 \u2126(n1/4),\nTr(\u03a3\u03a3\u0302\u22121i,n) \u2264 d ( 1 + 2(1 + \u03b1) \u221a d\nn\n)2 ,\n\u03bbmax(\u03a3\u0302 \u22121 i,n) \u2264\n1\n\u03bbmin(\u03a3)\n( 1 + 2(1 + \u03b1) \u221a d\nn\n)2 ,\nwhere C = C\u2206+8CTr \u03c32min \u221a \u03bbmin and the last inequality is obtained by multiplying by \u03a3\u22121\u03a3 to whiten \u03a3\u0302i,n and using Proposition 12, and \u03bbmax(AB) \u2264 \u03bbmax(A)\u03bbmax(B) and finally \u03bbmax(\u03a3\u22121) = 1/\u03bbmin(\u03a3). We can invert the first inequality as\n1 ki,n \u2264 1 k\u2217i,n \u2212 C \u221a nd\u2212 \u2126(n1/4) \u2264 1 k\u2217i,n +O\n( 2C\nk\u2217i,n\n\u221a d\nn ) \u2264 1 k\u2217i,n +O ( \u221a d \u03c32min(\u03bbminn) 3/2 ) , (59)\nwhere the last inequality is obtained by recalling that k\u2217i,n = \u0398(\u03bbin) and using the definition of C (where we ignore C\u2206 and CTr). We can then rewrite the high-probability loss as\nE\n[ Tr ( \u03a3\u03a3\u0302\u22121i,n ) ki,n ( \u03c32i + \u2016\u03b2i\u20162 \u03bbmax(\u03a3\u0302 \u22121 i,n) ki,n )\u2223\u2223\u2223E\u03b4] \u2264 d\u03c32i k\u2217i,n +O ( 1 \u03c32min ( d \u03bbminn )3/2) \u2264 L\u2217n +O ( 1 \u03c32min ( d \u03bbminn )3/2) .\nBy recalling the regret Rn = maxi Li,n(\u03b2\u03bbi,n)\u2212 L\u2217n, bringing the bounds above together and setting \u03b4 = O(n\u22123/2\u2212 ) for any > 0 and a suitable multiplicative constant, we obtain the final regret bound\nRn \u2264 O ( 1\n\u03c32min ( d \u03bbminn )3/2) ."}, {"heading": "D.3. High Probability Bound for Trace-UCB Loss (Proof of Theorem 7)", "text": "In this section, we start by defining a new loss function for algorithm A:\nL\u0303n(A) = max i\u2208[m] \u2016\u03b2i \u2212 \u03b2\u0302i,n\u20162\u03a3. (60)\nNote that L\u0303n(A) is a random variable as \u03b2\u0302i,n is random, and the expectation is only taken with respect to the test point X \u223c F (leading to the \u03a3-norm). We expect results of the following flavor: let \u03b4 > 0, then with probability at least 1\u2212 \u03b4,\nL\u0303n(A)\u2212 L\u0303\u2217n \u2264 O\u0303  \u2211\nj\n\u03c32j d\nn\n3/2  , (61)\nwhen A corresponds to TRACE-UCB, and L\u0303\u2217n to the optimal static allocation under ordinary least squares.\nWe start by focusing on L\u0303n(A), and proving Theorem 7:\nTheorem. Let \u03b4 > 0, and assume \u2016\u03b2i\u20162 \u2264 Z for all i, for some Z > 0. With probability at least 1\u2212 \u03b4,\nL\u0303n(A) \u2264 \u2211m j=1 \u03c3 2 j\nn\n( d+ 2 log 3m\n\u03b4\n) +O ( 1\n\u03c32min\n( d\nn\u03bbmin\n)3/2) , (62)\nwhere \u03bbmin = \u03c32min/ \u2211 j \u03c3 2 j .\nProof. We define a set of events that help us control the loss, and then we show that these events simultaneously hold with high probability. In particular, we need the following events:\n1. EG \u2261 the good event holds (for all arms i, and all times t), which includes a confidence interval for \u03c3\u03022i,t and the trace of the empirical covariance matrix.\nHolds with probability 1\u2212 \u03b4G. This event is described and controlled in Proposition 3 and Proposition 4.\n2. EM,i \u2261 the confidence intervals Ci,t created for arm i at time t contain the true \u03b2i at all times t \u2014based on the vector-valued martingale in (Abbasi-Yadkori et al., 2011).\nHolds with probability 1\u2212 \u03b4M,i. This event is described and controlled in Theorem 13.\n3. EC,i,t \u2261 the empirical covariance \u03a3\u0302i,t for arm i at time t is close to \u03a3. This event is a direct consequence of event EG.\n4. EB,i,t \u2261 the first t observations pulled at arm i have norm reasonably bounded. The empirical average norm is not too far from its mean. Holds with probability 1\u2212 \u03b4B,i,t. This event is described and controlled in Corollary 15.\nLet H be the set of all the previous events. Then, by the union bound\nP (\u2229 \u2208H ) \u2265 1\u2212 \u2211 \u2208H \u03b4 . (63)\nOur goal is to show that if \u2229 \u2208H holds, then the loss L\u0303n(A) = maxi\u2208[m] \u2016\u03b2i \u2212 \u03b2\u0302i,n\u20162\u03a3 is upper bounded by a quantity that resembles the expected loss of the algorithm that knows the \u03c32i \u2019s in advance.\nFix \u03b4 > 0. We want \u03b4 = \u2211 \u2208H \u03b4 , and we would like to assign equal weight to all the sets of events. First, \u03b4G = \u03b4/3.\nAlso, \u2211 i \u03b4M,i = \u03b4/3, implying \u03b4M,i = \u03b4/3m for every arm i \u2208 [m]. Finally, to bound observation norms, we set\u2211\ni \u2211 t \u03b4B,i,t = \u03b4/3. It follows that we can take \u03b4B,i,t = \u03b4/3mT , even though t really ranges from d to n.\nAssume that EG, EM,i and EB,i,t hold for all arms i and times t. Then, by Theorem 5, the final number of pulls for arm i can be lower bounded by\nki \u2265 \u03c32i\u2211 j \u03c3 2 j n\u2212 c (\u221a \u03c32i \u03c32min + 1 )\u221a \u03c32i\u2211 j \u03c3 2 j dn+ o (\u221a dn ) , (64)\nwhere c = 2 ( 1 + \u221a 2 log(12mn/\u03b4)/d ) .\nFor notational simplicity, we denote by \u03b2\u0302i,t the estimate after t pulls. Thus, with respect to our previous notation where \u03b2\u0302i,n referred to our final estimate, we have that \u03b2\u0302i,ki,n = \u03b2\u0302i,n as ki,n is the total number of pulls for arm i.\nIf the EM,i events hold, then we know that our \u03b2\u0302i,t estimates are not very far from the true values \u03b2i when t is large. In particular, we know that the error is controlled by the radius Ri,t of the confidence ellipsoids. We expect these radiuses to decrease with the number of observations per arm, t. As we have a lower bound on the total number of pulls for arm i, ki,n, if the confidence ellipsoids apply, then we can directly obtain an upper bound on the radius Ri,t at the end of the process.\nWe need to do a bit of work to properly bound \u2016\u03b2\u0302i,ki,n \u2212 \u03b2i\u20162\u03a3.\nFix arm i, and assume EM,i holds. In addition, assume \u2016\u03b2i\u20162 \u2264 Z for all i. Let V\u0304i,t = \u03bbI +XTi,tXi,t, where Xi,t contains the first t observations pulled by arm i. We modify the proof of Theorem 2 in (Abbasi-Yadkori et al., 2011) by taking x = (V\u0302t/t)(\u03b2\u0302t \u2212 \u03b2\u2217) in their equation 5 (we are using their notation in the latter expression). Assume the algorithm pulls arm i a total of t times \u2014ki,n is a stopping time with respect to the \u03c3-algebra that includes the environment (other arms)\u2014 then, by Theorem 13\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u2016V\u0304i,t/t \u2264 \u03c3i\u221a t \u221a\u221a\u221a\u221a2 log(det (V\u0304i,t)1/2 det (\u03bbI)\u22121/2 \u03b4M,i ) + \u221a \u03bb t Z. (65)\nWe would like to upper bound \u2016\u03b2\u0302i,ki,n \u2212 \u03b2i\u2016\u03a3 by means of \u2016\u03b2\u0302i,ki,n \u2212 \u03b2i\u2016V\u0304i,ki,n/ki,n . Note that when t grows, V\u0304i,t/t\u2192 \u03a3 as the regularization is washed out. The distance between \u03a3\u0302i,t = V\u0304i,t/t\u2212 (\u03bb/t)I and \u03a3 is captured by event C,i,t.\nFormally, as EG holds, we know that the difference between \u03a3 and \u03a3\u0302i,t is bounded in operator norm for any i and t by\n\u2016\u03a3\u2212 \u03a3\u0302i,t\u2016 \u2264 2 ( 1 + \u221a 2\nd log\n2\n\u03b4G\n)\u221a d\nt \u2016\u03a3\u2016 = c\n\u221a d\nt \u03bbmax(\u03a3). (66)\nThen, as a consequence, for all x \u2208 Rs\nxT (\u03a3\u2212 \u03a3\u0302i,t)x \u2264 c \u03bbmax(\u03a3) \u221a d\nt \u2016x\u201622. (67)\nIn particular, by taking x = \u03b2\u0302i,t \u2212 \u03b2i,\nc \u03bbmax(\u03a3)\n\u221a d\nt \u2016\u03b2\u0302i,t \u2212 \u03b2i\u201622 \u2265 (\u03b2\u0302i,t \u2212 \u03b2i)T (\u03a3\u2212 \u03a3\u0302i,t)(\u03b2\u0302i,t \u2212 \u03b2i) (68)\n= \u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 \u2212 \u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3\u0302i,t . (69)\nIn addition, note that \u2016x\u20162 \u03a3\u0302i,t = \u2016x\u20162 V\u0304i,t/t \u2212 (\u03bb/t)\u2016x\u201622. We conclude that\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 \u2264 \u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3\u0302i,t + c \u03bbmax(\u03a3) \u221a d t \u2016\u03b2\u0302i,t \u2212 \u03b2i\u201622 (70)\n= \u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162V\u0304i,t/t +\n( c \u03bbmax(\u03a3) \u221a d\nt \u2212 \u03bb t\n) \u2016\u03b2\u0302i,t \u2212 \u03b2i\u201622. (71)\nOn the other hand, we know that \u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 \u2265 \u03bbmin(\u03a3)\u2016\u03b2\u0302i,t \u2212 \u03b2i\u201622.\nTherefore, by (65)\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 \u2264 1\n1\u2212 1\u03bbmin(\u03a3)\n( c \u03bbmax(\u03a3) \u221a d t \u2212 \u03bb t )\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162V\u0304i,t/t (72)\n\u2264 1 1\u2212 \u03b3t  \u03c3i\u221a t \u221a\u221a\u221a\u221a2 log(det (V\u0304i,t)1/2 det (\u03bbI)\u22121/2 \u03b4M,i ) + \u221a \u03bbZ\u221a t 2 (73) \u2264 1\n1\u2212 \u03b3t 1 t\n\u03c3i \u221a\u221a\u221a\u221a2(1\n2 log\n( det ( V\u0304i,t )\ndet (\u03bbI)\n) + log ( 1\n\u03b4M,i\n)) + \u221a \u03bbZ 2 (74) \u2264 1\n1\u2212 \u03b3t 1 t\n\u03c3i \u221a\u221a\u221a\u221a\u221a2 1 2 t\u2211 j=1 \u2016Xj\u20162V\u0304 \u22121i,t + log ( 1 \u03b4M,i )+\u221a\u03bbZ  2 , (75)\nwhere we defined \u03b3t = ( c \u03bbmax(\u03a3) \u221a d t \u2212 \u03bb t ) /\u03bbmin(\u03a3), and we used Lemma 11 in (Abbasi-Yadkori et al., 2011) which\nshows that\nlog\n( det ( V\u0304i,t )\ndet (\u03bbI)\n) \u2264\nt\u2211 j=1 \u2016Xj\u20162V\u0304 \u22121i,t . (76)\nWe would like to approximate the V\u0304 \u22121i,t norm, by means of the inverse covariance norm, \u03a3 \u22121. The whitened equation that is equivalent to (67) \u2014 see Lemma 12 \u2014 is given by \u2016I \u2212 \u02c6\u0304\u03a3i,t\u2016 \u2264 , with = c \u221a d/t.\nIt implies that for any j = 1, . . . , d, 1\u2212 c \u221a d\nt \u2212O\n( d\nt\n) \u2264 \u03bbj( \u02c6\u0304\u03a3i,t) \u2264 1 + c \u221a d\nt +O\n( d\nt\n) . (77)\nThe V\u0304 \u22121i,t norm can be bounded as follows\n\u2016x\u20162 V\u0304 \u22121i,t\n= xT V\u0304 \u22121i,t x = x T ( \u03bbI + XTi,tXi,t )\u22121 x (78)\n= xT\u03a3\u22121/2\u03a31/2 ( \u03bbI + XTi,tXi,t )\u22121 \u03a31/2\u03a3\u22121/2x (79)\n= x\u0304T ( \u03bb\u03a3\u22121 + X\u0304Ti,tX\u0304i,t )\u22121 x\u0304 (80)\n= 1 t x\u0304T ( \u03bb t \u03a3\u22121 + \u02c6\u0304\u03a3\u22121i,t )\u22121 x\u0304, (81)\nwhere x\u0304 denotes the whitened version of x. We can now apply the matrix inversion lemma to see that\n\u2016x\u20162 V\u0304 \u22121i,t\n= 1 t x\u0304T ( \u03bb t \u03a3\u22121 + \u02c6\u0304\u03a3\u22121i,t )\u22121 x\u0304 (82)\n= 1\nt x\u0304T\n( \u02c6\u0304\u03a3i,t \u2212 \u02c6\u0304\u03a3i,t\u03a3\u22121/2 ( t\n\u03bb I + \u03a3\u22121/2 \u02c6\u0304\u03a3i,t\u03a3\n\u22121/2 )\u22121\n\u03a3\u22121/2 \u02c6\u0304\u03a3i,t ) x\u0304 (83)\n= 1 t x\u0304T ( \u02c6\u0304\u03a3i,t \u2212 \u02c6\u0304\u03a3i,t\u03a3\u22121/2R\u22121\u03a3\u22121/2 \u02c6\u0304\u03a3i,t ) x\u0304, (84)\nwhere we implicitly defined R = (t/\u03bb)I + \u03a3\u22121/2 \u02c6\u0304\u03a3i,t\u03a3\u22121/2, a positive definite matrix. We upper bound the previous expression to conclude that\n\u2016x\u20162 V\u0304 \u22121i,t\n= 1 t x\u0304T ( \u02c6\u0304\u03a3i,t \u2212 \u02c6\u0304\u03a3i,t\u03a3\u22121/2R\u22121\u03a3\u22121/2 \u02c6\u0304\u03a3i,t ) x\u0304 (85)\n\u2264 1 t x\u0304T \u02c6\u0304\u03a3i,tx\u0304 \u2264\n\u03bbmax( \u02c6\u0304\u03a3i,t)\nt \u2016x\u0304\u201622 \u2264\n1 + c \u221a d/t+O (d/t)\nt \u2016x\u0304\u201622. (86)\nIf we now go back to (76), using the previous results, we see that\nt\u2211 j=1 \u2016Xj\u20162V\u0304 \u22121i,t \u2264\n( 1 + c \u221a d\nt +O\n( d\nt ))1 t t\u2211 j=1 \u2016X\u0304j\u201622  . (87) Substituting the upper bound in (75):\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 \u2264 1 1\u2212 \u03b3t 1 t\n\u03c3i \u221a\u221a\u221a\u221a\u221a2 1 2 t\u2211 j=1 \u2016Xj\u20162V\u0304 \u22121i,t + log ( 1 \u03b4M,i )+\u221a\u03bbZ  2\n(88)\n\u2264 1 1\u2212 \u03b3t 1 t\n\u03c3i \u221a\u221a\u221a\u221a\u221a(1 + c\u221ad\nt +O\n( d\nt ))1 t t\u2211 j=1 \u2016X\u0304j\u201622 + 2 log 1 \u03b4M,i + \u221a \u03bbZ  2 .\nBy Corollary 15, with probability 1 \u2212 \u03b4B,i,t, the empirical average norm of the white gaussian observations is controlled by\n1\nt t\u2211 j=1 \u2016X\u0304j\u20162 \u2264 d+ 8 log ( 1 \u03b4B,i,t )\u221a d t . (89)\nAs \u03b4B,i,t = \u03b4/3mn and \u03b4M,i = \u03b4/3m, we conclude that\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 \u2264 1 1\u2212 \u03b3t 1 t\n\u03c3i \u221a\u221a\u221a\u221a(1 + c\u221ad\nt +O\n( d\nt\n))( d+ 8 log ( 3mn\n\u03b4\n)\u221a d\nt\n) + 2 log ( 3m\n\u03b4\n) + \u221a \u03bbZ 2\n\u2264 1 1\u2212 ( c\u03bbmax(\u03a3) \u221a d t \u2212 \u03bb t ) /\u03bbmin(\u03a3) 1 t\n\u03c3i \u221a\u221a\u221a\u221a(d+ (c+ 8 log 3mn\n\u03b4\n)\u221a d\nt +O\n( d\nt\n)) + 2 log 3m\n\u03b4 + \u221a \u03bbZ 2 . (90)\nAt this point, recall that under our events\nki,n \u2265 k\u2217i,n \u2212 C \u221a nd\u2212 \u2126(n1/4), (91)\nwhere C = C\u2206+8CTr \u03c32min \u221a \u03bbmin . As (90) decreases in t, we will bound the error \u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 by taking the number of pulls t = (\u03c32i / \u2211 j \u03c3 2 j )n+O( \u221a dn) (in particular, the RHS of (91)).\nIf we take \u03bb = 1/n, we have that\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 (92)\n\u2264 1 1\u2212 ( c\u03bbmax(\u03a3) \u221a d t \u2212 \u03bb t ) /\u03bbmin(\u03a3) 1 t\n\u03c3i \u221a\u221a\u221a\u221a(d+ (c+ 8 log 3mn\n\u03b4\n)\u221a d\nt +O\n( d\nt\n)) + 2 log 3m\n\u03b4 + \u221a \u03bbZ\n2\n\u2264 ( 1 + c \u03bbmax(\u03a3)\n\u03bbmin(\u03a3)\n\u221a d\nt +O\n( d\nt\n)) 1\nt\n\u03c3i \u221a\u221a\u221a\u221a(d+ (c+ 8 log 3mn\n\u03b4\n)\u221a d\nt +O\n( d\nt\n)) + 2 log 3m\n\u03b4 + \u221a \u03bbZ\n2\n\u2264 ( 1 +O (\u221a d\nt\n)) 1\nt\n\u03c32i ( d+ 2 log 3m\n\u03b4 +\n( c+ 8 log 3mn\n\u03b4\n)\u221a d\nt\n) + Z2\nn + 2Z\u03c3i\n\u221a d+ 2 log 3m\u03b4\nn + o\n(\u221a d\nn ) . Now, by (91) and (59), and using the \u03bbi = \u03c32i / \u2211 j \u03c3 2 j notation\n\u2016\u03b2\u0302i,t \u2212 \u03b2i\u20162\u03a3 (93)\n\u2264 ( 1 +O (\u221a d\nn )) [\u03c32i (d+ 2 log 3m\u03b4 )+ \u03c32i (c+ 8 log 3mn\u03b4 )\u221adt + 2Z\u03c3i\u221a dn + o(\u221a dn)] k\u2217i,n \u2212 C \u221a nd\u2212 \u2126(n1/4)\n= ( 1 +O (\u221a d\nn )) [\u03c32i (d+ 2 log 3m\u03b4 )+ (\u03c32i (c+ 8 log 3mn\u03b4 )+ 2Z\u03c3i)\u221adt + o(\u221a dn)] k\u2217i,n \u2212 C \u221a nd\u2212 \u2126(n1/4)\n= ( 1 +O (\u221a d\nn\n))( 1\nk\u2217i,n +O\n( \u221a d\n\u03c32min(\u03bbminn) 3/2\n))[ \u03c32i ( d+ 2 log 3m\n\u03b4\n) + O\u0303 (\u221a d\nn\n)]\n= \u03c32i k\u2217i,n\n( d+ 2 log 3m\n\u03b4\n) +O ( 1\n\u03c32min ( d \u03bbminn )3/2) . (94)"}, {"heading": "E. Loss of a RLS-based Learning Algorithm", "text": ""}, {"heading": "E.1. Distribution of RLS estimates", "text": "Proposition 16. Given a linear regression problem with observations Y = XT\u03b2 + with Gaussian noise with variance \u03c32, after n contexts X and the corresponding observations Y, the ridge estimate of parameter \u03bb is obtained as\n\u03b2\u0302\u03bb = (XTX + \u03bbI)\u22121XTY = WXTY,\nwith W = (XTX + \u03bbI)\u22121, and its distribution conditioned on X is \u03b2\u0302\u03bb | X \u223c N ( \u03b2 \u2212 \u03bbW\u03b2, \u03c32 W(XTX)WT ) . (95)\nProof. Recalling the definition of the OLS estimator \u03b2\u0302 (assuming it exists), we can easily rewrite the RLS estimator as\n\u03b2\u0302\u03bb = (XTX + \u03bbI)\u22121(XTX)(XTX)\u22121XTY = (XTX + \u03bbI)\u22121(XTX)\u03b2\u0302.\nThis immediately gives that the conditional distribution of \u03b2\u0302\u03bb is Gaussian as for \u03b2\u0302. We just need to compute the corresponding mean vector and the covariance matrix. We first notice that the RLS estimator is biased as\nE[\u03b2\u0302\u03bb \u2223\u2223X] = (XTX + \u03bbI)\u22121(XTX)\u03b2.\nLet S = XTX, then we can further rewrite the bias as E[\u03b2\u0302\u03bb \u2223\u2223X] = (S + \u03bbSS\u22121)\u22121S\u03b2 = (S(I + \u03bbS\u22121))\u22121S\u03b2\n= (I + \u03bbS\u22121)\u22121\u03b2 = ( I \u2212 \u03bb(S + \u03bbI)\u22121 ) \u03b2\n= \u03b2 \u2212 \u03bb(S + \u03bbI)\u22121\u03b2 = \u03b2 \u2212 \u03bbW\u03b2,\nwhere we used the matrix inversion lemma. Recalling that the covariance of \u03b2\u0302 is \u03c32(XTX)\u22121, the covariance of \u03b2\u0302\u03bb is then Cov [ \u03b2\u0302\u03bb|X ] = W(XTX)Cov [ \u03b2\u0302|X ] (XTX)WT = \u03c32W(XTX)WT."}, {"heading": "E.2. Loss Function of a RLS-based Algorithm", "text": "We start by proving the loss function in the case of a static algorithm. Lemma 17. Let A be a learning algorithm that selects instance i for ki,n times, where ki,n is a fixed quantity chosen in advance, and that returns estimates \u03b2\u0302\u03bbi obtained by RLS with regularization \u03bb. Then its loss after n steps can be expressed as\nLn(Astat) = max i\u2208[m]\nTr ( \u03a3E [ Wi,n ( \u03c32iX T i,nXi,n + \u03bb 2\u03b2i\u03b2 T i ) WTi,n ]) , (96)\nwhere Wi,n = (XTi,nXi,n + \u03bbI) \u22121, and Xni is the matrix with the ki,n contexts from instance i.\nProof. The proof follows the same steps as in App. A up to Eq. 22, where we have\nLn(Astat) = max i\u2208[m] Tr\n( EXi [ E i [ \u03a3(\u03b2i \u2212 \u03b2\u0302i)(\u03b2i \u2212 \u03b2\u0302i)T \u2223\u2223Xi]]). Following Proposition 16, we can refine the inner expectation as\nE [ (\u03b2\u0302 \u2212 \u03b2)(\u03b2\u0302 \u2212 \u03b2)T | X ] = E [ (\u03b2\u0302 \u2212 \u03b2 + \u03bbW\u03b2 \u2212 \u03bbW\u03b2)(\u03b2\u0302 \u2212 \u03b2 + \u03bbW\u03b2 \u2212 \u03bbW\u03b2)T | X\n] = E [ (\u03b2\u0302 \u2212 E[\u03b2\u0302 | X]\u2212 \u03bbW\u03b2)(\u03b2\u0302 \u2212 E[\u03b2\u0302 | X]\u2212 \u03bbW\u03b2)T | X\n] = E [ (\u03b2\u0302 \u2212 E[\u03b2\u0302 | X])(\u03b2\u0302 \u2212 E[\u03b2\u0302 | X])T | X ] + \u03bb2W\u03b2\u03b2TWT\n= \u03c32 W(XTX)WT + \u03bb2W\u03b2\u03b2TWT = W [ \u03c32XTX + \u03bb2\u03b2\u03b2T ] WT.\nPlugging the final expression back into Ln(Astatic) we obtain the desired expression.\nWe notice that a result similar to Lemma 9 holds for RLS estimates as well.\nProposition 18. Assume the noise is Gaussian. Let \u03c3\u03022 be the estimate for \u03c32 computed by using the residuals of the OLS solution \u03b2\u0302. Then, \u03b2\u0302\u03bb and \u03c3\u03022 are independent random variable conditionally to X.\nProof. As shown in the proof of Proposition 16, we have \u03b2\u0302\u03bb = (XTX + \u03bbI)\u22121(XTX)\u03b2\u0302 and we know that functions of independent random variables are themselves independent. Since the matrix mapping \u03b2\u0302 to \u03b2\u0302\u03bb is fixed given X, and \u03b2\u0302 and \u03c3\u03022 are conditionally independent from Lemma 9, then the statement follows.\nWe can now combine Proposition 18 and Lemma 17 to conclude that a similar expression to Eq. 97 holds for the ridge estimators also when a non-static algorithm such as TRACE-UCB is run.\nLemma 19. Let A be a learning algorithm such that It is chosen as a function of Dt\u22121 = {X1, I1, YI1,1, . . . , Xt\u22121, It\u22121, YIt\u22121,t\u22121}, and that it returns estimates \u03b2\u0302\u03bbi obtained by RLS with regularization \u03bb. Then its loss after n steps can be expressed as\nLn(A) = max i\u2208[m]\nTr ( \u03a3E [ Wi,n ( \u03c32iX T i,nXi,n + \u03bb 2\u03b2i\u03b2 T i ) WTi,n ]) , (97)\nwhere Wi,n = (XTi,nXi,n + \u03bbI) \u22121, and Xi,n is the matrix with the ki,n contexts from instance i.\nProof. The proof follows immediately by extending Lemma 9 to \u03b2\u0302\u03bb as, by Proposition 18, \u03b2\u0302\u03bb and \u03c3\u03022OLS are independent. Then, we proceed in a way similar to that in the proof of Lemma 2 to perform the required conditioning."}, {"heading": "F. Sparse Trace-UCB Algorithm", "text": ""}, {"heading": "F.1. Summary", "text": "High-dimensional linear regression models are remarkably common in practice. Companies tend to record a large number of features of their customers, and feed them to their prediction models. There are also cases in which the number of problem instances under consideration m is large, e.g., too many courses in the MOOC example described in the introduction. Unless the horizon n is still proportionally large w.r.t. md, these scenarios require special attention. In particular, algorithms like TRACE-UCB that adaptively use contexts in their allocation strategy become more robust than their context-free counterparts.\nA natural assumption in such scenarios is sparsity, i.e., only a small subset of features are relevant to the prediction problem at hand (have non-zero coefficient). In our setting of m problem instances, it is often reasonable to assume that these instances are related to each other, and thus, it makes sense to extend the concept of sparsity to joint sparsity, i.e., a sparsity pattern across the instances. Formally, we assume that there exists a s d such that\n|S| \u2206= | \u222ai\u2208[m] supp(\u03b2i)| = s, (98)\nwhere supp(\u03b2i) = {j \u2208 [d] : \u03b2(j)i 6= 0} denotes the support of the i\u2019th problem instance. A special case of joint sparsity is when |supp(\u03b2i)| \u2248 s, for all i, i.e., most of the relevant features are shared across the instances.\nIn this section, we focus on the scenario where dm > n. When we can only allocate a small (relative to d) number of contexts to each problem instance, proper balancing of contexts becomes extremely important, and thus, the algorithms that do not take into account context in their allocation are destined to fail. Although TRACE-UCB has the advantage of using context in its allocation strategy, it still needs to quickly discover the relevant features (those in the support) and only use those in its allocation strategy.\nThis motivates a two-stage algorithm, we call it SPARSE-TRACE-UCB, whose pseudocode is in Algorithm 2. In the first stage, the algorithm allocates contexts uniformly to all the instances, L contexts per instance, and then recovers the support. In the second stage, it relies on the discovered support S\u0302, and applies the standard TRACE-UCB to all the instances, but only takes into account the features in S\u0302. Note that L should be large enough that with high probability, support is exactly discovered, i.e., S\u0302 = S.\nThere exists a large literature on how to perform simultaneous support discovery in jointly sparse linear regression problems (Negahban & Wainwright, 2011; Obozinski et al., 2011; Wang et al., 2013), which we discuss in detail below.\nMost of these algorithms minimize the regularized empirical loss\nmin M\u2208Rd\u00d7m\n1\nk m\u2211 i=1 \u2016Yi \u2212Xi M[, i]\u20162 + \u03bb \u2016M\u2016,\nwhere k is the number of samples per problem, M be the matrix whose i\u2019th column is M[, i] = \u03b2\u0302i, Xi \u2208 Rk\u00d7d, and Yi = Xi\u03b2i + i. In particular, they use la/lb block regularization norm, i.e., \u2016M\u2016la/lb = \u2016v\u2016la , where vi = \u2016M[i, ]\u2016lb and M[i, ] is the i\u2019th row of M. In short, the SPARSE-TRACE-UCB algorithm uses the l1/l2 block regularization Lasso algorithm (Wang et al., 2013), an extension of the algorithm in (Obozinski et al., 2011), for its support discovery stage.\nWe extend the guarantees of Theorem 7 to the high dimensional case with joint sparsity, assuming s is known.\nThe following is the main result of this section: Theorem 20. Let \u03b41 > 0. Assume \u2016\u03b2i\u20162 \u2264 Z for all i, for some Z > 0, and assume the parameters (n, d, s, \u03b2i,\u03a3) satisfy conditions C1 to C5 in (Wang et al., 2013). Let \u03c8 be the sparsity overlap function defined in (Obozinski et al., 2011). If L > 2(1 + v) \u03c8 log(d\u2212 s)\u03c1u(\u03a3(1:m)SCSC |S)/\u03b3\n2 for some constant v > 0, and n\u2212Lm \u2265 (s+ 1)m, then, with probability at least 1\u2212 \u03b41 \u2212 \u03b42,\nL\u0303n(A) \u2264 \u2211 j \u03c3 2 j\nn\u2212 Lm\n( s+ 2 log 3m\n\u03b41\n) +\n2c\u221a \u03c32min\n( s \u2211 j \u03c3 2 j\nn\u2212 Lm\n)3/2 + o (z) , (99)\nwhere c \u2264 2 ( 1 + \u221a 2 log(12mn/\u03b41)/s ) and we defined \u03b42 = m exp(\u2212c0 log s) + exp(\u2212c1 log(d \u2212 s)) for positive\nconstants c0, c1 > 0, and z = (s/(n\u2212 Lm))3/2.\nThe exact technical assumptions and the proof are given and discussed in below. We simply combine the high-probability results of Theorem 7, and the high-probability support recovery of Theorem 2 in (Wang et al., 2013).\nIn addition, we provide Corollary 21, where we study the regime of interest where the support overlap is complete (for simplicity), n = C1 ms log d md for C1 > 0, and L = C2 s log d, for C1 \u2212 C2 > 0. Corollary 21. Under the assumptions of Theorem 20, let \u03b41 > 0, assume n = C1 ms log d, the support of all arms are equal, and set L = C2 s log d, for C\u0304 := C1 \u2212 C2 > 0. Then, with probability at least 1\u2212 \u03b41 \u2212 \u03b42,\nL\u0303n(A) \u2264 \u2211 j \u03c3 2 j\nC\u0304ms log d\n( s+ 2 log 3m\n\u03b41\n) +\n2c\u221a \u03c32min\n( \u2211 j \u03c3 2 j\nC\u0304m log d\n)3/2 + o (z) (100)\nwhere c \u2264 2 ( 1 + \u221a 2 log(12mn/\u03b41)/s ) and we defined \u03b42 = m exp(\u2212c0 log s) + exp(\u2212c1 log(d \u2212 s)) for constants\nc0, c1 > 0, and z = ( C\u0304m log d )\u22123/2 .\nAlgorithm 2 contains the pseudocode of our Sparse-TRACE-UCB algorithm.\nAlgorithm 2 Sparse-TRACE-UCB Algorithm. 1: for i = 1, . . . ,m do 2: Select problem instance i exactly L times 3: end for 4: Run l1/l2 Lasso to recover support S\u0304 = \u222ai supp(\u03b2\u0302i,L) 5: for i = 1, . . . ,m do 6: Select problem instance i exactly s+ 1 times 7: Compute its OLS estimates \u03b2\u0302i,m(L+s+1) and \u03c3\u03022i,m(L+s+1) with respect to dimensions in S\u0304. 8: end for 9: for steps t = m(L+ s+ 1) + 1, . . . , n do 10: for problem instance 1 \u2264 i \u2264 m do 11: Compute score based on S\u0304 dimensions only:\nsi,t\u22121 = \u03c3\u03022i,t\u22121 + \u2206i,t\u22121 ki,t\u22121 Tr ( \u03a3\u03a3\u0302\u22121i,t\u22121 ) 12: end for 13: Select problem instance It = arg maxi\u2208[m] si,t 14: Observe Xt and YIt,t 15: Update OLS estimators \u03b2\u0302It,t and \u03c3\u0302 2 It,t\nbased on S\u0304 16: end for 17: Return RLS estimates {\u03b2\u0302\u03bbi }mi=1, with \u03b2\u0302\u03bbij = 0 if j /\u2208 S\u0304\nGiven our pure exploration perspective, it is obviously more efficient to learn the true supports as soon as possible. That way we can adjust our behavior by collecting the right data based on our initial findings. Note that this is not always the case; for example, if the total number of pulls is unknown. Then it is not clear what is the right amount of budget to invest upfront to recover the supports (see tracking algorithms and doubling trick).\nWe briefly describe Algorithm 2 in words. First, in the recovery stage we sequentially pull all arms a number of times, say L times. We do not take into account the context, and just apply a round robin technique to pull each arm exactly L times. In total, there are exactly s components that are non-zero for at least one arm (out of d). After the Lm pulls, we use a block-regularized Lasso algorithm to recover the joint sparsity pattern. We discuss some of the alternatives later. The outcome of this stage is a common support S\u0302 := \u222ai supp(\u03b2\u0302i). With high probability we recover the true support S\u0302 = S. In the second stage, or pure exploration stage, the original TRACE-UCB algorithm is applied. The TRACE-UCB algorithm works by computing an estimate \u03c3\u03022i at each step t for each arm i. Then, it pulls the arm maximizing the score\nsi,t\u22121 = \u03c3\u03022i,t\u22121 + \u2206i,t\u22121 ki,t\u22121 Tr ( \u03a3\u03a3\u0302\u22121i,t\u22121 ) .\nThe key observation is that in the second stage we only consider the components of each context that are in S\u0302. In particular, we start by pulling s+ 1 times each arm so that we can compute the initial OLS estimates \u03b2\u0302OLSi and \u03c3\u0302 2 i . We keep updating those estimates when an arm is pulled, and the trace is computed with respect to the components in S\u0302 only.\nFinally, we return the Ridge estimates based only on the data collected in the second stage."}, {"heading": "F.2. A note on the Static Allocation", "text": "What is the optimal static performance in this setting if the \u03c32\u2019s are known? For simplicity, suppose we pull arm i exactly (\u03c32i / \u2211 j \u03c3 2 j ) n times. We are interested in Lasso guarantees for \u2016XT (\u03b2\u0302i \u2212 \u03b2i)\u201622. Note in this case we can actually set \u03bbi as a function of \u03c32i as required in most Lasso analyses, because \u03c3 2 i is known.\nA common guarantee is as follows (see (Hastie et al., 2015; Raskutti et al., 2010)). With high probability\n\u2016\u03b2\u0302i \u2212 \u03b2i\u201622 \u2264 c2\u03c32i \u03b32 \u03c4s log d k ,\nwhere k is the number of observations, d the ambient dimension, s the efficient dimension, \u03b3 is the restricted eigenvalues constant for \u03a3, \u03c4 > 2 is the parameter that tunes the probability bound, and c is a universal constant.\nThus, if we set k = (\u03c32i / \u2211 j \u03c3 2 j ) n, then we obtain that whp\n\u2016\u03b2\u0302i \u2212 \u03b2i\u201622 \u2264 c2\u03c4\n\u03b32  m\u2211 j=1 \u03c32j  s log d n . (101)\nNote that the latter event is independent across different i \u2208 [m], so all of them simultaneously hold with high probability. The term \u03b3\u22122 was expected as depending on the correlation levels in \u03a3 the problem can be easier or harder. In addition, note that as \u2016\u03b2\u0302i \u2212 \u03b2i\u20162\u03a3 = Tr(\u03a3(\u03b2\u0302i \u2212 \u03b2i)(\u03b2\u0302i \u2212 \u03b2i)T ), we have that\n\u03bbmin(\u03a3) \u2016\u03b2\u0302i \u2212 \u03b2i\u201622 \u2264 \u2016\u03b2\u0302i \u2212 \u03b2i\u20162\u03a3 \u2264 \u03bbmax(\u03a3)\u2016\u03b2\u0302i \u2212 \u03b2i\u201622. (102)"}, {"heading": "F.3. Simultaneous Support Recovery", "text": "There has been a large amount of research on how to perform simultaneous support recovery in sparse settings for multiple regressions. Let M be the matrix whose i-th column is M(i) = \u03b2i.\nA common objective function after k observations per problem is\nmin M\u0304\u2208Rd\u00d7m\n1\nk m\u2211 j=1 \u2016Yj \u2212XjM\u0304(j)\u20162 + \u03bb \u2016M\u0304\u2016, (103)\nwhere we assumed Yj = Xj\u03b2j + j , and Xj \u2208 Rk\u00d7d,Yj , j \u2208 Rk and \u03b2j \u2208 Rd.\nThe la/lb block regularization norm is\n\u2016M\u0304\u2016la/lb = \u2016v\u2016a, where vj = \u2016M\u0304j\u2016b M\u0304j is the j-th row of M\u0304. (104)\nThere are a few differences among the most popular pieces of work.\nNegahban and Wainwright (Negahban & Wainwright, 2011) consider random Gaussian designs Xj \u223c N (0,\u03a3j) with random Gaussian noise (and common variance). The regularization norm is l1/l\u221e. In words, they take the sum of the absolute values of the maximum element per row in M\u0304. This forces sparsity (via the l1 norm), but once a row is selected there is no penalty in increasing the \u03b2\u0304 components up to the current maximum of the row. They tune \u03bb as in the standard analysis of Lasso, that is, proportionally to \u03c32, which is unknown in our case. Results are non-asymptotic, and recovery happens with high probability when the number of observations is k > Cs(m+log d). They show that if the overlap is not large enough (2/3 of the support, for m = 2 regression problems), then running independent Lasso estimates has higher statistical efficiency. We can actually directly use the results in (Negahban & Wainwright, 2011) if we assume an upper bound \u03c32max \u2264 R is known.\nObozinski, Wainwright and Jordan (Obozinski et al., 2011) use l1/l2 block regularization (aka Multivariate Group Lasso). Their design is random Gaussian, but it is fixed across regressions: Xj = X. They provide asymptotic guarantees under the scaling k, d, s\u2192\u221e, d\u2212 s\u2192\u221e, and standard assumptions like bounded \u03a3-eigenspectrum, the irrepresentable condition, and self-incoherence. The first condition is not only required for support recovery, but also for l2 consistency. The last two conditions are not required for risk consistency, while essential for support recovery. To capture the amount of non-zero pattern overlap among regressions, they define the sparsity overlap function \u03c8, and their sample requirements are a function of \u03c8. In particular, one needs k > C \u03c8 log(d \u2212 s), where the constant C depends on quantities related to the covariance matrix of the design matrices, and \u03c8 can be equal to s/m, if all the patterns overlap, and at most s if they are disjoint.\nTheir theorems use a sequence of regularization parameters\n\u03bbk =\n\u221a f(d) log d\nk , where f(d)\u2192\u221e as d\u2192\u221e,\nin such a way that \u03bbk \u2192 0 as k, d \u2192 \u221e. Finally, k > 2s is also required. They also provide a two-stage algorithm for efficient estimation of individual supports for each regression problem. All these optimization problems are convex, and can be efficiently solved in general.\nTo overcome the issue of common designs (we do not pull each context several times), we use the results by Wang, Liang, and Xing in (Wang et al., 2013). They extend the guarantees in (Obozinski et al., 2011) to the case where the design matrices are independently sampled for each regression problem. In order to formally present their result, we describe some assumptions. Let \u03a3(i) be the covariance matrix for the design observations of the i-th regression (in our case, they are all equal to \u03a3), and S the union of the sparse supports across regressions.\n\u2022 C1 There exists \u03b3 \u2208 (0, 1] such that \u2016|A\u2016|\u221e \u2264 1\u2212 \u03b3, where\nAjs = max 1\u2264i\u2264m \u2223\u2223\u2223\u2223 (\u03a3(i)SCS (\u03a3(i)SS)\u22121) js \u2223\u2223\u2223\u2223, (105) for j \u2208 SC and s \u2208 S.\n\u2022 C2 There are constants 0 < Cmin \u2264 Cmax <\u221e, such that the eigenvalues of all matrices \u03a3(i) are in [Cmin, Cmax].\n\u2022 C3 There exists a constant Dmax <\u221e such that\nmax 1\u2264i\u2264m\n|\u2016 (\n\u03a3 (i) SS )\u22121 \u2016|\u221e \u2264 Dmax. (106)\n\u2022 C4 Define the regularization parameter\n\u03bbk =\n\u221a f(d) log d\nk , where f(d)\u2192\u221e as d\u2192\u221e, (107)\nsuch that \u03bbk \u2192 0 as k \u2192\u221e.\n\u2022 C5 Define \u03c1(k, s, \u03bbk) as\n\u03c1(k, s, \u03bbk) :=\n\u221a 8\u03c32maxs log s\nk Cmin + \u03bbk\n( Dmax + 12s\nCmin \u221a k\n) , (108)\nand assume \u03c1(k, s, \u03bbk)/b\u2217min = o(1), where b \u2217 min = minj\u2208S \u2016Mj\u20162.\nWe state the main theorem in (Wang et al., 2013); k is the number of observations per regression.\nTheorem 22. Assume the parameters (k, d, s,M,\u03a3(1:m)) satisfy conditions C1 to C5. If for some small constant v > 0,\nk > 2(1 + v) \u03c8 log(d\u2212 s) \u03c1u(\u03a3\n(1:m) SCSC |S) \u03b32 , (109)\nthen the l1/l2 regularized problem given in (103) has a unique solution M\u0302, the support union supp(M\u0302) equals the true support S, and \u2016M\u0302\u2212M\u2016l\u221e/l2 = o(b\u2217min), with probability greater than\n1\u2212m exp(\u2212c0 log s)\u2212 exp(\u2212c1 log(d\u2212 s)), (110)\nwhere c0 and c1 are constants.\nThe following proposition is also derived in (Wang et al., 2013) (Proposition 1): Proposition 23. Assume \u03a3(1:m) satisfy C2, then \u03c8 is bounded by\ns\nm Cmin \u2264 \u03c8 = \u03c8\n( M,\u03a3(1:m) ) \u2264 s Cmin . (111)\nFor our purposes, there is a single \u03a3, which implies that we can remove the max expressions in C1 and C3. Corollary 2 in (Wang et al., 2013) establishes that when supports are equal for all arms, the number of samples required per arm is reduced by a factor of m."}, {"heading": "F.4. High-Dimensional Trace-UCB Guarantees", "text": "If the support overlap is complete we can reduce the sampling complexity of the first stage by a factor of m; we only need\nLm > 2(1 + v) s log(d\u2212 s) \u03c1u(\u03a3\n(1:m) SCSC |S)\nCmin \u03b32 (112)\nobservations in total, for some small constant v > 0.\nNow we show our main result for high-dimensional Trace-UCB, Theorem 20. Theorem. Let \u03b41 > 0. Assume \u2016\u03b2i\u20162 \u2264 Z for all i, for some Z > 0, and assume the parameters (n, d, s, \u03b2i,\u03a3) satisfy conditions C1 to C5 in (Wang et al., 2013). Let \u03c8 be the sparsity overlap function defined in (Obozinski et al., 2011). If L > 2(1 + v) \u03c8 log(d\u2212 s)\u03c1u(\u03a3(1:m)SCSC |S)/\u03b3\n2 for some constant v > 0, and n\u2212Lm \u2265 (s+ 1)m, then, with probability at least 1\u2212 \u03b41 \u2212 \u03b42,\nL\u0303n(A) \u2264 \u2211 j \u03c3 2 j\nn\u2212 Lm\n( s+ 2 log 3m\n\u03b41\n) +\n2c\u221a \u03c32min\n( s \u2211 j \u03c3 2 j\nn\u2212 Lm\n)3/2 + o (z) , (113)\nwhere c \u2264 2 ( 1 + \u221a 2 log(12mn/\u03b41)/s ) and we defined \u03b42 = m exp(\u2212c0 log s) + exp(\u2212c1 log(d \u2212 s)) for positive\nconstants c0, c1 > 0, and z = (s/(n\u2212 Lm))3/2.\nProof. We start by assuming the recovered support S\u0302 is equal to the true support S. This event, say ES , holds with probability at least 1\u2212 \u03b42 by Theorem 22 when L satisfies (112).\nThen, we fix \u03b41 > 0, and run the second stage applying the Trace-UCB algorithm in the s-dimensional space given by the components in S\u0302.\nBy Theorem 7, if n\u2212 Lm \u2265 (s+ 1)m, then, with probability at least 1\u2212 \u03b41, the following holds:\nL\u0303n(A)S \u2264 \u2211 j \u03c3 2 j\nn\u2212 Lm\n( s+ 2 log 3m\n\u03b41\n) +\n2c\u221a \u03c32min\n( s \u2211 j \u03c3 2 j\nn\u2212 Lm\n)3/2 + o (( s\nn\u2212 Lm\n)3/2) , (114)\nwhere L\u0303n(A)S denotes the loss restricted to the components in \u03b2 that are in S\u0302 (and \u03a3S). However, under event ES , we recovered the true support, and our final estimates for \u03b2ij for each j 6\u2208 S and arm iwill be equal to zero, which corresponds to their true value. Hence L\u0303n(A) = L\u0303n(A)S .\nWe conclude that (114) holds with probability at least 1\u2212 \u03b41 \u2212 \u03b42.\nOne regime of interest is when n = C1 ms log d md. In addition, let us assume complete support overlap across arms, so \u03c8 = s/Cm. Then, we set the number of initial pulls per arm to be L = C2 s log d, with C1 > C2.\nIn this case, we have that Corollary 21 holds."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We explore the sequential decision-making problem where the goal is to estimate a number of linear models uniformly well, given a shared budget of random contexts independently sampled from a known distribution. For each incoming context, the decision-maker selects one of the linear models and receives an observation that is corrupted by the unknown noise level of that model. We present Trace-UCB, an adaptive allocation algorithm that learns the models\u2019 noise levels while balancing contexts accordingly across them, and prove bounds for its simple regret in both expectation and high-probability. We extend the algorithm and its bounds to the high dimensional setting, where the number of linear models times the dimension of the contexts is more than the total budget of samples. Simulations with real data suggest that Trace-UCB is remarkably robust, outperforming a number of baselines even when its assumptions are violated.", "creator": "LaTeX with hyperref package"}}}