{"id": "1703.00767", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Attentive Recurrent Comparators", "abstract": "many problems in artificial intelligence and machine learning work can already be reduced to the problem of quantitative comparison of two entities. in deep learning the simplest ubiquitous architecture used for this task is the siamese neural network which maps each entity to a representation through a certain learnable function and expresses similarity through the distances among the entities depicted in the representation space. in this paper, we argue that such a complex static and simple invariant mapping is both naive and unnatural. we also develop a novel neural model notion called attentive correlation recurrent comparators ( arcs ) that dynamically compares two entities and test the model extensively on the omniglot dataset. in reducing the task of collaborative similarity learning, our simplistic model that does not use any convolutions performs weaker on par characteristics with deep baseline convolutional siamese networks and significantly better when deeper convolutional layers are also used. in the challenging intrinsic task dimension of one - shot learning on the same dataset, an arc based model indirectly achieves the first super - human performance for a neural method with itself an error rate of 1. 5 \\ %.", "histories": [["v1", "Thu, 2 Mar 2017 12:47:40 GMT  (288kb,D)", "http://arxiv.org/abs/1703.00767v1", null], ["v2", "Sun, 5 Mar 2017 12:23:16 GMT  (288kb,D)", "http://arxiv.org/abs/1703.00767v2", null], ["v3", "Fri, 30 Jun 2017 07:37:56 GMT  (392kb,D)", "http://arxiv.org/abs/1703.00767v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["pranav shyam", "shubham gupta", "ambedkar dukkipati"], "accepted": true, "id": "1703.00767"}, "pdf": {"name": "1703.00767.pdf", "metadata": {"source": "META", "title": "Attentive Recurrent Comparators", "authors": ["Pranav Shyam", "Shubham Gupta", "Ambedkar Dukkipati"], "emails": ["PRANAVM.CS13@RVCE.EDU.IN", "SHUBHAM.GUPTA@CSA.IISC.ERNET.IN", "AD@CSA.IISC.ERNET.IN"], "sections": [{"heading": "1. Introduction", "text": "Advancing Deep Learning systems to solve Artificial Intelligence tasks requires that models be capable of performing continual and rapid learning (Lake et al., 2016). Ideally, this means having a dynamic representation space where in the features observed in samples vary based on the entities that the model comes across over the course of its lifetime. But top-down hierarchical designs of models to perform such tasks are not very successful on real world data and there are many reasons for this (Santoro et al., 2016). First, most datasets are generally not designed with such higher order tasks in mind, thus researchers either work with synthetic data or fabricate higher level tasks based on traditional datasets - both of which constrain their utility. Second, hierarchical or meta models suffer from reduced supervision during training due to their inherent design.\nThird, with our experiments we found that the foundational architectures like Memory Augmented Neural Networks are still in their infancy and not ripe enough to be utilized in complex hierarchical systems.\nIn this paper, we present an alternative way that very crudely approximates having a dynamic representation space by building models which form the representation space lazily at the inference time such that it is conditioned on the test sample. A crucial ingredient for doing this is the development of relative representations - representations of all entities in relation to some other specific entity. For this purpose, we develop a novel and special class of models called Attentive Recurrent Comparators.\nWhen a person is asked to compare two objects and estimate their similarity, the person does so by repeatedly looking back and forth between the two objects. With each glimpse of an object, a specific observation is made. These observations made in both objects are then cumulatively used to come to a conclusion about their similarity. A crucial characteristic of this process is that new observations are made conditioned on the previous context that has been investigated so far by the observer. The observation and it\u2019s contextual location are based on intermediate deductions. These intermediate deductions are themselves based on the observations made so far in the two objects. A series of such guided observations and the entailing inferences are accumulated and finally the judgement on similarity is made.\nIn stark contrast to this, current similarity estimating systems in Deep Learning are analogues of the Siamese similarity learning system (Bromley et al., 1993). In this system, a fixed set of features is detected in both the objects. Detection of features is independent of the features present in the other object. The two objects are compared based on the mutual agreement in the detected features. More concretely, comparison between two objects in this system consists of measuring the distance between their vector embeddings. A neural network defines the mapping from the object to the corresponding embedding vector in target space. This neural network is trained to extract the most\nar X\niv :1\n70 3.\n00 76\n7v 1\n[ cs\n.C V\n] 2\nM ar\n2 01\n7\nsalient features from the object for the specific task in hand.\nThere is major underlying difference between the human approach discussed above and the siamese approach to the problem. In the human way, the information from the two objects is fused from the very beginning and this combined information primes the subsequent steps in comparison. There are multiple lookups on each of the objects and each of these lookups are conditioned on the observations of both the objects so far. In the siamese way, when the embeddings in the target space are compared the information fuses mostly at an abstract level and only in the last stage. We used the modern tools of attention and recurrence to build an end-to-end differentiable model that can learn to compare objects called Attentive Recurrent Comparators (ARCs). ARCs judge similarity of objects similar to the way people do as discussed above. More importantly, they make observations on one object conditioned on the other.\nWe tested ARCs across many visual tasks and compared it against strong baselines of prevalent methods. ARCs which did not use any convolutions showed comprable performance compared to Deep Convolutional Siamese Neural Networks on challenging tasks. Though Dense ARCs are as capable as ConvNets, a combination of both ARCs and convolutions produces more superior models (hereafter referred to as ConvARCs), capable of better generalization and performance. In the task of estimating the similarity of two characters from the Omniglot dataset for example, ARCs and Deep Convnets both achieve about 93.4% accuracy, whereas ConvARCs achieve 96.10% accuracy. In the task of face verification on the CASIA Webface dataset for example, ConvARCs achieved 81.73% accuracy surapassing the 75.85% achieved by a CNN baseline.\nWe then use ARCs for generating a relative representation space and use it for One shot learning. On the Omniglot one-shot classification task, our model achieved 98.5% accuracy significantly surpassing the current state of the art set by Deep Learning methods or other systems.\nFundamentally, the performance of ARCs shows the value of early fusion of information across the entire context of the task and the value of dynamic representations. Further, it also strengthens the view that attention and recurrence together can be as good as convolutions in some cases."}, {"heading": "2. Attentive Recurrent Comparators", "text": "Our ARC model is an algorithmic imitation of the human way discussed in Section 1 built with Deep Neural Networks. In this section, we detail the ARC model that can compare two images and judge their similarity but it is trivial to generalise it to more images and/or other modalities. See Figure 1 for a visual depiction of this model.\nThe model consists of a recurrent contoller and an attention mechanism that takes in an specially constructed presentation sequence as the input. Given two images {xa, xb}, we alternate between the two images for a finite number of presentations of each image to form the presentation sequence xa, xb, xa, xb, .... The model repeatedly cycles through both the images, attending to only one image at one time-step.\nFor time-step t the image presented is given by:"}, {"heading": "It \u2190\u2212 xa if t % 2 is 0 else xb", "text": "Over the course of many time steps, model observes many aspects of both the images. The observations are made by the model at each time step by directing its attention to a region of interest in each input. Since the contoller of the model is a Recurrent Neural Network, this round robin like cyclic presentation of images allows for early fusion of information from all the images. This makes the model aware of the context in which it is operating. Consequently, this provides feedback to the attention mechanism to attend on the relevant and crucial parts of each sample considering the observations made so far.\nThe attention mechanism focuses on a specific region of the image It to get the glimpse Gt."}, {"heading": "Gt \u2190\u2212 attend(It,\u2126t) where \u2126t = Wght\u22121", "text": "attend(.) is the attention mechanism described in the next sub-section, that acts on image It. \u2126t are the attention glimpse parameters which specify the location and size of\nthe attention window. At each step, we use the previous hidden state of the RNN contoller ht\u22121 to compute \u2126t. Wg is the projection matrix that maps the hidden state to the required number of attention parameters.\nNext, both the glimpse and previous hidden state are utilized to form the next hidden state.\nht \u2190\u2212 RNN(Gt, ht\u22121)\nRNN(.) is the update function for the recurrent contoller being used. This could be simple RNN or an LSTM.\nIf we make g glimpses (or observations) of each image, the hidden state of the RNN contoller at the final time-step ht = h2g can be then used as the relative representation of xa with respect to xb or vice versa. We arrived at the iterative cycling paradigm after trying out many approaches to attend to multiple images at once on a few toy datasets. Iterative cycling turned out to be more computationally efficient, scalable and statistically more consistent than other approaches we tested. Note that It for some t alternates between xa and xb, while the rest of the equations are exactly the same for all time steps."}, {"heading": "2.1. Attention Mechanism", "text": "The attention mechanism we use is incrementally derived from zoomable and differential image observation mechanism of DRAW Gregor et al. (2015). The attention window is defined by an N \u00d7 N 2D grid of Cauchy kernels. We found that the heavy tail of the Cauchy curve to aid in alleviating some of the vanishing gradient issues and it also sped up training.\nThe grid\u2019s location and size is defined based on the glimpse parameters. The N \u00d7N grid of kernels is placed at (x, y) on the S \u00d7 S image, with the central Cauchy kernel being located at (x, y). The distance between two adjacent Cauchy kernals either in the vertical or horizontal direction is \u03b4. In other words, the elemental square of the 2D grid is \u03b4 \u00d7 \u03b4 in size. The glimpse parameter set \u2126t is unpacked to get \u2126t \u2192 (x\u0302, y\u0302, \u03b4\u0302). x, y and \u03b4 are computed from x\u0302, y\u0302 and,\u03b4\u0302 using the following transforms:\nx = (S \u2212 1) (x\u0302+1)2 y = (S \u2212 1) (y\u0302+1) 2\n\u03b4 = SN (1\u2212 |\u03b4\u0302|) \u03b3 = e 1\u22122|\u0302\u03b4|\nThe location of a ith row, jth column\u2019s Cauchy kernel in terms of the pixel coordinates of the image is given by:\n\u00b5iX = x+(i\u2212(N+1)/2)\u03b4 \u00b5 j Y = y+(j\u2212(N+1)/2)\u03b4\nThe horizontal and vertical filterbank matrices are then calculated as:\nFX [i, a] = 1 ZX\n{ \u03c0\u03b3 [ 1 + ( a\u2212\u00b5iX \u03b3 )2]}\u22121 FY [j, b] =\n1 ZY\n{ \u03c0\u03b3 [ 1 + ( b\u2212\u00b5j Y\n\u03b3 )2]}\u22121 ZX and ZY are normalization constants such that they make \u03a3aFX [i, a] = 1 and \u03a3bFX [j, b] = 1\nFinal result of the attending to the image is given by:\nattend(It,\u2126t) = FY ItF T X\nattend thus gets an N \u00d7 N patch of the image, which is flattened and used in the model."}, {"heading": "2.2. Use of Convolutions", "text": "As will be seen in the experimental sections that follow, use of Convolutional feature extractors gave a significant boost in performance. Attention over raw images performed as well as Deep ResNets, but we found large improvements by using Convolutional feature extractors. Given an image, the application of several layers of Convolution produces a 3D solid of activations (or a stack of 2D feature maps). Attention over this corresponds to applying the same 2D attention (described in Section 2.1 above) over the entire depth of the 3D feature map. The attended sub solid is then flattened and used as the glimpse."}, {"heading": "3. Experiments and Analysis", "text": "Understanding the empirical functioning of an ARC and identifying factors affecting its performance requires both qualitative and quantitative studies. Qualitative analysis tells us what the model is doing when it is comparing 2 images and how this relates to human ways of comparison. Quantitative analysis shows the variations in performance when certain aspects of the model are changed and thus provide an estimate of their importance. For the analysis presented below, we use the simple ARC model (without convolutions) described in Section 2 above trained for the verification (or similarity learning) task on the Omniglot dataset. The verification task is a binary classification problem wherein the model is trained to predict whether the 2 drawings of characters provided belong to the same character or not."}, {"heading": "3.1. Empirical Analysis", "text": "Omniglot is a dataset by (Lake et al., 2015) that is specially designed to compare and contrast the learning abilities of humans and machines. The dataset contains handwritten characters of 50 languages/alphabets. Though there are 1623 characters, there are only 20 samples for each character drawn by distinct individuals. So this is in a diagonally opposite position when compared to MNIST or ImageNet\ndatasets. Data samples in the Omniglot dataset have an understandable structure with characters being composed of simple strokes drawn on a clean canvas. The dataset is also very diverse, which allows us to study various characteristics of our model under a wide range of conditions. Since our main result in the paper is also on the Omniglot dataset (next section), we train the ARC on this same dataset for this analysis to get an insight into the type of performance gains brought about by this architecture. Most importantly, this dataset, at least to some extent captures the essence of the real world situation that was described in the Introduction.\nThe final hidden state of the RNN controller hT is used to output at a single logistic neuron that estimates the probabilty of similarity. The whole setup is trained end to end with back-propagation and SGD. The particular model under consideration has an LSTM controller (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000). The number of glimpses per image was fixed to 8, thus the total number of recurrent steps being 16. 32\u00d732 greyscale images of characters were used and the attention glimpse resolution is 4 \u00d7 4. Similar/dissimilar pairs of character drawings are randomly chosen only within alphabet to make the task more challenging. Out of the 50 alphabets provided in the dataset, 30 were used for training and validation and the last 20 for testing, as per the Omniglot usage protocol followed by (Lake et al., 2015)."}, {"heading": "3.1.1. QUALITATIVE ANALYSIS", "text": "The following inferences were made after studying several cases of ARC\u2019s operation:\n1. The observations in one image are definitely being conditioned on the observations in the other image. This can be seen in figures 2a and 2b.\n2. The ARC seems to have learnt a fairly regular left to right parsing strategy, during which the attention window gradually reduces in size. This is quite similar to strategies found in other sequential attentive models like DRAW (Gregor et al., 2015).\n3. Deviation from such regular ordered parsing occurs if model finds some interesting feature in either character. This results in attention being fixated to that particular region of the character for a few subsequent glimpses.\n4. There is no strict coordination or correspondence chronologically between the attended regions of the two images. While instances of ARC focussing on the same aspect/stroke of two characters were common, there were plenty more instances wherein the ARC attended to different aspects/strokes in each image during an interval. We hypothesise that the RNN controller could be utilizing turns of glimpsing at an image to observe some other aspects which are not of immediate consequence.\n5. We also frequently encountered cases wherein the attention window, after parsing as described in point 2, would end up focusing on some blank, stroke-less region, as if it had stopped looking at the sample. We hypothesize that the model is preferring to utilize its recurrent transitions and not to be disturbed by any input stimuli."}, {"heading": "3.1.2. QUANTITATIVE ANALYSIS", "text": "We performed a simple yet very insightful ablation study to understand ARC\u2019s dynamics. ARC accumulates information about both the input images by a series of attentive observations. To see the how information content varied with obervations, we trained 8 separate binary classifiers to classify images as being similar or not based on hidden states of the LSTM controller at every even time-step. The performance of these classifiers is reported in Table 1. Since the ARC has an attention window of only 4 \u00d7 4 pixels, it can barely see anything in the first time step, where its attention is spread throughout the whole image. With more glimpses, finer observations bring in more precise information and the recurrent transitions make use of this knowledge, leading to higher accuracies. We also used the 8 binary classifiers to study how models confidence grows with\nmore glimpses and one such good example is provided in Figure 3."}, {"heading": "3.2. Similarity Learning Performance Compared to Siamese Networks", "text": "In this section we compare ARCs with other Deep Learning methods, mainly Siamese Neural Networks on two representitive datasets: Omniglot and CASIA WebFace Dataset. We consider strong convolutional baselines, which have been shown time and again to excel at such visual tasks. Particularly, we use Wide Resnets (WRNs) (Zagoruyko & Komodakis, 2016) which are the current state of the art models in image classification. Wide ResNets contain 4 blocks of convolutional feature extractors.\nConvARC models also used Wide Resnets for feature extraction but with one less block of convolutions. We used\nmoderate data augmentation consisting of translation, flipping, rotation and shearing, which we found to be critical for training ARC models (WRNs also were trained with the same augmentation). Hyper parameters were set for reasonable values for all our ARC models and no hyperparameter tuning of any kind was employed."}, {"heading": "3.2.1. OMNIGLOT", "text": "The same exact model used in the previous section was used for this comparision. The data split up of the Omniglot dataset used for this comparison is different from the above: 30 alphabets was used for training, 10 for validation and 10 for testing (this was in order to be comprable to the ConvNets in (Koch et al.)).The results are aggregated in Table 2. Our simple ARC model without using any convolutional layers obtains a performance that matches a AlexNet style 6 layer Deep Convnet. Using convolutional feature extractors, ARCs outperform the Wide ResNet based Siamese ConvNet baselines, even the ones containing an order of magnitude more parameters."}, {"heading": "3.2.2. CASIA WEBFACE", "text": "CASIA Webface is the largest public repository of faces consisting of 494,414 distinct images of over 10 thousand people. We split the data as follows: Training set: 70% (7402 people), validation set: 15% (1586 people) and Test set: 15% (1587 people). The images were downscaled to 32x32 pixels and an attention window of 4x4 pixels was used. The rest of the architecture is same as the Omniglot model. Results are tabluated in Table 3."}, {"heading": "4. Relative Representations for One Shot Classification", "text": "One shot learning requires learning models to be at the apotheosis of data efficiency. In the case of one shot classification, only a single example of each individual class is given and the model is expected to generalise to new samples of the same class. A classic example is of such learning behaviour is that of a human kid learning about the animal giraffe (Vinyals et al., 2016). The kid does not see thousands of images of Giraffes to learn about it. Rather, just from a single example, the kid can not only recognize it at any future point, but going further, she can also speculate on its other characteristics. While humans excel at this task, current Deep Learning systems are at the opposite end of the spectrum, where they are trained on millions of samples to achieve the kind of results that they are well known for. It can be easily argued that one shot learning is a critical ingredient in developing human level machine intelligence that can rapidly and continually learn."}, {"heading": "4.1. Dynamic and Relative Representations", "text": "Deep Neural Networks learn representations of data such that points closer in the representation space are semanitcally similar and points further apart are semantically dissimilar. Representation of a sample is computed by identifying a fixed set of features in it, and these features are learnt from scratch based on the data. In the end, a trained neural network can be interpreted as being composed of two aspects - a function that maps the input sample to a point in representation space and a classifier that learns a decision boundary in this representation space.\nRapid and continual learning requires that the representation space be dynamic which means that it is constantly changing with new evidence. All features that form a good representation aren\u2019t known at training time and entirely new concepts can come up during the lifetime of a con-\ntinual learning model. For example, suppose we have a model that can distinguish between cat and dog images. At some point during the models active lifespan, a new class/concept of trucks might get introduced. The features being observed from cat and dog images probably aren\u2019t very helpful in identifying trucks. Further, if a new concept of skycrapers is introduced, the classification between the new concepts becomes very hard. Ideally, the entire representation space should change when the new concepts of trucks and skyscrapers are introduced, in order to account for patterns and variations present in the new concepts in conjunction with old concepts. One way of training such systems is to have a meta learning system where in the model is trained to represent entities in space (rather than being trained to represent a entity) (Schaul & Schmidhuber, 2010) (Santoro et al., 2016). But it is very difficult to train end-to-end systems that have this ability. Here we propose an alternative strategy that could be a crude approximation of this ideal scenario.\nThe alternative approach is to lazily develop a representation space that is conditioned on the input test sample only at inference time. Until then, all samples presented to the model are just stored as is in a repository. When the test sample is given, we compare this sample with every sample in our repository using ARCs to form a relative representation of each sample (the representation being the final hidden state of the recurrent controller). In this representation space, which is relative to the input test sample, we use a trained a classifier that can identify the most similar sample pair, given the context of entire relative representation space. The class of the matching sample is then outputted as the prediction of the model. This relative representation space is dynamic as it changes relative to the test sample. In our example above, if the test image of the skyscraper class is given, the relative representations of all images of the dog, cat, truck and skyscraper classes are computed with ARCs. A classifier then takes in these relative representations and computes the closest match. Crucial to the working of this is ARCs - unlike Siamese Networks or models built with fixed feature extractors, ARCs compare entities based on what is present in the two images1. Here on, we refer to the repository of samples accumulated as the support set."}, {"heading": "4.2. Models", "text": ""}, {"heading": "4.2.1. NAIVE ARC MODEL", "text": "This is a trivial extension of ARC for used for verification to this task. The test image from the evaluation set is compared against all the images from the support set. It\n1this is again restriced to some set of features that the ARC has learnt to detect, but it is far more generic than simple feed forward feature extractors.\nis matched to the sample with maximum similarity and the corresponding class is the prediction of the model. Here, we are reducing the relative representations to similarity scores directly. The entire context of the relative representation space is not incorporated. Thus, this also acts as a baseline model compared to our Full Context model."}, {"heading": "4.2.2. FULL CONTEXT ARC", "text": "This model incorporates all the knowledge of the relative representation space generated before making a prediction. While Naive ARC model is simple and efficient, it does not incorporate the whole context in which our model is expected to make the decision of similarity. When the test sample is being compared with all support samples, the comparisons are all independently done.\nAs with the Naive method, we compare test sample from evaluation set with each image from support set in pairs. But instead of emitting a similarity score immediately, we collect the relative representations of each comparison. Relative representations are the final hidden state of the controller when the test image T is being compared to image Sj from the support set: ej = hLT,Sj . These embeddings are further processed by a Bi-Directional LSTM layer. This merges the information from all comparisons, thus providing the necessary context before score emission. A similar method is also used in Matching Networks (Vinyals et al., 2016).\ncj = [ \u2212\u2212\u2212\u2212\u2192 LSTM(ej); \u2190\u2212\u2212\u2212\u2212 LSTM(ej) ] \u2200j \u2208 [1, 20]\nEach embedding is mapped to a single score sj = f(cj), where f(.) is an affine transform followed by a nonlinearity. The final output is the normalized similarity with respect to all similarity scores.\npj = softmax(sj) \u2200j \u2208 [1, 20]\nThis whole process is to make sure that we adhere to the fundamental principle of deep learning, which is to optimise objectives that directly reflect the task. The normalisation allows for the expression of relative similarity rather than absolute similarity."}, {"heading": "4.3. Omniglot", "text": "One Shot Classification on this dataset is very challenging one as most Deep Learning systems do not work well out of the box on this dataset. (Lake et al., 2015) developed a dedicated system for such rapid knowledge acquisition called Bayesian Programming Learning, which surpasses human performance and is the current state of the art of all methods.\nThe dataset contains 1623 alphabets and is divided into a background set and an evaluation set. Background set con-\ntains 30 alphabets (964 characters) and only this set should be used to perform all learning (e.g. hyper-parameter inference or feature learning). The remaining 20 alphabets are for pure evaluation purposes only. Each character is a 105\u00d7 105 image.\nA one shot classification task is as follows: from a randomly chosen alphabet, 20 characters are chosen which becomes the support set. One character among these 20 becomes the test character. 2 drawers are chosen, one each for the support set and the test character. The task is to match the test drawing to the correct character\u2019s drawing in the support set. Assigning an image to one of the 20 characters given results in a 20-way, 1-shot classification task."}, {"heading": "4.3.1. BASELINES AND OTHER METHODS", "text": "We compare the two models discussed above with other methods in literature: starting from the simplest baseline of k-Nearest Neighbours to the latest meta-learning methods. The training and evaluation practises are non consistent.\nAcross Alphabets: Many papers recently, like Matching Networks (Vinyals et al., 2016) and MANNs (Santoro et al., 2016) have used 1200 chars for background set (instead of 964 specified by (Lake et al., 2015)). The remaining 423 characters are used for testing. Most importantly, the characters sampled for both training and evaluation are across all the alphabets in the training set.\nWithin Alphabets: This corresponds to standard Omniglot setting where characters are sampled within an alphabet and only the 30 background characters are used for training and validation.\nThe across alphabet task is much more simpler as it is easy to distinguish characters belonging to different languages, compared to distinguishing characters belonging to the same language.\nThere are large variations in the resolution of the images used as well. The Deep Siamese Network of Koch et al. uses 105x105 images and thus not comparable to out model, but we include it as it is the current best result using deep neural nets. The performance of MANNs in this standard setup is interpreted from the graph in the paper as the authors did not report it. It should also be noted that HBPL incorporates human stroke data into the model. Lake et al estimate the human performance to be at 95.5%."}, {"heading": "4.3.2. RESULTS", "text": "Results are presented in Table 4 and 5. Our ARC models outperform all previous methods according to both of the testing protocols and establish the corresponding state of the art results."}, {"heading": "4.4. miniImageNet", "text": "Recently, Vinyals et al. (2016) introduced a One Shot learning task taken based off of the popular ImageNet dataset and uses a testing protocol similar to Omniglot. The dataset consists of 60,000 colour images of size 84 84 with 100 classes of 600 examples each. As with the original paper, we used 80 classes for training and tested on the remaining 20 classes. We report results on 5-way one shot task, which is a one shot learning with 5 classes at a time. The same model used above was freshly trained for this task and the results are presented in Table 6.\nTable 6. 5 way One Shot Classification accuracies of various methods and our ARC models - miniImageNet\nMODEL ACCURACY\nRAW PIXELS W/ COSINE SIMILARITY 23.0% BASELINE CLASSIFIER (VINYALS ET AL., 2016) 38.4% MATCHING NETWORKS 46.6% NAIVE CONVARC 48.71% NAIVE CONVARC W/ L2 REGULARIZTION 49.14%"}, {"heading": "5. Related Work", "text": "Deep Neural Networks (Schmidhuber, 2015) (LeCun et al., 2015) are very complex parametrised functions which can be adapted to have the required behaviour by specifying a suitable objective function. Our overall model is a simple combination of the attention mechanism and recurrent neural networks (RNNs). We test our model by analysing its performance in similarity learning. We also test its generalisation ability by using it in a model built for the challenging task of one shot classification on hand-written character symbols.\nIt is known that attention brings in selectivity in processing information while reducing the processing load (Desimone & Duncan, 1995). Attention and (Recurrent) Neural Networks were combined in Schmidhuber & Huber (1991) to learn fovea trajectories. Later attention was used in conjunction with RBMs to learn what and where to attend in Larochelle & Hinton (2010) and in Denil et al. (2012). Hard Attention mechanism based on Reinforcement Learning was used in Mnih et al. (2014) and further extended to multiple objects in Ba et al. (2014); both of these models showed that the computation required at inference is significantly less compared to highly parallel Convolutional Networks, while still achieving good performance. A soft or differentiable attention mechanisms have been used in Graves (2013). A specialised form of location based soft attention mechanism, well suited for 2D images was developed for the DRAW architecture (Gregor et al., 2015), and this forms the basis of our attention mechanism in ARC.\nA survey of the methods and importance of measuring similarity of samples in Machine Learning is presented in Bellet et al. (2013). With respect to deep learning methods, the most popular architecture family is that of Siamese Networks (Bromley et al., 1993). The energy based derivation of the same is presented in Chopra et al. (2005). A bayesian framework for one shot visual recognition was presented in Fe-Fei et al. (2003). Lake et al. (2015) extensively study One Shot Learning and present a novel probabilistic framework called Hierarchical Bayesian Program Learning (HBPL) for rapid learning. They have also released the Omniglot dataset, which has become a testing ground for One Shot learning techniques. Recently, many Deep Learning methods have been developed to do one shot learning: Koch et al. use Deep Convolutional Siamese Networks for performing one shot classification. Matching Networks (Vinyals et al., 2016) and Memory Augmented Neural Networks (Santoro et al., 2016) are other approaches to perform continual or meta learning in the low data regime. All the models except the HBPL have inferior one shot classification performance compared to humans on the Omniglot Dataset."}, {"heading": "6. Conclusion and Future Work", "text": "At the most basic level, we presented a model that uses attention and recurrence to cycle through a set images repeatedly and estimate their similarity. We showed that this model is not only viable but also much better than the siamese neural networks in wide use today in terms of performance and generalization. But, taking a step back, we showed the value of having dynamic representations and presented a novel way of crudely approximating it. Our main result is in the task of One Shot classification on the Omniglot dataset, where we achieved state of the art performance surpassing HBPL\u2019s and human performance.\nOne potential downside of this model is that due to sequential execution of the recurrent core and by the very design of the model, it might be more computationally expensive than a distance metric methods. But we believe that advancing hardware speeds, such costs will be outweighed by the benefits of ARCs.\nThough presented in the context of images, ARCs can be used in any modality. There are innumerable ways to extend ARCs. Better attention mechanisms, higher resolution images, different datasets, hyper-parameter tuning, more complicated controllers etc are the simple things which could be employed to achieve better performance.\nMore interesting extensions would involve developing more complex architectures using this bottom-up, lazy approach to solve even more challenging AI tasks."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["Bellet", "Aur\u00e9lien", "Habrard", "Amaury", "Sebban", "Marc"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Neural mechanisms of selective visual attention", "author": ["Desimone", "Robert", "Duncan", "John"], "venue": "Annual review of neuroscience,", "citeRegEx": "Desimone et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Desimone et al\\.", "year": 1995}, {"title": "A bayesian approach to unsupervised one-shot learning of object categories", "author": ["Fe-Fei", "Li", "Fergus", "Robert", "Perona", "Pietro"], "venue": "In Computer Vision,", "citeRegEx": "Fe.Fei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fe.Fei et al\\.", "year": 2003}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Building machines that learn and think like people", "author": ["Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J"], "venue": "CoRR, abs/1604.00289,", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "arXiv preprint arXiv:1605.06065,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Learning to generate artificial fovea trajectories for target detection", "author": ["Schmidhuber", "Juergen", "Huber", "Rudolf"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Schmidhuber et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1991}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Timothy", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1606.04080,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "CoRR, abs/1605.07146,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Advancing Deep Learning systems to solve Artificial Intelligence tasks requires that models be capable of performing continual and rapid learning (Lake et al., 2016).", "startOffset": 146, "endOffset": 165}, {"referenceID": 13, "context": "But top-down hierarchical designs of models to perform such tasks are not very successful on real world data and there are many reasons for this (Santoro et al., 2016).", "startOffset": 145, "endOffset": 167}, {"referenceID": 7, "context": "The attention mechanism we use is incrementally derived from zoomable and differential image observation mechanism of DRAW Gregor et al. (2015). The attention window is defined by an N \u00d7 N 2D grid of Cauchy kernels.", "startOffset": 123, "endOffset": 144}, {"referenceID": 9, "context": "Omniglot is a dataset by (Lake et al., 2015) that is specially designed to compare and contrast the learning abilities of humans and machines.", "startOffset": 25, "endOffset": 44}, {"referenceID": 5, "context": "The particular model under consideration has an LSTM controller (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000).", "startOffset": 115, "endOffset": 134}, {"referenceID": 9, "context": "Out of the 50 alphabets provided in the dataset, 30 were used for training and validation and the last 20 for testing, as per the Omniglot usage protocol followed by (Lake et al., 2015).", "startOffset": 166, "endOffset": 185}, {"referenceID": 7, "context": "This is quite similar to strategies found in other sequential attentive models like DRAW (Gregor et al., 2015).", "startOffset": 89, "endOffset": 110}, {"referenceID": 16, "context": "A classic example is of such learning behaviour is that of a human kid learning about the animal giraffe (Vinyals et al., 2016).", "startOffset": 105, "endOffset": 127}, {"referenceID": 13, "context": "One way of training such systems is to have a meta learning system where in the model is trained to represent entities in space (rather than being trained to represent a entity) (Schaul & Schmidhuber, 2010) (Santoro et al., 2016).", "startOffset": 207, "endOffset": 229}, {"referenceID": 16, "context": "A similar method is also used in Matching Networks (Vinyals et al., 2016).", "startOffset": 51, "endOffset": 73}, {"referenceID": 9, "context": "(Lake et al., 2015) developed a dedicated system for such rapid knowledge acquisition called Bayesian Programming Learning, which surpasses human performance and is the current state of the art of all methods.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "Across Alphabets: Many papers recently, like Matching Networks (Vinyals et al., 2016) and MANNs (Santoro et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 13, "context": ", 2016) and MANNs (Santoro et al., 2016) have used 1200 chars for background set (instead of 964 specified by (Lake et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 9, "context": ", 2016) have used 1200 chars for background set (instead of 964 specified by (Lake et al., 2015)).", "startOffset": 77, "endOffset": 96}, {"referenceID": 16, "context": "Recently, Vinyals et al. (2016) introduced a One Shot learning task taken based off of the popular ImageNet dataset and uses a testing protocol similar to Omniglot.", "startOffset": 10, "endOffset": 32}, {"referenceID": 7, "context": "A specialised form of location based soft attention mechanism, well suited for 2D images was developed for the DRAW architecture (Gregor et al., 2015), and this forms the basis of our attention mechanism in ARC.", "startOffset": 129, "endOffset": 150}, {"referenceID": 10, "context": "Hard Attention mechanism based on Reinforcement Learning was used in Mnih et al. (2014) and further extended to multiple objects in Ba et al.", "startOffset": 69, "endOffset": 88}, {"referenceID": 0, "context": "(2014) and further extended to multiple objects in Ba et al. (2014); both of these models showed that the computation required at inference is significantly less compared to highly parallel Convolutional Networks, while still achieving good performance.", "startOffset": 51, "endOffset": 68}, {"referenceID": 0, "context": "(2014) and further extended to multiple objects in Ba et al. (2014); both of these models showed that the computation required at inference is significantly less compared to highly parallel Convolutional Networks, while still achieving good performance. A soft or differentiable attention mechanisms have been used in Graves (2013). A specialised form of location based soft attention mechanism, well suited for 2D images was developed for the DRAW architecture (Gregor et al.", "startOffset": 51, "endOffset": 332}, {"referenceID": 16, "context": "Matching Networks (Vinyals et al., 2016) and Memory Augmented Neural Networks (Santoro et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 13, "context": ", 2016) and Memory Augmented Neural Networks (Santoro et al., 2016) are other approaches to perform continual or meta learning in the low data regime.", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": "A survey of the methods and importance of measuring similarity of samples in Machine Learning is presented in Bellet et al. (2013). With respect to deep learning methods, the most popular architecture family is that of Siamese Networks (Bromley et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": "A survey of the methods and importance of measuring similarity of samples in Machine Learning is presented in Bellet et al. (2013). With respect to deep learning methods, the most popular architecture family is that of Siamese Networks (Bromley et al., 1993). The energy based derivation of the same is presented in Chopra et al. (2005). A bayesian framework for one shot visual recognition was presented in Fe-Fei et al.", "startOffset": 110, "endOffset": 337}, {"referenceID": 1, "context": "A survey of the methods and importance of measuring similarity of samples in Machine Learning is presented in Bellet et al. (2013). With respect to deep learning methods, the most popular architecture family is that of Siamese Networks (Bromley et al., 1993). The energy based derivation of the same is presented in Chopra et al. (2005). A bayesian framework for one shot visual recognition was presented in Fe-Fei et al. (2003). Lake et al.", "startOffset": 110, "endOffset": 429}, {"referenceID": 1, "context": "A survey of the methods and importance of measuring similarity of samples in Machine Learning is presented in Bellet et al. (2013). With respect to deep learning methods, the most popular architecture family is that of Siamese Networks (Bromley et al., 1993). The energy based derivation of the same is presented in Chopra et al. (2005). A bayesian framework for one shot visual recognition was presented in Fe-Fei et al. (2003). Lake et al. (2015) extensively study One Shot Learning and present a novel probabilistic framework called Hierarchical Bayesian Program Learning (HBPL) for rapid learning.", "startOffset": 110, "endOffset": 449}], "year": 2017, "abstractText": "Rapid and continual learning models require that the representation space they use be dynamic and constantly changing as the model encounters new evidence. While recently there have been many end-to-end, meta-learning based approaches, they have significant drawbacks. We present a novel model that crudely approximates having a dynamic representation space at inference time. The entire representation space is defined relative to the test example and the entire context of this relative representation space is considered before the model makes a prediction. We extensively test all aspects of our model across various real world datasets. In the challenging task of one-shot learning on the Omniglot dataset, our model achieves the first superhuman performance for a neural method with an error rate of 1.5%.", "creator": "LaTeX with hyperref package"}}}