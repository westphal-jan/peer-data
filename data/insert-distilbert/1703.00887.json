{"id": "1703.00887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "How to Escape Saddle Points Efficiently", "abstract": "this paper shows that a perturbed form of gradient descent converges to a second - order stationary point consisting in a number iterations which depends only one poly - logarithmically on dimension ( i. e., it is almost \" dimension - dependent free \" ). the convergence rate of this procedure matches the well - known convergence rate of gradient descent to infinitely first - order stationary points, particularly up to log factors. when all saddle points are non - degenerate, all second - order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. our systematic results can be later directly applied here to many machine learning applications, including deep learning. as a particular concrete example of such an application, we show that our results can be used directly to later establish sharp global convergence rates for matrix factorization. our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to match the non - convex optimization community.", "histories": [["v1", "Thu, 2 Mar 2017 18:35:24 GMT  (693kb)", "http://arxiv.org/abs/1703.00887v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["chi jin", "rong ge 0001", "praneeth netrapalli", "sham m kakade", "michael i jordan"], "accepted": true, "id": "1703.00887"}, "pdf": {"name": "1703.00887.pdf", "metadata": {"source": "CRF", "title": "How to Escape Saddle Points Efficiently", "authors": ["Chi Jin", "Rong Ge", "Praneeth Netrapalli", "Sham M. Kakade", "Michael I. Jordan"], "emails": ["chijin@cs.berkeley.edu", "rongge@cs.duke.edu", "praneeth@microsoft.com", "sham@cs.washington.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n00 88\n7v 1\n[ cs\n.L G\n] 2\nM ar\nOur results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community."}, {"heading": "1 Introduction", "text": "Given a function f : Rd \u2192 R, gradient descent aims to minimize the function via the following iteration: xt+1 = xt \u2212 \u03b7\u2207f(xt), where \u03b7 > 0 is a step size. Gradient descent and its variants (e.g., stochastic gradient) are widely used in machine learning applications due to their favorable computational properties. This is notably true in the deep learning setting, where gradients can be computed efficiently via backpropagation [Rumelhart et al., 1988].\nGradient descent is especially useful in high-dimensional settings because the number of iterations required to reach a point with small gradient is independent of the dimension (\u201cdimensionfree\u201d). More precisely, for a function that is \u2113-gradient Lipschitz (see Definition 1), it is well known that gradient descent finds an \u01eb-first-order stationary point (i.e., a point x with \u2016\u2207f(x)\u2016 \u2264 \u01eb)\n\u2217University of California, Berkeley. Email: chijin@cs.berkeley.edu \u2020Duke University. Email: rongge@cs.duke.edu \u2021Microsoft Research, India. Email: praneeth@microsoft.com \u00a7University of Washington. Email: sham@cs.washington.edu \u00b6University of California, Berkeley. Email: jordan@cs.berkeley.edu\nwithin \u2113(f(x0)\u2212 f\u22c6)/\u01eb2 iterations [Nesterov, 1998], where x0 is the initial point and f\u22c6 is the optimal value of f . This bound does not depend on the dimension of x. In convex optimization, finding an \u01eb-first-order stationary point is equivalent to finding an approximate global optimum.\nIn non-convex settings, however, convergence to first-order stationary points is not satisfactory. For non-convex functions, first-order stationary points can be global minima, local minima, saddle points or even local maxima. Finding a global minimum can be hard, but fortunately, for many non-convex problems, it is sufficient to find a local minimum. Indeed, a line of recent results show that, in many problems of interest, either all local minima are global minima (e.g., in tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al., 2016a], phase retrieval [Sun et al., 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014].\nOn the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2). Lee et al. [2016] proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding noise. However, this result does not bound the number of steps needed to reach a local minimum.\nThough these results establish that gradient descent can find local minima in a polynomial number of iterations, they are still far from being efficient. For instance, the number of iterations required in Ge et al. [2015] is at least \u2126(d4), where d is the underlying dimension. This is significantly suboptimal compared to rates of convergence to first-order stationary points, where the iteration complexity is dimension-free. This motivates the following question: Can gradient descent escape saddle points and converge to local minima in a number of iterations that is (almost) dimension-free?\nIn order to answer this question formally, this paper investigates the complexity of finding \u01ebsecond-order stationary points. For \u03c1-Hessian Lipschitz functions (see Definition 5), these points are defined as [Nesterov and Polyak, 2006]:\n\u2016\u2207f(x)\u2016 \u2264 \u01eb, and \u03bbmin(\u22072f(x)) \u2265 \u2212 \u221a \u03c1\u01eb.\nUnder the assumption that all saddle points are strict (i.e., for any saddle point xs, \u03bbmin(\u22072f(xs)) < 0), all second-order stationary points (\u01eb = 0) are local minima. Therefore, convergence to secondorder stationary points is equivalent to convergence to local minima.\nThis paper studies gradient descent with phasic perturbations (see Algorithm 1). For \u2113-smooth functions that are also Hessian Lipschitz, we show that perturbed gradient descent will converge to an \u01eb-second-order stationary point in O\u0303(\u2113(f(x0) \u2212 f\u22c6)/\u01eb2), where O\u0303(\u00b7) hides polylog factors. This guarantee is almost dimension free (up to polylog(d) factors), answering the above highlighted question affirmatively. Note that this rate is exactly the same as the well-known convergence rate\nAlgorithm 1 Perturbed Gradient Descent (Meta-algorithm)\nfor t = 0, 1, . . . do if perturbation condition holds then\nxt \u2190 xt + \u03bet, \u03bet uniformly \u223c B0(r) xt+1 \u2190 xt \u2212 \u03b7\u2207f(xt)\nof gradient descent to first-order stationary points [Nesterov, 1998], up to log factors. Furthermore, our analysis admits a maximal step size of up to \u2126(1/\u2113), which is the same as that in analyses for first-order stationary points.\nAs many real learning problems present strong local geometric properties, similar to strong convexity in the global setting [see, e.g. Bhojanapalli et al., 2016, Sun and Luo, 2016, Zheng and Lafferty, 2016], it is important to note that our analysis naturally takes advantage of such local structure. We show that when local strong convexity is present, the \u01eb-dependence goes from a polynomial rate, 1/\u01eb2, to linear convergence, log(1/\u01eb). As an example, we show that sharp global convergence rates can be obtained for matrix factorization as a direct consequence of our analysis."}, {"heading": "1.1 Our Contributions", "text": "This paper presents the first sharp analysis that shows that (perturbed) gradient descent finds an approximate second-order stationary point in at most polylog(d) iterations, thus escaping all saddle points efficiently. Our main technical contributions are as follows:\n\u2022 For \u2113-gradient Lipschitz, \u03c1-Hessian Lipschitz functions (possibly non-convex), gradient descent with appropriate perturbations finds an \u01eb-second-order stationary point in O\u0303(\u2113(f(x0)\u2212f\u22c6)/\u01eb2) iterations. This rate matches the well-known convergence rate of gradient descent to firstorder stationary points up to log factors.\n\u2022 Under a strict-saddle condition (see Assumption A2), this convergence result directly applies for finding local minima. This means that gradient descent can escape all saddle points with only logarithmic overhead in runtime.\n\u2022 When the function has local structure, such as local strong convexity (see Assumption A3.a), the above results can be further improved to linear convergence. We give sharp rates that are comparable to previous problem-specific local analysis of gradient descent with smart initialization (see Section 1.2).\n\u2022 All the above results rely on a new characterization of the geometry around saddle points: points from where gradient descent gets stuck at a saddle point constitute a thin \u201cband.\u201d We develop novel techniques to bound the volume of this band. As a result, we can show that after a random perturbation the current point is very unlikely to be in the \u201cband\u201d; hence, efficient escape from the saddle point is possible (see Section 5)."}, {"heading": "1.2 Related Work", "text": "Over the past few years, there have been many problem-specific convergence results for nonconvex optimization. One line of work requires a smart initialization algorithm to provide a coarse estimate lying inside a local neighborhood, from which popular local search algorithms\nenjoy fast local convergence [see, e.g., Netrapalli et al., 2013, Candes et al., 2015, Sun and Luo, 2016, Bhojanapalli et al., 2016]. While there are not many results that show global convergence for non-convex problems, Jain et al. [2015] show that gradient descent yields global convergence rates for matrix square-root problems. Although these results give strong guarantees, the analyses are heavily tailored to specific problems, and it is unclear how to generalize them to a wider class of non-convex functions.\nFor general non-convex optimization, there are a few previous results on finding second-order stationary points. These results can be divided into the following three categories, where, for simplicity of presentation, we only highlight dependence on dimension d and \u01eb, assuming that all other problem parameters are constant from the point of view of iteration complexity: Hessian-based: Traditionally, only second-order optimization methods were known to converge to second-order stationary points. These algorithms rely on computing the Hessian to distinguish between first- and second-order stationary points. Nesterov and Polyak [2006] designed a cubic regularization algorithm which converges to an \u01eb-second-order stationary point in O(1/\u01eb1.5) iterations. Trust region algorithms [Curtis et al., 2014] can also achieve the same performance if the parameters are chosen carefully. These algorithms typically require the computation of the inverse of the full Hessian per iteration, which can be very expensive. Hessian-vector-product-based: A number of recent papers have explored the possibility of using only Hessian-vector products instead of full Hessian information in order to find second-order stationary points. These algorithms require a Hessian-vector product oracle: given a function f , a point x and a direction u, the oracle returns \u22072f(x) \u00b7 u. Agarwal et al. [2016] and Carmon et al. [2016] presented accelerated algorithms that can find an \u01eb-second-order stationary point in O(log d/\u01eb7/4) steps. Also, Carmon and Duchi [2016] showed by running gradient descent as a subroutine to solve the subproblem of cubic regularization (which requires Hessian-vector product oracle), it is possible to find an \u01eb-second-order stationary pointin O(log d/\u01eb2) iterations. In many applications such an oracle can be implemented efficiently, in roughly the same complexity as the gradient oracle. Also, when the function has a Hessian Lipschitz property such an oracle can be approximated by differ-\nentiating the gradients at two very close points (although this may suffer from numerical issues, thus is seldom used in practice). Gradient-based: Another recent line of work shows that it is possible to converge to a secondorder stationary point without any use of the Hessian. These methods feature simple computation per iteration (only involving gradient operations), and are closest to the algorithms used in practice. Ge et al. [2015] showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/\u01eb) iterations, with polynomial of order at least four. This was improved in Levy [2016] to O(d3 \u00b7poly(1/\u01eb)) using normalized gradient descent. The current paper improves on both results by showing that perturbed gradient descent can actually find an \u01eb-second-order stationary point inO(polylog(d)/\u01eb2) steps, which matches the guarantee for converging to first-order stationary points up to polylog factors."}, {"heading": "2 Preliminaries", "text": "In this section, we will first introduce our notation, and then present some definitions and existing results in optimization which will be used later."}, {"heading": "2.1 Notation", "text": "We use bold upper-case letters A,B to denote matrices and bold lower-case letters x,y to denote vectors. Aij means the (i, j)\nth entry of matrix A. For vectors we use \u2016\u00b7\u2016 to denote the \u21132-norm, and for matrices we use \u2016\u00b7\u2016 and \u2016\u00b7\u2016F to denote spectral norm and Frobenius norm respectively. We use \u03c3max(\u00b7), \u03c3min(\u00b7), \u03c3i(\u00b7) to denote the largest, the smallest and the i-th largest singular values respectively, and \u03bbmax(\u00b7), \u03bbmin(\u00b7), \u03bbi(\u00b7) for corresponding eigenvalues.\nFor a function f : Rd \u2192 R, we use \u2207f(\u00b7) and \u22072f(\u00b7) to denote its gradient and Hessian, and f\u22c6 to denote the global minimum of f(\u00b7). We use notation O(\u00b7) to hide only absolute constants which do not depend on any problem parameter, and notation O\u0303(\u00b7) to hide only absolute constants and log factors. We let B (d) x (r) denote the d-dimensional ball centered at x with radius r; when it is clear from context, we simply denote it as Bx(r). We use PX (\u00b7) to denote projection onto the set X . Distance and projection are always defined in a Euclidean sense."}, {"heading": "2.2 Gradient Descent", "text": "The theory of gradient descent often takes its point of departure to be the study of convex minimization where the function is both \u2113-smooth and \u03b1-strongly convex:\nDefinition 1. A differentiable function f(\u00b7) is \u2113-smooth (or \u2113-gradient Lipschitz) if:\n\u2200x1,x2, \u2016\u2207f(x1)\u2212\u2207f(x2)\u2016 \u2264 \u2113\u2016x1 \u2212 x2\u2016.\nDefinition 2. A twice-differentiable function f(\u00b7) is \u03b1-strongly convex if \u2200x, \u03bbmin(\u22072f(x)) \u2265 \u03b1\nSuch smoothness guarantees imply that the gradient can not change too rapidly, and strong convexity ensures that there is a unique stationary point (and hence a global minimum). Standard analysis using these two properties shows that gradient descent converges linearly to a global optimum x\u22c6 (see e.g. [Bubeck et al., 2015]).\nTheorem 1. Assume f(\u00b7) is \u2113-smooth and \u03b1-strongly convex. For any \u01eb > 0, if we run gradient descent with step size \u03b7 = 1\u2113 , iterate xt will be \u01eb-close to x \u22c6 in iterations:\n2\u2113 \u03b1 log \u2016x0 \u2212 x\u22c6\u2016 \u01eb\nIn a more general setting, we no longer have convexity, let alone strong convexity. Though global optima are difficult to achieve in such a setting, it is possible to analyze convergence to first-order stationary points.\nDefinition 3. For a differentiable function f(\u00b7), we say that x is a first-order stationary point if \u2016\u2207f(x)\u2016 = 0; we also say x is an \u01eb-first-order stationary point if \u2016\u2207f(x)\u2016 \u2264 \u01eb.\nUnder an \u2113-smoothness assumption, it is well known that by choosing the step size \u03b7 = 1\u2113 , gradient descent converges to first-order stationary points.\nTheorem 2 ([Nesterov, 1998]). Assume that the function f(\u00b7) is \u2113-smooth. Then, for any \u01eb > 0, if we run gradient descent with step size \u03b7 = 1\u2113 and termination condition \u2016\u2207f(x)\u2016 \u2264 \u01eb, the output will be \u01eb-first-order stationary point, and the algorithm will terminate within the following number of iterations:\n\u2113(f(x0)\u2212 f\u22c6) \u01eb2 .\nNote that the iteration complexity does not depend explicitly on intrinsic dimension; in the literature this is referred to as \u201cdimension-free optimization.\u201d\nA first-order stationary point can be either a local minimum or a saddle point or a local maximum. For minimization problems, saddle points and local maxima are undesirable, and we abuse nomenclature to call both of them \u201csaddle points\u201d in this paper. The formal definition is as follows:\nDefinition 4. For a differentiable function f(\u00b7), we say that x is a local minimum if x is a first-order stationary point, and there exists \u01eb > 0 so that for any y in the \u01eb-neighborhood of x, we have f(x) \u2264 f(y); we also say x is a saddle point if x is a first-order stationary point but not a local minimum. For a twice-differentiable function f(\u00b7), we further say a saddle point x is strict (or non-degenerate) if \u03bbmin(\u22072f(x)) < 0.\nFor a twice-differentiable function f(\u00b7), we know a saddle point x must satify \u03bbmin(\u22072f(x)) \u2264 0. Intuitively, for saddle point x to be strict, we simply rule out the undetermined case \u03bbmin(\u22072f(x)) = 0, where Hessian information alone is not enough to check whether x is a local minimum or saddle point. In most non-convex problems, saddle points are undesirable.\nTo escape from saddle points and find local minima in a general setting, we move both the assumptions and guarantees in Theorem 2 one order higher. In particular, we require the Hessian to be Lipschitz:\nDefinition 5. A twice-differentiable function f(\u00b7) is \u03c1-Hessian Lipschitz if:\n\u2200x1,x2, \u2016\u22072f(x1)\u2212\u22072f(x2)\u2016 \u2264 \u03c1\u2016x1 \u2212 x2\u2016.\nThat is, Hessian can not change dramatically in terms of spectral norm. We also generalize the definition of first-order stationary point to higher order:\nAlgorithm 2 Perturbed Gradient Descent: PGD(x0, \u2113, \u03c1, \u01eb, c, \u03b4,\u2206f )\n\u03c7 \u2190 3max{log(d\u2113\u2206fc\u01eb2\u03b4 ), 4}, \u03b7 \u2190 c \u2113 , r \u2190\n\u221a c \u03c72 \u00b7 \u01eb \u2113 , gthres \u2190 \u221a c \u03c72 \u00b7 \u01eb, fthres \u2190 c \u03c73 \u00b7 \u221a \u01eb3 \u03c1 , tthres \u2190 \u03c7 c2 \u00b7 \u2113\u221a \u03c1\u01eb\ntnoise \u2190 \u2212tthres \u2212 1 for t = 0, 1, . . . do\nif \u2016\u2207f(xt)\u2016 \u2264 gthres and t\u2212 tnoise > tthres then x\u0303t \u2190 xt, tnoise \u2190 t xt \u2190 x\u0303t + \u03bet, \u03bet uniformly \u223c B0(r) if t\u2212 tnoise = tthres and f(xt)\u2212 f(x\u0303tnoise) > \u2212fthres then return x\u0303tnoise xt+1 \u2190 xt \u2212 \u03b7\u2207f(xt)\nDefinition 6. For a \u03c1-Hessian Lipschitz function f(\u00b7), we say that x is a second-order stationary point if \u2016\u2207f(x)\u2016 = 0 and \u03bbmin(\u22072f(x)) \u2265 0; we also say x is \u01eb-second-order stationary point if:\n\u2016\u2207f(x)\u2016 \u2264 \u01eb, and \u03bbmin(\u22072f(x)) \u2265 \u2212 \u221a \u03c1\u01eb\nSecond-order stationary points are very important in non-convex optimization because when all saddle points are strict, all second-order stationary points are exactly local minima.\nNote that the literature sometime defines \u01eb-second-order stationary point by two independent error terms; i.e., letting \u2016\u2207f(x)\u2016 \u2264 \u01ebg and \u03bbmin(\u22072f(x)) \u2265 \u2212\u01ebH . We instead follow the convention of Nesterov and Polyak [2006] by choosing \u01ebH = \u221a \u03c1\u01ebg to reflect the natural relations between the gradient and the Hessian. This definition of \u01eb-second-order stationary point can also differ by reparametrization (and scaling), e.g. Nesterov and Polyak [2006] use \u01eb\u2032 = \u221a\n\u01eb/\u03c1. We choose our parametrization so that the first requirement of \u01eb-second-order stationary point coincides with the requirement of \u01eb-first-order stationary point, for a fair comparison of our result with Theorem 2."}, {"heading": "3 Main Result", "text": "In this section we show that it possible to modify gradient descent in a simple way so that the resulting algorithm will provably converge quickly to a second-order stationary point.\nThe algorithm that we analyze is a perturbed form of gradient descent (see Algorithm 2). The algorithm is based on gradient descent with step size \u03b7. When the norm of the current gradient is small (\u2264 gthres) (which indicates that the current iterate x\u0303t is potentially near a saddle point), the algorithm adds a small random perturbation to the gradient. The perturbation is added at most only once every tthres iterations.\nTo simplify the analysis we choose the perturbation \u03bet to be uniformly sampled from a ddimensional ball1. The use of the threshold tthres ensures that the dynamics are mostly those of gradient descent. If the function value does not decrease enough (by fthres) after tthres iterations, the algorithm outputs x\u0303tnoise . The analysis in this section shows that under this protocol, the output x\u0303tnoise is necessarily \u201cclose\u201d to a second-order stationary point.\nWe first state the assumptions that we require.\n1Note that uniform sampling from a d-dimensional ball can be done efficiently by sampling U 1\nd \u00d7 Y \u2016Y\u2016 where\nU \u223c Uniform([0, 1]) and Y \u223c N (0, Id) [Harman and Lacko, 2010].\nAssumption A1. Function f(\u00b7) is both \u2113-smooth and \u03c1-Hessian Lipschitz.\nThe Hessian Lipschitz condition ensures that the function is well-behaved near a saddle point, and the small perturbation we add will suffice to allow the subsequent gradient updates to escape from the saddle point. More formally, we have:\nTheorem 3. Assume that f(\u00b7) satisfies A1. Then there exists an absolute constant cmax such that, for any \u03b4 > 0, \u01eb \u2264 \u21132\u03c1 , \u2206f \u2265 f(x0)\u2212 f\u22c6, and constant c \u2264 cmax, PGD(x0, \u2113, \u03c1, \u01eb, c, \u03b4,\u2206f ) will output an \u01eb-second-order stationary point, with probability 1\u2212 \u03b4, and terminate in the following number of iterations:\nO\n( \u2113(f(x0)\u2212 f\u22c6)\n\u01eb2 log4 ( d\u2113\u2206f \u01eb2\u03b4 )) .\nStrikingly, Theorem 3 shows that perturbed gradient descent finds a second-order stationary point in almost the same amount of time that gradient descent takes to find first-order stationary point. The step size \u03b7 is chosen as O(1/\u2113) which is in accord with classical analyses of convergence to first-order stationary points. Though we state the theorem with a certain choice of parameters for simplicity of presentation, our result holds even if we vary the parameters up to constant factors.\nWithout loss of generality, we can focus on the case \u01eb \u2264 \u21132/\u03c1, as in Theorem 3. This is because in the case \u01eb > \u21132/\u03c1, standard gradient descent without perturbation\u2014Theorem 2\u2014easily solves the problem (since by A1, we always have \u03bbmin(\u22072f(x)) \u2265 \u2212\u2113 \u2265 \u2212 \u221a \u03c1\u01eb, which means that all \u01eb-second-order stationary points are \u01eb-first order stationary points). We believe that the dependence on at least one log d factor in the iteration complexity is unavoidable in the non-convex setting, as our result can be directly applied to the principal component analysis problem, for which the best known runtimes (for the power method or Lanczos method) incur a log d factor. Establishing this formally is still an open question however.\nTo provide some intuition for Theorem 3, consider an iterate xt which is not yet an \u01eb-secondorder stationary point. By definition, either (1) the gradient \u2207f(xt) is large, or (2) the Hessian \u22072f(xt) has a significant negative eigenvalue. Traditional analysis works in the first case. The crucial step in the proof of Theorem 3 involves handling the second case: when the gradient is small \u2016\u2207f(xt)\u2016 \u2264 gthres and the Hessian has a significant negative eigenvalue \u03bbmin(\u22072f(x\u0303t)) \u2264 \u2212 \u221a \u03c1\u01eb, then adding a perturbation, followed by standard gradient descent for tthres steps, decreases the function value by at least fthres, with high probability. The proof of this fact relies on a novel characterization of geometry around saddle points (see Section 5)\nIf we are able to make stronger assumptions on the objective function we are able to strengthen our main result. This further analysis is presented in the next section."}, {"heading": "3.1 Functions with Strict Saddle Property", "text": "In many real applications, objective functions further admit the property that all saddle points are strict [Ge et al., 2015, Sun et al., 2016a,b, Bhojanapalli et al., 2016, Ge et al., 2016]. In this case, all second-order stationary points are local minima and hence convergence to second-order stationary points (Theorem 3) is equivalent to convergence to local minima.\nTo state this result formally, we introduce a robust version of the strict saddle property [cf. Ge et al., 2015]:\nAssumption A2. Function f(\u00b7) is (\u03b8, \u03b3, \u03b6)-strict saddle. That is, for any x, at least one of following holds:\n\u2022 \u2016\u2207f(x)\u2016 \u2265 \u03b8.\n\u2022 \u03bbmin(\u22072f(x)) \u2264 \u2212\u03b3.\n\u2022 x is \u03b6-close to X \u22c6 \u2014 the set of local minima.\nIntuitively, the strict saddle assumption states that the Rd space can be divided into three regions: 1) a region where the gradient is large; 2) a region where the Hessian has a significant negative eigenvalue (around saddle point); and 3) the region close to a local minimum. With this assumption, we immediately have the following corollary:\nCorollary 4. Let f(\u00b7) satisfy A1 and A2. Then, there exists an absolute constant cmax such that, for any \u03b4 > 0,\u2206f \u2265 f(x0)\u2212f\u22c6, constant c \u2264 cmax, and letting \u01eb\u0303 = min(\u03b8, \u03b32/\u03c1), PGD(x0, \u2113, \u03c1, \u01eb\u0303, c, \u03b4,\u2206f ) will output a point \u03b6-close to X \u22c6, with probability 1\u2212 \u03b4, and terminate in the following number of iterations:\nO\n( \u2113(f(x0)\u2212 f\u22c6)\n\u01eb\u03032 log4 ( d\u2113\u2206f \u01eb\u03032\u03b4 )) .\nCorollary 4 shows that by substituting \u01eb in Theorem 3 using \u01eb\u0303 = min(\u03b8, \u03b32/\u03c1), the output of perturbed gradient descent will be in the \u03b6-neighborhood of some local minimum.\nNote although Corollary 4 only explicitly asserts that the output will lie within some fixed radius \u03b6 from a local minimum. In many real applications, we can further write \u03b6 as a function \u03b6(\u00b7) of gradient threshold \u03b8, so that when \u03b8 decreases, \u03b6(\u03b8) decreases linearly or polynomially depending on \u03b8. Meanwhile, parameter \u03b3 is always nondecreasing when \u03b8 decreases due to the nature of this strict saddle definition. Therefore, in these cases, the above corollary further gives a convergence rate to a local minimum."}, {"heading": "3.2 Functions with Strong Local Structure", "text": "The convergence rate in Theorem 3 is polynomial in \u01eb, which is similar to that of Theorem 2, but is worse than the rate of Theorem 1 because of the lack of strong convexity. Although global strong convexity does not hold in the non-convex setting that is our focus, in many machine learning problems the objective function may have a favorable local structure in the neighborhood of local minima [Ge et al., 2015, Sun et al., 2016a,b, Sun and Luo, 2016]. Exploiting this property can lead to much faster convergence (linear convergence) to local minima. One such property that ensures such convergence is a local form of smoothness and strong convexity:\nAssumption A3.a. In a \u03b6-neighborhood of the set of local minima X \u22c6, the function f(\u00b7) is \u03b1strongly convex, and \u03b2-smooth.\nHere we use different letter \u03b2 to denote the local smoothness parameter (in contrast to the global smoothness parameter \u2113). Note that we always have \u03b2 \u2264 \u2113. However, often even local \u03b1-strong convexity does not hold. We thus introduce the following relaxation:\nAssumption A3.b. In a \u03b6-neighborhood of the set of local minima X \u22c6, the function f(\u00b7) satisfies a (\u03b1, \u03b2)-regularity condition if for any x in this neighborhood:\n\u3008\u2207f(x),x\u2212 PX \u22c6(x)\u3009 \u2265 \u03b1\n2 \u2016x\u2212 PX \u22c6(x)\u20162 +\n1\n2\u03b2 \u2016\u2207f(x)\u20162. (1)\nAlgorithm 3 Perturbed Gradient Descent with Local Improvement: PGDli(x0, \u2113, \u03c1, \u01eb, c, \u03b4,\u2206f , \u03b2)\nx0 \u2190 PGD(x0, \u2113, \u03c1, \u01eb, c, \u03b4,\u2206f ) for t = 0, 1, . . . do\nxt+1 \u2190 xt \u2212 1\u03b2\u2207f(xt)\nHere PX \u22c6(\u00b7) is the projection on to the set X \u22c6. Note (\u03b1, \u03b2)-regularity condition is more general and is directly implied by standard \u03b2-smooth and \u03b1-strongly convex conditions. This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. [2016], Zheng and Lafferty [2016], where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.\nGradient descent naturally exploits local structure very well. In Algorithm 3, we first run Algorithm 2 to output a point within the neighborhood of a local minimum, and then perform standard gradient descent with step size 1\u03b2 . We can then prove the following theorem: Theorem 5. Let f(\u00b7) satisfy A1, A2, and A3.a (or A3.b). Then there exists an absolute constant cmax such that, for any \u03b4 > 0, \u01eb > 0,\u2206f \u2265 f(x0) \u2212 f\u22c6, constant c \u2264 cmax, and letting \u01eb\u0303 = min(\u03b8, \u03b32/\u03c1), PGDli(x0, \u2113, \u03c1, \u01eb\u0303, c, \u03b4,\u2206f , \u03b2) will output a point that is \u01eb-close to X \u22c6, with probability 1\u2212 \u03b4, in the following number of iterations:\nO\n( \u2113(f(x0)\u2212 f\u22c6)\n\u01eb\u03032 log4 ( d\u2113\u2206f \u01eb\u03032\u03b4 ) + \u03b2 \u03b1 log \u03b6 \u01eb ) .\nTheorem 5 says that if strong local structure is present, the convergence rate can be boosted to linear convergence (log 1\u01eb ). In this theorem we see that sequence of iterations can be decomposed into two phases. In the first phase, perturbed gradient descent finds a \u03b6-neighborhood by Corollary 4. In the second phase, standard gradient descent takes us from \u03b6 to \u01eb-close to a local minimum. Standard gradient descent and Assumption A3.a (or A3.b) make sure that the iterate never steps out of a \u03b6-neighborhood in this second phase, giving a result similar to Theorem 1 with linear convergence.\nFinally, we note our choice of local conditions (Assumption A3.a and A3.b) are not special. The interested reader can refer to Karimi et al. [2016] for other relaxed and alternative notions of convexity, which can also be potentially combined with Assumptions A1andA2 to yield convergence results of a similar flavor as that of Theorem 5."}, {"heading": "4 Example \u2014 Matrix Factorization", "text": "As a simple example to illustrate how to apply our general theorems to specific non-convex optimization problems, we consider a symmetric low-rank matrix factorization problem, based on the following objective function:\nmin U\u2208Rd\u00d7r\nf(U) = 1 2 \u2016UU\u22a4 \u2212M\u22c6\u20162F, (2)\nwhere M\u22c6 \u2208 Rd\u00d7d. For simplicity, we assume rank(M\u22c6) = r, and denote \u03c3\u22c61 := \u03c31(M\u22c6), \u03c3\u22c6r := \u03c3r(M\n\u22c6). Clearly, in this case the global minimum of function value is zero, which is achieved at V\u22c6 = TD1/2 where TDT\u22a4 is the SVD of the symmetric real matrix M\u22c6.\nThe following two lemmas show that the objective function in Eq. (2) satisfies the geometric assumptions A1, A2,and A3.b. Moreover, all local minima are global minima.\nLemma 6. For any \u0393 \u2265 \u03c3\u22c61, the function f(U) defined in Eq. (2) is 8\u0393-smooth and 12\u03931/2-Hessian Lipschitz, inside the region {U|\u2016U\u20162 < \u0393}.\nLemma 7. For function f(U) defined in Eq.(2), all local minima are global minima. The set of global minima is X \u22c6 = {V\u22c6R|RR\u22a4 = R\u22a4R = I}. Furthermore, f(U) satisfies:\n1. ( 124 (\u03c3 \u22c6 r ) 3/2, 13\u03c3 \u22c6 r , 1 3(\u03c3 \u22c6 r ) 1/2)-strict saddle property.\n2. (23\u03c3 \u22c6 r , 10\u03c3 \u22c6 1)-regularity condition in 1 3 (\u03c3 \u22c6 r ) 1/2 neighborhood of X \u22c6.\nOne caveat is that since the objective function is actually a fourth-order polynomial with respect to U, the smoothness and Hessian Lipschitz parameters from Lemma 6 naturally depend on \u2016U\u2016. Fortunately, we can further show that gradient descent (even with perturbation) does not increase \u2016U\u2016 beyond O(max{\u2016U0\u2016, (\u03c3\u22c61)1/2}). Then, applying Theorem 5 gives:\nTheorem 8. There exists an absolute constant cmax such that the following holds. For the objective function in Eq. (2), for any \u03b4 > 0 and constant c \u2264 cmax, and for \u03931/2 := 2max{\u2016U0\u2016, 3(\u03c3\u22c61)1/2}, the output of PGDli(U0, 8\u0393, 12\u0393 1/2, (\u03c3 \u22c6 r ) 2\n108\u03931/2 , c, \u03b4, r\u0393\n2\n2 , 10\u03c3 \u22c6 1), will be \u01eb-close to the global minimum set\nX \u22c6, with probability 1\u2212 \u03b4, after the following number of iterations:\nO\n(\nr\n( \u0393\n\u03c3\u22c6r\n)4 log4 ( d\u0393\n\u03b4\u03c3\u22c6r\n)\n+ \u03c3\u22c61 \u03c3\u22c6r log \u03c3\u22c6r \u01eb\n)\n.\nTheorem 8 establishes global convergence of perturbed gradient descent from an arbitrary initial point U0, including exact saddle points. Suppose we initialize at U0 = 0, then our iteration complexity becomes:\nO ( r(\u03ba\u22c6)4 log4(d\u03ba\u22c6/\u03b4) + \u03ba\u22c6 log(\u03c3\u22c6r/\u01eb) ) ,\nwhere \u03ba\u22c6 = \u03c3\u22c61/\u03c3 \u22c6 r is the condition number of the matrix M \u22c6. We see that in the second phase, when convergence occurs inside the local region, we require O(\u03ba\u22c6 log(\u03c3\u22c6r/\u01eb)) iterations which is the standard local linear rate for gradient descent. In the first phase, to find a neighborhood of the solution, our method requires a number of iterations scaling as O\u0303(r(\u03ba\u22c6)4). We suspect that this strong dependence on condition number arises from our generic assumption that the Hessian Lipschitz is uniformly upper bounded; it may well be the case that this dependence can be reduced in the special case of matrix factorization via a finer analysis of the geometric structure of the problem."}, {"heading": "5 Proof Sketch for Theorem 3", "text": "In this section we will present the key ideas underlying the main result of this paper (Theorem 3). We will first argue the correctness of Theorem 3 given two important intermediate lemmas. Then we turn to the main lemma, which establishes that gradient descent can escape from saddle points quickly. We present full proofs of all these results in Appendix A. Throughout this section, we use \u03b7, r, gthres, fthres and tthres as defined in Algorithm 2."}, {"heading": "5.1 Exploiting Large Gradient or Negative Curvature", "text": "Recall that an \u01eb-second-order stationary point is a point with a small gradient, and where the Hessian does not have a significant negative eigenvalue. Suppose we are currently at an iterate xt that is not an \u01eb-second-order stationary point; i.e., it does not satisfy the above properties. There are two possibilities:\n1. Gradient is large: \u2016\u2207f(xt)\u2016 \u2265 gthres, or 2. Around saddle point: \u2016\u2207f(xt)\u2016 \u2264 gthres and \u03bbmin(\u22072f(xt)) \u2264 \u2212 \u221a \u03c1\u01eb.\nThe following two lemmas address these two cases respectively. They guarantee that perturbed gradient descent will decrease the function value in both scenarios.\nLemma 9 (Gradient). Assume that f(\u00b7) satisfies A1. Then for gradient descent with stepsize \u03b7 < 1\u2113 , we have f(xt+1) \u2264 f(xt)\u2212 \u03b7 2\u2016\u2207f(xt)\u20162.\nLemma 10 (Saddle). (informal) Assume that f(\u00b7) satisfies A1, If xt satisfies \u2016\u2207f(xt)\u2016 \u2264 gthres and \u03bbmin(\u22072f(xt)) \u2264 \u2212 \u221a \u03c1\u01eb, then adding one perturbation step followed by tthres steps of gradient descent, we have f(xt+tthres)\u2212 f(xt) \u2264 \u2212fthres with high probability.\nWe see that Algorithm 2 is designed so that Lemma 10 can be directly applied. According to these two lemmas, perturbed gradient descent will decrease the function value either in the case of a large gradient, or around strict saddle points. Computing the average decrease per step in function value yields the total iteration complexity. Since Algorithm 2 only terminate when the function value decreases too slowly, this guarantees that the output must be \u01eb-second-order stationary point (see Appendix A for formal proofs)."}, {"heading": "5.2 Main Lemma: Escaping from Saddle Points Quickly", "text": "The proof of Lemma 9 is straightforward and follows from traditional analysis. The key technical contribution of this paper is the proof of Lemma 10, which gives a new characterization of the geometry around saddle points.\nConsider a point x\u0303 that satisfies the the preconditions of Lemma 10 (\u2016\u2207f(x\u0303)\u2016 \u2264 gthres and \u03bbmin(\u22072f(x\u0303)) \u2264 \u2212 \u221a \u03c1\u01eb). After adding the perturbation (x0 = x\u0303 + \u03be), we can view x0 as coming from a uniform distribution over Bx\u0303(r), which we call the perturbation ball. We can divide this perturbation ball Bx\u0303(r) into two disjoint regions: (1) an escaping region Xescape which consists of all the points x \u2208 Bx\u0303(r) whose function value decreases by at least fthres after tthres steps; (2) a stuck region Xstuck = Bx\u0303(r)\u2212 Xescape. Our general proof strategy is to show that Xstuck consists of a very small proportion of the volume of perturbation ball. After adding a perturbation to x\u0303, point x0 has a very small chance of falling in Xstuck, and hence will escape from the saddle point efficiently.\nLet us consider the nature of Xstuck. For simplicity, let us imagine that x\u0303 is an exact saddle point whose Hessian has only one negative eigenvalue, and d\u22121 positive eigenvalues. Let us denote the minimum eigenvalue direction as e1. In this case, if the Hessian remains constant (and we have a quadratic function), the stuck region Xstuck consists of points x such that x \u2212 x\u0303 has a small e1 component. This is a straight band in two dimensions and a flat disk in high dimensions. However, when the Hessian is not constant, the shape of the stuck region is distorted. In two dimensions, it\nforms a \u201cnarrow band\u201d as plotted in Figure 2 on top of the gradient flow. In three dimensions, it forms a \u201cthin pancake\u201d as shown in Figure 1.\nThe major challenge here is to bound the volume of this high-dimensional non-flat \u201cpancake\u201d shaped region Xstuck. A crude approximation of this \u201cpancake\u201d by a flat \u201cdisk\u201d loses polynomial factors in the dimensionalilty, which gives a suboptimal rate. Our proof relies on the following crucial observation: Although we do not know the explicit form of the stuck region, we know it must be very \u201cthin,\u201d therefore it cannot have a large volume. The informal statement of the lemma is as follows:\nLemma 11. (informal) Suppose x\u0303 satisfies the precondition of Lemma 10, and let e1 be the smallest eigendirection of \u22072f(x\u0303). For any \u03b4 \u2208 (0, 1/3] and any two points w,u \u2208 Bx\u0303(r), if w \u2212 u = \u00b5re1 and \u00b5 \u2265 \u03b4/(2 \u221a d), then at least one of w,u is not in the stuck region Xstuck.\nUsing this lemma it is not hard to bound the volume of the stuck region: we can draw a straight line along the e1 direction which intersects the perturbation ball (shown as purple line segment in Figure 2). For any two points on this line segment that are at least \u03b4r/(2 \u221a d) away from each other (shown as red points w,u in Figure 2), by Lemma 11, we know at least one of them must not be in Xstuck. This implies if there is one point u\u0303 \u2208 Xstuck on this line segment, then Xstuck on this line can be at most an interval of length \u03b4r/ \u221a d around u\u0303. This establishes the \u201cthickness\u201d of Xstuck in the e1 direction, which is turned into an upper bound on the volume of the stuck region Xstuck by standard calculus."}, {"heading": "6 Conclusion", "text": "This paper presents the first (nearly) dimension-free result for gradient descent in a general nonconvex setting. We present a general convergence result and show how it can be further strengthened when combined with further structure such as strict saddle conditions and/or local regularity/convexity.\nThere are still many related open problems. First, in the presence of constraints, it is worthwhile to study whether gradient descent still admits similar sharp convergence results. Another important question is whether similar techniques can be applied to accelerated gradient descent. We hope that this result could serve as a first step towards a more general theory with strong, almost dimension free guarantees for non-convex optimization."}, {"heading": "A Detailed Proof of Main Theorem", "text": "In this section, we give detailed proof for the main theorem. We will first state two key lemmas that show how the algorithm can make progress when the gradient is large or near a saddle point, and show how the main theorem follows from the two lemmas. Then we will focus on the novel technique in this paper: how to analyze gradient descent near saddle point.\nA.1 General Framework\nIn order to prove the main theorem, we need to show that the algorithm will not be stuck at any point that either has a large gradient or is near a saddle point. This idea is similar to previous works (e.g.[Ge et al., 2015]). We first state a standard Lemma that shows if the current gradient is large, then we make progress in function value.\nLemma 12 (Lemma 9 restated). Assume f(\u00b7) satisfies A1, then for gradient descent with stepsize \u03b7 < 1\u2113 , we have:\nf(xt+1) \u2264 f(xt)\u2212 \u03b7\n2 \u2016\u2207f(xt)\u20162\nProof. By Assumption A1 and its property, we have:\nf(xt+1) \u2264f(xt) +\u2207f(xt)\u22a4(xt+1 \u2212 xt) + \u2113\n2 \u2016xt+1 \u2212 xt\u20162\n=f(xt)\u2212 \u03b7\u2016\u2207f(xt)\u20162 + \u03b72\u2113\n2 \u2016\u2207f(xt)\u20162 \u2264 f(xt)\u2212\n\u03b7 2 \u2016\u2207f(xt)\u20162\nThe next lemma says that if we are \u201cclose to a saddle points\u201d, i.e., we are at a point where the gradient is small, but the Hessian has a reasonably large negative eigenvalue. This is the main difficulty in the analysis. We show a perturbation followed by small number of standard gradient descent steps can also make the function value decrease with high probability.\nLemma 13 (Lemma 10 formal). There exist absolute constant cmax, for f(\u00b7) satisfies A1, and any c \u2264 cmax, and \u03c7 \u2265 1. Let \u03b7, r, gthres, fthres, tthres calculated same way as in Algorithm 2. Then, if x\u0303t satisfies:\n\u2016\u2207f(x\u0303t)\u2016 \u2264 gthres and \u03bbmin(\u22072f(x\u0303t)) \u2264 \u2212 \u221a \u03c1\u01eb\nLet xt = x\u0303t+\u03bet where \u03bet comes from the uniform distribution over B0(r), and let xt+i be the iterates of gradient descent from xt with stepsize \u03b7, then with at least probability 1\u2212 d\u2113\u221a\u03c1\u01ebe\u2212\u03c7, we have:\nf(xt+tthres)\u2212 f(x\u0303t) \u2264 \u2212fthres The proof of this lemma is deferred to Section A.2. Using this Lemma, we can then prove the\nmain Theorem.\nTheorem 3. There exist absolute constant cmax such that: if f(\u00b7) satisfies A1, then for any \u03b4 > 0, \u01eb \u2264 \u21132\u03c1 ,\u2206f \u2265 f(x0) \u2212 f\u22c6, and constant c \u2264 cmax, with probability 1 \u2212 \u03b4, the output of PGD(x0, \u2113, \u03c1, \u01eb, c, \u03b4,\u2206f ) will be \u01eb\u2212second order stationary point, and terminate in iterations:\nO\n( \u2113(f(x0)\u2212 f\u22c6)\n\u01eb2 log4 ( d\u2113\u2206f \u01eb2\u03b4 ))\nProof. Denote c\u0303max to be the absolute constant allowed in Theorem 13. In this theorem, we let cmax = min{c\u0303max, 1/2}, and choose any constant c \u2264 cmax.\nIn this proof, we will actually achieve some point satisfying following condition:\n\u2016\u2207f(x)\u2016 \u2264 gthres = \u221a c\n\u03c72 \u00b7 \u01eb, \u03bbmin(\u22072f(x)) \u2265 \u2212\n\u221a \u03c1\u01eb (3)\nSince c \u2264 1, \u03c7 \u2265 1, we have \u221a c\n\u03c72 \u2264 1, which implies any x satisfy Eq.(3) is also a \u01eb-second-order\nstationary point. Starting from x0, we know if x0 does not satisfy Eq.(3), there are only two possibilities:\n1. \u2016\u2207f(x0)\u2016 > gthres: In this case, Algorithm 2 will not add perturbation. By Lemma 12:\nf(x1)\u2212 f(x0) \u2264 \u2212 \u03b7\n2 \u00b7 g2thres = \u2212\nc2\n2\u03c74 \u00b7 \u01eb\n2\n\u2113\n2. \u2016\u2207f(x0)\u2016 \u2264 gthres: In this case, Algorithm 2 will add a perturbation of radius r, and will perform gradient descent (without perturbations) for the next tthres steps. Algorithm 2 will then check termination condition. If the condition is not met, we must have:\nf(xtthres)\u2212 f(x0) \u2264 \u2212fthres = \u2212 c \u03c73 \u00b7 \u221a \u01eb3 \u03c1\nThis means on average every step decreases the function value by\nf(xtthres)\u2212 f(x0) tthres \u2264 \u2212 c 3 \u03c74 \u00b7 \u01eb 2 \u2113\nIn case 1, we can repeat this argument for t = 1 and in case 2, we can repeat this argument for t = tthres. Hence, we can conclude as long as algorithm 2 has not terminated yet, on average, every step decrease function value by at least c 3\n\u03c74 \u00b7 \u01eb2\u2113 . However, we clearly can not decrease function value\nby more than f(x0)\u2212 f\u22c6, where f\u22c6 is the function value of global minima. This means algorithm 2 must terminate within the following number of iterations:\nf(x0)\u2212 f\u22c6 c3\n\u03c74 \u00b7 \u01eb2 \u2113\n= \u03c74 c3 \u00b7 \u2113(f(x0)\u2212 f \u22c6) \u01eb2 = O\n( \u2113(f(x0)\u2212 f\u22c6)\n\u01eb2 log4 ( d\u2113\u2206f \u01eb2\u03b4 ))\nFinally, we would like to ensure when Algorithm 2 terminates, the point it finds is actually an \u01eb-second-order stationary point. The algorithm can only terminate when the gradient is small, and the function value does not decrease after a perturbation and tthres iterations. We shall show every time when we add perturbation to iterate x\u0303t, if \u03bbmin(\u22072f(x\u0303t)) < \u2212 \u221a \u03c1\u01eb, then we will have f(xt+tthres)\u2212f(x\u0303t) \u2264 \u2212fthres. Thus, whenever the current point is not an \u01eb-second-order stationary point, the algorithm cannot terminate.\nAccording to Algorithm 2, we immediately know \u2016\u2207f(x\u0303t)\u2016 \u2264 gthres (otherwise we will not add perturbation at time t). By Lemma 13, we know this event happens with probability at least\n1 \u2212 d\u2113\u221a\u03c1\u01ebe\u2212\u03c7 each time. On the other hand, during one entire run of Algorithm 2, the number of times we add perturbations is at most:\n1 tthres \u00b7 \u03c7\n4 c3 \u00b7 \u2113(f(x0)\u2212 f \u22c6) \u01eb2 = \u03c73 c\n\u221a \u03c1\u01eb(f(x0)\u2212 f\u22c6)\n\u01eb2\nBy union bound, for all these perturbations, with high probability Lemma 13 is satisfied. As a result Algorithm 2 works correctly. The probability of that is at least\n1\u2212 d\u2113\u221a \u03c1\u01eb e\u2212\u03c7 \u00b7 \u03c7 3 c\n\u221a \u03c1\u01eb(f(x0)\u2212 f\u22c6)\n\u01eb2 = 1\u2212 \u03c7\n3e\u2212\u03c7 c \u00b7 d\u2113(f(x0)\u2212 f \u22c6) \u01eb2\nRecall our choice of \u03c7 = 3max{log(d\u2113\u2206f c\u01eb2\u03b4 ), 4}. Since \u03c7 \u2265 12, we have \u03c73e\u2212\u03c7 \u2264 e\u2212\u03c7/3, this gives:\n\u03c73e\u2212\u03c7 c \u00b7 d\u2113(f(x0)\u2212 f \u22c6) \u01eb2 \u2264 e\u2212\u03c7/3d\u2113(f(x0)\u2212 f \u22c6) c\u01eb2 \u2264 \u03b4\nwhich finishes the proof.\nA.2 Main Lemma: Escaping from Saddle Points Quickly\nNow we prove the main lemma (Lemma 13), which shows near a saddle point, a small perturbation followed by a small number of gradient descent steps will decrease the function value with high probability. This is the main step where we need new analysis, as the analysis previous works (e.g.[Ge et al., 2015]) do not work when the step size and perturbation do not depend polynomially in dimension d.\nIntuitively, after adding a perturbation, the current point of the algorithm comes from a uniform distribution over a d-dimensional ball centered at x\u0303, which we call perturbation ball. After a small number of gradient steps, some points in this ball (which we call the escaping region) will significantly decrease the function; other points (which we call the stuck region) does not see a significant decrease in function value. We hope to show that the escaping region constitutes at least 1\u2212 \u03b4 fraction of the volume of the perturbation ball.\nHowever, we do not know the exact form of the function near the saddle point, so the escaping region does not have a clean analytic description. Explicitly computing its volume can be very difficult. Our proof rely on a crucial observation: although we do not know the shape of the stuck region, we know the \u201cwidth\u201d of it must be small, therefore it cannot have a large volume. We will formalize this intuition later in Lemma 15.\nThe proof of the main lemma requires carefully balancing between different quantities including function value, gradient, parameter space and number of iterations. For clarify, we define following scalar quantities, which serve as the \u201cunits\u201d for function value, gradient, parameter space, and time (iterations). We will use these notations throughout the proof.\nLet the condition number be the ratio of the smoothness parameter (largest eigenvalue of Hessian) and the negative eigenvalue \u03b3: \u03ba = \u2113/\u03b3 \u2265 1, we define the following units:\nF := \u03b7\u2113 \u03b33 \u03c12 \u00b7 log\u22123(d\u03ba \u03b4 ), G := \u221a \u03b7\u2113 \u03b32 \u03c1 \u00b7 log\u22122(d\u03ba \u03b4 )\nS := \u221a \u03b7\u2113 \u03b3 \u03c1 \u00b7 log\u22121(d\u03ba \u03b4 ), T :=\nlog(d\u03ba\u03b4 )\n\u03b7\u03b3\nIntuitively, if we plug in our choice of learning rate \u03b7\u2113 = O(1) (which we will prove later) and hide the logarithmic dependences, we have F = O\u0303(\u03b3 3\n\u03c12 ),G = O\u0303(\u03b3\n2\n\u03c1 ),S = O\u0303( \u03b3 \u03c1 ), which is the only way to\ncorrectly discribe the units of function value, gradient, parameter space by just \u03b3 and \u03c1. Moreover,\nthese units are closely related, in particular, we know\n\u221a\nF \u00b7log(d\u03ba \u03b4 ) \u03b3 = G \u00b7log(d\u03ba \u03b4 ) \u03b3 = S .\nFor simplicity of later proofs, we first restate Lemma 13 into a slightly more general form as follows. Lemma 13 is directly implied following lemma.\nLemma 14 (Lemma 13 restated). There exists universal constant cmax, for f(\u00b7) satisfies A1, for any \u03b4 \u2208 (0, d\u03bae ], suppose we start with point x\u0303 satisfying following conditions:\n\u2016\u2207f(x\u0303)\u2016 \u2264 G and \u03bbmin(\u22072f(x\u0303)) \u2264 \u2212\u03b3\nLet x0 = x\u0303 + \u03be where \u03be come from the uniform distribution over ball with radius S /(\u03ba \u00b7 log(d\u03ba\u03b4 )), and let xt be the iterates of gradient descent from x0. Then, when stepsize \u03b7 \u2264 cmax/\u2113, with at least probability 1\u2212 \u03b4, we have following for any T \u2265 1cmax T :\nf(xT )\u2212 f(x\u0303) \u2264 \u2212F\nLemma 14 is almost the same as Lemma 13. It is easy to verify that by substituting \u03b7 = c\u2113 , \u03b3 =\u221a \u03c1\u01eb and \u03b4 = d\u2113\u221a\u03c1\u01ebe \u2212\u03c7 into Lemma 14, we immediately obtain Lemma 13.\nNow we will formalize the intuition that the \u201cwidth\u201d of stuck region is small.\nLemma 15 (Lemma 11 restated). There exists a universal constant cmax, for any \u03b4 \u2208 (0, d\u03bae ], let f(\u00b7), x\u0303 satisfies the conditions in Lemma 14, and without loss of generality let e1 be the minimum eigenvector of \u22072f(x\u0303). Consider two gradient descent sequences {ut}, {wt} with initial points u0,w0 satisfying: (denote radius r = S /(\u03ba \u00b7 log(d\u03ba\u03b4 )))\n\u2016u0 \u2212 x\u0303\u2016 \u2264 r, w0 = u0 + \u00b5 \u00b7 r \u00b7 e1, \u00b5 \u2208 [\u03b4/(2 \u221a d), 1]\nThen, for any stepsize \u03b7 \u2264 cmax/\u2113, and any T \u2265 1cmax T , we have:\nmin{f(uT )\u2212 f(u0), f(wT )\u2212 f(w0)} \u2264 \u22122.5F\nIntuitively, lemma 15 claims for any two points u0,w0 inside the perturbation ball, if u0 \u2212w0 lies in the direction of minimum eigenvector of \u22072f(x\u0303), and \u2016u0 \u2212w0\u2016 is greater than threshold \u03b4r/(2 \u221a d), then at least one of two sequences {ut}, {wt} will \u201cefficiently escape saddle point\u201d. In other words, if u0 is a point in the stuck region, consider any point w0 that is on a straight line along direction of e1. As long as w0 is slightly far (\u03b4r/ \u221a d) from u0, it must be in the escaping region. This is what we mean by the \u201cwidth\u201d of the stuck region being small. Now we prove the main Lemma using this observation:\nProof of Lemma 14. By adding perturbation, in worst case we increase function value by:\nf(x0)\u2212 f(x\u0303) \u2264 \u2207f(x\u0303)\u22a4\u03be + \u2113 2 \u2016\u03be\u20162 \u2264 G ( S\n\u03ba \u00b7 log(d\u03ba\u03b4 ) ) +\n1 2 \u2113(\nS \u03ba \u00b7 log(d\u03ba\u03b4 ) )2 \u2264 3 2 F\nOn the other hand, let radius r = S \u03ba\u00b7log(d\u03ba \u03b4 ) . We know x0 come froms uniform distribution over Bx\u0303(r). Let Xstuck \u2282 Bx\u0303(r) denote the set of bad starting points so that if x0 \u2208 Xstuck, then\nf(xT )\u2212 f(x0) > \u22122.5F (thus stuck at a saddle point); otherwise if x0 \u2208 Bx\u0303(r) \u2212 Xstuck, we have f(xT )\u2212 f(x0) \u2264 \u22122.5F .\nBy applying Lemma 15, we know for any x0 \u2208 Xstuck, it is guaranteed that (x0\u00b1\u00b5re1) 6\u2208 Xstuck where \u00b5 \u2208 [ \u03b4\n2 \u221a d , 1]. Denote IXstuck(\u00b7) be the indicator function of being inside set Xstuck; and vector\nx = (x(1),x(\u22121)), where x(1) is the component along e1 direction, and x(\u22121) is the remaining d\u2212 1 dimensional vector. Recall B(d)(r) be d-dimensional ball with radius r; By calculus, this gives an upper bound on the volumn of Xstuck:\nVol(Xstuck) = \u222b\nB (d) x\u0303\n(r) dx \u00b7 IXstuck(x)\n=\n\u222b\nB (d\u22121) x\u0303\n(r) dx(\u22121)\n\u222b x\u0303(1)+ \u221a r2\u2212\u2016x\u0303(\u22121)\u2212x(\u22121)\u20162\nx\u0303(1)\u2212 \u221a r2\u2212\u2016x\u0303(\u22121)\u2212x(\u22121)\u20162 dx(1) \u00b7 IXstuck(x)\n\u2264 \u222b\nB (d\u22121) x\u0303\n(r) dx(\u22121) \u00b7\n(\n2 \u00b7 \u03b4 2 \u221a d r\n)\n= Vol(B (d\u22121) 0 (r))\u00d7 \u03b4r\u221a d\nThen, we immediately have the ratio:\nVol(Xstuck) Vol(B\n(d) x\u0303 (r))\n\u2264 \u03b4r\u221a d \u00d7Vol(B(d\u22121)0 (r))\nVol(B (d) 0 (r))\n= \u03b4\u221a \u03c0d\n\u0393(d2 + 1) \u0393(d2 + 1 2) \u2264 \u03b4\u221a \u03c0d\n\u00b7 \u221a d\n2 +\n1 2 \u2264 \u03b4\nThe second last inequality is by the property of Gamma function that \u0393(x+1)\u0393(x+1/2) < \u221a x+ 12 as long as x \u2265 0. Therefore, with at least probability 1\u2212 \u03b4, x0 6\u2208 Xstuck. In this case, we have:\nf(xT )\u2212 f(x\u0303) =f(xT )\u2212 f(x0) + f(x0)\u2212 f(0) \u2264\u2212 2.5F + 1.5F \u2264 \u2212F\nwhich finishes the proof.\nA.3 Bounding the Width of Stuck Region\nIn order to prove Lemma 15, we do it in two steps:\n1. We first show if gradient descent from u0 does not decrease function value, then all the iterates must lie within a small ball around u0 (Lemma 16).\n2. If gradient descent starting from a point u0 stuck in a small ball around a saddle point, then gradient descent from w0 (moving u0 along e1 direction for at least a certain distance), will decreases the function value (Lemma 17).\nRecall we assumed without loss of generality e1 is the minimum eigenvector of \u22072f(x\u0303). In this context, we denote H := \u22072f(x\u0303), and for simplicity of calculation, we consider following quadratic approximation:\nf\u0303y(x) := f(y) +\u2207f(y)\u22a4(x\u2212 y) + 1 2 (x\u2212 y)\u22a4H(x\u2212 y) (4)\nNow we are ready to state two lemmas formally:\nLemma 16. For any constant c\u0302 \u2265 3, there exists absolute constant cmax: for any \u03b4 \u2208 (0, d\u03bae ], let f(\u00b7), x\u0303 satisfies the condition in Lemma 14, for any initial point u0 with \u2016u0 \u2212 x\u0303\u2016 \u2264 2S /(\u03ba \u00b7 log(d\u03ba\u03b4 )), define:\nT = min {\ninf t\n{ t|f\u0303u0(ut)\u2212 f(u0) \u2264 \u22123F } , c\u0302T }\nthen, for any \u03b7 \u2264 cmax/\u2113, we have for all t < T that \u2016ut \u2212 x\u0303\u2016 \u2264 100(S \u00b7 c\u0302).\nLemma 17. There exists absolute constant cmax, c\u0302 such that: for any \u03b4 \u2208 (0, d\u03bae ], let f(\u00b7), x\u0303 satisfies the condition in Lemma 14, and sequences {ut}, {wt} satisfy the conditions in Lemma 15, define:\nT = min {\ninf t\n{ t|f\u0303w0(wt)\u2212 f(w0) \u2264 \u22123F } , c\u0302T }\nthen, for any \u03b7 \u2264 cmax/\u2113, if \u2016ut \u2212 x\u0303\u2016 \u2264 100(S \u00b7 c\u0302) for all t < T , we will have T < c\u0302T .\nNote the conclusion T < c\u0302T in Lemma 17 equivalently means:\ninf t\n{ t|f\u0303w0(wt)\u2212 f(w0) \u2264 \u22123F } < c\u0302T\nThat is, for some T < c\u0302T , {wt} sequence \u201cescape the saddle point\u201d in the sense of sufficient function value decrease f\u0303w0(wt)\u2212 f(w0) \u2264 \u22123F . Now, we are ready to prove Lemma 15.\nProof of Lemma 15. W.L.O.G, let x\u0303 = 0 be the origin. Let (c (2) max, c\u0302) be the absolute constant so that Lemma 17 holds, also let c (1) max be the absolute constant to make Lemma 16 holds based on our current choice of c\u0302. We choose cmax \u2264 min{c(1)max, c(2)max} so that our learning rate \u03b7 \u2264 cmax/\u2113 is small enough which make both Lemma 16 and Lemma 17 hold. Let T \u22c6 := c\u0302T and define:\nT \u2032 = inf t\n{ t|f\u0303u0(ut)\u2212 f(u0) \u2264 \u22123F }\nLet\u2019s consider following two cases:\nCase T \u2032 \u2264 T \u22c6: In this case, by Lemma 16, we know \u2016uT \u2032\u22121\u2016 \u2264 O(S ), and therefore\n\u2016uT \u2032\u2016 \u2264\u2016uT \u2032\u22121\u2016+ \u03b7\u2016\u2207f(uT \u2032\u22121)\u2016 \u2264 \u2016uT \u2032\u22121\u2016+ \u03b7\u2016\u2207f(x\u0303)\u2016+ \u03b7\u2113\u2016uT \u2032\u22121\u2016 \u2264 O(S )\nBy choosing cmax small enough and \u03b7 \u2264 cmax/\u2113, this gives:\nf(uT \u2032)\u2212 f(u0) \u2264\u2207f(u0)\u22a4(uT \u2032 \u2212 u0) + 1 2 (uT \u2032 \u2212 u0)\u22a4\u22072f(u0)(uT \u2032 \u2212 u0) + \u03c1 6 \u2016uT \u2032 \u2212 u0\u20163\n\u2264f\u0303u0(ut)\u2212 f(u0) + \u03c1\n2 \u2016u0 \u2212 x\u0303\u2016\u2016uT \u2032 \u2212 u0\u20162 +\n\u03c1 6 \u2016uT \u2032 \u2212 u0\u20163\n\u2264\u2212 3F +O(\u03c1S 3) = \u22123F +O( \u221a \u03b7\u2113 \u00b7 F ) \u2264 \u22122.5F\nBy choose cmax \u2264 min{1, 1c\u0302}. We know \u03b7 < 1\u2113 , by Lemma 12, we know gradient descent always decrease function value. Therefore, for any T \u2265 1cmax T \u2265 c\u0302T = T \u22c6 \u2265 T \u2032, we have:\nf(uT )\u2212 f(u0) \u2264 f(uT \u22c6)\u2212 f(u0) \u2264 f(uT \u2032)\u2212 f(u0) \u2264 \u22122.5F\nCase T \u2032 > T \u22c6: In this case, by Lemma 16, we know \u2016ut\u2016 \u2264 O(S ) for all t \u2264 T \u22c6. Define\nT \u2032\u2032 = inf t\n{ t|f\u0303w0(wt)\u2212 f(w0) \u2264 \u22122F }\nBy Lemma 17, we immediately have T \u2032\u2032 \u2264 T \u22c6. Apply same argument as in first case, we have for all T \u2265 1cmax T that f(wT )\u2212 f(w0) \u2264 f(wT \u22c6)\u2212 f(w0) \u2264 \u22122.5F .\nNext we finish the proof by proving Lemma 16 and Lemma 17.\nA.3.1 Proof of Lemma 16\nIn Lemma 16, we hope to show if the function value did not decrease, then all the iterations must be constrained in a small ball. We do that by analyzing the dynamics of the iterations, and we decompose the d-dimensional space into two subspaces: a subspace S which is the span of significantly negative eigenvectors of the Hessian and its orthogonal compliment.\nRecall notation H := \u22072f(x\u0303) and quadratic approximation f\u0303y(x) as defined in Eq.(4). Since \u03b4 \u2208 (0, d\u03bae ], we always have log(d\u03ba\u03b4 ) \u2265 1. W.L.O.G, set u0 = 0 to be the origin, by gradient descent update function, we have:\nut+1 =ut \u2212 \u03b7\u2207f(ut)\n=ut \u2212 \u03b7\u2207f(0)\u2212 \u03b7 [\u222b 1\n0 \u22072f(\u03b8ut)d\u03b8\n]\nut\n=ut \u2212 \u03b7\u2207f(0)\u2212 \u03b7(H +\u2206t)ut =(I \u2212 \u03b7H \u2212 \u03b7\u2206t)ut \u2212 \u03b7\u2207f(0) (5)\nHere, \u2206t := \u222b 1 0 \u22072f(\u03b8ut)d\u03b8 \u2212 H. By Hessian Lipschitz, we have \u2016\u2206t\u2016 \u2264 \u03c1(\u2016ut\u2016 + \u2016x\u0303\u2016), and by smoothness of the gradient, we have \u2016\u2207f(0)\u2016 \u2264 \u2016\u2207f(x\u0303)\u2016+ \u2113\u2016x\u0303\u2016 \u2264 G + \u2113 \u00b7 2S /(\u03ba \u00b7 log(d\u03ba\u03b4 )) \u2264 3G . We will now compute the projections of ut in different eigenspaces of H. Let S be the subspace spanned by all eigenvectors of H whose eigenvalue is less than \u2212 \u03b3 c\u0302 log(d\u03ba \u03b4 ) . Sc denotes the subspace of remaining eigenvectors. Let \u03b1t and \u03b2t denote the projections of ut onto S and Sc respectively i.e., \u03b1t = PSut, and \u03b2t = PScut. We can decompose the update equations Eq.(5) into:\n\u03b1t+1 =(I\u2212 \u03b7H)\u03b1t \u2212 \u03b7PS\u2206tut \u2212 \u03b7PS\u2207f(0) (6) \u03b2t+1 =(I\u2212 \u03b7H)\u03b2t \u2212 \u03b7PSc\u2206tut \u2212 \u03b7PSc\u2207f(0) (7)\nBy definition of T , we know for all t < T :\n\u22123F < f\u03030(ut)\u2212 f(0) =\u2207f(0)\u22a4ut \u2212 1 2 u\u22a4t Hut \u2264 \u2207f(0)\u22a4ut \u2212 \u03b3 2 \u2016\u03b1t\u20162 c\u0302 log(d\u03ba\u03b4 ) + 1 2 \u03b2\u22a4t H\u03b2t\nCombined with the fact \u2016ut\u20162 = \u2016\u03b1t\u20162 + \u2016\u03b2t\u20162, we have:\n\u2016ut\u20162 \u2264 2c\u0302 log(d\u03ba\u03b4 )\n\u03b3\n(\n3F +\u2207f(0)\u22a4ut + 1 2 \u03b2\u22a4t H\u03b2t\n)\n+ \u2016\u03b2t\u20162\n\u226414 \u00b7max { G c\u0302 log(d\u03ba\u03b4 )\n\u03b3 \u2016ut\u2016,\nF c\u0302 log(d\u03ba\u03b4 ) \u03b3 , \u03b2\u22a4t H\u03b2tc\u0302 log(d\u03ba\u03b4 ) \u03b3 , \u2016\u03b2t\u20162\n}\nwhere last inequality is due to \u2016\u2207f(0)\u2016 \u2264 3G . This gives:\n\u2016ut\u2016 \u2264 14 \u00b7max\n \n\nG c\u0302 log(d\u03ba\u03b4 )\n\u03b3 ,\n\u221a\nF c\u0302 log(d\u03ba\u03b4 )\n\u03b3 ,\n\u221a\n\u03b2\u22a4t H\u03b2tc\u0302 log(d\u03ba\u03b4 ) \u03b3 , \u2016\u03b2t\u2016\n   (8)\nNow, we use induction to prove that\n\u2016ut\u2016 \u2264 100(S \u00b7 c\u0302) (9)\nClearly Eq.(9) is true for t = 0 since u0 = 0. Suppose Eq.(9) is true for all \u03c4 \u2264 t. We will now show that Eq.(9) holds for t+1 < T . Note that by the definition of S , F and G , we only need to bound the last two terms of Eq.(8) i.e., \u2016\u03b2t+1\u2016 and \u03b2\u22a4t+1H\u03b2t+1.\nBy update function of \u03b2t (Eq.(7)), we have:\n\u03b2t+1 \u2264(I\u2212 \u03b7H)\u03b2t + \u03b7\u03b4t (10)\nand the norm of \u03b4t is bounded as follows:\n\u2016\u03b4t\u2016 \u2264 \u2016\u2206t\u2016\u2016ut\u2016+ \u2016\u2207f(0)\u2016 \u2264 \u03c1 (\u2016ut\u2016+ \u2016x\u0303\u2016) \u2016ut\u2016+ \u2016\u2207f(0)\u2016\n\u2264 \u03c1 \u00b7 100c\u0302(100c\u0302 + 2/(\u03ba \u00b7 log(d\u03ba \u03b4 )))S 2 + G = [100c\u0302(100c\u0302 + 2) \u221a \u03b7\u2113+ 1]G \u2264 2G (11)\nThe last step follows by choosing small enough constant cmax \u2264 1100c\u0302(100c\u0302+2) and stepsize \u03b7 < cmax/\u2113.\nBounding \u2016\u03b2t+1\u2016: Combining Eq.(10), Eq.(11) and using the definiton of Sc, we have:\n\u2016\u03b2t+1\u2016 \u2264 (1 + \u03b7\u03b3\nc\u0302 log(d\u03ba\u03b4 ) )\u2016\u03b2t\u2016+ 2\u03b7G\nSince \u2016\u03b20\u2016 = 0 and t+ 1 \u2264 T , by applying above relation recurrsively, we have:\n\u2016\u03b2t+1\u2016 \u2264 t\u2211\n\u03c4=0\n2(1 + \u03b7\u03b3\nc\u0302 log(d\u03ba\u03b4 ) )\u03c4\u03b7G \u2264 2 \u00b7 3 \u00b7 T\u03b7G \u2264 6(S \u00b7 c\u0302) (12)\nThe second last inequality is because T \u2264 c\u0302T by definition, so that (1 + \u03b7\u03b3 c\u0302 log(d\u03ba \u03b4 ) )T \u2264 3.\nBounding \u03b2\u22a4t+1H\u03b2t+1: Using Eq.(10), we can also write the update equation as:\n\u03b2t =\nt\u22121\u2211\n\u03c4=0\n(I \u2212 \u03b7H)\u03c4\u03b4\u03c4\nCombining with Eq.(11), this gives\n\u03b2\u22a4t+1H\u03b2t+1 =\u03b72 t\u2211\n\u03c41=0\nt\u2211\n\u03c42=0\n\u03b4\u22a4\u03c41(I\u2212 \u03b7H) \u03c41H(I\u2212 \u03b7H)\u03c42\u03b4\u03c42\n\u2264\u03b72 t\u2211\n\u03c41=0\nt\u2211\n\u03c42=0\n\u2016\u03b4\u03c41\u2016\u2016(I \u2212 \u03b7H)\u03c41H(I\u2212 \u03b7H)\u03c42\u2016\u2016\u03b4\u03c42\u2016\n\u22644\u03b72G 2 t\u2211\n\u03c41=0\nt\u2211\n\u03c42=0\n\u2016(I \u2212 \u03b7H)\u03c41H(I\u2212 \u03b7H)\u03c42\u2016\nLet the eigenvalues of H to be {\u03bbi}, then for any \u03c41, \u03c42 \u2265 0, we know the eigenvalues of (I \u2212 \u03b7H)\u03c41H(I\u2212\u03b7H)\u03c42 are {\u03bbi(1\u2212\u03b7\u03bbi)\u03c41+\u03c42}. Let gt(\u03bb) := \u03bb(1\u2212\u03b7\u03bb)t, and setting its derivative to zero, we obtain: \u2207gt(\u03bb) = (1\u2212 \u03b7\u03bb)t \u2212 t\u03b7\u03bb(1\u2212 \u03b7\u03bb)t\u22121 = 0 We see that \u03bb\u22c6t = 1 (1+t)\u03b7 is the unique maximizer, and gt(\u03bb) is monotonically increasing in (\u2212\u221e, \u03bb\u22c6t ]. This gives:\n\u2016(I \u2212 \u03b7H)\u03c41H(I\u2212 \u03b7H)\u03c42\u2016 = max i\n\u03bbi(1\u2212 \u03b7\u03bbi)\u03c41+\u03c42 \u2264 \u03bb\u0302(1\u2212 \u03b7\u03bb\u0302)\u03c41+\u03c42 \u2264 1\n(1 + \u03c41 + \u03c42)\u03b7\nwhere \u03bb\u0302 = min{\u2113, \u03bb\u22c6\u03c41+\u03c42}. Therefore, we have:\n\u03b2\u22a4t+1H\u03b2t+1 \u22644\u03b72G 2 t\u2211\n\u03c41=0\nt\u2211\n\u03c42=0\n\u2016(I\u2212 \u03b7H)\u03c41H(I\u2212 \u03b7H)\u03c42\u2016\n\u22644\u03b7G 2 t\u2211\n\u03c41=0\nt\u2211\n\u03c42=0\n1 1 + \u03c41 + \u03c42 \u2264 8\u03b7TG 2 \u2264 8S 2\u03b3c\u0302 \u00b7 log\u22121(d\u03ba \u03b4 ) (13)\nThe second last inequality is because by rearrange summation:\nt\u2211\n\u03c41=0\nt\u2211\n\u03c42=0\n1\n1 + \u03c41 + \u03c42 =\n2t\u2211\n\u03c4=0\nmin{1 + \u03c4, 2t+ 1\u2212 \u03c4} \u00b7 1 1 + \u03c4 \u2264 2t+ 1 < 2T\nFinally, substitue Eq.(12) and Eq.(13) into Eq.(8), this gives:\n\u2016ut+1\u2016 \u226414 \u00b7max\n \n\nG c\u0302 log(d\u03ba\u03b4 )\n\u03b3 ,\n\u221a\nF c\u0302 log(d\u03ba\u03b4 )\n\u03b3 ,\n\u221a\n\u03b2\u22a4t H\u03b2tc\u0302 log(d\u03ba\u03b4 ) \u03b3 , \u2016\u03b2t\u2016\n \n\n\u2264100(S \u00b7 c\u0302)\nThis finishes the induction as well as the proof of the lemma.\nA.3.2 Proof of Lemma 17\nIn this Lemma we try to show if all the iterates from u0 are constrained in a small ball, iterates from w0 must be able to decrease the function value. In order to do that, we keep track of vector v which is the difference between u and w. Similar as before, we also decompose v into different eigenspaces. However, this time we only care about the projection of v on the direction e1 and its orthognal subspace.\nAgain, recall notation H := \u22072f(x\u0303), e1 as minimum eigenvector of H and quadratic approximation f\u0303y(x) as defined in Eq.(4). Since \u03b4 \u2208 (0, d\u03bae ], we always have log(d\u03ba\u03b4 ) \u2265 1. W.L.O.G, set u0 = 0 to be the origin. Define vt = wt \u2212 ut, by assumptions in Lemma 17, we have v0 = \u00b5[S /(\u03ba \u00b7 log(d\u03ba\u03b4 ))]e1, \u00b5 \u2208 [\u03b4/(2 \u221a d), 1]. Now, consider the update equation for wt:\nut+1 + vt+1 = wt+1 =wt \u2212 \u03b7\u2207f(wt) =ut + vt \u2212 \u03b7\u2207f(ut + vt)\n=ut + vt \u2212 \u03b7\u2207f(ut)\u2212 \u03b7 [\u222b 1\n0 \u22072f(ut + \u03b8vt)d\u03b8\n]\nvt\n=ut + vt \u2212 \u03b7\u2207f(ut)\u2212 \u03b7(H +\u2206\u2032t)vt =ut \u2212 \u03b7\u2207f(ut) + (I\u2212 \u03b7H\u2212 \u03b7\u2206\u2032t)vt\nwhere \u2206\u2032t := \u222b 1 0 \u22072f(ut+ \u03b8vt)d\u03b8\u2212H. By Hessian Lipschitz, we have \u2016\u2206\u2032t\u2016 \u2264 \u03c1(\u2016ut\u2016+ \u2016vt\u2016+ \u2016x\u0303\u2016). This gives the dynamic for vt satisfy:\nvt+1 = (I \u2212 \u03b7H\u2212 \u03b7\u2206\u2032t)vt (14) Since \u2016w0 \u2212 x\u0303\u2016 \u2264 \u2016u0 \u2212 x\u0303\u2016 + \u2016v0\u2016 \u2264 S /(\u03ba \u00b7 log(d\u03ba\u03b4 )), directly applying Lemma 16, we know wt \u2264 100(S \u00b7 c\u0302) for all t \u2264 T . By condition of Lemma 17, we know \u2016ut\u2016 \u2264 100(S \u00b7 c\u0302) for all t < T . This gives: \u2016vt\u2016 \u2264 \u2016ut\u2016+ \u2016wt\u2016 \u2264 200(S \u00b7 c\u0302) for all t < T (15) This in sum gives for t < T :\n\u2016\u2206\u2032t\u2016 \u2264 \u03c1(\u2016ut\u2016+ \u2016vt\u2016+ \u2016x\u0303\u2016) \u2264 \u03c1(300c\u0302S + S /(\u03ba \u00b7 log( d\u03ba\n\u03b4 ))) \u2264 \u03c1S (300c\u0302 + 1)\nOn the other hand, denote \u03c8t be the norm of vt projected onto e1 direction, and \u03d5t be the norm of vt projected onto remaining subspace. Eq.(14) gives us:\n\u03c8t+1 \u2265(1 + \u03b3\u03b7)\u03c8t \u2212 \u00b5 \u221a \u03c82t + \u03d5 2 t \u03d5t+1 \u2264(1 + \u03b3\u03b7)\u03d5t + \u00b5 \u221a \u03c82t + \u03d5 2 t\nwhere \u00b5 = \u03b7\u03c1S (300c\u0302 + 1). We will now prove via induction that for all t < T :\n\u03d5t \u2264 4\u00b5t \u00b7 \u03c8t (16) By hypothesis of Lemma 17, we know \u03d50 = 0, thus the base case of induction holds. Assume Eq.(16) is true for \u03c4 \u2264 t, For t+ 1 \u2264 T , we have:\n4\u00b5(t+ 1)\u03c8t+1 \u22654\u00b5(t+ 1) ( (1 + \u03b3\u03b7)\u03c8t \u2212 \u00b5 \u221a \u03c82t + \u03d5 2 t )\n\u03d5t+1 \u22644\u00b5t(1 + \u03b3\u03b7)\u03c8t + \u00b5 \u221a \u03c82t + \u03d5 2 t\nFrom above inequalities, we see that we only need to show:\n(1 + 4\u00b5(t+ 1)) \u221a\n\u03c82t + \u03d5 2 t \u2264 4(1 + \u03b3\u03b7)\u03c8t\nBy choosing \u221a cmax \u2264 1300c\u0302+1 min{ 12\u221a2 , 1 4c\u0302}, and \u03b7 \u2264 cmax/\u2113, we have\n4\u00b5(t+ 1) \u2264 4\u00b5T \u2264 4\u03b7\u03c1S (300c\u0302 + 1)c\u0302T = 4 \u221a \u03b7\u2113(300c\u0302 + 1)c\u0302 \u2264 1\nThis gives:\n4(1 + \u03b3\u03b7)\u03c8t \u2265 4\u03c8t \u2264 2 \u221a 2\u03c82t \u2265 (1 + 4\u00b5(t+ 1)) \u221a \u03c82t + \u03d5 2 t\nwhich finishes the induction.\nNow, we know \u03d5t \u2264 4\u00b5t \u00b7 \u03c8t \u2264 \u03c8t, this gives:\n\u03c8t+1 \u2265 (1 + \u03b3\u03b7)\u03c8t \u2212 \u221a 2\u00b5\u03c8t \u2265 (1 + \u03b3\u03b7\n2 )\u03c8t (17)\nwhere the last step follows from \u00b5 = \u03b7\u03c1S (300c\u0302 + 1) \u2264 \u221acmax(300c\u0302 + 1)\u03b3\u03b7 \u00b7 log\u22121(d\u03ba\u03b4 ) < \u03b3\u03b7 2 \u221a 2 .\nFinally, combining Eq.(15) and (17) we have for all t < T :\n200(S \u00b7 c\u0302) \u2265\u2016vt\u2016 \u2265 \u03c8t \u2265 (1 + \u03b3\u03b7\n2 )t\u03c80\n=(1 + \u03b3\u03b7\n2 )tc0\nS\n\u03ba log\u22121(\nd\u03ba \u03b4 ) \u2265 (1 + \u03b3\u03b7 2 )t \u03b4 2 \u221a d S \u03ba log\u22121( d\u03ba \u03b4 )\nThis implies:\nT < 1\n2\nlog(400\u03ba \u221a d\n\u03b4 \u00b7 c\u0302 log(d\u03ba\u03b4 )) log(1 + \u03b3\u03b72 )\n\u2264 log(400 \u03ba \u221a d\n\u03b4 \u00b7 c\u0302 log(d\u03ba\u03b4 )) \u03b3\u03b7 \u2264 (2 + log(400c\u0302))T\nThe last inequality is due to \u03b4 \u2208 (0, d\u03bae ] we have log(d\u03ba\u03b4 ) \u2265 1. By choosing constant c\u0302 to be large enough to satisfy 2 + log(400c\u0302) \u2264 c\u0302, we will have T < c\u0302T , which finishes the proof.\nB Improve Convergence by Local Structure\nIn this section, we show if the objective function has nice local structure (e.g. satisfies Assumptions A3.a or A3.b), then it is possible to combine our analysis with the local analysis in order to get very fast convergence to a local minimum.\nIn particular, we prove Theorem 5.\nTheorem 5. There exist absolute constant cmax such that: if f(\u00b7) satisfies A1, A2, and A3.a (or A3.b), then for any \u03b4 > 0, \u01eb > 0,\u2206f \u2265 f(x0) \u2212 f\u22c6, constant c \u2264 cmax, let \u01eb\u0303 = min(\u03b8, \u03b32/\u03c1), with probability 1\u2212 \u03b4, the output of PGDli(x0, \u2113, \u03c1, \u01eb\u0303, c, \u03b4,\u2206f , \u03b2) will be \u01eb-close to X \u22c6 in iterations:\nO\n( \u2113(f(x0)\u2212 f\u22c6)\n\u01eb\u03032 log4 ( d\u2113\u2206f \u01eb\u03032\u03b4 ) + \u03b2 \u03b1 log \u03b6 \u01eb )\nProof. Theorem 5 runs PGDli(x0, \u2113, \u03c1, \u01eb\u0303, c, \u03b4,\u2206f , \u03b2). According to algorithm 3, we know it calls PGD(x0, \u2113, \u03c1, \u01eb, c, \u03b4,\u2206f ) first (denote its output as x\u0302), then run standard gradient descent with learning rate 1\u03b2 starting from x\u0302.\nBy Corollary 4, we know x\u0302 is already in the \u03b6-neighborhood of X \u22c6, where X \u22c6 is the set of local minima. Therefore, to prove this theorem, we only need to show prove following two claims:\n1. Suppose {xt} is the sequence of gradient descent starting from x0 = x\u0302 with learning rate 1\u03b2 , then xt is always in the \u03b6-neighborhood of X \u22c6.\n2. Local structure (assumption A3.a or A3.b) allows iterates to converge to points \u01eb-close to X \u22c6 within O(\u03b2\u03b1 log \u03b6 \u01eb ) iterations.\nWe will focus on Assumption A3.b (as we will later see Assumption A3.a is a special case of Assumption A3.b). Assume xt is in \u03b6-neighborhood of X \u22c6, by gradient updates and the definition of projection, we have:\n\u2016xt+1 \u2212 PX \u22c6(xt+1)\u20162 \u2264\u2016xt+1 \u2212 PX \u22c6(xt)\u20162 = \u2016xt \u2212 \u03b7\u2207f(xt)\u2212PX \u22c6(xt)\u20162\n=\u2016xt \u2212 PX \u22c6(xt)\u20162 \u2212 2\u03b7\u3008\u2207f(xt),xt \u2212 PX \u22c6(xt)\u3009+ \u03b72\u2016\u2207f(xt)\u20162 \u2264\u2016xt \u2212 PX \u22c6(xt)\u20162 \u2212 \u03b7\u03b1\u2016xt \u2212 PX \u22c6(xt)\u20162 + (\u03b72 \u2212 \u03b7\n\u03b2 )\u2016\u2207f(x)\u20162\n\u2264(1\u2212 \u03b1 \u03b2 )\u2016xt \u2212 PX \u22c6(xt)\u20162\nThe second last inequality is due to (\u03b1, \u03b2)-regularity condition. The last inequality is because of the choice \u03b7 = 1\u03b2 .\nThere are two consequences of this calculation. First, it shows \u2016xt+1 \u2212 PX \u22c6(xt+1)\u20162 \u2264 \u2016xt \u2212 PX \u22c6(xt)\u20162. As a result if xt in \u03b6-neighborhood of X \u22c6, xt+1 is also in this \u03b6-neighborhood. Since x0 is in the \u03b6-neighborhood by Corollary 4, by induction we know all later iterations are in the same neighborhood.\nNow, since we know all the points xt are in the neighborhood, the equation also shows linear convergence rate (1 \u2212 \u03b1\u03b2 ). The initial distance is bounded by \u2016x0 \u2212 PX \u22c6(x0)\u2016 \u2264 \u03b6, therefore to converge to points \u01eb-close to X \u22c6, we only need the following number of iterations:\nlog(\u01eb/\u03b6) log(1\u2212 \u03b1/\u03b2) = O( \u03b2 \u03b1 log \u03b6 \u01eb ).\nThis finishes the proof under Assumption A3.b. Finally, we argue assumption A3.a implies A3.b. First, notice that if a function is locally strongly convex, then its local minima are isolated: for any two points x,x\u2032 \u2208 X \u22c6, the local region Bx(\u03b6) and Bx\u2032(\u03b6) must be disjoint (otherwise function f(x) is strongly convex in connected domain Bx(\u03b6) \u222a Bx\u2032(\u03b6) but has two distinct local minima, which is impossible). Therefore, W.L.O.G, it suffices to consider one perticular disjoint region, with unique local minimum we denote as x\u22c6, clearly, for all x \u2208 Bx\u22c6(\u03b6) we have PX \u22c6(x) = x\u22c6.\nNow by \u03b1-strong convexity:\nf(x\u22c6) \u2265 f(x) + \u3008\u2207f(x),x\u22c6 \u2212 x\u3009+ \u03b1 2 \u2016x\u2212 x\u22c6\u20162 (18)\nOn the other hand, for any x in this \u03b6-neighborhood, we already proved x \u2212 1\u03b2\u2207f(x) also in this \u03b6-neighborhood. By \u03b2-smoothness, we also have:\nf(x\u2212 1 \u03b2 \u2207f(x)) \u2264 f(x)\u2212 1 2\u03b2 \u2016\u2207f(x)\u20162 (19)\nCombining Eq.(18) and Eq.(19), and using the fact f(x\u22c6) \u2264 f(x\u2212 1\u03b2\u2207f(x)), we get:\n\u3008\u2207f(x),x\u2212 x\u22c6\u3009 \u2265 \u03b1 2 \u2016x\u2212 x\u22c6\u20162 + 1 2\u03b2 \u2016\u2207f(x)\u20162\nwhich finishes the proof."}, {"heading": "C Geometric Structures of Matrix Factorization Problem", "text": "In this Section we investigate the global geometric structures of the matrix factorization problem. These properties are summarized in Lemmas 6 and 7. Such structures allow us to apply our main Theorem and get fast convergence (as shown in Theorem 8).\nNote that our main results Theorems 3 and 5 are proved for functions f(\u00b7) whose input x is a vector. For the current function in 2, though the input U \u2208 Rd\u00d7r is a matrix, we can always vectorize it to be a vector in Rdr and apply our results. However, for simplicity of presentation, we still write everything in matrix form (without explicit vectorization), while the reader should keep in mind the operations are same if one vectorizes everything first.\nRecall for vectors we use \u2016\u00b7\u2016 to denote the 2-norm, and for matrices we use \u2016\u00b7\u2016 and \u2016\u00b7\u2016F to denote spectral norm, and Frobenius norm respectively. Furthermore, we always use \u03c3i(\u00b7) to denote the i-th largest singular value of the matrix.\nWe first show how the geometric properties (Lemma 6 and Lemma 7) imply a fast convergence (Theorem 8).\nTheorem 8. There exists an absolute constant cmax such that the following holds. For matrix factorization (2), for any \u03b4 > 0 and constant c \u2264 cmax, let \u03931/2 := 2max{\u2016U0\u2016, 3(\u03c3\u22c61)1/2}, suppose we run PGDli(U0, 8\u0393, 12\u0393 1/2, (\u03c3 \u22c6 r ) 2\n108\u03931/2 , c, \u03b4, r\u0393\n2\n2 , 10\u03c3 \u22c6 1), then:\n1. With probability 1, the iterates satisfy \u2016Ut\u2016 \u2264 \u03931/2 for every t \u2265 0.\n2. With probability 1\u2212\u03b4, the output will be \u01eb-close to global minima set X \u22c6 in following iterations:\nO\n(\nr\n( \u0393\n\u03c3\u22c6r\n)4 log4 ( d\u0393\n\u03b4\u03c3\u22c6r\n)\n+ \u03c3\u22c61 \u03c3\u22c6r log \u03c3\u22c6r \u01eb\n)\nProof of Theorem 8. Denote c\u0303max to be the absolute constant allowed in Theorem 5. In this theorem, we let cmax = min{c\u0303max, 12}, and choose any constant c \u2264 cmax.\nTheorem 8 consists of two parts. In part 1 we show that the iterations never bring the matrix to a very large norm, while in part 2 we apply our main Theorem to get fast convergence. We will first prove the bound on number of iterations assuming the bound on the norm. We will then proceed to prove part 1.\nPart 2: Assume part 1 of the theorem is true i.e., with probability 1, the iterates satisfy \u2016Ut\u2016 \u2264 \u03931/2 for every t \u2265 0. In this case, although we are doing unconstrained optimization, we can still use the geometric properties that hold inside this region. .\nBy Lemma 6 and 7, we know objective function Eq.(2) is 8\u0393-smooth, 12\u03931/2-Lipschitz Hessian, ( 124 (\u03c3 \u22c6 r ) 3/2, 13\u03c3 \u22c6 r , 1 3(\u03c3 \u22c6 r ) 1/2)-strict saddle, and holds (23\u03c3 \u22c6 r , 10\u03c3 \u22c6 1)-regularity condition in 1 3 (\u03c3 \u22c6 r ) 1/2 neighborhood of local minima (also global minima) X \u22c6. Furthermore, note f\u22c6 = 0 and recall \u03931/2 = 2max{\u2016U0\u2016, 3(\u03c3\u22c61)1/2}, then, we have:\nf(U0)\u2212 f\u22c6 = \u2016U0U\u22a40 \u2212M\u22c6\u20162F \u2264 2r\u2016U0U\u22a40 \u2212M\u22c6\u20162 \u2264 r\u03932\n2 .\nThus, we can choose \u2206f = r\u03932\n2 . Substituting the corresponding parameters from Theorem 5, we\nknow by running PGDli(U0, 8\u0393, 12\u0393 1/2, (\u03c3\n\u22c6 r ) 2\n108\u03931/2 , c, \u03b4, r\u0393\n2\n2 , 10\u03c3 \u22c6 1), with probability 1 \u2212 \u03b4, the output\nwill be \u01eb-close to global minima set X \u22c6 in iterations:\nO\n(\nr\n( \u0393\n\u03c3\u22c6r\n)4 log4 ( d\u0393\n\u03b4\u03c3\u22c6r\n)\n+ \u03c3\u22c61 \u03c3\u22c6r log \u03c3\u22c6r \u01eb\n)\n.\nPart 1: We will now show part 1 of the theorem. Recall PGDli (Algorithm 3) runs PGD (Algorithm 2) first, and then runs gradient descent within 13(\u03c3 \u22c6 r )\n1/2 neighborhood of X \u22c6. It is easy to verify that 13 (\u03c3 \u22c6 r )\n1/2 neighborhood of X \u22c6 is a subset of {U|\u2016U\u20162 \u2264 \u0393}. Therefore, we only need to show that first phase PGD will not leave the region. Specifically, we now use induction to prove the following for PGD:\n1. Suppose at iteration \u03c4 we add perturbation, and denote U\u0303\u03c4 to be the iterate before adding perturbation (i.e., U\u03c4 = U\u0303\u03c4 + \u03be\u03c4 , and U\u0303\u03c4 = U\u03c4\u22121 \u2212 \u03b7\u2207f(U\u03c4\u22121)). Then, \u2016U\u0303\u03c4\u2016 \u2264 12\u0393, and\n2. \u2016Ut\u2016 \u2264 \u0393 for all t \u2265 0.\nBy choice of parameters of Algorithm 2, we know \u03b7 = c8\u0393 . First, consider gradient descent step without perturbations:\n\u2016Ut+1\u2016 =\u2016Ut \u2212 \u03b7\u2207f(Ut)\u2016 = \u2016Ut \u2212 \u03b7(UtU\u22a4t \u2212M\u22c6)Ut\u2016 \u2264\u2016Ut \u2212 \u03b7UtU\u22a4t Ut\u2016+ \u03b7\u2016M\u22c6Ut\u2016 \u2264max\ni [\u03c3i(Ut)\u2212 \u03b7\u03c33i (Ut)] + \u03b7\u2016M\u22c6Ut\u2016\nFor the first term, we know function f(t) = t\u2212\u03b7t3 is monotonically increasing in [0, 1/\u221a3\u03b7]. On the other hand, by induction assumption, we also know \u2016Ut\u2016 \u2264 \u03931/2 \u2264 \u221a 8\u0393/(3c) = 1/ \u221a 3\u03b7. Therefore, the max is taken when i = 1:\n\u2016Ut+1\u2016 \u2264\u2016Ut\u2016 \u2212 \u03b7\u2016Ut\u20163 + \u03b7\u2016M\u22c6Ut\u2016 \u2264\u2016Ut\u2016 \u2212 \u03b7(\u2016Ut\u20162 \u2212 \u03c3\u22c61)\u2016Ut\u2016. (20)\nWe seperate our discussion into following cases.\nCase \u2016Ut\u2016 > 12\u03931/2: In this case \u2016Ut\u2016 \u2265 max{\u2016U0\u2016, 3(\u03c3\u22c61)1/2}. Recall \u03931/2 = 2max{\u2016U0\u2016, 3(\u03c3\u22c61)1/2}. Clearly, \u0393 \u2265 36\u03c3\u22c61 , we know:\n\u2016Ut+1\u2016 \u2264\u2016Ut\u2016 \u2212 \u03b7(\u2016Ut\u20162 \u2212 \u03c3\u22c61)\u2016Ut\u2016 \u2264 \u2016Ut\u2016 \u2212 c 8\u0393 ( 1 4 \u0393\u2212 \u03c3\u22c61) 1 2 \u03931/2\n\u2264\u2016Ut\u2016 \u2212 c 8\u0393 ( 1 4 \u0393\u2212 1 36 \u0393) 1 2 \u03931/2 = \u2016Ut\u2016 \u2212 c 72 \u03931/2.\nThis means that in each iteration, the spectral norm would decrease by at least c72\u0393 1/2.\nCase \u2016Ut\u2016 \u2264 12\u03931/2: From (20), we know that as long as \u2016Ut\u20162 \u2265 \u2016M\u22c6\u2016, we will always have \u2016Ut+1\u2016 \u2264 \u2016Ut\u2016 \u2264 12\u03931/2. For \u2016Ut\u20162 \u2264 \u2016M\u22c6\u2016, we have:\n\u2016Ut+1\u2016 \u2264\u2016Ut\u2016 \u2212 \u03b7(\u2016Ut\u20162 \u2212 \u03c3\u22c61)\u2016Ut\u2016 = \u2016Ut\u2016+ c\n8\u0393 (\u03c3\u22c61 \u2212 \u2016Ut\u20162)\u2016Ut\u2016\n\u2264\u2016Ut\u2016+ ((\u03c3\u22c61)1/2 \u2212 \u2016Ut\u2016)\u00d7 c\n8\u0393 ((\u03c3\u22c61) 1/2 + \u2016Ut\u2016)\u2016Ut\u2016\n\u2264\u2016Ut\u2016+ ((\u03c3\u22c61)1/2 \u2212 \u2016Ut\u2016)\u00d7 c\u03c3\u22c61 4\u0393 \u2264 (\u03c3\u22c61)1/2\nThus, in this case, we always have \u2016Ut+1\u2016 \u2264 12\u03931/2. In conclusion, if we don\u2019t add perturbation in iteration t, we have:\n\u2022 If \u2016Ut\u2016 > 12\u03931/2, then \u2016Ut+1\u2016 \u2264 \u2016Ut\u2016 \u2212 c72\u03931/2.\n\u2022 If \u2016Ut\u2016 \u2264 12\u03931/2, then \u2016Ut+1\u2016 \u2264 12\u03931/2.\nNow consider the iterations where we add perturbation. By the choice of radius of perturbation in Algorithm 2 , we increase spectral norm by at most :\n\u2016\u03bet\u2016 \u2264 \u2016\u03bet\u2016F \u2264 \u221a c\n\u03c72 (\u03c3\u22c6r ) 2 108\u03931/2 \u00b7 8\u0393 \u2264 1 2 \u03931/2\nThe first inequality is because \u03c7 \u2265 1 and c \u2264 1. That is, if before perturbation we have \u2016U\u0303t\u2016 \u2264 1 2\u0393\n1/2, then \u2016Ut\u2016 = \u2016U\u0303t + \u03bet\u2016 \u2264 \u03931/2. On the other hand, according to Algorithm 2, once we add perturbation, we will not add\nperturbation for next tthres = \u03c7\u00b724\u0393 c2\u03c3\u22c6r \u2265 24 c2 \u2265 48c iterations. Let T = min{inf i{Ut+i|\u2016Ut+i\u2016 \u2264 1 2\u0393 1/2}, tthres}:\n\u2016Ut+T\u22121\u2016 \u2264 \u2016Ut\u2016 \u2212 c 72 \u03931/2(T \u2212 1) \u2264 \u03931/2(1\u2212 c(T \u2212 1) 72 )\nThis gives T \u2264 36c < 48c \u2264 tthres. Let \u03c4 > t be the next time when we add perturbation (\u03c4 \u2265 t+ tthres), we immediately know \u2016UT+i\u2016 \u2264 12\u03931/2 for 0 \u2264 i < \u03c4 \u2212 T and \u2016U\u0303\u03c4\u2016 \u2264 12\u03931/2.\nFinally, \u2016U0\u2016 \u2264 12\u03931/2 by definition of \u0393, so the initial condition holds. This finishes induction and the proof of the theorem.\nIn the next subsections we prove the geometric structures.\nC.1 Smoothness and Hessian Lipschitz\nBefore we start proofs of lemmas, we first state some properties about gradient and Hessians. The gradient of the objective function f(U) is\n\u2207f(U) = 2(UU\u22a4 \u2212M\u22c6)U. Furthermore, we have the gradient and Hessian satisfy for any Z \u2208 Rd\u00d7r:\n\u3008\u2207f(U),Z\u3009 = 2\u3008(UU\u22a4 \u2212M\u22c6)U,Z\u3009, and (21) \u22072f(U)(Z,Z) = \u2016UZ\u22a4 + ZU\u22a4\u20162F + 2\u3008UU\u22a4 \u2212M\u22c6,ZZ\u22a4\u3009. (22)\nLemma 6. For any \u0393 \u2265 \u03c3\u22c61, inside the region {U|\u2016U\u20162 < \u0393}, f(U) defined in Eq.(2) is 8\u0393-smooth and 12\u03931/2-Hessian Lipschitz.\nProof. Denote D = {U|\u2016U\u20162 < \u0393}, and recall \u0393 \u2265 \u03c3\u22c61 .\nSmoothness: For any U,V \u2208 D, we have: \u2016\u2207f(U)\u2212\u2207f(V)\u2016F =2\u2016(UU\u22a4 \u2212M\u22c6)U\u2212 (VV\u22a4 \u2212M\u22c6)V\u2016F\n\u22642 [ \u2016UU\u22a4U\u2212VV\u22a4V\u2016F + \u2016M\u22c6(U\u2212V)\u2016F ]\n\u22642 [3 \u00b7 \u0393\u2016U\u2212V\u2016F + \u03c3\u22c61\u2016U\u2212V\u2016F] \u2264 8\u0393 \u00b7 \u2016U\u2212V\u2016F. The last line is due to following decomposition and triangle inequality:\nUU\u22a4U\u2212VV\u22a4V = UU\u22a4(U\u2212V) +U(U\u2212V)\u22a4V + (U\u2212V)V\u22a4V.\nHessian Lipschitz: For any U,V \u2208 D, and any Z \u2208 Rd\u00d7r, according to Eq.(22), we have: |\u22072f(U)(Z,Z)\u2212\u22072f(V)(Z,Z)| = \u2016UZ\u22a4 + ZU\u22a4\u20162F \u2212 \u2016VZ\u22a4 + ZV\u22a4\u20162F\n\ufe38 \ufe37\ufe37 \ufe38\nA\n+2\u3008UU\u22a4 \u2212VV\u22a4,ZZ\u22a4\u3009 \ufe38 \ufe37\ufe37 \ufe38\nB\n.\nFor term A, we have:\nA =\u3008UZ\u22a4 + ZU\u22a4, (U \u2212V)Z\u22a4 + Z(U\u2212V)\u22a4\u3009+ \u3008(U\u2212V)Z\u22a4 + Z(U\u2212V)\u22a4,VZ\u22a4 + ZV\u22a4\u3009 \u2264\u2016UZ\u22a4 + ZU\u22a4\u2016F\u2016(U \u2212V)Z\u22a4 + Z(U\u2212V)\u22a4\u2016F + \u2016(U\u2212V)Z\u22a4 + Z(U\u2212V)\u22a4\u2016F\u2016VZ\u22a4 + ZV\u22a4\u2016F \u22644\u2016U\u2016\u2016Z\u20162F\u2016U\u2212V\u2016F + 4\u2016V\u2016\u2016Z\u20162F\u2016U\u2212V\u2016F \u2264 8\u03931/2\u2016Z\u20162F\u2016U\u2212V\u2016F.\nFor term B, we have:\nB \u2264 2\u2016UU\u22a4 \u2212VV\u22a4\u2016F\u2016ZZ\u22a4\u2016F \u2264 4\u03931/2\u2016Z\u20162F\u2016U\u2212V\u2016F. The inequality is due to following decomposition and triangle inequality:\nUU\u22a4 \u2212VV\u22a4 = U(U\u2212V)\u22a4 + (U\u2212V)V\u22a4. Therefore, in sum we have:\nmax Z:\u2016Z\u2016F\u22641 |\u22072f(U)(Z,Z)\u2212\u22072f(V)(Z,Z)| \u2264 max Z:\u2016Z\u2016F\u22641 12\u03931/2\u2016Z\u20162F\u2016U\u2212V\u2016F\n\u226412\u03931/2\u2016U\u2212V\u2016F.\nC.2 Strict-Saddle Property and Local Regularity\nRecall the gradient and Hessian of objective function is calculated as in Eq.(21) and Eq.(22). We first prove an elementary inequality regarding to the trace of product of two symmetric PSD matrices. This lemma will be frequently used in the proof.\nLemma 18. For A,B \u2208 Rd\u00d7d both symmetric PSD matrices, we have:\n\u03c3min(A)tr(B) \u2264 tr(AB) \u2264 \u2016A\u2016tr(B)\nProof. Let A = VDV\u22a4 be the eigendecomposition of A, where D is diagonal matrix, and V is orthogonal matrix. Then we have:\ntr(AB) = tr(DV\u22a4BV) = d\u2211\ni=1\nDii(V \u22a4BV)ii.\nSince B is PSD, we know V\u22a4BV is also PSD, thus the diagonal entries are non-negative. That is, (V\u22a4BV)ii \u2265 0 for all i = 1, . . . , d. Finally, the lemma follows from the fact that \u03c3min(A) \u2264 Dii \u2264 \u2016A\u2016 and tr(V\u22a4BV) = tr(BVV\u22a4) = tr(B).\nNow, we are ready to prove Lemma 7.\nLemma 7. For f(U) defined in Eq.(2), all local minima are global minima. The set of global minima is X \u22c6 = {V\u22c6R|RR\u22a4 = R\u22a4R = I}. Furthermore, f(U) satisfies:\n1. ( 124 (\u03c3 \u22c6 r ) 3/2, 13\u03c3 \u22c6 r , 1 3(\u03c3 \u22c6 r ) 1/2)-strict saddle property, and\n2. (23\u03c3 \u22c6 r , 10\u03c3 \u22c6 1)-regularity condition in 1 3 (\u03c3 \u22c6 r ) 1/2 neighborhood of X \u22c6. Proof. Let us denote the set X \u22c6 := {V\u22c6R|RR\u22a4 = R\u22a4R = I}, in the end of proof, we will show this set is the set of all local minima (which is also global minima).\nThroughout the proof of this lemma, we always focus on the first-order and second-order property for one matrix U. For simplicity of calculation, when it is clear from the context, we denote U\u22c6 = PX \u22c6(U) and \u2206 = U \u2212 PX \u22c6(U). By definition of X \u22c6, we know U\u22c6 = V\u22c6RU and \u2206 = U\u2212V\u22c6RU, where\nRU = argmin R:RR\u22a4=R\u22a4R=I\n\u2016U\u2212V\u22c6R\u20162F\nWe first prove following claim, which will used in many places across this proof:\nU\u22a4U\u22c6 = U\u22a4V\u22c6RU is a symmetric PSD matrix. (23)\nThis because by expanding the Frobenius norm, and letting the SVD of V\u22c6\u22a4U be ADB\u22a4, we have:\nargmin R:RR\u22a4=R\u22a4R=I \u2016U\u2212V\u22c6R\u20162F = argmin R:RR\u22a4=R\u22a4R=I \u2212\u3008U,V\u22c6R\u3009\n= argmin R:RR\u22a4=R\u22a4R=I \u2212tr(U\u22a4V\u22c6R) = argmin R:RR\u22a4=R\u22a4R=I \u2212tr(DA\u22a4RB)\nSince A,B,R are all orthonormal matrix, we know A\u22a4RB is also orthonormal matrix. Moreover for any orthonormal matrix T, we have:\ntr(DT) = \u2211\ni\nDiiTii \u2264 \u2211\ni\nDii\nThe last inequality is because Dii is singular value thus non-negative, and T is orthonormal, thus Tii \u2264 1. This means the maximum of tr(DT) is achieved when T = I, i.e., the minimum of \u2212tr(DA\u22a4RB) is achieved when R = AB\u22a4. Therefore, U\u22a4V\u22c6RU = BDA\u22a4AB\u22a4 = BDB\u22a4 is symmetric PSD matrix.\nStrict Saddle Property: In order to show the strict saddle property, we only need to show that for any U satisfying \u2016\u2207f(U)\u2016F \u2264 124 (\u03c3\u22c6r )3/2 and \u2016\u2206\u2016F = \u2016U\u2212U\u22c6\u2016F \u2265 13(\u03c3\u22c6r )1/2, we always have \u03c3min(\u22072f(U)) \u2264 \u221213\u03c3\u22c6r .\nLet\u2019s consider Hessian \u22072(U) in the direction of \u2206 = U\u2212U\u22c6. Clearly, we have:\nUU\u22a4 \u2212M\u22c6 = UU\u22a4 \u2212 (U\u2212\u2206)(U\u2212\u2206)\u22a4 = (U\u2206\u22a4 +\u2206U\u22a4)\u2212\u2206\u2206\u22a4\nand by (21):\n\u3008\u2207f(U),\u2206\u3009 =2\u3008(UU\u22a4 \u2212M\u22c6)U,\u2206\u3009 = \u3008UU\u22a4 \u2212M\u22c6,\u2206U\u22a4 +U\u2206\u22a4\u3009 =\u3008UU\u22a4 \u2212M\u22c6,UU\u22a4 \u2212M\u22c6 +\u2206\u2206\u22a4\u3009\nTherefore, by Eq.(22) and above two equalities, we have:\n\u22072f(U)(\u2206,\u2206) =\u2016U\u2206\u22a4 +\u2206U\u22a4\u20162F + 2\u3008UU\u22a4 \u2212M\u22c6,\u2206\u2206\u22a4\u3009 =\u2016UU\u22a4 \u2212M\u22c6 +\u2206\u2206\u22a4\u20162F + 2\u3008UU\u22a4 \u2212M\u22c6,\u2206\u2206\u22a4\u3009 =\u2016\u2206\u2206\u22a4\u20162F \u2212 3\u2016UU\u22a4 \u2212M\u22c6\u20162F + 4\u3008UU\u22a4 \u2212M\u22c6,UU\u22a4 \u2212M\u22c6 +\u2206\u2206\u22a4\u3009 =\u2016\u2206\u2206\u22a4\u20162F \u2212 3\u2016UU\u22a4 \u2212M\u22c6\u20162F + 4\u3008\u2207f(U),\u2206\u3009\nConsider the first two terms, by expanding, we have:\n3\u2016UU\u22a4 \u2212M\u22c6\u20162F \u2212 \u2016\u2206\u2206\u22a4\u20162F = 3\u2016(U\u22c6\u2206\u22a4 +\u2206U\u22c6\u22a4) + \u2206\u2206\u22a4\u20162F \u2212 \u2016\u2206\u2206\u22a4\u20162F =3 \u00b7 tr ( 2U\u22c6\u22a4U\u22c6\u2206\u22a4\u2206+ 2(U\u22c6\u22a4\u2206)2 + 4U\u22c6\u22a4\u2206\u2206\u22a4\u2206+ (\u2206\u22a4\u2206)2 ) \u2212 tr((\u2206\u22a4\u2206)2) =tr ( 6U\u22c6\u22a4U\u22c6\u2206\u22a4\u2206+ 6(U\u22c6\u22a4\u2206)2 + 12U\u22c6\u22a4\u2206\u2206\u22a4\u2206+ 2(\u2206\u22a4\u2206)2 ) =tr((4 \u221a 3\u2212 6)U\u22c6\u22a4U\u22c6\u2206\u22a4\u2206+ (12\u2212 4 \u221a 3)U\u22c6\u22a4(U\u22c6 +\u2206)\u2206\u22a4\u2206+ 2( \u221a 3U\u22c6\u22a4\u2206+\u2206\u22a4\u2206)2) \u2265(4 \u221a 3\u2212 6)tr(U\u22c6\u22a4U\u22c6\u2206\u22a4\u2206) \u2265 (4 \u221a 3\u2212 6)\u03c3\u22c6r\u2016\u2206\u20162F\nwhere the second last inequality is because U\u22c6\u22a4(U\u22c6+\u2206)\u2206\u22a4\u2206 = U\u22c6\u22a4U\u2206\u22a4\u2206 is the product of two symmetric PSD matrices (thus its trace is non-negative); the last inequality is by Lemma 18.\nFinally, in case we have \u2016\u2207f(U)\u2016F \u2264 124(\u03c3\u22c6r )3/2 and \u2016\u2206\u2016F = \u2016U\u2212U\u22c6\u2016F \u2265 13(\u03c3\u22c6r )1/2\n\u03c3min(\u22072f(U)) \u2264 1\n\u2016\u2206\u20162F \u22072f(U)(\u2206,\u2206) \u2264 \u2212(4\n\u221a 3\u2212 6)\u03c3\u22c6r + 4 \u3008\u2207f(U),\u2206\u3009 \u2016\u2206\u20162F\n\u2264\u2212 (4 \u221a 3\u2212 6)\u03c3\u22c6r + 4 \u2016\u2207f(U)\u2016F \u2016\u2206\u2016F \u2264 \u2212(4 \u221a 3\u2212 6.5)\u03c3\u22c6r \u2264 \u2212 1 3 \u03c3\u22c6r\nLocal Regularity: In 13 (\u03c3 \u22c6 r ) 1/2 neigborhood of X \u22c6, by definition, we know,\n\u2016\u2206\u20162F = \u2016U\u2212U\u22c6\u20162F \u2264 1\n9 \u03c3\u22c6r .\nClearly, by Weyl\u2019s inequality, we have \u2016U\u2016 \u2264 \u2016U\u22c6\u2016 + \u2016\u2206\u2016 \u2264 43 (\u03c3\u22c61)1/2, and \u03c3r(U) \u2265 \u03c3r(U\u22c6)\u2212 \u2016\u2206\u2016 \u2265 23(\u03c3\u22c6r )1/2. Moreover, since U\u22c6\u22a4U is symmetric matrix, we have:\n\u03c3r(U \u22c6\u22a4U) =\n1\n2\n(\n\u03c3r(U \u22a4U\u22c6 +U\u22c6\u22a4U)\n)\n\u22651 2\n(\n\u03c3r(U \u22a4U+U\u22c6\u22a4U\u22c6)\u2212 \u2016(U\u2212U\u22c6)\u22a4(U\u2212U\u22c6)\u2016\n)\n\u22651 2\n(\n\u03c3r(U \u22a4U) + \u03c3r(U \u22c6\u22a4U\u22c6)\u2212 \u2016\u2206\u20162F )\n\u22651 2 (1 + 4 9 \u2212 1 9 )\u03c3\u22c6r = 2 3 \u03c3\u22c6r .\nAt a highlevel, we will prove (\u03b1, \u03b2)-regularity property (1) by proving that:\n1. \u3008\u2207f(x),x\u2212 PX \u22c6(x)\u3009 \u2265 \u03b1\u2016x\u2212 PX \u22c6(x)\u20162, and\n2. \u3008\u2207f(x),x\u2212 PX \u22c6(x)\u3009 \u2265 1\u03b2\u2016\u2207f(x)\u20162.\nAccording to (21), we know:\n\u3008\u2207f(U),U \u2212 PX \u22c6(U)\u3009 =2\u3008(UU\u22a4 \u2212M\u22c6)U,\u2206\u3009 = 2\u3008U\u2206\u22a4 +\u2206U\u22c6\u22a4,\u2206U\u22a4\u3009 =2(tr(U\u2206\u22a4U\u2206\u22a4) + tr(\u2206U\u22c6\u22a4U\u2206\u22a4))\n=2(\u2016\u2206\u22a4U\u20162F + tr(U\u22c6\u22a4U\u2206\u22a4\u2206)). (24)\nThe last equality is because \u2206\u22a4U is symmetric matrix. Since U\u22c6\u22a4U is symmetric PSD matrix, and recall \u03c3r(U \u22c6\u22a4U) \u2265 23\u03c3\u22c6r , by Lemma 18 we have:\n\u3008\u2207f(U),U \u2212 PX \u22c6(U)\u3009 \u2265 \u03c3r(U\u22c6\u22a4U)tr(\u2206\u22a4\u2206) \u2265 2\n3 \u03c3\u22c6r\u2016\u2206\u20162F. (25)\nOn the other hand, we also have:\n\u2016\u2207f(U)\u20162F =4\u3008(UU\u22a4 \u2212M\u22c6)U, (UU\u22a4 \u2212M\u22c6)U\u3009 =4\u3008(U\u2206\u22a4 +\u2206U\u22c6\u22a4)U, (U\u2206\u22a4 +\u2206U\u22c6\u22a4)U\u3009\n=4\n\ntr[(\u2206\u22a4UU\u22a4\u2206)U\u22a4U] + 2tr[\u2206\u22a4UU\u22a4U\u22c6\u2206\u22a4U] \ufe38 \ufe37\ufe37 \ufe38\nA\n+tr(U\u22c6\u22a4UU\u22a4U\u22c6\u2206\u22a4\u2206) \ufe38 \ufe37\ufe37 \ufe38\nB\n\n .\nFor term A, by Lemma 18, and \u2206\u22a4U being a symmetric matrix, we have:\nA \u2264 \u2016U\u22a4U\u2016\u2016\u2206\u22a4U\u20162F + 2\u2016U\u22a4U\u22c6\u2016\u2016\u2206\u22a4U\u20162F \u2264 ( 16\n9 +\n8 3 )\u03c3\u22c61\u2016\u2206\u22a4U\u20162F \u2264 5\u03c3\u22c61\u2016\u2206\u22a4U\u20162F\nFor term B, by Eq.(23) we can denote C = U\u22c6\u22a4U = U\u22a4U\u22c6 which is symmetric PSD matrix, by Lemma 18, we have:\nB =tr(C2\u2206\u22a4\u2206) = tr(C(C1/2\u2206\u22a4\u2206C1/2))\n\u2264\u2016C\u2016tr(C1/2\u2206\u22a4\u2206C1/2) = \u2016C\u2016tr(C\u2206\u22a4\u2206) \u2264 4 3 \u03c3\u22c61tr(U \u22c6\u22a4U\u2206\u22a4\u2206).\nCombining with (24) we have:\n\u2016\u2207f(U)\u20162F \u2264 \u03c3\u22c61(20\u2016\u2206\u22a4U\u20162F + 16 3 tr(U\u22c6\u22a4U\u2206\u22a4\u2206)) \u2264 10\u03c3\u22c61\u3008\u2207f(U),U \u2212 PX \u22c6(U)\u3009. (26)\nCombining (25) and (26), we have:\n\u3008\u2207f(U),U\u2212 PX \u22c6(U)\u3009 \u2265 1\n3 \u03c3\u22c6r\u2016U\u2212 PX \u22c6(U)\u20162F +\n1\n20\u03c3\u22c61 \u2016\u2207f(U)\u20162F."}], "references": [{"title": "Finding approximate local minima for nonconvex optimization in linear time", "author": ["Naman Agarwal", "Zeyuan Allen-Zhu", "Brian Bullins", "Elad Hazan", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1611.01146,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Global optimality of local search for low rank matrix recovery", "author": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1605.07221,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2016}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel J Candes", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Candes et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Candes et al\\.", "year": 1985}, {"title": "Gradient descent efficiently finds the cubic-regularized non-convex newton step", "author": ["Yair Carmon", "John C Duchi"], "venue": "arXiv preprint arXiv:1612.00547,", "citeRegEx": "Carmon and Duchi.,? \\Q2016\\E", "shortCiteRegEx": "Carmon and Duchi.", "year": 2016}, {"title": "Accelerated methods for nonconvex optimization", "author": ["Yair Carmon", "John C Duchi", "Oliver Hinder", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1611.00756,", "citeRegEx": "Carmon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carmon et al\\.", "year": 2016}, {"title": "The loss surface of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "A trust region algorithm with a worst-case iteration complexity of\\ mathcal {O}(\\ epsilon\u02c6{-3/2}) for nonconvex optimization", "author": ["Frank E Curtis", "Daniel P Robinson", "Mohammadreza Samadi"], "venue": "Mathematical Programming,", "citeRegEx": "Curtis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Curtis et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In COLT,", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Matrix completion has no spurious local minimum", "author": ["Rong Ge", "Jason D Lee", "Tengyu Ma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2016}, {"title": "On decompositional algorithms for uniform sampling from n-spheres and n-balls", "author": ["Radoslav Harman", "Vladim\u0131\u0301r Lacko"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Harman and Lacko.,? \\Q2010\\E", "shortCiteRegEx": "Harman and Lacko.", "year": 2010}, {"title": "Computing matrix squareroot via non convex local search", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1507.05854,", "citeRegEx": "Jain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2015}, {"title": "Linear convergence of gradient and proximalgradient methods under the Polyak-Lojasiewicz condition", "author": ["Hamed Karimi", "Julie Nutini", "Mark Schmidt"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Karimi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2016}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "Gradient descent only converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "The power of normalization: Faster evasion of saddle points", "author": ["Kfir Y Levy"], "venue": "arXiv preprint arXiv:1611.04831,", "citeRegEx": "Levy.,? \\Q2016\\E", "shortCiteRegEx": "Levy.", "year": 2016}, {"title": "Introductory lectures on convex programming volume", "author": ["Yu Nesterov"], "venue": "i: Basic course. Lecture notes,", "citeRegEx": "Nesterov.,? \\Q1998\\E", "shortCiteRegEx": "Nesterov.", "year": 1998}, {"title": "Cubic regularization of newton method and its global performance", "author": ["Yurii Nesterov", "Boris T Polyak"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov and Polyak.,? \\Q2006\\E", "shortCiteRegEx": "Nesterov and Polyak.", "year": 2006}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Non-square matrix sensing without spurious local minima via the burer-monteiro approach", "author": ["Dohyung Park", "Anastasios Kyrillidis", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1609.03240,", "citeRegEx": "Park et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Park et al\\.", "year": 2016}, {"title": "Gradient methods for the minimisation of functionals", "author": ["Boris T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1963\\E", "shortCiteRegEx": "Polyak.", "year": 1963}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Complete dictionary recovery over the sphere i: Overview and the geometric picture", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "A geometric analysis of phase retrieval", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "In Information Theory (ISIT),", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Guaranteed matrix completion via non-convex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Sun and Luo.,? \\Q2016\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2016}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "author": ["Qinqing Zheng", "John Lafferty"], "venue": "arXiv preprint arXiv:1605.07051,", "citeRegEx": "Zheng and Lafferty.,? \\Q2016\\E", "shortCiteRegEx": "Zheng and Lafferty.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "This is notably true in the deep learning setting, where gradients can be computed efficiently via backpropagation [Rumelhart et al., 1988].", "startOffset": 115, "endOffset": 139}, {"referenceID": 16, "context": "within l(f(x0)\u2212 f\u22c6)/\u01eb2 iterations [Nesterov, 1998], where x0 is the initial point and f\u22c6 is the optimal value of f .", "startOffset": 34, "endOffset": 50}, {"referenceID": 8, "context": ", in tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 9, "context": ", 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]).", "startOffset": 27, "endOffset": 44}, {"referenceID": 13, "context": ", 2016], and certain classes of deep neural networks [Kawaguchi, 2016]).", "startOffset": 53, "endOffset": 70}, {"referenceID": 5, "context": "Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014].", "startOffset": 121, "endOffset": 147}, {"referenceID": 17, "context": "For \u03c1-Hessian Lipschitz functions (see Definition 5), these points are defined as [Nesterov and Polyak, 2006]:", "startOffset": 82, "endOffset": 109}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks.", "startOffset": 26, "endOffset": 524}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2).", "startOffset": 26, "endOffset": 1005}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2). Lee et al. [2016] proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding noise.", "startOffset": 26, "endOffset": 1244}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2). Lee et al. [2016] proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding noise. However, this result does not bound the number of steps needed to reach a local minimum. Though these results establish that gradient descent can find local minima in a polynomial number of iterations, they are still far from being efficient. For instance, the number of iterations required in Ge et al. [2015] is at least \u03a9(d4), where d is the underlying dimension.", "startOffset": 26, "endOffset": 1685}, {"referenceID": 16, "context": "of gradient descent to first-order stationary points [Nesterov, 1998], up to log factors.", "startOffset": 53, "endOffset": 69}, {"referenceID": 6, "context": "Trust region algorithms [Curtis et al., 2014] can also achieve the same performance if the parameters are chosen carefully.", "startOffset": 24, "endOffset": 45}, {"referenceID": 0, "context": ", 2015, Sun and Luo, 2016, Bhojanapalli et al., 2016]. While there are not many results that show global convergence for non-convex problems, Jain et al. [2015] show that gradient descent yields global convergence rates for matrix square-root problems.", "startOffset": 27, "endOffset": 161}, {"referenceID": 0, "context": ", 2015, Sun and Luo, 2016, Bhojanapalli et al., 2016]. While there are not many results that show global convergence for non-convex problems, Jain et al. [2015] show that gradient descent yields global convergence rates for matrix square-root problems. Although these results give strong guarantees, the analyses are heavily tailored to specific problems, and it is unclear how to generalize them to a wider class of non-convex functions. For general non-convex optimization, there are a few previous results on finding second-order stationary points. These results can be divided into the following three categories, where, for simplicity of presentation, we only highlight dependence on dimension d and \u01eb, assuming that all other problem parameters are constant from the point of view of iteration complexity: Hessian-based: Traditionally, only second-order optimization methods were known to converge to second-order stationary points. These algorithms rely on computing the Hessian to distinguish between first- and second-order stationary points. Nesterov and Polyak [2006] designed a cubic regularization algorithm which converges to an \u01eb-second-order stationary point in O(1/\u01eb1.", "startOffset": 27, "endOffset": 1079}, {"referenceID": 0, "context": "Agarwal et al. [2016] and Carmon et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agarwal et al. [2016] and Carmon et al. [2016] presented accelerated algorithms that can find an \u01eb-second-order stationary point in O(log d/\u01eb7/4) steps.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Agarwal et al. [2016] and Carmon et al. [2016] presented accelerated algorithms that can find an \u01eb-second-order stationary point in O(log d/\u01eb7/4) steps. Also, Carmon and Duchi [2016] showed by running gradient descent as a subroutine to solve the subproblem of cubic regularization (which requires Hessian-vector product oracle), it is possible to find an \u01eb-second-order stationary pointin O(log d/\u01eb2) iterations.", "startOffset": 0, "endOffset": 183}, {"referenceID": 8, "context": "Ge et al. [2015] showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/\u01eb) iterations, with polynomial of order at least four.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Ge et al. [2015] showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/\u01eb) iterations, with polynomial of order at least four. This was improved in Levy [2016] to O(d3 \u00b7poly(1/\u01eb)) using normalized gradient descent.", "startOffset": 0, "endOffset": 205}, {"referenceID": 16, "context": "Theorem 2 ([Nesterov, 1998]).", "startOffset": 11, "endOffset": 27}, {"referenceID": 16, "context": "We instead follow the convention of Nesterov and Polyak [2006] by choosing \u01ebH = \u221a \u03c1\u01ebg to reflect the natural relations between the gradient and the Hessian.", "startOffset": 36, "endOffset": 63}, {"referenceID": 16, "context": "We instead follow the convention of Nesterov and Polyak [2006] by choosing \u01ebH = \u221a \u03c1\u01ebg to reflect the natural relations between the gradient and the Hessian. This definition of \u01eb-second-order stationary point can also differ by reparametrization (and scaling), e.g. Nesterov and Polyak [2006] use \u01eb\u2032 = \u221a \u01eb/\u03c1.", "startOffset": 36, "endOffset": 292}, {"referenceID": 10, "context": "Note that uniform sampling from a d-dimensional ball can be done efficiently by sampling U 1 d \u00d7 Y \u2016Y\u2016 where U \u223c Uniform([0, 1]) and Y \u223c N (0, Id) [Harman and Lacko, 2010].", "startOffset": 147, "endOffset": 171}, {"referenceID": 1, "context": "This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. [2016], Zheng and Lafferty [2016], where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.", "startOffset": 131, "endOffset": 158}, {"referenceID": 1, "context": "This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. [2016], Zheng and Lafferty [2016], where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.", "startOffset": 131, "endOffset": 185}, {"referenceID": 12, "context": "The interested reader can refer to Karimi et al. [2016] for other relaxed and alternative notions of convexity, which can also be potentially combined with Assumptions A1andA2 to yield convergence results of a similar flavor as that of Theorem 5.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "[Ge et al., 2015]).", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "[Ge et al., 2015]) do not work when the step size and perturbation do not depend polynomially in dimension d.", "startOffset": 0, "endOffset": 17}], "year": 2017, "abstractText": "This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost \u201cdimension-free\u201d). The convergence rate of this procedure matches the wellknown convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.", "creator": "LaTeX with hyperref package"}}}