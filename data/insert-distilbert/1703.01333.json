{"id": "1703.01333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Towards Monetary Incentives in Social Q&A Services", "abstract": "community problem based question answering ( cqa ) services receive a large volume traffic of questions today. it is increasingly challenging to motivate domain experts to adequately give timely customers answers. recently, payment - based cqa services dramatically explore new incentive models to engage real - world game experts management and celebrities by allowing them to accurately set a price on their answers. in this paper, we perform a data - processing driven analysis methodology on fenda, a payment - based independent cqa service that has gained initial mainstream success with this incentive model. using a large dataset of 220k daily paid questions ( worth 1 million usd ) over two months, we examine how monetary incentives affect different involved players in the system and their predicted over - time engagement. our study reveals several key findings : while monetary incentive branding enables quick answers from experts, it also drives certain users to aggressively game share the systems for profits. in addition, this incentive model turns cqa subscription service into a supplier - driven marketplace where users may need to proactively publicly adjust their price as needed. we find famous people are unwilling to lower their price, which in turn hurts their equity income and engagement - level over time. based on studying our results, we discuss all the implications to future payment - based cqa design.", "histories": [["v1", "Fri, 3 Mar 2017 20:36:38 GMT  (1140kb)", "https://arxiv.org/abs/1703.01333v1", null], ["v2", "Fri, 28 Apr 2017 01:48:18 GMT  (1840kb)", "http://arxiv.org/abs/1703.01333v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["steve t k jan", "chun wang", "qing zhang", "gang wang"], "accepted": false, "id": "1703.01333"}, "pdf": {"name": "1703.01333.pdf", "metadata": {"source": "CRF", "title": "Towards Monetary Incentives in Social Q&A Services", "authors": ["Steve T.K. Jan", "Chun Wang", "Qing Zhang", "Gang Wang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n01 33\n3v 2\n[ cs\n.A I]\n2 8\nA pr\n2 01\n7\nCommunity-based question answering (CQA) services are facing key challenges to motivate domain experts to provide timely answers. Recently, CQA services are exploring new incentive models to engage experts and celebrities by allowing them to set a price on their answers. In this paper, we perform a data-driven analysis on two emerging payment-based CQA systems: Fenda (China) and Whale (US). By analyzing a large dataset of 220K questions (worth 1 million USD collectively), we examine how monetary incentives affect different players in the system. We find that, while monetary incentive enables quick answers from experts, it also drives certain users to aggressively game the system for profits. In addition, in this supplier-driven marketplace, users need to proactively adjust their price to make profits. Famous people are unwilling to lower their price, which in turn hurts their income and engagement over time. Finally, we discuss the key implications to future CQA design."}, {"heading": "INTRODUCTION", "text": "The success of community based question answering (CQA) services depends on high-quality content from users, particularly from domain experts. With highly engaging experts, services like Quora and StackOverflow attract hundreds of millions of visitors worldwide [41]. However, for most CQA systems, domain experts are answering questions voluntarily for free. As the question volume keeps growing, it becomes difficult to draw experts\u2019 attention to a particular question, let alone getting answers on-demand [33].\nTo motivate experts, one possible direction is to introduce monetary incentives [10]. Recently, Quora started a beta test on \u201cknowledge prize\u201d, which allows users to put cash rewards on their questions.While Quora is slowly accumulating interested users (less than 10 paid answers per month), another payment-based service called Fenda1 is rising quickly in China. Fenda is a social network app that connects users to well-known domain experts and celebrities to ask questions with payments. Launched in May 2016, Fenda quickly gained 10 million registered users, 500K paid questions, and 2 million US dollar revenue in just two months [39].\nFenda leads a new wave of payment-based CQA services that socially engage users with real-world domain experts. Similar services are emerging in China (Zhihu, Weibo QA) and US (Whale, Campfire.fm). The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].\n1http://fd.zaih.com/fenda\nSo, is monetary incentive the solution to strong expert engagement in CQA systems? How does monetary incentive affect the behavior of different players in the system and their overtime engagement? These questions are critical for paymentbased CQA design, and platforms like Fenda provide a unique opportunity to study them. First, Fenda is the first supplierdriven CQA marketplace, where answerers (experts) set their own price. Users ask questions to a specific person instead of an anonymous crowd using payments. In addition, Fenda\u2019s incentive model not only rewards answerers, but also those who ask good questions. After a question is answered, other users need to pay a small amount ($0.14) to listen to the answer. This money is split evenly between the asker and the answerer (Figure 1). A good question may attract enough listeners to compensate the initial question fee.\nIn this paper, we describe our experience and findings in an effort to understand the impact of monetary incentives on CQA systems, through a detailed measurement of Fenda (China) and a similar system Whale2 (US). We collected a dataset of 88,540 users and 212,082 answers from Fenda (two months in 2016), and 1,419 users and 9,199 answers from Whale (6 months during 2016\u20132017), involving more than 1 million USD transactions. Given the drastic differences between payment-based CQA systems and mainstream systems such as Quora and StackOverflow, our study has significant implications for the future direction of CQA design.\nOur study has a number of key findings.\n\u2022 First, we seek to understand the effectiveness of monetary incentives to engage domain experts. Our result shows this attempt is successful. Both Fenda and Whale attract a small group of high-profile experts and celebrities who make significant contributions to the CQA community. For example, Fenda experts count for 0.5% of the user population, but have contributed a quarter of all answers and driven nearly half of the financial revenue.\n\u2022 Second, we analyze how the incentive model affects user behavior, and find a mixed effect. On the positive side, monetary incentive enables quick answers (average delay 10\u2013 23 hours) and motivates users to ask good questions (40% of the Fenda questions successfully drew enough interested audience to cover the askers\u2019 cost). However, we did find a small number of manipulative users including \u201cbounty hunters\u201d who aggressively asked questions to make money from listeners, and \u201ccollusive users\u201d who work together to manipulate their perceived popularity.\n2https://askwhale.com/\n\u2022 Third, we study the dynamics between money and user engagement over time. In a supplier-driven CQA marketplace, users need to set the price of their answers. We find different pricing strategies of users have distinct impacts on their own engagement level. Users who proactively adjust their price are more likely to increase income and engagement level. Certain highly famous people, however, are unwilling to lower their price, which in turn hurts their income and social engagement.\nTo the best of our knowledge, this is the first empirical study on supplier-driven CQA marketplaces. Our study provides practical guidelines for other arising payment-based CQA services (Quora knowledge prize, Zhihu, Campfire.fm) and reveals key implications for future CQA system design. We believe this is a first step towards understanding the economy of community-based knowledge sharing."}, {"heading": "RELATED WORK", "text": "Community Based Question Answering (CQA). In recent years, researchers have studied CQA services from various aspects [32]. Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27]. Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16]. Finally, researchers also studied Q&A activities in online social networks [25, 7]. As the sizes of CQA systems rapidly grow, it becomes challenging to engage with experts for timely and high-quality answers [33].\nUser Motivations in CQA. A successful CAQ system requires active and sustainable user participations. Prior works have summarized three main user motivations to answer questions online: \u201cintrinsic\u201d, \u201csocial\u201d and \u201cextrinsic\u201d [13]. Intrinsic motivation refers to the psychic reward (e.g., enjoyment) that users gain through helping others [42, 24]. Social factors refer to the benefits of social interactions, e.g., gaining respect and enhancing reputation. Intrinsic and social factors are critical incentives for non-payment based CQA services [13]. Extrinsic factors refer to money and virtual rewards (e.g., badges and credit points) [24, 6].\nMonetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19]. Most of them are defunct. Compared with Fenda and Whale, these systems focus on building a CQA market on an anonymous crowd, instead of a social network that engages real-world experts. Users are primarily driven by financial incentives without a strong sense of community [20, 10]. This is concerning since research shows monetary incentive plays an important role in getting users started in CQA, but it is the social factors that contribute to the persistent participation [28].\nResearchers have studied the impact of monetary incentives but the conclusions vary. Some researchers find that monetary incentives improves the answer quality [9] and response rate in social Q&A [43]. Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11]. Studies also show that payment-"}, {"heading": "Asker Answerer", "text": "based Q&A can reduce low-quality questions since users are more selective regarding what to ask [10, 11].\nCrowdsourcing Marketplace. Broadly speaking, a paymentbased CQA service is a specialized crowdsourcing marketplace. Today, most crowdsourcing marketplaces (e.g. Mechanical Turk) are \u201ccustomer-driven\u201d where customers post their tasks and set the task price [17]. In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14]. Fenda and Whale, on the other hand, represent \u201csupplier-driven\u201dmarketplaces where experts (the suppliers) get to set the price for their answers (products). Part of our analysis is to understand users\u2019 pricing strategies and its impact on their Q&A activities and financial income."}, {"heading": "RESEARCH QUESTIONS AND METHOD", "text": "Systems like Fenda andWhale are leading the way to socially engage with real-world experts for question answering. The introduction of monetary incentives makes user interactions even more complex. If not carefully designed, monetary incentives can lead the systems down to the wrong path with users chasing financial profits and losing engagement in the long run. In this paper, we use Fenda as the primary platform to investigate how monetary incentives impact the user behavior and engagement-level in CQA systems. We also include Whale (a younger and smaller system) in our analysis for comparison and validation purposes.\nWe choose Fenda and Whale for two main reasons. First, Fenda andWhale represent the first supplier-drivenCQAmarketplaces with a unique incentive model to motivate both question askers and respondents. Second, the system (Fenda in particular) has received an initial success with a significant volume of data and revenue flow. We aim to understand the reasons behind their success and potential problems moving forward, which will benefit future CQA design. We did not include Quora since Quora hasn\u2019t accumulated sufficient paid content (e.g.,< 10 paid answers per month).\nBackground of Fenda. Fenda is a payment-basedQ&A app in China, which connects users in a Twitter-like social network. Launched in May 2016, Fenda quickly gained 10 million registered users and over 2 million US dollars worth of questions answers in the first two months [39, 18].\nAs shown in Figure 1, Fenda has a unique monetary incentive model to reward both question askers and answerers. A user (asker) can ask another user (answerer) a question by paying the price set by the answerer. The answerer then responds over the phone by recording a 1-minute audio message. If the answerer doesn\u2019t respond within 48 hours, the payment will be refunded. Any other user on Fenda can listen to the answer by paying a fixed amount of 1 Chinese Yuan ($0.14), and it will be split evenly between the asker and answerer. A good question may attract enough listeners to compensate the initial cost for the asker. Users set the price for their answers and can change the price anytime. Fenda charges 10% of the money made by any user.\nThere are two types of users on Fenda: verified real-world experts (e.g., doctors, entrepreneurs, movie stars) and normal users. There is an expert list that contains all the experts that have been verified and categorized by the Fenda administrators. Users can browse questions from the social news feed or from the public stream of popular answers (a small sample). To promote user engagement, Fenda selects 2-4 answers daily on the public stream for free-listening for a limited time.\nBackground of Whale. Whale is a highly similar system launched in the US in September 2016. There are a few differences: First, Whale users record video (instead of audio) as their answers. Second, Whale has free questions and paid questions. For paid questions, it is also the answerer that sets the price, but Whale takes a higher cut (20%) from the question fee. Third, listeners use the virtual currency \u201cwhale coins\u201d to watch the paid answers. Users can receive a few free coins from the platform after logging-in each day, and they may also purchase paid coins in bulks ($0.29 \u2013 $0.32 per coin). Only when a listener uses paid coins to unlock a question will the asker and answerer receive the extra payment ($0.099 each).\nOur Questions. In the following, we use Fenda andWhale as the platform to analyze how monetary incentives impact user behavior and their engagement-level. We take a data-driven approach to answer the following key questions.\n\u2022 First, as an expert-driven CQA system, to what extent does the system rely on experts to generate content and revenue? What types of experts are more likely to make a profit?\n\u2022 Second, how does the monetary incentive affect the question answering process? Does money truly enable on-demand question answering from experts? Can users make money by asking (good) questions? Will monetary incentives encourage users to game the system for profits?\n\u2022 Third, in this supplier-driven market, how do users set and dynamically adjust the price of their answers? How does the pricing strategy affect their income and engagement-level over time?"}, {"heading": "DATA COLLECTION", "text": "We start by collecting a large dataset from Fenda and Whale through their mobile APIs. Our data collection focused on user profiles, which contained a full list of historical questions answered by the user. To obtain a large set of active users,\nwe explored different options (some of which did not work). First, we find that there is no centralized list to crawl all registered users. Second, a user\u2019s follower list is not public (only the total number is visible), and thus social graph crawling is not feasible. To these ends, we started our crawling from the expert list. For each expert, we collected their answered questions and the askers of those questions. Then we collected the askers\u2019 profiles to get their answered question list and extract new askers. We repeated this process until no new users appeared. In this way, we collected a large set of active users who asked or answered at least one question.3\nWe collected data from Fenda in July 2016. The dataset contains 88,540 user profiles and 212,082 question-answer pairs ranging from May 12 to July 27, 2016. Each question is characterized by the asker\u2019s userID, question text, a timestamp, question price, and the number of listeners. Each answer is characterized by the answerer\u2019s userID, a length of the audio and a timestamp. UserIDs in our dataset have been fully anonymized. We briefly estimated the coverage of the Fenda dataset. Fenda announced that they had 500,000 answers as of June 27, 2016 [39]. Up to the same date, our dataset covers 155,716 answers (about 31%). For Whale, we collected 1,419 user profiles and 9,199 question-answer pairs (1114 paid questions and 8085 free questions) from September 7, 2016 to March 8, 2017. It is difficult to estimate the coverage of the Whale dataset since there is no public statistics on the Whale user population. Table 1 shows a summary of our data."}, {"heading": "ENGAGING WITH DOMAIN EXPERTS", "text": "As a CQA system driven by real-world experts, we first explore the roles and impact of domain experts in the system. More specifically, we examine the contributions of domain experts to the community in terms of generating content and driving financial revenue.\nFenda maintains a list of verified experts and celebrities, who are typically already well-known in their respective domains. As of the time of data collection, there were 4370 verified experts classified into 44 categories by Fenda administrators. We refer these 4,370 users as experts and the rest 84,170 users as normal users. Whale has a similar expert list (118 experts), and we refer the rest 1301 Whale users as normal users."}, {"heading": "Question Answering", "text": "The small group of experts have contributed to a significant portion of the answers. Out of the 212K answers in the Fenda dataset, 171K (81%) are from experts. Using this dataset, we can briefly estimate the experts\u2019 contribution in the context of the entire network. On June 27 2016, Fenda officially announced total 500K answers and 10 million users [39]. Up to the same date, our dataset shows the 4,370 experts (0.44% of the population) have contributed 122K answers (24.4% of total answers). Individually, experts in Fenda (FD) also\n3Our study has received IRB approval: protocol # 16-1143.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20\nC D\nF o\nf A ns\nw er\ns\nPrice ($)\nWH-Normal WH-Expert FD-Normal FD-Expert\nFigure 4. Price of each answer.\n0\n0.2\n0.4\n0.6\n0.8\n1\n100 101 102 103 104\nC D\nF o\nf A ns\nw er\ns\n# of Listeners\nFD-Normal FD-Expert WH-Normal WH-Expert\nFigure 5. # of Listeners per answer.\nanswered significantly more questions than normal users as shown in Figure 2. Whale (WH) has a similar situation where 118 experts (8% of users) have contributed 4,967 answers (54% of answers)."}, {"heading": "Money", "text": "Experts play an important role in driving revenue. In total, the questions in the Fenda dataset were worth $1,169,994 including payments from askers and listeners.4 Experts\u2019 answers generated $1,106,561, counting for a dominating 95% of total revenue in our dataset. To gauge experts\u2019 contribution in the context of the entire network, we again performed an estimation: Fenda reached 2 million revenue as of June 27 in 2016 [39]. Up to this same date (June 27), expert answers in our dataset have attracted $909,876, counting for a significant 45% of the 2 million revenue. Figure 4 and Figure 5 show that, on average, experts charge higher ($2.9 vs. $1.0) and draw more listeners (27 vs. 5) than normal users.\nIndividually, experts also make more money than normal users. Figure 3 shows the total income for users who answered at least one question. On Fenda, 50% of experts made more than $23, while a small group of experts (5%) made more than $1000. The highest earning is $33,130 by Sicong Wang, a businessman and the son of a Chinese billionaire. He answered 31 questions related to gossip and investment. He charged $500 for each of his answers, which drew 9484 listeners ($664 extra earning) per answer on average.\nOn Whale, experts are also the major contributors to the revenue flow. The total collected questions on Whale worth $2,309 and experts contributed to $2,028 (89%). Compared with Fenda (FD), Whale (WH) users earned significantly less money (Figure 3). A possible reason, as shown in Figure 4, is that most users (more than 80%) provide answers for free.\n4We convert Chinese Yuan to US dollar based on $1 = 6.9 Yuan."}, {"heading": "Expert Categories", "text": "Experts of different categories have distinct earning patterns. Table 2 shows the top 10 categories ranked by the total earnings per category. In Fenda, the most popular experts are related to professional consulting. The top category is health, followed by career, business, and relationship. In the health category, many experts are real-world physicians and pediatricians. They give Fenda users medical advice on various (non-life-threatening) issues such as headache and flu with the expense of several dollars. Other popular categories such as movies and entertainments contain questions to celebrities about their insider experience, gossip and opinions on trending events.\nWhale, on the other hand, has fewer experts. The highest earning experts are related to startups and technology. Note that Whale experts can belong to multiple categories, so the percentage of accumulated income exceeds 100%.\nIn Figure 6, we further illustrate the distinct earning patterns of Fenda experts.We omit the result forWhale due to its short expert list. For each category, we compute the average price and the number of answers per expert. The red lines represent the average values across all experts, which divide them into 4 sections. Experts in health, entertainment, relationship, and real estate often charge high and answer many questions. These experts are among the topearning groups; Experts in business set the price high but don\u2019t answer many questions; Less-serious categories such as funny and comics have fewer and less expensive questions. Finally, digital represents experts who answer lots of questions with a low price. The results also reflect the different user perceived values for different domain knowledge.\n0\n0.2\n0.4\n0.6\n0.8\n1\n-6 -4 -2 0 2 4 6\nIncome per Asker per Question ($)\nFenda Whale\nFigure 8. Income of askers per question.\n0\n0.2\n0.4\n0.6\n0.8\n1\n-20 -10 0 10 20\nC D\nF o\nf A sk\ner s\nTotal Income per Asker ($)\nFenda Whale\nFigure 9. Total income of askers."}, {"heading": "IMPACT OF MONETARY INCENTIVES", "text": "So far we show that Fenda and Whale are highly dependent on domain experts\u2019 contribution. Then the question is how to motivate experts to deliver timely and high-quality answers. In this section, we perform extensive analysis on themonetary incentivemodel to understand its impact on user behavior. Noticeably, Fenda andWhale use money to reward both question answerers and askers. To this end, we first analyze answerers to understand how they price their answers, and whether payments lead to on-demand responses. Second, we focus on askers analyzing whether and how users make money by asking the right questions. Finally, we seek to identify abnormal users such as \u201cbounty hunters\u201d who aggressively or strategically game the system for profits."}, {"heading": "Answerers", "text": "To motivate users (particularly domain experts), both Fenda and Whale allow users to determine the price for their answers. In the following, we investigate how money affects the way users answer questions. Particularly, we examine if monetary incentives truly enable on-demand quick answers.\nSetting the Answer Price. To understand how users set a price for their answers, we calculate the Pearson correlation [31] between a user\u2019s price and different behaviormetrics. In Table 3, we observe that the price has positive and significant correlations with the number of followers, listeners, and answered questions. A possible explanation is that users with many followers and listeners are real-world celebrities who have the confidence to set a higher price. The higher price may also motivate them to answer more questions. Note that these are correlation results, which do not reflect causality.\nSurprisingly, there is no significant correlation between price and response time (for both Fenda and Whale). This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].\nAnswering On-demand? We further examine the response time to see if monetary incentives truly enable answering questions on-demand. As shown in Figure 7, answers arrive\nfast on Fenda: 33% of answers arrived within an hour and 85% arrived within a day. Note that there is a clear cut-off at 48 hours. This is the time when un-answered questions will be refunded, which motivates users to answer questions quickly. After 48 hours, users can still answer those questions for free. We find that only 0.7% of the answers arrived after the deadline, but we cannot estimate how many questions remain unanswered due to the lack of related data. Despite the high price charged by experts, experts respond slower than normal users.\nThe result for Whale is very similar. Figure 7 shows that for paid questions, 50%\u201370% of answers arrived within a day and normal users respond faster than experts. Comparing to Fenda, Whale has a slightly longer delay possibly because recording a video incurs more overhead than recording a voice message.\nWe then compare Fenda and Whale with other CQA systems in Table 4. The response delay in Fenda and Whale is shorter than that of Google Answers and StackOverflow, but longer than that of Yahoo Answers. As payment-based systems, Fenda/Whale beats Google Answers probably because Fenda/Whale only asks for a short audio/video, while Google Answers require lengthy text. Compared to Yahoo Answers, we believe it is the crowdsourcing factor that plays the role. Systems like Yahoo Answers crowdsource questions to a whole community where anyone could deliver the answer. Instead, Fenda/Whale\u2019s question is targeted to a specific user, which may lead to a longer delay even with payments."}, {"heading": "Askers", "text": "Fenda and Whale implement the first monetary incentive model to reward users for asking good questions.More specifically, once a user\u2019s question gets answered, this user (the question asker) can earn a small amount of money from people who want to listen to the answer. This model, if executed as designed, should motivate users to contribute high-quality questions for the community.\nCan Askers Make Money? For each question, the question asker\u2019s income is half of listeners\u2019 payments, with Fenda\u2019s commission fee and initial question fee deducted. Our result shows that Fenda askers are motivated to ask good questions that attract broad interests. As shown in Figure 8, out of all questions, 40% have successfully attracted enough listeners to return a positive profit to the asker. For individual askers, Figure 9 shows 40% of them have a positive total income. However, for Whale, the vast majority of askers did not earn money. Part of the reason is most people only ask free questions. More importantly, Whale gives away free coins every day to motivate users to login. If a listener uses free coins (instead of paid coins), the asker will not receive any money.\nHow Do Askers Make Money? To understand why certain users make money (and others don\u2019t), we compare askers who have positive income with those with negative income in Table 5. Specifically, we examine to whom they ask questions (i.e., the number followers and listeners of the answerer), average question price, and total questions asked. A two-sample t-Test [31] shows the significance of the differences between the two groups of askers.\nOn Fenda, users of positive income are more likely send questions to people who have more listeners and charge less. The counter-intuitive result is the number of followers: asking people with more followers is more likely to lose money. A possible explanation is the inherent correlation between a user\u2019s number of followers and her answer price \u2014 famous people would charge higher and the money from listeners cannot cover the initial cost. We also observe that askers with a higher income often asked more questions. Again, correlation does not reflect causality: it is possible that the positive income motivates users to ask more questions, or people who asked more questions get more experienced in earning money.\nIt is hard to interpret the Whale results in Table 5 since only a very small of fraction of askers have a positive income (Figure 9). Noticeably, askers with positive income exclusively ask free questions (average price = 0)."}, {"heading": "Abnormal Users", "text": "Next, we examine suspicious users in the CQA market who seek to game the system for financial profits.\nBounty Hunters. We first focus on askers who aggressively ask questions to gain profits (or \u201cbounty hunters\u201d). Our intuition is that those users would ask a lot of questions, particular to experts. To identify potential bounty hunters in Fenda, we examine outliers in Figure 10, which is a scatter plot for the number of questions a user asked versus the ratio of questions\nto experts. We find clear outliers at the right side (e.g., users with >100 questions). Most of these users end up with positive income. They asked way more questions than other users (who asked 2.27 questions on average), and exclusively interact with experts (ratio of expert questions is close to 1). The most extreme example is a user who asked more than 1300 questions in two months, with 95% of questions to experts. This user earned $194.20, which is much higher than the average income of askers (-$1.95).\nTo further examine these outliers, we select askers who asked more than 100 questions. This gives us 111 users who count for 0.13% of askers in our dataset but aggressively asked 11% of the questions. In addition, they seem to carefully target experts who charge a lower price ($0.80 per answer) but still draw significant listeners (15.5 per answer). As a comparison, the rest of the experts on average charge $2.49 and draw 23.0 listeners per answer.\nWe performed the same analysis on Whale and did not find such outlier users because most askers onWhale did not make a positive profit (Figure 9). We omit the results for brevity.\nCollusive Users. In addition to bounty hunters, there could be \u201ccollusive\u201d users who work together to make money. For example, an asker may collude with an answerer by asking many questions (with an extremely low price) to create the illusion that the answerer is very popular. Then both the asker and the answerer can make money from the listeners of these questions. This collusion may be even conducted by a single attacker who controlled both the asker and answerer accounts.\nTo identify collusive users, we focus on answerers whose questions are primarily asked by the same user. Figure 11 shows a scatter plot for the number of questions a user an-\nswered versus the maximum number of these questions asked by the same person. Users that are close to the diagonal are suspicious. Take the two marked dots in Figure 11(a) for example, user1 answered 435 questions and 309 (71%) were asked by the same asker. We notice that this asker did not ask any other users any questions. The questions between these two users charge $0.16 each, which is much lower than user1\u2019s other questions ($0.25 on average). By using a lower price for collusion, the two users can minimize their loss \u2014 the 10% commission fee to Fenda. In this way, user1 earned $689.9 in total and this asker also earned $244 from the listeners. The second example follows the same pattern. 120 out of 274 questions that user2 answered were from the same asker (who only asked 128 questions in total). This leads to financial returns for both user2 ($116.42) and the asker ($27.5).\nFigure 11(b) shows the result of Whale. The example user (user3) answered 31 questions, 18 of which were from the same asker. This asker only asked these 18 questions and all 18 questions were free of charge. This is likely an attempt to boost user3\u2019s popularity.\nDiscussion. Our analysis shows that monetary incentives did foster manipulative behavior. On the positive side, these users (bounty hunters or collusive users) are actually working hard to come up with interesting questions in order to earn money from listeners. On the negative side, such behavior has a disruptive impact to the marketplace. For example, bounty hunters are injecting a large volume of questions to experts. The large volume of questions act as spam to experts, blocking other users\u2019 chance to get the experts\u2019 attention. For collusive behavior, it creates a fake perception of popularity, which could mislead listeners to making the wrong spending. In addition, collusion also introduces unfairness to other honest experts, which is likely to hurt the sustainability of the community in the long run.\nOur analysis focuses on the most likely attacks, and there could be other types of attacks. For example, answerers and listeners may also collude to bootstrap the \u201clistening count\u201d for a question and lure innocent users to pay for listening. However, we don\u2019t have the related data (e.g., listeners\u2019 IDs) to analyze this attack."}, {"heading": "DYNAMIC PRICING AND USER ENGAGEMENT", "text": "As supplier-driven marketplaces, Fenda and Whale allow users to set the price for their answers. How users set this price may affect their financial income and their interaction with other users. In this section, we turn to the dynamic aspect to analyze how users adjust their answer prices over time and how different pricing strategies affect their engagement level. Understanding this question is critical since keeping users (particularly experts) engaged is the key to building a sustainable CQA service.\nIn the following, we first identify common pricing strategies by applying unsupervised clustering on users\u2019 traces of price change. Then we analyze the identified clusters to understand what type of users they represent, and how their engagementlevel changes over time."}, {"heading": "Identifying Distinct Pricing Strategies", "text": "To characterize users\u2019 dynamic price change, we construct a list of features to group users with similar price change patterns.\nKey Features. For each user, we model their price change as a sequence of events. Given user i, our dataset contains the complete list of her answers and the price for each answer. We use Pi to denote user i\u2019s price sequencePi = [pi,1, pi,2, ..., pi,Ni ] where Ni is the total number of answers of user i. A price change event happens when pi, j\u22121 , pi, j for any j \u2208 [2,Ni]. We denote the price change sequence asCi = [ci,1,ci,2, ...ci,Mi ] where Mi is a number of times for price change and ci, j is a price change event (price-up, price-down, or same-price).\nTable 6 list our 9 features: the overall frequency of price change (i.e., Mi Ni ), a frequency for price-up and price-down, and the frequency difference between price-up and down. In addition, we consider the average price change magnitude for price-up and price-down events. Finally, we consider the maximum number of consecutive events of same-price, price-up and price-down in the sequence.\nUser Clustering. Based on these features, we then cluster similar users into groups. First, we compute the pair-wise Euclidean distance between users based on their feature vectors. This produces a fully connected similarity graph [37] where each node is a user and edges are weighted by distance. Then, we apply hierarchical clustering algorithm [4] to detect groups of users with similar price change patterns. We choose hierarchical clustering for two reasons: 1) It does not pre-define the number of clusters. 2) It is deterministic and the clustering result does not depend on the initial seeding.\nTo determine the number of clusters, we use modularity, a well-known metric to measure clustering quality [4]. High modularity means users are more densely connected within each cluster than to the rest of the users. We choose the number of clusters that yields the highest modularity.\nFor this analysis, we only consider users who have answered enough questions (more than 10). Otherwise, discussing their dynamic price change and engagement would be less meaningful. On Fenda, this filtering produces 2094 users who have answered 171,322 questions (85% of all questions). On Whale, however, only 68 users meet the criteria. These users answered 986 paid questions (89% of all paid questions). We will primarily focus on Fenda, and also include Whale\u2019s results for completeness."}, {"heading": "Clustering Results", "text": "Our method produces 3 clusters for both Fenda (modularity 0.59) and Whale (modularity 0.62). To understand the pricing strategy of each cluster, we plot their feature value distributions in Figure 12. Due to space limitation, we plot 4 (out of 9) most distinguishing features that have the largest variance among the 3 clusters selected by Chi-Squared statistic [31]. The three clusters on Fenda are:\n\u2022 CL.1 (33%): Frequent price up and down. 687 users (76% are experts) who have a high price change frequency. Price up and down are almost equally frequent.\n\u2022 CL.2 (43%): Rarely changing price. 908 users (76% are experts) who rarely change their price.\n\u2022 CL.3 (24%): Frequent price up. 499 users (74% are experts) who increase price frequently but rarely lower their price.\nWe find that the 3 types of pricing patterns on Fenda correspond to users of different popularity. As shown in Table 7, cluster 1 represents the least popular answerers, who have the least followers and listeners but answered more questions. These users constantly adjust their price, possibly to test the market. Cluster 3 represents the most popular experts and celebrities. They charge higher than others and keep increasing the price. Cluster 2 stands between cluster 1 and 3 in terms of popularity, and its users rarely change the price.\nWhale\u2019s 3 clusters only contain 68 users in total. We include the results for completeness:\n\u2022 CL.1 (60%): Rarely changing price. 41 users (32% are experts) with the least frequent price change.\n\u2022 CL.2 (22%): Frequent price up and down. 15 users (87% are experts) who frequently change/drop the price.\n\u2022 CL.3 (18%): Occasional price up and down. 12 users (92% are experts) who occasionally change the price.\nAs shown in Table 7, Whale\u2019s cluster 3 contains the most popular users, followed by cluster 1 and 2. Figure 12(b) \u201cPrice\nUp-Down\u201d shows the most popular users of cluster 3 are relatively balanced in terms of increasing versus decreasing the price. The less popular users of cluster 1 and 2 are more leaning towards decreasing the price. Compared to Fenda, all the clusters of Whale adjust their price rather frequently. This shows that popular users on Fenda already have the luxury to keep increasing the price. On Whale, even the most popular users are frequently adjusting their price, possibly due to the limited earning opportunities (a much lower payment per question)."}, {"heading": "Impact to User Engagement", "text": "Next, we analyze how price adjustments affect a user\u2019s engagement-level over time. Price is a key parameter within users\u2019 control, and adjusting price is a way to test their answers\u2019 value in the market. Intuitively, this price can affect a user\u2019s incoming questions, earnings and social interactions, which are key incentive factors to keep users engaged.\nFenda. Figure 13(a) shows the interplay between price change and engagement level over time for 3 identified clusters on Fenda. We quantify engagement-level using number of answers per day. To measure changes over time, we divide a user\u2019s lifespan (time between her first and last answer in our dataset) into two even parts. Then we compute the differences for average price and engagement-level between the later half and first half. In a similar way, we also measure the changes in income (Figure 13(b)) and listeners (Figure 13(c)), which represent the strength of monetary and social incentives\nWe observe different patterns: for cluster 2 and 3, more users are located in the lower right corner than upper right, indicating a decrease of engagement, income and number of listeners. A possible explanation is that there is a mismatch between the answer\u2019s price and its value, but users did not make the right adjustments. In contrast, we find a significant number of users in cluster 1 located in the upper left corner. By lowering their price, these users get to answer more questions, and receive more money and listeners over time. We validate the statistical significance of the results by calculating the Pearson correlation [31] between the price change (x) and behavior metrics (y) for all three clusters in Figure 13.We find 8/9 of the correlations are significant (p < 0.05) except for cluster1\u2019s income/day metric.\nOur result suggests that users need to set their price carefully to match their market value. This requires proactive price adjustments and lowering their price when necessary. Right now, highly popular users on Fenda (e.g., cluster 3) are less motivated or unwilling to lower their price, which in turn hurts their income and engagement level over time.\nWhale. We performed the same analysis on Whale and found none of the correlationswere statistically significant (possibly due to the small sample size)."}, {"heading": "DISCUSSION", "text": "Our analysis shows both positive impacts of monetary incentives and some concerning issues in the long run. Below, we discuss the key implications to future CQA design.\nFirst, Answering On-demand. Fenda and Whale adopt a supplier-driven model where experts set a price for their answer. This model is suitable for targeted questions (users know who to ask), but can have a longer delay compared to crowdsourcing (where anyone can be a potential answerer). Fenda and Whale achieve faster responses than most CQA services, but are still not as fast as the crowdsourcing based Yahoo Answers. Recently, Fenda added a new crowdsourcing channel for \u201cmedical\u201d and \u201clegal\u201d questions. This channel is customer-driven: users post their questions with a cash reward, and any experts can give their answers to compete for the reward. We did a quick crawling on the crowdsourcing channel and obtained 1344 questions. We find their average response time is 4.38 hours, which is even faster than the 8.25 hours of Yahoo Answers (Table 4). A promising future direction is to explore a hybrid design of customer-driven and supplier-driven model to further improve CQA efficiency.\nSecond, Rewarding Good Questions. Fenda and Whale are the first systems that reward users financially for asking good questions. This leads to a mixed effect. On the positive side, users are motivated to ask good questions that attract broad interests. 40% of the questions on Fenda received enough listeners to cover the asker\u2019s cost. Whale, however, is less successful in profiting the askers due to the \u201cfree coin\u201d design. On the negative side, this model motivates a small number of users to game the system for profits.We find \u201cbounty hunters\u201d who aggressively ask questions to low-priced experts, and \u201ccollusive\u201d users who work together to manipulate their perceived popularity. Note that this is different from the traditional cheating behavior in crowdsourcing platforms like MTurk, where cheaters often produce low-quality work [5]. In Fenda and Whale, manipulators mainly introduce unfairness, but they still need to come up with good questions to attract listeners.\nFinally, Unfairness in Supplier-driven Markets. In a supplierdriven marketplace, a well-known expert has the key advantage to receive questions. As a result, the financial income among answerers is highly uneven: top 5% answerers get about 90% of the total profits in Fenda. To attract questions, we find that less popular users need to carefully adjust their price (including dropping the price), while more popular users tend to increase their price. To help users to bootstrap popularity, Fenda recently introduced a system update, which allows users to set their answers \u201cfree-for-listening\u201d for 30 minutes after posting."}, {"heading": "FUTURE WORK AND LIMITATIONS", "text": "Fenda andWhale are among the recent wave of CQA systems that explore a new design space for payment-based knowledge sharing communities. They not only provide valuable lessons for other services, but also raise new questions.\nCommunication Mechanisms for CQA. Fenda and Whale let users record their answers in audio/video to avoid the inconvenience of typing text on the phone. Audio/video is likely to provide a more intimate experience for users, which, however, also makes it difficult to give longer answers. Future research may examine the proper communication channels (text, audio, video) for different Q&A contexts. Noticeably, real-time streaming can be a promising option for CQA, given the huge success of Periscope and Facebook Live [36].\nPreventing Abuse and Manipulation. Abusive activities are likely to be a common problem for payment-based CQA given that money is the incentive. Our work provides a first look (empirically) at the bounty hunters and collusive users in Fenda and Whale. Future research is needed to develop more systematic approaches to detect abusive players and design new incentive models to prevent/limit abuse.\nStudy Limitations. Our study has a few limitations. First, our study only focuses on two services: Fenda and Whale. A broader comparison with other payment-based CQA services can help to further generalize our results. Second, our dataset is not perfect. The crawler produces a dataset with a complete list of experts but an incomplete list of normal users.We argue that most of the missing users are likely lurkers (or inactive\nusers) who are less influential in the community.We also used Fenda\u2019s official numbers to justify parts of our results. Finally, Fenda and Whale are still exploring its way to building a sustainable CQA marketplace. It made a few changes before our paper submission as discussed above. We plan to continue to monitor these systems for future work."}, {"heading": "CONCLUSION", "text": "In this paper, we discuss lessons learned from the first supplier-driven payment-based CQA systems. By analyzing a large empirical dataset, we reveal the benefits of applying monetary incentives to CQA systems (fast response, high-quality questions) as well as potential concerns (bounty hunters and over time engagement). As more payment-based CQA systems arise (Campfire.fm, Quora Knowledge Prize, Zhihu Live), our research results can help system designers to make more informed design choices."}], "references": [{"title": "Knowledge sharing and yahoo answers: everyone knows something", "author": ["Lada A Adamic", "Jun Zhang", "Eytan Bakshy", "Mark S Ackerman"], "venue": "In Proc. of WWW", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Knowledge market design: A field experiment at Google Answers", "author": ["Yan Chen", "Tech-Hua Ho", "Yong-Mi Kim"], "venue": "Journal of Public Economic Theory 12,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Earnings And Ratings At Google Answers", "author": ["Benjamin Edelman"], "venue": "Economic Inquiry 50,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Community detection in graphs", "author": ["Santo Fortunato"], "venue": "Physics Reports", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Understanding Malicious Behavior in Crowdsourcing Platforms: The Case of Online Surveys", "author": ["Ujwal Gadiraju", "Ricardo Kawase", "Stefan Dietze", "Gianluca Demartini"], "venue": "In Proc. of CHI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Encouraging user behaviour with achievements: an empirical study", "author": ["Scott Grant", "Buddy Betts"], "venue": "In Proc. of MSR", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Who Wants to Know?: Question-asking and Answering Practices Among Facebook Users", "author": ["Rebecca Gray", "Nicole B. Ellison", "Jessica Vitak", "Cliff Lampe"], "venue": "In Proc. of CSCW", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Modeling Problem Difficulty and Expertise in Stackoverflow", "author": ["Benjamin V. Hanrahan", "Gregorio Convertino", "Les Nelson"], "venue": "In Proc. of CSCW", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Predictors of answer quality in online Q&A sites", "author": ["F Maxwell Harper", "Daphne Raban", "Sheizaf Rafaeli", "Joseph A Konstan"], "venue": "In Proc. of CHI", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "mimir: A market-based real-time question and answer service", "author": ["Gary Hsieh", "Scott Counts"], "venue": "In Proc. of CHI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Why pay?: exploring how financial incentives are used for question & answer", "author": ["Gary Hsieh", "Robert E Kraut", "Scott E Hudson"], "venue": "In Proc. of CHI", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Re-examining price as a predictor of answer quality in an online Q&A site", "author": ["Grace YoungJoo Jeon", "Yong-Mi Kim", "Yan Chen"], "venue": "In Proc. of CHI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Why users keep answering questions in online question answering communities: A theoretical and empirical investigation", "author": ["Xiao-Ling Jin", "Zhongyun Zhou", "Matthew KO Lee", "Christy MK Cheung"], "venue": "IJIM 33,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Evaluating the Crowd with Confidence", "author": ["Manas Joglekar", "Hector Garcia-Molina", "Aditya G. Parameswaran"], "venue": "In Proc. of SIGKDD", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Incentive mechanisms for crowdsourcing platforms", "author": ["Aikaterini Katmada", "Anna Satsiou", "Ioannis Kompatsiaris"], "venue": "In Proc. of INSCI", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "The Social World of Content Abusers in Community Question Answering", "author": ["Imrul Kayes", "Nicolas Kourtellis", "Daniele Quercia", "Adriana Iamnitchi", "Francesco Bonchi"], "venue": "In Proc. of WWW", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Crowdsourcing user studies with Mechanical Turk", "author": ["Aniket Kittur", "H. Chi", "Bongwon Suh"], "venue": "In Proc. of CHI", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "China\u2019s online question-and-answer platform Fenda raises US$25 million from investors. http://www.scmp.com/business/companies/article/ 1982524/chinas-online-question-and-answerplatform-fenda-raises-us25", "author": ["Phoenix Kwong"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Understanding mobile Q&A usage: an exploratory study", "author": ["Uichin Lee", "Hyanghong Kang", "Eunhee Yi", "Mun Yi", "Jussi Kantola"], "venue": "In Proc. of CHI", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Analyzing crowd workers in mobile pay-for-answer q&a", "author": ["Uichin Lee", "Jihyoung Kim", "Eunhee Yi", "Juyup Sung", "Mario Gerla"], "venue": "In Proc. of CHI", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Routing questions to appropriate answerers in community question answering services", "author": ["Baichuan Li", "Irwin King"], "venue": "In Proc. of CIKM", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Design lessons from the fastest Q&A site in the west", "author": ["Lena Mamykina", "Bella Manoim", "Manas Mittal", "George Hripcsak", "Bj\u00f6rn Hartmann"], "venue": "In Proc. of CHI", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Financial incentives and the performance of crowds", "author": ["Winter Mason", "Duncan J Watts"], "venue": "ACM SigKDD Explorations Newsletter 11,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Questions in, knowledge in?: a study of naver\u2019s question answering community", "author": ["Kevin Kyung Nam", "Mark S Ackerman", "Lada A Adamic"], "venue": "In Proc. of CHI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Analyzing the Quality of Information Solicited from Targeted Strangers on Social Media", "author": ["Jeffrey Nichols", "Michelle Zhou", "Huahai Yang", "Jeon-Hyung Kang", "Xiao Hua Sun"], "venue": "In Proc. of CSCW", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Evolution of Experts in Question Answering Communities", "author": ["Aditya Pal", "Shuo Chang", "Joseph A. Konstan"], "venue": "In Proc. of ICWSM", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Question routing to user communities", "author": ["Aditya Pal", "Fei Wang", "Michelle X. Zhou", "Jeffrey Nichols", "Barton A. Smith"], "venue": "In Proc. of CIKM", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "The incentive structure in an online information market", "author": ["Daphne Ruth Raban"], "venue": "Journal of the American Society for Information Science and Technology 59,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Great Question! Question Quality in Community Q&A", "author": ["Sujith Ravi", "Bo Pang", "Vibhor Rastogi", "Ravi Kumar"], "venue": "In Proc. of ICWSM", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Evaluating and predicting answer quality in community QA", "author": ["Chirag Shah", "Jefferey Pomerantz"], "venue": "In Proc. of SIGIR", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Handbook of Parametric and Nonparametric Statistical Procedures", "author": ["David J. Sheskin"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "2016a. A Comprehensive Survey and Classification of Approaches for Community Question Answering", "author": ["Ivan Srba", "Maria Bielikova"], "venue": "ACM TWeb 10,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Why is Stack Overflow Failing? Preserving Sustainability in Community Question Answering", "author": ["I. Srba", "M. Bielikova"], "venue": "IEEE Software 33,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Internet-scale Collection of Human-reviewed Data", "author": ["Qi Su", "Dmitry Pavlov", "Jyh-Herng Chow", "Wendell C. Baker"], "venue": "In Proc. of WWW", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Towards Predicting the Best Answers in Community-based Question-Answering Services", "author": ["Qiongjie Tian", "Peng Zhang", "Baoxin Li"], "venue": "In Proc. of ICWSM", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Anatomy of a Personalized Livestreaming System", "author": ["Bolun Wang", "Xinyi Zhang", "Gang Wang", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In Proc. of IMC", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Unsupervised Clickstream Clustering For User Behavior Analysis", "author": ["Gang Wang", "Xinyi Zhang", "Shiliang Tang", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In Proc. of CHI", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Comparing IPL2 and Yahoo! Answers: A Case Study of Digital Reference and Community Based Question Answering", "author": ["Dan Wu", "Daqing He"], "venue": "In Proc. of IConf", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Putting a price on knowledge. http://www.globaltimes.cn/content/997510.shtml", "author": ["Li Xuanmin"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Predicting Long-term Impact of CQA Posts: A Comprehensive Viewpoint", "author": ["Yuan Yao", "Hanghang Tong", "Feng Xu", "Jian Lu"], "venue": "In Proc. of KDD", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Quora now has 100 million monthly visitors, up from 80 million in January. VentureBeat", "author": ["Ken Yeung"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Knowledge Contribution in Problem Solving Virtual Communities: The Mediating Role of Individual Motivations", "author": ["Jie Yu", "Zhenhui Jiang", "Hock Chuan Chan"], "venue": "In Proc. of SIGMIS CPR", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "A Market in Your Social Network: The Effects of Extrinsic Rewards on Friendsourcing and Relationships", "author": ["Haiyi Zhu", "Sauvik Das", "Yiqun Cao", "Shuang Yu", "Aniket Kittur", "Robert Kraut"], "venue": "In Proc. of CHI", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}], "referenceMentions": [{"referenceID": 40, "context": "With highly engaging experts, services like Quora and StackOverflow attract hundreds of millions of visitors worldwide [41].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "As the question volume keeps growing, it becomes difficult to draw experts\u2019 attention to a particular question, let alone getting answers on-demand [33].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "To motivate experts, one possible direction is to introduce monetary incentives [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "Launched in May 2016, Fenda quickly gained 10 million registered users, 500K paid questions, and 2 million US dollar revenue in just two months [39].", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].", "startOffset": 180, "endOffset": 191}, {"referenceID": 10, "context": "The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].", "startOffset": 180, "endOffset": 191}, {"referenceID": 19, "context": "The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].", "startOffset": 180, "endOffset": 191}, {"referenceID": 31, "context": "In recent years, researchers have studied CQA services from various aspects [32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 76, "endOffset": 83}, {"referenceID": 7, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 76, "endOffset": 83}, {"referenceID": 20, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 132, "endOffset": 140}, {"referenceID": 26, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 132, "endOffset": 140}, {"referenceID": 28, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 39, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 29, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 34, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 0, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 8, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 33, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 15, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "Finally, researchers also studied Q&A activities in online social networks [25, 7].", "startOffset": 75, "endOffset": 82}, {"referenceID": 6, "context": "Finally, researchers also studied Q&A activities in online social networks [25, 7].", "startOffset": 75, "endOffset": 82}, {"referenceID": 32, "context": "As the sizes of CQA systems rapidly grow, it becomes challenging to engage with experts for timely and high-quality answers [33].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Prior works have summarized three main user motivations to answer questions online: \u201cintrinsic\u201d, \u201csocial\u201d and \u201cextrinsic\u201d [13].", "startOffset": 122, "endOffset": 126}, {"referenceID": 41, "context": ", enjoyment) that users gain through helping others [42, 24].", "startOffset": 52, "endOffset": 60}, {"referenceID": 23, "context": ", enjoyment) that users gain through helping others [42, 24].", "startOffset": 52, "endOffset": 60}, {"referenceID": 12, "context": "Intrinsic and social factors are critical incentives for non-payment based CQA services [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": ", badges and credit points) [24, 6].", "startOffset": 28, "endOffset": 35}, {"referenceID": 5, "context": ", badges and credit points) [24, 6].", "startOffset": 28, "endOffset": 35}, {"referenceID": 1, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 10, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 19, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 18, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 19, "context": "Users are primarily driven by financial incentives without a strong sense of community [20, 10].", "startOffset": 87, "endOffset": 95}, {"referenceID": 9, "context": "Users are primarily driven by financial incentives without a strong sense of community [20, 10].", "startOffset": 87, "endOffset": 95}, {"referenceID": 27, "context": "This is concerning since research shows monetary incentive plays an important role in getting users started in CQA, but it is the social factors that contribute to the persistent participation [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 8, "context": "Some researchers find that monetary incentives improves the answer quality [9] and response rate in social Q&A [43].", "startOffset": 75, "endOffset": 78}, {"referenceID": 42, "context": "Some researchers find that monetary incentives improves the answer quality [9] and response rate in social Q&A [43].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11].", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11].", "startOffset": 115, "endOffset": 126}, {"referenceID": 10, "context": "Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11].", "startOffset": 115, "endOffset": 126}, {"referenceID": 9, "context": "based Q&A can reduce low-quality questions since users are more selective regarding what to ask [10, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 10, "context": "based Q&A can reduce low-quality questions since users are more selective regarding what to ask [10, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 16, "context": "Mechanical Turk) are \u201ccustomer-driven\u201d where customers post their tasks and set the task price [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14].", "startOffset": 85, "endOffset": 97}, {"referenceID": 22, "context": "In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14].", "startOffset": 85, "endOffset": 97}, {"referenceID": 13, "context": "In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14].", "startOffset": 85, "endOffset": 97}, {"referenceID": 38, "context": "Launched in May 2016, Fenda quickly gained 10 million registered users and over 2 million US dollars worth of questions answers in the first two months [39, 18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 17, "context": "Launched in May 2016, Fenda quickly gained 10 million registered users and over 2 million US dollars worth of questions answers in the first two months [39, 18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 38, "context": "Fenda announced that they had 500,000 answers as of June 27, 2016 [39].", "startOffset": 66, "endOffset": 70}, {"referenceID": 38, "context": "On June 27 2016, Fenda officially announced total 500K answers and 10 million users [39].", "startOffset": 84, "endOffset": 88}, {"referenceID": 38, "context": "To gauge experts\u2019 contribution in the context of the entire network, we again performed an estimation: Fenda reached 2 million revenue as of June 27 in 2016 [39].", "startOffset": 157, "endOffset": 161}, {"referenceID": 30, "context": "To understand how users set a price for their answers, we calculate the Pearson correlation [31] between a user\u2019s price and different behaviormetrics.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].", "startOffset": 144, "endOffset": 156}, {"referenceID": 22, "context": "This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].", "startOffset": 144, "endOffset": 156}, {"referenceID": 10, "context": "This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].", "startOffset": 144, "endOffset": 156}, {"referenceID": 37, "context": "We compare Fenda and Whale with different CQA sites including Yahoo Answers [38], Google Answers [3] and StackOverflow [22].", "startOffset": 76, "endOffset": 80}, {"referenceID": 2, "context": "We compare Fenda and Whale with different CQA sites including Yahoo Answers [38], Google Answers [3] and StackOverflow [22].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "We compare Fenda and Whale with different CQA sites including Yahoo Answers [38], Google Answers [3] and StackOverflow [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "A two-sample t-Test [31] shows the significance of the differences between the two groups of askers.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "This produces a fully connected similarity graph [37] where each node is a user and edges are weighted by distance.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "Then, we apply hierarchical clustering algorithm [4] to detect groups of users with similar price change patterns.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "To determine the number of clusters, we use modularity, a well-known metric to measure clustering quality [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 30, "context": "Due to space limitation, we plot 4 (out of 9) most distinguishing features that have the largest variance among the 3 clusters selected by Chi-Squared statistic [31].", "startOffset": 161, "endOffset": 165}, {"referenceID": 30, "context": "We validate the statistical significance of the results by calculating the Pearson correlation [31] between the price change (x) and behavior metrics (y) for all three clusters in Figure 13.", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Note that this is different from the traditional cheating behavior in crowdsourcing platforms like MTurk, where cheaters often produce low-quality work [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 35, "context": "Noticeably, real-time streaming can be a promising option for CQA, given the huge success of Periscope and Facebook Live [36].", "startOffset": 121, "endOffset": 125}], "year": 2017, "abstractText": "Community-based question answering (CQA) services are facing key challenges to motivate domain experts to provide timely answers. Recently, CQA services are exploring new incentive models to engage experts and celebrities by allowing them to set a price on their answers. In this paper, we perform a data-driven analysis on two emerging payment-based CQA systems: Fenda (China) and Whale (US). By analyzing a large dataset of 220K questions (worth 1 million USD collectively), we examine how monetary incentives affect different players in the system. We find that, while monetary incentive enables quick answers from experts, it also drives certain users to aggressively game the system for profits. In addition, in this supplier-driven marketplace, users need to proactively adjust their price to make profits. Famous people are unwilling to lower their price, which in turn hurts their income and engagement over time. Finally, we discuss the key implications to future CQA design. INTRODUCTION The success of community based question answering (CQA) services depends on high-quality content from users, particularly from domain experts. With highly engaging experts, services like Quora and StackOverflow attract hundreds of millions of visitors worldwide [41]. However, for most CQA systems, domain experts are answering questions voluntarily for free. As the question volume keeps growing, it becomes difficult to draw experts\u2019 attention to a particular question, let alone getting answers on-demand [33]. To motivate experts, one possible direction is to introduce monetary incentives [10]. Recently, Quora started a beta test on \u201cknowledge prize\u201d, which allows users to put cash rewards on their questions.While Quora is slowly accumulating interested users (less than 10 paid answers per month), another payment-based service called Fenda is rising quickly in China. Fenda is a social network app that connects users to well-known domain experts and celebrities to ask questions with payments. Launched in May 2016, Fenda quickly gained 10 million registered users, 500K paid questions, and 2 million US dollar revenue in just two months [39]. Fenda leads a new wave of payment-based CQA services that socially engage users with real-world domain experts. Similar services are emerging in China (Zhihu, Weibo QA) and US (Whale, Campfire.fm). The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20]. 1http://fd.zaih.com/fenda So, is monetary incentive the solution to strong expert engagement in CQA systems? How does monetary incentive affect the behavior of different players in the system and their overtime engagement? These questions are critical for paymentbased CQA design, and platforms like Fenda provide a unique opportunity to study them. First, Fenda is the first supplierdriven CQA marketplace, where answerers (experts) set their own price. Users ask questions to a specific person instead of an anonymous crowd using payments. In addition, Fenda\u2019s incentive model not only rewards answerers, but also those who ask good questions. After a question is answered, other users need to pay a small amount ($0.14) to listen to the answer. This money is split evenly between the asker and the answerer (Figure 1). A good question may attract enough listeners to compensate the initial question fee. In this paper, we describe our experience and findings in an effort to understand the impact of monetary incentives on CQA systems, through a detailed measurement of Fenda (China) and a similar system Whale (US). We collected a dataset of 88,540 users and 212,082 answers from Fenda (two months in 2016), and 1,419 users and 9,199 answers from Whale (6 months during 2016\u20132017), involving more than 1 million USD transactions. Given the drastic differences between payment-based CQA systems and mainstream systems such as Quora and StackOverflow, our study has significant implications for the future direction of CQA design. Our study has a number of key findings. \u2022 First, we seek to understand the effectiveness of monetary incentives to engage domain experts. Our result shows this attempt is successful. Both Fenda and Whale attract a small group of high-profile experts and celebrities who make significant contributions to the CQA community. For example, Fenda experts count for 0.5% of the user population, but have contributed a quarter of all answers and driven nearly half of the financial revenue. \u2022 Second, we analyze how the incentive model affects user behavior, and find a mixed effect. On the positive side, monetary incentive enables quick answers (average delay 10\u2013 23 hours) and motivates users to ask good questions (40% of the Fenda questions successfully drew enough interested audience to cover the askers\u2019 cost). However, we did find a small number of manipulative users including \u201cbounty hunters\u201d who aggressively asked questions to make money from listeners, and \u201ccollusive users\u201d who work together to manipulate their perceived popularity. 2https://askwhale.com/ \u2022 Third, we study the dynamics between money and user engagement over time. In a supplier-driven CQA marketplace, users need to set the price of their answers. We find different pricing strategies of users have distinct impacts on their own engagement level. Users who proactively adjust their price are more likely to increase income and engagement level. Certain highly famous people, however, are unwilling to lower their price, which in turn hurts their income and social engagement. To the best of our knowledge, this is the first empirical study on supplier-driven CQA marketplaces. Our study provides practical guidelines for other arising payment-based CQA services (Quora knowledge prize, Zhihu, Campfire.fm) and reveals key implications for future CQA system design. We believe this is a first step towards understanding the economy of community-based knowledge sharing.", "creator": "gnuplot 5.0 patchlevel 4"}}}