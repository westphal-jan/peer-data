{"id": "1703.01671", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "Controlling for Unobserved Confounds in Classification Using Correlational Constraints", "abstract": "as generalized statistical behavior classifiers become integrated into real - world applications, it is important to consider not only their accuracy but also their robustness to changes in all the data distribution. in this paper, we consider the case where there is an unobserved confounding price variable $ rs z $ that influences both the features $ \\ mathbf { x } $ and the class variable $ y $. when the influence of $ z $ changes from training simulation to testing data, we find that the weighted classifier accuracy can degrade rapidly. in our approach, simply we assume that we can predict the value of $ worth z $ at one training repetition time with some error. the prediction for $ z $ is then fed synchronized to pearl's back - door adjustment code to build model our model. because of the attenuation bias caused by measurement error in $ z $, standard approaches to controlling for $ z $ are ineffective. further in response, we propose a method to try properly control for the influence of $ z $ by at first estimating its relationship with the class variable $ y $, then updating predictions making for $ z $ to match that estimated relationship. by adjusting the influence predictions of $ z $, we show that we help can can build a model that exceeds conventional competing baselines on its accuracy tests as well as on perceived robustness over a large range of confounding relationships.", "histories": [["v1", "Sun, 5 Mar 2017 21:57:25 GMT  (4454kb,D)", "http://arxiv.org/abs/1703.01671v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["virgile landeiro", "aron culotta"], "accepted": false, "id": "1703.01671"}, "pdf": {"name": "1703.01671.pdf", "metadata": {"source": "CRF", "title": "Controlling for Unobserved Confounds in Classification Using Correlational Constraints", "authors": ["Virgile Landeiro", "Aron Culotta"], "emails": ["vlandeir@hawk.iit.edu,", "aculotta@iit.edu"], "sections": [{"heading": "1 Introduction", "text": "Statistical classifiers have become widely used to inform important decisions such as whether to approve a loan (Hand and Henley 1997), hire a job candidate (Miller 2015), or release a criminal defendant on bond (Monahan and Skeem 2016). Given the significant real-world consequences of such decisions, it is critical that we can identify and remove sources of systematic bias in classification algorithms. For example, some evidence suggests that existing criminal recidivism models may be racially biased (Angwin et al. 2016).\nOne important type of classifier bias arises from confounding variables. A confounder z is a variable that is correlated both with the input variables (or features) x and the target variable (or label) y of a classifier. When z is not included in the model, the true relationship between x and y can be improperly estimated; in the social sciences \u2013 originally in econometrics \u2013 this is called omitted variable bias. While omitted variable bias is a core focus of social science (King, Keohane, and Verba 1994), it has received much less attention in machine learning communities, where prediction accuracy is the main concern. Con-\nfounding variables can be particularly problematic in highdimensional settings, such as text classification, where models may contain thousands or millions of parameters, making manual inspection of models impractical. The common use of text classification in computational social science applications (Lazer et al. 2009) further adds to the urgency of the problem.\nSeveral studies with interests in public health focused on tracking the influenza rates in the USA by using Twitter as a sensor (Paul and Dredze 2011). These studies demonstrated that machine learning offers more accurate, inexpensive, and fast tracking methods than what is currently used by the CDC. De Choudhury, Counts, and Horvitz built models to predict postpartum changes in emotion and behavior using Twitter data and managed to identify mothers who will change significantly following childbirth with an accuracy of 71% using observations about their prenatal behavior (De Choudhury, Counts, and Horvitz 2013). In a more recent study, Koratana et al. collected Yik Yak data \u2013 an anonymous social network popular among students \u2013 to study anonymous health issues and substance use on college campuses (Koratana et al. 2016). The results of these studies are encouraging for the field of computational social science but only a few of them are taking into account the effect of possible confounders. A growing body of work tries to mitigate the effect of observed confounding variables using causal inference techniques. For instance, Cunha, Weber, and Pappa use a matching approach for causal inference to estimate the effect of online support on weight loss using data from Reddit, and De Choudhury et al. leverage propensity score matching to detect users that transition from posting about mental health concerns to posting about suicidal ideation on Reddit. In this paper, we wish to provide methods for researchers in computational social sciences to conduct observational studies while controlling for confounding variables even though these might not be directly observed.\nIn recent work (Landeiro and Culotta 2016), a text classification algorithm was proposed based on Pearl\u2019s back-door adjustment (Pearl 2003) as a framework for prediction that controls for an observed confounding variable. It was found that this approach results in classifiers that are significantly more robust to shifts in the relationship between confounder z and class label y. However, an important limitation of this prior work is that it assumes that a training set is available in\nar X\niv :1\n70 3.\n01 67\n1v 1\n[ cs\n.A I]\n5 M\nar 2\n01 7\nwhich every instance is annotated for both class label y and confounder z. This is problematic because there are many confounders we may want to control for (e.g., income, age, gender, race/ethnicity) that are often rarely available and difficult for humans to label, particularly in addition to the primary label y.\nA natural solution is to build statistical classifiers for confounders z, and use the predicted values of z to control for these confounders. However, the measurement error of z introduces attenuation bias (Chesher 1991) in the back-door adjustment, resulting in classifiers that are still confounded by z.\nIn this paper, we present a classification algorithm based on Pearl\u2019s back-door adjustment to control for an unobserved confounding variable. Our approach assumes we have a preliminary classifier that can predict the value of the confounder z, and that we have an estimate of the error rate of this z-classifier. We offer two methods to adjust for the mislabeled z to improve the effectiveness of backdoor adjustment. A straightforward approach is to remove training instances for which the confidence of the predicted label for z is too low. While we do find this approach can reduce attenuation bias, it must discard many training examples, degrading the y-classifier. Our second approach instead uses the error rate of the z-classifier to estimate the correlation between y and z in the training set. The assignment to z is then optimized to match this estimated correlation, while also maximizing classification accuracy. We compare our methods on two real-world text classification tasks: predicting the location of a Twitter user and predicting if a Twitter user is smoking or not. Both prediction tasks are using users\u2019 tweets as input data and are confounded by gender. The resulting model exhibits significant improvements in both accuracy and robustness, with some settings producing similar results as fully-observed back-door adjustment."}, {"heading": "2 Related Work", "text": "In the machine learning field, selection bias has received some attention (Zadrozny 2004; Bareinboim, Tian, and Pearl 2014). It arises when the population of a study is not selected randomly. Instead, some users are more inclined to be selected for the study than others, making it more difficult to draw conclusions from the general population. If we denote S whether or not an element of the population is selected, there is presence of selection bias when p(S = 1|X,Y ) 6= p(S = 1). Dataset shift (QuioneroCandela et al. 2009) is a similar issue that appears when the joint distribution of features and labels changes between the training dataset and the testing dataset (i.e. ptr(X,Y ) 6= pte(X,Y )). Covariate shift (Bickel, Bru\u0308ckner, and Scheffer 2009; Sugiyama, Krauledat, and Mu\u0308ller 2007) is a specific case of dataset shift in which only the inputs distribution is different from training to testing (i.e. ptr(X) 6= pte(X)). Similarly, when the underlying target distribution p(Y ) changes over time, either in a sudden way or gradually, then this is called concept drift (Tsymbal 2004; Widmer and Kubat 1996). Recent work has studied \u201cfairness\u201d in machine learning (Zemel et al. 2013; Hajian and\nDomingo-Ferrer 2013) as well as attempted to remove features that introduce bias (Pedreshi, Ruggieri, and Turini 2008; Fukuchi, Sakuma, and Kamishima 2013). Kuroki and Pearl (2014) propose an extension of back-door adjustment to deal with measurement error in the confounder, but it does not scale well when x is high dimensional, as in our setting of text classification.\nAlthough all these types of biases are important to conduct a valid observational study, in this paper we direct our attention to the problem of learning under confounding bias shift. In other words, we aim to build a classifier that is robust to changes in the relation between the target variable Y of a classifier and an external confounding variable Z. Landeiro and Culotta (2016) use back-door adjustment for text classification, but assume confounders are observed at training time. This paper introduces methods to enable back-door adjustment to work effectively when confounders are unobserved and when the features are high dimensional."}, {"heading": "3 Methods", "text": "In this section, we first review prior work using back-door adjustment to control for observed confounders in text classification. We then introduce two methods for applying back-door adjustments when the confounder is unobserved at training time and must instead be predicted by a separate classifier.\n3.1 Adjusting for observed confounders Suppose one wishes to estimate the causal effect of a variable x on a variable y when a randomized controlled trial is not possible. If a sufficient set of confounding variables z is available, one can use the back-door adjustment equation as follows:\np(y|do(x)) = \u2211 z p(y|x, z)\u00d7 p(z) (1)\nThe back-door criterion (Pearl 2003) is a graphical test that determines whether z is a sufficient set of variables to estimate the causal effect. This criterion requires that no node in z is a descendant of x and that z blocks every path between x and y that contains an arrow pointing to x. Notice p(y|x) 6= p(y|do(x)): this do-notation is used in causal inference to indicate that an intervention has been made on x. Omitting the predicted confounder z\u2032, it depicts a standard discriminative approach to classification, e.g., modeling p(y|x) with a logistic regression classifier conditioned on the observed term vector x. We assume that the confounder z\u2032 influences both the term vector through p(x|z) as well as the target label through p(y|z\u2032). The structure of this model ensures that z\u2032 meets the back-door criterion for adjustment.\nBack-door adjustment was originally introduced for causal inference problems \u2014 i.e., to estimate the causal effect of performing action x on outcome y. Recently, Landeiro and Culotta (2016) have shown that back-door adjustment can also be used to improve classification robustness. By controlling for a confounder z, the resulting classifier is robust to changes in the relationship between z and y.\nFrom the perspective of standard supervised classification, the approach works as follows: Assume we are given a training set D = {(xi, yi)}. If we suspect that a classifier trained on D is confounded by some additional variable z, we augment the training set by including z as a feature for each instance: D\u2032 = {(xi, yi, zi)}. We then fit a classifier on D\u2032, and at testing time apply Equation 1 to classify new examples \u2014 p(y|x) = \u2211 z p(y|x, z)p(z) \u2014 where p(z) is simply computed from the observed frequencies of z in D\u2032. By controlling for the effect of z, the resulting classifier is robust to the case where p(y|z) changes from training to testing data.\nIn the experiments below, we consider the problem of predicting a user\u2019s location y based on the text of their tweets x, confounded by the user\u2019s gender z. That is, in the training data, there exists a correlation between gender and location, but we want the classifier to ignore that correlation. When the above procedure is applied to a logistic regression classifier, the result is that the magnitudes of coefficients for terms that correlate with gender are greatly reduced, thereby minimizing the effect of gender on the classifier\u2019s predictions.\n3.2 Adjusting for unobserved confounders In the previous approach, it was assumed that we had access to a training set D = {(xi, yi, zi)}; that is, each instance is annotated both for the label y and confounder z. This is a burdensome assumption, given that ultimately we will need to control for many possible confounders (e.g., gender, race/ethnicity, age, etc.). Because many of these confounders are unobserved and/or difficult to obtain, it is necessary to develop adjustment methods that can handle noise in the assignment to z in the training data.\nOur approach assumes we have an (imperfect) classifier for z, trained on a secondary training set Dz = {(xi, zi)} \u2014 we call this the preliminary study, with the resulting preliminary classifier p(z|x). This is combined with the dataset Dy = {(xi, yi)}, used to train the primary classifier p(y|x). The advantage of allowing for separate training sets Dy and Dz is that it is often easier to annotate z variables for some users than others; for example, Pennacchiotti and Popescu (2011) build training data for ethnicity classification by searching for online users that explicitly state their ethnicity in their user profiles.\nAfter training on Dz , the preliminary classifier is applied to Dy to augment it with predicted annotations for confounder z: D = {(xi, yi, z\u2032i)}ni=1, where z\u2032i denotes the predicted value of zi. A tempting approach is to simply apply back-door adjustment as usual to this dataset, ignoring the noise introduced by z\u2032. However, the resulting classifier will no longer properly control for the confounder z for at least two related reasons:\n1. The observed correlation between y and z\u2032 in the training data will underestimate the actual correlation (i.e., |r(y, z\u2032)| < |r(y, z)|). This attenuation bias reduces the coefficients for the z features, which in turn prevents back-door adjustment from reducing the coefficients of features in x that correlate with z.\n2. Similarly, because some training instances have misla-\nbeled annotations for z, it is more difficult to detect which features in x correlate with z, thereby preventing backdoor adjustment from reducing those coefficients.\nTo verify this claim, we conduct an experiment in which we observe z but we inject increasing amounts of noise in z (e.g., with probability p, change the assignment to zi to be incorrect). In other words, we synthetically decrease the quality of our observations of z and we observe how that influences the performance of back-door adjustment. We then measure how the accuracy of the primary classifier for y varies on a testing set in which the influence of z is decreased (i.e., z correlates strongly with y in the training set, but only weakly in the testing set). These experiments will be discussed in more detail in Section 4.\nWe can see in Figure 1 that the F1 score quickly decreases as we add more noise to the confounding variable annotations, indicating the need for new methods to adjust for unobserved confounders. Notice that when noise is 0, backdoor adjustment greatly improves F1 (from .79 F1 with no adjustment to .85 F1), demonstrating the effectiveness of this approach when the confounder is observed at training time. In the following two sections, we propose two methods to fix these issues.\nThresholding on confidence of z predictions Our first approach is fairly simple; its objective is to directly reduce the number of mislabeled annotations in z\u2032. Our preliminary model produces the value z\u2032i (the prediction of the true confounder zi) as well as p(zi = z\u2032i|xi) (the confidence of the prediction; i.e., the posterior distribution over z). We use these posteriors to remove predictions with low confidence. By setting a threshold \u2208 [0.5, 1], we filter the original dataset D = {xi, yi, z\u2032i} by keeping an instance i only if\nit satisfies p(zi = z\u2032i|xi) \u2265 . For well-calibrated classifiers like logistic regression, we expect to remove mostly mislabeled data points by thresholding at . Making vary between 0.5 and 1 allows us to modify the output of the preliminary study in order to obtain a sub-dataset with as many points correctly labeled as possible. Moreover, when the error of our preliminary classifier is symmetric, this process will also move the estimated correlation r(y, z\u2032) towards the true correlation r(y, z).\nWith this smaller set of training instances, we run backdoor adjustment without modification. However, one important drawback of this method is that we remove instances from our training dataset. Depending on the quality of the preliminary classifier and the setting of , only a small fraction of training instances may potentially remain. Thus, in the next section we consider an alternative approach that does not require discarding training instances.\nCorrelation matching While the above approach aims to reduce errors in z\u2032, and as a side effect improves the estimate of r(y, z), in this section we propose an approach that directly tries to improve the estimate of r(y, z) while also reducing errors in z. Let r\u2032 = r(y, z\u2032) be the observed correlation between y and z\u2032, and let r = r(y, z) be the true (unobservable) correlation between y and z in the training data for y, D = {xi, yi, z\u2032i}. Our proposed approach builds on the insight of Francis, Coats, and Gibson (1999), who show that r\u2032 can be estimated from r using the variances of y and z as well as the variances of the errors in y and z:\nr\u2032 =\n\u221a 1\n(1 + Vey Vy )(1 + VezVz ) \u00d7 r (2)\nwhere Vz is the variance of z, and Vez is the variance of error on z, and analogously for Vy , Vey . Since in our setting y is observed, we can set Vey = 0 and solve for r:\nr\u2032 =\n\u221a 1\n1 + VezVz \u00d7 r (3)\n\u21d2 r = r\u2032 \u00d7 \u221a 1 +\nVez Vz\n(4)\nThus, the factor by which r\u2032 underestimates r is proportional to the ratio of the variance of the error in z to the variance of z.\nWe can estimate the terms Vz and Vez using crossvalidation on the preliminary training data Dz = {(xi, zi)}. Let z\u2032i be the value predicted by the preliminary classifier on instance xi \u2208 Dz , where i is in the testing fold of one cross-validation split of the data. Let ezi = |zi \u2212 z\u2032i| be the absolute error of z on instance i. Then, we can first compute the mean absolute error of z\u2032i as \u00b5ez = 1 |Dz| \u2211 zi\u2208Dz e z i . The estimated variance of the errors in z is then:\nV\u0302ez = 1 |Dz| \u2211 z\u2208Dz (ezi \u2212 \u00b5ez)2 (5)\nSince this variance in the error of z in turn affects the observed variance of z, we can then estimate\nV\u0302z = Vz\u2032 \u2212 V\u0302ez (6)\nwhere Vz\u2032 is the variance of predictions z\u2032 in the target training data D.\nPlugging the estimates of Equations 5 and 6 into Equation 4 enables us to estimate the true correlation between y and z in the target training data D. We will refer to this estimated correlation as r\u0302.\nAs an example, consider a datasetD = {(xi, yi, z\u2032i)}. The original correlation r(y, z\u2032) \u2261 r\u2032 may be .5, but the true correlation r(y, z) \u2261 r may be .8. Depending on the variances of z and its error, the estimated correlation may be r\u0302 = .75. The next step in the procedure is to optimize the assignment to z\u2032 to minimize the difference |r\u2032 \u2212 r\u0302|. That is, we use r\u0302 as a soft constraint, and attempt to match that constraint by changing the assignments to z\u2032.\nLet Z be the set of all possible assignments to z in the training setD (i.e., if z is a binary variable and |D| = n, then |Z| = 2n). Let zj = {zj1 . . . zjn} \u2208 Z be a vector of assignments to z, and let r\u2032(zj) indicate the correlation r(zj , y). Then our objective is to choose an assignment from Z to minimize r\u2032(zj) \u2212 r\u0302, while still maximizing the probability of that assignment according to the preliminary classifier for z. We can write this objective as follows:\nz\u2217 \u2190 argmax zj\u2208Z  1 n \u2211 zji\u2208zj p(zi = z j i |xi) \u2212|r\u0302\u2212r\u2032(zj)| (7) Thus, we search for an optimal assignment z\u2217 that maximizes the average posterior of the predicted z value, while minimizing the difference between the estimated correlation r\u0302 and the observed correlation r\u2032(zj).\nThis optimization problem can be approached in several ways. We implement a greedy hill-climbing algorithm that iterates through the values in z\u2032 sorted by confidence and flips the value if it reduces |r\u2212 r\u2032|. The steps are as follows:\n1. Initialize zj to the most probable assignment according to p(z|x).\n2. Initialize I to be all instances sorted in descending order of confidence p(z|x).\n3. While |r\u0302 \u2212 r\u2032(zj)| is decreasing: (a) Pop the next instance (xi, z j i , yi) from I\n(b) If flipping the label zji reduces the error |r\u0302\u2212 r\u2032(zj)|, do so. Else, skip to the next instance.\n4. Return the final zj . For example, consider the case where r\u2032(zj) < r\u0302. If the\ninstance popped in step 3(a) has labels (yi = 1, z\u2032i = 0), then we know that flipping zi to 1 would increase the correlation between y and z\u2032. By considering flips in descending order of p(z|x), we ensure that we first flip assignments that are likely to be incorrect. In the experiments below, we find that this approach often converges after a relatively small number of flips.\nThe advantages of this approach are that it not only produces assignments to z that better align with the expected correlation r\u0302, but it also results in more accurate assignments to z. The latter is possible because we are using prior knowledge about the relationship between z and y to assign values\nof z when the classifier is uncertain. As with the thresholding approach of the previous section, once the new assignments to z are found, back-door adjustment is run without modification."}, {"heading": "4 Experiments", "text": "We conducted text classification experiments in which the relationship between the confounder z and the class variable y varies between the training and testing set. We consider the scenario in which we directly control the discrepancy between training and testing. Thus, we can determine how well a confounder has been controlled by measuring how robust the method performs across a range of discrepancy levels.\nTo sample train/test sets with different p(y|z) distributions, we assume we have labeled datasets Dtrain, Dtest, with elements {(xi, yi, zi)}, where yi and zi are binary variables. We introduce a bias parameter p(y = 1|z = 1) = b; by definition, p(y = 0|z = 1) = 1\u2212b. For each experiment, we sample without replacement from each set D\u2032train \u2286 Dtrain, D\u2032test \u2286 Dtest. To simulate a change in p(y|z), we use different bias terms for training and testing, btrain, btest. We thus sample according to the following constraints: ptrain(y = 1|z = 1) = btrain, ptest(y = 1|z = 1) = btest, ptrain(Y ) = ptest(Y ), and ptrain(Z) = ptest(Z).\nThe last two constraints are to isolate the effect of changes to p(y|z). Thus, we fix p(y) and p(z), but vary p(y|z) from training to testing data. We emphasize that we do not alter any of the actual labels in the data; we merely sample instances to meet these constraints. In the rest of the paper, we note rtrain(y, z) (respectively rtest(y, z)) the correlation between y and z in the training set (resp. testing set). We also denote \u03b4yz = rtrain(y, z)\u2212 rtest(y, z)."}, {"heading": "4.1 Datasets", "text": "Location / Gender For our first dataset, we use the data from Landeiro and Culotta (2016), where the task is to predict the location of a Twitter user from their messages, with gender as a potential confounder. Thus, x is a term vector, y is location, and z is gender. The data contain geolocated tweets from New York City (NYC) and Los Angeles (LA). There are 246,930 tweets for NYC and 218,945 for LA over a four-day period (June 15th to June 18th, 2015). Gender labels are derived by cross-referencing the user\u2019s name (from the profile) with U.S. Census name data, removing ambiguous names. For each user, we have up to the most recent 3,200 tweets, which we represent each as a single binary unigram vector per user, using standard tokenization. Finally, we subsample this collection and keep the tweets from 6,000 users such that gender and location are uniformly distributed over the users.\nSmoker / Gender In our second dataset, the task is to predict if a Twitter user is a smoker or not, with gender as a potential confounder. We start from approx. 3M tweets collected in January and February 2014 using cigarettes related keywords. We randomly pick 40K tweets for which we can identify the user\u2019s gender using the Twitter screen name and the U.S. Census name data. We then manually annotate 4.5K\nof these tweets on whether they show that a user is a smoker (yes) or a non-smoker (no) while discarding uncertain tweets (unknown). We use this data to train a classifier (F1 score = 0.84) to label the remaining 35.5K tweets on the smoker dimension. In order to avoid mislabeled tweets as much as possible, we only keep predictions with a confidence of at least 95%, yielding an additional 5.5K automatically labeled tweets. These 10K (4.5K manually annotated + 5.5K automatically annotated) tweets have been written by 9K unique users. For each of these users, we collect the most recent tweets (up to 200). Because some users set their profile to be private or because some users that existed in early 2014 have now deleted their account, we obtain at least 20 tweets for 4.6K users. Then we collect all the cigarettes related tweets published by a user in the first two months of 2014 and add them to our dataset. Finally, we remove users in order to build a balanced dataset on both annotated dimensions and eventually obtain a dataset of 4084 users."}, {"heading": "5 Results", "text": "We use the following notations to describe the results below:\n\u2022 \u03b4yz = rtrain(y, z) \u2212 rtest(y, z) is the discrepancy between the correlation of y and z in training versus testing.\n\u2022 r(y, z) (respectively r(y, z\u2032)) is the true (resp. observed) correlation between y and z.\n\u2022 r(y, z\u2032 ) (respectively r(y, z\u2032cm)) is r(y, z\u2032) after it has been adjusting using the thresholding method (resp. the correlation matching method).\n\u2022 F1z (respectively F1y) is the F1 score for a z (resp. y) classifier, i.e. for the preliminary (resp. main) study.\n5.1 Effects of correlation adjustments on F1z For this first part of our results, we obtain quasi-identical outcomes for both datasets. Therefore, we only present the results from the location/gender dataset. thresholding method: We make vary between 0.5 and 0.95 and observe how this reduces the difference between r(y, z\u2032 ) and r(y, z). Figure 2(a) shows the result of one setting where r(y, z) = 0.4. The figure demonstrates that by increasing , r (y, z) gets closer to the true r(y, z), and the performance of our external study is improved. This indicates that the classifier is well calibrated (since high confidence predictions are more likely to be correct). However, it takes a high value of to get a correct approximation of the true association between y and z, meaning that we need to discard a large amount of data points from our preliminary study to approximate r(y, z). For example, at = .9, roughly half of the training instances remain. Correlation matching method: For this method, we make the true correlation r(y, z) change between \u22120.8 and 0.8 and we plot the results on Figure 2(b). We observe in the top plot that after adjustment, our estimate rcm(y, z) is within 0.1 of the true correlation in the worst case against 0.4 without adjustment. This is a clear improvement in the correlation estimation. (For comparison, achieving a similarly accurate estimate using thresholding requires removing 60% of 1500 instances.) We can also notice that the performance of our preliminary study greatly increases when we improve the estimation of r(y, z), particularly when r(y, z) is high. For example, when r(y, z) is .8, the F1z improves from .77 to .9, on average. Thus, correlation matching appears to both recover the true correlation while simultaneously improving the quality of the classifications of z.\n5.2 Effects of correlation adjustments on F1y Location / Gender Fixed F1z = 0.784: As our primary result, we report the F1y obtained by different correlation adjustment methods across a range of shifts in the discrepancy between training and testing. For the Twitter dataset, the best performance we get in the preliminary study is F1z = 0.784. We then compare testing F1y as rtrain(y, z) and rtest(y, z) vary. The results are shown in Figure 3(a). Without any adjustment, the performance we get is close to Logistic Regression. When using thresholding, the performance is slightly improved\nin the extreme cases but only by a few points at most. However, when using the correlation matching method, we improve F1y by 10 to 15 points in the most extreme cases. For comparison, the figure also shows the fully observed case\n(z+BA), which uses back-door adjustment on the true values of z. We can see that correlation matching is comparable to the fully observed case, even with a 20% error rate on z. These results show that by getting a better estimate of the association between y and z, we can reduce attenuation bias and improve the robustness of our classifier, even though our observation of z is noisy. Variable F1z: We showed in the previous section that when we use our preliminary study with F1z = 0.784, we can build a robust classifier using the correlation matching method combined with back-door adjustment. We also saw in Figure 1 that back-door adjustment when z is observed at training time is sensitive to noise in z. As a similar study, we want to see how sensitive the correlation adjustment methods are to the quality of F1z . To do so, we increasingly add noise to the dataset used to train the preliminary classifier (Dz = {xi, zi}) to make F1z decrease. Because we want to visualize F1y against two variables (F1z and \u03b4yz), we visualize the results in a heatmap. In order to make the results clear to the reader, here are additional details to understand what is displayed on the heatmap: The x-axis of a heatmap is \u03b4yz and the y-axis is F1z . The line plot on the left of the heatmap shows F1z given F1y averaged over all possible values for \u03b4yz . The error bars are the standard deviations of F1y , indicating how sensitive the model is to variations of \u03b4yz . Similarly, the scatter plot above the heatmap shows F1y given \u03b4yz averaged over all possible values for F1z . The error bars are the standard deviations of F1y for the matching \u03b4yz .\nMoreover, Table 2 displays the values of the standard deviations shown in the scatter plot at the left of each heatmap as a measure of robustness. Figure 4(a) shows the heatmap of results for back-door adjustment when we use the predictions of the preliminary study but none of the methods to fix the mislabeled values in z\u2032 are used. Figures 4(b) and 4(c) respectively show the heatmaps of results when we use thresholding with = 0.75 and correlation matching. Similar to Figure 3(a), thresholding only brings small improvement to no adjustment at all. Furthermore, when F1z decreases, the correlation adjustment using thresholding is performing worse than when we are not doing any corre-\nlation adjustment as well as it is less robust. Clearly, the thresholding method is more sensitive to the quality of the preliminary study than the other methods.\nThe correlation matching method (Figure 2(b)) does outperform the other methods in robustness and F1y for most of the cases but when F1z < 0.645, as we can see by the wider range of red values in Figure 4(c). In this latter case, it performs worse than the method without adjustment. This method is also sensitive to the quality of the preliminary study as we can see that the averaged F1y decreases with F1z . Let us remind one more time that we are considering here only preliminary studies with an F1z of at most 0.784. Therefore, F1z could be up to 22 points greater with a different dataset. This would hopefully lead to similar results than when F1z = 0.784 with correlation matching and better results in F1y and robustness with thresholding. Smoker / Gender Fixed F1z = 0.791: Similarly to the previous experiment, we report F1y while making \u03b4yz vary as our primary result in Figure 3(b). We observe that predicting if a user smokes or not is a much more difficult task than our previous binary location prediction task, as the maximum yielded F1y is around .75 when it was approximately .9 in the previous task. We also notice that the robustness of the back-door adjustment methods is not as good as for the location/gender dataset. The correlation matching method manages to performs closely to z + BA for \u03b4yz \u2265 \u22120.75 and outperforms all other methods for \u03b4yz \u2265 1 but we also witness an accuracy drop on the left part of the plot. In addition to this drop, our two most robust methods (z + BA and correlation matching) are outperformed by approximately 5 points when there is no difference between the training correlation and the testing correlation (when \u03b4yz = 0). Variable F1z: When making F1z vary with the smoker/gender dataset, we observe comparable outcomes as the ones displayed in the heatmaps of Figure 4 but with a lesser overall accuracy. As back-door adjustment was not performing as well as with the location/gender dataset in the fixed F1z case, it logically also does not perform as well when F1z varies. If we obtain a V-shaped heatmaps similar to Figures 4(b) and 4(c), the slope indicating that the\nclassifier\u2019s\u2019 performance deteriorates when F1z decreases is steeper. This may show that our adjustments methods are more sensitive to noise in the confounding variable when the classification task is overall harder. We do not display the resulting heatmap for the smoker/gender experiment in this paper for brevity but we will make the dataset and the code to reproduce the results available online."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed two methods of using backdoor adjustment to control for an unobserved confounder. Using two real-life datasets extracted from Twitter, we have found that correlation matching on the predicted confounder associated with back-door adjustment can retrieve the underlying correlation r(y, z) and perform closely to back-door adjustment with an observed confounder. We also showed that thresholding can be used to slightly improve the predictions compared to logistic regression. If thresholding will not be able to adjust for the unobserved confounder z when F1z < 0.75, we showed that correlation matching provides a way to adjust for an unobserved confounder and outperform plain back-door adjustment as long as F1z > 0.65. In future work, we will consider hybrid methods that combine thresholding and correlation matching to increase robustness as F1z decreases."}], "references": [{"title": "Recovering from selection bias in causal and statistical inference", "author": ["Tian Bareinboim", "E. Pearl 2014] Bareinboim", "J. Tian", "J. Pearl"], "venue": "In Proceedings of The TwentyEighth Conference on Artificial Intelligence (CE Brodley and P", "citeRegEx": "Bareinboim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bareinboim et al\\.", "year": 2014}, {"title": "Discriminative learning under covariate shift", "author": ["Br\u00fcckner Bickel", "S. Scheffer 2009] Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "Journal of Machine Learning Research 10(Sep):2137\u20132155", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "A warm welcome matters! the link between social feedback and weight loss in/r/loseit", "author": ["Weber Cunha", "T.O. Pappa 2017] Cunha", "I. Weber", "G.L. Pappa"], "venue": "arXiv preprint arXiv:1701.05225", "citeRegEx": "Cunha et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cunha et al\\.", "year": 2017}, {"title": "Discovering shifts to suicidal ideation from mental health content in social media", "author": ["De Choudhury"], "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Choudhury,? \\Q2016\\E", "shortCiteRegEx": "Choudhury", "year": 2016}, {"title": "Predicting postpartum changes in emotion and behavior via social media", "author": ["Counts De Choudhury", "M. Horvitz 2013] De Choudhury", "S. Counts", "E. Horvitz"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Choudhury et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choudhury et al\\.", "year": 2013}, {"title": "How high can a correlation", "author": ["Coats Francis", "D.P. Gibson 1999] Francis", "A.J. Coats", "D.G. Gibson"], "venue": null, "citeRegEx": "Francis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Francis et al\\.", "year": 1999}, {"title": "Prediction with model-based neutrality", "author": ["Sakuma Fukuchi", "K. Kamishima 2013] Fukuchi", "J. Sakuma", "T. Kamishima"], "venue": "In Machine Learning and Knowledge Discovery in Databases", "citeRegEx": "Fukuchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fukuchi et al\\.", "year": 2013}, {"title": "A methodology for direct and indirect discrimination prevention in data mining. Knowledge and Data Engineering, IEEE Transactions on 25(7):1445\u20131459", "author": ["Hajian", "S. Domingo-Ferrer 2013] Hajian", "J. Domingo-Ferrer"], "venue": null, "citeRegEx": "Hajian et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hajian et al\\.", "year": 2013}, {"title": "Statistical classification methods in consumer credit scoring: a review. Journal of the Royal Statistical Society: Series A (Statistics in Society", "author": ["Hand", "D.J. Henley 1997] Hand", "W.E. Henley"], "venue": null, "citeRegEx": "Hand et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hand et al\\.", "year": 1997}, {"title": "Designing social inquiry: Scientific inference in qualitative research", "author": ["Keohane King", "G. Verba 1994] King", "R.O. Keohane", "S. Verba"], "venue": null, "citeRegEx": "King et al\\.,? \\Q1994\\E", "shortCiteRegEx": "King et al\\.", "year": 1994}, {"title": "Studying anonymous health issues and substance use on college campuses with yik yak", "author": ["Koratana"], "venue": "In Workshops at the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Koratana,? \\Q2016\\E", "shortCiteRegEx": "Koratana", "year": 2016}, {"title": "Measurement bias and effect restoration in causal inference", "author": ["Kuroki", "M. Pearl 2014] Kuroki", "J. Pearl"], "venue": null, "citeRegEx": "Kuroki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuroki et al\\.", "year": 2014}, {"title": "Robust text classification in the presence of confounding bias", "author": ["Landeiro", "V. Culotta 2016] Landeiro", "A. Culotta"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Landeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Landeiro et al\\.", "year": 2016}, {"title": "Life in the network: the coming age of computational social science", "author": ["Lazer"], "venue": null, "citeRegEx": "Lazer,? \\Q2009\\E", "shortCiteRegEx": "Lazer", "year": 2009}, {"title": "Can an algorithm hire better than a human? The New York Times 25", "author": ["C.C. Miller"], "venue": null, "citeRegEx": "Miller,? \\Q2015\\E", "shortCiteRegEx": "Miller", "year": 2015}, {"title": "Risk assessment in criminal sentencing", "author": ["Monahan", "J. Skeem 2016] Monahan", "J.L. Skeem"], "venue": "Annual Review of Clinical Psychology", "citeRegEx": "Monahan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Monahan et al\\.", "year": 2016}, {"title": "You are what you tweet: Analyzing twitter for public health", "author": ["Paul", "M.J. Dredze 2011] Paul", "M. Dredze"], "venue": null, "citeRegEx": "Paul et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2011}, {"title": "Discrimination-aware data mining", "author": ["Ruggieri Pedreshi", "D. Turini 2008] Pedreshi", "S. Ruggieri", "F. Turini"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Pedreshi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pedreshi et al\\.", "year": 2008}, {"title": "A machine learning approach to twitter user classification", "author": ["Pennacchiotti", "M. Popescu 2011] Pennacchiotti", "Popescu", "A.-M"], "venue": null, "citeRegEx": "Pennacchiotti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pennacchiotti et al\\.", "year": 2011}, {"title": "Dataset shift in machine learning", "author": ["M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": "The MIT Press.", "citeRegEx": "Sugiyama et al\\.,? 2009", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2009}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["Krauledat Sugiyama", "M. M\u00fcller 2007] Sugiyama", "M. Krauledat", "K.-R. M\u00fcller"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Sugiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2007}, {"title": "Learning in the presence of concept drift and hidden contexts. Machine learning 23(1):69\u2013101", "author": ["Widmer", "G. Kubat 1996] Widmer", "M. Kubat"], "venue": null, "citeRegEx": "Widmer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Widmer et al\\.", "year": 1996}, {"title": "Learning fair representations", "author": ["Zemel"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Zemel,? \\Q2013\\E", "shortCiteRegEx": "Zemel", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Statistical classifiers have become widely used to inform important decisions such as whether to approve a loan (Hand and Henley 1997), hire a job candidate (Miller 2015), or release a criminal defendant on bond (Monahan and Skeem 2016).", "startOffset": 157, "endOffset": 170}, {"referenceID": 22, "context": "Recent work has studied \u201cfairness\u201d in machine learning (Zemel et al. 2013; Hajian and Domingo-Ferrer 2013) as well as attempted to remove features that introduce bias (Pedreshi, Ruggieri, and Turini 2008; Fukuchi, Sakuma, and Kamishima 2013). Kuroki and Pearl (2014) propose an extension of back-door adjustment to deal with measurement error in the confounder, but it does not scale well when x is high dimensional, as in our setting of text classification.", "startOffset": 56, "endOffset": 267}], "year": 2017, "abstractText": "As statistical classifiers become integrated into realworld applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. In this paper, we consider the case where there is an unobserved confounding variable z that influences both the features x and the class variable y. When the influence of z changes from training to testing data, we find that the classifier accuracy can degrade rapidly. In our approach, we assume that we can predict the value of z at training time with some error. The prediction for z is then fed to Pearl\u2019s back-door adjustment to build our model. Because of the attenuation bias caused by measurement error in z, standard approaches to controlling for z are ineffective. In response, we propose a method to properly control for the influence of z by first estimating its relationship with the class variable y, then updating predictions for z to match that estimated relationship. By adjusting the influence of z, we show that we can build a model that exceeds competing baselines on accuracy as well as on robustness over a range of confounding relationships.", "creator": "LaTeX with hyperref package"}}}