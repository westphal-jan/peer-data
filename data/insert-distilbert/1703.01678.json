{"id": "1703.01678", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "Data-Dependent Stability of Stochastic Gradient Descent", "abstract": "we establish a data - dependent statistical notion of algorithmic stability for stochastic gradient descent ( sgd ) estimates and employ it to develop novel generalization bounds. so this is in contrast to previous distribution - free algorithmic quantitative stability results for sgd which bounds depend on the worst - case constants. by virtue of the data - dependent argument, narrowing our bounds provide new insights into learning with sgd on underlying convex and non - convex problems. in the convex case, we show that the bound on the generalization goal error is multiplicative in the risk at the initialization point. unfortunately in the non - convex case, we prove that the expected curvature of the objective function around surrounding the initialization point has crucial influence on the generalization error. in both cases, our results suggest a simple data - driven strategy to stabilize sgd by pre - screening its initialization.", "histories": [["v1", "Sun, 5 Mar 2017 22:22:34 GMT  (29kb,D)", "https://arxiv.org/abs/1703.01678v1", null], ["v2", "Wed, 22 Mar 2017 17:16:17 GMT  (33kb,D)", "http://arxiv.org/abs/1703.01678v2", null], ["v3", "Fri, 26 May 2017 17:33:50 GMT  (119kb,D)", "http://arxiv.org/abs/1703.01678v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "christoph h lampert"], "accepted": false, "id": "1703.01678"}, "pdf": {"name": "1703.01678.pdf", "metadata": {"source": "CRF", "title": "Data-Dependent Stability of Stochastic Gradient Descent", "authors": ["Ilja Kuzborskij", "Christoph H. Lampert"], "emails": ["ilja.kuzborskij@idiap.ch", "chl@ist.ac.at"], "sections": [{"heading": "1 Introduction", "text": "Stochastic gradient descent (SGD) has become one of the workhorses of modern machine learning. In particular, it is the optimization method of choice for training highly complex and non-convex models, such as neural networks. When it was observed that these models generalize better (suffer less from overfitting) than classical machine learning theory suggests, a large theoretical interest emerged to explain this phenomenon. Given that SGD at best finds a local minimum of the non-convex objective function, it has been argued that all such minima might be equally good. However, at the same time, a large body of empirical work and tricks of trade, such as early stopping, suggests that in practice one might not even reach a minimum, yet nevertheless observes excellent performance.\nIn this work we follow an alternative route that aims to directly analyze the generalization ability of SGD by studying how sensitive it is to small perturbations in the training set. This is known as algorithmic stability approach [5] and was used recently [16] to establish generalization bounds for both convex and non-convex learning settings. To do so they employed a rather restrictive notion of stability that does not depend on the data, but captures only intrinsic characteristics of the learning algorithm and global properties of the objective function. Consequently, their analysis results in worst-case guarantees that in some cases tend to be too pessimistic. As recently pointed out in [38], deep learning might indeed be such a case, as this notion of stability is insufficient to give deeper theoretical insights, and a less restrictive one is desirable.\nAs our main contribution in this work we establish that a data-dependent notion of algorithmic stability, very similar to the On-Average Stability [34], holds for SGD when applied to convex as well as non-convex learning problems. As a consequence we obtain new generalization bounds that depend on the data-generating distribution and the initialization point of an algorithm. For convex loss functions, the bound on the generalization error is multiplicative in the risk at the initialization point. For the non-convex loss functions, besides the risk, it is also critically controlled by the expected second-order information about the objective function at the initialization point. We further corroborate our findings empirically\nar X\niv :1\n70 3.\n01 67\n8v 3\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 7\nand show that, indeed, the data-dependent generalization bound is tighter than the worst-case counterpart on non-convex objective functions. Finally, the nature of the data-dependent bounds allows us to state optimistic bounds that switch to the faster rate of convergence subject to the vanishing empirical risk.\nIn particular, our findings justify the intuition that SGD is more stable in less curved areas of the objective function and link it to the generalization ability. This also backs up numerous empirical findings in the deep learning literature that solutions with low generalization error occur in less curved regions. At the same time, in pessimistic scenarios, our bounds are no worse than those of [16].\nFinally, we exemplify an application of our bounds, and propose a simple yet principled transfer learning scheme for the convex and non-convex case, which is guaranteed to transfer from the best source of information. In addition, this approach can also be used to select a good initialization given a number of random starting positions. This is a theoretically sound alternative to the purely random commonly used in non-convex learning.\nThe rest of the paper is organized as follows. We revisit the connection between stability and generalization of SGD in Section 3 and introduce a data-dependent notion of stability in Section 4. We state the main results in Section 5, in particular, Theorem 3 for the convex case, and Theorem 5 for the non-convex one. Next we demonstrate empirically that the bound shown in Theorem 5 is tighter than the worst-case one in Section 5.2.1. Finally, we suggest application of these bounds by showcasing principled transfer learning approaches in Section 5.3, and we conclude in Section 6."}, {"heading": "2 Related Work", "text": "Algorithmic stability has been a topic of interest in learning theory for a long time, however, the modern approach on the relationship between stability and generalization goes back to the milestone work of [5]. They analyzed several notions of stability, which fall into two categories: distribution-free and distributiondependent ones. The first category is usually called uniform stability and focuses on the intrinsic stability properties of an algorithm without regard to the data-generating distribution. Uniform stability was used to analyze many algorithms, including regularized Empirical Risk Minimization (ERM) [5], randomized aggregation schemes [10], and recently SGD by [16, 24], and [31]. Despite the fact that uniform stability has been shown to be sufficient to guarantee learnability, it can be too pessimistic, resulting in worst-case rates.\nIn this work we are interested in the data-dependent behavior of SGD, thus the emphasis will fall on the distribution-dependent notion of stability, known as on-average stability, explored throughly in [34]. The attractive quality of this less restrictive stability type is that the resulting bounds are controlled by how stable the algorithm is under the data-generating distribution. For instance, in [5] and [9], the on-average stability is related to the variance of an estimator. In [33, Sec. 13], the authors show risk bounds that depend on the expected empirical risk of a solution to the regularized ERM. In turn, one can exploit this fact to state improved optimistic risk bounds, for instance, ones that exhibit fast-rate regimes [20, 14], or even to design enhanced algorithms that minimize these bounds in a data-driven way, e.g. by exploiting side information as in transfer [21, 3] and metric learning [30]. Here, we mainly focus on the later direction in the context of SGD: how stable is SGD under the data-generating distribution given an initialization point? We also touch the former direction by taking advantage of our data-driven analysis and show optimistic bounds as a corollary.\nWe will study the on-average stability of SGD for both convex and non-convex loss functions. In the convex setting, we will relate stability to the risk at the initialization point, while previous data-driven stability arguments usually consider minimizers of convex ERM rather than a stochastic approximation [33, 20]. Beside convex problems, our work also covers the generalization ability of SGD on non-convex problems. Here, we borrow techniques of [16] and extend them to the distribution-dependent setting. That said, while bounds of [16] are stated in terms of worst-case quantities, ours reveal new connections to the data-dependent second-order information. These new insights also partially justify empirical observations in deep learning about the link between the curvature and the generalization error [17, 19, 6]. At the same time, our work is an alternative to the theoretical studies of neural network objective functions [7, 18], as\nwe focus on the direct connection between the generalization and the curvature. In this light, our work is also related to non-convex optimization by SGD. Literature on this subject typically studies rates of convergence to the stationary points [13, 1, 32], and ways to avoid saddles [12, 23]. However, unlike these works, and similarly to [16], we are interested in the generalization ability of SGD, and thanks to the stability approach, involvement of stationary points in our analysis is not necessary.\nFinally, we propose an example application of our findings in Transfer Learning (TL). For instance, by controlling the stability bound in a data-driven way, one can choose an initialization that leads to improved generalization. This is related to TL where one transfers from pre-trained models [22, 36, 29, 3], especially popular in deep learning due to its data-demanding nature [11]. Theoretical literature on this topic is mostly focused on the ERM setting and PAC-bounds, while our analysis of SGD yields such guarantees as a corollary."}, {"heading": "3 Stability of Stochastic Gradient Descent", "text": "First, we introduce definitions used in the rest of the paper."}, {"heading": "3.1 Definitions", "text": "We will denote with small and capital bold letters respectively column vectors and matrices, e.g., a \u201c ra1, a2, . . . , adsT P Rd andA P Rd1\u02c6d2 , }a} is understood as a Euclidean norm and }A}2 as the spectral norm. We denote enumeration by rns \u201c t1, . . . , nu for n P N.\nWe indicate an example space by Z and its member by z P Z . For instance, in a supervised setting Z \u201c X \u02c6 Y , such that X is the input and Y is the output space of a learning problem. We assume that training and testing examples are drawn iid from a probability distribution D over Z . In particular, we will denote the training set as S \u201c tziumi\u201c1 \u201e Dm.\nFor a parameter space H, we define a learning algorithm as a map A : Zm \u00de\u00d1 H and for brevity we will use the notation AS \u201c ApSq. In the following we assume that H \u010e Rd. To measure the accuracy of a learning algorithm A, we have a loss function fpw, zq, which measures the cost incurred by predicting with parameters w P H on an example z. The risk of w, with respect to the distribution D, and the empirical risk given a training set S are then defined as\nRpwq :\u201c E z\u201eD\nrfpw, zqs, and pRSpwq :\u201c 1\nm\nm \u00ff i\u201c1 fpw, ziq ."}, {"heading": "3.2 Uniform Stability and Generalization", "text": "On an intuitive level, a learning algorithm is said to be stable whenever a small perturbation in the training set does not affect its outcome too much. Of course, there is a number of ways to formalize the perturbation and the extent of the change in the outcome, and we will discuss some of them below. The most important consequence of a stable algorithm is that it generalizes from the training set to the unseen data sampled from the same distribution. In other words, the difference between the risk RpASq and the empirical risk pRSpASq of the algorithm\u2019s output is controlled by the quantity that captures how stable the algorithm is. So, to observe good performance, or a decreasing true risk, we must have a stable algorithm and decreasing empirical risk (training error), which usually comes by design of the algorithm. In this work we focus on the stability of the Stochastic Gradient Descent (SGD) algorithm, and thus, as a consequence, we study its generalization ability.\nRecently, [16] used a stability argument to prove generalization bounds for learning with SGD. Specifically, the authors extended the notion of the uniform stability originally proposed by [5], to accommodate randomized algorithms.\nDefinition 1 (Uniform stability). A randomized algorithm A is -uniformly stable if for all datasets S, Spiq P Zm such that S and Spiq differ in the i-th example, we have\nsup zPZ,iPrms\n\"\nE A rfpAS , zq \u00b4 fpASpiq , zqs\n*\n\u010f .\nSince SGD is a randomized algorithm, we have to cope with two sources of randomness: the data-generating process and the randomization of the algorithm A itself, hence we have statements in expectation. The following theorem of [16] shows that the uniform stability implies generalization in expectation.\nTheorem 1. Let A be -uniformly stable. Then, \u02c7\n\u02c7 \u02c7 \u02c7 E S,A\n\u201d pRSpASq \u00b4RpASq \u0131\n\u02c7 \u02c7 \u02c7 \u02c7 \u010f .\nThus it suffices to characterize the uniform stability of an algorithm to state a generalization bound. In particular, [16] showed generalization bounds for SGD under different assumptions on the loss function f . Despite that these results hold in expectation, other forms of generalization bounds, such as highprobability ones, can be derived from the above [34].\nApart from SGD, uniform stability has been used before to prove generalization bounds for many learning algorithms [5]. However, these bounds typically suggest worst-case generalization rates, and rather reflect intrinsic stability properties of an algorithm. In other words, uniform stability is oblivious to the data-generating process and any other side information, which might reveal scenarios where generalization occurs at a faster rate. In turn, these insights could motivate the design of improved learning algorithms. In the following we address some limitations of analysis through uniform stability by using a less restrictive notion of stability. We extend the setting of [16] by proving data-dependent stability bounds for convex and non-convex loss functions. In addition, we also take into account the initialization point of an algorithm as a form of supplementary information, and we dedicate special attention to its interplay with the data-generating distribution. Finally, we discuss situations where one can explicitly control the stability of SGD in a data-dependent way."}, {"heading": "4 Data-dependent Stability Bounds for SGD", "text": "In this section we describe a notion of data-dependent algorithmic stability, that allows us to state generalization bounds which depend not only on the properties of the learning algorithm, but also on the additional parameters of the algorithm. We indicate such additional parameters by \u03b8, and therefore we denote stability as a function p\u03b8q. In particular, in the following we will be interested in scenarios where \u03b8 describes the data-generating distribution and the initialization point of SGD.\nDefinition 2 (On-Average stability). A randomized algorithm A is p\u03b8q-on-average stable if it is true that\nsup iPrms\n\"\nE A E S,z rfpAS , zq \u00b4 fpASpiq , zqs\n*\n\u010f p\u03b8q ,\nwhere S iid\u201eDm and Spiq is its copy with i-th example replaced by z iid\u201eD.\nOur definition of on-average stability resembles the notion introduced by [34]. The difference lies in the fact that we take supremum over index of replaced example. A similar notion was also used by [5] and later by [10] for analysis of a randomized aggregation schemes, however their definition involves absolute difference of losses. The dependence on \u03b8 also bears similarity to recent work of [24], however, there, it is used in the context of uniform stability. The following theorem shows that on-average - stable random algorithm is guaranteed to generalize in expectation.\nTheorem 2. Let an algorithm A be p\u03b8q-on-average stable. Then,\nE S E A\n\u201d RpASq \u00b4 pRSpASq \u0131 \u010f p\u03b8q .\nProof (sketch). For any S \u201c tziumi\u201c1 iid\u201eDm, let Spiq be its copy with i-th example replaced by z iid\u201eD. We relate expected empirical risk and expected risk by\nE S E A rRpASqs \u201c E S E A r pRSpASqs ` \u03b4 , where \u03b4 \u201c\n1\nm\nm \u00ff i\u201c1 E S,z E A rfpAS , zq \u00b4 fpASpiq , zqs .\nWe further get that\n\u03b4 \u010f sup iPrms\n\"\nE S,z E A rfpAS , zq \u00b4 fpASpiq , zqs\n*\n\u010f p\u03b8q .\nThe theorem follows as by definition, the r.h.s. is bounded by p\u03b8q."}, {"heading": "5 Main Results", "text": "Before presenting our main results in this section, we discuss algorithmic details and assumptions. We will study the following variant of SGD: given a training set S \u201c tziumi\u201c1\niid\u201e Dm, step sizes t\u03b1tuTt\u201c1, random indices I \u201c tjtuTt\u201c1, and an initialization point w1, perform updates\nwt`1 \u201c wt \u00b4 \u03b1t\u2207fpwt, zjtq\nfor T \u010f m steps. We assume that the indices in I are sampled from the uniform distribution over rms without replacement, and that this is the only source of randomness for SGD. In practice this corresponds to permuting the training set before making a pass through it, as it is commonly done in practical applications. Next, we introduce statements about the loss functions f used in the following.\nDefinition 3 (Lipschitz f ). A loss function f is L-Lipschitz if }\u2207fpw, zq} \u010f L, @w P H and @z P Z . Note that this also implies that |fpw, zq \u00b4 fpv, zq| \u010f L}w \u00b4 v} .\nDefinition 4 (Smooth f ). A loss function is \u03b2-smooth if @w,v P H and @z P Z , }\u2207fpw, zq \u00b4 \u2207fpv, zq} \u010f \u03b2}w \u00b4 v} , which also implies fpw, zq \u00b4 fpv, zq \u010f \u2207fpv, zqJpw \u00b4 vq ` \u03b22 }w \u00b4 v} 2 .\nDefinition 5 (Lipschitz Hessians). A loss function f has a \u03c1-Lipschitz Hessian if @w,v P H and @z P Z , }\u22072fpw, zq \u00b4\u22072fpv, zq}2 \u010f \u03c1}w \u00b4 v} .\nThe last condition is occasionally used in analysis of SGD [12] and holds whenever f has a bounded third derivative. All presented theorems assume that the loss function used by SGD is non-negative, Lipschitz, and \u03b2-smooth. Examples of such commonly used loss functions are the logistic/softmax losses and neural networks with sigmoid activations. Convexity of loss functions or Lipschitzness of Hessians will only be required for some results, and we will denote it explicitly when necessary. Proofs for all the statements in this section are given in the supplementary material."}, {"heading": "5.1 Convex Losses", "text": "First, we present a new and data-dependent stability result for convex losses.\nTheorem 3. Assume that f is convex, and that SGD\u2019s step sizes satisfy \u03b1t \u010f 2\u03b2 , @t P rT s. Then SGD is pD,w1q-on-average stable with\npD,w1q \u010f 2L\na\n2\u03b2Rpw1q m\nT \u00ff t\u201c1 \u03b1t .\nUnder the same assumptions, [16] showed a uniform stability bound \u010f 2L2m \u0159T t\u201c1 \u03b1t. Our bound\ndiffers since it involves a multiplicative risk at the initialization point, that is a\nRpw1q, in place of a Lipschitz constant. Thus, our bound corroborates the intuition that whenever we start at a good location of the objective function, the algorithm is more stable and thus generalizes better. In the extreme case of Rpw1q \u201c 0, the theorem confirms that SGD, in expectation, does not need to make any updates and is therefore perfectly stable. Note, that a result of this type cannot be obtained through the more restrictive uniform stability, precisely because such bounds on the stability must hold even for a worst-case choice of data distribution and initialization. In contrast, the notion of stability we employ depends on the data-generating distribution, which allowed us to introduce dependency on the risk.\nFurthermore, consider that we start at arbitrary locationw1: assuming that the loss function is bounded for a concrete H and Z , the rate of our bound up to a constant is no worse than that of [16]. Finally, one can always tighten this result by taking the minimum of two stability bounds.\nData-dependent argument, very similar to the one used in the proof of Theorem 3, can be also applied to prove the following optimistic bound for learning on convex problems with SGD.\nTheorem 4. Assume that f is convex, and that SGD\u2019s step sizes satisfy \u03b1t \u201c ct \u010f 2 \u03b2 , @t P rT s. Then the output of SGD obeys\nE S,A\n\u201d RpASq \u00b4 pRSpASq \u0131 \u010f 4 4 a\n\u03b2Rpw1q ? cT\nm\nc\nE S,A\n\u201d pRSpASq \u0131 ` 16 a \u03b2Rpw1qcT m2 . (1)\nThe bound of Theorem 4 is usually called optimistic because for a vanishing expected empirical risk, it manifests the fast decay of the generalization error. In particular, in our case, the fast rate is Op a\nRpw1qT {m2q. For the common choice of m \u201c OpT q, this expression reduces to the more familiar looking Op a\nRpw1q{mq. Optimistic bounds for convex learning were extensively studied in recent years in PAC and stochastic optimization settings. PAC literature approached such bounds through relative VC bounds [37], local Rademacher complexity [2], and Rademacher bounds for smooth loss classes [35]. Stochastic optimization literature usually studied optimistic bounds constructively, e.g. for stochastic mirror descent [35] when learning with smooth losses, and stochastic online Newton step [25] for exp-concave loss functions. Here we focus on comparison to [35], since their results assume only smoothness of the loss function, while others impose stronger assumptions. In particular, we consider Corollary 3 of [35], showing the bound on the excess risk ESrRpASqss\u00b4arg minwPHRpwq for stochastic optimization. In other words, their bound characterizes consistency of an algorithm w.r.t. the minimizer in the class, and therefore it is not directly comparable to ours. However, their proof technique also allows to obtain the bound on the generalization error of a shape similar to the consistency one (similarly as in [35, Theorem 1]). The main difference of our bound (1) from [35] is a novel multiplicative dependency on the risk at the initialization point Rpw1q, and thus our bound suggests improvement over previous one in warm-start scenarios, especially where initialization point is close to the optimal."}, {"heading": "5.2 Non-convex Losses", "text": "Now we state a new stability result for non-convex losses.\nTheorem 5. Assume that fp\u00a8, zq P r0, 1s and has a \u03c1-Lipschitz Hessian, and that step sizes of a form \u03b1t \u201c ct satisfy c \u010f min ! 1 \u03b2 , 1 4p2\u03b2 lnpT qq2 ) . Then SGD is pD,w1q-on-average stable with\npD,w1q \u010f 1` 1c\u03b3 m ` 2cL2 \u02d8 1 1`c\u03b3 \u02c6 E S,A rRpASqs \u00a8 T \u02d9 c\u03b3 1`c\u03b3 , where (2)\n\u03b3 :\u201c min !\n\u03b2, E z\n\u201c \u203a \u203a\u22072fpw1, zq \u203a \u203a\n2\n\u2030 ` c\u03c1p1` lnpT qq a 2\u03b2Rpw1q ) . (3)\nIn particular, \u03b3 characterizes how the curvature at the initialization point affects stability, and hence the generalization error of SGD. Since \u03b3 heavily affects the rate of convergence in (2), and in most situations\nsmaller \u03b3 yields higher stability, we now look at a few cases of its behavior. Consider a regime such that \u03b3 is of the order \u0398\u0303 \u00b4 Er}\u22072fpw1, zq}2s ` a Rpw1q \u00af\n, or in other words, that stability is controlled by the curvature and the risk of the initialization point w1. This suggests that starting from a point in a less curved region with low risk should yield higher stability, and therefore as predicted by our theory, allow for faster generalization. In addition, we observe that the considered stability regime offers a principled way to pre-screen a good initialization point in practice, by choosing the one that minimizes spectral norm of the Hessian and the risk.\nNext, we focus on a more specific case. Suppose that we choose a step size \u03b1t \u201c ct such that \u03b3 \u010f \u0398\u0303 ` Er}\u22072fpw1, zq}2s \u02d8\n, yet not too small, so that the empirical risk can still be decreased. Then, stability is dominated by the curvature around w1. Indeed, lower generalization errors on non-convex problems, such as training deep neural networks, have been observed empirically when SGD is actively guided [17, 15, 6] or converges to solutions with low curvature [19]. However, to the best of our knowledge, Theorem 5 is the first to establish a theoretical link between the curvature of the loss function and the generalization ability of SGD in a data-dependent sense.\nTheorem 5 allows us to show the following statement that further reinforces the effect of the initialization point on the generalization error.\nCorollary 1. Under conditions of Theorem 5 we have that SGD is pD,w1q-on-average stable with\npD,w1q \u010f O \u02dc 1` 1c\u03b3 m pRpw1q \u00a8 T q c\u03b3 1`c\u03b3 \u00b8 . (4)\nWe take a moment to discuss the role of the risk term in pRpw1q \u00a8 T q c\u03b3\n1`c\u03b3 . Observe that pD,w1q \u00d1 0 as Rpw1q \u00d1 0, in other words, the generalization error approaches zero as the risk of the initialization point vanishes. This is an intuitive behavior, however, uniform stability does not capture this due to its distribution-free nature. Finally, we note that [16, Theorem 3.8] showed a bound similar to (2), however, in place of \u03b3 their bound has a Lipschitz constant of the gradient. The crucial difference lies in term \u03b3 which is now not merely a Lipschitz constant, but rather depends on the data-generating distribution and initialization point of SGD. We compare to their bound by considering the worst case scenario, namely, that SGD is initialized in a point with high curvature, or altogether, that the objective function is highly curved everywhere. Then, at least our bound is no worse than the one of [16], since \u03b3 \u010f \u03b2.\nTheorem 5 also allows us to prove an optimistic generalization bound for learning with SGD on non-convex objectives.\nCorollary 2. Under conditions of Theorem 5 we have that the output of SGD obeys\nE S,A\n\u201d RpASq \u00b4 pRSpASq \u0131 \u010f O \u02dc 1` 1c\u03b3 m \u00a8max # \u02c6 E S,A \u201d pRSpASq \u0131 \u00a8 T \u02d9 c\u03b3 1`c\u03b3 , \u02c6 T m \u02d9c\u03b3 +\u00b8 .\nAn important consequence of Corollary 2, is that for a vanishing expected empirical risk, in particular for ES,Ar pRSpASqs \u201c O ` T c\u03b3\nm1`c\u03b3\n\u02d8 , the generalization error behaves as O ` T c\u03b3\nm1`c\u03b3\n\u02d8\n. Considering the full pass, that is m \u201c OpT q, we have an optimistic generalization error of order O p1{mq instead of Opm\u00b4 1 1`c\u03b3 q. We note that PAC bounds with similar optimistic message (although not directly comparable), but without curvature information can also be obtained through empirical Bernstein bounds as in [26]. However, a PAC bound does not suggest a way to minimize non-convex empirical risk in general, where, on the other hand, SGD is known to work reasonably well."}, {"heading": "5.2.1 Tightness of Non-convex Bounds", "text": "Next we empirically assess the tightness of our non-convex generalization bounds on real data. In the following experiment we train a neural network with three convolutional layers interlaced with maxpooling, followed by the fully connected layer with 16 units, on the MNIST dataset. This totals in a model\nwith 18K parameters. Figure 1 compares our data-dependent bound (2) to the distribution-free one of [16, Theorem 3.8]. As as a reference we also include an empirical estimate of the generalization error taken as an absolute difference of the validation and training average losses. Since our bound also depends on the initialization point, we plot (2) for multiple \u201cwarm-starts\u201d, ie.with SGD initialized from a pre-trained position. We consider 7 such warm-starts at every 200 steps, and report data-dependent quantities used to compute (2) just beneath the graph. Our first observation is that, clearly, the data-dependent bound gives tighter estimate, by roughly one order of magnitude. Second, simulating start from a pre-trained position suggests even tighter estimates: we suspect that this is due to decreasing validation error which is used as an empirical estimate for Rpw1q which effects heavily bound (2).\nFigure 1: Empirical tightness of datadependent and uniform generalization bounds evaluated by training a convolutional neural network.\nWe compute an empirical estimate of the expected Hessian spectral norm by the power iteration method using an efficient Hessian-vector multiplication method [28]. Since bounds depend on constants L, \u03b2, and \u03c1, we estimate them heuristically by tracking maximal values of the gradient and Hessian norms throughout optimization. We compute bounds with estimates pL \u201c 78.72, p\u03b2 \u201c 1692.28, p\u03c1 \u201c 3823.73, and c \u201c 10\u00b43. Note that actual constants can only be larger than estimated ones, and thus, discrepancy between the worst-case and the data-dependent bound can be even larger."}, {"heading": "5.3 Application to Transfer Learning", "text": "One example application of data-dependent bounds presented before lies in Transfer Learning (TL), where we are interested in achieving faster generalization on a target task by exploiting side information that originates from different but related source tasks. The literature on TL explored many ways to do so, and here we will focus on the one that is most compatible with our bounds. More formally, suppose that the target task at hand is characterized by a joint probability distribution D, and as before we have a training set S iid\u201eDm. Some TL approaches also assume access to the data sampled from the distributions associated with the source tasks. Here we follow a conservative approach \u2013 instead of the source data, we receive a set of source hypotheses\nwsrck (K k\u201c1 \u0102 H, trained on the source tasks. The goal of a learner is to come up with a target hypothesis, which in the optimistic scenario generalizes better by relying on source hypotheses. In the TL literature this is known as Hypothesis Transfer Learning (HTL) [22], that is, we transfer from the source hypotheses which act as a proxy to the source tasks and the risk Rpwsrck q quantifies how much source and target tasks are related. In the following we will consider SGD for HTL, where the source hypotheses act as initialization points. First, consider learning with convex losses: Theorem 3 depends on Rpw1q, thus it immediately quantifies the relatedness of source and target tasks. So it is enough to pick the point that minimizes the stability bound to transfer from the most related source. Then, bounding Rpwsrck q by pRSpwsrck q through Hoeffding bound along with union bound gives with high probability that\nmin kPrKs pD,wsrck q \u010f min kPrKs O\n\u02dc\npRSpwsrck q ` c logpKq m\n\u00b8\n.\nHence, the most related source is the one that simply minimizes empirical risk. Similar conclusions where drawn in HTL literature, albeit in the context of ERM. Matters are slightly more complicated in the non-convex case. We take a similar approach, however, now we minimize stability bound (4), and for the\nsake of simplicity assume that we make a full pass over the data, so T \u201c m. Minimizing the following empirical upper bound select the best source.\nProposition 1. Let p\u03b3\u02d8k \u201c 1 m \u0159m i\u201c1 }\u22072fpwsrck , ziq}2 ` \u03bb\nb\npRSpwsrck q \u02d8 O \u00b4 4 a logpKq{m \u00af\n, where \u03bb \u201c c\u03c1p1` lnpT qq ? 2\u03b2. Then with high probability we have that\nmin kPrKs pD,wsrck q \u010f min kPrKs O\n\u00a8\n\u02dd\n\u02c6\n1` 1 cp\u03b3\u00b4k\n\u02d9 pRSpwsrck q cp\u03b3` k 1`cp\u03b3` k \u00a8 a logpKq\nm 1 1`cp\u03b3` k\n\u02db\n\u201a .\nWe also note that there is no restriction on the origin of the source hypotheses wsrck . In general, these can even be random guesses, in which case we would be pre-screening a good starting position. Finally, p\u03b3k involves estimation of the spectral norm of the Hessian, which is computationally cheaper to evaluate compared to the complete Hessian matrix [28]. This is particularly relevant for deep learning, where computation of the Hessian matrix can be prohibitively expensive."}, {"heading": "6 Conclusions and Future Work", "text": "In this work we proved data-dependent stability bounds for SGD and revisited its generalization ability. We presented novel bounds for convex and non-convex smooth loss functions, partially controlled by data-dependent quantities, while previous stability bounds for SGD were derived through the worst-case analysis. In particular, for non-convex learning, we demonstrated theoretically that generalization of SGD is heavily affected by the expected curvature around the initialization point. We demonstrated empirically that our bound is indeed tighter compared to the uniform one. In addition, our data-dependent analysis also allowed us to show optimistic bounds on the generalization error of SGD, which exhibit fast rates subject to the vanishing empirical risk of the algorithm\u2019s output.\nIn future work we further intend to explore our theoretical findings experimentally and evaluate the feasibility of the transfer learning based on the second-order information. Another direction lies in making our bounds adaptive. So far we have presented bounds that have data-dependent components, however the step size cannot be adjusted depending on the data, e.g. as in [39]. This was partially addressed by [24], albeit in the context of uniform stability, and we plan to extend this idea to the context of data-dependent stability."}, {"heading": "Acknowledgments", "text": "This work was in parts funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement no 637076).\nThis work was in parts funded by the European Research Council under the European Union\u2019s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036."}, {"heading": "A Proofs", "text": "In this section we present proofs of all the statements.\nProof of Theorem 2. Indicate by S \u201c tziumi\u201c1 and S1 \u201c tz1iumi\u201c1 independent training sets sampled i.i.d. from D, and let Spiq \u201c tz1, . . . , zi\u00b41, z1i, zi`1, . . . , zmu, such that z1i\niid\u201eD. We relate expected empirical risk and expected risk by\nE S E A\n\u201d pRSpASq \u0131\n\u201cE S E A\n\u00ab\n1\nm\nm \u00ff i\u201c1 fpAS , ziq\nff\n\u201c E S,S1 E A\n\u00ab\n1\nm\nm \u00ff i\u201c1 fpASpiq , z1iq\nff\n\u201c E S,S1 E A\n\u00ab\n1\nm\nm \u00ff i\u201c1 fpAS , z1iq\nff\n\u00b4 \u03b4\n\u201cE S E A rRpASqs \u00b4 \u03b4 ,\nwhere\n\u03b4 \u201c E S,S1 E A\n\u00ab\n1\nm\nm \u00ff\ni\u201c1\n` fpAS , z1iq \u00b4 fpASpiq , z1iq \u02d8\nff\n\u201c 1 m\nm \u00ff i\u201c1 E S,z1i E A \u201c fpAS , z1iq \u00b4 fpASpiq , z1iq \u2030 .\nRenaming z1i as z and taking sup over i we get that\n\u03b4 \u010f sup iPrms\n\"\nE S,z E A rfpAS , zq \u00b4 fpASpiq , zqs\n*\n.\nThis completes the proof.\nA.1 Preliminaries\nWe say that the SGD gradient update rule is an operator Gt : H \u00de\u00d1 H, such that\nGtpwq :\u201c w \u00b4 \u03b1t\u2207fpw, zitq ,\nand it is also a function of the training set S and a random index set I . Then,wt`1 \u201c Gtpwtq, throughout t \u201c 1, . . . , T . Moreover we will use the notation wS,t to indicate the output of SGD ran on a training set S, at step t, and define\n\u03b4tpS, zq :\u201c }wS,t \u00b4wSpiq,t} .\nNext, we summarize a few instrumental facts about Gt and few statements about the loss functions used in our proofs.\nDefinition 6 (Expansiveness). A gradient update rule is \u03b7-expansive if for all w,v,\n}Gtpwq \u00b4Gtpvq} \u010f \u03b7}w \u00b4 v} .\nThe following lemma characterizes expansiveness for the gradient update rule under different assumptions on f .\nLemma 1 (Lemma 3.6 in [16]). Assume that f is \u03b2-smooth. Then, we have that:\n1) Gt is p1` \u03b1t\u03b2q-expansive,\n2) If f in addition is convex, then, for any \u03b1t \u010f 2\u03b2 , the gradient update rule Gt is 1-expansive.\nAn important consequence of \u03b2-smoothness of f is self-boundedness [33], which we will use on many occasions.\nLemma 2 (Self-boundedness). For \u03b2-smooth non-negative function f we have that\n}\u2207fpw, zq} \u010f a 2\u03b2fpw, zq .\nSelf-boundedness in turn implies the following boundedness of a gradient update rule.\nCorollary 3. Assume that f is \u03b2-smooth and non-negative. Then,\n}w \u00b4Gtpwq} \u201c \u03b1t}\u2207fpw, zjtq} \u010f \u03b1t min ! b 2\u03b2fpw, zjtq, L ) .\nProof. By Lemma 2\n}\u03b1t\u2207fpw, zjtq} \u010f \u03b1t b 2\u03b2fpw, zjtq ,\nand also by Lipschitzness of f , }\u03b1t\u2207fpw, zjtq} \u010f \u03b1tL.\nNext we introduce a bound that relates the risk of the output at step t to the risk of the initialization pointw1 through the variance of the gradient. Given an appropriate choice of step size, this bound will be crucial at stating stability bounds that depend on the risk at w1. The proof idea is similar to the one of [13]. In particular, it does not require convexity of the loss function.\nLemma 3. Assume that the loss function f is \u03b2-smooth. Then, for wS,t we have that\nE S rfpwS,t, zjtq \u00b4 fpw1, zjtqs \u010f\nt\u00b41 \u00ff k\u201c1 \u03b1k \u02c6 \u03b1k\u03b2 2 \u00b4 1 \u02d9 E S \u201c }\u2207fpwS,k, zjkq} 2 \u2030 .\nProof. For brevity let wk \u201c wS,k. The \u03b2-smoothness of f for any zjt we have\nfpwk`1, zjtq \u00b4 fpwk, zjtq \u010f \u2207fpwk, zjtqJpwk`1 \u00b4wkq ` \u03b2\n2 }wk`1 \u00b4wk}2 .\nConsidering SGD update wk`1 \u201c wk \u00b4 \u03b1k\u2207fpwk, zjkq, where zjk iid\u201eD, and summing both sides from 1 to t\u00b4 1 we get\nfpwt, zjtq \u00b4 fpw1, zjtq \u010f \u00b4 t\u00b41 \u00ff\nk\u201c1 \u03b1k\u2207fpwk, zjtqJ\u2207fpwk, zjkq `\n\u03b2\n2\nt\u00b41 \u00ff k\u201c1 \u03b12k}\u2207fpwk, zjkq} 2 .\nTaking expectation w.r.t. S and z on both sides, using the fact thatwS,k does not depend on zjk nor on zjt , and that zjk , zjt iid\u201eD, we have that\nE S,z rfpwS,t, zjtq \u00b4 fpw1, zjtqs \u010f \u00b4 t\u00b41 \u00ff\nk\u201c1 \u03b1k E S,z\n\u201c \u2207fpwS,k, zjtqJ\u2207fpwS,k, zjkq \u2030 ` \u03b2 2\nt\u00b41 \u00ff k\u201c1 \u03b12k E S,z \u201c }\u2207fpwS,k, zjkq} 2 \u2030\n\u201c \u03b1k \u02c6 \u03b1k\u03b2\n2 \u00b4 1\n\u02d9 t \u00ff\nk\u201c1 \u03b12k E S,z\n\u201c }\u2207fpwS,k, zjkq} 2 \u2030 .\nThe following lemma is a consequence of Lemma 3 and self-boundedness.\nLemma 4. Assume that the loss function f is \u03b2-smooth and non-negative, and that step sizes obey \u03b1t \u010f 2\u03b2 . Then @t P rT s we have that\nE S,z r}\u2207fpwS,t, zjtq}s \u010f\na\n2\u03b2Rpw1q .\nProof. By Lemma 2, }\u2207fpwS,t, zjtq} \u010f a 2\u03b2fpwS,t, zjtq. Now, we invoke Lemma 3 assuming that the step size is set such that \u03b1t \u010f 2\u03b2 to get that\nE S,z r}\u2207fpwS,t, zjtq}s \u010f\na\n2\u03b2 E S\n\u201d b\nfpwS,t, zjtq \u0131\n\u010f b\n2\u03b2 E S rfpwS,t, zjtqs (By Jensen\u2019s inequality.)\n\u010f b\n2\u03b2 E S rfpw1, zjtqs \u201c\na\n2\u03b2Rpw1q . (By Lemma 3.)\nThe following lemma is similar to Lemma 3.11 of [16], and is instrumental in bounding the stability of SGD. However, we make an adjustment and state it in expectation over the data. Note that it does not require convexity of the loss function.\nLemma 5. Assume that the loss function fp\u00a8, zq P r0, 1s is L-Lipschitz for all z. Then, for every t0 P t0, 1, 2, . . .mu we have that,\nE S,z E A\n\u201c fpwS,T , zq \u00b4 fpwSpiq,T , zq \u2030\n(5)\n\u010f L E S,z\n\u201e\nE A r\u03b4T pS, zq | \u03b4t0pS, zq \u201c 0s\n\n` E S,A rRpASqs t0 m . (6)\nProof. We proceed with elementary decomposition, Lipschitzness of f , and using the fact that f is non-negative to have that\nfpwS,T , zq \u00b4 fpwSpiq,T , zq (7) \u201c `\nfpwS,T , zq \u00b4 fpwSpiq,T , zq \u02d8 I t\u03b4t0pS, zq \u201c 0u ` `\nfpwS,T , zq \u00b4 fpwSpiq,T , zq \u02d8 I t\u03b4t0pS, zq \u2030 0u \u010f L\u03b4T pS, zqI t\u03b4t0pS, zq \u201c 0u ` fpwS,T , zqI t\u03b4t0pS, zq \u2030 0u . (8)\nTaking expectation w.r.t. algorithm randomization, we get that\nE A\n\u201c fpwS,T , zq \u00b4 fpwSpiq,T , zq \u2030\n(9)\n\u010f LE A r\u03b4T pS, zqI t\u03b4t0pS, zq \u201c 0us ` E A rfpwS,T , zqI t\u03b4t0pS, zq \u2030 0us . (10)\nRecall that i P rms is the index where S and Spiq differ, and introduce a random variable \u03c4A taking on the index of the first time step where SGD uses the example zi or a replacement z. Note also that \u03c4A does not depend on the data. When \u03c4A \u0105 t0, then it must be that \u03b4t0pS, zq \u201c 0, because updates on both S and Spiq are identical until t0. A consequence of this is that I t\u03b4t0pS, zq \u2030 0u \u010f I t\u03c4A \u010f t0u. Thus the rightmost term in (10) is bounded as\nE A rfpwS,T , zqI t\u03b4t0pS, zq \u2030 0us \u010f E A rfpwS,T , zqI t\u03c4A \u010f t0us .\nNow, focus on the r.h.s. above. Recall that we assume randomization by sampling from the uniform distribution over rms without replacement, and denote a realization by tjiumi\u201c1. Then, we can always express our randomization as permutation function \u03c0ApSq \u201c tzjiu m i\u201c1. In addition, introduce an algorithm\nGD : Zm \u00de\u00d1 H, which is identical to A, except that it passes over the training set S sequentially without randomization. That said, we have that\nE A rfpwS,T , zqI t\u03c4A \u010f t0us \u201c E A\n\u201c fpGD\u03c0ApSq, zqI t\u03c4A \u010f t0u \u2030 ,\nand taking expectation over the data,\nE S,z\n\u201e\nE A rfpwS,T , zqI t\u03c4A \u010f t0us\n\n\u201c E A\n\u201e\nE S,z\n\u201c fpGD\u03c0ApSq, zq \u2030 I t\u03c4A \u010f t0u  .\nNow observe that for any realization of A, ES,z \u201c fpGD\u03c0ApSq, zq \u2030 \u201c EA ES,z rfpAS , zqs because expectation w.r.t. S and z does not change under our randomization 1. Thus, we have that\nE A\n\u201e\nE S,z\n\u201c fpGD\u03c0ApSq, zq \u2030 I t\u03c4A \u010f t0u \n\u201c E S,A rRpASqsPp\u03c4A \u010f t0q .\nNow assuming that \u03c4A is uniformly distributed over rms we have that\nP p\u03c4A \u010f t0q \u201c t0 m .\nPutting this together with (7) and (8), we finally get that\nE S,z E A\n\u201c fpwS,T , zq \u00b4 fpwSpiq,T , zq \u2030\n\u010f L E S,z\n\u201e\nE A r\u03b4T pS, zqI t\u03b4t0pS, zq \u201c 0us\n\n` E S,A rRpASqs t0 m\n\u010f L E S,z\n\u201e\nE A r\u03b4T pS, zq | \u03b4t0pS, zq \u201c 0s\n\n` E S,A rRpASqs t0 m .\nThis completes the proof.\nWe spend a moment to highlight the role of conditional expectation in (6). Observe that we could naively bound (5) by the Lipschitzness of f , but Lemma 5 follows a more careful argument. First note that t0 is a free parameter. The expected distance in (6) between SGD outputs wS,t and wSpiq,t is conditioned on the fact that at step t0 outputs of SGD are still the same. This means that the perturbed point is encountered after t0. Then, the conditional expectation should be a decreasing function of t0: the later the perturbation occurs, the smaller deviation between wS,t and wSpiq,t we should expect. Later we use this fact to minimize the bound (6) over t0.\nA.2 Convex Losses\nIn this section we prove on-average stability for loss functions that are non-negative, \u03b2-smooth, and convex.\nTheorem 6. Assume that f is convex, and that SGD\u2019s is ran with step sizes t\u03b1tuTt\u201c1. Then, for every t0 P t0, 1, 2, . . .mu, SGD is pD,w1q-on-average stable with\npD,w1q \u010f 2\nm\nT \u00ff\nt\u201ct0`1 \u03b1t E S,z r}\u2207fpwt, zjtq}s ` E S,A rRpASqs\nt0 m .\n1Strictly speaking we could omit EAr\u00a8s and consider any randomization by reshuffling, but we keep expectation for the sake of clarity.\nProof. For brevity denote \u2206tpS, zq :\u201c EA r\u03b4tpS, zq | \u03b4t0pS, zq \u201c 0s. We start by applying Lemma 5:\nE S,z E A\n\u201c fpwS,T , zq \u00b4 fpwSpiq,T , zq \u2030 \u010f L E S,z r\u2206T pS, zqs ` E S,A rRpASqs t0 m . (11)\nOur goal is to bound the first term on the r.h.s. as a decreasing function of t0, so that eventually we can minimize the bound w.r.t. t0. At this point we focus on the first term, and the proof partially follows the outline of the proof of Theorem 3.7 in [16]. The strategy will be to establish the bound on \u2206T pS, zq by using a recursive argument. In fact we will state the bound on \u2206t`1pS, zq in terms of \u2206tpS, zq and then unravel the recursion. Finally, we will take expectation w.r.t. the data after we obtain the bound by recursion.\nTo do so, we distinguish two cases: 1) SGD encounters a perturbed point at step t, that is t \u201c i, and 2) the current point is the same in S and Spiq, so t \u2030 i. For the first case, we will use data-dependent boundedness of the gradient update rule, Corollary 3, that is\n}GtpwS,tq \u00b4GtpwSpiq,tq} \u010f \u03b4tpS, zq ` 2\u03b1t}\u2207fpwS,t, zjtq} .\nTo handle the second case, we will use the expansiveness of the gradient update rule, Lemma 1, which states that for convex loss functions, the gradient update rule is 1-expansive, so \u03b4t`1pS, zq \u010f \u03b4tpS, zq. Considering both cases of example selection, and noting that SGD encounters the perturbation w.p. 1m , we write EA for a step t as\n\u2206t`1pS, zq \u010f \u02c6 1\u00b4 1 m \u02d9 \u2206tpS, zq ` 1 m p\u2206tpS, zq ` 2\u03b1t}\u2207fpwS,t, zjtq}q\n\u201c\u2206tpS, zq ` 2\u03b1t}\u2207fpwS,t, zjtq}\nm .\nUnraveling the recursion from T to t0 and plugging the above into (11) yields\nE A E S,z r\u03b4T pS, zqs \u010f\n2\nm\nT \u00ff\nt\u201ct0`1 \u03b1t E S,z r}\u2207fpwt, zjtq}s ` E S,A rRpASqs\nt0 m .\nThis completes the proof.\nNext corollary is a simple consequence of Theorem 6 and Lemma 4.\nProof of Theorem 3. Consider Theorem 6 and set t0 \u201c 0. Then we have that\npD,w1q \u010f 2\nm\nT \u00ff t\u201c1 \u03b1t}\u2207fpwt, zjtq}\n\u010f 2 a 2\u03b2Rpw1q m T \u00ff\nt\u201c1 \u03b1t ,\nwhere the last inequality comes from Lemma 4 assuming that \u03b1t \u010f 2\u03b2 .\nProof of Theorem 4. For brevity denote r \u201c ES,A rRpASqs. Consider Theorem 6 and assume that the step size obeys \u03b1t \u201c ct \u010f 2 \u03b2 . We have\npD,w1q \u010f 2c\nm\nT \u00ff\nt\u201ct0`1\nES,z r}\u2207fpwt, zjtq}s t ` r t0 m\n\u010f 2c a 2\u03b2Rpw1q m ln \u02c6 T\nt0\n\u02d9\n` r t0 m\n(12)\n\u010f 2c a 2\u03b2Rpw1q m T t0 ` r t0 m ,\nwhere in (12) we used Lemma 4 to bound expectation of norm and bounded the sum of the step sizes by the logarithm. Now, setting t0 \u201c b 2 ? 2\u03b2Rpw1qcT r minimizes the bound above, and plugging it back we get that\npD,w1q \u010f 2 b 2 a\n2\u03b2Rpw1qcrT m .\nBy Theorem 2 we then have that\nr \u00b4 E S,A\n\u201d pRSpASq \u0131 \u010f 4 4 a\n\u03b2Rpw1q \u00a8 ? crT\nm (13)\nNow using a simple fact that for any non-negative A,B,C,\nA \u010f B ` C ? A\u00f1 A \u010f B ` C2 ` ? BC ,\nwe get from (13) that\nr \u00b4 E S,A\n\u201d pRSpASq \u0131 \u010f 4 4 a\n\u03b2Rpw1q ? cT\nm\nc\nE S,A\n\u201d pRSpASq \u0131 ` 16 a \u03b2Rpw1qcT m2 .\nThis completes the proof.\nA.3 Non-convex Losses\nOur proof of a stability bound for non-convex loss functions, Theorem 5, follows a general outline of [16, Theorem 3.8]. Namely, the outputs of SGD run on a training set S and its perturbed version Spiq will not differ too much, because by the time a perturbation is encountered, the step size has already decayed enough. So, on the one hand, stabilization is enforced by the diminishing the step size, and on the other hand, by how much updates expand the distance between the gradients after the perturbation. Since [16] work with uniform stability, they capture the expansiveness of post-perturbation update by the Lipschitzness of the gradient. In combination with a recursive argument, their bound has exponential dependency on the Lipschitz constant of the gradient. We argue that the Lipschitz continuity of the gradient can be too pessimistic in general. Instead, we rely on a local data-driven argument: considering that we initialize SGD at point w1, how much do updates expand the gradient under the distribution of interest? The following crucial lemma characterizes such behavior in terms of the curvature at w1.\nLemma 6. Assume that the loss function fp\u00a8, zq is \u03b2-smooth and that its Hessian is \u03c1-Lipschitz. Then, \u203a\n\u203aGtpwS,tq \u00b4GtpwSpiq,tq \u203a \u203a \u010f p1` \u03b1t\u03betpS, zqq \u03b4tpS, zq\nwhere\n\u03betpS, zq :\u201c \u203a \u203a\u22072fpw1, ztq \u203a \u203a 2 ` \u03c1\nc\n\u03b2\n2\nT \u00ff k\u201c1 \u03b1k \u00b4 b fpwS,k, zjkq ` b fpwSpiq,k, z1jkq \u00af .\nFurthermore, for any t P rT s,\nE S,z r\u03betpS, zqs \u010f E z\n\u201c\u203a \u203a\u22072fpw1, zq \u203a \u203a\n2\n\u2030 ` c\u03c1p1` lnpT qq a 2\u03b2Rpw1q .\nProof. Recall that the randomness of the algorithm is realized through sampling without replacement from the uniform distribution over rms. Apart from that we will not be concerned with the randomness of the algorithm, and given the set of random variables tjiumi\u201c1, for brevity we will use indexing notation z1, z2, . . . , zm to indicate zj1 , zj2 , . . . , zjm . Next, let S piq \u201c tz1iu m i\u201c1, and introduce a shorthand notation fkpwq \u201c fpw, zkq and fk1pwq \u201c fpw, z1kq. We start by applying triangle inequality to get \u203a\n\u203aGtpwS,tq \u00b4GtpwSpiq,tq \u203a \u203a \u010f }wS,t \u00b4wSpiq,t} ` \u03b1t \u203a \u203a\u2207ftpwS,tq \u00b4\u2207ftpwSpiq,tq \u203a \u203a .\nIn the following we will focus on the second term of r.h.s. above. Given SGD outputs wS,t and wSpiq,t with t \u0105 i, our goal here is to establish how much do gradients grow apart with every new update. This behavior can be characterized assuming that gradient is Lipschitz continuous, however, we conduct a local analysis. Specifically, we observe how much do updates expand gradients, given that we start at some pointw1 under the data-generating distribution. So, instead of the Lipschitz constant, expansiveness rather depends on the curvature aroundw1. On the other hand, we are dealing with outputs at an arbitrary time step t, and therefore we first have to relate them to the initialization pointw1. We do so by using the gradient update rule and telescopic sums, and conclude that this relationship is controlled by the sum of gradient norms along the update path. We further establish that this sum is controlled by the risk of w1, through self-bounding property of the loss function and Lemma 3. Thus, the proof consists of two parts: 1) Decomposition into curvature and gradients along the update path, and 2) bounding those gradients.\n1) Decomposition. Introduce \u03b4t :\u201c wSpiq,t \u00b4wS,t. By Taylor theorem we get that\n\u2207ftpwS,tq \u00b4\u2207ftpwSpiq,tq \u201c \u017c 1\n0\n\u00b4 \u22072ftpwS,t ` \u03c4\u03b4tq \u00b4\u22072ftpw1q \u00af d\u03c4\u03b4t `\u22072ftpw1q\u03b4t .\nTaking norm on both sides, applying triangle inequality, Cauchy-Schwartz inequality, and assuming that Hessians are \u03c1-Lipschitz we obtain\n}\u2207ftpwS,tq \u00b4\u2207ftpwSpiq,tq} (14)\n\u010f \u017c 1\n0\n\u203a \u203a\u22072ftpwS,t ` \u03c4\u03b4tq \u00b4\u22072ftpw1q \u203a \u203ad\u03c4}\u03b4t} ` \u203a \u203a\u22072ftpw1q \u203a \u203a }\u03b4t}\n\u010f \u03c1 \u017c 1\n0 }wS,t \u00b4w1 ` \u03c4\u03b4t}d\u03c4}\u03b4t} `\n\u203a \u203a\u22072ftpw1q \u203a \u203a }\u03b4t} . (15)\n2) Bounding gradients. Using telescoping sums and SGD update rule we get that\nwS,t \u00b4w1 ` \u03c4\u03b4t \u201c wS,t \u00b4w1 ` \u03c4 ` wSpiq,t \u00b4w1 `w1 \u00b4wS,t \u02d8\n\u201c t\u00b41 \u00ff\nk\u201c1 pwS,k`1 \u00b4wS,kq ` \u03c4\nt\u00b41 \u00ff\nk\u201c1\n` wSpiq,k`1 \u00b4wSpiq,k \u02d8\n\u00b4 \u03c4 t\u00b41 \u00ff\nk\u201c1 pwS,k`1 \u00b4wS,kq\n\u201c p\u03c4 \u00b4 1q t\u00b41 \u00ff\nk\u201c1 \u03b1k\u2207fkpwS,kq \u00b4 \u03c4\nt\u00b41 \u00ff k\u201c1 \u03b1k\u2207fk1pwSpiq,kq .\nPlugging above into the integral of (15) we have\n\u017c 1\n0\n\u203a \u203a \u203a \u203a \u203a t\u00b41 \u00ff\nk\u201c1 \u03b1k\n` p\u03c4 \u00b4 1q\u2207fkpwS,kq \u00b4 \u03c4\u2207fk1pwSpiq,kq \u02d8\n\u203a \u203a \u203a \u203a \u203a d\u03c4\n\u010f 1 2\n\u203a \u203a \u203a \u203a \u203a t\u00b41 \u00ff\nk\u201c1 \u03b1k\u2207fkpwS,kq\n\u203a \u203a \u203a \u203a \u203a ` 1 2 \u203a \u203a \u203a \u203a \u203a t\u00b41 \u00ff\nk\u201c1 \u03b1k\u2207fk1pwSpiq,kq\n\u203a \u203a \u203a \u203a \u203a\n\u010f c \u03b2\n2\nt\u00b41 \u00ff k\u201c1 \u03b1k \u00b4 b fkpwS,kq ` b fk1pwSpiq,kq \u00af ,\nwhere the last inequality comes from the self-bounding property of \u03b2-smooth functions, Lemma 2. Plugging this result back into (15) completes the proof of the first statement.\nBounding ES,zr\u03betpS, zqs. Now we briefly focus on the expectation of \u03betpS, zq, and relate it to the risk of w1 and expectation Hessian. By definition of \u03betpS, zq\nE S,z r\u03betpS, zqs \u010f \u03c1\nc\n\u03b2\n2\nT \u00ff k\u201c1 \u03b1k \u02c6 E S,z \u201d b fpwS,k, zjkq \u0131 ` E S,z \u201db fpwSpiq,k, z1jkq \u0131 \u02d9 ` E S,z \u201c \u203a \u203a\u22072fpw1, ztq \u203a \u203a 2 \u2030 .\nBy Jensen\u2019s inequality and applying Lemma 3 assuming that \u03b1t \u010f 2\u03b2 we have,\nE \u201d b fpwS,k, zjkq \u0131 \u010f b E rfpwS,k, zjkqs \u010f a Rpw1q .\nWe arrive at the same bound for the perturbed term by renaming z1jk into zjk , using the fact that wSpiq,k does not depend on z1jk under the randomization of SGD. Finally putting things together,\nE S,z r\u03betpS, zqs \u010f \u03c1\na 2\u03b2Rpw1q T \u00ff\nk\u201c1 \u03b1k ` E z\n\u201c }\u22072fpw1, zq} \u2030 ,\nand upper bounding \u0159T k\u201c1 \u03b1k \u010f c p1` lnpT qq proves the second statement.\nNext, we need the following statement to prove our stability bound.\nProposition 2 (Bernstein-type inequality). Let Z be a zero-mean real-valued r.v., such that |Z| \u010f b and ErZ2s \u010f \u03c32. Then for all |c| \u010f 12b , we have that E \u201c ecZ \u2030 \u010f ec2\u03c32 .\nProof. Stated inequality is a consequence of a Bernstein-type inequality for moment generating functions, Theorem 2.10 in [4]. Observe that zero-centered r.v. Z bounded by b satisfies Bernstein\u2019s condition, that is\n|ErpZ \u00b4 ErZsqqs| \u010f q! 2 \u03c32bk\u00b42 for all integers q \u011b 3 .\nThis in turn satisfies condition for Bernstein-type inequality stating that\nE rexp pcpZ \u00b4 ErZsqqs \u010f exp \u02c6 c2\u03c32{2 1\u00b4 b|c| \u02d9 .\nChoosing |c| \u010f 12b verifies the statement.\nNow we are ready to prove Theorem 5, which bounds the pD,w1q-on-average stability of SGD.\nProof of Theorem 5. For brevity denote r :\u201c ES,A rRpASqs and\n\u2206tpS, zq :\u201c E A r\u03b4tpS, zq | \u03b4t0pS, zq \u201c 0s .\nBy Lemma 5, for all t0 P rms,\nE S,z E A\n\u201c fpwS,T , zq \u00b4 fpwSpiq,T , zq \u2030 \u010f L E S,z r\u2206T pS, zqs ` r t0 m . (16)\nMost of the proof is dedicated to bounding the first term in (16). We deal with this similarly as in [16]. Specifically, we state the bound on \u2206T pS, zq by using a recursion. In our case, however, we also have an expectation w.r.t. the data, and to avoid complications with dependencies, we first unroll the recursion for the random quantities, and only then take the expectation. At this point the proof crucially relies on the product of exponentials arising from the recursion, and all relevant random quantities end up inside of them. We alleviate this by Proposition 2. Finally, we conclude by minimizing (16) w.r.t. t0. Thus we have three steps: 1) recursion, 2) bounding Erexpp\u00a8 \u00a8 \u00a8 qs, and 3) tuning of t0.\n1) Recursion. We begin by stating the bound on \u2206T pS, zq by recursion. Thus we will first state the bound on \u2206t`1pS, zq in terms of \u2206tpS, zq, and other relevant quantities and then unravel the recursion. As in the convex case, we distinguish two cases: 1) SGD encounters the perturbed point at step t, that is t \u201c i, and 2) the current point is the same in S and Spiq, so t \u2030 i. For the first case, we will use worst-case boundedness of Gt, Corollary 3, that is, }GtpwS,tq \u00b4 GtpwSpiq,tq} \u010f \u03b4tpS, zq ` 2\u03b1tL . To handle the second case we will use Lemma 6, namely,\n\u203a \u203aGtpwS,tq \u00b4GtpwSpiq,tq \u203a \u203a \u010f p1` \u03b1t\u03betpS, zqq \u03b4tpS, zq .\nIn addition, as a safety measure we will also take into account that the gradient update rule is at most p1 ` \u03b1t\u03b2q-expansive by Lemma 1. So we will work with the function \u03c8tpS, zq :\u201c min t\u03betpS, zq, \u03b2u instead of \u03betpS, zq. and decompose the expectation w.r.t. A for a step t. Noting that SGD encounters the perturbed example with probability 1m ,\n\u2206t`1pS, zq \u010f \u02c6 1\u00b4 1 m \u02d9 p1` \u03b1t\u03c8tpS, zqq\u2206tpS, zq ` 1 m p2\u03b1tL`\u2206tpS, zqq\n\u201c \u02c6 1` \u02c6 1\u00b4 1 m \u02d9 \u03b1t\u03c8tpS, zq \u02d9 \u2206tpS, zq ` 2\u03b1tL m\n\u010f exp p\u03b1t\u03c8tpS, zqq\u2206tpS, zq ` 2\u03b1tL\nm , (17)\nwhere the last inequality follows from 1` x \u010f exppxq. This inequality is not overly loose for x P r0, 1s, and, in our case it becomes instrumental in handling the recursion.\nNow, observe that relation xt`1 \u010f atxt ` bt with xt0 \u201c 0 unwinds from T to t0 as xT \u010f \u0159T t\u201ct0`1 bt \u015bT k\u201ct`1 ak. Consequently, having \u2206t0pS, zq \u201c 0, we unwind (17) to get\n\u2206T pS, zq \u010f T \u00ff\nt\u201ct0`1\n\u02dc\nT \u017a\nk\u201ct`1 exp\n\u02c6\nc\u03c8kpS, zq k\n\u02d9\n\u00b8\n2cL\nmt\n\u201c T \u00ff\nt\u201ct0`1 exp\n\u02dc\nc T \u00ff\nk\u201ct`1\n\u03c8kpS, zq k\n\u00b8\n2cL\nmt . (18)\n2) Bounding Erexpp\u00a8 \u00a8 \u00a8 qs. We take expectation w.r.t. S and z on both sides and focus on the expectation of the exponential in (18). First, introduce \u00b5k :\u201c ES,zr\u03c8kpS, zqs, and proceed as\nE S,z\n\u00ab\nexp\n\u02dc\nc T \u00ff\nk\u201ct`1\n\u03c8kpS, zq k\n\u00b8ff\n\u201c E S,z\n\u00ab\nexp\n\u02dc\nc T \u00ff\nk\u201ct`1\n\u03c8kpS, zq \u00b4 \u00b5k k\n\u00b8ff\nexp\n\u02dc\nc T \u00ff\nk\u201ct`1\n\u00b5k k\n\u00b8\n. (19)\nObserve that zero-mean version of \u03c8kpS, zq is bounded as\nT \u00ff\nk\u201ct`1\n|\u03c8kpS, zq \u00b4 \u00b5k| k \u010f 2\u03b2 lnpT q ,\nand assume the setting of c as c \u010f 1 2p2\u03b2 lnpT qq2 . By Proposition 2, we have\nE\n\u00ab\nexp\n\u02dc\nc T \u00ff\nk\u201ct`1\n\u03c8kpS, zq \u00b4 \u00b5k k\n\u00b8ff\n\u010f exp\n\u00a8\n\u02ddc2 E\n\u00bb\n\u2013\n\u02dc\nT \u00ff\nk\u201ct`1\n\u03c8kpS, zq \u00b4 \u00b5k k\n\u00b82 fi\nfl\n\u02db\n\u201a\n\u201c exp\n\u00a8\n\u02dd\nc 2 E\n\u00bb\n\u2013\n\u02dc\n1\n2\u03b2 lnpT q\nT \u00ff\nk\u201ct`1\n\u03c8kpS, zq \u00b4 \u00b5k k\n\u00b82 fi\nfl\n\u02db\n\u201a\n\u010f exp \u02dc c\n2 E\n\u00ab \u02c7\n\u02c7 \u02c7 \u02c7 \u02c7\nT \u00ff\nk\u201ct`1\n\u03c8kpS, zq \u00b4 \u00b5k k\n\u02c7 \u02c7 \u02c7 \u02c7 \u02c7 ff\u00b8\n\u010f exp \u02dc c\n2\nT \u00ff\nk\u201ct`1\nE r|\u03c8kpS, zq \u00b4 \u00b5k|s k\n\u00b8\n\u010f exp \u02dc c T \u00ff\nk\u201ct`1\n\u00b5k k\n\u00b8\n.\nGetting back to (19) we conclude that\nE S,z\n\u00ab\nexp\n\u02dc\nc T \u00ff\nk\u201ct`1\n\u03c8kpS, zq k\n\u00b8ff \u010f exp \u02dc c T \u00ff\nk\u201ct`1\n2\u00b5k k\n\u00b8\n. (20)\nNext, we give an upper-bound on \u00b5k, that is \u00b5k \u010f min t\u03b2,ES,zr\u03bekpS, zqsu. Finally, we bound ES,zr\u03bekpS, zqs using the second result of Lemma 6, which holds for any k P rT s, to get that \u00b5k \u010f \u03b3, with \u03b3 defined in (3).\n3) Tuning of t0. Now we turn our attention back to (18). Considering that we took an expectation w.r.t. the data, we use (20) and the fact that \u00b5k \u010f \u03b3 to get that\nE S,z r\u2206T pS, zqs \u010f\nT \u00ff\nt\u201ct0`1 exp\n\u02dc\n2c\u03b3 T \u00ff\nk\u201ct`1\n1\nk\n\u00b8\n2cL\nmt\n\u010f T \u00ff\nt\u201ct0`1 exp\n\u02c6\n2c\u03b3 ln\n\u02c6\nT\nt\n\u02d9\u02d9\n2cL\nmt\n\u201c 2cL m ` T 2c\u03b3 \u02d8\nT \u00ff\nt\u201ct0`1 t\u00b42c\u03b3\u00b41\n\u010f 1 2c\u03b3 2cL m\n\u02c6\nT\nt0\n\u02d92c\u03b3\n.\nPlug the above into (16) to get\nE S,z E A\n\u201c fpwS,T , zq \u00b4 fpwSpiq,T , zq \u2030\n\u010f L 2\n\u03b3m\n\u02c6\nT\nt0\n\u02d92c\u03b3\n` r t0 m . (21)\nLet q \u201c 2c\u03b3. Then, setting\nt0 \u201c \u02c6 2cL2\nr\n\u02d9 1 1`q\nT q 1`q\nminimizes (21). Plugging t0 back we get that (21) equals to\n1` 1q m ` 2cL2 \u02d8 1 1`q prT q q 1`q .\nThis completes the proof.\nThis theorem implies the following result that is further controlled by the initialization point.\nProof of Corollary 1. Consider statement of Theorem 5. Assuming that step size \u03b1t \u010f 2\u03b2 , Lemma 3 implies that ES,A rRpASqs \u010f Rpw1q, which completes the proof.\nA.3.1 Optimistic Rates for Learning with Non-convex Loss Functions\nNext we will prove an optimistic bound based on Theorem 5, in other words, the bound that demonstrates fast convergence rate subject to the vanishing empirical risk. First we will need the following technical statement.\nLemma 7. [8, Lemma 7.2] Let c1, c2, . . . , cl \u0105 0 and s \u0105 q1 \u0105 q2 \u0105 . . . \u0105 ql\u00b41 \u0105 0. Then the equation\nxs \u00b4 c1xq1 \u00b4 c2xq2 \u00b4 \u00a8 \u00a8 \u00a8 \u00b4 cl\u00b41xql\u00b41 \u00b4 cl \u201c 0\nhas a unique positive solution x\u2039. In addition,\nx\u2039 \u010f max \" plc1q 1 s\u00b4q1 , plc2q 1 s\u00b4q2 , \u00a8 \u00a8 \u00a8 , plcl\u00b41q 1 s\u00b4ql\u00b41 , plclq 1 s * .\nNext we prove a useful technical lemma similarly as in [27, Lemma 7].\nLemma 8. Let a, c \u0105 0 and 0 \u0103 \u03b1 \u0103 1. Then the inequality\nx\u00b4 ax\u03b1 \u00b4 c \u010f 0\nimplies x \u010f max ! 2 \u03b1 1\u00b4\u03b1a 1 1\u00b4\u03b1 , p2cq\u03b1 a ) ` c .\nProof. Consider a function hpxq \u201c x\u00b4 ax\u03b1 \u00b4 c. Applying Lemma 7 with s \u201c 1, l \u201c 2, c1 \u201c a, c2 \u201c c, and q1 \u201c \u03b1 we get that hpxq \u201c 0 has a unique positive solution x\u2039 and\nx\u2039 \u010f max ! p2aq 1 1\u00b4\u03b1 , 2c ) . (22)\nMoreover, the inequality hpxq \u010f 0 is verified for x \u201c 0, and limx\u00d1`8 hpxq \u201c `8, so we have that hpxq \u010f 0 implies x \u010f x\u2039. Now, using this fact and the fact that hpx\u2039q \u201c 0, we have that\nx \u010f x\u2039 \u201c a px\u2039q\u03b1 ` c ,\nand upper-bounding x\u2039 by (22) we finally have\nx \u010f amax ! p2aq \u03b1 1\u00b4\u03b1 , p2cq\u03b1 ) ` c ,\nwhich completes the proof.\nProof of Corollary 2. Consider Theorem 5 and observe that it verifies condition of Lemma 8 with x \u201c ES,A rRpASqs, c \u201c ES,A \u201d pRSpASq \u0131 , \u03b1 \u201c c\u03b31`c\u03b3 , and\na \u201c 1` 1c\u03b3 m ` 2cL2 \u02d8 1 1`c\u03b3 T c\u03b3 1`c\u03b3 .\nNote that \u03b1{p1\u00b4 \u03b1q \u201c c\u03b3 and 1{p1\u00b4 \u03b1q \u201c 1` c\u03b3. Then, we obtain that\nE S,A\n\u201d RpASq \u00b4 pRSpASq \u0131 \u010f max # 2c\u03b3 \u02dc 1` 1c\u03b3 m \u00b81`c\u03b3 ` 2cL2 \u02d8 T c\u03b3 ,\n\u02c6\n2 E S,A\n\u201d pRSpASq \u0131\n\u02d9 c\u03b3 1`c\u03b3 \u02dc 1` 1c\u03b3 m ` 2cL2 \u02d8 1 1`c\u03b3 T c\u03b3 1`c\u03b3 \u00b8+\n\u201c max # \u02c6\n2` 2 c\u03b3\n\u02d91`c\u03b3 `\ncL2 \u02d8\n\u02c6\nT c\u03b3\nm1`c\u03b3\n\u02d9\n,\n1` 1c\u03b3 m ` 2cL2 \u02d8 1 1`c\u03b3 \u02c6 2 E S,A \u201d pRSpASq \u0131 \u00a8 T \u02d9 c\u03b3 1`c\u03b3 + .\nThis completes the proof.\nProof of Proposition 1. Consider minimizing (4) over a discrete set of source hypotheses wsrck (K k\u201c1,\nmin kPrKs pD,wsrck q \u010f min kPrKs O\n\u02dc\n1` 1c\u03b3k m pRpwsrck q \u00a8 T q c\u03b3k 1`c\u03b3k\n\u00b8\n, (23)\nand recall that \u03b3k \u201c E\nz\u201eD\n\u201c }\u22072fpwsrck , zq}2 \u2030\n` \u03bb b\nRpwsrck q ,\nsuch that \u03bb \u201c c\u03c1p1` lnpT qq ? 2\u03b2. Let\n\u03b3\u0303k \u201c 1\nm\nm \u00ff i\u201c1 }\u22072fpwsrck , ziq}2 ` \u03bb b pRSpwsrck q .\nBy Hoeffding inequality, with high probability, we have that |\u03b3k \u00b4 \u03b3\u0303k| \u010f O \u00b4\n1 4 ? m\n\u00af\n. Now we further upper bound (23) by upper bounding Rpwsrck q and applying union bound to get\nmin kPrKs pD,wsrck q \u010f min kPrKs O\n\u00a8\n\u02da \u02da \u02dd\n\u02c6\n1` 1 cq\u03b3\u00b4k\n\u02d9\n\u02dc\npRSpwsrck q ` c logpKq m\n\u00b8 cp\u03b3` k\n1`cp\u03b3` k m \u00b4 1 1`cp\u03b3` k\n\u02db\n\u2039 \u2039 \u201a\n\u010f min kPrKs O\n\u00a8\n\u02dd\n\u02c6\n1` 1 cq\u03b3\u00b4k\n\u02d9 pRSpwsrck q cp\u03b3` k 1`cp\u03b3` k \u00a8 a logpKq\nm 1 1`cp\u03b3` k\n\u02db\n\u201a ,\nwhich concludes the proof."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error is multiplicative in the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization. As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk.", "creator": "LaTeX with hyperref package"}}}