{"id": "1703.01760", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Trust-aware Collaborative Denoising Auto-Encoder for Top-N Recommendation", "abstract": "both feedback of profile ratings and trust relationships can be extensively used to reveal user preference to improve recommendation performance, especially suited for healthy cold domain users. however, the high - order correlations between tow kind bits of data are not always ignored by existing works. towards tackled this problem, we propose a correlative denoising autoencoder ( codae ) model to learn correlations from copying both rating and trust data for top - n recommendation. first, a novel deep learning model codae, in managing which two mid - layers from separate utility stack denoising autoencoders are fused into one shared layer. advancing previous works which utilize these data inputs in various shallow level, this model can effectively extract high - order correlations from low - level representations of these aggregate data for recommendation. directly second, to further learn from implicit corrections between these two same autoencoders, we develop a significantly novel correlative regulation to build the relations between other hidden layers of the two separate autoencoders. lastly in this way, scaling this model also can learn correlations more effectively and thus improve the recommendation quality. comprehensive experiments on two public datasets demonstrate that the codae also significantly outperforms other state - of - the - technology art approaches in top - n recommendation task.", "histories": [["v1", "Mon, 6 Mar 2017 08:35:58 GMT  (43kb,D)", "https://arxiv.org/abs/1703.01760v1", "Under review as a conference paper at IJCAI 2017"], ["v2", "Mon, 8 May 2017 02:10:39 GMT  (123kb,D)", "http://arxiv.org/abs/1703.01760v2", "Submitted to a journal"]], "COMMENTS": "Under review as a conference paper at IJCAI 2017", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.SI", "authors": ["yiteng pan", "fazhi he", "haiping yu"], "accepted": false, "id": "1703.01760"}, "pdf": {"name": "1703.01760.pdf", "metadata": {"source": "CRF", "title": "Trust-aware Collaborative Denoising Auto-Encoder for Top-N Recommendation", "authors": ["Yiteng Pana", "Fazhi Hea", "Haiping Yua"], "emails": [], "sections": [{"heading": null, "text": "tastes for improving recommendation performance, especially for cold users. However, both of them are facing data sparsity problem, which may severely degrade recommendation performance. In this paper, we propose to utilize the idea of Denoising Auto-Encoders (DAE) to tackle this problem. Specially, we propose a novel deep learning model, the Trust-aware Collaborative Denoising AutoEncoder (TDAE), to learn compact and effective representations from both rating and trust data for top-N recommendation. In particular, we present a novel neutral network with a weighted hidden layer to balance the importance of these representations. Moreover, we propose a novel correlative regularization to bridge relations between user preferences in different perspectives. We also conduct comprehensive experiments on two public datasets to compare with several state-of-theart approaches. The results demonstrate that the proposed method significantly outperforms other comparisons for top-N recommendation task.\nKeywords: Recommender Systems, Top-N Recommendation, Denoising Auto-Encoders, Deep Learning"}, {"heading": "1. Introduction", "text": "In recent years, recommender systems are widely used in most web applications to improve user experience. Although numerous recommendation algorithms have been proposed, there are still some well-known issues remaining open, such as data sparsity and cold start. Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].\nPreprint submitted to Information Sciences May 9, 2017\nar X\niv :1\n70 3.\n01 76\n0v 2\n[ cs\n.I R\n] 8\nM ay\nWith the development of social media, trust-aware recommendation algorithms attracts more and more attentions recently. Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5]. Their results demonstrate that the trust relationships are effective to help modeling user preference and improving recommendation performance.\nAlthough existing works propose different ways to incorporate trust information into recommendation, there are still two critical issues with these trust-aware algorithms. First, most of them model the trust relationships with shallow model and ignore the high-order interactions among each users\u2019 friends; it is possible for a user to take all the opinions of his friends into account and then come out his own thinking rather than linearly combine all of them. Second, the trust relationships are also facing the sparse problem as well as ratings. This may limits the improvement of trust-aware algorithms and make it difficult to utilize deep model to learn high-order information from trust data.\nBased on these motivations, we propose a novel deep model TDAE to tackle top-N recommendation task. In this model, we attempt to model user preferences in two perspectives: representations based on rating and trust data. Inspired of the idea of Auto-Encoder that reconstruct input data through a narrow neutral network, we build the TDAE model with a narrow shared layer which fuse userspecific preferences and user representations from rating and trust data. Moreover, to prevent from overfitting, we also consider the correlations between user-specific preference and these representations, and then improve the performance of recommendation.\nIn summary, the contributions in this paper is demonstrated as following:\n\u2022 In this paper, we propose a novel deep learning model to learn user preferences from rating and trust data. Toward the big challenge of data sparse for this problem, the TDAE model is built by fusing two denoising autoencoders with a weighted layer, which is used to balance the importance of rating and trust data. This model can also easily be extended for other recommendation tasks with additional information.\n\u2022 To keep away from overfitting, we further propose a correlative regularization to constraint the learning process. Since we model user preferences in two perspectives, we argue that they can be used to predict each other to a certain degree. This motivate us to propose the Correlative regularization to build relations between the layers in same level. This regularization can efficient improve the effectiveness and robust of TDAE model.\n\u2022 We conduct comprehensive experiments with two datasets to compare our approach with state-of-the-art algorithms on Top-N recommendation task. There are several works show clearly that Top-N recommendation is more close to real application scenarios than rating prediction. So we adopt ranking-sensitive metrics to evaluate the TDAE model, i.e., MAP and NDCG. The results demonstrate that our model significant outperform other comparisons, and is further improved by incorporating correlative regularization."}, {"heading": "2. Related Works", "text": "In this section, we discuss the related works in three branches of our TDAE model: (1) trust-aware recommendation algorithms; (2) recommendation with deep learning; (3) top-N recommendation algorithm."}, {"heading": "2.1. Trust-aware Recommendation", "text": "Trust-aware recommendation algorithms have demonstrated great potential to improve recommendation quality in recent years [5, 6, 8, 10]. Specially, Jamali and Ester propose the SocialMF model by leveraging trust propagation mechanism to model user preference and integrating with matrix factorization for recommendation [5]. Ma et al. then propose a SoReg method by exactly modeling the influence and propagation mechanism between users [6]. Based on the observation that a user demonstrate different preference with roles of truster and trustee, Yang et al. proposed the TrustMF algorithm to further improve performance [8]. To handle the sparse problem of ratings and trust relationships, the TrustSVD model is proposed by taking both explicit and implicit feedback of user trust and ratings into account for rating prediction [10].\nHowever, all these methods utilize trust data in shallow level and ignore the factor that trust relationships are very complex. To learn high-order information from these data, a big challenge is that trust relationships are very sparse and not sufficient to support deep model. Towards this problem, we propose a deep model to learn high-order representations by taking both feedback of ratings and trust relationships into account. First, we propose a TDAE by connecting two AutoEncoders and user-specific preference with a weighted hidden layer to fuse these user preferences in two perspectives. Second, to relieve the data spare problem of rating and trust data, we propose a explicit correlative regularization to constraint the relations between these preferences for each user."}, {"heading": "2.2. Deep Learning for Recommendation", "text": "In recent years, with the rapidly development of deep learning techniques in computer vision and neutral language processing domains, it raise a question that how to utilize deep learning techniques for recommender systems? This problem attracts more and more attentions recently and has become a hot topic in the field of recommender systems.\nThere are numerous works have been proposed to tackle this problem, and they can roughly be categorized into two classes: rating-based methods and auxiliary data based methods. Rating-based methods focus on utilizing deep learning model for recommendation solely based on ratings [17, 18, 19]. These methods leverage the denoising autoencoder (DAE) to learn compact representations of users or items from sparse rating data for recommendation. Their results demonstrate great improvement compare with previous linear models, such as matrix factorization. Auxiliary data based methods propose to learn compact representations from auxiliary data such as content, tag or images, and then combined with traditional matrix factorization method for recommendation [20, 21, 3, 22]. By leveraging these data with deep model, these methods can further push the performance of recommendation to a higher level.\nHowever, all these above existing works focus on utilizing neural networks to learn representations from only one kind of information, such as ratings [17], contents [21], tags [3], or images [22]. In practice, there are numerous works been proposed to learn multiple features from different views [23, 24, 25, 26]. This motivate us to raise a question: how to utilize Auto-Encoder model to learn representations from two kinds of information for recommendation?\nIn this paper, we propose to utilize deep learning model to learn user preferences from both rating and trust data simultaneously. Compared with those aforementioned methods, the TDAE model are consist of two Auto-Encoders to model two kinds of data. They are tied together with a weighted hidden layer, which fuses user preferences of two perspectives. Moreover, inspired of the idea of multimodal deep learning [23], we further propose a novel correlative regularization to build relation between these user preferences for improving performance."}, {"heading": "2.3. Top-N Recommendation Algorithm", "text": "Traditional recommendation algorithms most focus on predict the rating number that user may rate on a particular item [27, 12, 17], i.e., rating prediction task. However, in most scenarios, the goal of recommender systems are to predict a item list for each user to satisfy his/her taste. Therefore, a number of works have been\nproposed to tackle the top-N recommendation problem, which is more suitable for real application [28, 29, 30, 11, 19].\nFor example, in [28], a ranking-oriented approach has been proposed to measure confidence for each user-item pair and improve the matrix factorization method for top-N recommendation. Specially, a Bayesian Personalized Ranking (BPR) [29] algorithm is proposed to direct learn the ranking relation based on implicit feedback for top-N recommendation. More recently, the Collaborative Denoising Auto-Encoders (CDAE) [19] is proposed to utilize the technique of Denoising Auto-Encoders (DAE) to further improve performance. In this paper, the authors propose to predict item list for each user based on user-specific vector and implicit feedback of users. Compared with precious works, this method demonstrates significantly improvement.\nIn this paper, inspired of the CDAE model, we propose an novel Auto-Encoder structure to learn user preferences based on rating and trust data. Compared with CDAE, we focus on the combination of these two kinds of information. Specially, we use a weighted layer and a correlative regularization to learn compact user representations and significantly improve the performance for top-N recommendation task."}, {"heading": "3. TDAE: Trust-aware Collaborative Denoising Auto-Encoder", "text": ""}, {"heading": "3.1. Problem Description", "text": "Assume there are a set of users U = {1, ..., n} and a set of items I = {1, ...,m}, the task in this paper is to generate a list withN items for each user u to satisfy his/her taste. In our system, we have a user-item rating matrix R \u2208 Rn\u00d7m and a user-user trust matrix T \u2208 Rn\u00d7n. There are only few entries in both of them are known and others are missing. For each user u, we denoteORu to represent the item set that user u rated on and O\u0304Ru for the rest of unknown data set; We adopt OTu indicates the user set that useru trusted on and O\u0304Tu for others."}, {"heading": "3.2. Denoising Auto-Encoders", "text": "To handle the sparse problem, we utilize the idea of Denoising Auto-Encoders (DAE) model to build the TDAE model, which is described in next section. DAE model [31] is essentially an improved version of autoencoder [32]. It aims to prevent deep neutral networks from overfitting by reconstructing clean input data from its noising version through a narrow neutral network. In general, the output of the mid-layer represents the compact representations of the input data, and can be used for any other tasks.\nWith the inputs X and its corrupted version X\u0303 , the DAE can be formulated by the following objective function:\nLAE = ||X \u2212 nn(X\u0303)||22 + \u03bb \u2211 l (||Wl||22 + ||bl||22) (1)\nWhere nn(\u00b7) is a symmetric neutral network with parameters Wl and bl of layer l \u2208 {1, ..., L}; || \u00b7 ||2F denote Frobenius norm and \u03bb is a hype-parameter to control the l2 regularization term."}, {"heading": "3.3. The TDAE model", "text": "In recent years, Denoising Auto-Encoders model have been used to improve the performance of recommendation [18, 19, 22]. However, most existed works focus on utilizing Auto-Encoder model for only one kind of information, such as ratings, contents or tags. This motivate us to propose the TDAE model, which utilizes Denoising Auto-Encoder model to learn exactly user preferences from both of rating and trust data.\nThe graphical model of TDAE is demonstrated in figure 1. We can see that this network is started with a encoder layer, followed by a weighted layer and then ended with a decoder layer. Essentially, we tackle the problem that how to learn representations from two kind of sparse information through a weighted layer to balance contributions and a correlative regularization to exchange information.\nIn our approach, we utilize the idea of DAE model described in section 3.2 to build TDAE model. The basic idea of DAE is to reconstruct data from their corrupted version through a narrow network. The most commonly choices are Gaussian noise and drop-out noise. We employ the drop-out noise in our model, which is also used in [19] for top-N recommendation. For each entry x of inputs R and T , the corresponding corrupted version x\u0303 is defined by:\nP (x\u0303 = 0) = q\nP (x\u0303 = \u03b4x) = 1\u2212 q (2)\nWhere q is the probability that randomly drop out a unit; \u03b4 is used to bias the corruptions, which set the clean inputs to \u03b4 = 1/(1 \u2212 q) times their original values.\nAs shown in figure 1, we first map the rating and trust inputs into low-dimensional space with a encoder layer, which is given by:\nZRu = f(W T R\u0303u + b) ZTu = f(V T T\u0303u + c)\n(3)\nWhere R\u0303u and T\u0303u denote the corrupted version of rating and trust data; ZRu and ZTu represent the latent user preferences of u that learn from rating and trust data, respectively; R\u0303u and T\u0303u denote the corrugated rating and trust data for user u; Parameters W \u2208 Rm\u00d7k, V \u2208 Rn\u00d7k, b \u2208 Rm\u00d71, c \u2208 Rn\u00d71 with dimension of k are training to learn user preferences; f(\u00b7) is a element-wise mapping function (e.g., identity function f(x) = x or sigmoid function f(x) = 1/(1 + e\u2212x)), which we adopt sigmoid function in this paper.\nThen we propose a weighted layer to integrate these two kinds of representations. A straightforward approach is to direct concatenate representations from rating and trust data for each user. However, the correlations between ratings and trust data are highly non-linear with different distribution [23]. It means that the information with higher variance may have stronger impact on the outputs, even if the other one may contains important information.\nTo balance the influences of these two kinds of data in TDAE, we develop a weighted hidden layer to fuse these representations. By this way, we can easily tune the contributions of these information for modeling user preference.\nPu = \u03b1Z R u + (1\u2212 \u03b1)ZTu (4)\nWhere Pu denotes the integrated user preference of user u; \u03b1 is a hype-parameter\nto balance the influences of ZRu and Z T u .\nAt last, the TDAE network is followed by two decoder layers to reconstruct original inputs from corrupted data. These two layers are formulated by:\nR\u0302u = g(W \u2032TPu + b \u2032)\nT\u0302u = g(V \u2032TPu + c \u2032) (5)\nWhere R\u0302u and T\u0302u are the prediction value of rating and trust for each user; Parameters W \u2032 \u2208 Rm\u00d7k, V \u2032 \u2208 Rn\u00d7k, b\u2032 \u2208 Rm\u00d71, c\u2032 \u2208 Rn\u00d71 are training to reconstruct inputs; g(\u00b7) is also a element-wise mapping function, and we utilize sigmoid function in this paper.\nTo learn compact representations, we take both reconstruction errors of ratings and trust relationships into account, where existed works mostly ignore the trust relationship. Then we have the objective function of TDAE to minimize as following:\nLT = l(R, R\u0302) + l(T, T\u0302 ) + \u03bbT 2 \u2126(W,W \u2032, V, V \u2032, b, b\u2032, c, c\u2032) (6)\nWhere l(\u00b7) denotes the loss function to compute reconstruction errors; \u2126(\u00b7) is a regularization term that make use of l2 norm and defined by:\n\u2126(\u00b7) =||W ||2F + ||W \u2032||2F + ||V ||2F + ||V \u2032||2F +||b||2F + ||b\u2032||2F + ||c||2F + ||c\u2032||2F\n(7)\nWhere \u03bbT is a hype-parameter to control the model complexity. Specially, we utilize a element-wise cross entropy loss for l(\u00b7) in this paper, which demonstrated to be most suitable for top-N recommendation situation in [19]. Since g(\u00b7) is a sigmoid function, the cross entropy loss is equal to the logistic loss which is defined by:\nl(y, y\u0302) = \u2212ylog(y\u0302)\u2212 (1\u2212 y)log(1\u2212 y\u0302) (8)"}, {"heading": "3.4. Correlations", "text": "There are a critical issue for trust-aware recommendation algorithms: both ratings and trust relationships are very sparse and facing severe overfitting problem; This may raise the risk for Auto-Encoder model to get trapped into local optimal. To improve the recommendation accuracy against sparse problem, we propose a novel correlative regularization term to build relations between the two kinds of information in TDAE.\nIntuitively, since representations ZRu and Z T u represent user preferences for user u in different perspectives, there should exist implicit relation between these two representations. This motivate us to propose a novel regularization term to bridge a relationship between them to exchange information and thus enhance the robust for sparse problem.\nInspired by the idea of Auto-Encoder that reconstruct data from itself through a neural network, we argue that the correlative representations can be predicted by each other through a reconstruction function. Based on this idea, we propose a novel Correlative regularization term to build the relation between the rating and trust data, which is given by:\nLC = ||ZRu \u2212 \u03b80ZTu ||2F + ||ZTu \u2212 \u03b81ZRu ||2F (9)\nWhere parameters {\u03b81, \u03b82} denote the parameters to reconstruct data from its corresponding layer, where we use a linear map function here. Note that any other neutral networks can also be used to build the relations.\nFinally, we have the improved version of TDAE, which taking explicit corrections between hidden layers in TDAE into account to enhance robust. The objective function of TDAE model is rewritten by:\nLT = l(R, R\u0302) + l(T, T\u0302 ) + \u03b2(||ZRu \u2212 \u03b80ZTu ||2F + ||ZTu \u2212 \u03b81ZRu ||2F )\n+ \u03bbT 2 \u2126(W,W \u2032, V, V \u2032, b, b\u2032, c, c\u2032) + \u03bbC 2 R(\u03b80, \u03b81)\n(10)\nWhere \u03b2 is used to control the importance of correlative regularization; R(\u00b7) = ||\u03b80||2F + ||\u03b81||2F is a regularization term to constraint the model complexity, which utilizes l2 norm in this paper; \u03bbC is hype-parameter to control this regularization term."}, {"heading": "3.5. Complexity Analysis", "text": "We apply Stochastic Gradient Descent (SGD) algorithm to train the TDAE model and implement it with the open library TensorFlow1. Since we contain two kinds of information, including rating and trust data, the input dimensionality of each user equals to the sum of item number m and user number n. Then the time complexity for each iteration over all users is O(nk(m+n)). This is not effective when the number of users and items are very large. Toward this problem, we\n1https://www.tensorflow.org/\nuse the learning strategy in [19] for our system. In consideration of that most entries in rating matrix R and trust matrix T are missing and labeled as zeros, we only sample a small subset SRu and S T u from zero entries set O\u0304Ru and O\u0304Tu for each user. Then we compute gradients for each user based on the collection of SRu \u222a STu \u222a ORu \u222a OTu . To prevent data imbalance problem, the sizes of sampled data SRu and S T u equal to ORu and OTu , respectively. In this way, the complexity of our model turns to be O(k(|OR| + |OT |)), which is much more practical than before and suitable for large datasets."}, {"heading": "4. Experiments and Results", "text": ""}, {"heading": "4.1. Datasets", "text": "To evaluate our approach with other state-of-the-arts algorithms, we utilize two real world datasets with both rating and trust data for comparison: Ciao and Epinions datasets. These datasets are independently crawled from two famous e-commerce website, Ciao.com and Epinions.com [33]. Users can rate items on these websites and build trust relation with other users to help making decision. The rating number is an integer range from 1 to 5. Small number indicate dislike while large for the opposite. The trust relationships are formulated in binary format, where 1 for trust and 0 for distrust. The statistics of these datasets are demonstrate in table 1.\nTo address the top-N recommendation task, we remove all ratings that less than 4 stars for all datasets and keep others with score of 1. This preprocessing method aims at recommender item list that users liked, and is widely used in existing works [19]. We then drop those users and items with less than 5 ratings [29].\nWe conduct a 5-fold cross-validation for training and testing. Specially, each dataset are split into 5 folds, and in each time 4 folds are used for training and the remaining one for testing. We conduct the experiments for 5 times to guarantee that each fold have been used for testing. The mean performance will be reported as the results of our experiments."}, {"heading": "4.2. Evaluation Metrics", "text": "In recent years, top-N recommendation have been proved to be more close to real world scenario than rating prediction [34]. In this case, we present each user a item list with N items that have not be rated in training data to fit their potential tastes. Therefore, we adopt ranking-based metrics Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG) in our experiments\nto evaluate the top-N recommendation performance. These two metrics take ranking of the recommender item list into account, and are wildly adopted in existed literature [34].\nLet Iu to denote the set of items that user u have rated in test data, and I\u0302N,u to represent the N predicted items with highest value for user u. Then we have the definition of Precision:\nPrecision@N = |Iu \u2229 I\u0302N,u|/N (11)\nTo more accrue evaluate the performance of precision at all positions of recommended items, Average precision gives higher weighs to the items that user adopted in test data. AP@N is defined as the weighted average of precisions with N recommender items:\nAP@N =\n\u2211N k=1 Precision@k \u00d7 rel(k)\nmin{N, |Iu|} (12)\nWhere Precision@k is the precision with k recommended items, and rel(k) = 1 indicate the item at rank k is adopted. Finally, MAP@N is defined as the mean value of AP@N across all users.\nFor each user, Discounted Cumulative Gain (DCG@N) is defined as:\nDCG@N = N\u2211 k=1 2rel(k) \u2212 1 log2(k + 1)\n(13)\nThe Normalized Discounted Cumulative Gain (NDCG) is the normalized DCG over the ideal iDCG@N, and we denote the mean value of NDCG over all users as results in our experiments."}, {"heading": "4.3. Comparisons and Parameter Settings", "text": "Since we focus on the top-N recommendation problem, it is unreasonable to compare with those for rating prediction task, such as SVD++ [12] or TrustSVD [10]. On this account, we select several state-of-the-art algorithms as comparisons to evaluate our approach:\n\u2022 Pop. This is a commonly used basic algorithm which recommender items according to their popularity in training data.\n\u2022 BPR [29]. This a simple and widely used ranking algorithm for recommendation. It is implemented by learning pairwise relation of rated and unrated items for each user rather than direct learning to predict ratings.\n\u2022 GBPR [35]. This algorithm relax the individual and independence assumptions of BPR model. The authors propose an improved assumption by introducing rich interactions among users. The the size of user group in GBPR is fixed to 5 as suggested in coordinating reference\n\u2022 SBPR [36]. This work improve the BPR model by considering social relation in the learning process, with the assumption that users tend to prefer items that rated by their friends.\n\u2022 SPF [37]. This work proposes a Social Poisson Factorization (SPF) method to model rating and social data with Poisson distribution.\n\u2022 CDAE [19]. The authors develop a deep learning recommendation model by leveraging Stacked Denoising Autoencoder (SDAE) technique. This work further injects user-special preference into hidden layer to improve performance.\nWe implement the TDAE model based on the TensorFlow library 2, and utilize stochastic gradient descent (SGD) algorithm to minimized the loss function of equation 10. In all experiments, we tune the parameters by trail and error in our experiments or according to the suggestions in corresponding references, and report the best results for comparison.\nSpecially, we find that the noise variance make small impact on the results in our experiments. This phenomenon is the same as that in [19], and therefore the drop out possibility q is fixed to 0.2 in our experiments.\n2https://www.tensorflow.org/"}, {"heading": "4.4. Experimental Results", "text": ""}, {"heading": "4.4.1. Validations on all users", "text": "We now demonstrate the performance of TDAE model and compare it with other state-of-the-art algorithms mentioned in section 4.3. Table 3 shows the best results on metrics of MAP@10 and NDCG@10. Note that a larger value of these metrics indicates a better performance.\nIn table 3, we can see that the deep learning model CDAE significantly outperforms precious shallow model (BPR/GBPR/SBPR). It proves that deep learning technique have great potential to improve recommendation, and is worthy of\nfurther development. In Particular, the TDAE and TDAE+ model significantly outperforms other model in metrics of MAP@10 and NDCG@10 for both Ciao and Epinions datasets. We also demonstrate the results with k = 5 and k = 10. The results shows that with dimensionality increase, the performance goes better, especially for Epinions data."}, {"heading": "4.4.2. Validations on cold users", "text": "As mentioned in introduction section, rating and trust data are both very sparse and suffer the performance. In order to further evaluate the capabilities of these methods in views of Cold Users, we conduct validations on users with different rating number. In figure 2, we demonstrate the comparisons results in metric of MAP@10 with k = 10. Since the metric NDCG@10 is in consistent with MAP@10, we omit the results in metric NDCG@10.\nWe can see that the TDAE model outperforms other comparisons for users with different rating numbers. This proves the effectiveness of the TDAE model for not only cold users but also dense users. Specially, we find that the improvement for TDAE is increasing along with the rating number grows. This maybe because the Auto-Encoder models are good at capturing nonlinear information for complex data. Moreover, the improvement of TDAE in Epinions is bigger than that in Ciao for users with rating number larger than 200. This maybe due\nto the Epinions dataset contains more rating data than that in Ciao, and the AutoEncoders can capture more information from these data."}, {"heading": "4.4.3. Impact of parameter \u03b1 on the results", "text": "We use parameter \u03b1 to balance the influences of rating and trust data. Larger values of \u03b1 indicates more impact of rating data for modeling user preferences. If we set \u03b1 = 1, the TDAE only makes predictions based on user ratings and becomes close to the basic Auto-Encoder model. However, if we set \u03b1 = 0, the TDAE only makes predictions based on user trust information and ignores user ratings.\nIn figure 3, we demonstrates the impact of \u03b1 on the results in dataset Ciao and Epinions with k = 10. In these figures, TDAE achieves its best results with \u03b1 = 0.6 and \u03b1 = 0.8 for datasets Ciao and Epinions, respectively. We can see that the best value of \u03b1 for Ciao is smaller than that for Epinions, and both of them are larger than 0.5. This may indicates that users in Ciao are more likely to accept suggestions of their friends than those in Epinions. We can also say that rating data is more important than trust data for both datasets."}, {"heading": "4.4.4. The influence of correlative regularization", "text": "To evaluate the effectiveness of the proposed correlative regularization, we also conduct a serious of experiments to compare the TDAE model with and without this regularization. The comparison results is demonstrated in table 4 with metrics of MAP@10 and NDCG@10. Note that TDAE0 denotes a special\nversion of TDAE that set \u03b2 = 0 for the correlative regularization term. Specially, to evaluate the stability and robust, we also demonstrate the confidence intervals correspond to a 95% range for the 5-folds cross-validation.\nThe experiments results in table 4 demonstrate the TDAE model performs better than the TDAE0 in both datasets with k = 5 and k = 10. This implies that the proposed Correlative regularization can effectively improve the performance. Obviously, the confidence interval of TDAE is much smaller than that of TDAE0 model. This phenomenon proves that this regularization can make the algorithm more stable and robust. Moreover, we can see that the improvement in Epinions dataset is lager than that in Ciao dataset, which may indicate that the corrections between rating and trust data is more complex and difficult to learn in Epinions."}, {"heading": "5. Conclusions", "text": "In this paper, we propose a Trust-aware Collaborative Denoising Autoencoder (TDAE) for the top-N recommendation problem. TDAE learns high-order correlations from rating and trust data through two stacked denoising autoencoders which is united by a shared layer. Moreover, a robust Correlative regularization is proposed to build the relationship between hidden layers in TDAE. The results of several experiments demonstrate that TDAE significantly outperforms stateof-the-art algorithms. We also compare the performance of TDAE and TDAE+ model to evaluate the effectiveness of correlative regularization and demonstrate\nthat it can not only improve the performance but also increase stability of TDAE. The TDAE is a flexible model and easily extended to learn compact representations from other kinds of information.\nFor future works, we intended to further develop the TDAE model for at least three directions but not limited. Firstly, since the rating and trust data are both very sparse, we intend to introduce information of items (such as images or videos) to improve recommendation performance and make use of the recent proposed methods [38, 39, 40, 41, 42]. Secondly, different from that in computer vision field, the data used in recommender systems are very spare and not suitable for most existing deep learning framework (e.g., Caffe, Theano, Torch or TensorFlow). This attract us to utilize multi-core CPU / many-core GPU power [43, 44, 45] for sparse inputs in the future works. Thirdly, the TDAE is a flexible model and can easily be extended to learn representations from other kinds of information. We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]"}, {"heading": "Acknowledgments", "text": "This research has been supported by the National Science Foundation of China (Grant No.61472289) and the National Key Research and Development Project (Grant No.2016YFC0106305)."}], "references": [{"title": "Relational collaborative topic regression for recommender systems", "author": ["H. Wang", "W.-J. Li"], "venue": "IEEE Transactions on Knowledge and Data Engineering 27 (5) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Topic tensor factorization for recommender system", "author": ["X. Zheng", "W. Ding", "Z. Lin", "C. Chen"], "venue": "Information Sciences 372 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "X", "author": ["H. Wang"], "venue": "Shi, D.-Y. Yeung, Relational Stacked Denoising Autoencoder for Tag Recommendation., in: Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining tag correlation and user social relation for microblog recommendation", "author": ["H. Ma", "M. Jia", "D. Zhang", "X. Lin"], "venue": "Information Sciences 385 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks", "author": ["M. Jamali", "M. Ester"], "venue": "in: Proceedings of the Fourth ACM Conference on Recommender Systems, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Recommender Systems with Social Regularization", "author": ["H. Ma", "D. Zhou", "C. Liu", "M.R. Lyu", "I. King"], "venue": "in: Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "An effective recommendation method for cold start new users using trust and distrust networks", "author": ["C.C. Chen", "Y.-H. Wan", "M.-C. Chung", "Y.-C. Sun"], "venue": "Information Sciences 224 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Social Collaborative Filtering by Trust", "author": ["B. Yang", "Y. Lei", "D. Liu", "J. Liu"], "venue": "in: Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, Beijing, China", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Personalized news recommendation via implicit social experts", "author": ["C. Lin", "R. Xie", "X. Guan", "L. Li", "T. Li"], "venue": "Information Sciences 254 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "N", "author": ["G. Guo", "J. Zhang"], "venue": "Yorke-Smith, TrustSVD: Collaborative Filtering with Both the Explicit and Implicit Influence of User Trust and of Item Ratings., in: Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving top-K recommendation with truster and trustee relationship in user trust network", "author": ["C. Park", "D. Kim", "J. Oh", "H. Yu"], "venue": "Information Sciences 374 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "in: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Unifying rating-oriented and rankingoriented collaborative filtering for improved recommendation", "author": ["Y. Shi", "M. Larson", "A. Hanjalic"], "venue": "Information Sciences 229 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Mixed factorization for collaborative recommendation with heterogeneous explicit feedbacks", "author": ["W. Pan", "S. Xia", "Z. Liu", "X. Peng", "Z. Ming"], "venue": "Information Sciences 332 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Social network analysis", "author": ["J. Scott"], "venue": "Sociology 22 (1) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "On Measuring Social Friend Interest Similarities in Recommender Systems", "author": ["H. Ma"], "venue": "in: Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval, New York, NY, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "AutoRec: Autoencoders Meet Collaborative Filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "L. Xie"], "venue": "in: Proceedings of the 24th International Conference on World Wide Web, New York, NY, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Collaborative Filtering via Marginalized Denoising Auto-encoder", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "in: Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative Denoising Auto- Encoders for Top-N Recommender Systems", "author": ["Y. Wu", "C. DuBois", "A.X. Zheng", "M. Ester"], "venue": "in: Proceedings of the Ninth ACM International Conference on Web Search and Data Mining, New York, NY, USA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "author": ["R. He", "J. McAuley"], "venue": "in: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, Phoenix, Arizona", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Collaborative Deep Learning for Recommender Systems", "author": ["H. Wang", "N. Wang", "D.-Y. Yeung"], "venue": "in: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative Deep Learning of Hybrid Representations for Image Recommendations", "author": ["C. Lei", "D. Liu", "W. Li", "Z.-J. Zha", "H. Li"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "in: Proceedings of the 28th international conference on machine learning", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Cross-modal Retrieval with Correspondence Autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "in: Proceedings of the 22Nd ACM International Conference  Submitted to a journal on Multimedia, New York, NY, USA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble manifold regularized sparse low-rank approximation for multiview feature embedding", "author": ["L. Zhang", "Q. Zhang", "L. Zhang", "D. Tao", "X. Huang", "B. Du"], "venue": "Pattern Recognition 48 (10) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A sparse and discriminative tensor to vector projection for human gait feature representation", "author": ["L. Zhang", "L. Zhang", "D. Tao", "B. Du"], "venue": "Signal Processing 106 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Restricted Boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "in: Proceedings of the 24th international conference on Machine learning", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Collaborative Filtering for Implicit Feedback Datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "in: Eighth IEEE International Conference on Data Mining, 2008. ICDM \u201908", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "in: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, Arlington, Virginia, United States", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving the accuracy of top-N recommendation using a preference model", "author": ["J. Lee", "D. Lee", "Y.-C. Lee", "W.-S. Hwang", "S.-W. Kim"], "venue": "Information Sciences 348 (20) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "in: Proceedings of the 25th International Conference on Machine Learning, New York, NY, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "others", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Greedy layerwise training of deep networks, Advances in neural information processing systems 19 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "eTrust: Understanding trust evolution in an online world", "author": ["J. Tang", "H. Gao", "H. Liu", "A. Das Sarma"], "venue": "in: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Eigenrank: a ranking-oriented approach to collaborative filtering", "author": ["N.N. Liu", "Q. Yang"], "venue": "in: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "GBPR: Group Preference Based Bayesian Personalized Ranking for One-class Collaborative Filtering", "author": ["W. Pan", "L. Chen"], "venue": "in: Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, Beijing, China", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Leveraging Social Connections to Improve Personalized Ranking for Collaborative Filtering", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "in: Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, New York, NY, USA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "A Probabilistic Model for Using Social Networks in Personalized Item Recommendation", "author": ["A.J. Chaney", "D.M. Blei", "T. Eliassi-Rad"], "venue": "in: Proceedings of the 9th ACM Conference on Recommender Systems, New York, NY, USA", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time object tracking via compressive feature selection", "author": ["K. Li", "F. He", "X. Chen"], "venue": "Frontiers of Computer Science 10 (4) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "F", "author": ["J. Sun"], "venue": "He, Y.-l. Chen, X. Chen, A multiple template approach for robust tracking of fast motion target, Applied Mathematics-A Journal of Chinese Universities 31 (2) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "An efficient similarity-based level set model for medical image segmentation", "author": ["H. Yu", "F. He", "Y. Pan", "X. Chen"], "venue": "Journal of Advanced Mechanical Design, Systems, and Manufacturing 31 (1) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic Strategy based parallel Ant Colony Optimization on GPUs for TSPs", "author": ["Y. Zhou", "F. He", "Y. Qiu"], "venue": "Science China Information Sciences 60 (6) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Optimization of parallel iterated local search algorithms on graphics processing unit", "author": ["Y. Zhou", "F. He", "Y. Qiu"], "venue": "The Journal of Supercomputing 72 (6) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "ZNNi: Maximizing the Inference Throughput of 3D Convolutional Networks on CPUs and GPUs", "author": ["A. Zlateski", "K. Lee", "H.S. Seung"], "venue": "in: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, Piscataway, NJ, USA", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Meta-operation conflict resolution for humanhuman interaction in collaborative feature-based CAD systems", "author": ["Y. Cheng", "F. He", "Y. Wu", "D. Zhang"], "venue": "Cluster Computing 19 (1) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep learning approach for relationship extraction from interaction context in social manufacturing paradigm", "author": ["J. Leng", "P. Jiang"], "venue": "Knowledge-Based Systems 100 ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Service-Oriented Feature-Based Data Exchange for Cloud-Based Design and Manufacturing", "author": ["Y. Wu", "F. He", "D. Zhang", "X. Li"], "venue": "IEEE Transactions on Services", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Quantitative optimization of interoperability during feature-based data exchange", "author": ["D. Zhang", "F. He", "S.H. Han", "X.X. Li"], "venue": "Integrated Computer-Aided Engineering 23 (1) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "A local start search algorithm to compute exact Hausdorff Distance for arbitrary point sets", "author": ["Y. Chen", "F. He", "Y. Wu", "N. Hou"], "venue": "Pattern Recognition 67 ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2017}, {"title": "Effectiveness of Conflict Management Strategies in Peer Review Process of Online Collaboration Projects", "author": ["W. Huang", "T. Lu", "H. Zhu", "G. Li", "N. Gu"], "venue": "in: Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "A string-wise CRDT algorithm for smart and large-scale collaborative editing systems, Advanced Engineering Informatics.doi:10.1016/j.aei.2016.10.005", "author": ["X. Lv", "F. He", "W. Cai", "Y. Cheng"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2016}, {"title": "Multiagent-Based Allocation of Complex Tasks in Social Networks", "author": ["W. Wang", "Y. Jiang"], "venue": "IEEE Transactions on Emerging Topics in Computing 3 (4) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient improved particle swarm optimization based on prey behavior of fish schooling", "author": ["X. Yan", "F. He", "Y. Chen", "Z. Yuan"], "venue": "Journal of Advanced Mechanical Design, Systems, and Manufacturing 9 (4) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Personalized Recommendation Based on Evolutionary Multi-Objective Optimization", "author": ["Y. Zuo", "M. Gong", "J. Zeng", "L. Ma", "L. Jiao"], "venue": "IEEE Computational Intelligence Magazine 10 (1) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 139, "endOffset": 145}, {"referenceID": 1, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 139, "endOffset": 145}, {"referenceID": 2, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 152, "endOffset": 158}, {"referenceID": 4, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 5, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 6, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 7, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 8, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 9, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 10, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 179, "endOffset": 202}, {"referenceID": 11, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 233, "endOffset": 245}, {"referenceID": 12, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 233, "endOffset": 245}, {"referenceID": 13, "context": "Towards these problems, a lot of researchers propose to leverage additional information to help modeling users and items, such as contents [1, 2], tags [3, 4], social information [5, 6, 7, 8, 9, 10, 11] or multiple feedback of users [12, 13, 14].", "startOffset": 233, "endOffset": 245}, {"referenceID": 14, "context": "Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5].", "startOffset": 81, "endOffset": 89}, {"referenceID": 9, "context": "Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 7, "context": "Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 5, "context": "Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 4, "context": "Based on the phenomenon that users\u2019 tastes are often influenced by their friends [15, 16], there are numerous works been proposed to integrate trust information into recommender system [10, 8, 6, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 4, "context": "Trust-aware Recommendation Trust-aware recommendation algorithms have demonstrated great potential to improve recommendation quality in recent years [5, 6, 8, 10].", "startOffset": 149, "endOffset": 162}, {"referenceID": 5, "context": "Trust-aware Recommendation Trust-aware recommendation algorithms have demonstrated great potential to improve recommendation quality in recent years [5, 6, 8, 10].", "startOffset": 149, "endOffset": 162}, {"referenceID": 7, "context": "Trust-aware Recommendation Trust-aware recommendation algorithms have demonstrated great potential to improve recommendation quality in recent years [5, 6, 8, 10].", "startOffset": 149, "endOffset": 162}, {"referenceID": 9, "context": "Trust-aware Recommendation Trust-aware recommendation algorithms have demonstrated great potential to improve recommendation quality in recent years [5, 6, 8, 10].", "startOffset": 149, "endOffset": 162}, {"referenceID": 4, "context": "Specially, Jamali and Ester propose the SocialMF model by leveraging trust propagation mechanism to model user preference and integrating with matrix factorization for recommendation [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "then propose a SoReg method by exactly modeling the influence and propagation mechanism between users [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "proposed the TrustMF algorithm to further improve performance [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "To handle the sparse problem of ratings and trust relationships, the TrustSVD model is proposed by taking both explicit and implicit feedback of user trust and ratings into account for rating prediction [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 16, "context": "Rating-based methods focus on utilizing deep learning model for recommendation solely based on ratings [17, 18, 19].", "startOffset": 103, "endOffset": 115}, {"referenceID": 17, "context": "Rating-based methods focus on utilizing deep learning model for recommendation solely based on ratings [17, 18, 19].", "startOffset": 103, "endOffset": 115}, {"referenceID": 18, "context": "Rating-based methods focus on utilizing deep learning model for recommendation solely based on ratings [17, 18, 19].", "startOffset": 103, "endOffset": 115}, {"referenceID": 19, "context": "Auxiliary data based methods propose to learn compact representations from auxiliary data such as content, tag or images, and then combined with traditional matrix factorization method for recommendation [20, 21, 3, 22].", "startOffset": 204, "endOffset": 219}, {"referenceID": 20, "context": "Auxiliary data based methods propose to learn compact representations from auxiliary data such as content, tag or images, and then combined with traditional matrix factorization method for recommendation [20, 21, 3, 22].", "startOffset": 204, "endOffset": 219}, {"referenceID": 2, "context": "Auxiliary data based methods propose to learn compact representations from auxiliary data such as content, tag or images, and then combined with traditional matrix factorization method for recommendation [20, 21, 3, 22].", "startOffset": 204, "endOffset": 219}, {"referenceID": 21, "context": "Auxiliary data based methods propose to learn compact representations from auxiliary data such as content, tag or images, and then combined with traditional matrix factorization method for recommendation [20, 21, 3, 22].", "startOffset": 204, "endOffset": 219}, {"referenceID": 16, "context": "However, all these above existing works focus on utilizing neural networks to learn representations from only one kind of information, such as ratings [17], contents [21], tags [3], or images [22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "However, all these above existing works focus on utilizing neural networks to learn representations from only one kind of information, such as ratings [17], contents [21], tags [3], or images [22].", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "However, all these above existing works focus on utilizing neural networks to learn representations from only one kind of information, such as ratings [17], contents [21], tags [3], or images [22].", "startOffset": 177, "endOffset": 180}, {"referenceID": 21, "context": "However, all these above existing works focus on utilizing neural networks to learn representations from only one kind of information, such as ratings [17], contents [21], tags [3], or images [22].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "In practice, there are numerous works been proposed to learn multiple features from different views [23, 24, 25, 26].", "startOffset": 100, "endOffset": 116}, {"referenceID": 23, "context": "In practice, there are numerous works been proposed to learn multiple features from different views [23, 24, 25, 26].", "startOffset": 100, "endOffset": 116}, {"referenceID": 24, "context": "In practice, there are numerous works been proposed to learn multiple features from different views [23, 24, 25, 26].", "startOffset": 100, "endOffset": 116}, {"referenceID": 25, "context": "In practice, there are numerous works been proposed to learn multiple features from different views [23, 24, 25, 26].", "startOffset": 100, "endOffset": 116}, {"referenceID": 22, "context": "Moreover, inspired of the idea of multimodal deep learning [23], we further propose a novel correlative regularization to build relation between these user preferences for improving performance.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "Top-N Recommendation Algorithm Traditional recommendation algorithms most focus on predict the rating number that user may rate on a particular item [27, 12, 17], i.", "startOffset": 149, "endOffset": 161}, {"referenceID": 11, "context": "Top-N Recommendation Algorithm Traditional recommendation algorithms most focus on predict the rating number that user may rate on a particular item [27, 12, 17], i.", "startOffset": 149, "endOffset": 161}, {"referenceID": 16, "context": "Top-N Recommendation Algorithm Traditional recommendation algorithms most focus on predict the rating number that user may rate on a particular item [27, 12, 17], i.", "startOffset": 149, "endOffset": 161}, {"referenceID": 27, "context": "proposed to tackle the top-N recommendation problem, which is more suitable for real application [28, 29, 30, 11, 19].", "startOffset": 97, "endOffset": 117}, {"referenceID": 28, "context": "proposed to tackle the top-N recommendation problem, which is more suitable for real application [28, 29, 30, 11, 19].", "startOffset": 97, "endOffset": 117}, {"referenceID": 29, "context": "proposed to tackle the top-N recommendation problem, which is more suitable for real application [28, 29, 30, 11, 19].", "startOffset": 97, "endOffset": 117}, {"referenceID": 10, "context": "proposed to tackle the top-N recommendation problem, which is more suitable for real application [28, 29, 30, 11, 19].", "startOffset": 97, "endOffset": 117}, {"referenceID": 18, "context": "proposed to tackle the top-N recommendation problem, which is more suitable for real application [28, 29, 30, 11, 19].", "startOffset": 97, "endOffset": 117}, {"referenceID": 27, "context": "For example, in [28], a ranking-oriented approach has been proposed to measure confidence for each user-item pair and improve the matrix factorization method for top-N recommendation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Specially, a Bayesian Personalized Ranking (BPR) [29] algorithm is proposed to direct learn the ranking relation based on implicit feedback for top-N recommendation.", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "More recently, the Collaborative Denoising Auto-Encoders (CDAE) [19] is proposed to utilize the technique of Denoising Auto-Encoders (DAE) to further improve performance.", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "DAE model [31] is essentially an improved version of autoencoder [32].", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "DAE model [31] is essentially an improved version of autoencoder [32].", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "The TDAE model In recent years, Denoising Auto-Encoders model have been used to improve the performance of recommendation [18, 19, 22].", "startOffset": 122, "endOffset": 134}, {"referenceID": 18, "context": "The TDAE model In recent years, Denoising Auto-Encoders model have been used to improve the performance of recommendation [18, 19, 22].", "startOffset": 122, "endOffset": 134}, {"referenceID": 21, "context": "The TDAE model In recent years, Denoising Auto-Encoders model have been used to improve the performance of recommendation [18, 19, 22].", "startOffset": 122, "endOffset": 134}, {"referenceID": 18, "context": "We employ the drop-out noise in our model, which is also used in [19] for top-N recommendation.", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "However, the correlations between ratings and trust data are highly non-linear with different distribution [23].", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "Specially, we utilize a element-wise cross entropy loss for l(\u00b7) in this paper, which demonstrated to be most suitable for top-N recommendation situation in [19].", "startOffset": 157, "endOffset": 161}, {"referenceID": 18, "context": "use the learning strategy in [19] for our system.", "startOffset": 29, "endOffset": 33}, {"referenceID": 32, "context": "com [33].", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "This preprocessing method aims at recommender item list that users liked, and is widely used in existing works [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "We then drop those users and items with less than 5 ratings [29].", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "Evaluation Metrics In recent years, top-N recommendation have been proved to be more close to real world scenario than rating prediction [34].", "startOffset": 137, "endOffset": 141}, {"referenceID": 33, "context": "These two metrics take ranking of the recommender item list into account, and are wildly adopted in existed literature [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Comparisons and Parameter Settings Since we focus on the top-N recommendation problem, it is unreasonable to compare with those for rating prediction task, such as SVD++ [12] or TrustSVD [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "Comparisons and Parameter Settings Since we focus on the top-N recommendation problem, it is unreasonable to compare with those for rating prediction task, such as SVD++ [12] or TrustSVD [10].", "startOffset": 187, "endOffset": 191}, {"referenceID": 28, "context": "\u2022 BPR [29].", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "\u2022 GBPR [35].", "startOffset": 7, "endOffset": 11}, {"referenceID": 35, "context": "\u2022 SBPR [36].", "startOffset": 7, "endOffset": 11}, {"referenceID": 36, "context": "\u2022 SPF [37].", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "\u2022 CDAE [19].", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "This phenomenon is the same as that in [19], and therefore the drop out possibility q is fixed to 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "Firstly, since the rating and trust data are both very sparse, we intend to introduce information of items (such as images or videos) to improve recommendation performance and make use of the recent proposed methods [38, 39, 40, 41, 42].", "startOffset": 216, "endOffset": 236}, {"referenceID": 38, "context": "Firstly, since the rating and trust data are both very sparse, we intend to introduce information of items (such as images or videos) to improve recommendation performance and make use of the recent proposed methods [38, 39, 40, 41, 42].", "startOffset": 216, "endOffset": 236}, {"referenceID": 39, "context": "Firstly, since the rating and trust data are both very sparse, we intend to introduce information of items (such as images or videos) to improve recommendation performance and make use of the recent proposed methods [38, 39, 40, 41, 42].", "startOffset": 216, "endOffset": 236}, {"referenceID": 40, "context": "Firstly, since the rating and trust data are both very sparse, we intend to introduce information of items (such as images or videos) to improve recommendation performance and make use of the recent proposed methods [38, 39, 40, 41, 42].", "startOffset": 216, "endOffset": 236}, {"referenceID": 41, "context": "This attract us to utilize multi-core CPU / many-core GPU power [43, 44, 45] for sparse inputs in the future works.", "startOffset": 64, "endOffset": 76}, {"referenceID": 42, "context": "This attract us to utilize multi-core CPU / many-core GPU power [43, 44, 45] for sparse inputs in the future works.", "startOffset": 64, "endOffset": 76}, {"referenceID": 43, "context": "This attract us to utilize multi-core CPU / many-core GPU power [43, 44, 45] for sparse inputs in the future works.", "startOffset": 64, "endOffset": 76}, {"referenceID": 44, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 85, "endOffset": 105}, {"referenceID": 45, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 85, "endOffset": 105}, {"referenceID": 46, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 85, "endOffset": 105}, {"referenceID": 47, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 85, "endOffset": 105}, {"referenceID": 48, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 85, "endOffset": 105}, {"referenceID": 49, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 124, "endOffset": 136}, {"referenceID": 50, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 124, "endOffset": 136}, {"referenceID": 51, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 124, "endOffset": 136}, {"referenceID": 52, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 163, "endOffset": 175}, {"referenceID": 53, "context": "We intend to extend the proposed method for some other applications, such as CAD/CAM [46, 47, 48, 49, 50], social computing [51, 52, 53] and intelligent computing [54, 55, 56]", "startOffset": 163, "endOffset": 175}], "year": 2017, "abstractText": "Both feedback of ratings and trust relationships can be used to reveal users\u2019 tastes for improving recommendation performance, especially for cold users. However, both of them are facing data sparsity problem, which may severely degrade recommendation performance. In this paper, we propose to utilize the idea of Denoising Auto-Encoders (DAE) to tackle this problem. Specially, we propose a novel deep learning model, the Trust-aware Collaborative Denoising AutoEncoder (TDAE), to learn compact and effective representations from both rating and trust data for top-N recommendation. In particular, we present a novel neutral network with a weighted hidden layer to balance the importance of these representations. Moreover, we propose a novel correlative regularization to bridge relations between user preferences in different perspectives. We also conduct comprehensive experiments on two public datasets to compare with several state-of-theart approaches. The results demonstrate that the proposed method significantly outperforms other comparisons for top-N recommendation task.", "creator": "LaTeX with hyperref package"}}}