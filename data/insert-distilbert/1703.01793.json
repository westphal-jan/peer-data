{"id": "1703.01793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging", "abstract": "music auto - tagging is often handled in a similar manner to image classification by regarding the 2d audio spectrogram as image data. however, music auto - tagging process is distinguished uniquely from image classification in that the audio tags are highly diverse and have different levels of abstractions. considering this issue, we propose a convolutional neural networks ( cnn ) - based architecture that embraces multi - monitor level and multi - scaled features. the architecture is trained in three steps. assuming first, we conduct supervised feature learning to capture local audio features using a set of cnns with different input sizes. second, we extract fresh audio features from each layer of the pre - trained convolutional networks separately and aggregate them altogether given a long audio clip. finally, we put them into fully - connected networks and make final predictions of the tags. our experiments show that using the combination of multi - level and multi - scale features is highly effective in music auto - tagging and the proposed method outperforms previous state - demonstration of - the - arts on the magnatagatune dataset and the million song dataset. we further show already that the proposed architecture is useful greatly in knowledge transfer learning.", "histories": [["v1", "Mon, 6 Mar 2017 09:57:25 GMT  (2332kb,D)", "http://arxiv.org/abs/1703.01793v1", "5 pages, 5 figures, 2 tables"], ["v2", "Wed, 7 Jun 2017 17:21:04 GMT  (2434kb,D)", "http://arxiv.org/abs/1703.01793v2", "5 pages, 5 figures, 2 tables"]], "COMMENTS": "5 pages, 5 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.MM cs.SD", "authors": ["jongpil lee", "juhan nam"], "accepted": false, "id": "1703.01793"}, "pdf": {"name": "1703.01793.pdf", "metadata": {"source": "CRF", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging", "authors": ["Jongpil Lee"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014convolutional neural networks, feature aggregation, music auto-tagging, transfer learning\nI. INTRODUCTION\nMU sic auto-tagging is a task that predicts descriptivewords of music from the audio signals. Recently, as convolutional neural networks (CNN) has become the de-facto standard in image classification, the deep learning approach has been actively explored in music auto-tagging as well by using the spectrogram and its variants as input data and so recasting it as a multi-label classification task on the 2-D timefrequency images [1]\u2013[4].\nHowever, music auto-tagging is distinguished from image classification in that the tags consist of words that are highly diverse and have different levels of abstractions. For example, some words, particularly instrument-related ones, such as female vocalist, guitar and saxophone are objective descriptions of specific sound sources. They tend to be local and repetitive within an audio clip and are basically predicted from the physical properties of the sound sources. On the other hand, other words such as rock, happy and 80s are discriminative descriptions of the overall content in terms of genre, mood and years. They are more global and comprehensive, requiring longer audio segments to predict them.\nWhile the tags are positioned in different levels or timescales in a hierarchy, the majority of previous work predicted them from the same level or scale of features as typically\nJ. Lee and J. Nam are with the Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology (KAIST), South Korea.\ndone in image classification. There are a few that handled this issue explicitly by comparing or combining multi-layer or mutl-scale audio features. In terms of feature level, Lee et. al. used convolutional restricted Boltzmann machine to learn hierarchical features in an unsupervised manner [5]. They compared each layer of features and their combination for music genre and artist classification. However, the evaluation was not sufficiently comprehensive as they used small datasets. Hamel and Eck applied deep neural networks pretrained with deep belief networks to learn hierarchical features and compared each layer of features [6]. However, the learned features were obtained from single frames of spectrogram, which is too local to capture musically meaningful and rich patterns. In terms of time scale, Hamel et. al. investigated combining different resolutions of spectrograms [7], and Dieleman and Schrauwen improved the approach further using Gaussian and Laplacian pyramids [8]. However, they conducted the multiscaling and feature concatenation only on the input layer, focusing on the spectrograms.\nThe general consensus from the previous work is that individual tags have different performance sensitivity to different time scales and levels of features, and so combining all of them provides the best results. With this lesson, we propose a CNN-based architecture that handles multi-level and multi-scale of audio features more comprehensively. The architecture is trained in three steps: local feature learning, feature aggregation and global classification. The local feature learning is carried out using a set of CNNs. They are trained in a supervised manner with the tag labels, taking different sizes of input frames and accordingly more hidden layers. The feature aggregation step extracts local features from all layers and time-scales using the pre-trained CNNs and summarizes them into a single feature vector. The last stage performs final predictions of tags from the aggregated feature vector using a fully-connected neural network. By nature of the separated steps, this architecture is capable of transfer learning, that is, by conducting local feaure learning with one large dataset and then feature aggreation and global classification with another dataset.\nWe evaluate the proposed architecture with two popularly used datasets primarily for music auto-tagging and also in transfer learning settings where music genre classification is performed using pre-trained CNNs. Our experiments show how different combinations of multi-layer and multi-timescale features improve the accuracy and also the architecture outperforms previous state-of-the-arts.\nar X\niv :1\n70 3.\n01 79\n3v 1\n[ cs\n.N E\n] 6\nM ar\n2 01\n7"}, {"heading": "II. PROPOSED METHOD", "text": "The overall architecture that we propose is illustrated in Figure 1. The followings describe the three steps to train it."}, {"heading": "A. Local feature learning", "text": "In the first step, we perform supervised feature learning with a set of CNNs. We chose the segment sizes such that the hidden layers capture multi-level audio features within one to several musical beats for different beats per minute (BPM). For example, 18, 27 and 54 frames correspond to 420, 630 and 1260 msec. They take care of at least one beat long in songs with 48 to 143 BPM, which is the tempo range that covers the majority of popular music. The CNNs are configured to conduct 1-D convolution in all layers, assuming that low-frequency and high-frequency content do not share the weights (as opposed to images) and so the whole frequency range is under the receptive field of the filters. We determined the filter width \u201d3\u201d in the convolutional layers by referring to the VGG net [9]. We first built 27 frames model, which is composed of 3 convolutional layers ((3,128)-(3)-(3,128)-(3)- (3,256)-(3), (filter length, number of filters)-(pool length)), 1\nfully-connected layer (256), and final prediction layer (50). Using this as a reference configuration, For the 18 frames model, the last convolutional layer was replaced by (2,256)- (2) and for models with more than 54 frames as input, we simply added (2,256)-(2) layers according to the input size. Zero-padding is applied to each convolution layer to preserve the size. The convolution stride is fixed to 1 and max-pooling stride is set to the same as the pooling length. We perform the back-propagation with tag lables from the dataset. Each of the models can be actully used to predict the tags for a long audio clip by averaging the local predictions. We call them \u201dlocal models\u201d"}, {"heading": "B. Multi-level and multi-scale feature aggregation", "text": "The pre-trained CNNs can be viewed as feature extractors. Since a single CNN model can extract different levels of features given the input size and we train them with different input sizes, we can extract multi-level and multi-scale features from them. In order to handle the long audio clips (typically, about 30 secs), we aggregate them into a single large feature vector and use them as a song-level representation. For example, the output shape of each convolution layer of one segment in a 27 frames model is (27,128)-(9,128)-(3,256). After extracting features for all segments in 30 seconds audio, the song-level feature dimension become (46,27,128)-(46,9,128)-(46,3,256). In order to extract the most representative feature, we apply max-pooling over each segment separately. We then summarize them into single feature vectors by average pooling over the long audio clip separately for each layer. This scheme, that is, max-pooling followed by average pooling, was used as an effective means to summarize local features [10], [11]. As a result, the dimensionality of the concatenated feature vector will be determined by the sum of the numbers of filters from all layers. For instance, the 27 frames model will have 128 + 128 + 256 dimensional feature vector. This is repeated for all different input sizes and they are finally concatenated into a large feature vector."}, {"heading": "C. Global classification", "text": "In this step, we make final predictions of tags from the aggregated multi-level and multi-scale features. We train another classifier, which a neural network with two fully connected hidden layers with 512 or 1024 units depending on the datasets. Since the feature aggregation and global classification steps are separated from the local feature leanring, we can conduct transfer learning, which has been explored effectively as well for music audio data [12], [13], using pretrained CNNs with a large dataset. In our experiment, we evaluate the transfer learning setting for several different datasets."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Datasets", "text": "To evaluate the proposed architecture, we primarily used the Magnatagatune (MTT) dataset [14] and Million song dataset (MSD) annotated with the Last.fm tags [15]. We filtered out the tags and used most frequently labeled 50 tags, following\nthe previous work [2], [4]1. Also, all songs in the two datasets were trimmed to 29.1 second long. We used AUC (Area Under Receiver Operating Characteristic) as a primary evaluation metric for music auto-tagging. In addition, we used two genre classification datasets, GTZAN [16] and Tagtraum genre annotations on MSD [17], in a transfer learning setting where the pre-trained CNNs with MSD is used as a feature extractor. We used fault-filtered split list for GTZAN [18] and stratified split with 80% training data of CD2C version for Tagtraum."}, {"heading": "B. Training details", "text": "Mel-frequency spectrogram with 128 bins are used as the input representation. The parameters are set to 22.05 kHz sampling rate (by resampling when necessary), 512 samples of hop size, 1024 samples of Hanning window, and magnitude compression with a nonlinear curve, log(1 + C|A|) where A is the magnitude and C is set to 10. As a result, each clip has 1250 frames and is divided into 69, 46, 23, 11 and 5 segments for the corresponding 18, 27, 54, 108 and 216 frames models, respectively. The detailed parameters to train the networks are as follows: sigmoid activation for output layer with binary cross entropy loss, batch normalization [19] and ReLU activation for every intermediate layer, 0.5 of dropout\n1 https://github.com/keunwoochoi/MSD split for tagging\nfor hidden fully connected layers and stochastic gradient descent with 0.9 Nesterov momentum. Also, we conducted the input normalization simply by dividing standard deviation after subtracting mean value of entire input data."}, {"heading": "C. Compared models", "text": "A typical approach for music auto-tagging is to take about 3 second-long audio segments as input and average the outputs to make final predictions for an audio clip (e.g. [2]). Here we call them \u201clocal\u201d models as already denoted in Section II-A. On the other hand, we call our proposed architecture \u201cglobal\u201d models as it aggregates features from local models and makes final predictions directly from the audio clip. In our experiment, we evaluate the two models with various combinations of input sizes and feature levels."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": ""}, {"heading": "A. Comparison of local and global models", "text": "Figure 2 shows the evaluation results for the local and global models on MTT and MSD for different input sizes. From the local models, the AUC reaches the highest level when the input size is 108 frames (about 2.5 second), indicating that taking 3 second as input size is actually a reasonable choice. However, the proposed global models consistently outperform the local models and the performance increment is more vivid on MSD."}, {"heading": "B. Effect of multi-level features", "text": "Figure 3 dissects the effect of multi-level features further in the global model. When a single-level feature is used, higher-level features (L3) apparently work better than lower ones (L1 and L2). When multi-level features are concatenated, the AUC levels consistently increases on both datasets. One interesting result is that each layer have different importance. For example, on MTT, the absence of L1 features decreases the AUC most. On the other hand, on MSD, the absence of L3 features makes a highest drop. This may attribute to difference in tag words between the two datasets. For example, MTT contains more instrument-related tags than genre or mood tags, compared to MSD."}, {"heading": "C. Effect of multi-scaled features", "text": "We now discuss the effect of multi-scale features. Figure 4 shows the results for different combinations of varying input size in the global models. Compared to multi-level features,\nthe performance gain is not strong but the use of multi-scaled features are definitely helpful. The best result is achieved in both MSD and MTT when 18, 27 and 54 frames models are combined. This may be inferred from Figure 2"}, {"heading": "D. Performances visualization of individual tags", "text": "We investigate the global model even further by comparing the performance sensitivity of individual tags to different feature levels and time scales as illustrated in Figure 5. In the multi-level comparison (top), since supervised training is performed with the tags in the local feature learning stage, gradual increment is expected as the the layer goes up. This trend, however, does not work consistenly for every single tag. For example, some tags including blues, chill, guitar and 80s favor L2 features more than L3. The non-gradual increment is observed in the multi-scale comparison (bottom) as well. Some tags including heavy metal, experimental, progressive rock, alternative, chill, guitar and 90s favor 54 frames whereas others including hard rock, easy listening, female vocalist and 70s work better on 18 frames. Overall, we can ensure that the best AUCs in almost all tags are achieved when the multi-level and multi-scale features are concatenated."}, {"heading": "E. Transfer learning and comparison to state-of-the-arts", "text": "In Table I, we show the performance of the proposed architecture in the transfer learning settings where MSD is used to pre-train the CNNs as a feature extractor and other datasets are for the final classification. Interestingly, the autotagging performance on MTT is even greater than those using local models trained from the MTT dataset itself. Also, it shows the music genre classification results on Tagtraum (15 genres) and GTZAN (10 genres). To our knowledge, we report the performance on the Tagtraum dataset for the first time. From Table II, the accuracy on the fault-filtered GTZAN is greater than previously reported ones. Table II also compares the best results in the local and global models to those from\nprevious state-of-the-arts on MTT and MSD. They show that our proposed architecture is highly effective."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this paper, we presented a CNN-based architecture, which is designed considering different levels of abstractions in music tags. We showed the effectiveness of the architecture by evaluating different combinations of the multi-level and multi-scale features and also by applying it to transfer learning settings. Finally, we showed that our proposed architecture achieves the best results on the three popularly used datasets. As future work, we plan to train the architecture in a multitask learning manner by optimizing the local CNNs and global aggregated networks simultaneously."}], "references": [{"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Experimenting with musically motivated convolutional neural networks", "author": ["J. Pons", "T. Lidy", "X. Serra"], "venue": "2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI). IEEE, 2016, pp. 1\u20136.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 805\u2013811.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 22, 2009, pp. 1096\u20131104.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks", "author": ["P. Hamel", "D. Eck"], "venue": "Proceedings of the 11th International Conference on Music Information Retrieval (ISMIR). Utrecht, The Netherlands, 2010, pp. 339\u2013344.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Building musically-relevant audio features through multiple timescale representations", "author": ["P. Hamel", "Y. Bengio", "D. Eck"], "venue": "Proceedings of the 13th International Conference on Music Information Retrieval (ISMIR), 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiscale approaches to music audio feature learning", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "14th International Society for Music Information Retrieval Conference (ISMIR-2013). Pontif\u0131\u0301cia Universidade Cat\u00f3lica do Paran\u00e1, 2013, pp. 116\u2013121.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning sparse feature representations for music annotation and retrieval", "author": ["J. Nam", "J. Herrera", "M. Slaney", "J.O. Smith"], "venue": "Proceedings of the 13th International Conference on Music Information Retrieval (ISMIR), 2012, pp. 565\u2013570.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A deep bag-of-features model for music auto-tagging", "author": ["J. Nam", "J. Herrera", "K. Lee"], "venue": "arXiv preprint arXiv:1508.04999, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning in mir: Sharing learned latent representations for music audio classification and similarity", "author": ["P. Hamel", "M.E.P. Davies", "K. Yoshii", "M. Goto"], "venue": "14th International Conference on Music Information Retrieval (ISMIR \u201913), 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning by supervised pre-training for audio-based music classification", "author": ["A. Van Den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "Conference of the International Society for Music Information Retrieval (ISMIR 2014), 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of algorithms using games: The case of music tagging", "author": ["E. Law", "K. West", "M.I. Mandel", "M. Bay", "J.S. Downie"], "venue": "ISMIR, 2009, pp. 387\u2013392.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), vol. 2, no. 9, 2011, pp. 591\u2013596.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on speech and audio processing, vol. 10, no. 5, pp. 293\u2013302, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving genre annotations for the million song dataset", "author": ["H. Schreiber"], "venue": "Proceedings of the 16th International Conference on Music Information Retrieval (ISMIR), 2015, pp. 241\u2013247.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning and music adversaries", "author": ["C. Kereliuk", "B.L. Sturm", "J. Larsen"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2059\u20132071, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying topological persistence in convolutional neural network for music audio signals", "author": ["J.-Y. Liu", "S.-K. Jeng", "Y.-H. Yang"], "venue": "arXiv preprint arXiv:1608.07373, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "arXiv preprint arXiv:1609.04243, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning temporal features using a deep neural network and its application to music genre classification", "author": ["I.-Y. Jeong", "K. Lee"], "venue": "Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 434\u2013440.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recently, as convolutional neural networks (CNN) has become the de-facto standard in image classification, the deep learning approach has been actively explored in music auto-tagging as well by using the spectrogram and its variants as input data and so recasting it as a multi-label classification task on the 2-D timefrequency images [1]\u2013[4].", "startOffset": 336, "endOffset": 339}, {"referenceID": 3, "context": "Recently, as convolutional neural networks (CNN) has become the de-facto standard in image classification, the deep learning approach has been actively explored in music auto-tagging as well by using the spectrogram and its variants as input data and so recasting it as a multi-label classification task on the 2-D timefrequency images [1]\u2013[4].", "startOffset": 340, "endOffset": 343}, {"referenceID": 4, "context": "used convolutional restricted Boltzmann machine to learn hierarchical features in an unsupervised manner [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Hamel and Eck applied deep neural networks pretrained with deep belief networks to learn hierarchical features and compared each layer of features [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "investigated combining different resolutions of spectrograms [7], and Dieleman and Schrauwen improved the approach further using Gaussian and Laplacian pyramids [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "investigated combining different resolutions of spectrograms [7], and Dieleman and Schrauwen improved the approach further using Gaussian and Laplacian pyramids [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "We determined the filter width \u201d3\u201d in the convolutional layers by referring to the VGG net [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 9, "context": "This scheme, that is, max-pooling followed by average pooling, was used as an effective means to summarize local features [10], [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "This scheme, that is, max-pooling followed by average pooling, was used as an effective means to summarize local features [10], [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Since the feature aggregation and global classification steps are separated from the local feature leanring, we can conduct transfer learning, which has been explored effectively as well for music audio data [12], [13], using pretrained CNNs with a large dataset.", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "Since the feature aggregation and global classification steps are separated from the local feature leanring, we can conduct transfer learning, which has been explored effectively as well for music audio data [12], [13], using pretrained CNNs with a large dataset.", "startOffset": 214, "endOffset": 218}, {"referenceID": 13, "context": "To evaluate the proposed architecture, we primarily used the Magnatagatune (MTT) dataset [14] and Million song dataset (MSD) annotated with the Last.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "fm tags [15].", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "the previous work [2], [4]1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "the previous work [2], [4]1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "In addition, we used two genre classification datasets, GTZAN [16] and Tagtraum genre annotations on MSD [17], in a transfer learning setting where the pre-trained CNNs with MSD is used as a feature extractor.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "In addition, we used two genre classification datasets, GTZAN [16] and Tagtraum genre annotations on MSD [17], in a transfer learning setting where the pre-trained CNNs with MSD is used as a feature extractor.", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "We used fault-filtered split list for GTZAN [18] and stratified split with 80% training data of CD2C version for Tagtraum.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "The detailed parameters to train the networks are as follows: sigmoid activation for output layer with binary cross entropy loss, batch normalization [19] and ReLU activation for every intermediate layer, 0.", "startOffset": 150, "endOffset": 154}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Bag of multi-scaled features [8] 0.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "898 1D CNN [2] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 12, "context": "8815 Transfer learning [13] 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "88 Persistent CNN [20] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "9013 2D CNN [4] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "851 CRNN [21] - 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "862 2D CNN [18] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "632 Temporal features [22] 0.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "Music auto-tagging is often handled in a similar manner to image classification by regarding the 2D audio spectrogram as image data. However, music auto-tagging is distinguished from image classification in that the tags are highly diverse and have different levels of abstractions. Considering this issue, we propose a convolutional neural networks (CNN)-based architecture that embraces multi-level and multi-scaled features. The architecture is trained in three steps. First, we conduct supervised feature learning to capture local audio features using a set of CNNs with different input sizes. Second, we extract audio features from each layer of the pre-trained convolutional networks separately and aggregate them altogether given a long audio clip. Finally, we put them into fully-connected networks and make final predictions of the tags. Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the Magnatagatune dataset and the million song dataset. We further show that the proposed architecture is useful in transfer learning.", "creator": "LaTeX with hyperref package"}}}