{"id": "1703.02573", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "data noising is an effective technique for regularizing neural network models. while noising software is widely adopted in application domains topics such as vision and speech, commonly used noising primitives have not been ever developed for rendering discrete sequence - level settings ( such as language modeling. in modeling this paper, we derive a complicated connection between input noising in neural network language models and smoothing in $ n $ - gram models. using this connection, we draw upon ideas downstream from fourier smoothing to develop effective noising schemes. we already demonstrate performance gains when practically applying the proposed schemes to language modeling and machine translation. finally, we provide empirical theoretical analysis validating the relationship between noising and smoothing.", "histories": [["v1", "Tue, 7 Mar 2017 19:56:26 GMT  (314kb,D)", "http://arxiv.org/abs/1703.02573v1", "ICLR 2017"]], "COMMENTS": "ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ziang xie", "sida i wang", "jiwei li", "daniel l\\'evy", "aiming nie", "dan jurafsky", "rew y ng"], "accepted": true, "id": "1703.02573"}, "pdf": {"name": "1703.02573.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "emails": ["zxie@cs.stanford.edu,", "sidaw@cs.stanford.edu,", "danilevy@cs.stanford.edu,", "anie@cs.stanford.edu,", "ang@cs.stanford.edu,", "jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequencelevel settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in ngram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing."}, {"heading": "1 INTRODUCTION", "text": "Language models are a crucial component in many domains, such as autocompletion, machine translation, and speech recognition. A key challenge when performing estimation in language modeling is the data sparsity problem: due to large vocabulary sizes and the exponential number of possible contexts, the majority of possible sequences are rarely or never observed, even for very short subsequences.\nIn other application domains, data augmentation has been key to improving the performance of neural network models in the face of insufficient data. In computer vision, for example, there exist well-established primitives for synthesizing additional image data, such as by rescaling or applying affine distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012). Similarly, in speech recognition adding a background audio track or applying small shifts along the time dimension has been shown to yield significant gains, especially in noisy settings (Deng et al., 2000; Hannun et al., 2014). However, widely-adopted noising primitives have not yet been developed for neural network language models.\nClassic n-gram models of language cope with rare and unseen sequences by using smoothing methods, such as interpolation or absolute discounting (Chen & Goodman, 1996). Neural network models, however, have no notion of discrete counts, and instead use distributed representations to combat the curse of dimensionality (Bengio et al., 2003). Despite the effectiveness of distributed representations, overfitting due to data sparsity remains an issue. Existing regularization methods, however, are typically applied to weights or hidden units within the network (Srivastava et al., 2014; Le et al., 2015) instead of directly considering the input data.\nIn this work, we consider noising primitives as a form of data augmentation for recurrent neural network-based language models. By examining the expected pseudocounts from applying the noising schemes, we draw connections between noising and linear interpolation smoothing. Using this connection, we then derive noising schemes that are analogues of more advanced smoothing methods. We demonstrate the effectiveness of these schemes for regularization through experiments on language modeling and machine translation. Finally, we validate our theoretical claims by examining the empirical effects of noising.\nar X\niv :1\n70 3.\n02 57\n3v 1\n[ cs\n.L G\n] 7\nM ar\n2 01\n7"}, {"heading": "2 RELATED WORK", "text": "Our work can be viewed as a form of data augmentation, for which to the best of our knowledge there exists no widely adopted schemes in language modeling with neural networks. Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015). Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem.\nFeature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016).\nThe technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al. (2015). However, to the best of our knowledge, no analysis has been provided besides reasoning that zeroing embeddings may result in a model ensembling effect similar to that in standard dropout. This analysis is applicable to classification tasks involving sum-of-embeddings or bag-of-words models, but does not capture sequence-level effects. Bengio et al. (2015) also make an empirical observation that the method of randomly replacing words with fixed probability with a draw from the uniform distribution improved performance slightly for an image captioning task; however, they do not examine why performance improved."}, {"heading": "3 METHOD", "text": ""}, {"heading": "3.1 PRELIMINARIES", "text": "We consider language models where given a sequence of indices X = (x1, x2, \u00b7 \u00b7 \u00b7 , xT ), over the vocabulary V , we model\np(X) = T\u220f t=1 p(xt|x<t)\nIn n-gram models, it is not feasible to model the full context x<t for large t due to the exponential number of possible histories. Recurrent neural network (RNN) language models can (in theory) model longer dependencies, since they operate over distributed hidden states instead of modeling an exponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012).\nAn L-layer recurrent neural network is modeled as h(l)t = f\u03b8(h (l) t\u22121, h (l\u22121) t ), where l denotes the layer index, h(0) contains the one-hot encoding of X , and in its simplest form f\u03b8 applies an affine transformation followed by a nonlinearity. In this work, we use RNNs with a more complex form of f\u03b8, namely long short-term memory (LSTM) units (Hochreiter & Schmidhuber, 1997), which have been shown to ease training and allow RNNs to capture longer dependencies. The output distribution over the vocabulary V at time t is p\u03b8(xt|x<t) = softmax(g\u03b8(h(L)t )), where g : R|h| \u2192 R|V | applies an affine transformation. The RNN is then trained by minimizing over its parameters \u03b8 the sequence cross-entropy loss `(\u03b8) = \u2212\u2211t log p\u03b8(xt|x<t), thus maximizing the likelihood p\u03b8(X). As an extension, we also consider encoder-decoder or sequence-to-sequence (Cho et al., 2014; Sutskever et al., 2014) models where given an input sequence X and output sequence Y of length TY , we model\np(Y |X) = TY\u220f t=1 p(yt|X, y<t).\nand minimize the loss `(\u03b8) = \u2212\u2211t log p\u03b8(yt|X, y<t). This setting can also be seen as conditional language modeling, and encompasses tasks such as machine translation, where X is a source lan-\nguage sequence and Y a target language sequence, as well as language modeling, where Y is the given sequence and X is the empty sequence."}, {"heading": "3.2 SMOOTHING AND NOISING", "text": "Recall that for a given context length l, an n-gram model of order l + 1 is optimal under the loglikelihood criterion. Hence in the case where an RNN with finite context achieves near the lowest possible cross-entropy loss, it behaves like an n-gram model.\nLike n-gram models, RNNs are trained using maximum likelihood, and can easily overfit (Zaremba et al., 2014). While generic regularization methods such L2-regularization and dropout are effective, they do not take advantage of specific properties of sequence modeling. In order to understand sequence-specific regularization, it is helpful to examine n-gram language models, whose properties are well-understood.\nSmoothing for n-gram models When modeling p(xt|x<t), the maximum likelihood estimate c(x<t, xt)/c(x<t) based on empirical counts puts zero probability on unseen sequences, and thus smoothing is crucial for obtaining good estimates. In particular, we consider interpolation, which performs a weighted average between higher and lower order models. The idea is that when there are not enough observations of the full sequence, observations of subsequences can help us obtain better estimates.1 For example, in a bigram model, pinterp(xt|xt\u22121) = \u03bbp(xt|xt\u22121)+ (1\u2212\u03bb)p(xt), where 0 \u2264 \u03bb \u2264 1.\nNoising for RNN models We would like to apply well-understood smoothing methods such as interpolation to RNNs, which are also trained using maximum likelihood. Unfortunately, RNN models have no notion of counts, and we cannot directly apply one of the usual smoothing methods. In this section, we consider two simple noising schemes which we proceed to show correspond to smoothing methods. Since we can noise the data while training an RNN, we can then incorporate well-understood generative assumptions that are known to be helpful in the domain. First consider the following two noising schemes:\n\u2022 unigram noising For each xi in x<t, with probability \u03b3 replace xi with a sample from the unigram frequency distribution.\n\u2022 blank noising For each xi in x<t, with probability \u03b3 replace xi with a placeholder token \u201c \u201d.\nWhile blank noising can be seen as a way to avoid overfitting on specific contexts, we will see that both schemes are related to smoothing, and that unigram noising provides a path to analogues of more advanced smoothing methods."}, {"heading": "3.3 NOISING AS SMOOTHING", "text": "We now consider the maximum likelihood estimate of n-gram probabilities estimated using the pseudocounts of the noised data. By examining these estimates, we draw a connection between linear interpolation smoothing and noising.\nUnigram noising as interpolation To start, we consider the simplest case of bigram probabilities. Let c(x) denote the count of a token x in the original data, and let c\u03b3(x) def = Ex\u0303 [c(x\u0303)] be the expected count of x under the unigram noising scheme. We then have\np\u03b3(xt|xt\u22121) = c\u03b3(xt\u22121, xt)\nc\u03b3(xt\u22121)\n= [(1\u2212 \u03b3)c(xt\u22121, xt) + \u03b3 p(xt\u22121)c(xt)]/c(xt\u22121) = (1\u2212 \u03b3)p(xt|xt\u22121) + \u03b3 p(xt),\nwhere c\u03b3(x) = c(x) since our proposal distribution q(x) is the unigram distribution, and the last line follows since c(xt\u22121)/p(xt\u22121) = c(xt)/p(xt) is equal to the total number of tokens in the training\n1For a thorough review of smoothing methods, we defer to Chen & Goodman (1996).\nset. Thus we see that the noised data has pseudocounts corresponding to interpolation or a mixture of different order n-gram models with fixed weighting.\nMore generally, let x\u0303<t be noised tokens from x\u0303. We consider the expected prediction under noise\np\u03b3(xt|x<t) = Ex\u0303<t [p(xt|x\u0303<t)] = \u2211 J \u03c0(|J |)\ufe38 \ufe37\ufe37 \ufe38 p(|J| swaps) \u2211 xK p(xt|xJ , xK)\ufe38 \ufe37\ufe37 \ufe38 p(xt|noised context) \u220f z\u2208xK p(z)\ufe38\ufe37\ufe37\ufe38 p(drawing z)\nwhere the mixture coefficients are \u03c0(|J |) = (1 \u2212 \u03b3)|J|\u03b3t\u22121\u2212|J| with \u2211J \u03c0(|J |) = 1. J \u2286 {1, 2, . . . , t \u2212 1} denotes the set of indices whose corresponding tokens are left unchanged, and K the set of indices that were replaced.\nBlank noising as interpolation Next we consider the blank noising scheme and show that it corresponds to interpolation as well. This also serves as an alternative explanation for the gains that other related work have found with the \u201cword-dropout\u201d idea (Kumar et al., 2015; Dai & Le, 2015; Bowman et al., 2015). As before, we do not noise the token being predicted xt. Let x\u0303<t denote the random variable where each of its tokens is replaced by \u201c \u201d with probability \u03b3, and let xJ denote the sequence with indices J unchanged, and the rest replaced by \u201c \u201d. To make a prediction, we use the expected probability over different noisings of the context\np\u03b3(xt|x<t) = Ex\u0303<t [p(xt|x\u0303<t)] = \u2211 J \u03c0(|J |)\ufe38 \ufe37\ufe37 \ufe38 p(|J| swaps) p(xt|xJ)\ufe38 \ufe37\ufe37 \ufe38 p(xt|noised context) ,\nwhere J \u2286 {1, 2, . . . , t\u22121}, which is also a mixture of the unnoised probabilities over subsequences of the current context. For example, in the case of trigrams, we have\np\u03b3(x3|x1, x2) = \u03c0(2) p(x3|x1, x2) + \u03c0(1) p(x3|x1, ) + \u03c0(1) p(x3| , x2) + \u03c0(0) p(x3| , )\nwhere the mixture coefficient \u03c0(i) = (1\u2212 \u03b3)i\u03b32\u2212i."}, {"heading": "3.4 BORROWING TECHNIQUES", "text": "With the connection between noising and smoothing in place, we now consider how we can improve the two components of the noising scheme by considering:\n1. Adaptively computing noising probability \u03b3 to reflect our confidence about a particular input subsequence.\n2. Selecting a proposal distribution q(x) that is less naive than the unigram distribution by leveraging higher order n-gram statistics.\nNoising Probability Although it simplifies analysis, there is no reason why we should choose fixed \u03b3; we now consider defining an adaptive \u03b3(x1:t) which depends on the input sequence. Consider the following bigrams:\n\u201cand the\u201d \u201cHumpty Dumpty\u201d\nThe first bigram is one of the most common in English corpora; its probability is hence well estimated and should not be interpolated with lower order distributions. In expectation, however, using fixed \u03b30 when noising results in the same lower order interpolation weight \u03c0\u03b30 for common as well as rare bigrams. Intuitively, we should define \u03b3(x1:t) such that commonly seen bigrams are less likely to be noised.\nThe second bigram, \u201cHumpty Dumpty,\u201d is relatively uncommon, as are its constituent unigrams. However, it forms what Brown et al. (1992) term a \u201csticky pair\u201d: the unigram \u201cDumpty\u201d almost always follows the unigram \u201cHumpty\u201d, and similarly, \u201cHumpty\u201d almost always precedes \u201cDumpty\u201d. For pairs with high mutual information, we wish to avoid backing off from the bigram to the unigram distribution.\nLet N1+(x1, \u2022) def= |{x2 : c(x1, x2) > 0}| be the number of distinct continutions following x1, or equivalently the number of bigram types beginning with x1 (Chen & Goodman, 1996). From the above intuitions, we arrive at the absolute discounting noising probability\n\u03b3AD(x1) = \u03b30 N1+(x1, \u2022)\u2211 x2 c(x1, x2)\nwhere for 0 \u2264 \u03b30 \u2264 1 we have 0 \u2264 \u03b3AD \u2264 1, though in practice we can also clip larger noising probabilities to 1. Note that this encourages noising of unigrams that precede many possible other tokens while discouraging noising of common unigrams, since if we ignore the final token,\u2211 x2 c(x1, x2) = c(x1).\nProposal Distribution While choosing the unigram distribution as the proposal distribution q(x) preserves unigram frequencies, by borrowing from the smoothing literature we find another distribution performs better. We again begin with two motivating examples:\n\u201cSan Francisco\u201d \u201cNew York\u201d\nBoth bigrams appear frequently in text corpora. As a direct consequence, the unigrams \u201cFrancisco\u201d and \u201cYork\u201d also appear frequently. However, since \u201cFrancisco\u201d and \u201cYork\u201d typically follow \u201cSan\u201d and \u201cNew\u201d, respectively, they should not have high probability in the proposal distribution as they might if we use unigram frequencies (Chen & Goodman, 1996). Instead, it would be better to increase the proposal probability of unigrams with diverse histories, or more precisely unigrams that complete a large number of bigram types. Thus instead of drawing from the unigram distribution, we consider drawing from q(x) \u221d N1+(\u2022, x) Note that we now noise the prediction xt in addition to the context x1:t\u22121. Combining this new proposal distribution with the discounted \u03b3AD(x1) from the previous section, we obtain the noising analogue of Kneser-Ney smoothing.\nTable 1 summarizes the discussed noising schemes."}, {"heading": "3.5 TRAINING AND TESTING", "text": "During training, noising is performed per batch and is done online such that each epoch of training sees a different noised version of the training data. At test time, to match the training objective we should sample multiple corrupted versions of the test data, then average the predictions (Srivastava et al., 2014). In practice, however, we find that simply using the maximum likelihood (uncorrupted) input sequence works well; evaluation runtime remains unchanged."}, {"heading": "3.6 EXTENSIONS", "text": "The schemes described are for the language model setting. To extend them to the sequence-tosequence or encoder-decoder setting, we noise both x<t as well as y<t. While in the decoder we\nhave y<t and yt as analogues to language model context and target prediction, it is unclear whether noising x<t should be beneficial. Empirically, however, we find this to be the case (Table 4)."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 LANGUAGE MODELING", "text": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012). The PTB dataset contains 929k training tokens, 73k validation tokens, and 82k test tokens. Following Zaremba et al. (2014), we use minibatches of size 20 and unroll for 35 time steps when performing backpropagation through time. All models have two hidden layers and use LSTM units. Weights are initialized uniformly in the range [\u22120.1, 0.1]. We consider models with hidden sizes of 512 and 1500.\nWe train using stochastic gradient descent with an initial learning rate of 1.0, clipping the gradient if its norm exceeds 5.0. When the validation cross entropy does not decrease after a training epoch, we halve the learning rate. We anneal the learning rate 8 times before stopping training, and pick the model with the lowest perplexity on the validation set.\nFor regularization, we apply feed-forward dropout (Pham et al., 2014) in combination with our noising schemes. We report results in Table 2 for the best setting of the dropout rate (which we find to match the settings reported in Zaremba et al. (2014)) as well as the best setting of noising\nprobability \u03b30 on the validation set.2 Figure 1 shows the training and validation perplexity curves for a noised versus an unnoised run.\nOur large models match the state-of-the-art regularization method for single model performance on this task. In particular, we find that picking \u03b3AD(x1) and q(x) corresponding to Kneser-Ney smoothing yields significant gains in validation perplexity, both for the medium and large size models. Recent work (Merity et al., 2016; Zilly et al., 2016) has also achieved impressive results on this task by proposing different architectures which are orthogonal to our data augmentation schemes.\nText8 In order to determine whether noising remains effective with a larger dataset, we perform experiments on the Text8 corpus3. The first 90M characters are used for training, the next 5M for validation, and the final 5M for testing, resulting in 15.3M training tokens, 848K validation tokens, and 855K test tokens. We preprocess the data by mapping all words which appear 10 or fewer times to the unknown token, resulting in a 42K size vocabulary. Other parameter settings are the same as described in the Penn Treebank experiments, besides that only models with hidden size 512 are considered, and noising is not combined with feed-forward dropout. Results are given in Table 3."}, {"heading": "4.2 MACHINE TRANSLATION", "text": "For our machine translation experiments we consider the English-German machine translation track of IWSLT 20154. The IWSLT 2015 corpus consists of sentence-aligned subtitles of TED and TEDx talks. The training set contains roughly 190K sentence pairs with 5.4M tokens. Following Luong & Manning (2015), we use TED tst2012 as a validation set and report BLEU score results (Papineni et al., 2002) on tst2014. We limit the vocabulary to the top 50K most frequent words for each language.\n2Code will be made available at: http://deeplearning.stanford.edu/noising 3http://mattmahoney.net/dc/text8.zip 4http://workshop2015.iwslt.org/\nWe train a two-layer LSTM encoder-decoder network (Sutskever et al., 2014; Cho et al., 2014) with 512 hidden units in each layer. The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al., 2015). The initial learning rate is 1.0 and we start halving the learning rate when the relative difference in perplexity on the validation set between two consecutive epochs is less than 1%. We follow training protocols as described in Sutskever et al. (2014): (a) LSTM parameters and word embeddings are initialized from a uniform distribution between [\u22120.1, 0.1], (b) inputs are reversed, (c) batch size is set to 128, (d) gradient clipping is performed when the norm exceeds a threshold of 5. We set hidden unit dropout rate to 0.2 across all settings as suggested in Luong et al. (2015). We compare unigram, blank, and bigram Kneser-Ney noising. Noising rate \u03b3 is selected on the validation set.\nResults are shown in Table 4. We observe performance gains for both blank noising and unigram noising, giving roughly +0.7 BLEU score on the test set. The proposed bigram Kneser-Ney noising scheme gives an additional performance boost of +0.5-0.7 on top of the blank noising and unigram noising models, yielding a total gain of +1.4 BLEU."}, {"heading": "5 DISCUSSION", "text": "5.1 SCALING \u03b3 VIA DISCOUNTING\nWe now examine whether discounting has the desired effect of noising subsequences according to their uncertainty. If we consider the discounting\n\u03b3AD(x1) = \u03b30 N1+(x1, \u2022) c(x1)\nwe observe that the denominator c(x1) can dominate than the numerator N1+(x1, \u2022). Common tokens are often noised infrequently when discounting is used to rescale the noising probability, while rare tokens are noised comparatively much more frequently, where in the extreme case when a token appears exactly once, we have \u03b3AD = \u03b30. Due to word frequencies following a Zipfian power law distribution, however, common tokens constitute the majority of most texts, and thus discounting leads to significantly less noising.\nWe compare the performance of models trained with a fixed \u03b30 versus a \u03b30 rescaled using discounting. As shown in Figure 2, bigram discounting leads to gains in perplexity for a much broader range of \u03b30. Thus the discounting ratio seems to effectively capture the \u201cright\u201d tokens to noise."}, {"heading": "5.2 NOISED VERSUS UNNOISED MODELS", "text": "Smoothed distributions In order to validate that data noising for RNN models has a similar effect to that of smoothing counts in n-gram models, we consider three models trained with unigram noising as described in Section 4.1 on the Penn Treebank corpus with \u03b3 = 0 (no noising), \u03b3 = 0.1, and \u03b3 = 0.25. Using the trained models, we measure the Kullback-Leibler divergence DKL(p\u2016q) =\u2211 i pi log(pi/qi) over the validation set between the predicted softmax distributions, p\u0302, and the uniform distribution as well as the unigram frequency distribution. We then take the mean KL divergence over all tokens in the validation set.\nRecall that in interpolation smoothing, a weighted combination of higher and lower order n-gram models is used. As seen in Figure 3, the softmax distributions of noised models are significantly closer to the lower order frequency distributions than unnoised models, in particular in the case of the unigram distribution, thus validating our analysis in Section 3.3.\nUnseen n-grams Smoothing is most beneficial for increasing the probability of unobserved sequences. To measure whether noising has a similar effect, we consider bigrams and trigrams in the validation set that do not appear in the training set. For these unseen bigrams (15062 occurrences) and trigrams (43051 occurrences), we measure the perplexity for noised and unnoised models with near-identical perplexity on the full set. As expected, noising yields lower perplexity for these unseen instances."}, {"heading": "6 CONCLUSION", "text": "In this work, we show that data noising is effective for regularizing neural network-based sequence models. By deriving a correspondence between noising and smoothing, we are able to adapt advanced smoothing methods for n-gram models to the neural network setting, thereby incorporating well-understood generative assumptions of language. Possible applications include exploring noising for improving performance in low resource settings, or examining how these techniques generalize to sequence modeling in other domains."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Will Monroe for feedback on a draft of this paper, Anand Avati for help running experiments, and Jimmy Wu for computing support. We also thank the developers of Theano (Theano Development Team, 2016) and Tensorflow (Abadi et al., 2016). Some GPUs used in this work were donated by NVIDIA Corporation. ZX, SW, and JL were supported by an NDSEG Fellowship, NSERC PGS-D Fellowship, and Facebook Fellowship, respectively. This project was funded in part by DARPA MUSE award FA8750-15-C-0242 AFRL/RIKF."}, {"heading": "A SKETCH OF NOISING ALGORITHM", "text": "We provide pseudocode of the noising algorithm corresponding to bigram Kneser-Ney smoothing for n-grams (In the case of sequence-to-sequence tasks, we estimate the count-based parameters separately for source and target). To simplify, we assume a batch size of one. The noising algorithm is applied to each data batch during training. No noising is applied at test time.\nAlgorithm 1 Bigram KN noising (Language modeling setting) Require counts c(x), number of distinct continuations N1+(x, \u2022), proposal distribution q(x) \u221d N1+(\u2022, x) Inputs X , Y batch of unnoised data indices, scaling factor \u03b30\nprocedure NOISEBGKN(X,Y ) . X = (x1, . . . , xt), Y = (x2, . . . , xt+1) X\u0303, Y\u0303 \u2190 X,Y for j = 1, . . . , t do\n\u03b3 \u2190 \u03b30N1+(xj , \u2022)/c(xj) if \u223c Bernoulli(\u03b3) then\nx\u0303j \u223c Categorical(q) . Updates X\u0303 y\u0303j \u223c Categorical(q) end if end for return X\u0303, Y\u0303 . Run training iteration with noised batch\nend procedure"}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal Of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Chen and Goodman.,? \\Q1996\\E", "shortCiteRegEx": "Chen and Goodman.", "year": 1996}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai and Le.,? \\Q2015\\E", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Large-vocabulary speech recognition under adverse acoustic environments", "author": ["Li Deng", "Alex Acero", "Mike Plumpe", "Xuedong Huang"], "venue": "In ICSLP,", "citeRegEx": "Deng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2000}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": null, "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Association for Computatonal Linguistics (ACL),", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Gradient-based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Minh-Thang Luong", "Christopher D Manning"], "venue": "In Proceedings of the International Workshop on Spoken Language Translation,", "citeRegEx": "Luong and Manning.,? \\Q2015\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Statistical language models based on neural networks", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": "PhD thesis, PhD thesis, Brno University of Technology", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Recurrent dropout without memory loss", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "venue": "arXiv preprint arXiv:1603.05118,", "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Altitude training: Strong bounds for single-layer dropout", "author": ["S. Wager", "W. Fithian", "S.I. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wager et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2014}, {"title": "Data augmentation via levy processes", "author": ["Stefan Wager", "William Fithian", "Percy Liang"], "venue": "arXiv preprint arXiv:1603.06340,", "citeRegEx": "Wager et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2016}, {"title": "Feature noising for log-linear structured prediction", "author": ["Sida I Wang", "Mengqiu Wang", "Stefan Wager", "Percy Liang", "Christopher D Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "In computer vision, for example, there exist well-established primitives for synthesizing additional image data, such as by rescaling or applying affine distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 175, "endOffset": 220}, {"referenceID": 15, "context": "In computer vision, for example, there exist well-established primitives for synthesizing additional image data, such as by rescaling or applying affine distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 175, "endOffset": 220}, {"referenceID": 10, "context": "Similarly, in speech recognition adding a background audio track or applying small shifts along the time dimension has been shown to yield significant gains, especially in noisy settings (Deng et al., 2000; Hannun et al., 2014).", "startOffset": 187, "endOffset": 227}, {"referenceID": 12, "context": "Similarly, in speech recognition adding a background audio track or applying small shifts along the time dimension has been shown to yield significant gains, especially in noisy settings (Deng et al., 2000; Hannun et al., 2014).", "startOffset": 187, "endOffset": 227}, {"referenceID": 4, "context": "Neural network models, however, have no notion of discrete counts, and instead use distributed representations to combat the curse of dimensionality (Bengio et al., 2003).", "startOffset": 149, "endOffset": 170}, {"referenceID": 27, "context": "Existing regularization methods, however, are typically applied to weights or hidden units within the network (Srivastava et al., 2014; Le et al., 2015) instead of directly considering the input data.", "startOffset": 110, "endOffset": 152}, {"referenceID": 18, "context": "Existing regularization methods, however, are typically applied to weights or hidden units within the network (Srivastava et al., 2014; Le et al., 2015) instead of directly considering the input data.", "startOffset": 110, "endOffset": 152}, {"referenceID": 32, "context": "Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015).", "startOffset": 213, "endOffset": 270}, {"referenceID": 26, "context": "Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015).", "startOffset": 213, "endOffset": 270}, {"referenceID": 11, "context": "Classical regularization methods such as L2-regularization are typically applied to the model parameters, while dropout is applied to activations which can be along the forward as well as the recurrent directions (Zaremba et al., 2014; Semeniuta et al., 2016; Gal, 2015).", "startOffset": 213, "endOffset": 270}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015).", "startOffset": 199, "endOffset": 266}, {"referenceID": 18, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015).", "startOffset": 199, "endOffset": 266}, {"referenceID": 31, "context": "Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013).", "startOffset": 139, "endOffset": 158}, {"referenceID": 30, "context": "(2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016).", "startOffset": 172, "endOffset": 192}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al.", "startOffset": 227, "endOffset": 618}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al.", "startOffset": 227, "endOffset": 945}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al.", "startOffset": 227, "endOffset": 966}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al.", "startOffset": 227, "endOffset": 987}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al. (2015). However, to the best of our knowledge, no analysis has been provided besides reasoning that zeroing embeddings may result in a model ensembling effect similar to that in standard dropout.", "startOffset": 227, "endOffset": 1039}, {"referenceID": 1, "context": "Others have introduced methods for recurrent neural networks encouraging the hidden activations to remain stable in norm, or constraining the recurrent weight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015; Le et al., 2015). These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem. Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show that noising can inject appropriate generative assumptions into discriminative models to reduce their generalization error, but do not consider sequence models (Wager et al., 2016). The technique of randomly zero-masking input word embeddings for learning sentence representations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and adopted by others such as Bowman et al. (2015). However, to the best of our knowledge, no analysis has been provided besides reasoning that zeroing embeddings may result in a model ensembling effect similar to that in standard dropout. This analysis is applicable to classification tasks involving sum-of-embeddings or bag-of-words models, but does not capture sequence-level effects. Bengio et al. (2015) also make an empirical observation that the method of randomly replacing words with fixed probability with a draw from the uniform distribution improved performance slightly for an image captioning task; however, they do not examine why performance improved.", "startOffset": 227, "endOffset": 1398}, {"referenceID": 4, "context": "Recurrent neural network (RNN) language models can (in theory) model longer dependencies, since they operate over distributed hidden states instead of modeling an exponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012).", "startOffset": 201, "endOffset": 237}, {"referenceID": 23, "context": "Recurrent neural network (RNN) language models can (in theory) model longer dependencies, since they operate over distributed hidden states instead of modeling an exponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012).", "startOffset": 201, "endOffset": 237}, {"referenceID": 8, "context": "As an extension, we also consider encoder-decoder or sequence-to-sequence (Cho et al., 2014; Sutskever et al., 2014) models where given an input sequence X and output sequence Y of length TY , we model p(Y |X) = TY \u220f", "startOffset": 74, "endOffset": 116}, {"referenceID": 28, "context": "As an extension, we also consider encoder-decoder or sequence-to-sequence (Cho et al., 2014; Sutskever et al., 2014) models where given an input sequence X and output sequence Y of length TY , we model p(Y |X) = TY \u220f", "startOffset": 74, "endOffset": 116}, {"referenceID": 32, "context": "Like n-gram models, RNNs are trained using maximum likelihood, and can easily overfit (Zaremba et al., 2014).", "startOffset": 86, "endOffset": 108}, {"referenceID": 17, "context": "This also serves as an alternative explanation for the gains that other related work have found with the \u201cword-dropout\u201d idea (Kumar et al., 2015; Dai & Le, 2015; Bowman et al., 2015).", "startOffset": 125, "endOffset": 182}, {"referenceID": 5, "context": "This also serves as an alternative explanation for the gains that other related work have found with the \u201cword-dropout\u201d idea (Kumar et al., 2015; Dai & Le, 2015; Bowman et al., 2015).", "startOffset": 125, "endOffset": 182}, {"referenceID": 6, "context": "However, it forms what Brown et al. (1992) term a \u201csticky pair\u201d: the unigram \u201cDumpty\u201d almost always follows the unigram \u201cHumpty\u201d, and similarly, \u201cHumpty\u201d almost always precedes \u201cDumpty\u201d.", "startOffset": 23, "endOffset": 43}, {"referenceID": 27, "context": "At test time, to match the training objective we should sample multiple corrupted versions of the test data, then average the predictions (Srivastava et al., 2014).", "startOffset": 138, "endOffset": 163}, {"referenceID": 31, "context": "4 Zaremba et al. (2014) 82.", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "4 Gal (2015) variational dropout (tied weights) 77.", "startOffset": 2, "endOffset": 13}, {"referenceID": 11, "context": "4 Gal (2015) variational dropout (tied weights) 77.3 75.0 Gal (2015) (untied weights, Monte Carlo) \u2014 73.", "startOffset": 2, "endOffset": 69}, {"referenceID": 11, "context": "We also compare to the variational method of Gal (2015), who also train LSTM models with the same hidden dimension.", "startOffset": 45, "endOffset": 56}, {"referenceID": 23, "context": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012).", "startOffset": 161, "endOffset": 176}, {"referenceID": 25, "context": "For regularization, we apply feed-forward dropout (Pham et al., 2014) in combination with our noising schemes.", "startOffset": 50, "endOffset": 69}, {"referenceID": 23, "context": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012). The PTB dataset contains 929k training tokens, 73k validation tokens, and 82k test tokens. Following Zaremba et al. (2014), we use minibatches of size 20 and unroll for 35 time steps when performing backpropagation through time.", "startOffset": 162, "endOffset": 301}, {"referenceID": 23, "context": "Penn Treebank We train networks for word-level language modeling on the Penn Treebank dataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012). The PTB dataset contains 929k training tokens, 73k validation tokens, and 82k test tokens. Following Zaremba et al. (2014), we use minibatches of size 20 and unroll for 35 time steps when performing backpropagation through time. All models have two hidden layers and use LSTM units. Weights are initialized uniformly in the range [\u22120.1, 0.1]. We consider models with hidden sizes of 512 and 1500. We train using stochastic gradient descent with an initial learning rate of 1.0, clipping the gradient if its norm exceeds 5.0. When the validation cross entropy does not decrease after a training epoch, we halve the learning rate. We anneal the learning rate 8 times before stopping training, and pick the model with the lowest perplexity on the validation set. For regularization, we apply feed-forward dropout (Pham et al., 2014) in combination with our noising schemes. We report results in Table 2 for the best setting of the dropout rate (which we find to match the settings reported in Zaremba et al. (2014)) as well as the best setting of noising", "startOffset": 162, "endOffset": 1190}, {"referenceID": 22, "context": "Recent work (Merity et al., 2016; Zilly et al., 2016) has also achieved impressive results on this task by proposing different architectures which are orthogonal to our data augmentation schemes.", "startOffset": 12, "endOffset": 53}, {"referenceID": 33, "context": "Recent work (Merity et al., 2016; Zilly et al., 2016) has also achieved impressive results on this task by proposing different architectures which are orthogonal to our data augmentation schemes.", "startOffset": 12, "endOffset": 53}, {"referenceID": 24, "context": "Following Luong & Manning (2015), we use TED tst2012 as a validation set and report BLEU score results (Papineni et al., 2002) on tst2014.", "startOffset": 103, "endOffset": 126}, {"referenceID": 28, "context": "We train a two-layer LSTM encoder-decoder network (Sutskever et al., 2014; Cho et al., 2014) with 512 hidden units in each layer.", "startOffset": 50, "endOffset": 92}, {"referenceID": 8, "context": "We train a two-layer LSTM encoder-decoder network (Sutskever et al., 2014; Cho et al., 2014) with 512 hidden units in each layer.", "startOffset": 50, "endOffset": 92}, {"referenceID": 2, "context": "The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 21, "context": ", 2014) with the dot alignment function (Luong et al., 2015).", "startOffset": 40, "endOffset": 60}, {"referenceID": 2, "context": "The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al., 2015). The initial learning rate is 1.0 and we start halving the learning rate when the relative difference in perplexity on the validation set between two consecutive epochs is less than 1%. We follow training protocols as described in Sutskever et al. (2014): (a) LSTM parameters and word embeddings are initialized from a uniform distribution between [\u22120.", "startOffset": 41, "endOffset": 372}, {"referenceID": 2, "context": "The decoder uses an attention mechanism (Bahdanau et al., 2014) with the dot alignment function (Luong et al., 2015). The initial learning rate is 1.0 and we start halving the learning rate when the relative difference in perplexity on the validation set between two consecutive epochs is less than 1%. We follow training protocols as described in Sutskever et al. (2014): (a) LSTM parameters and word embeddings are initialized from a uniform distribution between [\u22120.1, 0.1], (b) inputs are reversed, (c) batch size is set to 128, (d) gradient clipping is performed when the norm exceeds a threshold of 5. We set hidden unit dropout rate to 0.2 across all settings as suggested in Luong et al. (2015). We compare unigram, blank, and bigram Kneser-Ney noising.", "startOffset": 41, "endOffset": 703}], "year": 2017, "abstractText": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequencelevel settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in ngram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "creator": "LaTeX with hyperref package"}}}