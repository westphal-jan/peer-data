{"id": "1703.02883", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Memory Enriched Big Bang Big Crunch Optimization Algorithm for Data Clustering", "abstract": "cluster analysis plays an important role in decision making process for many knowledge - based mathematical systems. there exist a wide variety of different approaches for clustering applications including the heuristic techniques, probabilistic models, and traditional hierarchical algorithms. in this previous paper, a novel heuristic approach based research on big money bang - modeled big crunch algorithm is proposed proposed again for the clustering problems. while the proposed method not only takes advantage of heuristic nature to alleviate typical clustering algorithms such as k - means, but it also benefits from the memory based scheme as compared to its similar heuristic techniques. furthermore, the estimation performance of the explicitly proposed new algorithm itself is investigated based on several benchmark test functions as well as on the well - known datasets. the statistical experimental results therefore show the significant superiority of the proposed method over the similar hierarchical algorithms.", "histories": [["v1", "Wed, 8 Mar 2017 15:50:35 GMT  (439kb)", "http://arxiv.org/abs/1703.02883v1", "17 pages, 3 figures, 8 tables"]], "COMMENTS": "17 pages, 3 figures, 8 tables", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kayvan bijari", "hadi zare", "hadi veisi", "hossein bobarshad"], "accepted": false, "id": "1703.02883"}, "pdf": {"name": "1703.02883.pdf", "metadata": {"source": "CRF", "title": "Memory Enriched Big Bang Big Crunch Optimization Algorithm for Data Clustering", "authors": ["Kayvan Bijari", "Hossein Bobarshad"], "emails": ["h.zare@ut.ac.ir"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n02 88\n3v 1\n[ cs\n.A I]\n8 M\nar 2\n01 7\nKeywords Evolutionary Algorithms \u00b7 Big Bang-Big Crunch Algorithm \u00b7 Clustering \u00b7 Unsupervised Learning"}, {"heading": "1 Introduction", "text": "Unsupervised learning can be considered as an important category of machine learning techniques to uncover the interesting hidden patterns from the dataset. Unsupervised learning methods can be generally divided into clustering, dimensionality reduction, image segmentation, object recognition, and text mining techniques [1]. One of the most common unsupervised learning methods is clustering. Clustering is the task of assigning a set of objects, usually vectors in a multidimensional space, into clusters in such a way that the objects in the same cluster are more similar to each other than to those in other clusters [2]. Cluster analysis has attracted attention of many researchers in different fields, including data mining [3], sequence mining [4], image processing [5], feature selection techniques [6], spatial data analysis [3], bioinformatics [7,8], marketing [3], city planning [3], and earthquake studies [3].\nH. Zare ( ) University of Tehran E-mail: h.zare@ut.ac.ir\nDue to not well-defined nature of clustering problems and growing importance of clustering in a variety of different fields, lots of clustering approaches have emerged. Since clustering problem is NP-hard by its nature [9], meta-heuristic methods are proper tools to deal with this issue. Among many studies in metaheuristic approaches for clustering, one can mention clustering algorithm based on tabu search [10], genetic algorithms [11], ant colony [12], particle swarm optimization [13], and bee colony [14]. Meta-heuristic methods also make use of a computationally efficient clustering algorithm and seek to find better results. One common approach which can easily be used along with meta-heuristic techniques is k-means algorithm [15]. The k-means is one of the well-known clustering methods. In spite of its simplicity and efficiency, it suffers from serious problems such as sensitivity to the initial position of cluster centers, empty clusters occurrence, and trapping in local optima. As a result, k-means can be prevented from finding global optimum.\nThere exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].\nOne of the well-known models in theoretical physics is the Big Bang theory for illustration of the universe existence and its evolution from the past known historical spans over its large-scale evolution. A novel optimization algorithm named Big Bang-Big Crunch algorithm(BB-BC) based on these theories is first initiated in [24] which have been applied in many works including economic power systems [25,26] and signal processing [27]. On the one hand, the BB-BC algorithm has been started from theoretical concepts of cosmological physics. On the other hand, the BB-BC algorithm outperforms a wide category of evolutionary algorithms which are very sensitive to initial solutions. Due to its modification of the initial solution in the process of the algorithm, BB-BC is aimed at achieving the optimal solution. Thus, BB-BC could be selected as a proper choice for a variety of different optimization and intractable problems.\nWhile the BB-BC are used in several works, it suffers from disadvantages such as slow convergence speed and trapping in local optimum solutions available in most of the optimization problems [28]. The problem of converging to local optimum solutions occurred for the BB-BC approach due to greedily looking around the best ever found solutions. Due to its explorative nature, BB-BC lacks a splendid exploitation factor. Such optimization strategies should have a mechanism to make a trade-off between exploration and exploitation.\nIn this paper a new heuristic clustering algorithm is developed. We designed memory enriched BB-BC(ME-BB-BC) algorithm to solve the aforementioned drawbacks of the traditional BB-BC method. The proposed algorithm takes advantages of typical BB-BC algorithm and enhances it with the proper balance between exploration and exploitation factors. Proposed approach not only is capable of performing clustering task but it could also be used for other general optimization problems. Results on benchmark evaluation functions and real benchmark datasets indicate that ME-BB-BC outperforms significantly the typical BB-BC method and other meta-heuristic algorithms.\nThe remainder of the paper is structured as, Section 2 presents the cluster analysis. The related BB-BC algorithm is given in Section 3. The proposed approach based on BB-BC algorithm is introduced in Section 4 and is evaluated and analyzed on benchmark functions in Section 5. Section 6 is devoted to fundamentals of clustering using the proposed algorithm. Experimental results of the proposed clustering approach are illustrated in Section 7. Finally Section 8 concludes the paper with suggestions for future works."}, {"heading": "2 Cluster analysis", "text": "Clustering is the procedure of dividing a set of objects each explained by a vector of attributes into a finite number of clusters in a way that based on similarity functions, objects in the same cluster will be similar to each other and different from the objects in other clusters. The k-means is a computationally efficient clustering method which has been widely used [3]. While it is proved that the process of the algorithm will always converge [29], k-means does not guarantee to find an optimal solution. The algorithm is also essentially sensitive to the primary cluster centers. Moreover, k-means also suffers from the occurrence of empty clusters during its iterations. If there is a cluster with no instance, k-means is unable to update that cluster centroids.\nThe procedure of k-means is as follows. First, k-means assigns initial values to centroids and then it continues for several iterations. In each iteration, k-means create clusters by assigning all data points to their nearest centroids and then substitute the mean of each cluster for its centroid. As stopping criterion, the number of iteration can be determined a priori or iterations continue until there is no changes in centroids. Also, a combination of these two criteria is possible.\nThe essential aim of a clustering algorithm, such as k-means, is to discover a proper assignment of data points to clusters and find an arrangement of \u00b5k vectors in a manner that sum of the squares of the distances of each data point to its nearest centroid is least. For every data point xn, a corresponding set of binary indicators such as rn,k \u2208 {0, 1} is presented. Where k \u2208 {1, . . . ,K} describes which of the K clusters, the data point xn falls into. So if data point xn is assigned to cluster k then rn,k = 1, and rn,j = 0 for j \u2208 {1, . . . ,K|j 6= k}. Therefore, an objective function can be characterized as equation 1, which represents the sum of the squares of the distances of each data point to its assigned cluster(vector \u00b5k). So the final goal is to find values for the rn,k and \u00b5k in such a way that F is minimized [30].\nF =\nN\u2211\nn=1\nK\u2211\nk=1\nrn,k \u00d7 (||Xn \u2212 \u00b5k|| 2) (1)"}, {"heading": "3 An overview of Big Bang-Big Crunch optimization algorithm", "text": "There exist many theories about how the universe evolved at the first place, and the two famous theories in this regard are namely Big bang and Big crunch, BBBC theories. Erol and Eksin [24] made use of these theories and introduced the BB-BC optimization algorithm. According to this theory, due to dissipation, Big\nBang phase creates randomness along with disorder, while in the Big Crunch phase the randomly created particles will be drawn into an order. Big Bang-Big Crunch algorithm (BB-BC) starts with the big bang phase through the generation of random points around an initially chosen point and it tries to shrink the created points into a single optimized one through the center of mass in the big crunch phase. Finally, after repeating the two phases for a limited number of times, the algorithm converges to an ideal solution.\nSimilar to other evolutionary algorithms [31], this method has a candidate solution where some new particles are randomly distributed around it based on a uniform manner throughout the search space. The random nature of the Big Bang is associated with the energy dissipation or transmission from an ordered state to a disordered state i.e. transmission from a candidate solution to a set of new particles(solution candidates).\nThe Big Bang phase is pursued by the Big Crunch phase. In this phase the new random distributed particles are drawn into an order via the center of mass. After a sequentially repetitions of Big Bang and Big Crunch steps, the distribution of randomness during Big Bang phase becomes more and more smaller and finally the algorithm converges to a solution. The process of calculating the center of mass is according to equation (2).\nx c j =\n\u2211N j=1 xi j fi\u2211N j=1 1 fi , for i = 1, 2, . . . , N (2)\nwhere xcj is the j-th component of the center of mass, x i j is the j-th component of i-th candidate, f i is fitness value of the i-th candidate, and finally N is the number of all candidates. It should be noted that in the optimization problems, fitness(f) of each candidate solution is calculated based on general fitness function of the optimization problem, (specially for clustering applications the equation (4) is used in this paper). The algorithm then generates new population of particles according to equation (3).\nx i,new j = x c j + r \u00d7\n(xmaxj \u2212 x min j )\n1 + k (3)\nwhere xi,newj is the new value of j-th component of the i-th particle x, r is a random number with a standard normal distribution, and k is the iteration index. Also xmaxj and x min j are maximum and minimum acceptable values for xj . Algorithm 1 shows the pseudo code of the Big Bang-Big Crunch Algorithm."}, {"heading": "4 The proposed algorithm", "text": "In this section first we introduce the proposed algorithm and describe the important elements of the algorithm in Subsection 4.1. Then sensitivity analysis of the proposed method is investigated through benchmark functions in Subsection 4.2.\nAlgorithm 1 Big Bang Big Crunch Algorithm\nInput: fitness function, number of stars Output: output of optimization problem 1: Initialisation: 2: starting point = Generate a random starting point with respect to range constraints. 3: num of stars = number of stars 4: dim = dimension of solution 5: repeat 6: Big Bang Phase: \u22b2 create mass around starting point 7: for i = 1 to num of stars do 8: for j = 1 to dim do 9: mass[i, j] = generate a star based on (3) 10: end for 11: end for 12: Big Crunch Phase: 13: c.o.m = calculate center of mass based on (2) 14: starting point = c.o.m \u22b2 update 15: until max number of iterations or convergence\n4.1 The description of proposed method\nExploration and exploitation are two important components of evolutionary algorithms. In order to act successfully, each search algorithm needs to provide a good trade-off between these two factors. Exploration is the process of searching new solution regions of the search space, exploitation on the other hand is to search in the neighborhood of previously found solutions. As an example of exploration in the BB-BC algorithm equation (3) seeks to search in the new solution regions by randomly dispatching points in solution space. It can be observed from the cycles of the BB-BC algorithm, that it greedily drops the current center of mass in favor of a better one at the end of each big bang and big crunch cycle.\nAlthough the BB-BC algorithm explores the solution space greatly, it suffers from lack of proper and effective exploitation. Because of the total exploration of the search space to compute the center of masses in each iteration, the efficiency of the algorithm is sensitive to these points in each step. Moreover, it is more likely to have some local solutions in the previously computed center of masses through the process of the algorithm. If we can use these points in an efficient manner, they could yield us to more robust and better results versus the original BB-BC algorithm.\nTo make use of earlier found centers of masses and enhancing the exploitation of the algorithm, a memory with limited size is added to the process of the algorithm in an smart vein to propose a new approach entitled as Memory Enriched Big Bang Big Crunch, ME-BB-BC. We describe the stages of the ME-BB-BC and its procedure in the following paragraph.\nAt the end of each big bang and big crunch cycles, the calculated center of mass will be stored in the memory. Initially it is assumed that all of the saved centers of masses in the memory are good points for generating the particles forming the new center of masses. Furthermore, if the memory gets full during the algorithm, the worst solution will be substituted by the new center of mass based on the fitness of the currently saved solutions.\nWe enhance the particle generation based on a probabilistic random walk manner in such a way that the adjustable parameter \u03b1 is considered as the selection\nAlgorithm 2 Memory Enriched BB-BC\nInput: fitness function, memory size, number of stars Output: output of optimization problem 1: Initialization: 2: starting point = Generate a random starting point with respect to range constraints. 3: solution memory = memory with size memory size 4: num of stars = number of stars 5: dim = dimension of solution 6: \u03b1 = 0.1 \u22b2 memory selection rate 7: repeat 8: Big Bang Phase: \u22b2 create mass around starting point 9: for i = 1 to num of stars do 10: for j = 1 to dim do 11: if rand(0, 1) <= \u03b1 then 12: idx = rand([1, ...,memory size]) 13: mass[i, j] = solution memory[idx, j] \u22b2 select from memory 14: else 15: mass[i, j] = generate a star based on (3) 16: end if 17: end for 18: end for 19: Big Crunch Phase: 20: c.o.m = calculate center of mass based on (2) 21: if solution memory is not full then 22: add c.o.m into solution memory 23: else 24: worse = find worse solution in solution memory 25: if fitness(c.o.m) > fitness(worse) then 26: remove worse from the solution memory 27: add c.o.m into solution memory 28: end if 29: end if 30: starting point = c.o.m \u22b2 update 31: \u03b1 = \u03b1+ 0.01 \u00d7 \u03b1 \u22b2 update 32: until max number of iterations or convergence\nprobability of the solutions in the memory and 1 \u2212 \u03b1 is the probability for the total search space. Hence, the good aspects of the dimensions of the points in the memory are utilized in the proposed method. Moreover, the weight probabilities are linearly increased as algorithm goes by to consider more importance on the memory points. This exploitation refinement idea is similar to the decreasing values of pitch adjustment rate in harmony search algorithm [32] and inertia weight in PSO algorithm [33]. Such strategy results in better performance of the metaheuristic algorithms, due to the fact of more exploration at beginning and more exploitative at the end in the search space of the algorithm [31].\nThe details of the proposed algorithm is presented in Algorithm 2. It describes the modified big bang phase and big crunch phases of our method. The algorithm initially depends on the max number of iterations, \u03b1, memory size, and number of stars, lines 1-6 initialize these parameters. Lines 8-18 are devoted to the modified Big Bang phase of the proposed algorithm in which random particles around starting point are generated based on (3) or selected from the solution memory, in lines 19-20 center of mass is calculated according to (2). Furthermore, in lines 21-29 the proposed algorithm checks the solution memory and saves the new center of mass\nto memory. At the end of the algorithm, lines 30-31 update the parameters of the proposed method.\nBased on the pseudo code of the ME-BB-BC, the computational complexity of the algorithm is O(r\u00d7 (n+ s)), where r is the total iterations of the algorithm, n is the number of stars, and s is the complexity of finding the worst solution in the memory. Since the dim (the dimension of optimization problem) is highly less than the r and s parameters, it is skipped in the complexity analysis of the algorithm. While the computational complexity of the proposed approach is slightly more than the traditional BB-BC method, O(r \u00d7 n), the proposed method yields to significant improvement versus the earlier one.\n4.2 Analysis of the proposed algorithm\nWe initially investigate the sensitivity of the ME-BB-BC algorithm to its adjustable parameter \u03b1. Then, the rate of convergence of the algorithm and its behavior on dealing with this major issue are discussed aligned with the other well-known meta-heuristic methods through several benchmark functions.\nThe adjustable parameters of the proposed approach are num of stars, determining how vast the exploration factor should be considered and \u03b1, the balancing probability between exploration and exploitation stages of the algorithm. On the one hand, more increase the value of num of stars leads more exploration the space of solutions and achieving to probably more optimal solutions. On the other hand, more exploration of the solution space affects heavily on the time complexity of the algorithm. We have applied the value of the parameter num of stars to 200 based on empirical experiments and recommended settings in related works based on the trade of between the exploration of solution space and time complexity of the ME-BB-BC method.\nThe parameter \u03b1 is initiated with small values in the beginning stages of the algorithm and is grown to larger amounts at the subsequent stages of the algorithm\nto balance between exploration and exploitation of the search strategy. Moreover, we have designed a careful sensitivity analysis over adjustable parameter \u03b1 based on Levy and Rastrigin benchmark functions. The details of the results could be observed in Figure 1. Increasing the parameter \u03b1 generally results in better fitness values which is satisfied for both of Levy and Rastrigin functions with the 50 assumed dimension as observed in Sub-Figures 1a and 1b.\nOne of the general problems with the evolutionary algorithms is the slow rate of convergence. The ME-BB-BC algorithm contains a solution memory, and adjustable parameters \u03b1 and num of stars to balance the rate of exploration and exploitation via using the solution memory where the rate of convergence of the algorithm depends on these two stages. We have compared the convergence of the proposed approach with the PSO, Grey Wolf Optimizer GWO [34], BB-BC techniques in Figure 2 based on the number of iterations versus the value of fitness function through four benchmark functions, Rastrigin, Sphere, Levy, and Step. The obtained results in Sub-Figures 2a, 2b, 2c and 2d indicate that the ME-BB-BC algorithm has better rate of convergence than the other algorithms and the decreasing pattern of the ME-BB-BC approach proves its less sensitivity to local optimums."}, {"heading": "5 The evaluation of the proposed approach", "text": "To evaluate the ME-BB-BC, initially, a bunch of benchmark functions is used. Table 1 shows the benchmark functions where the optimal solution for all of them is zero. Furthermore, to more investigate the proposed algorithm and check the robustness of its results, statistical tests are performed on the obtained results.\nThe proposed algorithm is compared to Genetic Algorithm(GA),Particle Swarm Optimization(PSO), Grey Wolf Optimizer(GWO), and original BB-BC algorithm. Each algorithm has been run 50 times for each benchmark function and average, best and standard deviation of costs has been reported. In this study, the dimension of the benchmark functions is set to 50 and the maximum number of iterations is 100. Table 2 shows the experimental results. The ME-BB-BC has performed the optimization task efficiently as compared to the other methods based on the cost function evaluation metrics. Moreover, lower standard deviation pointed us that the optimization algorithm converges to close results in different runs of the algorithm.\nThe nonparametric Friedman\u2019s statistical test is used to the 30 different runs of the proposed algorithm on Rosenbrock, Rastrigin, Sphere, and Step benchmark function with 50 dimension to investigate whether there exist significant differences among results or not. Table 3 presents output from Friedman\u2019s test. Application of Friedman\u2019s test indicates that there is no statistically significant difference between the obtained results over the different runs of the proposed algorithm.\nTo further investigate the results of the proposed algorithm, t-test was applied in order to point out the difference between results of the ME-BB-BC and GWO, as best algorithm among other heuristic methods, Table 4 presents the t-test com-\npare of ME-BB-BC algorithm versus GWO algorithm over Rosenbrock, Rastrigin, Sphere, and Step benchmark functions. These results indicate that there is a significance difference between results of the ME-BB-BC and GWO algorithm.\nTable 3: Results of the non-parametric Friedman\u2019s statistical test\nBenchmark Function Significance\nBenchmark Function p-value\nRosenbrock 0.042 Rastrigin 0.031 Sphere 0.015 Step 0.013"}, {"heading": "6 Clustering using ME-BB-BC", "text": "Due to the finding the interesting patterns from the dataset in the clustering situations, the meta-heuristic techniques are used extensively to uncover the hidden clusters from the dataset. Some advantages of these algorithms to perform clustering task can be mentioned such as less sensitivity to the initial starting points, reduction of the amount of computation, and discover clusters with arbitrary shapes [35].\nIn this paper memory enriched big bang-big crunch algorithm proposed for clustering task. Besides, to improve the traditional clustering algorithms like kmeans method, the proposed approach performs the clustering task in an efficient way. The proposed clustering algorithm operates in a four main steps that each will be illustrated in the following.\n1. Starting Point: The proposed algorithm starts with an initial answer as a starting point of the procedure. This answer consists of a vector of centers generated randomly in the range of allowable values. The example of a candidate solution for a clustering problem is given in Figure 3.\nIf the stopping criteria of the algorithm are satisfied including reaching the maximum number of iterations, or fixed centers during consecutive iterations, the proposed algorithm will stop and the best solution among all solutions is reported. Otherwise, above steps will be repeated until the stopping conditions are satisfied. The details are presented in Algorithm 3.\nAlgorithm 3 The proposed clustering method based on memory enriched BB-BC Algorithm\nInput: fitness function, memory size, number of stars, number of clusters Output: optimal cluster centers 1: Initialisation: 2: solution memory = memory with size memory size 3: dim = dimension of clusters 4: k = number of clusters 5: centers = (c11, . . . , c dim 1 , . . . , c 1 k , . . . , cdim k ) \u22b2 randomly generate clusters centers with\nrespect to variable range limitations 6: \u03b1 = 0.1 \u22b2 memory selection rate 7: num of stars = number of stars 8: repeat 9: Big Bang Phase: \u22b2 create mass around starting point 10: for i = 1 to num of stars do 11: for j = 1 to dim do 12: if rand(0, 1) <= \u03b1 then 13: idx = rand([1, ...,memory size]) 14: mass[i, j] = solution memory[idx, j] \u22b2 select from memory 15: else 16: mass[i, j] = generate a star based on equ. 3 17: end if 18: end for 19: end for 20: for each star \u2208 mass do 21: mass fitness[star] = fitness(star) based on equ. 1 22: end for 23: Big Crunch Phase: 24: c.o.m = calculate center of mass (equ. 2) 25: if solution memory is not full then 26: add c.o.m into solution memory 27: else 28: worse = find the worst solution in solution memory 29: if fitness(c.o.m) > fitness(worse) then 30: remove worse from the solution memory 31: add c.o.m into solution memory 32: end if 33: end if 34: centers = c.o.m \u22b2 update 35: \u03b1 = \u03b1+ 0.01 \u00d7 \u03b1 \u22b2 update 36: until max number of iterations or convergence"}, {"heading": "7 Experimental results", "text": "The proposed algorithm on several standard datasets is examined where the description of the dataset are given in the Subsection 7.1. Then the obtained results are presented and discussed in Subsection 7.2.\n7.1 Standard datasets\nThe proposed method is experimentally evaluated using several standard datasets, including Iris, Wine, CMC, and Vowel. These datasets have been employed in many works and can be achieved from UCI Machine Learning Repository [36].\nThe properties of the datasets are summarized in Table 5. The detail for each of these datasets is as follows.\nTable 5: summarized characteristic of real datasets\nDataset #Objects #Features #Clusters\nIris 150 4 3(50, 50, 50) Wine 178 13 3(59, 71, 48) CMC 1473 9 3(629, 334, 510) Vowel 871 3 6(72, 89, 172, 151, 207, 180) Glass 214 9 6(70, 76, 17, 13, 9, 29) Cancer 638 9 2(444, 239)\n\u2013 Iris: This dataset contains 150 instances of iris plants, with 4 attributes and is divided to 3 categories each cluster contains 50 objects. \u2013 Wine: This data is the consequences of investigation of wines developed in the same area in Italy yet got from three unique cultivars. The analysis determines the amounts of 13 constituents found in each of the 3 sorts of wines. This dataset contains investigations of 178 instances. \u2013 CMC: This dataset is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. The instances are married women who were either not pregnant or don\u2019t know whether they were or not. The issue is to foresee the present preventative technique decision of a woman considering her demographic and financial attributes. This dataset contains 1473 items with 9 attributes and 3 clusters. \u2013 Vowel: The Vowel dataset comprises of 871 Indian Telugu vowel sounds. There are 6 overlapping classes and 3 features. Vowel dataset contains samples with low, medium and high dimensions. \u2013 Glass: Glass dataset has 214 instances with 9 features. The dataset has 6 unique clusters of different sort of windows. \u2013 Cancer: Wisconsin Breast Cancer Dataset has 683 samples with 9 traits. malignant and benign are two clusters which instances of this dataset falls into.\n7.2 Results\nThe proposed algorithm is compared to the Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Grey Wolf Optimizer (GWO), original Big Bang-Big Crunch algorithm (BB-BC) and k-means algorithm. It should be noted that all algorithms have been implemented in MATLAB and their parameters are being set according to their reference paper. Each algorithm has been run for 50 consecutive times on system with Windows 7, 4 Gigabyte of RAM and core i5 2.66 GHz processor. Table 6 presents best, average and standard deviation of different runs of applying these algorithms for given datasets. According to equation 1, the lowest the value of fitness function is, the better the clustering quality will be. By all it means the lower value for sum of cluster distance is more desirable. Lower standard\ndeviation indicates that the optimization algorithm converges to close results in different runs of the algorithm. The experimental results have shown us better performance of the ME-BB-BC algorithm as compared to the other methods.\nFurthermore, to statistically investigate the obtained results and analyze the robustness of results of the proposed algorithm over different runs of the algorithm, non-parametric Friedman\u2019s test is applied to the 30 different runs of the proposed clustering approach on clustering datasets. Table ?? presents output of Friedman\u2019s test. Application of Friedman\u2019s test indicates that there is no statistically significant difference between the obtained results over the different runs of the proposed clustering algorithm.\nAlso the k-means algorithm can be combined with meta-heuristic algorithms for clustering applications. In this regard, we have designed a hybrid clustering\nalgorithm based on a slight change in the ME-BB-BC algorithm aligned with kmeans named as kMEBB where the step 3 of the original algorithm in Section 6 is modified by applying the typical k-means procedure before the final evaluation stage for further improvement of the generated solutions of the Big Bang phase.\nWe compare the performance of the ME-BB-BC algorithm with the hybrid kMEBB algorithm on the datasets in Table 5. Based on the obtained results in Table 8, ME-BB-BC performs equally or better than kMEBB on these datasets. These results reassured us that the ME-BB-BC can be regarded as a proper choice for clustering applications among other meta-heuristic techniques."}, {"heading": "8 Conclusion and future work", "text": "In this paper, via a memory enriched approach aligned to the classical BB-BC algorithm, we are succeeded to enhance significantly its efficiency versus other standard meta-heuristic approaches for benchmark optimization functions and also clustering applications. Not only the deficiencies of the BB-BC method were alleviated through the smart way of using the memory of previously created solutions, but also these solutions were combined with newly candidate ones in a probabilistic random walk manner to improve the exploitation and exploration of the proposed method. Furthermore, this algorithm has been applied for clustering aims. To evaluate the performance of the proposed algorithm, the experimental results were compared with other similar data clustering algorithms. Implementation results on benchmark functions and clustering datasets showed us the superiority of the proposed algorithm over other algorithms. There exist different directions to extend the proposed evolutionary algorithm for future works as the following.\n\u2013 A hybrid clustering algorithm of big bang-big crunch and k-means which improves shortcomings of the k-means method. \u2013 The investigation of the proposed method aligned with statistical model based clustering algorithms. The learning parameters of the model based clustering paradigm are usually approximated through an iterative procedure named as Expectation Maximization (EM ) algorithm [1]. While EM suffers from slow rate convergence, the ME-BB-BC approach would alleviate this major problem as a future work on this field. \u2013 The proposed algorithm can also be used for multi objective optimization. \u2013 Application of ME-BB-BC in technical settings such as power dispatch systems."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for providing helpful comments and recommendations which improves the paper significantly."}], "references": [{"title": "Machine Learning: A Probabilistic Perspective", "author": ["K. Murphy"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Data clustering: 50 years beyond k-means,", "author": ["A.K. Jain"], "venue": "Pattern recognition letters,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Cluster identification and separation in the growing self-organizing map: application in protein sequence classification,", "author": ["N. Ahmad", "D. Alahakoon", "R. Chau"], "venue": "Neural Computing and Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Adaptive k-means clustering algorithm for mr breast image segmentation,", "author": ["H.M. Moftah", "A.T. Azar", "E.T. Al-Shammari", "N.I. Ghali", "A.E. Hassanien", "M. Shoman"], "venue": "Neural Computing and Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1917}, {"title": "A hybrid particle swarm optimization for feature subset selection by integrating a novel local search strategy,", "author": ["P. Moradi", "M. Gholampour"], "venue": "Applied Soft Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Pso-based k-means clustering with enhanced cluster matching for gene expression data,", "author": ["Y.-K. Lam", "P.W.-M. Tsang", "C.-S. Leung"], "venue": "Neural Computing and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Clustering based on median and closest string via rank distance with applications on dna,", "author": ["L.P. Dinu", "R.T. Ionescu"], "venue": "Neural Computing and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Algorithmic complexity: three np-hard problems in computational statistics,", "author": ["W.J. Welch"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Al-Sultan, \u201cA tabu search approach to the clustering problem,", "author": ["S. K"], "venue": "Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Genetic k-means algorithm,\u201d Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["K. Krishna", "M.N. Murty"], "venue": "IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "An ant colony approach for clustering,", "author": ["P. Shelokar", "V.K. Jayaraman", "B.D. Kulkarni"], "venue": "Analytica Chimica Acta,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A particle swarm optimization approach to clustering,", "author": ["T. Cura"], "venue": "Expert Systems with Applications,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "An artificial bee colony approach for clustering,", "author": ["C. Zhang", "D. Ouyang", "J. Ning"], "venue": "Expert Systems with Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A hybridized approach to data clustering,", "author": ["Y.-T. Kao", "E. Zahara", "I.-W. Kao"], "venue": "Expert Systems with Applications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Jordehi, \u201cEnhanced leader pso (elpso): A new pso variant for solving global optimisation problems,", "author": ["R. A"], "venue": "Applied Soft Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Enhanced leader pso (elpso): a new algorithm for allocating distributed tcscs in power systems,", "author": ["A.R. Jordehi", "J. Jasni", "N.A. Wahab", "M. Kadir", "M. Javadi"], "venue": "International Journal of Electrical Power & Energy Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A new particle swarm optimization algorithm with adaptive inertia weight based on bayesian techniques,", "author": ["L. Zhang", "Y. Tang", "C. Hua", "X. Guan"], "venue": "Applied Soft Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Akhlaghian, \u201cAn unsupervised feature selection algorithm based on ant colony optimization,", "author": ["S. Tabakhi", "P. Moradi"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Brainstorm optimisation algorithm (bsoa): An efficient algorithm for finding optimal location and setting of facts devices in electric power systems,", "author": ["A.R. Jordehi"], "venue": "International Journal of Electrical Power & Energy Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Seeker optimisation (human group optimisation) algorithm with chaos,", "author": ["A.R. Jordehi"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Jordehi, \u201cAn efficient chaotic water cycle algorithm for optimization tasks,", "author": ["A.A. Heidari", "R.A. Abbaspour", "A. R"], "venue": "Neural Computing and Applications,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Jordehi, \u201cA chaotic artificial immune system optimisation algorithm for solving global continuous optimisation problems,", "author": ["R. A"], "venue": "Neural Computing and Applications,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "A new optimization method: big bang\u2013big crunch,", "author": ["O.K. Erol", "I. Eksin"], "venue": "Advances in Engineering Software,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Tlbo based voltage stable environment friendly economic dispatch considering real and reactive power constraints,", "author": ["H. Verma", "P. Mafidar"], "venue": "Journal of The Institution of Engineers (India): Series B,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Preventive and corrective control applications in power systems via big bang\u2013big crunch optimization,", "author": ["C.F. Kucuktezcan", "V.I. Genc"], "venue": "International Journal of Electrical Power & Energy Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Big bang-big crunch optimization for parameter estimation in structural systems,", "author": ["H. Tang", "J. Zhou", "S. Xue", "L. Xie"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Jordehi, \u201cA chaotic-based big bang\u2013big crunch algorithm for solving global optimisation problems,", "author": ["R. A"], "venue": "Neural Computing and Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Innovative Computational Intelligence: A Rough Guide to 134 Clever Algorithms", "author": ["B. Xing", "W.-J. Gao"], "venue": "New York, NY: Springer,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Empirical study of particle swarm optimization,", "author": ["Y. Shi", "R.C. Eberhart"], "venue": "Proceedings of the 1999 Congress on Evolutionary Computation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "A survey on nature inspired metaheuristic algorithms for partitional clustering,", "author": ["S.J. Nanda", "G. Panda"], "venue": "Swarm and Evolutionary Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "UCI machine learning repository.", "author": ["M. Lichman"], "venue": "http://archive.ics.uci.edu/ml,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Unsupervised learning methods can be generally divided into clustering, dimensionality reduction, image segmentation, object recognition, and text mining techniques [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "Clustering is the task of assigning a set of objects, usually vectors in a multidimensional space, into clusters in such a way that the objects in the same cluster are more similar to each other than to those in other clusters [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 2, "context": "Cluster analysis has attracted attention of many researchers in different fields, including data mining [3], sequence mining [4], image processing [5], feature selection techniques [6], spatial data analysis [3], bioinformatics [7,8], marketing [3], city planning [3], and earthquake studies [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "Cluster analysis has attracted attention of many researchers in different fields, including data mining [3], sequence mining [4], image processing [5], feature selection techniques [6], spatial data analysis [3], bioinformatics [7,8], marketing [3], city planning [3], and earthquake studies [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "Cluster analysis has attracted attention of many researchers in different fields, including data mining [3], sequence mining [4], image processing [5], feature selection techniques [6], spatial data analysis [3], bioinformatics [7,8], marketing [3], city planning [3], and earthquake studies [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "Cluster analysis has attracted attention of many researchers in different fields, including data mining [3], sequence mining [4], image processing [5], feature selection techniques [6], spatial data analysis [3], bioinformatics [7,8], marketing [3], city planning [3], and earthquake studies [3].", "startOffset": 228, "endOffset": 233}, {"referenceID": 6, "context": "Cluster analysis has attracted attention of many researchers in different fields, including data mining [3], sequence mining [4], image processing [5], feature selection techniques [6], spatial data analysis [3], bioinformatics [7,8], marketing [3], city planning [3], and earthquake studies [3].", "startOffset": 228, "endOffset": 233}, {"referenceID": 7, "context": "Since clustering problem is NP-hard by its nature [9], meta-heuristic methods are proper tools to deal with this issue.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Among many studies in metaheuristic approaches for clustering, one can mention clustering algorithm based on tabu search [10], genetic algorithms [11], ant colony [12], particle swarm optimization [13], and bee colony [14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "Among many studies in metaheuristic approaches for clustering, one can mention clustering algorithm based on tabu search [10], genetic algorithms [11], ant colony [12], particle swarm optimization [13], and bee colony [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Among many studies in metaheuristic approaches for clustering, one can mention clustering algorithm based on tabu search [10], genetic algorithms [11], ant colony [12], particle swarm optimization [13], and bee colony [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "Among many studies in metaheuristic approaches for clustering, one can mention clustering algorithm based on tabu search [10], genetic algorithms [11], ant colony [12], particle swarm optimization [13], and bee colony [14].", "startOffset": 197, "endOffset": 201}, {"referenceID": 12, "context": "Among many studies in metaheuristic approaches for clustering, one can mention clustering algorithm based on tabu search [10], genetic algorithms [11], ant colony [12], particle swarm optimization [13], and bee colony [14].", "startOffset": 218, "endOffset": 222}, {"referenceID": 13, "context": "One common approach which can easily be used along with meta-heuristic techniques is k-means algorithm [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 233, "endOffset": 244}, {"referenceID": 15, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 233, "endOffset": 244}, {"referenceID": 16, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 233, "endOffset": 244}, {"referenceID": 17, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 296, "endOffset": 300}, {"referenceID": 18, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 356, "endOffset": 363}, {"referenceID": 19, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 356, "endOffset": 363}, {"referenceID": 20, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 394, "endOffset": 398}, {"referenceID": 21, "context": "There exists many nature inspired algorithms for solving complex optimization problems which are used extensively in research works and technological settings including a variety of particle swarm optimization, PSO, based approaches [16, 17,18], ant colony based method for unsupervised learning [19], algorithms which have emerged from human interactions [20,21], water cycle chaotic behavior [22], and human body systems [23].", "startOffset": 423, "endOffset": 427}, {"referenceID": 22, "context": "A novel optimization algorithm named Big Bang-Big Crunch algorithm(BB-BC) based on these theories is first initiated in [24] which have been applied in many works including economic power systems [25,26] and signal processing [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "A novel optimization algorithm named Big Bang-Big Crunch algorithm(BB-BC) based on these theories is first initiated in [24] which have been applied in many works including economic power systems [25,26] and signal processing [27].", "startOffset": 196, "endOffset": 203}, {"referenceID": 24, "context": "A novel optimization algorithm named Big Bang-Big Crunch algorithm(BB-BC) based on these theories is first initiated in [24] which have been applied in many works including economic power systems [25,26] and signal processing [27].", "startOffset": 196, "endOffset": 203}, {"referenceID": 25, "context": "A novel optimization algorithm named Big Bang-Big Crunch algorithm(BB-BC) based on these theories is first initiated in [24] which have been applied in many works including economic power systems [25,26] and signal processing [27].", "startOffset": 226, "endOffset": 230}, {"referenceID": 26, "context": "While the BB-BC are used in several works, it suffers from disadvantages such as slow convergence speed and trapping in local optimum solutions available in most of the optimization problems [28].", "startOffset": 191, "endOffset": 195}, {"referenceID": 22, "context": "Erol and Eksin [24] made use of these theories and introduced the BB-BC optimization algorithm.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "Similar to other evolutionary algorithms [31], this method has a candidate solution where some new particles are randomly distributed around it based on a uniform manner throughout the search space.", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "This exploitation refinement idea is similar to the decreasing values of pitch adjustment rate in harmony search algorithm [32] and inertia weight in PSO algorithm [33].", "startOffset": 164, "endOffset": 168}, {"referenceID": 27, "context": "Such strategy results in better performance of the metaheuristic algorithms, due to the fact of more exploration at beginning and more exploitative at the end in the search space of the algorithm [31].", "startOffset": 196, "endOffset": 200}, {"referenceID": 8, "context": "5ixi) 4 [-5, 10] [0, .", "startOffset": 8, "endOffset": 16}, {"referenceID": 8, "context": "i=2 i(2x2i \u2212 xi\u22121) 2 [-10, 10] [0, .", "startOffset": 21, "endOffset": 30}, {"referenceID": 29, "context": "Some advantages of these algorithms to perform clustering task can be mentioned such as less sensitivity to the initial starting points, reduction of the amount of computation, and discover clusters with arbitrary shapes [35].", "startOffset": 221, "endOffset": 225}, {"referenceID": 30, "context": "These datasets have been employed in many works and can be achieved from UCI Machine Learning Repository [36].", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "The learning parameters of the model based clustering paradigm are usually approximated through an iterative procedure named as Expectation Maximization (EM ) algorithm [1].", "startOffset": 169, "endOffset": 172}], "year": 2017, "abstractText": "Cluster analysis plays an important role in decision making process for many knowledge-based systems. There exist a wide variety of different approaches for clustering applications including the heuristic techniques, probabilistic models, and traditional hierarchical algorithms. In this paper, a novel heuristic approach based on big bang-big crunch algorithm is proposed for clustering problems. The proposed method not only takes advantage of heuristic nature to alleviate typical clustering algorithms such as k-means, but it also benefits from the memory based scheme as compared to its similar heuristic techniques. Furthermore, the performance of the proposed algorithm is investigated based on several benchmark test functions as well as on the well-known datasets. The experimental results shows the significant superiority of the proposed method over the similar algorithms.", "creator": "LaTeX with hyperref package"}}}