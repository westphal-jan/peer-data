{"id": "1703.03208", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Compressed Sensing using Generative Models", "abstract": "the goal of compressed sensing is to then estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors constrained in the relevant domain. for almost all results in almost this literature, the structure is represented by sparsity in offering a well - chosen basis. we commonly show how to achieve guarantees similar to highly standard compressed sensing but without employing sparsity at precisely all. instead, we strictly suppose that vectors may lie nearly near also the range domain of a generative gamma model $ g : \\ mathbb { r } ^ k \\ to \\ mathbb { r } ^ n $. our main theorem is that, if $ g $ is $ l $ - 99 lipschitz, then roughly $ o ( k \\ log l ) $ random error gaussian gamma measurements suffice for an $ \\ ell _ us 2 / \\ ell _ 2 $ recovery guarantee. we demonstrate measuring our results using generative models from published variational autoencoder and multiple generative adversarial networks. our method can use $ 5 $ - $ 10 $ x fewer vector measurements each than another lasso for the same accuracy.", "histories": [["v1", "Thu, 9 Mar 2017 10:11:03 GMT  (1604kb,D)", "http://arxiv.org/abs/1703.03208v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["ashish bora", "ajil jalal", "eric price", "alexandros g dimakis"], "accepted": true, "id": "1703.03208"}, "pdf": {"name": "1703.03208.pdf", "metadata": {"source": "CRF", "title": "Compressed Sensing using Generative Models", "authors": ["Ashish Bora", "Ajil Jalal", "Eric Price", "Alexandros G. Dimakis"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Compressive or compressed sensing is the problem of reconstructing an unknown vector x\u2217 \u2208 Rn after observing m < n linear measurements of its entries, possibly with added noise:\ny = Ax\u2217 + \u03b7,\nwhere A \u2208 Rm\u00d7n is called the measurement matrix and \u03b7 \u2208 Rm is noise. Even without noise, this is an underdetermined system of linear equations, so recovery is impossible unless we make an assumption on the structure of the unknown vector x\u2217. We need to assume that the unknown vector is \u201cnatural,\u201d or \u201csimple,\u201d in some applicationdependent way.\nThe most common structural assumption is that the vector x\u2217 is k-sparse in some known basis (or approximately k-sparse). Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x\u2217 if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) [35, 7, 14, 6]. The problem is also called high-dimensional sparse linear regression and there is vast literature on establishing conditions for different recovery algorithms, different assumptions on the design of A and generalizations of RIP and REC for other structures, see e.g. [6, 33, 1, 30, 3].\nThis significant interest is justified since a large number of applications can be expressed as recovering an unknown vector from noisy linear measurements. For example, many tomography problems can be expressed in this framework: x\u2217 is the unknown true tomographic image and the linear measurements are obtained by x-ray or other physical sensing system that produces sums or more general linear projections of the unknown pixels. Compressed sensing has been studied extensively for medical applications including computed tomography (CT) [8], rapid MRI [31] and neuronal spike train recovery [21]. Another impressive application is the \u201csingle pixel camera\u201d [15], where digital micro-mirrors provide linear combinations to a single pixel sensor that then uses compressed sensing reconstruction algorithms to reconstruct an image. These results have been extended by combining sparsity with additional structural assumptions [4, 22], and by generalizations such as translating sparse vectors into low-rank matrices [33, 3, 17]. These \u2217University of Texas at Austin, Department of Computer Science, email: ashish.bora@utexas.edu \u2020University of Texas at Austin, Department of Electrical and Computer Engineering, email: ajiljalal@utexas.edu \u2021University of Texas at Austin, Department of Computer Science, email: ecprice@cs.utexas.edu \u00a7University of Texas at Austin, Department of Electrical and Computer Engineering, email: dimakis@austin.utexas.edu\nar X\niv :1\n70 3.\n03 20\n8v 1\n[ st\nat .M\nL ]\n9 M\nar 2\n01 7\nresults can improve performance when the structural assumptions fit the sensed signals. Other works perform \u201cdictionary learning,\u201d seeking overcomplete bases where the data is more sparse (see [9] and references therein).\nIn this paper instead of relying on sparsity, we use structure from a generative model. Recently, several neural network based generative models such as variational auto-encoders (VAEs) [26] and generative adversarial networks (GANs) [19] have found success at modeling data distributions. In these models, the generative part learns a mapping from a low dimensional representation space z \u2208 Rk to the high dimensional sample space G(z) \u2208 Rn. While training, this mapping is encouraged to produce vectors that resemble the vectors in the training dataset. We can therefore use any pre-trained generator to approximately capture the notion of an vector being \u201cnatural\u201d in our domain: the generator defines a probability distribution over vectors in sample space and tries to assign higher probability to more likely vectors, for the dataset it has been trained on. We expect that vectors \u201cnatural\u201d to our domain will be close to some point in the support of this distribution, i.e., in the range of G.\nOur Contributions: We present an algorithm that uses generative models for compressed sensing. Our algorithm simply uses gradient descent to optimize the representation z \u2208 Rk such that the corresponding image G(z) has small measurement error \u2016AG(z)\u2212 y\u201622. While this is a nonconvex objective to optimize, we empirically find that gradient descent works well, and the results can significantly outperform Lasso with relatively few measurements.\nWe obtain theoretical results showing that, as long as gradient descent finds a good approximate solution to our objective, our output G(z) will be almost as close to the true x\u2217 as the closest possible point in the range of G.\nThe proof is based on a generalization of the Restricted Eigenvalue Condition (REC) that we call the Set-Restricted Eigenvalue Condition (S-REC). Our main theorem is that if a measurement matrix satisfies the S-REC for the range of a given generator G, then the measurement error minimization optimum is close to the true x\u2217. Furthermore, we show that random Gaussian measurement matrices satisfy the S-REC condition with high probability for large classes of generators. Specifically, for d-layer neural networks such as VAEs and GANs, we show that O(kd log n) Gaussian measurements suffice to guarantee good reconstruction with high probability. One result, for ReLU-based networks, is the following:\nTheorem 1.1. Let G : Rk \u2192 Rn be a generative model from a d-layer neural network using ReLU activations. Let A \u2208 Rm\u00d7n be a random Gaussian matrix for m = O(kd log n), scaled so Ai,j \u223c N(0, 1/m). For any x\u2217 \u2208 Rn and any observation y = Ax\u2217 + \u03b7, let z\u0302 minimize \u2016y \u2212 AG(z)\u20162 to within additive of the optimum. Then with 1\u2212 e\u2212\u2126(m) probability,\n\u2016G(z\u0302)\u2212 x\u2217\u20162 \u2264 6 min z\u2217\u2208Rk \u2016G(z\u2217)\u2212 x\u2217\u20162 + 3\u2016\u03b7\u20162 + 2 .\nLet us examine the terms in our error bound in more detail. The first two are the minimum possible error of any vector in the range of the generator and the norm of the noise; these are necessary for such a technique, and have direct analogs in standard compressed sensing guarantees. The third term comes from gradient descent not necessarily converging to the global optimum; empirically, does seem to converge to zero, and one can check post-observation that this is small by computing the upper bound \u2016y \u2212AG(z\u0302)\u20162.\nWhile the above is restricted to ReLU-based neural networks, we also show similar results for arbitrary L-Lipschitz generative models, for m \u2248 O(k logL). Typical neural networks have poly(n)-bounded weights in each layer, so L \u2264 nO(d), giving for all activation functions the same O(kd log n) sample complexity as for ReLU networks.\nTheorem 1.2. Let G : Rk \u2192 Rn be an L-Lipschitz function. Let A \u2208 Rm\u00d7n be a random Gaussian matrix for m = O(k log Lr\u03b4 ), scaled so Ai,j \u223c N(0, 1/m). For any x\n\u2217 \u2208 Rn and any observation y = Ax\u2217 + \u03b7, let z\u0302 minimize \u2016y \u2212AG(z)\u20162 to within additive of the optimum over vectors with \u2016z\u0302\u20162 \u2264 r. Then with 1\u2212 e\u2212\u2126(m) probability,\n\u2016G(z\u0302)\u2212 x\u2217\u20162 \u2264 6 min z\u2217\u2208Rk \u2016z\u2217\u20162\u2264r \u2016G(z\u2217)\u2212 x\u2217\u20162 + 3\u2016\u03b7\u20162 + 2 + 2\u03b4.\nThe downside is two minor technical conditions: we only optimize over representations z with \u2016z\u2016 bounded by r, and our error gains an additive \u03b4 term. Since the dependence on these parameters is log(rL/\u03b4), and L is something like nO(d), we may set r = nO(d) and \u03b4 = 1/nO(d) while only losing constant factors, making these conditions\nvery mild. In fact, generative models normally have the coordinates of z be independent uniform or Gaussian, so \u2016z\u2016 \u2248 \u221a k nd, and a constant signal-to-noise ratio would have \u2016\u03b7\u20162 \u2248 \u2016x\u2217\u2016 \u2248 \u221a n 1/nd.\nWe remark that, while these theorems are stated in terms of Gaussian matrices, the proofs only involve the distributional Johnson-Lindenstrauss property of such matrices. Hence the same results hold for matrices with subgaussian entries or fast-JL matrices [2]."}, {"heading": "2 Our Algorithm", "text": "All norms are 2-norms unless specified otherwise.\nLet x\u2217 \u2208 Rn be the vector we wish to sense. Let A \u2208 Rm\u00d7n be the measurement matrix and \u03b7 \u2208 Rm be the noise vector. We observe the measurements y = Ax\u2217 + \u03b7. Given y and A, our task is to find a reconstruction x\u0302 close to x\u2217.\nA generative model is given by a deterministic functionG : Rk \u2192 Rn, and a distribution PZ over z \u2208 Rk. To generate a sample from the generator, we can draw z \u223c PZ and the sample then is G(z). Typically, we have k n, i.e. the generative model maps from a low dimensional representation space to a high dimensional sample space.\nOur approach is to find a vector in representation space such that the corresponding vector in the sample space matches the observed measurements. We thus define the objective to be\nloss(z) = \u2016AG(z)\u2212 y\u20162 (1)\nBy using any optimization procedure, we can minimize loss(z) with respect to z. In particular, if the generative model G is differentiable, we can evaluate the gradients of the loss with respect to z using backpropagation and use standard gradient based optimizers. If the optimization procedure terminates at z\u0302, our reconstruction for x\u2217 is G(z\u0302). We define the measurement error to be \u2016AG(z\u0302)\u2212 y\u20162 and the reconstruction error to be \u2016G(z\u0302)\u2212 x\u2217\u20162."}, {"heading": "3 Related Work", "text": "Several recent lines of work explore generative models for reconstruction. The first line of work attempts to project an image on to the representation space of the generator. These works assume full knowledge of the image, and are special cases of the linear measurements framework where the measurement matrix A is identity. Excellent reconstruction results with SGD in the representation space to find an image in the generator range have been reported by [28] with stochastic clipping and [11] with logistic measurement loss. A different approach is introduced in [16] and [12]. In their method, a recognition network that maps from the sample space vector x to the representation space vector z is learned jointly with the generator in an adversarial setting.\nA second line of work explores reconstruction with structured partial observations. The inpainting problem consists of predicting the values of missing pixels given a part of the image. This is a special case of linear measurements where each measurement corresponds to an observed pixel. The use of Generative models for this task has been studied in [38], where the objective is taken to be a combination of L1 error in measurements and a perceptual loss term given by the discriminator. Super-resolution is a related task that attempts to increase the resolution of an image. We can view this problem as observing local spatial averages of the unknown higher resolution image and hence cast this as another special case of linear measurements. For prior work on super-resolution see e.g. [37, 13, 23] and references therein.\nWe also take note of the related work of [18] that connects model-based compressed sensing with the invertibility of Convolutional Neural Networks.\nA related result appears in [5], which studies the measurement complexity of an RIP condition for smooth manifolds. This is analogous to our S-REC for the range of G, but the range of G is neither smooth (because of ReLUs) nor a manifold (because of self-intersection). Their recovery result was extended in [20] to unions of two manifolds."}, {"heading": "4 Theoretical Results", "text": "We begin with a brief review of the Restricted Eigenvalue Condition (REC) in standard compressed sensing. The REC is a sufficient condition on A for robust recovery to be possible. The REC essentially requires that all \u201capproximately sparse\u201d vectors are far from the nullspace of the matrix A. More specifically, A satisfies REC for a constant \u03b3 > 0 if for all approximately sparse vectors x, \u2016Ax\u2016 \u2265 \u03b3\u2016x\u2016. (2) It can be shown that this condition is sufficient for recovery of sparse vectors using Lasso. If one examines the structure of Lasso recovery proofs, a key property that is used is that the difference of any two sparse vectors is also approximately sparse (for sparsity up to 2k). This is a coincidence that is particular to sparsity. By contrast, the difference of two vectors \u201cnatural\u201d to our domain may not itself be natural. The condition we need is that the difference of any two natural vectors is far from the nullspace of A.\nWe propose a generalized version of the REC for a set S \u2286 Rn of vectors, the Set-Restricted Eigenvalue Condition (S-REC):\nDefinition 1. Let S \u2286 Rn. For some parameters \u03b3 > 0, \u03b4 \u2265 0, a matrix A \u2208 Rm\u00d7n is said to satisfy the S-REC(S, \u03b3, \u03b4) if \u2200 x1, x2 \u2208 S,\n\u2016A(x1 \u2212 x2)\u2016 \u2265 \u03b3\u2016x1 \u2212 x2\u2016 \u2212 \u03b4.\nThere are two main differences between the S-REC and the standard REC in compressed sensing. First, the condition applies to differences of vectors in an arbitrary set S of \u201cnatural\u201d vectors, rather than just the set of approximately k-sparse vectors in some basis. This will let us apply the definition to S being the range of a generative model.\nSecond, we allow an additive slack term \u03b4. This is necessary for us to achieve the S-REC when S is the output of general Lipschitz functions. Without it, the S-REC depends on the behavior of S at arbitrarily small scales. Since there are arbitrarily many such local regions, one cannot guarantee the existence of an A that works for all these local regions. Fortunately, as we shall see, poor behavior at a small scale \u03b4 will only increase our error by O(\u03b4).\nThe S-REC definition requires that for any two vectors in S, if they are significantly different (so the right hand side is large), then the corresponding measurements should also be significantly different (left hand side). Hence we can hope to approximate the unknown vector from the measurements, if the measurement matrix satisfies the S-REC.\nBut how can we find such a matrix? To answer this, we present two lemmas showing that random Gaussian matrices of relatively few measurements m satisfy the S-REC for the outputs of large and practically useful classes of generative models G : Rk \u2192 Rn.\nIn the first lemma, we assume that the generative model G(\u00b7) is L-Lipschitz, i.e., \u2200 z1, z2 \u2208 Rk, we have\n\u2016G(z1)\u2212G(z2)\u2016 \u2264 L\u2016z1 \u2212 z2\u2016.\nNote that state of the art neural network architectures with linear layers, (transposed) convolutions, max-pooling, residual connections, and all popular non-linearities satisfy this assumption. In Lemma 8.5 in the Appendix we give a simple bound on L in terms of parameters of the network; for typical networks this is nO(d). We also require the input z to the generator to have bounded norm. Since generative models such as VAEs and GANs typically assume their input z is drawn with independent uniform or Gaussian inputs, this only prunes an exponentially unlikely fraction of the possible outputs.\nLemma 4.1. Let G : Rk \u2192 Rn be L-Lipschitz. Let\nBk(r) = {z | z \u2208 Rk, \u2016z\u2016 \u2264 r}\nbe an L2-norm ball in Rk. For \u03b1 < 1, if\nm = \u2126\n( k\n\u03b12 log\nLr\n\u03b4\n) ,\nthen a random matrixA \u2208 Rm\u00d7n with IID entries such thatAij \u223c N ( 0, 1m ) satisfies the S-REC(G(Bk(r)), 1\u2212\u03b1, \u03b4) with 1\u2212 e\u2212\u2126(\u03b12m) probability.\nAll proofs, including this one, are deferred to Appendix A.\nNote that even though we proved the lemma for an L2 ball, the same technique works for any compact set.\nFor our second lemma, we assume that the generative model is a neural network with such that each layer is a composition of a linear transformation followed by a pointwise non-linearity. Many common generative models have such architectures. We also assume that all non-linearities are piecewise linear with at most two pieces. The popular ReLU or LeakyReLU non-linearities satisfy this assumption. We do not make any other assumption, and in particular, the magnitude of the weights in the network do not affect our guarantee.\nLemma 4.2. Let G : Rk \u2192 Rn be a d-layer neural network, where each layer is a linear transformation followed by a pointwise non-linearity. Suppose there are at most c nodes per layer, and the non-linearities are piecewise linear with at most two pieces, and let\nm = \u2126\n( 1\n\u03b12 kd log c ) for some \u03b1 < 1. Then a random matrixA \u2208 Rm\u00d7n with IID entriesAij \u223c N (0, 1m ) satisfies the S-REC(G(R\nk), 1\u2212 \u03b1, 0) with 1\u2212 e\u2212\u2126(\u03b12m) probability.\nTo show Theorems 1.1 and 1.2, we just need to show that the S-REC implies good recovery. In order to make our error guarantee relative to `2 error in the image space Rn, rather than in the measurement space Rm, we also need that A preserves norms with high probability [10]. Fortunately, Gaussian matrices (or other distributional JL matrices) satisfy this property.\nLemma 4.3. Let A \u2208 Rm\u00d7n by drawn from a distribution that (1) satisfies the S-REC(S, \u03b3, \u03b4) with probability 1\u2212p and (2) has for every fixed x \u2208 Rn, \u2016Ax\u2016 \u2264 2\u2016x\u2016 with probability 1\u2212 p.\nFor any x\u2217 \u2208 Rn and noise \u03b7, let y = Ax\u2217 + \u03b7. Let x\u0302 approximately minimize \u2016y \u2212Ax\u2016 over x \u2208 S, i.e.,\n\u2016y \u2212Ax\u0302\u2016 \u2264 min x\u2208S \u2016y \u2212Ax\u2016+ .\nThen,\n\u2016x\u0302\u2212 x\u2217\u2016 \u2264 ( 4\n\u03b3 + 1 ) min x\u2208S \u2016x\u2217 \u2212 x\u2016+ 1 \u03b3 (2\u2016\u03b7\u2016+ + \u03b4)\nwith probability 1\u2212 2p.\nCombining Lemma 4.1, Lemma 4.2, and Lemma 4.3 gives Theorems 1.1 and 1.2. In our setting, S is the range of the generator, and x\u0302 in the theorem above is the reconstruction G(z\u0302) returned by our algorithm."}, {"heading": "5 Models", "text": "In this section we describe the generative models used in our experiments. We used two image datasets and two different generative model types (a VAE and a GAN). This provides some evidence that our approach can work with many types of models and datasets.\nIn our experiments, we found that it was helpful to add a regularization term L(z) to the objective to encourage the optimization to explore more in the regions that are preferred by the respective generative models (see comparison to unregularized versions in Fig. 1). Thus the objective function we use for minimization is\n\u2016AG(z)\u2212 y\u20162 + L(z).\nBoth VAE and GAN typically imposes an isotropic Gaussian prior on z. Thus \u2016z\u20162 is proportional to the negative log-likelihood under this prior. Accordingly, we use the following regularizer:\nL(z) = \u03bb\u2016z\u20162, (3)\nwhere \u03bb measures the relative importance of the prior as compared to the measurement error."}, {"heading": "5.1 MNIST with VAE", "text": "The MNIST dataset consists of about 60, 000 images of handwritten digits, where each image is of size 28\u00d7 28 [27]. Each pixel value is either 0 (background) or 1 (foreground). No pre-processing was performed. We trained VAE on this dataset. The input to the VAE is a vectorized binary image of input dimension 784. We set the size of the representation space k = 20. The recognition network is a fully connected 784 \u2212 500 \u2212 500 \u2212 20 network. The generator is also fully connected with the architecture 20 \u2212 500 \u2212 500 \u2212 784. We train the VAE using the Adam optimizer [25] with a mini-batch size 100 and a learning rate of 0.001.\nWe found that using \u03bb = 0.1 in Eqn. (3) gave the best performance, and we use this value in our experiments.\nThe digit images are reasonably sparse in the pixel space. Thus, as a baseline, we use the pixel values directly for sparse recovery using Lasso. We set shrinkage parameter to be 0.1 for all the experiments."}, {"heading": "5.2 CelebA with DCGAN", "text": "CelebA is a dataset of more than 200, 000 face images of celebrities [29]. The input images were cropped to a 64\u00d7 64 RGB image, giving 64\u00d7 64\u00d7 3 = 12288 inputs per image. Each pixel value was scaled so that all values are between [\u22121, 1]. We trained a DCGAN 1 [34, 24] on this dataset. We set the input dimension k = 100 and use a standard normal distribution. The architecture follows that of [34]. The model was trained by one update to the discriminator and two updates to the generator per cycle. Each update used the Adam optimizer [25] with minibatch size 64, learning rate 0.0002 and \u03b21 = 0.5.\nWe found that using \u03bb = 0.001 in Eqn. (3) gave the best results and thus, we use this value in our experiments.\nFor baselines, we perform sparse recovery using Lasso on the images in two domains: (a) 2D Discrete Cosine Transform (2D-DCT) and (b) 2D Daubechies-1 Wavelet Transform (2D-DB1). While the we provide Gaussian measurements of the original pixel values, the L1 penalty is on either the DCT coefficients or the DB1 coefficients of each color channel of an image. For all experiments, we set the shrinkage parameter to be 0.1 and 0.00001 respectively for 2D-DCT, and 2D-DB1.\n1Code reused from https://github.com/carpedm20/DCGAN-tensorflow"}, {"heading": "6 Experiments and Results", "text": ""}, {"heading": "6.1 Reconstruction from Gaussian measurements", "text": "We takeA to be a random matrix with IID Gaussian entries with zero mean and standard deviation of 1/m. Each entry of noise vector \u03b7 is also an IID Gaussian random variable. We compare performance of different sensing algorithms qualitatively and quantitatively. For quantitative comparison, we use the reconstruction error = \u2016x\u0302 \u2212 x\u2217\u20162, where x\u0302 is an estimate of x\u2217 returned by the algorithm. In all cases, we report the results on a held out test set, unseen by the generative model at training time."}, {"heading": "6.1.1 MNIST", "text": "The standard deviation of the noise vector is set such that \u221a\nE[\u2016\u03b7\u20162] = 0.1. We use Adam optimizer [25], with a learning rate of 0.01. We do 10 random restarts with 1000 steps per restart and pick the reconstruction with best measurement error.\nIn Fig. 1a, we show the reconstruction error as we change the number of measurements both for Lasso and our algorithm. We observe that our algorithm is able to get low errors with far fewer measurements. For example, our algorithm\u2019s performance with 25 measurements matches Lasso\u2019s performance with 400 measurements. Fig. 2a shows sample reconstructions by Lasso and our algorithm.\nHowever, our algorithm is limited since its output is constrained to be in the range of the generator. After 100 measurements, our algorithm\u2019s performance saturates, and additional measurements give no additional performance. Since Lasso has no such limitation, it eventually surpasses our algorithm, but this takes more than 500 measurements of the 784-dimensional vector. We expect that a more powerful generative model with representation dimension k > 20 can make better use of additional measurements."}, {"heading": "6.1.2 celebA", "text": "The standard deviation of entries in the noise vector is set such that \u221a E[\u2016\u03b7\u20162] = 0.01. We optimize use Adam optimizer [25], with a learning rate of 0.1. We do 2 random restarts with 500 update steps per restart and pick the reconstruction with best measurement error.\nIn Fig. 1b, we show the reconstruction error as we change the number of measurements both for Lasso and our algorithm. In Fig. 3 we show sample reconstructions by Lasso and our algorithm. We observe that our algorithm is able to produce reasonable reconstructions with as few as 500 measurements, while the output of the baseline algorithms is quite blurry. Similar to the results on MNIST, if we continue to give more measurements, our algorithm saturates, and for more than 5000 measurements, Lasso gets a better reconstruction. We again expect that a more powerful generative model with k > 100 would perform better in the high-measurement regime."}, {"heading": "6.2 Super-resolution", "text": "Super-resolution is the task of constructing a high resolution image from a low resolution version of the same image. This problem can be thought of as special case of our general framework of linear measurements, where the measurements correspond to local spatial averages of the pixel values. Thus, we try to use our recovery algorithm to perform this task with measurement matrix A tailored to give only the relevant observations. We note that this measurement matrix may not satisfy the S-REC condition (with good constants \u03b3 and \u03b4), and consequently, our theorems may not be applicable."}, {"heading": "6.2.1 MNIST", "text": "We construct a low resolution image by spatial 2 \u00d7 2 pooling with a stride of 2 to produce a 14 \u00d7 14 image. These measurements are used to reconstruct the original 28 \u00d7 28 image. Fig. 2b shows reconstructions produced by our algorithm on images from a held out test set. We observe sharp reconstructions which closely match the fine structure in the ground truth."}, {"heading": "6.2.2 celebA", "text": "We construct a low resolution image by spatial 4 \u00d7 4 pooling with a stride of 4 to produce a 16 \u00d7 16 image. These measurements are used to reconstruct the original 64\u00d7 64 image. In Fig. 4 we show results on images from a held out test set. We see that our algorithm is able to fill in the details to match the original image."}, {"heading": "6.3 Understanding sources of error", "text": "Although better than baselines, our reconstructions still admit some error. There are three sources of this error: (a) Representation error: the image being sensed is far from the range of the generator (b) Measurement error: The finite set of random measurements do not contain all the information about the unknown image (c) Optimization error: The optimization procedure did not find the best z.\nIn this section we present some experiments that suggest that the representation error is the dominant term. In our first experiment, we ensure that the representation error is zero, and try to minimize the sum of other two errors. In the second experiment, we ensure that the measurement error is zero, and try to minimize the sum of other two."}, {"heading": "6.3.1 Sensing images from the range of the generator", "text": "Our first approach is to sense an image that is in the range of the generator. More concretely, we sample a z\u2217 from PZ . Then we pass it through the generator to get x\u2217 = G(z\u2217). Now, we pretend that this is a real image and try to sense that. This method eliminates the representation error and allows us to check if our gradient based optimization procedure is able to find z\u2217 by minimizing the objective.\nIn Fig. 6a and Fig. 6b, we show the reconstruction error for images in the range of the generators trained on MNIST and celebA datasets respectively. We see that we get almost perfect reconstruction with very few measurements. This suggests that objective is being properly minimized and we indeed get z\u0302 close to z\u2217. i.e. the sum of optimization error and the measurement error is not very large, in the absence of the representation error."}, {"heading": "6.3.2 Quantifying representation error", "text": "We saw that in absence of the representation error, the overall error is small. However from Fig. 1, we know that the overall error is still non-zero. So, in this experiment, we seek to quantify the representation error, i.e., how far are the real images from the range of the generator?\nFrom the previous experiment, we know that the z\u0302 recovered by our algorithm is close to z\u2217, the best possible value, if the image being sensed is in the range of the generator. Based on this, we make an assumption that this property is also true for real images. With this assumption, we get an estimate to the representation error as follows: We sample real images from the test set. Then we use the full image in our algorithm, i.e., our measurement matrix A is identity. This eliminates the measurement error. Using these measurements, we get the reconstructed image G(z\u0302) through our algorithm. The estimated representation error is then \u2016G(z\u0302) \u2212 x\u2217\u20162. We repeat this procedure several times over randomly sampled images from our dataset and report average representation error values. The task of finding the closest image in the range of the generator has been studied in prior work [11, 16, 12].\nOn the MNIST dataset, we get average per pixel representation error of 0.005. The recovered images are shown in Fig. 7. In contrast with only 100 Gaussian measurements, we are able to get a per pixel reconstruction error of about 0.009.\nOn the celebA dataset, we get average per pixel representation error of 0.020. The recovered images are shown in Fig. 5. On the other hand, with only 500 Gaussian measurements, we get a per pixel reconstruction error of about 0.028.\nThese experiments suggest that the representation error is the major component of the total error. Thus, a more flexible generative model can help to decrease the overall error on both datasets."}, {"heading": "7 Conclusion", "text": "We demonstrate how to perform compressed sensing using generative models from neural nets. These models can represent data distributions more concisely than standard sparsity models, while their differentiability allows for fast signal reconstruction. This will allow compressed sensing applications to make significantly fewer measurements.\nOur theorems and experiments both suggest that, after relatively few measurements, the signal reconstruction gets close to the optimal within the range of the generator. To reach the full potential of this technique, one should use larger generative models as the number of measurements increase. Whether this can be expressed more concisely than by training multiple independent generative models of different sizes is an open question.\nGenerative models are an active area of research with ongoing rapid improvements. Because our framework applies to general generative models, this improvement will immediately yield better reconstructions with fewer measurements. We also believe that one could also use the performance of generative models for our task as one benchmark for the quality of different models."}, {"heading": "Acknowledgements", "text": "We would like to thank Philipp Kra\u0308henbu\u0308hl for helpful discussions."}, {"heading": "8 Appendix A", "text": "Lemma 8.1. Given S \u2286 Rn, y \u2208 Rm, A \u2208 Rm\u00d7n, and \u03b3, \u03b4, 1, 2 > 0, if matrix A satisfies the S-REC(S, \u03b3, \u03b4), then for any two x1, x2 \u2208 S, such that \u2016Ax1 \u2212 y\u2016 \u2264 1 and \u2016Ax2 \u2212 y\u2016 \u2264 2, we have\n\u2016x1 \u2212 x2\u2016 \u2264 1 + 2 + \u03b4\n\u03b3 .\nProof.\n\u2016x1 \u2212 x2\u2016 \u2264 1\n\u03b3 (\u2016Ax1 \u2212Ax2\u2016+ \u03b4) ,\n= 1\n\u03b3 (\u2016(Ax1 \u2212 y)\u2212 (Ax2 \u2212 y)\u2016+ \u03b4) ,\n\u2264 1 \u03b3 (\u2016(Ax1 \u2212 y)\u2016+ \u2016(Ax2 \u2212 y)\u2016+ \u03b4) ,\n\u2264 1 + 2 + \u03b4 \u03b3 ."}, {"heading": "8.1 Proof of Lemma 4.1", "text": "Definition 2. A random variable X is said to be subgamma(\u03c3,B) if \u2200 \u2265 0, we have\nP (|X \u2212 E[X]| \u2265 ) \u2264 2 max ( e\u2212 2/(2\u03c32), e\u2212B /2 ) .\nLemma 8.2. Let G : Rk \u2192 Rn be an L-Lipschitz function. Let Bk(r) be the L2-ball in Rk with radius r, S = G(Bk(r)), and M be a \u03b4/L-net on Bk(r) such that |M | \u2264 k log ( 4Lr\n\u03b4\n) . Let A be a Rm\u00d7n random matrix with IID\nGaussian entries with zero mean and variance 1/m. If\nm = \u2126 ( k log Lr\n\u03b4\n) ,\nthen for any x \u2208 S, if x\u2032 = arg minx\u0302\u2208G(M) \u2016x\u2212 x\u0302\u2016, we have \u2016A(x\u2212 x\u2032)\u2016 = O(\u03b4) with probability 1\u2212 e\u2212\u2126(m).\nNote that for any given point x\u2032 in S, if we try to find its nearest neighbor of that point in an \u03b4-net on S, then the difference between the two is at most the \u03b4. In words, this lemma says that even if we consider measurements made on these points, i.e. a linear projection using a random matrix A, then as long as there are enough measurements, the difference between measurements is of the same order \u03b4. If the point x\u2032 was in the net, then this can be easily achieved by Johnson-Lindenstrauss Lemma. But to argue that this is true for all x\u2032 in S, which can be an uncountably large set, we construct a chain of nets on S. We now present the formal proof.\nProof. Observe that \u2016Ax\u20162\n\u2016x\u20162 is subgamma ( 1\u221a m , 1 m ) . Thus, for any f > 0,\n\u2265 2 + 4 m log 2 f \u2265 max\n(\u221a 2\nm log\n2 f , 2 m log 2 f ) is sufficient to ensure that\nP (\u2016Ax\u2016 \u2265 (1 + )\u2016x\u2016) \u2264 f.\nNow, let M = M0 \u2286 M1 \u2286 M2, \u00b7 \u00b7 \u00b7 \u2286 Ml be a chain of epsilon nets of Bk(r) such that Mi is a \u03b4i/L-net and \u03b4i = \u03b40/2 i, with \u03b40 = \u03b4. We know that there exist nets such that\nlog |Mi| \u2264 k log ( 4Lr\n\u03b4i\n) \u2264 ik + k log ( 4Lr\n\u03b40\n) .\nLet Ni = G(Mi). Then due to Lipschitzness of G, Ni\u2019s form a chain of epsilon nets such that Ni is a \u03b4i-net of S = G(Bk(r)), with |Ni| = |Mi|.\nFor i \u2208 {0, 1, 2 \u00b7 \u00b7 \u00b7 , l \u2212 1}, let Ti = {xi+1 \u2212 xi | xi+1 \u2208 Ni+1, xi \u2208 Ni}.\nThus,\n|Ti| \u2264 |Ni+1||Ni|. =\u21d2 log |Ti| \u2264 log |Ni+1|+ | log |Ni|,\n\u2264 (2i+ 1)k + 2k log ( 4Lr\n\u03b40\n) ,\n\u2264 3ik + 2k log ( 4Lr\n\u03b40\n) .\nNow assume m = 3k log ( 4Lr\n\u03b40\n) ,\nlog(fi) = \u2212(m+ 4ik), and\ni = 2 + 4\nm log\n2 fi ,\n= 2 + 4\nm log 2 + 4 +\n16ik\nm ,\n= O(1) + 16ik\nm .\nBy choice of fi and i, we have \u2200i \u2208 [l \u2212 1],\u2200t \u2208 Ti,\nP (\u2016At\u2016 > (1 + i)\u2016t\u2016) \u2264 fi.\nThus by union bound, we have\nP (\u2016At\u2016 \u2264 (1 + i)\u2016t\u2016,\u2200i,\u2200t \u2208 Ti) \u2265 1\u2212 l\u22121\u2211 i=0 |Ti|fi.\nNow,\nlog(|Ti|fi) = log(|Ti|) + log(fi), \u2264 \u2212k log ( 4Lr\n\u03b40\n) \u2212 ik,\n= \u2212m/3\u2212 ik.\n=\u21d2 l\u22121\u2211 i=0 |Ti|fi \u2264 e\u2212m/3 l\u22121\u2211 i=0 e\u2212ik,\n\u2264 e\u2212m/3 ( 1\n1\u2212 e\u22121\n) ,\n\u2264 2e\u2212m/3.\nObserve that for any x \u2208 S, we can write\nx = x0 + (x1 \u2212 x0) + (x2 \u2212 x1) . . . (xl \u2212 xl\u22121) + xf .\nx\u2212 x0 = l\u22121\u2211 i=0 (xi+1 \u2212 xi) + xf .\nwhere xi \u2208 Ni and xf = x\u2212 xl.\nSince each xi+1 \u2212 xi \u2208 Ti, with probability at least 1\u2212 2e\u2212m/3, we have l\u22121\u2211 i=0 \u2016A(xi+1 \u2212 xi)\u2016 = l\u22121\u2211 i=0 (1 + i)\u2016(xi+1 \u2212 xi)\u2016,\n\u2264 l\u22121\u2211 i=0 (1 + i)\u03b4i,\n= \u03b40 l\u22121\u2211 i=0 1 2i ( O(1) + 16ik m ) ,\n= O(\u03b40) + \u03b40 16k\nm l\u22121\u2211 i=0 ( i 2i ) ,\n= O(\u03b40).\nNow, \u2016xf\u2016 = \u2016x \u2212 xl\u2016 \u2264 dl = \u03b40 2l , and \u2016xi+1 \u2212 xi\u2016 \u2264 \u03b4i due to properties of epsilon-nets. We know that \u2016A\u2016 \u2264 2 + \u221a n/m with probability at least 1 \u2212 2e\u2212m/2 (Corollary 5.35 [36]). By setting l = log(n), we get that,\n\u2016A\u2016\u2016xf\u2016 \u2264 ( 2 + \u221a n\nm ) \u03b40 2l = O(\u03b40) with probability \u2265 1\u2212 2e\u2212m/2.\nCombining these two results, and noting that it is possible to choose x\u2032 = x0, we get that with probability 1\u2212 e\u2212\u2126(m),\n\u2016A(x\u2212 x\u2032)\u2016 = \u2016A(x\u2212 x0)\u2016,\n\u2264 l\u22121\u2211 i=0 \u2016A(xi+1 \u2212 xi)\u2016+ \u2016Axf\u2016, = O(\u03b40) + \u2016A\u2016\u2016xf\u2016, = O(\u03b4).\nLemma. Let G : Rk \u2192 Rn be L-Lipschitz. Let\nBk(r) = {z | z \u2208 Rk, \u2016z\u2016 \u2264 r}\nbe an L2-norm ball in Rk. For \u03b1 < 1, if\nm = \u2126\n( k\n\u03b12 log\nLr\n\u03b4\n) ,\nthen a random matrixA \u2208 Rm\u00d7n with IID entries such thatAij \u223c N ( 0, 1m ) satisfies the S-REC(G(Bk(r)), 1\u2212\u03b1, \u03b4) with 1\u2212 e\u2212\u2126(\u03b12m) probability.\nProof. We construct a \u03b4\nL -net, N , on Bk(r). There exists a net such that\nlog |N | \u2264 k log ( 4Lr\n\u03b4\n) .\nSince N is a \u03b4\nL -cover of Bk(r), due to the L-Lipschitz property of G(\u00b7), we get that G(N) is a \u03b4-cover of G(Bk(r)).\nLet T denote the pairwise differences between the elements in G(N), i.e.,\nT = {G(z1)\u2212G(z2) | z1, z2 \u2208 N}.\nThen,\n|T | \u2264 |N |2, =\u21d2 log |T | \u2264 2 log |N |,\n\u2264 2k log ( 4Lr\n\u03b4\n) .\nFor any z, z\u2032 \u2208 Bk, \u2203 z1, z2 \u2208 N , such thatG(z1), G(z2) are \u03b4-close toG(z) andG(z\u2032) respectively. Thus, by triangle inequality,\n\u2016G(z)\u2212G(z\u2032)\u2016 \u2264 \u2016G(z)\u2212G(z1)\u2016+ \u2016G(z1)\u2212G(z2)\u2016+ \u2016G(z2)\u2212G(z\u2032)\u2016,\n\u2264 \u2016G(z1)\u2212G(z2)\u2016+ 2\u03b4.\nAgain by triangle inequality,\n\u2016AG(z1)\u2212AG(z2)\u2016 \u2264 \u2016AG(z1)\u2212AG(z)\u2016+ \u2016AG(z)\u2212AG(z\u2032)\u2016+ \u2016AG(z\u2032)\u2212AG(z2)\u2016.\nNow, by Lemma 8.2, with probability 1\u2212 e\u2212\u2126(m), \u2016AG(z1)\u2212 AG(z)\u2016 = O(\u03b4), and \u2016AG(z\u2032)\u2212 AG(z2)\u2016 = O(\u03b4). Thus,\n\u2016AG(z1)\u2212AG(z2)\u2016 \u2264 \u2016AG(z)\u2212AG(z\u2032)\u2016+O(\u03b4).\nBy the Johnson-Lindenstrauss Lemma, for a fixed x \u2208 Rn, P [ \u2016Ax\u20162 < (1\u2212 \u03b1)\u2016x\u20162 ] < exp(\u2212\u03b12m). Therefore, we can union bound over all vectors in T to get\nP(\u2016Ax\u20162 \u2265 (1\u2212 \u03b1)\u2016x\u20162, \u2200x \u2208 T ) \u2265 1\u2212 e\u2212\u2126(\u03b1 2m).\nSince \u03b1 < 1, and z1, z2 \u2208 N , G(z1)\u2212G(z2) \u2208 T , we have\n(1\u2212 \u03b1)\u2016G(z1)\u2212G(z2)\u2016 \u2264 \u221a\n1\u2212 \u03b1\u2016G(z1)\u2212G(z2)\u2016, \u2264 \u2016AG(z1)\u2212AG(z2)\u2016.\nCombining the three results above we get that with probability 1\u2212 e\u2212\u2126(\u03b12m),\n(1\u2212 \u03b1)\u2016G(z)\u2212G(z\u2032)\u2016 \u2264 (1\u2212 \u03b1)\u2016G(z1)\u2212G(z2)\u2016+O(\u03b4), \u2264 \u2016AG(z1)\u2212AG(z2)\u2016+O(\u03b4), \u2264 \u2016AG(z)\u2212AG(z\u2032)\u2016+O(\u03b4).\nThus, A satisfies S-REC(S, 1\u2212 \u03b1, \u03b4) with probability 1\u2212 e\u2212\u2126(\u03b12m)."}, {"heading": "8.2 Proof of Lemma 4.2", "text": "Lemma 8.3. Consider c different k \u2212 1 dimensional hyperplanes in Rk. Consider the k-dimensional faces (hereafter called k-faces) generated by the hyperplanes, i.e. the elements in the partition of Rk such that relative to each hyperplane, all points inside a partition are on the same side. Then, the number of k-faces is O(ck).\nProof. Proof is by induction, and follows [32].\nLet f(c, k) denote the number of k\u2212faces generated in Rk by c different (k \u2212 1)-dimensional hyperplanes. As a base case, let k = 1. Then (k \u2212 1)-dimensional hyperplanes are just points on a line. c points partition R into c+ 1 pieces. This gives f(c, 1) = O(c).\nNow, assuming that f(c, k \u2212 1) = O(ck\u22121) is true, we need to show f(c, k) = O(ck). Assume we have (c \u2212 1) different hyperplanes H = {h1, h2, . . . , hc\u22121} \u2282 Rk, and a new hyperplane hc is added. hc intersects H at (c \u2212 1) different (k \u2212 2)-faces given by F = {fj | fj = hj \u2229 hc, 1 \u2264 j \u2264 (c \u2212 1)}. The (k \u2212 2)-faces in F partition hc into f(c\u2212 1, k \u2212 1) different (k \u2212 1)-faces. Additionally, each (k \u2212 1)-face in hc divides an existing k-face into two. Hence the number of new k-faces introduced by the addition of hc is f(c\u2212 1, k \u2212 1). This gives the recursion\nf(c, k) = f(c\u2212 1, k) + f(c\u2212 1, k \u2212 1), = f(c\u2212 1, k) +O(ck\u22121), = O(ck).\nLemma. Let G : Rk \u2192 Rn be a d-layer neural network, where each layer is a linear transformation followed by a pointwise non-linearity. Suppose there are at most c nodes per layer, and the non-linearities are piecewise linear with at most two pieces, and let\nm = \u2126\n( 1\n\u03b12 kd log c ) for some \u03b1 < 1. Then a random matrixA \u2208 Rm\u00d7n with IID entriesAij \u223c N (0, 1m ) satisfies the S-REC(G(R\nk), 1\u2212 \u03b1, 0) with 1\u2212 e\u2212\u2126(\u03b12m) probability.\nProof. Consider the first layer of G. Each node in this layer can be represented as a hyperplane in Rk, where the points on the hyperplane are those where the input to the node switches from one linear piece to the other. Since there are at most c nodes in this layer, by Lemma 8.3, the input space is partitioned by at most c different hyperplanes, into O(ck) k-faces. Applying this over the d layers of G, we get that the input space Rk is partitioned into at most ckd sets.\nRecall that the non-linearities are piecewise linear, and the partition boundaries were made precisely at those points where the non-linearities change from one piece to another. This means that within each set of the input partition, the output is a linear function of the inputs. Thus G(Rk) is a union of ckd different k-faces in Rn.\nWe now use an oblivious subspace embedding to bound the number of measurements required to embed the range of G(\u00b7). For a single k-face S \u2286 Rn, a random matrix A \u2208 Rm\u00d7n with IID entries such that Aij \u223c N ( 0, 1m ) satisfies S-REC(S, 1\u2212 \u03b1, 0) with probability 1\u2212 e\u2212\u2126(\u03b12m) if m = \u2126(k/\u03b12).\nSince the range ofG(\u00b7) is a union of ckd different k-faces, we can union bound over all of them, such thatA satisfies the S-REC(G(Rk), 1\u2212\u03b1, 0) with probability 1\u2212ckde\u2212\u2126(\u03b12m). Thus, we get thatA satisfies the S-REC(G(Rk), 1\u2212\u03b1, 0) with probability 1\u2212 e\u2212\u2126(\u03b12m) if\nm = \u2126\n( kd log c\n\u03b12\n) ."}, {"heading": "8.3 Proof of Lemma 4.3", "text": "Lemma. LetA \u2208 Rm\u00d7n by drawn from a distribution that (1) satisfies the S-REC(S, \u03b3, \u03b4) with probability 1\u2212p and (2) has for every fixed x \u2208 Rn, \u2016Ax\u2016 \u2264 2\u2016x\u2016 with probability 1\u2212 p. For any x\u2217 \u2208 Rn and noise \u03b7, let y = Ax\u2217 + \u03b7. Let x\u0302 approximately minimize \u2016y \u2212Ax\u2016 over x \u2208 S, i.e.,\n\u2016y \u2212Ax\u0302\u2016 \u2264 min x\u2208S \u2016y \u2212Ax\u2016+ .\nThen\n\u2016x\u0302\u2212 x\u2217\u2016 \u2264 ( 4\n\u03b3 + 1 ) min x\u2208S \u2016x\u2217 \u2212 x\u2016+ 1 \u03b3 (2\u2016\u03b7\u2016+ + \u03b4)\nwith probability 1\u2212 2p.\nProof. Let x = arg minx\u2208S \u2016x\u2217 \u2212 x\u2016. Then we have by Lemma 8.1 and the hypothesis on x\u0302 that\n\u2016x\u2212 x\u0302\u2016 \u2264 \u2016Ax\u2212 y\u2016+ \u2016Ax\u0302\u2212 y\u2016+ \u03b4 \u03b3 ,\n\u2264 2\u2016Ax\u2212 y\u2016+ + \u03b4 \u03b3 , \u2264 2\u2016A(x\u2212 x \u2217)\u2016+ 2\u2016\u03b7\u2016+ + \u03b4\n\u03b3 ,\nas long as A satisfies the S-REC, as happens with probability 1 \u2212 p. Now, since x and x\u2217 are independent of A, by assumption we also have \u2016A(x\u2212 x\u2217)\u2016 \u2264 2\u2016x\u2212 x\u2217\u2016 with probability 1\u2212 p. Therefore\n\u2016x\u2217 \u2212 x\u0302\u2016 \u2264 \u2016x\u2212 x\u2217\u2016+ 4\u2016x\u2212 x \u2217\u2016+ 2\u2016\u03b7\u2016+ + \u03b4\n\u03b3\nas desired."}, {"heading": "8.4 Lipschitzness of Neural Networks", "text": "Lemma 8.4. Consider any two functions f and g. If f is Lf -Lipschitz and g is Lg-Lipschitz, then their composition f \u25e6 g is LfLg-Lipschitz.\nProof. For any two x1, x2,\n\u2016f(g(x1))\u2212 f(g(x2))\u2016 \u2264 Lf\u2016g(x1)\u2212 g(x2)\u2016, \u2264 LfLg\u2016x1 \u2212 x2\u2016.\nLemma 8.5. If G is a d-layer neural network with at most c nodes per layer, all weights \u2264 wmax in absolute value, and M -Lipschitz non-linearity after each layer, then G(\u00b7) is L-Lipschitz with L = (Mcwmax)d.\nProof. Consider any linear layer with input x, weight matrix W and bias vector b. Thus, f(x) = Wx + b. Now for any two x1, x2,\n\u2016f(x1)\u2212 f(x2)\u2016 = \u2016Wx1 + b\u2212Wx2 + b\u2016, = \u2016W (x1 \u2212 x2)\u2016, \u2264 \u2016W\u2016\u2016(x1 \u2212 x2)\u2016, \u2264 cwmax\u2016(x1 \u2212 x2)\u2016.\nLet fi(\u00b7), i \u2208 [d] denote the function for the i-th layer in G. Since each layer is a composition of a linear function and a non-linearity, by Lemma 8.4, have that fi is Mcwmax-Lipschitz.\nSinceG = f1 \u25e6f2 \u25e6 . . . fd, by repeated application of Lemma 8.4, we get thatG is L-Lipschitz with L = (Mcwmax)d."}, {"heading": "9 Appendix B", "text": ""}, {"heading": "9.1 Noise tolerance", "text": "To understand the noise tolerance of our algorithm, we do the following experiment: First we fix the number of measurements so that Lasso does as well as our algorithm. From Fig. 1a, and Fig. 1b we see that this point is at m = 500 for MNIST and m = 2500 for celebA. Now, we look at the performance as the noise level increases. Hyperparameters are kept fixed as we change the noise level for both Lasso and for our algorithm.\nIn Fig. 8a, we show the results on the MNIST dataset. In Fig. 8a, we show the results on celebA dataset. We observe that our algorithm has more noise tolerance than Lasso."}, {"heading": "9.2 Other models", "text": ""}, {"heading": "9.2.1 End to end training on MNIST", "text": "Instead of using a generative model to reconstruct the image, another approach is to learn from scratch a mapping that takes the measurements and outputs the original image. A major drawback of this approach is that it necessitates learning a new network if get a different set of measurements.\nIf we use a random matrix for every new image, the input to the network is essentially noise, and the network does not learn at all. Instead we are forced to use a fixed measurement matrix. We explore two approaches. First is to randomly sample and fix the measurement matrix and learn the rest of the mapping. In the second approach, we jointly optimize the measurement matrix as well.\nWe do this for 10, 20 and 30 measurements for the MNIST dataset. We did not use additive noise. The reconstruction errors are shown in Fig. 9. The reconstructions can be seen in Fig. 10."}, {"heading": "9.3 More results", "text": "Here, we show more results on the reconstruction task, with varying number of measurements on both MNIST and celebA. Fig. 11 shows reconstructions on MNIST with 25, 100 and 400 measurements. Fig. 12, Fig. 13 and Fig. 14 show results on celebA dataset."}], "references": [{"title": "Fast global convergence rates of gradient methods for high-dimensional statistical recovery", "author": ["Alekh Agarwal", "Sahand Negahban", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "The fast johnson\u2013lindenstrauss transform and approximate nearest neighbors", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "SIAM Journal on Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Optimization with sparsity-inducing penalties", "author": ["Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Model-based compressive sensing", "author": ["Richard G Baraniuk", "Volkan Cevher", "Marco F Duarte", "Chinmay Hegde"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1982}, {"title": "Random projections of smooth manifolds", "author": ["Richard G Baraniuk", "Michael B Wakin"], "venue": "Foundations of computational mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["Emmanuel J Candes", "Justin K Romberg", "Terence Tao"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Prior image constrained compressed sensing (piccs): a method to accurately reconstruct dynamic ct images from highly undersampled projection data sets", "author": ["Guang-Hong Chen", "Jie Tang", "Shuai Leng"], "venue": "Medical physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Compressed sensing and dictionary learning", "author": ["Guangliang Chen", "Deanna Needell"], "venue": "Proceedings of Symposia in Applied Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Compressed sensing and best k-term approximation", "author": ["A. Cohen", "W. Dahmen", "R. DeVore"], "venue": "J. Amer. Math. Soc,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Inverting the generator of a generative adversarial network", "author": ["Antonia Creswell", "Anil Anthony Bharath"], "venue": "arXiv preprint arXiv:1611.05644,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Image super-resolution using deep convolutional networks", "author": ["Chao Dong", "Chen Change Loy", "Kaiming He", "Xiaoou Tang"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Compressed sensing", "author": ["David L Donoho"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Single-pixel imaging via compressive sampling", "author": ["Marco F Duarte", "Mark A Davenport", "Dharmpal Takbar", "Jason N Laska", "Ting Sun", "Kevin F Kelly", "Richard G Baraniuk"], "venue": "IEEE signal processing magazine,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Corrupted sensing: Novel guarantees for separating structured signals", "author": ["Rina Foygel", "Lester Mackey"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Towards understanding the invertibility of convolutional neural networks. 2017", "author": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Signal recovery on incoherent manifolds", "author": ["Chinmay Hegde", "Richard G Baraniuk"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Compressive sensing recovery of spike trains using a structured sparsity model", "author": ["Chinmay Hegde", "Marco F Duarte", "Volkan Cevher"], "venue": "In SPARS\u201909-Signal Processing with Adaptive Sparse Structured Representations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "A nearly-linear time framework for graph-structured sparsity", "author": ["Chinmay Hegde", "Piotr Indyk", "Ludwig Schmidt"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Accurate image super-resolution using very deep convolutional networks", "author": ["Jiwon Kim", "Jung Kwon Lee", "Kyoung Mu Lee"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "A tensorflow implementation of \u201cdeep convolutional generative adversarial networks", "author": ["Taehoon Kim"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Precise recovery of latent vectors from generative adversarial networks", "author": ["Zachary C Lipton", "Subarna Tripathi"], "venue": "arXiv preprint arXiv:1702.04782,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Sparse mri: The application of compressed sensing for rapid mr imaging", "author": ["Michael Lustig", "David Donoho", "John M Pauly"], "venue": "Magnetic resonance in medicine,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K Ravikumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1996}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Image super-resolution via sparse representation", "author": ["Jianchao Yang", "John Wright", "Thomas S Huang", "Yi Ma"], "venue": "IEEE transactions on image processing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}], "referenceMentions": [{"referenceID": 33, "context": "Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x\u2217 if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) [35, 7, 14, 6].", "startOffset": 305, "endOffset": 319}, {"referenceID": 6, "context": "Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x\u2217 if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) [35, 7, 14, 6].", "startOffset": 305, "endOffset": 319}, {"referenceID": 13, "context": "Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x\u2217 if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) [35, 7, 14, 6].", "startOffset": 305, "endOffset": 319}, {"referenceID": 5, "context": "Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x\u2217 if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) [35, 7, 14, 6].", "startOffset": 305, "endOffset": 319}, {"referenceID": 5, "context": "[6, 33, 1, 30, 3].", "startOffset": 0, "endOffset": 17}, {"referenceID": 31, "context": "[6, 33, 1, 30, 3].", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "[6, 33, 1, 30, 3].", "startOffset": 0, "endOffset": 17}, {"referenceID": 29, "context": "[6, 33, 1, 30, 3].", "startOffset": 0, "endOffset": 17}, {"referenceID": 2, "context": "[6, 33, 1, 30, 3].", "startOffset": 0, "endOffset": 17}, {"referenceID": 7, "context": "Compressed sensing has been studied extensively for medical applications including computed tomography (CT) [8], rapid MRI [31] and neuronal spike train recovery [21].", "startOffset": 108, "endOffset": 111}, {"referenceID": 30, "context": "Compressed sensing has been studied extensively for medical applications including computed tomography (CT) [8], rapid MRI [31] and neuronal spike train recovery [21].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "Compressed sensing has been studied extensively for medical applications including computed tomography (CT) [8], rapid MRI [31] and neuronal spike train recovery [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Another impressive application is the \u201csingle pixel camera\u201d [15], where digital micro-mirrors provide linear combinations to a single pixel sensor that then uses compressed sensing reconstruction algorithms to reconstruct an image.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "These results have been extended by combining sparsity with additional structural assumptions [4, 22], and by generalizations such as translating sparse vectors into low-rank matrices [33, 3, 17].", "startOffset": 94, "endOffset": 101}, {"referenceID": 21, "context": "These results have been extended by combining sparsity with additional structural assumptions [4, 22], and by generalizations such as translating sparse vectors into low-rank matrices [33, 3, 17].", "startOffset": 94, "endOffset": 101}, {"referenceID": 31, "context": "These results have been extended by combining sparsity with additional structural assumptions [4, 22], and by generalizations such as translating sparse vectors into low-rank matrices [33, 3, 17].", "startOffset": 184, "endOffset": 195}, {"referenceID": 2, "context": "These results have been extended by combining sparsity with additional structural assumptions [4, 22], and by generalizations such as translating sparse vectors into low-rank matrices [33, 3, 17].", "startOffset": 184, "endOffset": 195}, {"referenceID": 16, "context": "These results have been extended by combining sparsity with additional structural assumptions [4, 22], and by generalizations such as translating sparse vectors into low-rank matrices [33, 3, 17].", "startOffset": 184, "endOffset": 195}, {"referenceID": 8, "context": "Other works perform \u201cdictionary learning,\u201d seeking overcomplete bases where the data is more sparse (see [9] and references therein).", "startOffset": 105, "endOffset": 108}, {"referenceID": 25, "context": "Recently, several neural network based generative models such as variational auto-encoders (VAEs) [26] and generative adversarial networks (GANs) [19] have found success at modeling data distributions.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Recently, several neural network based generative models such as variational auto-encoders (VAEs) [26] and generative adversarial networks (GANs) [19] have found success at modeling data distributions.", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "Hence the same results hold for matrices with subgaussian entries or fast-JL matrices [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 27, "context": "Excellent reconstruction results with SGD in the representation space to find an image in the generator range have been reported by [28] with stochastic clipping and [11] with logistic measurement loss.", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Excellent reconstruction results with SGD in the representation space to find an image in the generator range have been reported by [28] with stochastic clipping and [11] with logistic measurement loss.", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "A different approach is introduced in [16] and [12].", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "A different approach is introduced in [16] and [12].", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "[37, 13, 23] and references therein.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "[37, 13, 23] and references therein.", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[37, 13, 23] and references therein.", "startOffset": 0, "endOffset": 12}, {"referenceID": 17, "context": "We also take note of the related work of [18] that connects model-based compressed sensing with the invertibility of Convolutional Neural Networks.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "A related result appears in [5], which studies the measurement complexity of an RIP condition for smooth manifolds.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "Their recovery result was extended in [20] to unions of two manifolds.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "In order to make our error guarantee relative to `2 error in the image space R, rather than in the measurement space R, we also need that A preserves norms with high probability [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 26, "context": "The MNIST dataset consists of about 60, 000 images of handwritten digits, where each image is of size 28\u00d7 28 [27].", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "We train the VAE using the Adam optimizer [25] with a mini-batch size 100 and a learning rate of 0.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "CelebA is a dataset of more than 200, 000 face images of celebrities [29].", "startOffset": 69, "endOffset": 73}, {"referenceID": 32, "context": "We trained a DCGAN 1 [34, 24] on this dataset.", "startOffset": 21, "endOffset": 29}, {"referenceID": 23, "context": "We trained a DCGAN 1 [34, 24] on this dataset.", "startOffset": 21, "endOffset": 29}, {"referenceID": 32, "context": "The architecture follows that of [34].", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Each update used the Adam optimizer [25] with minibatch size 64, learning rate 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "We use Adam optimizer [25], with a learning rate of 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "We optimize use Adam optimizer [25], with a learning rate of 0.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "The task of finding the closest image in the range of the generator has been studied in prior work [11, 16, 12].", "startOffset": 99, "endOffset": 111}, {"referenceID": 15, "context": "The task of finding the closest image in the range of the generator has been studied in prior work [11, 16, 12].", "startOffset": 99, "endOffset": 111}, {"referenceID": 11, "context": "The task of finding the closest image in the range of the generator has been studied in prior work [11, 16, 12].", "startOffset": 99, "endOffset": 111}], "year": 2017, "abstractText": "The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model G : R \u2192 R. Our main theorem is that, if G is L-Lipschitz, then roughly O(k logL) random Gaussian measurements suffice for an `2/`2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 5-10x fewer measurements than Lasso for the same accuracy.", "creator": "LaTeX with hyperref package"}}}