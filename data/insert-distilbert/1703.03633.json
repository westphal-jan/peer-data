{"id": "1703.03633", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2017", "title": "Learning Gradient Descent: Better Generalization and Longer Horizons", "abstract": "training deep neural networks is initially a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. work trying different combinations can be quite labor - intensive and time consuming. recently, researchers have tried to use deep cognitive learning algorithms to exploit the landscape of cracking the loss function chain of the training problem of interest, and learn how to optimize over it in an automatic way. in this paper, we propose a new learning - memory to - learn design model and some useful and practical modelling tricks. our optimizer outperforms generic, hand - crafted optimization algorithms and state - of - the - art learning - to - learn optimizers by deepmind in many tasks. together we demonstrate the effectiveness of our algorithms operating on a number of tasks, including deep mlps, cnns, and simple lstms.", "histories": [["v1", "Fri, 10 Mar 2017 11:30:03 GMT  (3751kb,D)", "https://arxiv.org/abs/1703.03633v1", "9 pages, 8 figures, 3 tables, submitted to ICML 2017"], ["v2", "Mon, 13 Mar 2017 02:45:49 GMT  (3751kb,D)", "http://arxiv.org/abs/1703.03633v2", "9 pages, 8 figures, 3 tables"], ["v3", "Sat, 10 Jun 2017 16:25:37 GMT  (3538kb,D)", "http://arxiv.org/abs/1703.03633v3", "Accepted to ICML 2017, 9 pages, 9 figures, 4 tables"]], "COMMENTS": "9 pages, 8 figures, 3 tables, submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["kaifeng lv", "shunhua jiang", "jian li"], "accepted": true, "id": "1703.03633"}, "pdf": {"name": "1703.03633.pdf", "metadata": {"source": "META", "title": "Learning Gradient Descent: Better Generalization and Longer Horizons", "authors": ["Kaifeng Lv", "Shunhua Jiang", "Jian Li"], "emails": ["ing@163.com>,", "Jiang<linda6582@163.com>,", "<lijian83@mail.tsinghua.edu.cn>."], "sections": [{"heading": "1. Introduction", "text": "Training a neural network can be viewed as solving an optimization problem for a highly non-convex loss function. Gradient-based algorithms are by far the most widely used algorithms for training neural networks, such as basic SGD, Adagrad, RMSprop, Adam, etc. For a particular neural network, it is unclear a priori which one is the best optimization algorithm, and how to set up the hyperparameters (such as learning rates). It usually takes a lot of time and experienced hands to identify the best optimization algorithm together with best hyperparameters, and possibly some other tricks are necessary to make the network work.\n*Equal contribution \u2020The research is supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61632016. 1Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China. Correspondence to: Kaifeng Lv <vfleaking@163.com>, Shunhua Jiang<linda6582@163.com>, Jian Li <lijian83@mail.tsinghua.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s)."}, {"heading": "1.1. Existing Work", "text": "To address the above issue, a promising approach is to use machine learning algorithms to replace the hard-coded optimization algorithms, and hopefully, the learning algorithm is capable of learning a good strategy, from experience, to explore the landscape of the loss function and adaptively choose good descent steps. In a high level, the idea can be categorized under the umbrella of learning-tolearn (or meta-learning), a broad area known to learning community for more than two decades.\nUsing deep learning for training deep neural networks was initiated in a recent paper (Andrychowicz et al., 2016). The authors proposed an optimizer using coordinatewise Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) that takes the gradients of the optimizee as input and outputs the updates for each optimizee parameters. We call this optimizer DMoptimizer throughout this paper, and we use the term optimizee to refer to the loss function of the neural network being optimized. The authors showed that DMoptimizer outperforms traditional optimization algorithms in solving the task on which it is trained, and it also generalizes well to the same type of tasks. In one of their experiments, they trained DMoptimizer to minimize the average loss of a 100-step training process of a 1-hidden-layer Multilayer Perceptron (MLP) with sigmoid as the activation function, and the optimizer was shown to have generalization ability to some extent: it also performs well on such MLP with one more hidden layer or double hidden neurons. However, there are still some limitations:\n1. If the activation function of the MLP is changed from sigmoid to ReLU in the test phase, DMoptimizer performs poorly to train such MLP. In other words, their algorithms fail to generalize to different activations.\n2. Even though the authors showed that DMoptimizer performs well to train the optimizee for 200 descent steps, the loss increases dramatically for much longer horizons. In other words, their algorithms fail to handle a relatively large number of descent steps."}, {"heading": "1.2. Our Contributions", "text": "In this paper, we propose two new training tricks and a new model to improve the results of training a recurrent neural\nar X\niv :1\n70 3.\n03 63\n3v 3\n[ cs\n.L G\n] 1\n0 Ju\nn 20\n17\nnetwork (RNN) to optimize the loss functions of real-world neural networks.\nThe most effective trick is Random Scaling, which is used when training the RNN optimizer to improve its generalization ability by randomly scaling the parameters of the optimizee. The other trick is to combine the loss function of the optimizee with other simple convex functions, which helps to accelerate the training process. With the help of our new training tricks, our new model, called RNNprop, achieves notable improvements upon previous work after being trained on a simple 1-hidden-layer MLP:\n1. It can train optimizees for longer horizons. In particular, when RNNprop is only trained to minimize the final loss of a 100-step training process, in testing phase it can successfully train optimizees for several thousand steps.\n2. It can generalize to a variety of neural networks including much deeper MLPs, CNNs, and simple LSTMs. On these tasks it achieves better or at least comparable performance with traditional optimization algorithms."}, {"heading": "2. Other Related Work", "text": ""}, {"heading": "2.1. Learning to Learn", "text": "The notion of learning to learn or meta-learning has been used to address the concept of learning meta-knowledge about the learning process for years. However, there is no agreement on the exact definition of meta-learning, and various concepts have been developed by different authors (Thrun & Pratt, 1998; Vilalta & Drissi, 2002; Brazdil et al., 2008).\nIn this paper, we view the training process of a neural network as an optimization problem, and we use an RNN as an optimizer to train other neural networks. The usage of another neural network to direct the training of neural networks has been put forward by Naik and Mammone (1992). In their early work, Cotter and Younger (1990; 1999) argued that RNNs can be used to model adaptive optimization algorithms (Prokhorov et al., 2002). This idea was further developed in (Younger et al., 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems. Recently, as shown in Section 1.1, Andrychowicz et al. (2016) proposed a more general optimizer model using LSTM to learn gradient descent, and our work directly follows their work. In another recent paper (Chen et al., 2016), an RNN is used to take current position and value as input and outputs the next position, and it works well for black-box optimization and simple RL tasks.\nFrom a reinforcement learning perspective, the optimizer can be viewed as a policy which takes the current state as input and output the next action (Schmidhuber et al., 1999). Two recent papers (Daniel et al., 2016; Hansen, 2016) trained adaptive controllers to adjust the hyperparameters (learning rate) of traditional optimization algorithms from this perspective. Their method can be regarded as hyperparameter optimization. More general methods have been introduced in (Li & Malik, 2017; Wang et al., 2016) which also take the RL perspective and train a neural network to model a policy."}, {"heading": "2.2. Traditional Optimization Algorithms", "text": "A great number of optimization algorithms have been proposed to improve the performance of vanilla gradient descent, including Momentum(Tseng, 1998), Adagrad(Duchi et al., 2011), Adadelta(Zeiler, 2012), RMSprop(Tieleman & Hinton, 2012), Adam(Kingma & Ba, 2015). The update rules of several common optimization algorithms are listed in Table 1."}, {"heading": "3. Rethinking of Optimization Problems", "text": ""}, {"heading": "3.1. Problem Formalization", "text": "We are interested in finding an optimizer that undertakes the optimization tasks for different optimizees. An optimizee is a function f(\u03b8) to be minimized. In the case when the optimizee is stochastic, that is, the value of f(\u03b8) de-\npends on the sample d selected from a dataset D, the goal of an optimizer is to minimize\n1 |D| \u2211 d\u2208D fd(\u03b8) (1)\nover the variables \u03b8.\nWhen optimizing an optimizee on a datasetD, the behavior of an optimizer can be summarized by the following loop. For each step:\n1. Given the current parameters \u03b8t and a sample dt \u2208 D, perform forward and backward propagation to compute the function value yt = fdt(\u03b8t) and the gradient gt = \u2207fdt(\u03b8t);\n2. Based on the current state ht (of the optimizer) and the gradient gt, the optimizer produces the new state ht+1 and proposes an increment \u2206\u03b8t;\n3. Update the parameters by setting \u03b8t+1 = \u03b8t + \u2206\u03b8t.\nIn the initialization phase, h0 is produced by the optimizer, and \u03b80 is generated according to the initialization rule of the given optimizee. At the end of the loop, we take \u03b8T as the final optimizee parameters."}, {"heading": "3.2. Some Insight into Adaptivity", "text": "Table 1 summaries optimization algorithms that are most commonly used when training neural networks. All of these optimization algorithms have some degree of adaptivity, that is, they are able to adjust the effective step size |\u2206\u03b8t| when training.\nWe can divide these algorithms into two classes. The first class includes SGD and Momentum, as they determine the effective step size by the absolute size of gradients. The second class includes Adagrad, Adadelta, RMSprop, and Adam. These algorithms maintain the sum or the moving average of past gradients g2t , which can be seen as, with a little abuse of terminology, the second raw moment (or uncentered variance). Then, these algorithms produce the effective step size only by the relative size of the gradient, namely, the gradient divided by the square root of the second moment coordinatewise.\nIn a training process, as the parameters gradually approach to a local minimum, a smaller effective step size is required for a more careful local optimization. To obtain such smaller effective step size, these two classes of algorithms have two different mechanisms. For the first class, if we take the full gradient, the effective step size automatically gets smaller when approaching to a local minimum. However, since we use stochastic gradient descent, the effective step size may not be small enough, even if \u03b8 is not\nfar from a local minimum. For the second class, a smaller effective step size |\u2206\u03b8t,i| of each coordinate i is mainly induced by a relatively smaller partial derivative comparing with past partial derivatives. When approaching to a local minimum, the gradient may fluctuate due to stochastic nature. Algorithms of the second class can decrease the effective step size of each coordinate in accordance with the fluctuation amplitude of that coordinate, i.e., a coordinate with larger uncentered variance yields smaller effective step size. Thus, the algorithms of the second class are able to further decrease effective step size for the coordinates with more uncertainty, and they are more robust than those of the first class.\nTo get more insight into the difference between these two classes of algorithms, we consider what happens if we scale the optimizee by a factor c, i.e., let f\u0303(\u03b8) = cf(\u03b8). Ideally, the scaling should not affect the behaviors of the algorithms. However, for the algorithms of the first class, since \u2207f\u0303(\u03b8) = c\u2207f(\u03b8), the effective step size is also scaled by c. Hence, the behaviors of the algorithms change completely. But for the algorithms of the second class, they behave the same on f\u0303(\u03b8) and f(\u03b8) since the scale factor c is canceled out. Thus the algorithms of the second class are more robust with respect to scaling.\nThe above observation, albeit very simple, is a key inspiration for our new model. On the one hand, we use some training tricks so that our model can be exposed to functions with different scales at the training stage. On the other hand, we take relative gradients as input so that our optimizer belongs to the second class. In the following section, we introduce our training tricks and new model in details."}, {"heading": "4. Methods", "text": "Our RNN optimizer operates coordinatewise on parameters \u03b8, which follows directly from (Andrychowicz et al., 2016). The RNN optimizer handles the gradients coordinatewise and maintains hidden states for every coordinate respectively. The parameters of the RNN itself are shared between different coordinates. In this way, the RNN optimizer can train optimizees with any number of parameters."}, {"heading": "4.1. Random Scaling", "text": "We propose a training trick, called Random Scaling, to prevent overfitting when training our model. Before introducing our ideas, consider what happens if we train an RNN optimizer to minimize f(\u03b8) = \u03bb\u2016\u03b8\u201622 with initial parameter \u03b80. Clearly, \u03b8t+1 = \u03b8t \u2212 12\u03bb\u2207f(\u03b8t) is the optimal policy since the lowest point can be reached in just one step. However, if the RNN optimizer learns to follow this rule exactly, testing this RNN optimizer on the same function with different \u03bbmight produce a modest or even bad result.\nThe method to solve this issue is rather simple: We randomly pick a \u03bb for every iteration when training our RNN optimizer. Notice that we can also pick a random number to scale all the parameters to achieve the same goal. To further generalize this idea, we design our training trick, Random Scaling, which coordinatewise randomly scales the parameters of the objective function in the training stage.\nIn more details, for each iteration of training the optimizer on a loss function f(\u03b8) with initial parameter \u03b80, we first randomly pick a vector c of the same dimension as \u03b8, where each coordinate of c is sampled independently from a distribution D0. Then, we train our model on a new optimizee\nfc(\u03b8) = f(c\u03b8) (2)\nwith initial parameter c\u22121\u03b80, where all the multiplication and inversion operations are performed coordinatewise. In this way, the RNN optimizer is forced to learn an adaptive policy to determine the best effective step size, rather than to learn the best effective step size itself of a particular task."}, {"heading": "4.2. Combination with Convex Functions", "text": "Now we introduce another training trick. It is clear that we should train our RNN optimizer on optimizees implemented with neural networks. However, due to non-convex and stochastic nature of neural networks, it may be hard for an RNN to learn the basic idea of gradient descent.\nOur idea is loosely inspired by the proximal algorithms (see e.g., (Parikh & Boyd, 2014)). To make training easier, we combine the original optimizee function f with an n-dim convex function g to get a new optimizee function F\nF (\u03b8,x) = f(\u03b8) + g(x). (3)\nFor every iteration of training RNN optimizer, we generate a random vector v in n-dim vector space, and the function g is defined as\ng(x) = 1\nn n\u2211 i=1 (xi \u2212 vi)2, (4)\nwhere the initial value of x is also generated randomly.\nWithout this trick, the RNN optimizer wanders around aimlessly on the non-convex loss surface of function f in the beginning stage of training. After we combine the optimizee with function g, since g has the good property of convexity, our RNN optimizer soon learns some basic knowledge of gradient descent from these additional optimizee coordinates. This knowledge is shared with other coordinates because the RNN optimizer processes its input coordinatewise. In this way, we can accelerate the training process of the RNN optimizer. As the training continues, the RNN optimizer further learns a better method with gradient decent as a baseline.\nWe can apply Random Scaling on the function g as well to make the behavior of the RNN optimizer more robust."}, {"heading": "4.3. RNNprop Model", "text": "Aside from the above two tricks, we also design a new model RNNprop as shown in Figure 1. All the operations in our model are coordinatewise, following the idea of DMoptimizer idea in (Andrychowicz et al., 2016).\nThe main difference between RNNprop and DMoptimizer is the input. The input m\u0303t and g\u0303t are defined as follows:\nm\u0303t = m\u0302tv\u0302 \u22121/2 t , (5)\ng\u0303t = gtv\u0302 \u22121/2 t , (6)\nwhere m\u0302t, v\u0302t are defined the same way as Adam in Table 1. This change of the input has three advantages. First, this input contains no information about the absolute size of gradients, so our algorithm belongs to the second class automatically and hence is more robust. Second, this manipulation of gradients can be seen as a kind of normalization so that the input values are bounded by a constant, which is somewhat easier for a neural network to learn. Lastly, if our model outputs a constant times m\u0303t, it reduces to Adam. Similarly, if our model outputs a constant times g\u0303t, then it reduces to RMSprop. Hence, the hope is that by further optimizing the parameters of RNNprop, it is capable of achieving better performance than Adam and RMSprop with fixed learning rate.\nThe input is preprocessed by a fully-connected layer with ELU (Exponential Linear Unit) as the activation function (Clevert et al., 2015) before being handled by the RNN. The central part of our model is the RNN, which is a twolayer coordinatewise LSTM that is same as DMoptimizer. The RNN outputs a single vector xout, and the increment is taken as\n\u2206\u03b8t = \u03b1 tanh(xout). (7)\nThis formula can be viewed as a variation of gradient clipping so that all effective step sizes are bounded by the preset parameter \u03b1. In all our experiments, we just set a large enough value \u03b1 = 0.1."}, {"heading": "5. Experiments", "text": "We trained two RNN optimizers, one to reproduce DMoptimizer in (Andrychowicz et al., 2016), the other to implement RNNprop with our new training tricks. Their performances were compared in a number of experiments. 1\nWe use the same optimizee as in (Andrychowicz et al., 2016) to train these two optimizers, which is the crossentropy loss of a simple MLP on the MNIST dataset. For convenience, we address this MLP as the base MLP. It has one hidden layer of 20 hidden units and uses sigmoid as activation function. The value of f(\u03b8) is computed using a minibatch of 128 random pictures. For each iteration during training, the optimizers are allowed to run for 100 steps. Optimizers are trained using truncated Backpropagation Trough Time (BPTT). We split the 100 steps into 5 periods of 20 steps. In each period, we initialize the initial parameter \u03b80 and initial hidden state h0 from the last period or generate them if it is the first period. Adam is used to minimize the loss L(\u03c6) = 1T \u2211T t=1 wtf(\u03b8t). We trained DMoptimizer using the loss with wt = 1 for all t as in (Andrychowicz et al., 2016). For RNNprop we set wT = 1 and wt = 0 for other t. In this way, the optimizer is not strictly required to produce a low loss at each step, so it can be more flexible. We also notice that this loss results in slightly better performance.\nThe structure of our model RNNprop is shown in Section 4.3. The RNN is a two-layer LSTM whose hidden state size is 20. To avoid division by zero, in actual experiments we add another term = 10\u22128, and the input is changed to\nm\u0303t = m\u0302t(v\u0302 1/2 t + ) \u22121, (8)\ng\u0303t = gt(v\u0302 1/2 t + ) \u22121. (9)\nThe parameters \u03b21 and \u03b22 for computing mt and gt are\n1Our code can be found at https://github.com/ vfleaking/rnnprop.\nsimply set to 0.95. In preprocessing, the input is mapped to a 20-dim vector for each coordinate.\nWhen training RNNprop, we first apply Random Scaling to the optimizee function f and the convex function g respectively, where g is defined as Equation (4), and then we combine them together as introduced in Section 4.2. We set the dimension of the convex function g to be n = 20 and generate the vectors v and x from [\u22121, 1]n uniformly randomly. To generate each coordinate of the vector c in Random Scaling, we first generate a number p from [\u2212L,L] uniformly randomly, and then take exp(p) as the value of that coordinate, where exp is the natural exponential function. This implementation is aimed to produce c of different order of magnitude, e.g., Pr[ 110 \u2264 ci \u2264 1 9 ] = Pr[9 \u2264 ci \u2264 10]. We also tried other transformations including using uniform distribution, scaling the entire function directly, randomly dropping some coordinates, etc. This version of Random Scaling is selected after comprehensive comparison. In the experiments we set L = 3 for the function f and L = 1 for the function g.\nWe save all the parameters of the RNN optimizers every 1000 iterations when training. For DMoptimizer, we select the saved optimizer with the best performance on the validation task, same as in (Andrychowicz et al., 2016). Since RNNprop tends not to overfit to the training task because of the Random Scaling method, we simply select the saved optimizer with lowest average train loss, which is the moving average of the losses of the past 1000 iterations with decay factor 0.9. The selected optimizers are then tested on other different tasks. Their performances are compared with the best traditional optimization algorithms whose learning rates are carefully chosen and other hyperparameters are set to the default values in Tensorflow (Abadi et al., 2016). All the initial optimizee parameters used in the experiments are generated independently from the Gaussian distribution N(0, 0.1).\nAll figures shown in this section were plotted after running the optimization process multiple times with random initial values and data. We removed the outliers with exceedingly large loss value when plotting the loss curves. No loss value of RNNprop was removed when plotting the figures."}, {"heading": "5.1. Generalization to More Steps", "text": "We first test optimizers on the task used in the training stage, which is to optimize the base MLP for 100 steps. Both DMoptimizer and RNNprop outperform all traditional optimization algorithms. DMoptimizer has better performance possibly because of overfitting. We then test optimizers to run for more steps on the base MLP. The left plot of Figure 2 indicates that RNNprop can achieve comparable performance with traditional algorithms for 2000 steps while DMoptimizer fails.\nWe also test the optimizers for much more steps: 10000 steps, as shown in the right plot of Figure 2. It is clear that DMoptimizer loses the ability to decrease the loss after about 400 steps and its loss begins to increase dramatically. RNNprop, on the other hand, is able to decrease the loss continuously, though it slows down gradually and traditional algorithms overtake it. The main reason is that RNNprop is trained to run for only 100 steps, and 10000- step training process may be significantly different from 100-step training process. Additionally, traditional optimization algorithms are able to achieve good performance on both tasks because we explicitly adjusted their learning rates to adapt to these tasks.\nFigure 3 shows how the final loss after 10000 steps changes when using different learning rates. For example, Adam can outperform RNNprop only if its learning rate lies in the narrow interval from 0.004 to 0.01.\nFor other optimizees, RNNprop shows similar ability to train for longer horizons. Due to space constraints, we do not discuss them in details."}, {"heading": "5.2. Generalization to Different Activation Functions", "text": "We test the optimizers on the base MLP with different activation functions. As shown in Figure 4, if the activation function is changed from sigmoid to ReLU, RNNprop can still achieve better performance than traditional algorithms while DMoptimizer fails. For other activations, RNNprop also generalizes well as shown in Table 2."}, {"heading": "5.3. Generalization to Deeper MLP", "text": "In deep neural networks, different layers may have different optimal learning rates, but traditional algorithms only have one global learning rate for all the parameters. Our RNN optimizer can achieve better performance benefited from\nits more adaptive behavior.\nWe tested the optimizers on deeper MLPs. More hidden layers are added to the base MLP, all of which have 20 hidden units and use sigmoid as activation function. As shown in Figure 6, RNNprop can always outstrip traditional algorithms until the MLP becomes too deep and none of them can decrease its loss in 100 steps. Figure 5 shows the loss curves on the MLP with 5 hidden layers as an example."}, {"heading": "5.4. Generalization to Different Structures", "text": ""}, {"heading": "5.4.1. CNN", "text": "The CNN optimizees are the cross-entropy losses of convolutional neural networks (CNN) with similar structure as VGGNet (Simonyan & Zisserman, 2015) on dataset MNIST or dataset CIFAR-10. All convolutional layers use 3\u00d73 filters and the window of each max-pooling layer is of size 2\u00d72 with stride 2. We use c to denote a convolutional layer, p to denote a max-pooling layer and f to denote a fully-connected layer. Three CNNs are used in the experiments: CNN with structure c-c-p-f on MNIST, CNN\nwith structure c-c-p-c-c-p-f-f on MNIST and CNN with structure c-c-p-f on CIFAR-10.\nThe results are shown in Figure 7. RNNprop can outperform traditional algorithms on CNN with structure c-c-p-f on dataset MNIST. On the other two CNNs, only the best traditional algorithm outperforms RNNprop. Even though (Andrychowicz et al., 2016) showed that DMoptimizer that is trained on CNNs can train CNNs faster than traditional algorithms, in our experiments DMoptimizer fails to train any of the CNNs when the training set is fixed to the base MLP."}, {"heading": "5.4.2. LSTM", "text": "The optimizers are also tested on the mean squared loss of an LSTM with hidden state size 20 on a simple task: given a sequence f(0), . . . , f(9) with additive noise, the LSTM needs to predict the value of f(10). Here f(x) = A sin(\u03c9x+\u03c6). When generating the dataset, we uniformly randomly choose A \u223c U(0, 10), \u03c9 \u223c U(0, \u03c0/2), \u03c6 \u223c U(0, 2\u03c0), and we draw the noise from the Gaussian distribution N(0, 0.1).\nEven though the task is completely different from the task that is used for training, RNNprop still has comparable or even better performance than traditional algorithms, which may be due to the fact the structure inside LSTM is similar to that of the base MLP with sigmoid in between.\nWe also adjust the settings of the task. As shown in Figure 3, RNNprop still achieve good results when we use a smaller noise from the distribution N(0, 0.01) or use a two-layer LSTM instead of one-layer."}, {"heading": "5.5. Control Experiment", "text": "To assess the effectiveness of each contribution separately, we also trained three more RNN optimizers: DMoptimizer trained with the two tricks and two RNNprop, each trained with one of the two tricks respectively.\nRecall that the trick of combining with convex function aims to accelerate the training of RNN optimizers. We test the performance of RNNprop whose own parameters are trained for different numbers of iterations, with or without this trick. The result is shown in Table 4. With this trick RNN optimizer can achieve a good result with fewer iterations of training.\nTo assess the other two contributions, we select the trained optimizers in the same way as RNNprop. In Figure 9, we test their performances on the base MLP with activation replaced with ReLU for 1000 steps. From the figure, we conclude that Random Scaling is the most effective trick."}, {"heading": "6. Conclusion", "text": "In this paper, we present a new learning-to-learn model with several useful tricks. We show that our new optimizer has better generalization ability than the state-of-art learning-to-learn optimizers. After trained using a simple MLP, our new optimizer achieves better or comparable performance with traditional optimization algorithms when training more complex neural networks or when training for longer horizons.\nWe believe it is possible to further improve the generalization ability of our optimizer. Indeed, on some tasks in our experiments, our optimizer did not outperform the best traditional optimization algorithms, in particular when training for much longer horizon or when training neural networks on different datasets. In the future, we aim to further develop a more generic optimizer with more elaborate designing, so that it can achieve better performance on a wider range of tasks that are analogous with the optimizee used in training."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Metalearning: Applications to Data Mining", "author": ["P. Brazdil", "C.G. Carrier", "C. Soares", "R. Vilalta"], "venue": null, "citeRegEx": "Brazdil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brazdil et al\\.", "year": 2008}, {"title": "Learning to learn for global optimization of black box functions", "author": ["Y. Chen", "M.W. Hoffman", "S.G. Colmenarejo", "M. Denil", "T.P. Lillicrap", "N. de Freitas"], "venue": "arXiv preprint arXiv:1611.03824,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Fixed-weight networks can learn", "author": ["N.E. Cotter", "P.R. Conwell"], "venue": "In IJCNN International Joint Conference on Neural Networks,", "citeRegEx": "Cotter and Conwell,? \\Q1990\\E", "shortCiteRegEx": "Cotter and Conwell", "year": 1990}, {"title": "Learning step size controllers for robust neural network training", "author": ["C. Daniel", "J. Taylor", "S. Nowozin"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Using deep q-learning to control optimization hyperparameters", "author": ["S. Hansen"], "venue": "arXiv preprint arXiv:1602.04062,", "citeRegEx": "Hansen,? \\Q2016\\E", "shortCiteRegEx": "Hansen", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["S. Hochreiter", "A. Younger", "P. Conwell"], "venue": "International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning to optimize", "author": ["K. Li", "J. Malik"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Li and Malik,? \\Q2017\\E", "shortCiteRegEx": "Li and Malik", "year": 2017}, {"title": "Meta-neural networks that learn by learning", "author": ["D.K. Naik", "R.J. Mammone"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Naik and Mammone,? \\Q1992\\E", "shortCiteRegEx": "Naik and Mammone", "year": 1992}, {"title": "Adaptive behavior with fixed weights in rnn: an overview", "author": ["D.V. Prokhorov", "L.A. Feldkarnp", "I.Y. Tyukin"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Prokhorov et al\\.,? \\Q2018\\E", "shortCiteRegEx": "Prokhorov et al\\.", "year": 2018}, {"title": "Simple Principles of Metalearning", "author": ["J. Schmidhuber", "J. Zhao", "M. Wiering"], "venue": "Istituto Dalle Molle Di Studi Sull\u2019Intelligenza Artificiale,", "citeRegEx": "Schmidhuber et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1999}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Tseng,? \\Q1998\\E", "shortCiteRegEx": "Tseng", "year": 1998}, {"title": "A perspective view and survey of meta-learning", "author": ["R. Vilalta", "Y. Drissi"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Vilalta and Drissi,? \\Q2002\\E", "shortCiteRegEx": "Vilalta and Drissi", "year": 2002}, {"title": "Learning to reinforcement learn", "author": ["J.X. Wang", "Z. Kurthnelson", "D. Tirumala", "H. Soyer", "J.Z. Leibo", "R. Munos", "C. Blundell", "D. Kumaran", "M. Botvinick"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Fixedweight on-line learning", "author": ["A.S. Younger", "P.R. Conwell", "N.E. Cotter"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Younger et al\\.", "year": 1999}, {"title": "Metalearning with backpropagation", "author": ["A.S. Younger", "S. Hochreiter", "P.R. Conwell"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Younger et al\\.", "year": 2001}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "However, there is no agreement on the exact definition of meta-learning, and various concepts have been developed by different authors (Thrun & Pratt, 1998; Vilalta & Drissi, 2002; Brazdil et al., 2008).", "startOffset": 135, "endOffset": 202}, {"referenceID": 21, "context": "This idea was further developed in (Younger et al., 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems.", "startOffset": 35, "endOffset": 82}, {"referenceID": 9, "context": "This idea was further developed in (Younger et al., 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems.", "startOffset": 35, "endOffset": 82}, {"referenceID": 2, "context": "In another recent paper (Chen et al., 2016), an RNN is used to take current position and value as input and outputs the next position, and it works well for black-box optimization and simple RL tasks.", "startOffset": 24, "endOffset": 43}, {"referenceID": 14, "context": "From a reinforcement learning perspective, the optimizer can be viewed as a policy which takes the current state as input and output the next action (Schmidhuber et al., 1999).", "startOffset": 149, "endOffset": 175}, {"referenceID": 5, "context": "Two recent papers (Daniel et al., 2016; Hansen, 2016) trained adaptive controllers to adjust the hyperparameters (learning rate) of traditional optimization algorithms from this perspective.", "startOffset": 18, "endOffset": 53}, {"referenceID": 7, "context": "Two recent papers (Daniel et al., 2016; Hansen, 2016) trained adaptive controllers to adjust the hyperparameters (learning rate) of traditional optimization algorithms from this perspective.", "startOffset": 18, "endOffset": 53}, {"referenceID": 19, "context": "More general methods have been introduced in (Li & Malik, 2017; Wang et al., 2016) which also take the RL perspective and train a neural network to model a policy.", "startOffset": 45, "endOffset": 82}, {"referenceID": 8, "context": "The usage of another neural network to direct the training of neural networks has been put forward by Naik and Mammone (1992). In their early work, Cotter and Younger (1990; 1999) argued that RNNs can be used to model adaptive optimization algorithms (Prokhorov et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 6, "context": ", 2001; Hochreiter et al., 2001) and gradient descent is used to train an RNN optimizer on convex problems. Recently, as shown in Section 1.1, Andrychowicz et al. (2016) proposed a more general optimizer model using LSTM to learn gradient descent, and our work directly follows their work.", "startOffset": 8, "endOffset": 170}, {"referenceID": 17, "context": "A great number of optimization algorithms have been proposed to improve the performance of vanilla gradient descent, including Momentum(Tseng, 1998), Adagrad(Duchi et al.", "startOffset": 135, "endOffset": 148}, {"referenceID": 6, "context": "A great number of optimization algorithms have been proposed to improve the performance of vanilla gradient descent, including Momentum(Tseng, 1998), Adagrad(Duchi et al., 2011), Adadelta(Zeiler, 2012), RMSprop(Tieleman & Hinton, 2012), Adam(Kingma & Ba, 2015).", "startOffset": 157, "endOffset": 177}, {"referenceID": 22, "context": ", 2011), Adadelta(Zeiler, 2012), RMSprop(Tieleman & Hinton, 2012), Adam(Kingma & Ba, 2015).", "startOffset": 17, "endOffset": 31}, {"referenceID": 3, "context": "The input is preprocessed by a fully-connected layer with ELU (Exponential Linear Unit) as the activation function (Clevert et al., 2015) before being handled by the RNN.", "startOffset": 115, "endOffset": 137}, {"referenceID": 0, "context": "Their performances are compared with the best traditional optimization algorithms whose learning rates are carefully chosen and other hyperparameters are set to the default values in Tensorflow (Abadi et al., 2016).", "startOffset": 194, "endOffset": 214}], "year": 2017, "abstractText": "Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.", "creator": "LaTeX with hyperref package"}}}