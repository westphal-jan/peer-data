{"id": "1703.04783", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Multichannel End-to-end Speech Recognition", "abstract": "the mathematical field of speech recognition is in the midst of a paradigm shift : end - to - long end neural networks are developing challenging the dominance of hidden markov models as a core technology. using an attention mechanism in a conventional recurrent encoder - decoder architecture solves the dynamic time alignment problem, allowing joint direct end - to - end training of the acoustic and language modeling components. in this paper we extend the end - to - end framework to encompass microphone array signal processing for noise suppression and speech enhancement within via the acoustic encoding network. this result allows the beamforming components to each be optimized jointly within the recognition architecture to improve the end - to - end speech audio recognition objective. experiments on the noisy speech benchmarks ( chime - 4 and ami ) show that our multichannel end - to - end system outperformed the attention - based baseline with input from a conventional laser adaptive beamformer.", "histories": [["v1", "Tue, 14 Mar 2017 22:28:51 GMT  (2107kb,D)", "http://arxiv.org/abs/1703.04783v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["tsubasa ochiai", "shinji watanabe", "takaaki hori", "john r hershey"], "accepted": true, "id": "1703.04783"}, "pdf": {"name": "1703.04783.pdf", "metadata": {"source": "META", "title": "Multichannel End-to-end Speech Recognition ", "authors": ["Tsubasa Ochiai", "Shinji Watanabe", "Takaaki Hori", "John R. Hershey"], "emails": ["EUP1105@MAIL4.DOSHISHA.AC.JP", "WATANABE@MERL.COM", "THORI@MERL.COM", "HERSHEY@MERL.COM"], "sections": [{"heading": "1. Introduction", "text": "Existing automatic speech recognition (ASR) systems are based on a complicated hybrid of separate components, including acoustic, phonetic, and language models (Jelinek, 1976). Such systems are typically based on deep neural network acoustic models combined with hidden Markov models to represent the language and phonetic contextdependent state and their temporal alignment with the acoustic signal (DNN-HMM) (Bourlard & Morgan, 1994; Hinton et al., 2012). As a simpler alternative, end-to-end speech recognition paradigm has attracted great research\ninterest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015). This paradigm simplifies the above hybrid architecture by subsuming it into a single neural network. Specifically, an attentionbased encoder-decoder framework (Chorowski et al., 2014) integrates all of those components using a set of recurrent neural networks (RNN), which map from acoustic feature sequences to character label sequences.\nHowever, existing end-to-end frameworks have focused on clean speech, and do not include speech enhancement, which is essential to good performance in noisy environments. For example, recent industrial applications (e.g., Amazon echo) and benchmark studies (Barker et al., 2016; Kinoshita et al., 2016) show that multichannel speech enhancement techniques, using beamforming methods, produce substantial improvements as a pre-processor for conventional hybrid systems, in the presence of strong background noise. In light of the above trends, this paper extends the existing attention-based encoder-decoder framework by integrating multichannel speech enhancement. Our proposed multichannel end-to-end speech recognition framework is trained to directly translate from multichannel acoustic signals to text.\nA key concept of the multichannel end-to-end framework is to optimize the entire inference procedure, including the beamforming, based on the final ASR objectives, such as word/character error rate (WER/CER). Traditionally, beamforming techniques such as delay-and-sum and filterand-sum are optimized based on a signal-level loss function, independently of speech recognition task (Benesty et al., 2008; Van Veen & Buckley, 1988). Their use in ASR requires ad-hoc modifications such as Wiener post-filtering or distortionless constraints, as well as steering mechanisms determine a look direction to focus the beamformer on the target speech (Wo\u0308lfel & McDonough, 2009). In contrast, our framework incorporates recently proposed neural beamforming mechanisms as a differentiable component to allow joint optimization of the multichannel speech\nar X\niv :1\n70 3.\n04 78\n3v 1\n[ cs\n.S D\n] 1\n4 M\nar 2\nenhancement within the end-to-end system to improve the ASR objective.\nRecent studies on neural beamformers can be categorized into two types: (1) beamformers with a filter estimation network (Xiao et al., 2016a; Li et al., 2016) and (2) beamformers with a mask estimation network (Heymann et al., 2016; Erdogan et al., 2016). Both methods obtain an enhanced signal based on the formalization of the conventional filter-and-sum beamformer in the time-frequency domain. The main difference between them is how the multichannel filters are produced by the neural network. In the former approach, the multichannel filter coefficients are direct outputs of the network. In the latter approach, a network first estimates time-frequency masks, which are used to compute expected speech and noise statistics. Then, using these statistics, the filter coefficients are computed based on the well-known MVDR (minimum variance distortionless response) formalization (Capon, 1969). In both approaches, the estimated filter coefficients are then applied to the multichannel noisy signal to enhance the speech signal. Note that the mask estimation approach has the advantage of leveraging well-known techniques, but it requires parallel data composed of aligned clean and noisy speech, which are usually difficult to obtain without data simulation.\nRecently, it has been reported that the mask estimationbased approaches (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016) achieve great performance in noisy speech recognition benchmarks (e.g., CHiME 3 and 4 challenges)1. Although this paper proposes to incorporate both mask and filter estimation approaches in an endto-end framework, motivated by those successes, we focus more on the mask estimation, implementing it along with the MVDR estimation as a differentiable network. Our MVDR formulation estimates the speech image at the reference microphone and includes selection of the reference microphone using an attention mechanism. By using channel-independent mask estimation along with this reference selection, the model can generalize to different microphone array geometries (number of channels, microphone locations, and ordering), unlike the filter estimation approach. Finally, because the masks are latent variables in the end-to-end training, we no longer need parallel clean and noisy speech.\nThe main advantages of our proposed multichannel end-toend speech recognition system are:\n1. Overall inference from speech enhancement to recognition is jointly optimized for the ASR objective.\n1Yoshioka et al. 2015 uses a clustering technique to perform mask estimation rather than the neural network-based techniques, but it uses the same MVDR formulation for filter estimation.\n2. The trained system can be used for input signals with arbitrary number and order of channels.\n3. Parallel clean and noisy data are not required. We can optimize the speech enhancement component with noisy signals and their transcripts."}, {"heading": "2. Overview of attention-based encoder-decoder networks", "text": "This section explains a conventional attention-based encoder-decoder framework, which is used to directly deal with variable length input and output sequences. The framework consists of two RNNs, called encoder and decoder respectively, and an attention mechanism, which connects the encoder and decoder, as shown in Figure 1. Given a T -length sequence of input features O = {ot \u2208 RDO |t = 1, \u00b7 \u00b7 \u00b7 , T}, the network generates an N -length sequence of output labels Y = {yn \u2208 V|n = 1, \u00b7 \u00b7 \u00b7 , N}, where ot is a DO-dimensional feature vector (e.g., log Mel filterbank) at input time step t, and yn is a label symbol (e.g., character) at output time step n in label set V .\nFirst, given an input sequence O, the encoder network transforms it to an L-length high-level feature sequence H = {hl \u2208 RDH |l = 1, \u00b7 \u00b7 \u00b7 , L}, where hl is a DHdimensional state vector at a time step l of encoder\u2019s top layer. In this work, the encoder network is composed of a bidirectional long short-term memory (BLSTM) recurrent network. To reduce the input sequence length, we apply a subsampling technique (Bahdanau et al., 2016) to some layers. Therefore, l represents the frame index subsampled from t and L is less than T .\nNext, the attention mechanism integrates all encoder outputs H into a DH-dimensional context vector cn \u2208 RDH\nbased on an L-dimensional attention weight vector an \u2208 [0, 1]L, which represents a soft alignment of encoder outputs at an output time step n. In this work, we adopt a location-based attention mechanism (Chorowski et al., 2015), and an and cn are formalized as follows:\nfn = F \u2217 an\u22121, (1) kn,l = w Ttanh(VSsn + V Hhl + V Ffn,l + b), (2)\nan,l = exp(\u03b1kn,l)\u2211L l=1 exp(\u03b1kn,l) , cn = L\u2211 l=1 an,lhl, (3)\nwhere w \u2208 R1\u00d7DW , VH \u2208 RDW\u00d7DH , VS \u2208 RDW\u00d7DS , VF \u2208 RDW\u00d7DF are trainable weight matrices, b \u2208 RDW is a trainable bias vector, F \u2208 RDF\u00d71\u00d7Df is a trainable convolution filter. sn \u2208 RDS is a DS-dimensional hidden state vector obtained from an upper decoder network at n, and \u03b1 is a sharpening factor (Chorowski et al., 2015). \u2217 denotes the convolution operation.\nThen, the decoder network incrementally updates a hidden state sn and generates an output label yn as follows:\nsn = Update(sn\u22121, cn\u22121, yn\u22121), (4) yn = Generate(sn, cn), (5)\nwhere the Generate(\u00b7) and Update(\u00b7) functions are composed of a feed forward network and an LSTM-based recurrent network, respectively.\nNow, we can summarize these procedures as follows: P (Y |O) = \u220f n P (yn|O, y1:n\u22121), (6)\nH = Encoder(O), (7) cn = Attention(an\u22121, sn, H), (8) yn = Decoder(cn, y1:n\u22121), (9)\nwhere Encoder(\u00b7) = BLSTM(\u00b7), Attention(\u00b7) corresponds to Eqs. (1)-(3), and Decoder(\u00b7) corresponds to Eqs. (4) and (5). Here, special tokens for start-of-sentence (sos) and end-of-sentence (eos) are added to the label set V . The decoder starts the recurrent computation with the (sos) label and continues to generate output labels until the (eos) label is emitted. Figure 1 illustrates such procedures.\nBased on the cross-entropy criterion, the loss function is defined using Eq. (6) as follows:\nL = \u2212 lnP (Y \u2217|O) = \u2212 \u2211 n lnP (y\u2217n|O, y\u22171:n\u22121), (10)\nwhere Y \u2217 is the ground truth of a whole sequence of output labels and y\u22171:n\u22121 is the ground truth of its subsequence until an output time step n\u2212 1.\nIn this framework, the whole networks including the encoder, attention, and decoder can be optimized to generate\nthe correct label sequence. This consistent optimization of all relevant procedures is the main motivation of the endto-end framework."}, {"heading": "3. Neural beamformers", "text": "This section explains neural beamformer techniques, which are integrated with the encoder-decoder network in the following section. This paper uses frequency-domain beamformers rather than time-domain ones, which achieve significant computational complexity reduction in multichannel neural processing (Li et al., 2016; Sainath et al., 2016). In the frequency domain representation, a filter-and-sum beamformer obtains an enhanced signal as follows:\nx\u0302t,f = C\u2211 c=1 gt,f,cxt,f,c, (11)\nwhere xt,f,c \u2208 C is an STFT coefficient of c-th channel noisy signal at a time-frequency bin (t, f). gt,f,c \u2208 C is a corresponding beamforming filter coefficient. x\u0302t,f \u2208 C is an enhanced STFT coefficient, and C is the numbers of channels.\nIn this paper, we adopt two types of neural beamformers, which basically follow Eq. (11); 1) filter estimation network and 2) mask estimation network. Figure 2 illustrates the schematic structure of each approach. The main difference between them is how to compute the filter coefficient gt,f,c. The following subsections describe each approach."}, {"heading": "3.1. Filter estimation network approach", "text": "The filter estimation network directly estimates a timevariant filter coefficients {gt,f,c}T,F,Ct=1,f=1,c=1 as the outputs of the network, which was originally proposed in (Li et al., 2016). F is the dimension of STFT features.\nThis approach uses a single real-valued BLSTM network to predict the real and imaginary parts of the complex-valued filter coefficients at an every time step. Therefore, we introduce multiple (2 \u00d7 C) output layers to separately compute the real and imaginary parts of the filter coefficients for each channel. Then, the network outputs time-variant filter coefficients gt,c = {gt,f,c}Ff=1 \u2208 CF at a time step t for c-th channel as follows;\nZ = BLSTM({x\u0304t}Tt=1), (12) <(gt,c) = tanh(W<c zt + b<c ), (13) =(gt,c) = tanh(W=c zt + b=c ), (14)\nwhere Z = {zt \u2208 RDZ |t = 1, \u00b7 \u00b7 \u00b7 , T}is a sequence of DZdimensional output vectors of the BLSTM network. x\u0304t = {<(xt,f,c),=(xt,f,c)}F,Cf=1,c=1 \u2208 R2FC is an input feature of a 2FC-dimensional real-value vector for the BLSTM network. This is obtained by concatenating the real and imaginary parts of all STFT coefficients in all channels. <(gt,c) and =(gt,c) is the real and imaginary part of filter coefficients, W<c \u2208 RF\u00d7DZ and W=c \u2208 RF\u00d7DZ are the weight matrices of the output layer for c-th channel, and b<c \u2208 RF and b=c \u2208 RF are their corresponding bias vectors. Using the estimated filters gt,c, the enhanced STFT coefficients x\u0302t,f are obtained based on Eq. (11).\nThis approach has several possible problems due to its formalization. The first issue is the high flexibility of the estimated filters {gt,f,c}T,F,Ct=1,f=1,c=1, which are composed of a large number of unconstrained variables (2TFC) estimated from few observations. This causes problems such as training difficulties and over-fitting. The second issue is that the network structure depends on the number and order of channels. Therefore, a new filter estimation network has to be trained when we change microphone configurations."}, {"heading": "3.2. Mask estimation network approach", "text": "The key point of the mask estimation network approach is that it constrains the estimated filters based on wellfounded array signal processing principles. Here, the network estimates the time-frequency masks, which are used to compute the time-invariant filter coefficients {gf,c}F,Cf=1,c=1 based on the MVDR formalizations. This is the main difference between this approach and the filter estimation network approach described in Section 3.1. Also, mask-based beamforming approaches have achieved great performance in noisy speech recognition benchmarks (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016). Therefore, this paper proposes to use a maskbased MVDR beamformer, where overall procedures are formalized as a differentiable network for the subsequent end-to-end speech recognition system. Figure 3 summarizes the overall procedures to compute the filter coefficients, which is a detailed flow of Figure 2 (b)."}, {"heading": "3.2.1. MASK-BASED MVDR FORMALIZATION", "text": "One of the MVDR formalizations computes the timeinvariant filter coefficients g(f) = {gf,c}Cc=1 \u2208 CC in Eq. (11) as follows (Souden et al., 2010):\ng(f) = \u03a6N(f)\u22121\u03a6S(f)\nTr(\u03a6N(f)\u22121\u03a6S(f)) u, (15)\nwhere \u03a6S(f) \u2208 CC\u00d7C and \u03a6N(f) \u2208 CC\u00d7C are the crosschannel power spectral density (PSD) matrices (also known as spatial covariance matrices) for speech and noise signals, respectively. u \u2208 RC is the one-hot vector representing a reference microphone, and Tr(\u00b7) is the matrix trace operation. Note that although the formula contains a matrix inverse, the number of channels is relatively small, and so the forward pass and derivatives can be efficiently computed.\nBased on (Yoshioka et al., 2015; Heymann et al., 2016), the PSD matrices are robustly estimated using the expectation with respect to time-frequency masks as follows:\n\u03a6S(f) = 1\u2211T\nt=1m S t,f T\u2211 t=1 mSt,fxt,fx \u2020 t,f , (16)\n\u03a6N(f) = 1\u2211T\nt=1m N t,f T\u2211 t=1 mNt,fxt,fx \u2020 t,f , (17)\nwhere xt,f = {xt,f,c}Cc=1 \u2208 CC is the spatial vector of an observed signal for each time-frequency bin, mSt,f \u2208 [0, 1] and mNt,f \u2208 [0, 1] are the time-frequency masks for speech and noise, respectively. \u2020 represents the conjugate transpose."}, {"heading": "3.2.2. MASK ESTIMATION NETWORK", "text": "In the mask estimation network approach, we use two realvalued BLSTM networks; one for a speech mask and the other for a noise mask. Each network outputs the timefrequency mask as follows:\nZSc = BLSTM S({x\u0304t,c}Tt=1), (18)\nmSt,c = sigmoid(W SzSt,c + b S), (19)\nZNc = BLSTM N({x\u0304t,c}Tt=1), (20)\nmNt,c = sigmoid(W NzNt,c + b N), (21)\nwhere ZSc = {zSt,c \u2208 RDZ |t = 1, \u00b7 \u00b7 \u00b7 , T} is the output sequence of DZ-dimensional vectors of the BLSTM network to obtain a speech mask over c-th channel\u2019s input STFTs. ZNc is the BLSTM output sequence for a noise mask. x\u0304t,c = {<(xt,f,c),=(xt,f,c)}Ff=1 \u2208 R2F is an input feature of a 2F -dimensional real-value vector. This is obtained by concatenating the real and imaginary parts of all STFT features at c-th channel. mSt,c = {mSt,f,c}Ff=1 \u2208 [0, 1]F and mNt,c are the estimated speech and noise masks for every c-th channel at a time step t, respectively. WS,WN \u2208 RF\u00d7DZ are the weight matrices of the output layers to finally output speech and noise masks, respectively, and bS,bN \u2208 RF are their corresponding bias vectors.\nAfter computing the speech and noise masks for each channel, the averaged masks are obtained as follows:\nmSt = 1\nC C\u2211 c=1 mSt,c, m N t = 1 C C\u2211 c=1 mNt,c. (22)\nWe use these averaged masks to estimate the PSD matrices as described in Eqs. (16) and (17). The MVDR beamformer through this BLSTM mask estimation is originally proposed in (Heymann et al., 2016), but our neural beamformer further extends it with attention-based reference selection, which is described in the next subsection."}, {"heading": "3.2.3. ATTENTION-BASED REFERENCE SELECTION", "text": "To incorporate the reference microphone selection in a neural beamformer framework, we use a soft-max for the vector u in Eq. (15) derived from an attention mechanism. In this approach, the reference microphone vector u is estimated from time-invariant feature vectors qc and rc as follows:\nk\u0303c = v Ttanh(VQqc + V Rrc + b\u0303), (23) uc = exp(\u03b2k\u0303c)\u2211C c=1 exp(\u03b2k\u0303c) , (24)\nwhere v \u2208 R1\u00d7DV ,VZ \u2208 RDV\u00d72DZ ,VR \u2208 RDV\u00d72F are trainable weight parameters, b\u0303 \u2208 RDV is a trainable bias vector. \u03b2 is the sharpening factor. We use two types of\nfeatures; 1) the time-averaged state vector qc \u2208 R2DZ extracted from the BLSTM networks for speech and noise masks in Eqs. (18) and (20), i.e.,\nqc = 1\nT T\u2211 t=1 {zSt,c, zNt,c}, (25)\nand 2) the PSD feature rc \u2208 R2F , which incorporates the spatial information into the attention mechanism. The following equation represents how to compute rc:\nrc = 1\nC \u2212 1 C\u2211 c\u2032=1,c\u2032 6=c {<(\u03c6Sf,c,c\u2032),=(\u03c6Sf,c,c\u2032)}Ff=1, (26)\nwhere \u03c6Sf,c,c\u2032 \u2208 C is the entry in c-th row and c\u2032-th column of the speech PSD matrix \u03a6S(f) in Eq. (16). The PSD matrix represents correlation information between channels. To select a reference microphone, the spatial correlation related to speech signals is more informative, and therefore, we only use the speech PSD matrix \u03a6S(f) as a feature.\nNote that, in this mask estimation based MVDR beamformer, masks for each channel are computed separately using the same BLSTM network unlike Eq. (12), and the mask estimation network is independent of channels. Similarly, the reference selection network is also independent of channels, and the beamformer deals with input signals with arbitrary number and order of channels without re-training or re-configuration of the network."}, {"heading": "4. Multichannel end-to-end ASR", "text": "In this work, we propose a multichannel end-to-end speech recognition, which integrates all components with a single neural architecture. We adopt neural beamformers (Section 3) as a speech enhancement part, and the attention-based encoder-decoder (Section 2) as a speech recognition part.\nThe entire procedure to generate the sequence of output labels Y\u0302 from the multichannel inputs {Xc}Cc=1 is formalized as follows:\nX\u0302 = Enhance({Xc}Cc=1), (27) O\u0302 = Feature(X\u0302), (28)\nH\u0302 = Encoder(O\u0302), (29)\nc\u0302n = Attention(a\u0302n\u22121, s\u0302n, H\u0302), (30) y\u0302n = Decoder(c\u0302n, y\u03021:n\u22121). (31)\nEnhance(\u00b7) is a speech enhancement function realized by the neural beamformer based on Eq. (11) with the filter or mask estimation network (Section 3.1 or 3.2).\nFeature(\u00b7) is a feature extraction function. In this work, we use a normalized log Mel filterbank transform to obtain\no\u0302t \u2208 RDO computed from the enhanced STFT coefficients x\u0302t \u2208 CF as an input of attention-based encoder-decoder:\npt = {<(x\u0302t,f )2 + =(x\u0302t,f )2}Ff=1, (32) o\u0302t = Norm(log(Mel(pt))), (33)\nwhere pt \u2208 RF is a real-valued vector of the power spectrum of the enhanced signal at a time step t, Mel(\u00b7) is the operation of DO \u00d7 F Mel matrix multiplication, and Norm(\u00b7) is the operation of global mean and variance normalization so that its mean and variance become 0 and 1.\nEncoder(\u00b7), Attention(\u00b7), and Decoder(\u00b7) are defined in Eqs. (7), (8), and (9), respectively, with the sequence of the enhanced log Mel filterbank like features O\u0302 as an input.\nThus, we can build a multichannel end-to-end speech recognition system, which converts multichannel speech signals to texts with a single network. Note that because all procedures, such as enhancement, feature extraction, encoder, attention, and decoder, are connected with differentiable graphs, we can optimize the overall inference to generate a correct label sequence.\nRelation to prior works\nThere have been several related studies of neural beamformers based on the filter estimation (Li et al., 2016; Xiao et al., 2016a) and the mask estimation (Heymann et al., 2016; Erdogan et al., 2016; Xiao et al., 2016b). The main difference is that such preceding studies use a component-level training objective within the conventional hybrid frameworks, while our work focuses on the entire end-to-end objective. For example, Heymann et al., 2016; Erdogan et al., 2016 use a signal-level objective (binary mask classification or regression) to train a network given parallel clean and noisy speech data. Li et al., 2016; Xiao et al., 2016a;b use ASR objectives (HMM state classification or sequence discriminative training), but they are still based on the hybrid approach. Speech recognition with raw multichannel waveforms (Hoshen et al., 2015; Sainath et al., 2016) can also be seen as using a neural beamformer, where the filter coefficients are represented as network parameters, but again these methods are still based on the hybrid approach.\nAs regards end-to-end speech recognition, all existing studies are based on a single channel setup. For example, most studies focus on a standard clean speech recognition setup without speech enhancement. (Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016). Amodei et al., 2016 discusses endto-end speech recognition in a noisy environment, but this method deals with the noise robustness by preparing various types of simulated noisy speech for training data, and\ndoes not incorporate multichannel speech enhancement in their networks."}, {"heading": "5. Experiments", "text": "We study the effectiveness of our multichannel end-toend system compared to a baseline end-to-end system with noisy speech or beamformed inputs. We use the two multichannel speech recognition benchmarks, CHiME-4 (Vincent et al., 2016) and AMI (Hain et al., 2007).\nCHiME-4 is a speech recognition task in public noisy environments, consisting of speech recorded using a tablet device with 6-channel microphones. It consists of real and simulated data. The training set consists of 3 hours of real speech data uttered by 4 speakers and 15 hours of simulation speech data uttered by 83 speakers. The development set consists of 2.9 hours of real and simulation speech data uttered by 4 speakers, respectively. The evaluation set consists of 2.2 hours of real and simulation speech data uttered by 4 speakers, respectively. We excluded the 2nd channel signals, which is captured at the microphone located on the backside of the tablet, and used 5 channels for the following multichannel experiments (C = 5).\nAMI is a speech recognition task in meetings, consisting of speech recorded using 8-channel circular microphones (C = 8). It consists of only real data. The training set consists of about 78 hours of speech data uttered by 135 speakers. the development and evaluation sets consist of about 9 hours of speech data uttered by 18 and 16 speakers, respectively. The amount of training data (i.e., 78 hours) is larger than one for CHiME-4 (i.e., 18 hours), and we mainly used CHiME-4 data to demonstrate our experiments."}, {"heading": "5.1. Configuration", "text": ""}, {"heading": "5.1.1. ENCODER-DECODER NETWORKS", "text": "We used 40-dimensional log Mel filterbank coefficients as an input feature vector for both noisy and enhanced speech signals (DO = 40). In this experiment, we used 4-layer BLSTM with 320 cells in the encoder (DH = 320), and 1-layer LSTM with 320 cells in the decoder (DS = 320). In the encoder, we subsampled the hidden states of the first and second layers and used every second of hidden states for the subsequent layer\u2019s inputs. Therefore, the number of hidden states at the encoder\u2019s output layer is reduced to L = T/4. After every BLSTM layer, we used a linear projection layer with 320 units to combine the forward and backward LSTM outputs. For the attention mechanism, 10 centered convolution filters (DF = 10) of width 100 (Df = 100) were used to extract the convolutional features. We set the attention inner product dimension as 320 (DW = 320), and used the sharpening factor \u03b1 = 2. To boost the optimization in a noisy environment, we adopted a joint\nCTC-attention multi-task loss function (Kim et al., 2016), and set the CTC loss weight as 0.1.\nFor decoding, we used a beam search algorithm similar to (Sutskever et al., 2014) with the beam size 20 at each output step to reduce the computation cost. CTC scores were also used to re-score the hypotheses with 0.1 weight. We adopted a length penalty term (Chorowski et al., 2015) to the decoding objective and set the penalty weight as 0.3. In the CHiME-4 experiments, we only allowed the hypotheses whose length were within 0.3\u00d7L and 0.75\u00d7L during decoding, while the hypothesis lengths in the AMI experiments were automatically determined based on the above scores. Note that we pursued a pure end-to-end setup without using any external lexicon or language models, and used CER as an evaluation metric."}, {"heading": "5.1.2. NEURAL BEAMFORMERS", "text": "256 STFT coefficients and the offset were computed from 25ms-width hamming window with 10ms shift (F = 257). Both filter and mask estimation network approaches used similar a 3-layer BLSTM with 320 cells (DZ = 320) without the subsampling technique. For the reference selection attention mechanism, we used the same attention inner product dimension (DV = 320) and sharpening factor \u03b2 = 2 as those of the encoder-decoder network."}, {"heading": "5.1.3. SHARED CONFIGURATIONS", "text": "All the parameters are initialized with the range [-0.1, 0.1] of a uniform distribution. We used the AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2013) for optimization. We initialized the AdaDelta hyperparameters \u03c1 = 0.95 and = 1\u22128. Once the loss over the validation set was degraded, we decreased the AdaDelta hyperparameter by multiplying it by 0.01 at each subsequent epoch. The training procedure was stopped after 15 epochs. During the training, we adopted multi-condition training strategy, i.e., in addition to the optimization with the enhanced features through the neural beamformers, we also used the noisy multichannel speech data as an input of encoder-decoder networks without through the neural beamformers to improve the robustness of the encoderdecoder networks. All the above networks are implemented by using Chainer (Tokui et al., 2015)."}, {"heading": "5.2. Results", "text": "Table 1 shows the recognition performances of CHiME4 with the five systems: NOISY, BEAMFORMIT, FILTER NET, MASK NET (REF), and MASK NET (ATT). NOISY and BEAMFORMIT were the baseline singlechannel end-to-end systems, which did not include the speech enhancement part in their frameworks. Their endto-end networks were trained only with noisy speech data\nby following a conventional multi-condition training strategy (Vincent et al., 2016). During decoding, NOISY used single-channel noisy speech data from \u2019isolated 1ch track\u2019 in CHiME-4 as an input, while BEAMFORMIT used the enhanced speech data obtained from 5-channel signals with BeamformIt (Anguera et al., 2007), which is well-known delay-and-sum beamformer, as an input.\nFILTER NET, MASK NET (REF), and MASK NET (ATT) were the multichannel end-to-end systems described in Section 4. To evaluate the validity of the reference selection, we prepared MASK NET (ATT) based on the maskbased beamformer with attention-based reference selection described in Section 3.2.3, and MASK NET (REF) with 5-th channel as a fixed reference microphone, which is located on the center front of the tablet device.\nTable 1 shows that BEAMFORMIT, FILTER NET, MASK NET (REF), and MASK NET (ATT) outperformed NOISY, which confirms the effectiveness of combining speech enhancement with the attention-based encoderdecoder framework. The comparison of MASK NET (REF) and MASK NET (ATT) validates the use of the attention-based mechanism for reference selection. FILTER NET, which is based on the filter estimation network described in Section 3.1, also improved the performance compared to NOISY, but worse than MASK NET (ATT). This is because it is difficult to optimize the filter estimation network due to a lack of restriction to estimate filter coefficients, and it needs some careful optimization, as suggested by (Xiao et al., 2016a). Finally, MASK NET (ATT) achieved better recognition performance than BEAMFORMIT, which proves the effectiveness of our joint integration rather than a pipe-line combination of speech enhancement and (end-to-end) speech recognition.\nTo further investigate the effectiveness of our proposed multichannel end-to-end framework, we also conducted the experiment on the AMI corpus. Table 2 compares the recognition performance of the three systems: NOISY, BEAMFORMIT, and MASK NET (ATT). In NOISY, we used noisy speech data from the 1st channel in AMI as an input to the system. Table 2 shows that, even in the AMI, our proposed MASK NET (ATT) achieved bet-\nter recognition performance than the attention-based baselines (NOISY and BEAMFORMIT), which also confirms the effectiveness of our proposed multichannel end-to-end framework. Note that BEAMFORMIT was worse than NOISY even with the enhanced signals. This phenomenon is sometimes observed in noisy speech recognition that the distortion caused by sole speech enhancement degrades the performance without re-training. Our end-to-end system jointly optimizes the speech enhancement part with the ASR objective, and can avoid such degradations."}, {"heading": "5.3. Influence on the number and order of channels", "text": "As we discussed in Section 3.2, one unique characteristic of our proposed MASK NET (ATT) is the robustness/invariance against the number and order of channels without re-training. Table 3 shows an influence of the CHiME-4 validation accuracies on the number and order of channels. The validation accuracy was computed conditioned on the ground truth labels y\u22171:n\u22121 in Eq. (10) during decoder\u2019s recursive character generation, which has a strong correlation with CER. The second column of the table represents the channel indices, which were used as an input of the same MASK NET (ATT) network.\nComparison of 5 6 4 3 1 and 3 4 1 5 6 shows that the order of channels did not affect the recognition performance of MASK NET (ATT) at all, as we expected. In addition, even when we used fewer three or four channels as an input, MASK NET (ATT) still outperformed NOISY (single channel). These results confirm that our proposed multichannel end-to-end system can deal with input signals with arbitrary number and order of channels, without any reconfiguration and re-training."}, {"heading": "5.4. Visualization of beamformed features", "text": "To analyze the behavior of our developed speech enhancement component with a neural beamformer, Figure 4 visualizes the spectrograms of the same CHiME-4 utterance with the 5-th channel noisy signal, enhanced signal with BeamformIt, and enhanced signal with our proposed MASK NET (ATT). We could confirm that the BeamformIt and MASK NET (ATT) successfully suppressed the noises comparing to the 5-th channel signal by eliminat-\ning blurred red areas overall. In addition, by focusing on the insides of black boxes, the harmonic structure, which was corrupted in the 5-th channel signal, was recovered in BeamformIt and MASK NET (ATT).\nThis result suggests that our proposed MASK NET (ATT) successfully learned a noise suppression function similar to the conventional beamformer, although it is optimized based on the end-to-end ASR objective, without explicitly using clean data as a target."}, {"heading": "6. Conclusions", "text": "In this paper, we extended an existing attention-based encoder-decoder framework by integrating a neural beamformer and proposed a multichannel end-to-end speech recognition framework. It can jointly optimize the overall inference in multichannel speech recognition (i.e., from speech enhancement to speech recognition) based on the end-to-end ASR objective, and it can generalize to dif-\nferent numbers and configurations of microphones. The experimental results on challenging noisy speech recognition benchmarks, CHiME-4 and AMI, show that the proposed framework outperformed the end-to-end baseline with noisy and delay-and-sum beamformed inputs.\nThe current system still has data sparseness issues due to the lack of lexicon and language models, unlike the conventional hybrid approach. Therefore, the results reported in the paper did not reach the state-of-the-art performance in these benchmarks, but they are still convincing to show the effectiveness of the proposed framework. Our most important future work is to overcome these data sparseness issues by developing adaptation techniques of an end-to-end framework with the incorporation of linguistic resources."}], "references": [{"title": "Acoustic beamforming for speaker diarization of meetings", "author": ["Anguera", "Xavier", "Wooters", "Chuck", "Hernando", "Javier"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Anguera et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Anguera et al\\.", "year": 2011}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "The third \u2019CHiME\u2019 speech separation and recognition challenge: Analysis and outcomes", "author": ["Barker", "Jon", "Marxer", "Ricard", "Vincent", "Emmanuel", "Watanabe", "Shinji"], "venue": "Computer Speech & Language,", "citeRegEx": "Barker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barker et al\\.", "year": 2016}, {"title": "Microphone array signal processing, volume 1", "author": ["Benesty", "Jacob", "Chen", "Jingdong", "Huang", "Yiteng"], "venue": "Springer Science & Business Media,", "citeRegEx": "Benesty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Benesty et al\\.", "year": 2008}, {"title": "Connectionist speech recognition: A hybrid approach", "author": ["Bourlard", "Herv\u00e9", "Morgan", "Nelson"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Bourlard et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bourlard et al\\.", "year": 1994}, {"title": "High-resolution frequency-wavenumber spectrum analysis", "author": ["Capon", "Jack"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Capon and Jack.,? \\Q1969\\E", "shortCiteRegEx": "Capon and Jack.", "year": 1969}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc", "Vinyals", "Oriol"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "End-to-end continuous speech recognition using attention-based recurrent NN: First results", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Improved MVDR beamforming using single-channel mask prediction networks", "author": ["Erdogan", "Hakan", "Hershey", "John R", "Watanabe", "Shinji", "Mandel", "Michael", "Le Roux", "Jonathan"], "venue": "In Interspeech,", "citeRegEx": "Erdogan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Erdogan et al\\.", "year": 2016}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Neural network based spectral mask estimation for acoustic beamforming", "author": ["Heymann", "Jahn", "Drude", "Lukas", "Haeb-Umbach", "Reinhold"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Heymann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heymann et al\\.", "year": 2016}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Hoshen", "Yedid", "Weiss", "Ron J", "Wilson", "Kevin W"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hoshen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoshen et al\\.", "year": 2015}, {"title": "Continuous speech recognition by statistical methods", "author": ["Jelinek", "Frederick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Jelinek and Frederick.,? \\Q1976\\E", "shortCiteRegEx": "Jelinek and Frederick.", "year": 1976}, {"title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning", "author": ["Kim", "Suyoun", "Hori", "Takaaki", "Watanabe", "Shinji"], "venue": "arXiv preprint arXiv:1609.06773,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Neural network adaptive beamforming for robust multichannel speech recognition", "author": ["Li", "Bo", "Sainath", "Tara N", "Weiss", "Ron J", "Wilson", "Kevin W", "Bacchiani", "Michiel"], "venue": "In Interspeech,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Miao", "Yajie", "Gowayyed", "Mohammad", "Metze", "Florian"], "venue": "In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Reducing the computational complexity of multimicrophone acoustic models with integrated feature extraction", "author": ["Sainath", "Tara N", "Narayanan", "Arun", "Weiss", "Ron J", "Variani", "Ehsan", "Wilson", "Kevin W", "Bacchiani", "Michiel", "Shafran", "Izhak"], "venue": "In Interspeech,", "citeRegEx": "Sainath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2016}, {"title": "On optimal frequency-domain multichannel linear filtering for noise reduction", "author": ["Souden", "Mehrez", "Benesty", "Jacob", "Affes", "Sofi\u00e8ne"], "venue": "IEEE Transactions on audio, speech, and language processing,", "citeRegEx": "Souden et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Souden et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems (NIPS)", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Tokui", "Seiya", "Oono", "Kenta", "Hido", "Shohei", "Clayton", "Justin"], "venue": "In Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS,", "citeRegEx": "Tokui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Beamforming: A versatile approach to spatial filtering", "author": ["Van Veen", "Barry D", "Buckley", "Kevin M"], "venue": "IEEE ASSP Magazine,", "citeRegEx": "Veen et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Veen et al\\.", "year": 1988}, {"title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition", "author": ["Vincent", "Emmanuel", "Watanabe", "Shinji", "Nugraha", "Aditya Arie", "Barker", "Jon", "Marxer", "Ricard"], "venue": "Computer Speech & Language,", "citeRegEx": "Vincent et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2016}, {"title": "Distant speech recognition", "author": ["W\u00f6lfel", "Matthias", "McDonough", "John"], "venue": null, "citeRegEx": "W\u00f6lfel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "W\u00f6lfel et al\\.", "year": 2009}, {"title": "A study of learning based beamforming methods for speech recognition", "author": ["Xiao", "Xiong", "Xu", "Chenglin", "Zhang", "Zhaofeng", "Zhao", "Shengkui", "Sun", "Sining", "Watanabe", "Shinji"], "venue": "In CHiME 2016 workshop,", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Zhang", "Yu", "Chan", "William", "Jaitly", "Navdeep"], "venue": "arXiv preprint arXiv:1610.03022,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "As a simpler alternative, end-to-end speech recognition paradigm has attracted great research interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).", "startOffset": 103, "endOffset": 194}, {"referenceID": 6, "context": "As a simpler alternative, end-to-end speech recognition paradigm has attracted great research interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).", "startOffset": 103, "endOffset": 194}, {"referenceID": 16, "context": "As a simpler alternative, end-to-end speech recognition paradigm has attracted great research interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).", "startOffset": 103, "endOffset": 194}, {"referenceID": 7, "context": "Specifically, an attentionbased encoder-decoder framework (Chorowski et al., 2014) integrates all of those components using a set of recurrent neural networks (RNN), which map from acoustic feature sequences to character label sequences.", "startOffset": 58, "endOffset": 82}, {"referenceID": 2, "context": ", Amazon echo) and benchmark studies (Barker et al., 2016; Kinoshita et al., 2016) show that multichannel speech enhancement techniques, using beamforming methods, produce substantial improvements as a pre-processor for conventional hybrid systems, in the presence of strong background noise.", "startOffset": 37, "endOffset": 82}, {"referenceID": 3, "context": "Traditionally, beamforming techniques such as delay-and-sum and filterand-sum are optimized based on a signal-level loss function, independently of speech recognition task (Benesty et al., 2008; Van Veen & Buckley, 1988).", "startOffset": 172, "endOffset": 220}, {"referenceID": 15, "context": "Recent studies on neural beamformers can be categorized into two types: (1) beamformers with a filter estimation network (Xiao et al., 2016a; Li et al., 2016) and (2) beamformers with a mask estimation network (Heymann et al.", "startOffset": 121, "endOffset": 158}, {"referenceID": 11, "context": ", 2016) and (2) beamformers with a mask estimation network (Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 59, "endOffset": 103}, {"referenceID": 9, "context": ", 2016) and (2) beamformers with a mask estimation network (Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 59, "endOffset": 103}, {"referenceID": 11, "context": "Recently, it has been reported that the mask estimationbased approaches (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016) achieve great performance in noisy speech recognition benchmarks (e.", "startOffset": 72, "endOffset": 139}, {"referenceID": 9, "context": "Recently, it has been reported that the mask estimationbased approaches (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016) achieve great performance in noisy speech recognition benchmarks (e.", "startOffset": 72, "endOffset": 139}, {"referenceID": 1, "context": "To reduce the input sequence length, we apply a subsampling technique (Bahdanau et al., 2016) to some layers.", "startOffset": 70, "endOffset": 93}, {"referenceID": 8, "context": "In this work, we adopt a location-based attention mechanism (Chorowski et al., 2015), and an and cn are formalized as follows:", "startOffset": 60, "endOffset": 84}, {"referenceID": 8, "context": "sn \u2208 RS is a DS-dimensional hidden state vector obtained from an upper decoder network at n, and \u03b1 is a sharpening factor (Chorowski et al., 2015).", "startOffset": 122, "endOffset": 146}, {"referenceID": 15, "context": "This paper uses frequency-domain beamformers rather than time-domain ones, which achieve significant computational complexity reduction in multichannel neural processing (Li et al., 2016; Sainath et al., 2016).", "startOffset": 170, "endOffset": 209}, {"referenceID": 18, "context": "This paper uses frequency-domain beamformers rather than time-domain ones, which achieve significant computational complexity reduction in multichannel neural processing (Li et al., 2016; Sainath et al., 2016).", "startOffset": 170, "endOffset": 209}, {"referenceID": 15, "context": "The filter estimation network directly estimates a timevariant filter coefficients {gt,f,c} t=1,f=1,c=1 as the outputs of the network, which was originally proposed in (Li et al., 2016).", "startOffset": 168, "endOffset": 185}, {"referenceID": 11, "context": "Also, mask-based beamforming approaches have achieved great performance in noisy speech recognition benchmarks (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 111, "endOffset": 178}, {"referenceID": 9, "context": "Also, mask-based beamforming approaches have achieved great performance in noisy speech recognition benchmarks (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016).", "startOffset": 111, "endOffset": 178}, {"referenceID": 19, "context": "(11) as follows (Souden et al., 2010):", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "Based on (Yoshioka et al., 2015; Heymann et al., 2016), the PSD matrices are robustly estimated using the expectation with respect to time-frequency masks as follows:", "startOffset": 9, "endOffset": 54}, {"referenceID": 11, "context": "The MVDR beamformer through this BLSTM mask estimation is originally proposed in (Heymann et al., 2016), but our neural beamformer further extends it with attention-based reference selection, which is described in the next subsection.", "startOffset": 81, "endOffset": 103}, {"referenceID": 15, "context": "There have been several related studies of neural beamformers based on the filter estimation (Li et al., 2016; Xiao et al., 2016a) and the mask estimation (Heymann et al.", "startOffset": 93, "endOffset": 130}, {"referenceID": 11, "context": ", 2016a) and the mask estimation (Heymann et al., 2016; Erdogan et al., 2016; Xiao et al., 2016b).", "startOffset": 33, "endOffset": 97}, {"referenceID": 9, "context": ", 2016a) and the mask estimation (Heymann et al., 2016; Erdogan et al., 2016; Xiao et al., 2016b).", "startOffset": 33, "endOffset": 97}, {"referenceID": 12, "context": "Speech recognition with raw multichannel waveforms (Hoshen et al., 2015; Sainath et al., 2016) can also be seen as using a neural beamformer, where the filter coefficients are represented as network parameters, but again these methods are still based on the hybrid approach.", "startOffset": 51, "endOffset": 94}, {"referenceID": 18, "context": "Speech recognition with raw multichannel waveforms (Hoshen et al., 2015; Sainath et al., 2016) can also be seen as using a neural beamformer, where the filter coefficients are represented as network parameters, but again these methods are still based on the hybrid approach.", "startOffset": 51, "endOffset": 94}, {"referenceID": 7, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 8, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 6, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 16, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 27, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 14, "context": "(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).", "startOffset": 0, "endOffset": 164}, {"referenceID": 23, "context": "We use the two multichannel speech recognition benchmarks, CHiME-4 (Vincent et al., 2016) and AMI (Hain et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 14, "context": "CTC-attention multi-task loss function (Kim et al., 2016), and set the CTC loss weight as 0.", "startOffset": 39, "endOffset": 57}, {"referenceID": 20, "context": "For decoding, we used a beam search algorithm similar to (Sutskever et al., 2014) with the beam size 20 at each output step to reduce the computation cost.", "startOffset": 57, "endOffset": 81}, {"referenceID": 8, "context": "We adopted a length penalty term (Chorowski et al., 2015) to the decoding objective and set the penalty weight as 0.", "startOffset": 33, "endOffset": 57}, {"referenceID": 17, "context": "We used the AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2013) for optimization.", "startOffset": 69, "endOffset": 91}, {"referenceID": 21, "context": "All the above networks are implemented by using Chainer (Tokui et al., 2015).", "startOffset": 56, "endOffset": 76}, {"referenceID": 23, "context": "by following a conventional multi-condition training strategy (Vincent et al., 2016).", "startOffset": 62, "endOffset": 84}], "year": 2017, "abstractText": "The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.", "creator": "LaTeX with hyperref package"}}}