{"id": "1703.05320", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Legal Question Answering using Ranking SVM and Deep Convolutional Neural Network", "abstract": "this presented paper section presents a study of method employing ranking svm and convolutional neural network for two missions : legal information retrieval and collaborative question answering in the competition on analyzing legal information extraction / entailment. for the analytical first task, our proposed model used a triple of features ( lsi, manhattan, jaccard ), and is based on paragraph level instead of article level as in previous studies. in very fact, each single - paragraph statistical article corresponds to a particular ranking paragraph in a huge multiple - paragraph article. for the legal question panel answering task, additional statistical response features from information retrieval task integrated into convolutional computing neural network contribute to higher accuracy.", "histories": [["v1", "Thu, 16 Mar 2017 01:06:07 GMT  (346kb,D)", "http://arxiv.org/abs/1703.05320v1", "15 pages, 2 figures, Tenth International Workshop on Juris-informatics (JURISIN 2016) associated with JSAI International Symposia on AI 2016 (IsAI-2016)"]], "COMMENTS": "15 pages, 2 figures, Tenth International Workshop on Juris-informatics (JURISIN 2016) associated with JSAI International Symposia on AI 2016 (IsAI-2016)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["phong-khac do", "huy-tien nguyen", "chien-xuan tran", "minh-tien nguyen", "minh-le nguyen"], "accepted": false, "id": "1703.05320"}, "pdf": {"name": "1703.05320.pdf", "metadata": {"source": "CRF", "title": "Legal Question Answering using Ranking SVM and Deep Convolutional Neural Network", "authors": ["Phong-Khac Do", "Huy-Tien Nguyen", "Chien-Xuan Tran", "Minh-Tien Nguyen", "Minh-Le Nguyen"], "emails": ["nguyenml}@jaist.ac.jp"], "sections": [{"heading": null, "text": "Keywords: Learning to Rank, Ranking SVM, Convolutional Neural Network (CNN), Legal Information Retrieval, Legal Question Answering."}, {"heading": "1 Introduction", "text": "Legal text, along with other natural language text data, e.g. scientific literature, news articles or social media, has seen an exponential growth on the Internet and in specialized systems. Unlike other textual data, legal texts contain strict logical connections of law-specific words, phrases, issues, concepts and factors between sentences or various articles. Those are for helping people to make a correct argumentation and avoid ambiguity when using them in a particular case. Unfortunately, this also makes information retrieval and question answering on legal domain become more complicated than others.\nThere are two primary approaches to information retrieval (IR) in the legal domain [1]: manual knowledge engineering (KE) and natural language processing (NLP). In the KE approach, an effort is put into translating the way legal experts remember and classify cases into data structures and algorithms, which will be used for information retrieval. Although this approach often yields a good result, it is hard to be applied in practice because of time and financial cost when building the knowledge base. In contrast, NLP-based IR systems are more practical as they are designed to quickly process terabytes of data by utilizing\nar X\niv :1\n70 3.\n05 32\n0v 1\n[ cs\n.C L\n] 1\n6 M\nNLP techniques. However, several challenges are presented when designing such system. For example, factors and concepts in legal language are applied in a different way from common usage [2]. Hence, in order to effectively answer a legal question, it must compare the semantic connections between the question and sentences in relevant articles found in advance [3].\nGiven a legal question, retrieving relevant legal articles and deciding whether the content of a relevant article can be used to answer the question are two vital steps in building a legal question answering system. Kim et al. [3] exploited Ranking SVM with a set of features for legal IR and Convolutional Neural Network (CNN) [12] combining with linguistic features for question answering (QA) task. However, generating linguistic features is a non-trivial task in the legal domain. Carvalho et al. [2] utilized n-gram features to rank articles by using an extension of TF-IDF. For QA task, the authors adopted AdaBoost [20] with a set of similarity features between a query and an article pair [19] to classify a query-article pair into \u201cYES\u201d or \u201cNO\u201d. However, overfitting in training may be a limitation of this method. Sushimita et al. [6] used the voting of Hiemstra, BM25 and PL2F for IR task. Meanwhile, Tran et al. [7] used Hidden Markov model (HMM) as a generative query model for legal IR task. Kano [8] addressed legal IR task by using a keyword-based method in which the score of each keyword was computed from a query and its relevant articles using inverse frequency. After calculating, relevant articles were retrieved based on three ranked scores. These methods, however, lack the analysis of feature contribution, which can reveal the relation between legal and NLP domain. This paper makes the following contributions:\n\u2013 We conduct detailed experiments over a set of features to show the contribution of individual features and feature groups. Our experiments benefit legal domain in selecting appropriate features for building a ranking model. \u2013 We analyze the provided training data and conclude that: (i) splitting legal articles into multiple single-paragraph articles, (ii) carefully initializing parameters for CNN significantly improved the performance of legal QA system, and (iii) integrating additional features in IR task into QA task leads to better results. \u2013 We propose to classify a query-article pair into \u201cYES\u201d or \u201cNO\u201d by voting, in which the score of a pair is generated from legal IR and legal QA model.\nIn the following sections, we first show our idea along with data analysis in the context of COLIEE. Next, we describe our method for legal IR and legal QA tasks. After building a legal QA system, we show experimental results along with discussion and analysis. We finish by drawing some important conclusions."}, {"heading": "2 Proposed Method", "text": ""}, {"heading": "2.1 Basic Idea", "text": "In the context of COLIEE 2016, our approach is to build a pipeline framework which addresses two important tasks: IR and QA. In Figure 1, in training phase,\na legal text corpus was built based on all articles. Each training query-article pair for LIR task and LQA task was represented as a feature vector. Those feature vectors were utilized to train a learning-to-rank (L2R) model (Ranking SVM) for IR and a classifier (CNN) for QA. The red arrows mean that those steps were prepared in advance. In the testing phase, given a query q, the system extracts its features and computes the relevance score corresponding to each article by using the L2R model. Higher score yielded by SVM-Rank means the article is more relevant. As shown in Figure 1, the article ranked first with the highest score, i.e. 2.6, followed by other lower score articles. After retrieving a set of relevant articles, CNN model was employed to determine the \u201cYES\u201d or \u201cNO\u201d answer of the query based on these relevant articles."}, {"heading": "2.2 Data Observation", "text": "The published training dataset in COLIEE 20163 consists of a text file containing Japanese Civil Code and eight XML files. Each XML file contains multiple pairs of queries and their relevant articles, and each pair has a label \u201cYES\u201d or \u201cNO\u201d, which confirms the query corresponding to the relevant articles. There is a total of 412 pairs in eight XML files and 1,105 articles in the Japanese Civil Code file, and each query can have more than one relevant articles.\nAfter analyzing the dataset in the Civil Code file, we observed that the content of a query is often more or less related to only a paragraph of an article instead of the entire content. Based on that, each article was treated as one of two types: single-paragraph or multiple-paragraph, in which a multiple-paragraph article is an article which consists of more than one paragraphs. There are 7 empty articles, 682 single-paragraph articles and the rest are multiple-paragraph.\nBased on our findings, we proposed to split each multiple-paragraph article into several independent articles according to their paragraphs. For instance, in Table 1, the Article 233 consisting of two paragraphs was split into two singleparagraph articles 233(1) and 233(2). After splitting, there are in total 1,663 single-paragraph articles.\nStopwords were also removed before building the corpus. Text was processed in the following order: tokenization, POS tagging, lemmatization, and stopword removal4. In [2], the stopword removal stage was done before the lemmatization\n3 http://webdocs.cs.ualberta.ca/\u223cmiyoung2/COLIEE2016/ 4 http://www.nltk.org/book/ch02.html\nstage, but we found that after lemmatizing, some words might become stopwords, for instance, \u201cdone\u201d becomes \u201cdo\u201d. Therefore, the extracted features based on words are more prone to be distorted, leading to lower ranking performance if stopword removal is carried out before lemmatization step. Terms were tokenized and lemmatized using NLTK5, and POS tagged by Stanford Tagger6."}, {"heading": "2.3 Legal Question Answering", "text": "Legal Information Retrieval\nIn order to build a legal IR, traditional models such as TF-IDF, BM25 or PL2F can be used to generate basic features for matching documents with a query. Nevertheless, to improve not only the accuracy but also the robustness of ranking function, it is essential to take into account a combination of fundamental features and other potential features. Hence, the idea is to build a L2R model, which incorporates various features to generate an optimal ranking function.\nAmong different L2R methods, Ranking SVM (SVM-Rank) [5], a state-ofthe-art pairwise ranking method and also a strong method for IR [4,18], was used. Our model is an extended version of Kim\u2019s model [3] with two new aspects. Firstly, there is a big distinction between our features and Kim\u2019s features. While Kim used three types of features: lexical words, dependency pairs, and TF-IDF score; we conducted a series of experiments to discover a set of best features among six features as shown in Table 2. Secondly, our model is applied to individual paragraphs as described in section 2.2 instead of the whole articles as in Kim\u2019s work.\nGiven n training queries {qi}ni=1, their associated document pairs (x (i) u , x (i) v ) and the corresponding ground truth label y (i) u,v, SVM Rank optimizes the objective function shown in Equation (1) subject to constraints (2), and (3):\nmin 1\n2 \u2016w\u20162 + \u03bb n\u2211 i=1 \u2211 u,v:y (i) u,v \u03be(i)u,v (1)\n5 http://www.nltk.org/ 6 http://nlp.stanford.edu/software/tagger.shtml\ns.t. wT (x(i)u \u2212 x(i)v ) \u2265 1\u2212 \u03be(i)u,v if y(i)u,v = 1 (2)\n\u03be(i)u,v \u2265 0, i = 1, ..., n (3)\nwhere: f(x) = wTx is a linear scoring function, (xu, xv) is a pairwise and \u03be (i) u,v is the loss. The document pairwise in our model is a pair of a query and an article.\nBased on the corpus constructed from all of the single-paragraph articles (see Section 2.2), three basic models were built: TF-IDF, LSI and Latent Dirichlet Allocation (LDA) [9]. Note that, LSI and LDA model transform articles and queries from their TF-IDF-weighted space into a latent space of a lower dimension. For COLIEE 2016 corpora, the dimension of both LSI and LDA is 300 instead of over 2,100 of TF-IDF model. Those features were extracted by using gensim library [10]. Additionally, to capture the similarity between a query and an article, we investigated other potential features described in Table 2. Normally, the Jaccard coefficient measures similarity between two finite sets based on the ratio between the size of the intersection and the size of the union of those sets. However, in this paper, we calculated Generalized Jaccard similarity as:\nJ(q, A) = J(X,Y ) = \u2211 imin(xi, yi)\u2211 imax(xi, yi)\n(4)\nand Jaccard distance as:\nD(q, A) = 1\u2212 J(q, A) (5)\nwhere X = {x1, x2, .., xn} and Y = {y1, y2, ..., yn} are two TF-IDF vectors of a query q and an article A respectively.\nThe observation in Section 2.2 also indicates that one of the important properties of legal documents is the reference or citation among articles. In other words, an article could refer to the whole other articles or to their paragraphs. In [2], if an article has a reference to other articles, the authors expanded it with words of referential ones. In our experiment, however, we found that this approach makes the system confused to rank articles and leads to worse performance. Because of that, we ignored the reference and only took into account individual articles themselves. The results of splitting and non-splitting are shown in Table 5.\nLegal Question Answering\nLegal Question Answering is a form of textual entailment problem [11], which can be viewed as a binary classification task. To capture the relation between a question and an article, a set of features can be used. In the COLLIE 2015, Kim [12] efficiently applied Convolution Neural Network (CNN) for the legal QA task. However, the small dataset is a limit of deep learning models. Therefores, we provided additional features to the CNN model.\nThe idea behind the QA is that we use CNN [3] with additional features. This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems [13,3,14,15] and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.g. TF-IDF and LSI.\nIn Figure 2, the input features v1, v2, ..., v400 are constructed and fed to the network as follows :\n\u2013 v1, v3, v5, ..., v399: a word embedding vector of the question sentence \u2013 v2, v4, ..., v400: a word embedding vector of the most relevant article\nsentence\nA sentence represented by a set of words was converted to a word embedding vector v2001 by using bag-of-words model (BOW) [16]. BOW model generates\na vector representation for a sentence by taking a summation over embedding of words in the sentence. The vector is then normalized by the length of the sentence:\ns = 1\nn n\u2211 i=1 si (6)\nwhere: s is a d-dimensional vector of a sentence, si is a d-dimensional vector of ith word in the sentence, n is the length of sentence. A word embedding model (d = 200) was trained by using Word2Vec[21] on the data of Japanese law corpus[2]. The corpus contains all Civil law articles of Japan\u2019s constitution7 with 13.5 million words from 642 cleaned and tokenized articles. A filter was denoted as a weight vector w with length h; w will have h parameters to be estimated. For each input vector S \u2208 Rd, the feature map vector O \u2208 Rd\u2212h+1 of the convolution operator with a filter w was obtained by applying repeatedly w to sub-vectors of S:\noi = w \u00b7 S[i : i+ h\u2212 1] (7)\nwhere: i = 0, 1, 2, ..., d\u2212 h+ 1 and (\u00b7) is dot product operation. Each feature map was fed to a pooling layer to generate potential features by using the average mechanism [17]. These features were concatenated to a single vector for classification by using Multi-Layer Perceptron with sigmoid activation. During training process, parameters of filters and perceptrons are learned to optimize the objective function.\nIn our model, 10 convolution filters (length = 2) were applied to two adjacent input nodes because these nodes are the same feature type. An average pooling layer (length = 100) is then utilized to synthesize important features. To enhance the performance of CNN, two additional statistic features: TF-IDF and LSI were concatenated with the result of the pooling layer, then fed them into a 2-layer Perceptron model to predict the answer."}, {"heading": "3 Results and Discussion", "text": ""}, {"heading": "3.1 Information Retrieval", "text": "Training model: For information retrieval task, 20% of query-article pairs are used for evaluating our model while the rest is for training. As we only consider single-paragraph articles in the training phase, if a multiple-paragraph article is relevant, all of its generated single-paragraph articles will be marked as relevant. In addition, the label for each query-article pair is set either 1 (relevant) or 0 (irrelevant). In our experiment, instead of selecting top k retrieved articles as relevant articles, we consider a retrieved article Ai as a relevant article if its score Si satisfies Equation (8):\nSi S0 \u2265 0.85 (8)\n7 www.japaneselawtranslation.go.jp\nwhere: S0 is the highest relevant score. In other words, the score ratio of a relevant article and the most relevant article should not be lower than 85% (choosing the value 0.85 for this threshold is simply heuristic based). This is to prevent a relevant article to have a very low score as opposed to the most relevant article.\nWe ran SVM-Rank with different combinations of features listed in Table 2, but due to limited space, we only report the result of those combinations which achieved highest F1-score. We compared our method to two baseline models TFIDF and LSI which only use Cosine similarity to retrieve the relevant articles. Results from Table 3 indicate that (LSI, Manhattan, Jaccard) is the triple of features which achieves the best result and the most stability.\nFeature Contribution: The contribution of each feature was investigated by using leave-one-out test. Table 4 shows that when all six features are utilized, the F1-score is approximately 0.55. However when excluding Jaccard, F1-score drops to around 0.5. In contrast, when other features are excluded individually from the feature set, the result remains stable or goes up slightly. From this result, we conclude that Jaccard feature significantly contributes to SVM-Rank performance.\nWe also analyzed the contribution of feature groups to the performance of SVM-Rank. When removing different triples of features from the feature set, it can be seen that (TF-IDF, Manhattan, Jaccard) combination witnesses the highest loss. Nevertheless, as shown in Table 3, the result of (LSI, Manhattan, Jaccard) combination is more stable and better.\nSplitting vs. Non-Splitting: As mentioned, we proposed to split a multipleparagraph article into several single-paragraph articles. Table 5 shows that after splitting, the F1-score performance increases by 0.05 and 0.04 with references and without references respectively. In both cases (with and without the reference), using single-paragraph articles always results a higher performance.\nResults from Table 5 also indicate that expanding the reference of an article negatively affects the performance of our model, reducing the F1-score by more than 0.02. This is because if we only expand the content of an article with the content of referential one, it is more likely to be noisy and distorted, leading to lower performance. Therefore, we conclude that a simple expansion of articles via their references does not always positively contribute to the performance of the model.\nTuning Hyperparmeter: Since linear kernel was used to train the SVM-Rank model, the role of trade-off training parameter was analyzed by tuning C value from 100 to 2000 with step size 100. Empirically, F1-score peaks at 0.6087 with C = 600 when it comes to COLIEE 2016 training dataset. We, therefore, use this value for training the L2R model.\nFormal run phase 1 - COLIEE 2016\nIn COLIEE 2016 competition, Table 6 shows the top three systems and the baseline for the formal run in phase 1 [24]. Among 7 submissions, iLis7 [22] was ranked first with outstanding performance (0.6261) by exploiting ensemble methods for legal IR. Several features such as syntactic similarity, lexical similarity, semantic similarity, were used as features for two ensemble methods Least Square Method (LSM) and Linear Discriminant Analysis (LDA).\nHUKB-2 [23] used a fundamental feature BM25 and applied mutatis mutandis for articles. If both an article and a query have conditional parts, they are\ndivided into two parts like conditional parts and the rest part before measuring their similarity. This investigation in conditional parts is valuable since it is a common structure in laws. Their F1-score in formal rune is the second highest (0.5532), which is slightly higher than our system (0.5478) using SVM-Rank and a set of features LSI, Manhattan, Jaccard. This shows that for phase 1, our model with a set of defined features is relatively competitive."}, {"heading": "3.2 Legal Question Answering", "text": "Compared Results: In Legal QA task, the proposed model was compared to the original CNN model and separate TF-IDF, LSI features. For evaluation, we took out 10% samples from training set for validation, and carried out experiments on dataset with balanced label distribution for training set, validation set and testing set.\nIn CNN models, we found that these models are sensitive to the initial value of parameters. Different values lead to large difference in results (\u00b1 5%). Therefore, each model was run n times (n=10) and we chose the best-optimized parameters against the validation set. Table 7 shows that CNN with additional features performs better. Also, CNN with LSI produces a better result as opposed to CNN with TF-IDF. We suspect that this is because TF-IDF vector is large but quite sparse (most values are zero), therefore it increases the number of parameters in CNN and consequently makes the model to be overfitted easily.\nTuning Hyperparmeter: To achieve the best configuration of CNN architecture, the original CNN model was run with different settings of number filter and hidden layer dimension. According to Table 8, the change of hyperparameter does not significantly affect to the performance of CNN. We, therefore, chose the configuration with the best performance and least number of parameters: 10 filters and 200 hidden layer size."}, {"heading": "3.3 Legal Question Answering System", "text": "In this stage, we illustrate our framework on COLIEE 2016 data. The framework was trained on XML files, from H18 to H23 and tested on XML file H24. Given a legal question, the framework first retrieves top five relevant articles and then transfers the question and relevant articles to CNN classifier. The running of framework was evaluated with 3 scenarios:\nScenario Accuracy%\nNo voting 45.6 Voting without ratio 49.4 Voting with ratio 48.1\nTable 9 shows results with different scenarios. The result of No voting approach is influenced by IR task\u2019s performance, so the accuracy is not as high as using voting. The relevant score disparity between the first and second relevant article is large, which causes a worse result of Voting with ratio compared to Voting without ratio.\nFormal run phase 2 & 3 - COLIEE 2016\nTable 10 lists the state-of-the art methods for the formal run 2016 in phase 2 and 3. In phase 2, two best systems are iList7 and KIS-1. iList7 applies major voting of decision tree, SVM and CNN with various features; KIS-1 just uses simple rules of subjective cases and an end-of-sentence expression. In phase 3, UofA achives the best score. It extracts the article segment which related to the query. This system also performs paraphrasing and detects condition-conclusionexceptions for the query/article. From the experimental results, deep learning models do not show their advantages in case of a small dataset. On the other hand, providing handcraft features and rules are shown to be useful in this case."}, {"heading": "4 Splitting and non-splitting error analysis", "text": "In this section, we show an example in which our proposed model using singleparagraph articles gives a correct answer in contrast with utilizing non-splitting\none. Given a query with id H20-26-3: \u201cA mandate contract is gratuitous contract in principle, but if there is a special provision, the mandatary may demand renumeration from the mandator.\u201d, which refers to Article 648:\nMethod Article Rank\nSplitting\n648(1) 1 653 2 648(3) 5 648(2) 29\nNon-splitting 653 1 648 6\nApparently, three paragraphs and the query share several words namely mandatary, remuneration, etc. In this case, however, the correct answer is only located in paragraph 1, which is ranked first in the single-paragraph model in contrast to two remaining paragraphs with lower ranks, 5th and 29th as shown in Table 11.\nArticle Content\n653 A mandate shall terminate when (i) The mandator or mandatary dies; (ii) The mandator or mandatary is subject to a ruling for the commencement of bankruptcy procedures; (iii) The mandatary is subject to an order for the commencement of guardianship.\nInterestingly, Article 653 has the highest relevant score in non-splitting method and rank 2nd in splitting approach. The reason for this is that Article 653 shares other words like mandatary, mandator as well. Therefore, it makes retrieval system confuse and yield incorrect order rank. By using single-paragraph, the system can find more accurately which part of the multiple-paragraph article is associated with the query\u2019s content."}, {"heading": "5 Conclusion", "text": "This work investigates Ranking SVM model and CNN for building a legal question answering system for Japan Civil Code. Experimental results show that feature selection affects significantly to the performance of SVM-Rank, in which a set of features consisting of (LSI, Manhattan, Jaccard) gives promising results for information retrieval task. For question answering task, the CNN model is sensitive to initial values of parameters and exerts higher accuracy when adding auxiliary features.\nIn our current work, we have not yet fully explored the characteristics of legal texts in order to utilize these features for building legal QA system. Properties such as references between articles or structured relations in legal sentences should be investigated more deeply. In addition, there should be more evaluation of SVM-Rank and other L2R methods to observe how they perform on this legal data using the same feature set. These are left as our future work."}, {"heading": "Acknowledgement", "text": "This work was supported by JSPS KAKENHI Grant number 15K16048, JSPS KAKENHI Grant Number JP15K12094, and CREST, JST."}], "references": [{"title": "Concept and Context in Legal Information Retrieval.", "author": ["Maxwell", "K. Tamsin", "Burkhard Schafer"], "venue": "JURIX", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Lexical-Morphological Modeling for Legal Text Analysis", "author": ["Danilo S. Carvalho", "Minh-Tien Nguyen", "Chien-Xuan Tran", "Minh-Le Nguyen"], "venue": "Ninth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A Convolutional Neural Network in Legal Question Answering", "author": ["Mi-Young Kim", "Ying Xu", "Randy Goebel"], "venue": "Ninth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning to rank: from pairwise approach to listwise approach.", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "Proceedings of the 24th international conference on Machine learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Training Linear SVMs in Linear Time Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).", "author": ["T. Joachims"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Legal Information Retrieval Task: Participation from ISM, Dhanbad", "author": ["Sushmita", "A. Kanapala", "S. Pal"], "venue": "Ninth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "An Approach for Retrieving Legal Texts", "author": ["Vu Duc Tran", "Anh Viet Phan", "Long Hai Trieu", "Minh Le Nguyen"], "venue": "Ninth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Keyword and Snippet Based Yes/No Question Answering System for COLIEE 2015", "author": ["Yoshinobu Kano"], "venue": "Ninth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Latent dirichlet allocation.", "author": ["Blei", "David M", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of machine Learning research", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Software framework for topic modelling with large corpora", "author": ["Radim \u0158eh\u030au\u0159ek", "Petr Sojka"], "venue": "Proc. LREC Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Recognizing textual entailment: Rational, evaluation and approacheserratum.", "author": ["Dagan", "Ido"], "venue": "Natural Language Engineering", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Convolutional neural networks for sentence classification.", "author": ["Kim", "Yoon"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Semantic Parsing for SingleRelation Question Answering.", "author": ["Yih", "Wen-tau", "Xiaodong He", "Christopher Meek"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web search.", "author": ["Shen", "Yelong", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Grgoire Mesnil"], "venue": "Proceedings of the 23rd International Conference on World Wide Web. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A convolutional neural network for modelling sentences.", "author": ["Kalchbrenner", "Nal", "Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep learning for answer sentence selection.", "author": ["Yu", "Lei", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "arXiv preprint arXiv:1412.1632", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A theoretical analysis of feature pooling in visual recognition.", "author": ["Boureau", "Y-Lan", "Jean Ponce", "Yann LeCun"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Learning to Rank Questions for Community Question Answering with Ranking SVM.", "author": ["Minh-Tien Nguyen", "Viet-Anh Phan", "Truong-Son Nguyen", "Minh-Le Nguyen"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Recognizing textual entailment in vietnamese text: an experimental study.", "author": ["Minh-Tien Nguyen", "Quang-Thuy Ha", "Thi-Dung Nguyen", "Tri-Thanh Nguyen", "LeMinh Nguyen"], "venue": "Knowledge and Systems Engineering (KSE),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A desicion-theoretic generalization of online learning and an application to boosting.", "author": ["Freund", "Yoav", "Robert E. Schapire"], "venue": "European conference on computational learning theory. Springer Berlin Heidelberg,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Efficient estimation of word representations in vector space.", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "An Ensemble Based Legal Information Retrieval and Entailment System", "author": ["Kiyoun Kim", "Seongwan Heo", "Sungchul Jung", "Kihyun Hong", "Young-Yik Rhim"], "venue": "Tenth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Civil Code Article Information Retrieval System based on Legal Terminology and Civil Code Article Structure", "author": ["Daiki Onodera", "Masaharu Yoshioka"], "venue": "Tenth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "COLIEE-2016: Evaluation of the Competition on Legal Information Extraction and Entailment", "author": ["Mi-Young Kim", "Randy Goebel", "Yoshinobu Kano", "Ken Satoh"], "venue": "Tenth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Legal Question Answering Using Paraphrasing and Entailment Analysis", "author": ["Mi-Young Kim", "Ying Xu", "Yao Lu", "Randy Goebel"], "venue": "Tenth International Workshop on Juris-informatics (JURISIN),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "There are two primary approaches to information retrieval (IR) in the legal domain [1]: manual knowledge engineering (KE) and natural language processing (NLP).", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "For example, factors and concepts in legal language are applied in a different way from common usage [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "Hence, in order to effectively answer a legal question, it must compare the semantic connections between the question and sentences in relevant articles found in advance [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "[3] exploited Ranking SVM with a set of features for legal IR and Convolutional Neural Network (CNN) [12] combining with linguistic features for question answering (QA) task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[3] exploited Ranking SVM with a set of features for legal IR and Convolutional Neural Network (CNN) [12] combining with linguistic features for question answering (QA) task.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "[2] utilized n-gram features to rank articles by using an extension of TF-IDF.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "For QA task, the authors adopted AdaBoost [20] with a set of similarity features between a query and an article pair [19] to classify a query-article pair into \u201cYES\u201d or \u201cNO\u201d.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "For QA task, the authors adopted AdaBoost [20] with a set of similarity features between a query and an article pair [19] to classify a query-article pair into \u201cYES\u201d or \u201cNO\u201d.", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "[6] used the voting of Hiemstra, BM25 and PL2F for IR task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used Hidden Markov model (HMM) as a generative query model for legal IR task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Kano [8] addressed legal IR task by using a keyword-based method in which the score of each keyword was computed from a query and its relevant articles using inverse frequency.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "In [2], the stopword removal stage was done before the lemmatization", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Among different L2R methods, Ranking SVM (SVM-Rank) [5], a state-ofthe-art pairwise ranking method and also a strong method for IR [4,18], was used.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "Among different L2R methods, Ranking SVM (SVM-Rank) [5], a state-ofthe-art pairwise ranking method and also a strong method for IR [4,18], was used.", "startOffset": 131, "endOffset": 137}, {"referenceID": 17, "context": "Among different L2R methods, Ranking SVM (SVM-Rank) [5], a state-ofthe-art pairwise ranking method and also a strong method for IR [4,18], was used.", "startOffset": 131, "endOffset": 137}, {"referenceID": 2, "context": "Our model is an extended version of Kim\u2019s model [3] with two new aspects.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "2), three basic models were built: TF-IDF, LSI and Latent Dirichlet Allocation (LDA) [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Those features were extracted by using gensim library [10].", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "In [2], if an article has a reference to other articles, the authors expanded it with words of referential ones.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "Legal Question Answering is a form of textual entailment problem [11], which can be viewed as a binary classification task.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "In the COLLIE 2015, Kim [12] efficiently applied Convolution Neural Network (CNN) for the legal QA task.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "The idea behind the QA is that we use CNN [3] with additional features.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems [13,3,14,15] and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.", "startOffset": 158, "endOffset": 170}, {"referenceID": 2, "context": "This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems [13,3,14,15] and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.", "startOffset": 158, "endOffset": 170}, {"referenceID": 13, "context": "This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems [13,3,14,15] and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.", "startOffset": 158, "endOffset": 170}, {"referenceID": 14, "context": "This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems [13,3,14,15] and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.", "startOffset": 158, "endOffset": 170}, {"referenceID": 15, "context": "A sentence represented by a set of words was converted to a word embedding vector v 1 by using bag-of-words model (BOW) [16].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "A word embedding model (d = 200) was trained by using Word2Vec[21] on the data of Japanese law corpus[2].", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "A word embedding model (d = 200) was trained by using Word2Vec[21] on the data of Japanese law corpus[2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "Each feature map was fed to a pooling layer to generate potential features by using the average mechanism [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "In COLIEE 2016 competition, Table 6 shows the top three systems and the baseline for the formal run in phase 1 [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "Among 7 submissions, iLis7 [22] was ranked first with outstanding performance (0.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "5310 HUKB-2 [23] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "5532 iLis7 [22] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "HUKB-2 [23] used a fundamental feature BM25 and applied mutatis mutandis for articles.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "Method Phase 2 Phase 3 iLis7 [22] 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "5158 UofA[25] 0.", "startOffset": 9, "endOffset": 13}], "year": 2017, "abstractText": "This paper presents a study of employing Ranking SVM and Convolutional Neural Network for two missions: legal information retrieval and question answering in the Competition on Legal Information Extraction/Entailment. For the first task, our proposed model used a triple of features (LSI, Manhattan, Jaccard), and is based on paragraph level instead of article level as in previous studies. In fact, each single-paragraph article corresponds to a particular paragraph in a huge multiple-paragraph article. For the legal question answering task, additional statistical features from information retrieval task integrated into Convolutional Neural Network contribute to higher accuracy.", "creator": "LaTeX with hyperref package"}}}