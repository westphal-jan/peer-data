{"id": "1703.06503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels", "abstract": "this work presents cltune, an analytic auto - tuner for opencl kernels. it evaluates and tunes kernel performance of a generic, user - defined search space of possible parameter - value combinations. example range parameters include the opencl workgroup size, vector data - sequence types, tile sizes, and loop unrolling factors. cltune approach can be best used in considering the following scenarios : 1 ) when there are too many tunable parameters to readily explore manually, 2 ) when performance scale portability across separate opencl devices is desired, or 3 ) when exploring the optimal parameters change based on input argument code values ( in e. g. matrix dimensions ). the auto - tuner is generic, easy to use, easily open - source, and eventually supports multiple search strategies to including simulated annealing modelling and functional particle swarm optimisation. primarily cltune is evaluated on two gpu case - studies inspired by the recent successes in deep learning : 2d convolution and matrix - multiplication ( gemm ). for 2d convolution, we demonstrate calculating the weak need tolerance for mathematical auto - tuning by optimizing for different filter sizes, achieving adequate performance on - par or better than the state - of - the - art. for general matrix - multiplication, we use cltune to explore developing a parameter space of encompassing more than two - hundred thousand configurations, we show the need for device - specific tuning, build and rather outperform the clblas library on nvidia, amd and intel gpus.", "histories": [["v1", "Sun, 19 Mar 2017 20:10:00 GMT  (221kb,D)", "http://arxiv.org/abs/1703.06503v1", "8 pages, published in MCSoC '15, IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), 2015"]], "COMMENTS": "8 pages, published in MCSoC '15, IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), 2015", "reviews": [], "SUBJECTS": "cs.PF cs.AI cs.DC", "authors": ["cedric nugteren", "valeriu codreanu"], "accepted": false, "id": "1703.06503"}, "pdf": {"name": "1703.06503.pdf", "metadata": {"source": "CRF", "title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels", "authors": ["Cedric Nugteren", "Valeriu Codreanu"], "emails": ["mail@cedricnugteren.nl", "valeriu.codreanu@surfsara.nl"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe programmability of parallel processors such as graphics processing units (GPUs), multi-core CPUs and accelerators (MIC, FPGAs) has been improved significantly over the past years. Programming languages have matured, development tools have increased capabilities, libraries were created or improved, and so on. These new and improved technologies have led to highly efficient parallel programs in domains such as quantum chemistry, astronomy, bioinformatics, machine learning and fluid dynamics.\nDespite these advancements, achieving close-to-peak performance remains a task for expert programmers in many cases [2]. And even experts might face optimisation problems where the space of design decisions is too large to explore. Or they might have to tune their code for a wide variety of devices, each with their own sensitivities to specific parameters. Furthermore, to achieve optimal performance, code can be tailored to specific input arguments (e.g. matrix dimensions), either at compile-time (off-line) or at run-time (on-line).\nWe present CLTune, an auto-tuner designed to address the above issues for both experts and non-experts. We choose OpenCL as a target programming language, as it is supported by almost all recent accelerators (GPUs, MIC, FPGAs) and even by processors with less parallelism (x86 CPUs, ARM CPUs). Although OpenCL code is portable across devices, it is definitely not performance portable: to achieve good performance it is necessary to tune design parameters, adjust the hierarchy of parallelism, and explore different algorithms [2]. With CLTune, users can specify tunable parameters along with\na list of values for their OpenCL kernels. Our auto-tuner is designed for the following scenarios:\n1) The search-space is too large to explore manually: For example, consider a case where the vector width for the input and output data can be varied, a 2D rectangular local workgroup is used (threadblock in CUDA terminology), the thread-coarsening factor (i.e. the amount of work per thread) can be adjusted, and a decision can be made whether or not to cache in local memory (shared memory in CUDA terminology). In such a case, the number of permutations will quickly grow into the thousands. Exploring this search-space is especially important for library designers: they are willing to spend extra effort to achieve the best possible performance.\n2) Efficient execution on a variety of devices is desired: For performance portability across devices, data-locality and algorithmic choices might impact performance the most. However, beyond that, even for devices coming from the same vendor or with the same architectural family, it might be worthwhile to explore workgroup configurations, loop unroll factors, or vector widths [2].\n3) The optimal parameters change based on input arguments: Example arguments are data-sizes or boolean variables. A use-case is an OpenCL kernel executed in a time-loop as part of a scientific simulation with a run-time fixed input size: perhaps the first tens of time-steps can be used to find optimal parameters, allowing the remainder time-steps to execute more efficiently. Another use-case is a fast Fourier transform (FFT) library of which the parameters can be tuned at compile-time when it is known which FFT-sizes are being used (e.g. [15]).\nThe contributions of this work are two-fold. First, we present CLTune, a new OpenCL-kernel auto-tuner which automatically explores a search-space of tuning parameters (section III). The tuner is designed to be generic, easy to use, flexible and customisable. CLTune is furthermore open-source and supports different search strategies: full-search, randomsearch, simulated annealing, and particle swarm optimisation. Second, we present two case-studies inspired by the recent successes with convolutional neural networks for deep-learning on GPUs [6], [11]: 2D convolution and matrix-multiplication (GEMM). For 2D convolution, our tuner matches the CUDAbased state-of-the-art [21] and achieves up to 1658 GFLOPS on a 11x11 filter and 207GB/s on a 3x3 filter running on an AMD HD7970 GPU. We show that tuning for specific filter sizes is worthwhile, yielding a performance boost of up to 56%. We also tune matrix-multiplication, matching the stateof-the-art [17], and achieving better performance compared to the clBLAS library. Thanks to CLTune, our GEMM kernel is the best-performing publicly available OpenCL implementation. We furthermore demonstrate that tuning for each device individually can lead to a factor 2 performance increase. ar X\niv :1\n70 3.\n06 50\n3v 1\n[ cs\n.P F]\n1 9\nM ar\n2 01\n7"}, {"heading": "II. RELATED WORK", "text": "There are many existing auto-tuners, of which ATLAS might be the best-known example. In the first category of related work, we discuss cases where auto-tuners were introduced to solve a specific OpenCL or GPU-related problem. Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15]. In contrast to CLTune, such tuners are tailored for a specific problem and are often not applicable beyond these problems. A more generic OpenCL auto-tuner is Maestro data-orchestration tuner [20], however it is orthogonal to this work since it works on data transfers rather than kernel. We are not aware of any existing generic tuner for OpenCL kernels.\nThere are however existing tuners which do target GPUs but work on a higher-level representation than OpenCL. Examples include tuners for a GPU skeleton programming framework [3], as part of a parallelizing compiler [8], for OpenMP 4.0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1]. Such tuners typically tune higher-level concepts and are thus not designed for finegrained parameter tuning. Furthermore, such tuners are not as generic as CLTune: users cannot define and tune their own (low-level) parameters."}, {"heading": "III. CLTUNE: AN AUTO-TUNER FOR OPENCL KERNELS", "text": "CLTune is an auto-tuner written in C++11 targeting OpenCL kernels. It is open-source and freely-available under the APACHE 2.0 license on GitHub1, which includes the two case-studies of this work as example codes. The tuner is implemented as a library with a C++ API, and can thus be used as stand-alone for off-line (compile-time) tuning or integrated into existing source-code for on-line (run-time) tuning. The API is designed such that all host OpenCL API calls are hidden from the user, i.e. CLTune takes care of tasks such as OpenCL device initialisation, kernel invocation, and device memory management.\nTo illustrate the ease of use of the tuner, we briefly discuss a simple example. Consider the example OpenCL kernel at the top of Fig. 1, which uses parameter WPT to vary the amount of work per thread. With this OpenCL kernel saved as the file copy.cl, CLTune can be used to explore performance for 1, 2 or 4 copies per thread. In the remainder of Fig. 1, a tuner is created and the copy-kernel added. The example uses the function DivGlobalSize to reduce the total number of threads as we increase the workload per thread. The tuner will compile three different versions of the copykernel (with WPT={1,2,4}) and will invoke each with a fresh device-copy of the supplied host-arguments (in_vector and out_vector). The kernel execution will be timed and the the value of best WPT value will be reported."}, {"heading": "A. Advanced usage", "text": "As we have seen, using CLTune can be quite simple. Nonetheless, the tuner is also equipped with more advanced features, which are introduced in this section: 1) manipulation\n1CLTune on GitHub: https://github.com/CNugteren/CLTune\nof the thread and workgroup configurations, 2) constraints on the parameters, and 3) result verification.\nThe copy-example of Fig. 1 could be extended by additionally tuning the local workgroup size. This is as easy as introducing a new parameter WG with values of for example {32, 64, 128, 256}, changing the old workgroup size from 64 into 1, and using MulLocalSize({\"WG\"}) to multiply 1 by WG. The reason that the thread dimensions in the example code are within braces is the following: the example uses a 1D arrangement, but they can be extended to 2D or 3D.\nUsers of the auto-tuner might want to set constraints on parameter combinations. An example is tuning a 2D workgroup size Xwg by Ywg: a 128x4 or 4x128 configuration might be allowed, but 128x128 might not. In other words, Xwg \u00b7 Ywg has to remain below a certain threshold. Such constraints can be specified by the user (e.g. as lambda expressions) and can thus be as complex as needed. The two case-studies discussed in this paper make heavy use of such constraints. CLTune furthermore automatically imposes constraints based on the device limits (workgroup dimensions, local memory size, etc.).\nCLTune also provides an interface to verify the results of each tested configuration. This can be used to make sure that all tested parameter permutations are indeed correct and no parameter-dependent bugs are present in the kernel. Verification only requires a reference kernel to be passed to the SetReference function in a similar way as AddKernel in Fig. 1. The outputs of each tested kernel are then automatically compared against the outputs of the reference kernel."}, {"heading": "B. Search strategies and search-space properties", "text": "In some cases it is not feasible or desirable to explore all points in the search-space, i.e. all permutations of all possible parameter values. This is the case when the search-space is too large or when the tuning is done on-line, as in this case the tuning-time affects performance of the actual application. Note that tuning-time is not only determined by execution time\nof the OpenCL kernel, but can also be limited by the repeated re-compilation of the slightly modified OpenCL kernel. Three alternatives to the default full-search are available for those scenarios: random-search, simulated annealing, and particle swarm optimisation.\nRandom-search is the simplest of the search strategies: it samples and tests a random configurable fraction of the entire search-space and reports the best-found configuration. Its efficacy is dependent on the shape of the search-space: when the percentage of good parameter combinations is low, the chance to find one is low as well (and vice-versa).\nBefore discussing the other two search strategies, we discuss properties of the search-space and alternative search strategies. In theory, we cannot make any assumptions about the search-space, as it is defined completely by the user of the tuner. Still, there are some useful observations possible given the simple fact that CLTune tunes OpenCL kernels:\n1) The number of values a parameter can take is typically low, as shown in earlier examples and the two casestudies. Examples are vector widths (2 to 5), threadcoarsening factors (2 to 8), and workgroup sizes (up to 5 per dimension). Furthermore, some parameters might be boolean and have only 2 possible values such as: unroll loops or not, use local memory or not. 2) The search-space can be highly dimensional. A 3 or 4 dimensional space is easily created in a simple example, and even a 14 dimensional one is possible as illustrated by the matrix-multiplication case-study. 3) Parameters are typically discrete and have many nonlinearities. For example, performance might increase when increasing the work per thread from 1 to 2 and from 2 to 4, but might decrease dramatically from 4 to 8 as register pressure suddenly becomes a bottleneck. 4) There can be a strong relation between parameters, e.g. a small number of threads in one workgroup dimension might only be useful if the number of threads is large in the second dimension.\nBecause of the expected non-linearities and boolean variables, methods based on derivatives or automatic differentiation are not applicable. Even derivative free methods such as direct search [10] are not suitable, since they assume that it is relatively cheap to explore all neighbours of a particular configuration. If the search-space is indeed narrow and highly dimensional, this is certainly not the case, as many or all configurations are (diagonal) neighbours of each other. Therefore, taking into account the above 4 observations, we focused on heuristics and iterative methods such as simulated annealing and particle swarm optimisation first. However, other search methods are easily pluggable into CLTune, so evolutionary search, gradient methods, stochastic optimisation or dynamic programming can be evaluated as part of future work."}, {"heading": "C. Simulated annealing", "text": "Simulated annealing (SA) is a heuristic search method inspired by annealing in metallurgy [9] which iteratively moves through the search-space from neighbour to neighbour and ends after a fixed number of iterations or when a certain criterion is reached. In principle, simulated annealing only moves to neighbours with better performance (lower energy\nin annealing terminology). However, to prevent getting stuck in a local optimum, the heuristic has a certain probability to make a step towards a worse configuration. This probability decreases over time as the annealing temperature decreases: at the end of the search the likelihood to move towards the global optimum is higher than at the start. This probability is further decreased as a function of the difference in performance between the current and a neighbour configuration.\nThe simulated annealing heuristic is implemented in CLTune. The search is initialized in a random configuration and continues until a user-defined number of configurations have been explored. At each step, a random neighbour s\u2032 of the current configuration s is chosen and its performance is evaluated. This neighbour has a probability P of becoming the new current state:\nP (t, t\u2032, T ) = { 1 if t\u2032 < t e\u2212(t \u2032\u2212t)\u00b7T\u22121 otherwise\nin which T is the annealing temperature, and t and t\u2032 represent the execution times of configurations s and s\u2032 respectively. The performance of simulated annealing in CLTune and its sensitivity to the parameter T are evaluated for the case-studies in sections V and VI."}, {"heading": "D. Particle swarm optimisation", "text": "Particle swarm optimisation (PSO) is an evolutionary search strategy in which a swarm of S communicating particles explore the search-space [7]. These particles are initially positioned randomly and are given a random velocity. At each step t, the new positions xt+1i of each particle i are calculated based on their current positions xti and their velocities v t i . These velocities are in turn updated as a function of their current velocities, their own best-known configuration so-far pti, and the overall best-known configuration so far g\nt. A variant of PSO is accelerated PSO [22], in which the new position in each dimension d is directly calculated based on the old position, removing the concept of velocity:\nxt+1i,d = \u03b1 d + \u03b2p t i,d + \u03b3g t d + (1\u2212 \u03b1\u2212 \u03b2 \u2212 \u03b3)xti,d\nin which \u03b1, \u03b2 and \u03b3 are probability parameters and d represents a random number within the range of the parameter in dimension d.\nCLTune uses a modified version of accelerated PSO. Since the search-space might be narrow and discrete, it makes little sense to compute a linear combination of fractions of other positions and round it to an integer. Instead, we omit the notion of continuity and compute the new position xt+1i,d in each dimension d as follows:\nxt+1i,d =  d with probability \u03b1 (random) pti,d with probability \u03b2 (local best) gtd with probability \u03b3 (global best) xti,d otherwise (current position)\nwith \u03b1 + \u03b2 + \u03b3 \u2264 1. Note that this is applied for each dimension separately: the likelihood of moving to a completely unrelated position is for example only \u03b1 to the power of d. The performance of PSO is evaluated in sections V and VI."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "In sections V and VI we will perform experiments on OpenCL GPUs from different vendors, chosen for their wide architectural variety. These devices and their relevant properties are listed in table I. We plan to extend our experiments in future work to non-GPU OpenCL devices (CPUs, MIC).\nAll data-types in this paper are single-precision floating point numbers. Furthermore, simulated annealing is configured with a variable temperature T = {2, 4, 6}, and PSO is configured with \u03b1 = 0.4, \u03b2 = 0 (no local-best influence as argued by [22]), \u03b3 = 0.4 and a variable swarm size S = {3, 6}."}, {"heading": "V. CASE-STUDY: 2D CONVOLUTION", "text": "Our first case-study is 2D convolution, which is for example for image filters such as blur or edge detection. This casestudy is motivated on one hand by the conceptual simplicity of convolution, and on the other hand by the recent renewed interest in GPU-based convolution within the context of convolutional neural networks for deep-learning. Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11]. In convolutional neural networks, many 2D convolutions have to be performed, typically with a range of compile-time fixed filter sizes, ranging for example from 3 by 3 to 11 by 11. An auto-tuner can be used to optimize the convolution code for each specific filter size, as is done for example by Theano. Auto-tuning will become even more important when moving from CUDA to OpenCL to enable execution on a wider range of devices. Convolution is also important outside the context of deep-learning, as is discussed in [19], in which it is also shown that convolution can be generalized to other image and video processing operations such as SAD and SIFT.\nThe 2D convolution operation is illustrated in Fig. 2. In this figure, an Xf by Yf filter F is applied onto an X by Y input image A, producing an output image B of the same size. For each x, y coordinate, the output value can be calculated as follows in case Xf = Yf = 3:\nBx,y = w \u00b7 i\u22641\u2211 i=\u22121 j\u22641\u2211 j=\u22121 Fi,jAx+i,y+j\nin which w is a weighing factor. The user-defined filter coefficients F are the same for each coordinate and are therefore a good match for OpenCL\u2019s constant memory."}, {"heading": "A. Tuning parameters", "text": "We implemented a highly tunable implementation of 2D convolution in OpenCL inspired by [18] and [21]. Details of our implementation are not the focus of this work, but the source is available for inspection as part of the CLTune examples and further discussion can be found in related work [21]. We do discuss the chosen tuning parameters:\n\u2022 The total amount of work is divided into rectangular local workgroups of size Xwg by Ywg , each tunable individually. This is illustrated by Fig. 3. \u2022 The amount of work per thread Xwpt and Ywpt is configurable in both dimensions. This results in a per-thread rectangular block of loads, computations and stores. Advantages might come from improved data-locality and reuse of filter coefficients and indexing variables. \u2022 The parameter L$ implements 3 caching strategies: 0) Caching is hardware-only, no local memory. 1) Input data is cached into the local memory as follows:\nevery thread caches the value at its coordinates x, y plus optionally up to three values from the halo of size Xhf and Yhf . This is illustrated in Fig. 3, in which the red example-thread (1) and blue example-thread (2) each load one additional halo value. The orange examplethread (3) loads 3 values since it is within Xhf and Yhf of the x and y borders respectively. 2) Extra helper threads are launched, creating workgroups of Xwg + 2 \u00b7Xhf by Ywg + 2 \u00b7 Yhf . Now, each thread caches only a single value (or more depending on the work per thread parameters). After caching into the local memory, the extra halo threads are canceled: they don\u2019t perform any computations.\n\u2022 If Xwpt is larger or equal than the vector width VW , multiple stores in the x-dimension can be combined into a single vector operation. Vector loads are only applied for the second local-memory strategy (L$ = 2). \u2022 The local memory is padded by PAD in in the xdimension to possibly avoid memory bank-conflicts. \u2022 Unrolling of the i and j loops over the filter coefficient is enabled or disabled by UNR."}, {"heading": "B. Evaluation of the search strategies", "text": "Before we evaluate the performance of the best-found results for 2D convolution, we analyse CLTune\u2019s search strategies. Within a search-space of 3424 permutations (see table II for the values), random-search, simulated annealing and PSO are each configured to explore 107 unique (1/32th) configurations. We use an image of 8192 by 4096 and a filter of 7x7 (later also 3x3 and 11x11).\nTo illustrate the behaviour of the search strategies, we present the search progress of various runs on a single device in Fig. 4. On the left, we see progress traces of 3 runs (in different colours) of the random-search strategy. Each search evaluates 107 configurations and picks the best as its final result, obtaining 56%, 74% and 94% of the best-known performance. The middle plot in Fig. 4 holds the results of 3 simulated annealing runs with T = 4. It shows a general tendency to move towards better configurations, but also shows that simulated annealing can jump to worse configurations when the temperature is still high. The right plot illustrates 3 runs of PSO with a swarm of 3 particles (solid, dashed, dotted), in which each particle visits 107/3 configurations. Careful observation shows that particles indeed have a tendency to move towards the swarm\u2019s best known position.\nBecause the evaluated search strategies are based on stochastic variables, we cannot draw conclusions from this limited set of experiments. Therefore, we ran each search experiment 128 times on different devices and with different parameters. For clarification: one search experiment explores 107 configurations. The results are shown in Fig. 5 as a violin plot: a combination of a boxplot and a rotated kernel density\nplot. We make several observations based on the results:\n\u2022 Auto-tuning pays off for 2D convolution for all devices: the distribution of the entire search-space (orange rightmost violin) shows that there are only very few configurations with good performance. In fact, setting the parameters to a random configuration yields a performance of less than 20% on average on each device. \u2022 Random search on 1/32th of the search-space is already sufficient to achieve an average performance of 77% on HD7970, 78% on K40m, 83% on GTX480, and as high as 92% on Iris. If that is not sufficient, then simulated annealing or full-search have to be used. \u2022 Simulated annealing performs better on average compared to random-search for the K40m and the GTX480. In some cases it shows bad worst-case performance, since the search might get stuck in a local optimum. The algorithm is moderately sensitive to T for these experiments. \u2022 PSO performs worse than random-search and is thus not suitable for this search-space. Future work will investigate how its performance changes given a larger swarm size and a longer search (more tested configurations)."}, {"heading": "C. Best-found results", "text": "The tuning parameters and their possible values are shown on the left in table II. The right hand side gives the best-found results for 3 different filter sizes and for two devices. These results again demonstrate the usefulness of auto-tuning: the optimal parameters vary across different filter sizes and across different devices, even those from the same vendor.\nTo demonstrate that our 2D convolution case-study is realistic and useful, we also investigate the absolute performance and compare against related work. Figure 6 first shows the performance relative to the theoretical architectural peak both in terms of GFLOPS and memory bandwidth2. We observe that our implementation follows convolution\u2019s mathematical property of becoming more compute intensive as filter sizes are increased. We also note that the GTX480 and HD7970 perform relatively better compared to the K40m due to their better balanced architecture. The Iris GPU is limited by its low memory bandwidth for 3x3 and 7x7. For 11x11, it suffers from a low register count.\nTo demonstrate the merits of tuning for a specific filter size, we evaluated the best-case parameters for the 3 filter sizes as found in table II on the other filter sizes. The results are shown in table III, in which relative performance is shown. In the worst case, only 64% of the performance is achieved: 56% performance can be gained when running an 11x11 filter with parameters tuned for a 3x3 filter.\n2GFLOPS computed as (1+2\u00b7Xf \u00b7Yf )\u00b7X\u00b7Y t and bandwidth as 2\u00b7X\u00b7Y t with\nt being the execution time.\nThe state-of-the-art in 2D convolution uses adaptive-tiling, a form of auto-tuning [21]. In their work, the authors also experiment on a GTX480 and achieve 326 GFLOPS for 3x3, 550 GFLOPS for 7x7, and 601 GFLOPS for 11x11. As shown on the right hand side of Fig. 6, we are able to match these numbers for 3x3 and 7x7 and even improve upon them for 11x11. Our improved performance is because they explore a smaller search space: no Ywpt, no vector loads and stores, and no extra halo threads. An alternative to 2D convolution is an FFT in frequency space, but as discussed in [21], 2D convolution is significantly faster for filter sizes below 19 by 19 compared to the highly optimized cuFFT library."}, {"heading": "VI. CASE-STUDY: MATRIX-MULTIPLICATION", "text": "Our second case-study concerns generalised dense matrixmatrix multiplication (GEMM) as found in the BLAS linear algebra libraries. This case-study is motivated partly by the same reasons as 2D convolution: matrix-multiplication is one of the key components of deep learning and other machine learning algorithms. Furthermore, its wide applicability and FLOP-heavy computation make it a popular target for autotuning. Some examples of recent OpenCL auto-tuning work on GEMM are [14], [16] and the clBLAS library.\nIn this case-study, we consider the multiplication of two matrices A and B according to:\nC = \u03b1ATB+ \u03b2C\nin which \u03b1 and \u03b2 are constants and AT is the transposed version of A. Apart from expecting a transposed input, we also assume that the matrix dimensions are powers of 2 and multiples of the tile sizes (see below). These assumptions can be resolved by a relatively-cheap pre-processing kernel, as is also suggested in [16]."}, {"heading": "A. Tuning parameters", "text": "We implemented a highly tunable parallel version of matrix-multiplication in OpenCL, inspired by [16] and the clBLAS library. As far as possible, we use the same parameter names as in [16]. The tuning parameters are illustrated in Fig. 8 and described below:\n1) 2D tiling is employed at workgroup-level using the parameters Mwg , Nwg , Kwg corresponding to the M , N and K matrix dimensions. 2) The local workgroup size is tunable in 2 dimensions: MdimC and NdimC . Combined with the two corresponding tiling parameters, this also defines the amount of work-per-thread in the M and N dimensions: Mwi = Mwg/MdimC and Nwi = Nwg/NdimC . Here, wi is an abbreviation for workitem (a thread), since coarsening is implemented using 2D register tiling at thread-level. 3) Caching of the 2D workgroup tile can be controlled per input matrix using L$A and L$B : manual caching using the local memory is enabled when set to yes.\n4) The local memory (when enabled) can be re-shaped according to MdimC \u00b7 NdimC = MdimA \u00b7 KdimA = KdimB \u00b7 NdimB . Here, MdimA and NdimB are extra tuning parameters and KdimA and KdimB are calculated according to the above equality. 5) A stride for accessing off-chip memory within a single thread can be enabled or disabled through Mstride (for matrices A and C) and Nstride (for matrix B). If enabled, the stride is set to MdimA and NdimB respectively, otherwise it is set to 1 (no stride). 6) The vector widths for loading and storing can be set using Mvec for matrices A and C and Nvec for matrix B. 7) The Kwg kernel-loop can be unrolled by a factor Kwi."}, {"heading": "B. Evaluation of the search strategies", "text": "As for 2D convolution, we first evaluate CLTune\u2019s search strategies. We use the same devices as listed in table I and use the same configurations of the same set of search strategies. For matrix-multiplication, our search-space consists of 241.600 unique configurations: 71x larger than for 2D convolution. The tested values are shown on the left in table IV. To keep our search experiments comparable in terms of run-time to 2D\nconvolution, we decide to explore only 1/2048th of the searchspace: 117 configurations. Our experiments consider squared matrices of 2048 by 2048 (M = N = K = 2048).\nAs before, we evaluate the search strategies by running each search 128 times. The violin plots are presented in Fig. 7. On top of the observations for convolution, we conclude that:\n\u2022 It is easier to find a good performing configuration compared to 2D convolution, judging by the average performance of the search space. \u2022 Simulated annealing and PSO perform well on all devices, in all cases outperforming the random-search strategy. \u2022 The GTX480 has a better balanced architecture for GEMM compared to the newer K40m, judging by the wider top of the search-space violin (orange, right hand side) and the high average and low standard deviation for the various search strategies."}, {"heading": "C. Best-found results", "text": "The best-found parameters for single-precision matrixmultiplication are given in table IV. The table shows a significant variety in best parameters across the four different devices, again demonstrating the merits of CLTune.\nThe best-found results are compared against the peak theoretical capabilities of the devices and against two libraries: cuBLAS 7.0 for K40m and 5.5 for GTX480, and clBLAS\n2.4.0 after running the included tuner. The results are shown in Fig. 9, which shows that our matrix-multiplication outperforms the clBLAS library in all cases, including the AMD GPU for which clBLAS was originally developed. We are not able to match cuBLAS on the K40m, as it uses assembly-level optimisations to reduce register pressure and remove registerbank conflicts [12]. Furthermore, we are not able to use CUDA\u2019s ldg instruction.\nTo demonstrate the merits of tuning for a specific device, we evaluated all best-case parameters from table IV on the K40m GPU. Using the parameters of the GTX480, HD7970 and Iris, we found a performance of respectively 50%, 61% and 79% of the K40m\u2019s best results. In other words, up to a factor of 2 can be gained by tuning for a specific device.\nOur GEMM implementation is roughly on-par with the auto-tuning work by Matsumoto et al. [16], [17]: they reach 2913 GFLOPS on a 18% lower clocked HD7970. Unfortunately, their OpenCL kernel implementation is not publicly available. Other work improves upon cuBLAS 4.0 by using assembly-level optimisations for NVIDIA GPUs [12], which are now integrated in the tested versions of cuBLAS."}, {"heading": "VII. CONCLUSIONS", "text": "This work introduced the CLTune auto-tuner for OpenCL kernels. We saw that the generic and open-source CLTune is easy to use and can be used for off-line or on-line tuning. Based on two case-studies and an evaluation on 4 different devices (Tesla K40m, GeForce GTX480, Radeon HD7970, Iris 5100), we conclude that the two advanced search strategies simulated annealing and particle swarm optimisation both have their merits, although their efficacy depends on the problem at hand. We furthermore demonstrated the merits of CLTune by 1) exploring a search space of more than two-hundred thousand configurations, 2) identifying significant differences between best-case parameters across devices, and 3) showing that an OpenCL 2D convolution code can be tuned to input arguments.\nWith the help of CLTune, we created two OpenCL kernels which are able to match or even improve upon the state-ofthe-art: 2D convolution and matrix-multiplication (GEMM). Both have an important advantage over existing work: performance portability and availability on all OpenCL devices. Furthermore, our 2D convolution code is the only tuned implementation available on non-CUDA devices, and our matrixmultiplication is the fastest publicly available source-level implementation for GPUs."}], "references": [{"title": "et al", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I. Goodfellow", "A. Bergeron"], "venue": "Theano: Deep Learning on GPUs with Python. In NIPS \u201911: Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "From CUDA to OpenCL: Towards a Performance-Portable Solution for Multi-Platform GPU Programming", "author": ["P. Du", "R. Weber", "P. Luszczek", "S. Tomov", "G. Peterson", "J. Dongarra"], "venue": "Parallel Computing, 38(8):391 \u2013 407", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards a Tunable Multi-Backend Skeleton Programming Framework for Multi-GPU Systems", "author": ["J. Enmyren", "C. Kessler"], "venue": "MCC-3: Swedish Workshop on Multicore Computing", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Auto-tuning a High-Level Language Targeted to GPU Codes", "author": ["S. Grauer-Gray", "L. Xu", "R. Searles", "S. Ayalasomayajula", "J. Cavazos"], "venue": "INPAR: Workshop on Innovative Parallel Computing", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatically Generating and Tuning GPU Code for Sparse Matrix-Vector Multiplication from a High- Level Representation", "author": ["D. Grewe", "A. Lokhmotov"], "venue": "GPGPU-4: General Purpose Processing on Graphics Processing Units. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "MM \u201914: Int. Conf. on Mult. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Particle Swarm Optimization", "author": ["J. Kennedy", "R. Eberhart"], "venue": "International Conference on Neural Networks. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "A Script- Based Autotuning Compiler System to Generate High-Performance CUDA Code", "author": ["M. Khan", "P. Basu", "G. Rudy", "M. Hall", "C. Chen", "J. Chame"], "venue": "ACM TACO,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Optimization by Simulated Annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science, 220(4598):671\u2013680", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Optimization by Direct Search: New Perspectives on some Classical and Modern Methods", "author": ["T.G. Kolda", "R.M. Lewis", "V. Torczon"], "venue": "SIAM review, 45(3):385\u2013482", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Imagenet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS \u201912: Advances in Neural Information Processing Systems, pages 1097\u20131105", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance Upper Bound Analysis and Optimization of SGEMM on Fermi and Kepler GPUs", "author": ["J. Lai", "A. Seznec"], "venue": "CGO \u201913: Code Generation and Optimization. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "OpenMPC: Extended OpenMP Programming and Tuning for GPUs", "author": ["S. Lee", "R. Eigenmann"], "venue": "SC: Int. Conf. on High Performance Computing Networking, Storage and Analysis. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "A Note on Auto-tuning GEMM for GPUs", "author": ["Y. Li", "J. Dongarra", "S. Tomov"], "venue": "ICCS: Int. Conf. on Computational Science. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "MPFFT: An Auto-Tuning FFT Library for OpenCL GPUs", "author": ["Y. Li", "Y.-Q. Zhang", "Y.-Q. Liu", "G.-P. Long", "H.-P. Jia"], "venue": "Journal of Computer Science and Technology, 28(1):90\u2013105", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance Tuning of Matrix Multiplication in OpenCL on Different GPUs and CPUs", "author": ["K. Matsumoto", "N. Nakasato", "S. Sedukhin"], "venue": "SC Companion. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Implementing Level-3 BLAS Routines in OpenCL on Different Processing Units", "author": ["K. Matsumoto", "N. Nakasato", "S. Sedukhin"], "venue": "Technical Report TR 2014-001, The University of Aizu", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Image Convolution with CUDA", "author": ["V. Podlozhnyuk"], "venue": "Technical report, NVIDIA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolution Engine: Balancing Efficiency & Flexibility in Specialized Computing", "author": ["W. Qadeer", "R. Hameed", "O. Shacham", "P. Venkatesan", "C. Kozyrakis", "M.A. Horowitz"], "venue": "ISCA-40: International Symposium on Computer Architecture. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Maestro: Data Orchestration and Tuning for OpenCL Devices", "author": ["K. Spafford", "J. Meredith", "J. Vetter"], "venue": "Euro-Par \u201910. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing Convolution Operations on GPUs Using Adaptive Tiling", "author": ["B. Van Werkhoven", "J. Maassen", "H.E. Bal", "F.J. Seinstra"], "venue": "Future Gener. Comput. Syst.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Accelerated Particle Swarm Optimization and Support Vector Machine for Business Optimization and Applications", "author": ["X.-S. Yang", "S. Deb", "S. Fong"], "venue": "Communications in Computer and Information Science, 136:53\u201366", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Despite these advancements, achieving close-to-peak performance remains a task for expert programmers in many cases [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Although OpenCL code is portable across devices, it is definitely not performance portable: to achieve good performance it is necessary to tune design parameters, adjust the hierarchy of parallelism, and explore different algorithms [2].", "startOffset": 233, "endOffset": 236}, {"referenceID": 1, "context": "However, beyond that, even for devices coming from the same vendor or with the same architectural family, it might be worthwhile to explore workgroup configurations, loop unroll factors, or vector widths [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Second, we present two case-studies inspired by the recent successes with convolutional neural networks for deep-learning on GPUs [6], [11]: 2D convolution and matrix-multiplication (GEMM).", "startOffset": 130, "endOffset": 133}, {"referenceID": 10, "context": "Second, we present two case-studies inspired by the recent successes with convolutional neural networks for deep-learning on GPUs [6], [11]: 2D convolution and matrix-multiplication (GEMM).", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "For 2D convolution, our tuner matches the CUDAbased state-of-the-art [21] and achieves up to 1658 GFLOPS on a 11x11 filter and 207GB/s on a 3x3 filter running on an AMD HD7970 GPU.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "We also tune matrix-multiplication, matching the stateof-the-art [17], and achieving better performance compared to the clBLAS library.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 83, "endOffset": 86}, {"referenceID": 13, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 130, "endOffset": 134}, {"referenceID": 14, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "A more generic OpenCL auto-tuner is Maestro data-orchestration tuner [20], however it is orthogonal to this work since it works on data transfers rather than kernel.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "Examples include tuners for a GPU skeleton programming framework [3], as part of a parallelizing compiler [8], for OpenMP 4.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Examples include tuners for a GPU skeleton programming framework [3], as part of a parallelizing compiler [8], for OpenMP 4.", "startOffset": 106, "endOffset": 109}, {"referenceID": 12, "context": "0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1].", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1].", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "Even derivative free methods such as direct search [10] are not suitable, since they assume that it is relatively cheap to explore all neighbours of a particular configuration.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Simulated annealing Simulated annealing (SA) is a heuristic search method inspired by annealing in metallurgy [9] which iteratively moves through the search-space from neighbour to neighbour and ends after a fixed number of iterations or when a certain criterion is reached.", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Particle swarm optimisation Particle swarm optimisation (PSO) is an evolutionary search strategy in which a swarm of S communicating particles explore the search-space [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 21, "context": "A variant of PSO is accelerated PSO [22], in which the new position in each dimension d is directly calculated based on the old position, removing the concept of velocity: x i,d = \u03b1 d + \u03b2p t i,d + \u03b3g t d + (1\u2212 \u03b1\u2212 \u03b2 \u2212 \u03b3)xi,d in which \u03b1, \u03b2 and \u03b3 are probability parameters and d represents a random number within the range of the parameter in dimension d.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "4, \u03b2 = 0 (no local-best influence as argued by [22]), \u03b3 = 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11].", "startOffset": 101, "endOffset": 104}, {"referenceID": 10, "context": "Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "Convolution is also important outside the context of deep-learning, as is discussed in [19], in which it is also shown that convolution can be generalized to other image and video processing operations such as SAD and SIFT.", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Tuning parameters We implemented a highly tunable implementation of 2D convolution in OpenCL inspired by [18] and [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "Tuning parameters We implemented a highly tunable implementation of 2D convolution in OpenCL inspired by [18] and [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "Details of our implementation are not the focus of this work, but the source is available for inspection as part of the CLTune examples and further discussion can be found in related work [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "The state-of-the-art in 2D convolution uses adaptive-tiling, a form of auto-tuning [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "An alternative to 2D convolution is an FFT in frequency space, but as discussed in [21], 2D convolution is significantly faster for filter sizes below 19 by 19 compared to the highly optimized cuFFT library.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Some examples of recent OpenCL auto-tuning work on GEMM are [14], [16] and the clBLAS library.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "Some examples of recent OpenCL auto-tuning work on GEMM are [14], [16] and the clBLAS library.", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "These assumptions can be resolved by a relatively-cheap pre-processing kernel, as is also suggested in [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Tuning parameters We implemented a highly tunable parallel version of matrix-multiplication in OpenCL, inspired by [16] and the clBLAS library.", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "As far as possible, we use the same parameter names as in [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "We are not able to match cuBLAS on the K40m, as it uses assembly-level optimisations to reduce register pressure and remove registerbank conflicts [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "[16], [17]: they reach 2913 GFLOPS on a 18% lower clocked HD7970.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16], [17]: they reach 2913 GFLOPS on a 18% lower clocked HD7970.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "0 by using assembly-level optimisations for NVIDIA GPUs [12], which are now integrated in the tested versions of cuBLAS.", "startOffset": 56, "endOffset": 60}], "year": 2017, "abstractText": "This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and tunes kernel performance of a generic, user-defined search space of possible parametervalue combinations. Example parameters include the OpenCL workgroup size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g. matrix dimensions). The auto-tuner is generic, easy to use, open-source, and supports multiple search strategies including simulated annealing and particle swarm optimisation. CLTune is evaluated on two GPU case-studies inspired by the recent successes in deep learning: 2D convolution and matrixmultiplication (GEMM). For 2D convolution, we demonstrate the need for auto-tuning by optimizing for different filter sizes, achieving performance on-par or better than the state-of-the-art. For matrix-multiplication, we use CLTune to explore a parameter space of more than two-hundred thousand configurations, we show the need for device-specific tuning, and outperform the clBLAS library on NVIDIA, AMD and Intel GPUs.", "creator": "LaTeX with hyperref package"}}}