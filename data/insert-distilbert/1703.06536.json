{"id": "1703.06536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient", "abstract": "a data selective classifier ( f, g ) comprises a classification function f and a binary selection partition function g, which determines if the main classifier abstains accurately from prediction, identifies or uses f to predict. the classifier is called pointwise - competitive if it classifies each point identically compared to the best classifier in hindsight ( from the same class ), whenever it does not abstain. the quality of such a classifier is quantified by its effective rejection mass, defined to be the probability denial mass of the points then it effectively rejects. a \" fast \" rejection time rate is achieved efficient if the rejection mass is bounded from parameters above by o ( 1 / m ) where criterion m is the number of labeled examples actually used to train the different classifier ( and o hides logarithmic factors ). pointwise - competitive selective ( pcs ) classifiers are intimately related to computational disagreement - based active learning system and it is known that in the above realizable case, a fast rejection rate of a known pcs algorithm ( called consistent selective strategy ) is genetically equivalent to avoiding an exponential speedup of the well - known cal active algorithm.", "histories": [["v1", "Sun, 19 Mar 2017 23:22:31 GMT  (726kb,D)", "http://arxiv.org/abs/1703.06536v1", null], ["v2", "Thu, 30 Mar 2017 11:08:28 GMT  (252kb,D)", "http://arxiv.org/abs/1703.06536v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roei gelbhart", "ran el-yaniv"], "accepted": false, "id": "1703.06536"}, "pdf": {"name": "1703.06536.pdf", "metadata": {"source": "CRF", "title": "The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient", "authors": ["Roei Gelbhart", "Ran El-Yaniv"], "emails": ["ROEIGE@CS.TECHNION.AC.IL", "RANI@CS.TECHNION.AC.IL"], "sections": [{"heading": null, "text": "We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke\u2019s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke\u2019s disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke\u2019s disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS. Keywords: Active learning, selective prediction, disagreement coefficient, selective sampling, selective classification, reject option, pointwise-competitive, selective classification, statistical learning theory, PAC learning, sample complexity, agnostic case"}, {"heading": "1. Introduction", "text": "Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2]. Given a training sample consisting of m labeled instances, the learning algorithm is required to output a selective classifier [3], defined to be a pair ( f ,g), where f is a prediction function, chosen from some hypothesis class F , and g : X \u2192 {0,1} is a selection function, serving as a qualifier for f as follows: for any x, if g(x) = 1, the classifier predicts f (x), and otherwise it abstains. The general performance of a selective classifier is quantified in terms of its coverage and risk, where coverage is the probabilistic mass of non-rejected instances, and risk is\nar X\niv :1\n70 3.\n06 53\n6v 1\n[ cs\n.L G\n] 1\n9 M\nthe normalized average loss of f restricted to non-rejected instances. Let f \u2217 be any (unknown) true risk minimizer1 in F for the given problem. The selective classifier ( f ,g) is said to be pointwisecompetitive if, for each x with g(x) = 1, it must hold that f (x) = f \u2217(x) for all f \u2217 \u2208 F [4]. Thus, pointwise-competitiveness w.h.p. over choices of the training sample, is a highly desirable property: it guarantees, for each non-rejected test point, the best possible classification obtainable using the best in-hindsight classifier from F . We don\u2019t restrict g to be from any specific hypothesis class, however, because we use disagreement based selective prediction, the selection of F will limit the possibilities of g. The scenario of a predefined decision functions hypothesis class is investigated in [5].\nPointwise-competitive selective classification (PCS) was first considered in the realizable case [3], for which a simple consistent selective strategy (CSS) was shown to achieve a bounded and monotonically increasing (with m) coverage in various non-trivial settings. Note that in the realizable case, any PCS strategy attains zero risk (over the sub-domain it covers). These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown. These bounds relied on the fact that the underlying probability distribution and the hypothesis class F will satisfy the so-called \u201c(\u03b21,\u03b22)-Bernstein property\u201d [7]. The coverage bounds in [4, 6] are dependent on the parameters \u03b21,\u03b22. This Bernstein property assumption (as presented in [7]), which allows for better concentration, can be problematic. First, it is defined with respect to a unique true risk minimizer f \u2217, a property which is unlikely to hold in noisy agnostic settings. Moreover, for arbitrary F , even for the 0/1 loss function, it is not necessarily known whether the Bernstein property can hold at all.2 We removed the Bernstein assumption from our analysis. Assuming that a selective classifier is w.h.p. pointwise-competitive, our key goal is a small rejection rate. We will say that a learner has a fast R\u2217 rejection rate, if w.h.p. the rejection rate is bounded by\npolylog( 1 R( f \u2217)+1/m ) \u00b7R( f \u2217)+ polylog(m,d,1/\u03b4) m .\nSelective classification is very closely related to the field of active learning (AL). In active learning, the learner can actively influence the learning process by selecting the points to be labeled. The incentive for introducing this extra flexibility is to reduce labeling efforts. A key question in theoretical studies of AL is how many label requests are sufficient to learn a given (unknown) target concept to a specified accuracy, a quantity called label complexity. For an AL algorithm satisfying the \u201cpassive example complexity\u201d property (consuming the same number of labeled/unlabeled examples as a passive algorithm for achieving the same error; see Definition 6.2), we will say it has R\u2217 exponential speedup, if w.h.p. the number of labels it requests is bounded by\npolylog( 1\nR( f \u2217)+1/m ) \u00b7R( f \u2217)m+polylog(m,d,1/\u03b4).\nThe connection between AL and confidence-rated prediction is quite intuitive. A (pointwisecompetitive) selective classifier P can be straightforwardly used as the querying component of an active learning algorithm. This reduction is most naturally demonstrated in the stream-based AL\n1. We assume that there exists an f \u2217 in F . Otherwise, we can artificially define f \u2217 to be any function whose risk is sufficiently close to inf f\u2208F (R( f )), for instance, not greater than a small multiplicative factor from this infimum. 2. It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.\nmodel: at each iteration, the active algorithm trains a selective classifier on the currently available labeled samples, and then decides to query a newly introduced (unlabeled) point x if P abstains on x.\nHanneke\u2019s disagreement coefficient [9] (see Definition 2.1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11]. The disagreement coefficient is the supremum of the relation between the disagreement mass of functions that are r-distanced from f \u2217 to r, over r. PCS classification is based on using generalization bounds to estimate the empirical error of f \u2217, and more specifically, its distance from the empirical error of the ERM. Whenever there is a unanimous agreement of all the functions that reside within a ball around the ERM, the classifier choses to classify. Thus, the abstain rate is dependent on the disagreement mass of the functions within the ball. The radius of the ball depends on the generalization bounds. The generalization bounds we use are of the form O\u0303(1/m) for the realizable case (we consider the realizable case here for simplicity). After observing m examples, we can bound the disagreement mass of a ball around the ERM, by multiplying the radius of the ball, which is O\u0303(1/m), with the disagreement coefficient. Thus, if for example, the disagreement coefficient is bounded by a constant, the abstain rate of some PCS algorithms can be bounded by O\u0303(1/m) for the realizable case. This gives a basic idea of the disagreement coefficient, which will be formally presented later on.\nNote that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13]. Specifically, an O(polylog(m)log(1/\u03b4)) version space compression set size minimal bound was shown in [12, Corollary 11], to be equivalent to an O(polylog(1/r)) disagreement coefficient.\nThe first contribution of this paper is a novel selective classifier, called ILESS, which utilizes a tighter generalization error bound than LESS and depends on R( f \u2217) (and interpolates the agnostic and realizable cases). Most importantly, the new strategy can be analyzed completely without the Bernstein condition.\nWe derive an active learning algorithm, called Active-ILESS, corresponding to our selective classifier, ILESS. Active-ILESS is constructed to work in a stream-based AL model and its querying function is extremely conservative: for each unlabeled example, the algorithm requests its label if and only if the labeling of the optimal classifier (from the same class) on this point cannot be inferred from information already acquired. This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18]. In [19], a computationally efficient algorithm for disagreement based AL.\nThe first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL. This result applies to the realizable setting only. Our first contribution is a similar equivalence relation between pointwise-competitive selective classification and AL, which applies to the more challenging agnostic case and smoothly interpolates the realizable and agnostic settings.\nOur second and main contribution is to show a complete equivalence between (i) selective classification with a fast R\u2217 rejection rate, (ii) AL with R\u2217 exponential speedup (represented by ActiveILESS), and (iii) the existence of an f \u2217 with a bounded disagreement coefficient. This is illustrated in Figure 1, where the blue errors indicate the equivalence relationships we prove in this paper, and\nthe red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows)."}, {"heading": "2. Definitions", "text": "Consider a domain X , and a binary label set Y = {\u00b11}. A learning problem is specified via a hypothesis class F and an unknown probability distribution PX ,Y . Given a sequence of labeled training examples Sm = ((x1,y1),(x2,y2), ...,(xm,ym)), such that \u2200i,(xi,yi) \u2208 X \u00d7Y , the empirical error of a hypothesis f over Sm is R\u0302( f ,Sm) , 1m \u2211 m i=1 `( f (xi),yi), where ` : Y \u00d7Y \u2192 R+ is a loss function. In this paper we will mainly focus on the zero-one loss function, `01(y,y\u2032) , 1{y 6= y\u2032}. The true (zero-one) error of f is R( f ) , EP [`01( f (x),y)]. An empirical risk minimizer hypothesis (henceforth an ERM) is\nf\u0302 (Sm), argmin f\u2208F R\u0302( f ,Sm), (1)\nand a true risk minimizer is f \u2217 , argmin f\u2208F R( f ). 3\n3. We assume that f \u2217 exists, and that it need not be unique, in which case f \u2217 refers to any one of the minimizers.\nWe acquire the following definitions from [4]. For any hypothesis class F , hypothesis f \u2208 F , distribution PX ,Y , sample Sm, and real number r > 0, define the true and empirical low-error sets,\nV ( f ,r), { f \u2032 \u2208 F : R( f \u2032)\u2264 R( f )+ r }\n(2)\nand V\u0302 ( f ,r), { f \u2032 \u2208 F : R\u0302( f \u2032,Sm)\u2264 R\u0302( f ,Sm)+ r } . (3)\nLet G\u2286 F . The disagreement set [9] and agreement set [3] w.r.t. G are defined, respectively, as\nDIS(G), {x \u2208 X : \u2203 f1, f2 \u2208 G s.t. f1(x) 6= f2(x)} (4)\nand AGR(G), {x \u2208 X : \u2200 f1, f2 \u2208 G s.t. f1(x) = f2(x)} . (5)\nIn selective classification [3], the learning algorithm receives Sm and is required to output a selective classifier, defined to be a pair ( f ,g), where f \u2208 F is a classifier, and g : X \u2192 {0,1} is a selection function, serving as a qualifier for f as follows. For any x \u2208 X , ( f ,g)(x) = f (x) iff g(x) = 1. Otherwise, the classifier outputs \u201cI don\u2019t know\u201d. For any selective classifier ( f ,g) we define its coverage to be\n\u03a6( f ,g), Pr X\u223cPX (g(X) = 1),\nand its complement, 1\u2212\u03a6, is called the abstain rate. For any f \u2208 F and r > 0, define the set B( f ,r) of all hypotheses that reside within a ball of radius r around f ,\nB( f ,r), {\nf \u2032 \u2208 F : Pr X\u223cPX\n{ f \u2032(X) 6= f (X) } \u2264 r } .\nFor any G\u2286 F , and distribution PX , we denote by \u2206G the volume of the disagreement set of G (see (4)), \u2206G , Pr{DIS(G)}.\nDefinition 2.1 (Disagreement Coefficient) Let r0 \u2265 0. Then, Hanneke\u2019s disagreement coefficient [9] of a classifier f \u2208 F with respect to the target distribution PX is\n\u03b8 f (r0), sup r>r0 \u2206B( f ,r) r , (6)\nand the general disagreement coefficient of the entire hypothesis class F is\n\u03b8(r0), sup f\u2208F \u03b8 f (r0). (7)\nNotice that this definition of the disagreement coefficient is independent of PY |X . Another commonly used definition of the disagreement coefficient does depend on a true risk minimizer f \u2217, as follows:\n\u03b8\u2032(r0) = sup r>r0 \u2206B( f \u2217,r) r . (8)\nClearly, it always holds that \u03b8\u2032 \u2264 \u03b8. The independence of \u03b8 of unknown quantities such as the underlying distribution (and f \u2217), however, is a convenient property that sometimes allows for a direct estimation of \u03b8, which only depends on the marginal distribution, PX . This is, for example, the case in active learning, where labels are expensive but information about the marginal distribution\n(provided by unlabeled examples) is cheap. Note also that the above definition of \u03b8\u2032 implicitly assumes a unique f \u2217. Nevertheless, the definition can be extended to cases where f \u2217 is not unique, in which case the infimum over all f \u2217 can be considered (the analysis can be extended accordingly using limits). For more on the disagreement coefficient, and examples of probabilities distributions and hypothesis classes for which it is bounded, see [16]."}, {"heading": "3. Convergence Bounds and LESS", "text": "We use a uniform convergence bound from [18, 23]. Define convergence slacks \u03c3R\u2212R\u0302(m,\u03b4,d,R, R\u0302) and \u03c3R\u0302\u2212R(m,\u03b4,d,R, R\u0302), given in terms of the training sample, Sm, its size, m, the confidence parameter, \u03b4, and the VC-dimension d of the class F . For any f \u2208 F ,\n\u03c3R\u2212R\u0302(m,\u03b4,d,R, R\u0302), min{ 4d ln(16med\u03b4 )\nm +\n\u221a 4d ln(16med\u03b4 )\nm \u00b7 R\u0302\ufe38 \ufe37\ufe37 \ufe38\n\u03c3\u0302R\u2212R\u0302(m,\u03b4,d,R\u0302)\n,\n\u221a 4d ln(16med\u03b4 ) m \u00b7R\ufe38 \ufe37\ufe37 \ufe38\n\u03c3\u0304R\u2212R\u0302(m,\u03b4,d,R)\n} (9)\nand\n\u03c3R\u0302\u2212R(m,\u03b4,d,R, R\u0302), min{ 4d ln(16med\u03b4 )\nm +\n\u221a 4d ln(16med\u03b4 )\nm \u00b7R\ufe38 \ufe37\ufe37 \ufe38\n\u03c3\u0304R\u0302\u2212R(m,\u03b4,d,R)\n,\n\u221a 4d ln(16med\u03b4 ) m \u00b7 R\u0302\ufe38 \ufe37\ufe37 \ufe38\n\u03c3\u0302R\u0302\u2212R(m,\u03b4,d,R\u0302)\n}. (10)\nTo simplify the analysis, we further decompose the above slack terms to their empirical and nonempirical components. For (9), we thus have, respectively,\n\u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302), 4d ln(16med\u03b4 )\nm +\n\u221a 4d ln(16med\u03b4 )\nm \u00b7 R\u0302 (11)\nand\n\u03c3\u0302R\u0302\u2212R(m,\u03b4,d, R\u0302),\n\u221a 4d ln(16med\u03b4 )\nm \u00b7 R\u0302. (12)\nSimilarly, the non-empirical part in these minimums are denoted by \u03c3\u0304R\u2212R\u0302 and \u03c3\u0304R\u0302\u2212R. With this notation, we can write, for example, \u03c3R\u2212R\u0302 = min{\u03c3\u0302R\u2212R\u0302, \u03c3\u0304R\u2212R\u0302}. Our Lemma 1 is taken from [18, Lemma 1], which is based on [23, Theorem 7] 4.\nLemma 1 ([18]) Let F be a hypothesis class with VC-dimension d. For any 0 < \u03b4 < 1, with probability of at least 1\u2212\u03b4 over the choice of Sm from P m, any hypothesis f \u2208 F satisfies\nR( f )\u2264 R\u0302( f )+\u03c3R\u2212R\u0302 ( m,\u03b4,d,R( f ), R\u0302( f ) ) (13)\nR\u0302( f )\u2264 R( f )+\u03c3R\u0302\u2212R ( m,\u03b4,d,R( f ), R\u0302( f ) ) . (14)\nStrategy 1 is the LESS algorithm of [4]. LESS learns w.h.p. a pointwise-competitive selective classifier, ( f ,g), where f \u2208F and g : X \u2192{0,1} is its selection function which determines whether to abstain or to classify. A pointwise-competitive selective classifier must satisfy the following condition: for each x with g(x) = 1, it must hold that f (x) = f \u2217(x) for all f \u2217 \u2208 F .\n4. In the original lemma from [18], there appears S(H ,n), the growth function. We plug in Sauer\u2019s Lemma, S(H ,n)\u2264 ( emd ) d , into Lemma 1 from [18] to get our lemma.\nRemark 2 The original definition of pointwise-competitiveness from [4] requires a single f \u2217. We widen the definition to cases for which there are more than one f \u2217, and require that a pointwisecompetitive selective classifier will be equal to all f \u2217, wherever g = 1. This extrapolation seems a bit strict. However, even if the requirement would have been relaxed to \u201cany f \u2217\u201d, any pointwisecompetitive selective classifier would still have been forced to identify with all f \u2217, as it is impossible to differentiate whether a set of functions are all f \u2217, or one is better than the rest.\nThe main idea behind LESS is that, w.h.p. all f \u2217 lie within a ball around an ERM hypothesis with error radius of 2\u03c3(m,\u03b4/4,d), where\n\u03c3(m,\u03b4,d), 2\n\u221a 2d ( ln 2med ) + ln 2\u03b4\nm (15)\nis the slack term of a certain uniform convergence bound. Therefore, if all the functions in that ball agree over the labeling of any instance x, we know with high probability that all f \u2217 label x the same way as the ERM. This property ensures that LESS is pointwise-competitive w.h.p.\nStrategy 1 Agnostic low-error selective strategy (LESS) Input: Sample set of size m, Sm,\nConfidence level \u03b4 Hypothesis class F with VC dimension d\nOutput: A selective classifier (h,g) 1: Set f\u0302 = ERM(F ,Sm), i.e., f\u0302 is any empirical risk minimizer from F 2: Set G = V\u0302 ( f\u0302 ,2\u03c3(m,\u03b4/4,d) ) 3: Construct g such that g(x) = 1\u21d0\u21d2 x \u2208 {X \\DIS (G)} 4: f = f\u0302"}, {"heading": "4. ILESS", "text": "Strategy 2 Improved Low-Error Selective Strategy (ILESS) Input: Sample set of size m, Sm,\nConfidence level \u03b4 Hypothesis class F with VC dimension d\nOutput: A selective classifier (h,g) 1: Set f\u0302 = ERM(F ,Sm), i.e., f\u0302 is any empirical risk minimizer from F 2: Set \u03c3ILESS = \u03c3\u0302R\u2212R\u0302 ( m,\u03b4,d, R\u0302( f\u0302 ,Sm) ) + \u03c3\u0304R\u0302\u2212R ( m,\u03b4,d, R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm)) ) 3: Set G = V\u0302 ( f\u0302 ,\u03c3ILESS\n) 4: Construct g such that g(x) = 1\u21d0\u21d2 x \u2208 {X \\DIS (G)} 5: h = f\u0302\nIn this section we introduce an improved version of LESS, called ILESS, which utilizes a radius of the form polylog(m,1/\u03b4,d) \u00b7 ( 1m + \u221a R( f \u2217) m ). Noting that the radius, 2\u03c3(m,\u03b4/4,d), used by LESS to define G = V\u0302 , is of the form polylog(m,1/\u03b4,d)/ \u221a\nm, we observe that in cases where R( f \u2217)\u2248 Cm , this new radius behaves as polylog(m,1/\u03b4,d)m . We later show that this radius allows ILESS to achieve a faster rejection decay rate than the one achieved by LESS.\nConsider the pseuodo-code of ILESS given in Strategy 2. We now analyze ILESS, and begin by showing in Lemma 3 that ILESS is pointwise-competitive w.h.p., i.e., for any x for which g(x) = 1, f (x) = f \u2217(x) for all f \u2217. The calculation of g appears to be very problematic, as for a specific x, a unanimous decision over an infinite number of functions must be ensured. This problem was shown to be reducible to finding an ERM under one constraint ([24, Lemma 6.1] a.k.a. the disbelief principle). This is a difficult problem, nonetheless, albeit one that could be estimated with heuristics.\nDefinition 4.1 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown probability distribution. Given a sample set Sm, drawn from PX ,Y , we denote by E the event where both inequalities (13) and (14) of Lemma 1 simultaneously hold. We know from the lemma that E occurs with probability of at least 1\u2212\u03b4.\nLemma 3 (ILESS is pointwise-competitive) Given that event E occurred (see Definition 4.1), for all f \u2217 \u2208 F , f \u2217 resides within G (from Strategy 2), and therefore, ILESS is pointwise-competitive w.h.p.\nProof From (14) it follows that,\nR\u0302( f \u2217,Sm) \u2264 R( f \u2217)+\u03c3R\u0302\u2212R(m,\u03b4,d,R( f \u2217), R\u0302( f \u2217,Sm))\n\u2264 R( f \u2217)+ \u03c3\u0304R\u0302\u2212R(m,\u03b4,d,R( f \u2217)). (16)\nAdditionally, by the definition of f \u2217, we know that it has the lowest true error, and using Inequality (13) from Lemma 1 we obtain,\nR( f \u2217) \u2264 R( f\u0302 ) \u2264 R\u0302( f\u0302 ,Sm)+\u03c3R\u2212R\u0302(m,\u03b4,d,R( f\u0302 ), R\u0302( f\u0302 ,Sm)) \u2264 R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm)). (17)\nFinally, by applying (17) in (16), we have,\nR\u0302( f \u2217,Sm)\u2264 R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm)) + \u03c3\u0304R\u0302\u2212R ( m,\u03b4,d, R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm)) ) ,\nR\u0302( f \u2217,Sm) \u2264 R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm))+ \u03c3\u0304R\u0302\u2212R ( m,\u03b4,d, R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm)) ) , which means that f \u2217 \u2208 G.\nLemma 4 below bounds the radius \u03c3ILESS of ILESS. The lemma utilizes the notation\nA , 4d ln( 16me\nd\u03b4 ),\nwith which, by the definition of \u03c3ILESS (see Strategy 2), we have, \u03c3ILESS = \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm))+ \u03c3\u0304R\u0302\u2212R ( m,\u03b4,d, R\u0302( f\u0302 ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302( f\u0302 ,Sm)) ) =\nA m + \u221a A m \u00b7 R\u0302( f\u0302 ,Sm)+ A m + \u221a A m \u00b7 [R\u0302( f\u0302 ,Sm)+ A m + \u221a A m \u00b7 R\u0302( f\u0302 ,Sm)].\n(18)\nLemma 4 Given that event E (see Definition 4.1) occurred, the radius of ILESS satisfies\n\u03c3ILESS = 6 A m +3 \u221a A m \u00b7R( f \u2217) = O(A m + \u221a A m \u00b7R( f \u2217)), (19)\nwhere A , 4d ln(16med\u03b4 ).\nProof Under our assumption, inequalities (13) and (14) hold for every f \u2208 F . We thus have\nR\u0302( f\u0302 ,Sm)\u2264 R\u0302( f \u2217,Sm)\u2264 R( f \u2217)+ A m + \u221a A m \u00b7R( f \u2217). (20)\nReplacing the three occurrences of R\u0302( f \u2217,Sm) in (18) with the R.H.S. of (20), and using the basic inequalities \u221a A+B\u2264 \u221a A+ \u221a B and \u221a AB\u2264 A/2+B/2, we get,\n\u03c3ILESS \u2264 A m + \u221a\u221a\u221a\u221aA m \u00b7 ( R( f \u2217)+ A m + \u221a A m \u00b7R( f \u2217) ) + A m +\n+ \u221a\u221a\u221a\u221a\u221aA m \u00b7 R( f \u2217)+ A m + \u221a A m \u00b7R( f \u2217)+ A m + \u221a\u221a\u221a\u221aA m \u00b7 ( R( f \u2217)+ A m + \u221a A m \u00b7R( f \u2217) ) \u2264 A\nm + \u221a A m \u00b7 ( R( f \u2217)+ A m + A 2m + 1 2 R( f \u2217) ) + A m +\n+ \u221a\u221a\u221a\u221aA m \u00b7 [ R( f \u2217)+ A m + A 2m + 1 2 R( f \u2217)+ A m + \u221a A m \u00b7 ( R( f \u2217)+ A m + A 2m + 1 2 R( f \u2217) )]\n\u2264 2A m + 3A 2m + 3 2 \u221a A m \u00b7R( f \u2217)+ \u221a\u221a\u221a\u221aA m \u00b7 [ 5A 2m + 3 2 R( f \u2217)+ \u221a A m \u00b7 ( 3A 2m + 3 2 R( f \u2217) )]\n\u2264 7A 2m + 3 2 \u221a A m \u00b7R( f \u2217)+ \u221a\u221a\u221a\u221aA m \u00b7 [ 5A 2m + 3 2 R( f \u2217)+ 3A 2m + \u221a A m \u00b7 3 2 R( f \u2217) ]\n\u2264 7A 2m + 3 2 \u221a A m \u00b7R( f \u2217)+ \u221a A m \u00b7 [ 5A 2m + 3 2 R( f \u2217)+ 3A 2m + 3A 4m + 3 4 R( f \u2217) ]\n\u2264 7A 2m + 3 2 \u221a A m \u00b7R( f \u2217)+ \u221a 19 4 A m + \u221a A m \u00b7 9 4 R( f \u2217)\n\u2264 6 A m +3 \u221a A m \u00b7R( f \u2217). (21)\nIn comparison, the radius of LESS is of order O( \u221a\nA m), which can be significantly larger when R( f \u2217)\nis small. This potential radius advantage translates to a potential coverage advantage of ILESS, as stated in the following theorem.\nTheorem 5 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown probability distribution. Given that event E (see Definition 4.1) occurred, for all f \u2217, the abstain rate is bounded by\n1\u2212\u03a6(ILESS)\u2264 \u03b8 f \u2217(R0) \u00b7R0,\nwhere\nR0 , 2 \u00b7R( f \u2217)+11 \u00b7 A m +6 \u00b7 \u221a A m \u00b7R( f \u2217).\nThis immediately implies (by definition) that\n1\u2212\u03a6(ILESS)\u2264 \u03b8(R0) \u00b7R0.\nRemark 6 Note that R0 = O(R( f \u2217)+ Am) due to \u221a A m \u00b7R( f \u2217)\u2264 1 2( A m +R( f \u2217)).\nProof We start by showing that G, defined in Strategy 2, resides within a ball around any specific f \u2217. To do so, we need to bound the true error of all functions in G.\nf \u2208 G \u21d2 R\u0302( f ,Sm)\u2264 R\u0302( f\u0302 ,Sm)+\u03c3ILESS (22)\n\u21d2 R\u0302( f ,Sm)\u2264 R( f \u2217)+ A m + \u221a A m \u00b7R( f \u2217)+6 A m +3 \u221a A m \u00b7R( f \u2217) (23)\n\u21d2 R\u0302( f ,Sm)\u2264 R( f \u2217)+7 \u00b7 A m +4 \u00b7 \u221a A m \u00b7R( f \u2217), (24)\nwhere inequality (22) is by the definition of G, and inequality (23) follows from (20) and (21) (under event E). We then have,\nR( f ) \u2264 R\u0302( f ,Sm)+ \u03c3\u0302R\u2212R\u0302(m,\u03b4,d, R\u0302) (25)\n\u2264 R\u0302( f ,Sm)+ A m + \u221a A m \u00b7 R\u0302( f ,Sm) (26)\n\u2264 R( f \u2217)+8 \u00b7 A m +4 \u00b7 \u221a A m \u00b7R( f \u2217)+ \u221a\u221a\u221a\u221aA m \u00b7 [ R( f \u2217)+7 \u00b7 A m +4 \u00b7 \u221a A m \u00b7R( f \u2217) ] (27)\n\u2264 R( f \u2217)+8 \u00b7 A m +4 \u00b7 \u221a A m \u00b7R( f \u2217)+ \u221a A m \u00b7 [ 3R( f \u2217)+9 \u00b7 A m ] (28)\n\u2264 R( f \u2217)+11 \u00b7 A m +6 \u00b7 \u221a A m \u00b7R( f \u2217), (29)\nwhere inequality (25) is (13) (which holds given E), inequality (26) follows directly from the definition of \u03c3\u0302R\u2212R\u0302, inequality (27) is obtained using (24), inequality (28) follows from \u221a AB\u2264 A/2+B/2,\nand (29) from \u221a A+B\u2264 \u221a A+ \u221a B.\nUsing (29), for all f \u2208 G, and any f \u2217 we have,\nPr X\u223cPX { f (X) 6= f \u2217(X)} = Pr X ,Y\u223cPX ,Y { f (X) 6= f \u2217(X)\u2227 f \u2217(X) = Y}+ Pr X ,Y\u223cPX ,Y { f (X) 6= f \u2217(X)\u2227 f \u2217(X) 6= Y}\n\u2264 Pr X ,Y\u223cPX ,Y\n{ f (X) 6= f \u2217(X)\u2227 f \u2217(X) = Y}+R( f \u2217)\n\u2264 Pr X ,Y\u223cPX ,Y\n{ f (X) 6= Y}+R( f \u2217)\n= R( f )+R( f \u2217)\n\u2264 2 \u00b7R( f \u2217)+11 \u00b7 A m +6 \u00b7 \u221a A m \u00b7R( f \u2217). (30)\nIt follows that\nf \u2208 B( f \u2217,2 \u00b7R( f \u2217)+11 \u00b7 A m +6 \u00b7 \u221a A m \u00b7R( f \u2217)) = B( f \u2217,R0),\nand, in particular, G\u2286 B( f \u2217,R0),\nso \u2206G\u2264 \u2206B( f \u2217,R0).\nThe abstain rate of ILESS equals \u2206G. We can now use the disagreement coefficient to bound the abstain rate from above,\n\u2206G\u2264 \u2206B( f \u2217,R0) = \u2206B( f \u2217,R0)\nR0 \u00b7R0 \u2264 \u03b8(R0) \u00b7R0, (31)\nwhich concludes the proof.\nAccording to Theorem 5, assuming the disagreement coefficient is \u03b8(r) = O(polylog(1/r)) for r \u2265 R( f \u2217), the rejection mass of ILESS, defined as the probability that the classifier trained by ILESS will output \u201cI don\u2019t know\u201d is bounded w.h.p. by\npolylog1( 1 R( f \u2217)+1/m ) \u00b7R( f \u2217)+ polylog2(m,d,1/\u03b4) m . (32)\nIn many cases, the disagreement coefficient, \u03b8(r), is bounded by a constant, or by O(polylog(1/r)) for all r > 0 (see [16]). For example, it was shown in [12], that for linear separators under mixture of Gaussians, and for axis-aligned rectangles under product densities over Rk, \u03b8(r) is bounded by O(polylog(1/r)) for all r > 0. For such cases, we know that (32) always holds, regardless of the size of R( f \u2217). The disagreement coefficient is only dependent on the marginal PX , the hypothesis class F , and the identity of the true risk minimizers, f \u2217 (which is not necessarily unique). This fact motivates the following definition of a rejection rate of a selective learning algorithm, which is only dependent on PX ,F and f \u2217.\nDefinition 4.2 (Fast R\u2217 Rejection Rate) Given PX ,F and f \u2217, if for any PY |X , for which f \u2217 is a true risk minimizer, the rejection mass of a selective classifier learning algorithm is bounded by probability of at least 1\u2212\u03b4 by (32), we say that the algorithm achieves a fast R\u2217 rejection rate, with polylog1 and polylog2 as its parameters.\nClearly, by Theorem 5, if \u03b8(r) = O(polylog(1/r)) for all r > 0, then ILESS has a fast R\u2217 rejection rate. In the next section, we will show the other direction; that is, if there is a PCS learning algorithm that has a fast R\u2217 rejection rate, then \u03b8(r) = O(polylog(1/r)) for all r > 0.\nAs long as the number of training examples that ILESS receives is not \u201ctoo large\u201d relative to 1/R( f \u2217), i.e., m 1R( f \u2217) , the rejection mass of ILESS is O( polylog(m,d,1/\u03b4) m ). When m is large, and R( f \u2217) becomes more dominant than 1m , our coverage bound is dominated by R( f \u2217). This should not surprise us, as ILESS achieves pointwise-competitiveness w.h.p., and any strategy that achieves pointwise-competitiveness cannot ensure a better rejection mass than R( f \u2217) without making more assumptions about the error or the distribution. This can be seen in the following example, in which \u03b8(r)\u2264 1 for all r > 0, but the rejection mass of any pointwise-competitive strategy is always at least R( f \u2217).\nExample 1 Given any 0 < \u03b5 < 0.5, let X = [0,1], and F = { f1, f2} where\nf1(x) = { 1, x < \u03b5 0, otherwise , f2(x) = { 1, x > 1\u2212 \u03b5 0, otherwise.\nLet PX be the uniform distribution over [0,1]. Assume that Y will always be zero. f1 and f2 are both f \u2217. Every pointwise-competitive classifier will have to output g(x) = 0 for every x in the disagreement set of f1 and f2. R( f \u2217) = \u03b5, and the rejection mass is 2\u03b5(= 2R( f \u2217))."}, {"heading": "5. From Selective Classification to the Disagreement Coefficient", "text": "We now turn to show a reduction from selective classification, to the disagreement coefficient.\nTheorem 7 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. Let PCS be an algorithm that returns a pointwise-competitive selective classifier w.h.p. If there exists an mmax s.t. for every m \u2264 mmax, with probability of at least 1\u2212 \u03b4, the abstain rate 1\u2212\u03a6 of PCS(Sm,\u03b4,F ,d) is bounded above as follows:\n1\u2212\u03a6(PCS)\u2264 polylog(m,d,1/\u03b4) m . (33)\nThen for every f \u2217 (every true risk minimizer), for every r \u2265 1/mmax,\n\u03b8 f \u2217(r)\u2264 8(polylog(1/r,d,1/r)+3).\nProof For any m \u2208 {2,3, ...,mmax}, denote by Sm a random training sample drawn from PX ,Y . Let Z be a random variable representing a single random unlabeled example sampled from PX , and let f \u2217 to be a specific true risk minimizer.\nGiven z \u2208 DIS ( B( f \u2217, 1m) ) , as used in [15, Lemma 47], we know that there exists a function hz \u2208 F s.t. hz(z) 6= f \u2217(z) and Pr(hz(X) 6= f \u2217(X)) \u2264 1m . We denote by PX ,Y z a new probability distribution that is identical to PX ,Y over all x \u2208 X with the exception of {x : hz(x) 6= f \u2217(x)}, over which it is defined to be Y , hz(x). It is easy to see that hz is an f \u2217 for such a distribution.\nDenote by e1 the probability event where (33) holds (for a specific m\u2264mmax). Denote by e2 the event where PCS has succeeded in returning a pointwise-competitive selective classifier ( fsm ,gsm) under Sm.\nDefine S\u2032m to be a modified Sm. For every x s.t. hz(x) 6= f \u2217(x), y changes to be y = hz(x). S\u2032m is a random training sample drawn from PX ,Y z. Denote by e3z the event where PCS has succeeded in returning a pointwise-competitive selective classifier ( fs\u2032m ,gs\u2032m) under S \u2032 m. hz is only defined for\ncases in which z \u2208 DIS ( B( f \u2217, 1m) ) , and thus we define that e3z will also include cases for which\nz /\u2208 DIS ( B( f \u2217, 1m) ) .\nUnder our assumptions, Pr(e1),Pr(e2)\u2265 1\u2212\u03b4. For every z \u2208 DIS(B( f \u2217, 1m)), Pr(e3z|z)\u2265 1\u2212\u03b4, and for every z /\u2208 DIS(B( f \u2217, 1m)), Pr(e3z|z) = 1, which implies that Pr(e3z) \u2265 1\u2212 \u03b4. We denote by hz(Sm) = f \u2217(Sm) the event where hz(x) = f \u2217(x) for all x \u2208 Sm. The explanations for the following equations follow.\nPr{Z \u2208 DIS (\nB( f \u2217, 1 m )\n) \u2227hz(Sm) = f \u2217(Sm)} (34)\n= Pr{Z \u2208 DIS (\nB( f \u2217, 1 m )\n) \u2227hz(Sm) = f \u2217(Sm)\u2227 e1\u2227 e2\u2227 e3z} (35)\n+Pr{Z \u2208 DIS (\nB( f \u2217, 1 m )\n) \u2227hz(Sm) = f \u2217(Sm) | \u00ac(e1\u2227 e2\u2227 e3z)} \u00b7Pr(\u00ac(e1\u2227 e2\u2227 e3z))\n\u2264 Pr{Z \u2208 DIS (\nB( f \u2217, 1 m )\n) \u2227hz(Sm) = f \u2217(Sm)\u2227 e1\u2227 e2\u2227 e3z}+3\u03b4 (36)\n\u2264 Pr{gsm(Z) = 0\u2227 e1\u2227 e2\u2227 e3z}+3\u03b4 (37) \u2264 Pr{gsm(Z) = 0\u2227 e1}+3\u03b4 (38) \u2264 Pr{gsm(Z) = 0 | e1}+3\u03b4 (39) \u2264 polylog(m,d,1/\u03b4) m +3\u03b4. (40)\nIn (34), it is convenient to view the random experiment as if we draw Z first, and then Sm. If Z \u2208 DIS(B( f \u2217, 1m)), then consider hz to be any function that holds hz(Z) 6= f\n\u2217(Z) and Pr(hz(X) 6= f \u2217(X)) \u2264 1m . If Z /\u2208 DIS(B( f\n\u2217, 1m)), then the event described in (34) does not occur, and hz is undefined. In (35), we use conditional probability, and in (36) we apply the union bound. Inequality (37) is justified as follows. If hz(Sm) = f \u2217(Sm), then the algorithm received the same input under PX ,Y z and PX ,Y . Given that e2 and e3z occurred, we know that the algorithm had successfully output a pointwise-competitive selective classifier for both probabilities, which means that whenever f \u2217 and hz disagree, gsm has to output zero; otherwise, it will not be pointwise-competitive for one of the distributions. By the definition of hz, hz(Z) 6= f \u2217(Z), which explains the inequality. (40) is driven from the definition of e1. Taking \u03b4 = 1m , we get,\nPr{Z \u2208 DIS (\nB( f \u2217, 1 m )\n) \u2227hz(Sm) = f \u2217(Sm)} \u2264\npolylog(m,d,m)+3 m . (41)\nThe following inequalities are derived using elementary conditional probability. In Equation (42) we use an argument taken from the proof of [15, Lemma 47]. hz \u2208 ( B( f \u2217, 1m) ) and thus the probability\nthat f \u2217 and hz will have the same labels over a sample of size m is at least (1\u2212 1m) m.\nPr{Z \u2208 DIS (\nB( f \u2217, 1 m )\n) \u2227hz(Sm) = f \u2217(Sm)}\n= Pr{hz(Sm) = f \u2217(Sm) | Z \u2208 DIS (\nB( f \u2217, 1 m )\n) } \u00b7Pr{Z \u2208 DIS ( B( f \u2217,\n1 m )\n) }\n\u2265 (1\u2212 1 m )m \u00b7Pr{Z \u2208 DIS\n( B( f \u2217,\n1 m )\n) } (42)\n\u2265 1 4 \u00b7\u2206B( f \u2217, 1 m ). (43)\nCombining (41) and (43), we get that for every m \u2208 {2,3, ...,mmax},\n\u2206B( f \u2217,1/m) 1/m \u2264 4(polylog(m,d,m)+3). (44)\nThe following inequalities follow from (44), and from the fact that \u2206B( f \u2217,x) and polylog1(x) are non-decreasing. For any r in [ 1mmax , 1 2 ],\n\u2206B( f \u2217,r) r \u2264 \u2206B( f \u2217, 1b1/rc)\n1 b1/rc\n\u00b7 1 r \u00b7 b1/rc\n\u2264 4(polylog(b1/rc,d,b1/rc)+3) \u00b7 1 r \u00b7 (1/r\u22121) \u2264 4(polylog(b1/rc,d,b1/rc)+3) \u00b7 1 1\u2212 r \u2264 8(polylog(1/r,d,1/r)+3)\nand for r in [12 ,1],\n\u2206B( f \u2217,r) r \u2264 1 1/2 = 2, (45)\nwhich concludes the proof.\nCorollary 8 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. If there exists an mmax s.t. for every m \u2264 mmax, with probability of at least 1\u2212 \u03b4, the abstain rate 1\u2212\u03a6 of ILESS(Sm,\u03b4,F ,d) is bounded above as follows:\n1\u2212\u03a6(ILESS)\u2264 polylog(m,d,1/\u03b4) m . (46)\nThen for every f \u2217 (every true risk minimizer), for every r \u2265 1/mmax,\n\u03b8 f \u2217(r)\u2264 8(polylog(1/r,d,1/r)+3).\nProof This is a direct result from Theorem 7, and from the fact that ILESS is PCS.\nGiven PX ,F and f \u2217, if any PCS has a fast R\u2217 rejection rate, we can apply Theorem 7 with a deterministic PY |X distribution for which Y = f \u2217(X), and get that R( f \u2217) = 0. Thus, by definition,\n1\u2212\u03a6(ILESS)\u2264 polylog1( 1 R( f \u2217)+1/m ) \u00b70+ polylog2(m,d,1/\u03b4) m . (47)\nWe can now apply Theorem 7 with mmax = \u221e, and get that the disagreement coefficient is bounded by polylog(1/r) for all r > 0. Thus, completing a two sided equivalence from PCS with a fast R\u2217 rejection rate to a bounded disagreement coefficient for all r > 0."}, {"heading": "6. Active-ILESS", "text": "Strategy 3 Agnostic low-error active learning strategy (Active-ILESS) Input: \u03b5 and/or m depending on the desired termination condition (error or labeling budget, respectively)\nConfidence level \u03b4 Hypothesis class F with VC dimension d An unlabeled input sequence sampled i.i.d from PX ,Y : x1,x2,x3, . . .\nOutput: A classifier f\u0302 . Initialize: Set S\u0302 = /0, G0 = F , t = 1. Perform for each example xt received:\n1: if xt \u2208 AGR(Gt\u22121): don\u2019t request label for xt and set yt = f (xt) using any f \u2208 Gt\u22121 otherwise: request label yt . 2: Set S\u0302 = S\u0302\u222a{(xt ,yt)}. 3: Set f\u0302 = f\u0302 (S\u0302) 4: if log2(t) \u2208 N:\n\u2022 Set \u03c3Active = \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302) ) + \u03c3\u0304R\u0302\u2212R ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)+ \u03c3\u0302R\u2212R\u0302( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)) ) \u2022 If \u03b5 was given as input and \u03c3Active < \u03b5, terminate and return f\u0302\n\u2022 Set Gt = V\u0302 ( f\u0302 ,\u03c3Active )\n\u2022 Set S\u0302 = /0. otherwise:\n\u2022 Gt = Gt\u22121 5: If m was given as input and t = m, terminate and return f\u0302 6: Set t = t +1\nIn this section we introduce, in Strategy 3, an agnostic active learning algorithm called Active-ILESS. Active-ILESS is very similar to Agnostic CAL [10], Algorithm 4.2 on page 36, and A2 [14]. Much like Agnostic CAL, Active-ILESS creates artificial labels (step 1). The two algorithms differ mainly in that Active-ILESS works in batches (inside each batch, the decision whether to query an example is made instantly and not at the end of the batch). This allows Active-ILESS to be a bit more conservative with its deltas. Moreover, while Agnostic CAL requires calculation of an ERM with many constraints (defined by the function LEARN in HSU\u2019s thesis), Active-ILESS requires a calculation of the ERM with only one constraint, as seen from the disbelief principle [24], already discussed in Section 4.\nAlthough ILESS is not novel in and of itself, we use its similarity to Agnostic CAL to demonstrate a deep connection between active learning and selective classification.\nIn Section 7 we use Active-ILESS to show an equivalence between active learning (represented by Active-ILESS) and selective classification (represented by a variant of ILESS, \u201cBatch-ILESS\u201d). The introduction of these new variants facilitates a straightforward proof of the equivalence relationship. This equivalence implies a novel relationship between selective and active classification in the agnostic setting.\nWe begin by analyzing Active-ILESS and showing that much like ILESS, f \u2217 \u2208Gt in each iteration t. The low-error set G, maintained by ILESS, contains all the hypotheses that have an empirical error smaller than R\u0302( f\u0302 )+\u03c3ILESS. In Lemma 1 we showed that this condition implies that f \u2217 resides within the low-error set G of ILESS. A proof that f \u2217 \u2208 Gt , after each iteration of Active-ILESS, cannot follow the same argument due to the fact that Active-ILESS, shown in Strategy 3, labels by itself each example whose label is not requested from the teacher, and obviously, since we consider an agnostic setting, these self-labels can differ from the true labels.\nActive-ILESS, as seen in Strategy 3, receives as a termination condition either \u03b5 > 0 and/or m, and terminates when the radius of its low-error set, Gt , is smaller than \u03b5, or when it has processed m examples.\nActive-ILESS changes its low-error set, Gt , only for t that are natural powers of 2. For each change, Active-ILESS begins to create fake labels for xt \u2208 AGR(Gt\u22121) that may or may not be equal to the real label of xt (under the original distribution). In fact, this Gt defines a new distribution, PX ,Y (Gt), and this distribution changes for every t that is a natural power of 2. With respect to a run of Active-ILESS, and t = 2i, i\u2208N, we denote by PX ,Y (Gt), the new probability distribution implied by Gt , and the fake labels created by the algorithm. RPX ,Y (Gt)( f ) will be the true risk under the new distribution, while RPX ,Y ( f ) is the true risk of f under the original distribution.\nDefinition 6.1 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. Given a run of Active-ILESS, we denote by K the event where both inequalities (48) and (49) hold simultaneously for every f \u2208 F , for all iterations of Active-ILESS where t = 2i, i \u2208 N. R\u0302( f ), R\u0302( f , S\u0302) for S\u0302 before it was initialized:\nRPX ,Y (Gt)( f )\u2264 R\u0302( f )+\u03c3R\u2212R\u0302 ( t 2 , \u03b4 2t ,d,R( f ), R\u0302( f ) ) (48)\nR\u0302( f )\u2264 RPX ,Y (Gt)( f )+\u03c3R\u0302\u2212R ( t 2 , \u03b4 2t ,d,R( f ), R\u0302( f ) ) (49)\nLemma 9 K occurs with probability of at least 1\u2212\u03b4.\nProof Gt changes only for iterations of the type 2i, i\u2208N. We know by Lemma 1 that the probability that inequalities (48) and (49) do not hold is smaller than \u03b4/(2t). By the union bound, the probability that one of these inequalities does not hold after any iteration is smaller than\n\u2211 t=2i,i\u2208N\n\u03b4 2t \u2264 \u03b4.\nLemma 10 If f \u2217, a true risk minimizer under probability distribution PX ,Y , resides within Gt , then it is also a true risk minimizer under probability distribution PX ,Y (Gt).\nProof\nargmin f\u2208F RPX ,Y (Gt)( f ) = argmin f\u2208F RPX ,Y ( f )\ufe38 \ufe37\ufe37 \ufe38 A +RPX ,Y (Gt)( f )\u2212RPX ,Y ( f )\ufe38 \ufe37\ufe37 \ufe38 B  . We know that f \u2217 minimizes A, and we note that every function that resides within Gt minimizes B, because every difference in the labeling between PX ,Y and PX ,Y (Gt) was done according to the label given by the unanimous decision of functions in Gt . Hence, f \u2217 minimizes A+B.\nThe proofs of the following four lemmas appear in the appendix. They all show basic good qualities of Active-ILESS.\nLemma 11 Given that event K (see Definition 6.1) occurred, each f \u2217 of the original distribution PX ,Y resides within Gt for all t. This implies that RPX ,Y (Gt)( f\n\u2217)\u2264 R( f \u2217), for all t, as every change in the labeling is done according to f \u2217.\nLemma 12 Given that event K (see Definition 6.1) occurred, and under the assumption that Active-ILESS terminated with the \u03b5 condition, the hypothesis returned by Active-ILESS, f\u0302 , holds:\nRPX ,Y ( f\u0302 )\u2264 RPX ,Y ( f \u2217)+ \u03b5.\nLemma 13 Given that event K (see Definition 6.1) occurred, the final radius of Active-ILESS satisfies\n\u03c3Active = O( B m + \u221a B m \u00b7R( f \u2217)), (50)\nwhere B , 16d ln(16m 2e\nd\u03b4 ).\nLemma 14 Given that event K (see Definition 6.1) occurred, the total number of examples that Active-ILESS(\u03b5) processed (without necessarily requesting labels) is\nO (\n1 \u03b5 ln( 1 \u03b5 )+ R( f \u2217) \u03b52 ln( R( f \u2217) \u03b52 )\n) ,\nwhere we hide factors of d, ln(1/\u03b4) under the O.\nDefinition 6.2 An active learner that generates a hypothesis whose true error is smaller than \u03b5 w.h.p., has passive example complexity, if it observes up to O ( 1 \u03b5 ln( 1 \u03b5 )+ R( f \u2217) \u03b52 ln( R( f \u2217) \u03b52 ) ) examples (not necessarily labeled).\nBy Lemmas 12 and 14 we know that Active-ILESS has passive example complexity. The definition of a fast R\u2217 rejection rate for selective classification induces the following related\ndefinition for exponential speedup of active learning algorithms.\nDefinition 6.3 (R\u2217 Exponential Speedup) Given PX ,F and f \u2217, we say that an active learner has R\u2217 exponential speedup, with polylog1 and polylog2 as its parameters, if for every PY |X for which f \u2217 is a true risk minimizer, and for every m > 0, with probability of at least 1\u2212 \u03b4, the number of labels requested by the active learner after observing m examples is not greater than\npolylog1( 1\nR( f \u2217)+1/m ) \u00b7R( f \u2217)m+polylog2(m,d,1/\u03b4).\nIn [10], Hsu introduced the agnostic CAL algorithm and showed (Theorem 4.3, page 41) that if the disagreement coefficient is bounded, then Agnostic CAL has R\u2217 exponential speedup (under our new definition). Any active algorithm that has passive example complexity and achieves R\u2217\nexponential speedup requires w.h.p. no more than O ( polylog(R( f \u2217)\n\u03b52 ) R( f \u2217)2 \u03b52 +polylog( 1 \u03b5 ) ) labels to\nreach a true error smaller than \u03b5. The proof is immediate by considering the cases R( f \u2217)\n\u03b5 \u2265 1 and R( f \u2217)\n\u03b5 < 1. The leading term of this bound is R( f \u2217)2 \u03b52 , which is also the case for A 2 [14]."}, {"heading": "7. A Reduction from Active-iLess to Batch-ILESS", "text": "In Strategy 4 we define a selective classifier, called Batch-ILESS, which uses Active-ILESS as its engine. Given a labeled sample Sm, Batch-ILESS simulates the active algorithm, by applying it over a uniformly random ordering of Sm in a straightforward manner (i.e., it sequentially introduces to the active algorithm an unlabeled example and reveals the label only if the active algorithm requests it). Upon termination, after the active algorithm has consumed all examples, our batch algorithm receives f\u0302 from the active algorithm and utilizes its last low-error set Gt to define its selection function.\nLemma 11 implies that Batch-ILESS is pointwise-competitive. We note that Lemma 4, Theorem 5 and Theorem 8, which were proven for ILESS, can also be proven for Batch-ILESS. We chose to prove it for ILESS, as it is more simple than Batch-ILESS, and doesn\u2019t require an active algorithm as its engine. We state these ideas formally, and give sketches for their proofs, in the Appendix in Lemma 21 and Theorem 22.\nStrategy 4 Batch Improved Low-Error Selective Strategy (Batch-ILESS) Input: Sample set of size m, Sm,\nConfidence level \u03b4 Hypothesis class F with VC dimension d\nOutput: A selective classifier (h,g) 1: Simulate Active-ILESS with a random ordering of Sm as its input stream; let Gt be the low-error set obtained by\nActive-ILESS in its last round, and let f\u0302 be its resulting classifier. 2: Construct g such that g(x) = 1\u21d0\u21d2 x \u2208 {X \\DIS (Gt)} 3: h = f\u0302\nThe following theorem shows a deep connection between the speedup of Active-ILESS to the rejection mass of Batch-ILESS for specific PX ,Y . An immediate corollary of this theorem is that if Active-ILESS has R\u2217 exponential speedup (see Definition 6.3), then Batch-ILESS has a fast R\u2217 rejection rate (see Definition 4.2).\nTheorem 15 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. If after observing m examples, with probability of at least 1\u2212\u03b4, the number of labels\nrequested by Active-ILESS is not greater than\npolylog1( 1\nR( f \u2217)+1/m ) \u00b7R( f \u2217)m+polylog2(m,d,1/\u03b4), (51)\nthen the rejection mass of Batch-ILESS is bounded w.h.p. by\n8 \u00b7polylog1( 1\nR( f \u2217)+1/m ) \u00b7R( f \u2217)+\n2 (\u221a ln(2/\u03b4)+ \u221a ln(2/\u03b4)+2polylog2(2m,2/\u03b4) )2\nm .\nProof Consider an application of Active-ILESS with \u03b4= \u03b40 over m0 , 2dlog(m+1)e examples. Denote by Xi an indicator random variable for the labeling of its ith example, 1\u2264 i\u2264 m0. With probability of at least 1\u2212\u03b40 over the choice of samples from PX ,Y ,\nm0 \u2211 i=1 Xi \u2264 polylog1( 1 R( f \u2217)+1/m0 ) \u00b7R( f \u2217)m0 +polylog2(m0,1/\u03b40). (52)\nWe know by the definition of Active-ILESS (Strategy 3), that the last m0/2 examples had the exact same probability, \u2206Gm0/2, of requiring a label, and that this is exactly the probability that Batch-ILESS will decide to abstain after receiving m examples, according to Strategy 4.\nWe now estimate \u2206Gm0/2 using the following version of the Chernoff bound given by Canny [25]. For the sake of self-containment, Canny\u2019s statement and proof of the bound are provided in Lemma 19 in the Appendix.\nThe statement of the lemma is as follows. Let X1,X2, . . . ,Xn be independent Bernoulli trials with Pr[Xi = 1] = p, let X , \u2211ni=1 Xi, and \u00b5 = EX . Then, for every \u03b1 > 0:\nPr(X < (1\u2212\u03b1)\u00b5)\u2264 exp(\u2212\u00b5\u03b12/2).\nApplying the Chernoff bound with the indicator variables of the last m0/2 examples, we have X = \u2211m0m0/2 Xi, \u00b5 = p m0 2 , and set p , \u2206Gm0/2. Select \u03b1 such that\nexp(\u2212pm0 2 \u03b12/2) = \u03b42.\nSolving for \u03b1,\n\u03b1 =\n\u221a 4ln(1/\u03b41)\nm0 p .\nWe conclude that with probability of at least 1\u2212\u03b41,\nX \u2265 (1\u2212\n\u221a 4ln(1/\u03b41)\nm0 p ) \u00b7 pm0 2\n\u21d4 0\u2265 pm0 2 \u2212 \u221a pm0 \u00b7 ln(1/\u03b41)\u2212X . (53)\nSolving the quadratic equation (53) for \u221a pm0, we get that\n\u221a pm0 \u2264\n\u221a ln(1/\u03b41)+ \u221a ln(1/\u03b41)+2X\n1 \u21d2 p\u2264 ( \u221a ln(1/\u03b41)+ \u221a ln(1/\u03b41)+2X)2\nm0 . (54)\nCombining (52) and (54), from the union bound we get that with probability of at least 1\u2212\u03b40\u2212\u03b41,\n\u2206Gm0/2 \u2264\n(\u221a ln(1/\u03b41)+ \u221a ln(1/\u03b41)+2polylog1( 1R( f \u2217)+1/m0 ) \u00b7R( f \u2217)m0 +2polylog2(m0,1/\u03b40) )2\nm0 .\nIf we take \u03b40 = \u03b41 = \u03b4/2, then, since m\u2264 m0 \u2264 2m, we can use \u221a a+b\u2264 \u221a a+ \u221a\nb and (a+b)2 \u2264 2a2 +2b2, to obtain\n\u2206Gm0/2 \u2264\n(\u221a ln(2/\u03b4)+ \u221a ln(2/\u03b4)+4polylog1( 1R( f \u2217)+1/m) \u00b7R( f \u2217)m+2polylog2(2m,2/\u03b4) )2 m\n\u2264\n(\u221a ln(2/\u03b4)+ \u221a ln(2/\u03b4)+2polylog2(2m,2/\u03b4)+ \u221a 4polylog1( 1 R( f \u2217)+1/m) \u00b7R( f \u2217)m )2 m\n\u2264 2 (\u221a ln(2/\u03b4)+ \u221a ln(2/\u03b4)+2polylog2(2m,2/\u03b4) )2 +2 (\u221a 4polylog1( 1 R( f \u2217)+1/m) \u00b7R( f \u2217)m )2\nm = 2 (\u221a ln(2/\u03b4)+ \u221a ln(2/\u03b4)+2polylog2(2m,2/\u03b4) )2\nm +8 \u00b7polylog1( 1 R( f \u2217)+1/m ) \u00b7R( f \u2217)\nCorollary 16 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. If after observing m examples, with probability of at least 1\u2212\u03b4, the number of labels requested by Active-ILESS is not greater than\npolylog1( 1\nR( f \u2217)+1/m ) \u00b7R( f \u2217)m+polylog2(m,d,1/\u03b4),\nthen for every r \u2265 R( f \u2217), \u03b8 f \u2217(r)\u2264 8 ( 2 (\u221a ln(2r)+ \u221a ln(2r)+2polylog2(2/r,2/r) )2 +8 \u00b7polylog1(1/r)+2 ) =O(polylog(1/r)).\nProof The proof follows from Theorems 15 and 7. Applying Theorem 15, we know that for m\u2264 1/R( f \u2217), the rejection mass of Batch-ILESS is bounded w.h.p. by,\n2 (\u221a ln(2/\u03b4)+ \u221a ln(2/\u03b4)+2polylog2(2m,2/\u03b4) )2 +8 \u00b7polylog1( 1R( f \u2217)+1/m)\nm .\nApplying Theorem 7 with mmax = 1/R( f \u2217), we get that for every r \u2265 R( f \u2217), \u03b8 f \u2217(r) \u2264 8 ( 2 (\u221a ln(2r)+ \u221a ln(2r)+2polylog2(2/r,2/r) )2 +8 \u00b7polylog1( 1\nR( f \u2217)+ r )+3 ) \u2264 8 ( 2 (\u221a ln(2r)+ \u221a ln(2r)+2polylog2(2/r,2/r) )2 +8 \u00b7polylog1(1/r)+3 ) .\nNote that the Theorem 7 does not require mmax to be an integer."}, {"heading": "8. From the Disagreement Coefficient to Active Learning", "text": "In this section we show that when \u03b8\u2032(r) is bounded by polylog1(1/r) for all r > R( f \u2217) for some specific PX ,Y , then the label complexity of Active-ILESS under the same PX ,Y is bounded by\npolylog2( 1\nR( f \u2217)+1/m ) \u00b7R( f \u2217)m+polylog3(m,d,1/\u03b4), (55)\nwhere the parameters of polylog2 and polylog3 are only dependent on polylog1(1/r). Thus, if \u03b8\u2032(r)\u2264 polylog1(1/r) for all r > 0, we get that Active-ILESS has R\u2217 exponential speedup. This direction has been shown before in [9, 10] for agnostic CAL and A2. For the sake of self-containment, we show it here for Active-ILESS. Due to the fact that Active-ILESS relies on ILESS, which we already have bounds for, the proof is straightforward.\nAs a preparation for the theorem, we present Lemma 17 (shown before in [16]), in which we introduce a small feature of the disagreement coefficient that will serve us later.\nLemma 17 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. For every f \u2208 F and 0 < r \u2264 1, \u03b8 f (r) \u00b7 r is a non-decreasing function.\nProof Given 0 < r1 < r2, we will show that \u03b8 f (r1) \u00b7 r1 \u2264 \u03b8 f (r2) \u00b7 r2. Assume by contradiction that\n\u03b8 f (r1) \u00b7 r1 > \u03b8 f (r2) \u00b7 r2,\ni.e.,\nsup r>r1 \u2206B( f ,r1) r1 \u00b7 r1 > sup r>r2 \u2206B( f ,r2) r2 \u00b7 r2.\nThis implies, that there exists r1 \u2264 r\u0302 < r2 s.t.\n\u2206B( f , r\u0302) r\u0302 r1 > sup r>r2 \u2206B( f ,r2) r2 r2 \u2265 \u2206B( f ,r2) r2 r2 = \u2206B( f ,r2).\nThis contradicts the known monotonicity of \u2206B( f ,x).\nTheorem 18 Let F be a hypothesis class with a finite VC dimension d, let PX ,Y be an unknown distribution, and f \u2217 is a true risk minimizer of PX ,Y . If for all r > R( f \u2217),\n\u03b8\u2032(r)\u2264 polylog1(1/r),\nthen the label complexity of Active-ILESS(m,\u03b4/2) is bounded by\npolylog1\n( 1\n5R( f \u2217)+14 Am\n) 2e \u00b7mR( f \u2217)+ log2(2/\u03b4)+56e \u00b7 log2 m \u00b7A \u00b7polylog1 ( 1\n5R( f \u2217)+14 Am\n) ,\nwhich has the same form of Equation (55).\nProof Each run of Active-ILESS(m,\u03b4/2) simulates log2 m runs of ILESS. We know by Lemma 9 that with probability of at least 1\u2212 \u03b4/2, inequalities (48) and (49) hold for each run. Recall that we denoted by K the event where both inequalities hold through out all runs of ILESS, which is exactly the definition of event E per run (see Definition 4.1). Under event K , Lemma 11 implies that all f \u2217 of the original distribution PX ,Y reside within Gt for all t. This also implies that all f \u2217 of the original distribution remain the true risk minimizers under PX ,Y (Gt), for all t, as they always benefit from the creation of the artificial labels.\nBecause the marginal of the distribution does not change during the run of Active-ILESS, and because event E holds for each iteration of ILESS, we can apply Theorem 5 for all of the runs of ILESS. We thus get that for every run of ILESS, the rejection mass is bounded by\n1\u2212\u03a6(ILESS)\u2264 \u03b8(R0) \u00b7R0,\nwhere\nR0 , 2 \u00b7R( f \u2217)+11 \u00b7 A m +6 \u00b7 \u221a A m \u00b7R( f \u2217).\nWe denoted by R( f \u2217) the true error according to the original distribution, which might be larger than the true error implied by the fake label distributions that the algorithm induces. However, according to Lemma 17, enlarging R0 can only weaken the bound, and thus, there is no problem doing so. We additionally bound R0 using \u221a AB\u2264 A/2+B/2 to get\nR0 \u2264 5 \u00b7R( f \u2217)+14 \u00b7 A m .\nGiven our bound on the disagreement coefficient, we conclude that\n1\u2212\u03a6(ILESS)\u2264 polylog1( 1 5 \u00b7R( f \u2217)+14 \u00b7 Am ) \u00b7 (5 \u00b7R( f \u2217)+14 \u00b7 A m ).\nEach activation of ILESS has delta equals \u03b44t , and thus, exactly as in Lemma 9, with probability of at least 1\u2212\u03b4/2, they all have a bounded rejection mass simultaneously. We assume that this event occurred. According to the definition of Gt in Strategy 3, the probability distribution of the artificial labeling done by Active-ILESS changes only when t is a natural power of 2. Thus, the probability of requesting label t > 2, denoted by Pt , is bounded by\nPt \u2264 polylog1\n( 1\n5 \u00b7R( f \u2217)+14 \u00b7 AT\n) \u00b75R( f \u2217)+ 14A \u00b7polylog1 ( 1 5\u00b7R( f \u2217)+14\u00b7 AT ) T , (56)\nwhere T = 2blog2(t\u22121)c\u22121. We now have a series of Poisson trials, X1,X2, . . . ,Xm, with Pr(Xt = 1) = Pt , and each Xi is an indicator variable for the labeling of the ith example. We use a version of the Chernoff bound [25] to bound the label complexity.5 The statement and a sketch of the proof of this bound are provided in Lemma 20 in the Appendix.\nFor independent Poisson variables X1,X2, . . . ,Xm, where Pr[Xi = 1] = pi, X , \u2211ni=1 Xi, and \u00b5 = EX , for every \u03b1 > 2e\u22121:\nPr(X > (1+\u03b1)\u00b5)\u2264 2\u2212\u00b5\u03b1.\n5. We found this useful bound in [26] (Theorem 5.4).\nTo bound \u00b5 = EX from above, we use inequality (56) and plug it into the definition of \u00b5.\n\u00b5 = P1 +P2 + m\n\u2211 i=3 Pt\n\u2264 2+ log2 m\u22121\n\u2211 k=1 2kP2k+1\n\u2264 2+m \u00b7polylog1\n( 1\n5R( f \u2217)+14 Am\n) \u00b7R( f \u2217)+ log2 m\u22121\n\u2211 k=1\n2k 14A \u00b7polylog1\n( 1\n5R( f \u2217)+14 Am ) 2k\u22121\n\u2264 2+m \u00b7polylog1\n( 1\n5R( f \u2217)+14 Am\n) \u00b7R( f \u2217)+28log2 m \u00b7A \u00b7polylog1 ( 1\n5R( f \u2217)+14 Am\n) .\n(57)\nWe need to choose an \u03b1 that satisfies both 2\u2212\u00b5\u03b1\u2264 \u03b4/2, and \u03b1> 2e\u22121. Clearly, \u03b1= log2(2/\u03b4)\u00b5 +2e\u22121 suffices. Hence, we get that with probability of at least 1\u2212\u03b4/2,\nX \u2264 (1+ log2(2/\u03b4) \u00b5 +2e\u22121)\u00b5\n= log2(2/\u03b4)+2e\u00b5.\nInequality (57) holds with probability of at least 1\u2212 \u03b4/2, and using the union bound, we get that with probability of at least 1\u2212\u03b4,\nX \u2264 log2(2/\u03b4)+2e ( 2+m \u00b7polylog1 ( 1\n5R( f \u2217)+14 Am\n) \u00b7R( f \u2217)+28log2 m \u00b7A \u00b7polylog1 ( 1\n5R( f \u2217)+14 Am\n))\n= polylog1\n( 1\n5R( f \u2217)+14 Am\n) 2e \u00b7mR( f \u2217)+ log2(2/\u03b4)+56e \u00b7 log2 m \u00b7A \u00b7polylog1 ( 1\n5R( f \u2217)+14 Am ) (58)\nThe dominant factor of Equation (58), if we ignore the logarithmic factors, is mR( f \u2217). Active-ILESS has passive example complexity (see Definition 6.2), which means that the total sample complexity is bounded by O\u0303(1\u03b5 + R( f \u2217) \u03b52 ), where O\u0303(\u00b7) hides logarithmic factors. Plugging the sample complexity into m in (58), we get that the total label complexity is bounded by O\u0303(R( f \u2217)2\n\u03b52 ), in cases for which ILESS has a fast R\u2217 rejection rate. In [27, Theorem 3], K\u00e4\u00e4ri\u00e4inen showed that for every active learning algorithm, under a specific (non-trivial) hypothesis class F , there exists a deterministic target function g, and a marginal distribution PX , s.t. the label complexity is \u2126\u0303(R( f \u2217)2\n\u03b52 ) (where \u2126\u0303(\u00b7) hides logarithmic factors)."}, {"heading": "9. Concluding Remarks", "text": "In this paper we focused on disagreement-based methods. Namely, we always required that f \u2217 remain inside a low-error subset of hypotheses w.h.p., and made decisions based on disagreement considerations. We introduced a new selective classification algorithm, called ILESS, whose rejection\n\u201cengine\u201d utilizes sharp generalization bounds (which depend on R( f \u2217)). Our analysis proves that ILESS has sometimes significantly better rejection guarantees relative to the best known pointwisecompetitive selective strategy of [4]. Moreover, the guarantees we provide for ILESS do not depend at all on the Bernstein assumption. For the general agnostic setting, we showed an equivalence relation between pointwise-competitive selective classification, active learning, and the disagreement coefficient (see Figure 1). This equivalence is formulated in terms of a fast R\u2217 rejection rate and R\u2217 exponential speedup (Definitions 4.2 and 6.3). Theorems 7 and 5 show that selective classification with a fast R\u2217 rejection rate is completely equivalent to having a disagreement coefficient bounded by polylog(1/r) for r > 0. In Section 6, in Strategy 3, we define Active-ILESS using ILESS implicitly as its engine (see State 4 in Strategy 3). We can replace ILESS with another pointwise-competitive selective algorithm, and thus construct a new active learner, that queries a label whenever the selective classifier abstains, and create a fake label according to the decision of the classifier whenever it decides to predict. Because the selective predictor is pointwise-competitive, we know that the underlying distribution induced by its fake labels is equivalent to a distribution defined by a deterministic labeling according to f \u2217 and the same PX . The algorithm will terminate using the exact same termination condition as ActiveILESS (when \u03c3Active < \u03b5), and thus the total sample complexity (labeled and unlabeled examples) will remain the same. The change will only be in the labeling criterion. Lemmas 10, 11, 12, 13, and 14 can all be generalized to such an algorithm.\nGoing in the other direction to create a selective classifier from a general active learner is more challenging. However, if the active learner follows the Active-ILESS paradigm, and in particular, uses a pointwise-competitive selective classifier to decide on label requests, then a new pointwisecompetitive selective classifier can be created in the same way that Batch-ILESS was created, and then we can obtain a restatement of Theorem 15 providing a reduction from an R\u2217 exponential speedup of the active algorithm, to a fast R\u2217 rejection rate of the selective classifier.\nDisagreement-based decision making in active and selective learning leads to \u201cdefensive\u201d algorithms. For example, in the active learning case, this means that a defensive algorithm will ask for more labels than a more aggressive algorithm. In selective classification, this defensiveness provides the power to be pointwise-competitive, but will entail an increased rejection rate. It would be interesting to consider more aggressive algorithms that could, for example, take into consideration an estimation of PX in order to ignore examples that cause disagreement only between functions that are very similar to each other (in terms of the probability mass of their difference). Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios. We believe that there is still work to be done for the agnostic scenario.\nMany aggressive algorithms could be devised under assumptions about knowledge of PX (that could be acquired during the run of the algorithm, and is given in the transductive case), or in a Bayesian setting where a prior distribution on F exists. When researching this direction, one might also want to define a cost over unlabeled examples, and discuss the trade-off between labeled and unlabeled examples. The main open question inspired by our results would be to identify similar correspondence between aggressive selective classification algorithms and aggressive active learners.\nAnother aspect of selective classification and active learning, which was not addressed in this paper, is differentiating between more and less noisy areas of the distribution. A noisy area could be defined as an area for which even the best classifier in the class could not achieve a low-error. This motivates a new type of labeling for selective prediction, where one can abstain for two reasons: (i)\nlack of knowledge in a specific region of X , i.e., not enough examples were observed in that region, and the generalization bounds are not sufficiently tight. (ii) The region was well explored, but even the best classifier performs poorly, and thus the answer is unknown (the region is noisy). In our paper, an active learner will query for both scenarios; however, a more clever active learner might only query examples of the first type, as examples of the second type cannot reduce its error."}, {"heading": "Appendix A.", "text": "Proof of Lemma 11 We prove the claim by induction over t for which Gt is different from Gt\u22121. The base case of the induction is clear. We now show that functions that are true risk minimizers of PX ,Y (Gt\u22121) reside within Gt . According to Lemma 10, f \u2217 is a true risk minimizer under PX ,Y (Gt\u22121) (given the induction hypothesis), and hence will also be within Gt . We refer by f \u2217 to a true risk minimizer according to PX ,Y (Gt\u22121). Using inequality (49) and the definition of \u03c3\u0304R\u0302\u2212R,\nR\u0302( f \u2217, S\u0302) \u2264 RPX ,Y (Gt\u22121)( f \u2217)+\u03c3R\u0302\u2212R ( t 2 , \u03b4 2t ,d,RPX ,Y (Gt\u22121)( f \u2217), R\u0302( f \u2217, S\u0302) )\n\u2264 RPX ,Y (Gt\u22121)( f \u2217)+ \u03c3\u0304R\u0302\u2212R ( t 2 , \u03b4 2t ,d,RPX ,Y (Gt\u22121)( f \u2217) ) ,\n(59)\nand by inequality (48) and the definition of f\u0302 we get,\nRPX ,Y (Gt\u22121)( f \u2217) \u2264 RPX ,Y (Gt\u22121)( f\u0302 ) \u2264 R\u0302( f\u0302 , S\u0302)+\u03c3R\u2212R\u0302 (\nt 2 , \u03b4 2t ,d,RPX ,Y (Gt\u22121)( f\u0302 ), R\u0302( f\u0302 , S\u0302) ) \u2264 R\u0302( f\u0302 , S\u0302)+ \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302) ) .\n(60)\nPlugging (60) into (59) we get,\nR\u0302( f \u2217, S\u0302) \u2264 R\u0302( f\u0302 , S\u0302)+ \u03c3\u0302R\u2212R\u0302 (\nt 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302) ) + \u03c3\u0304R\u0302\u2212R ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)+ \u03c3\u0302R\u2212R\u0302( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)) ) \u21d2 f \u2217 \u2208 Gt . (61)\nProof of Lemma 12 Let Gt\u22121 be the final low-error set of Active-ILESS, and let S\u0302 be the final set of examples. The following inequalities are derived from Lemma 9 and inequalities (59) and (60).\nRPX ,Y (Gt\u22121)( f\u0302 ) \u2264 R\u0302( f\u0302 , S\u0302)+ \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302) ) \u2264 R\u0302( f \u2217, S\u0302)+ \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)\n) \u2264 RPX ,Y (Gt\u22121)( f \u2217)+ \u03c3\u0304R\u0302\u2212R ( t 2 , \u03b4 2t ,d,RPX ,Y (Gt\u22121)( f \u2217) ) + \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)\n) \u2264 RPX ,Y (Gt\u22121)( f \u2217)+ \u03c3\u0304R\u0302\u2212R ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)+ \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302) )) + \u03c3\u0302R\u2212R\u0302 ( t 2 , \u03b4 2t ,d, R\u0302( f\u0302 , S\u0302)\n) \u2264 RPX ,Y (Gt\u22121)( f \u2217)+ \u03b5.\nBy Lemma 11 we know that f \u2217 resides within Gt\u22121, which implies that any change in PX ,Y (Gt\u22121) in comparison to PX ,Y reduces the true error of f \u2217. This also means that for every f \u2208 F ,\nRPX ,Y ( f )\u2212RPX ,Y (Gt\u22121)( f )\u2264 RPX ,Y ( f \u2217)\u2212RPX ,Y (Gt\u22121)( f \u2217),\nwhich results in RPX ,Y ( f\u0302 )\u2264 RPX ,Y ( f \u2217)+ \u03b5.\nProof of Lemma 13 The proof is similar to the proof of Lemma 4. We consider the last modification of Gt as a run of ILESS, under PX ,Y (Gt\u22121), with m0 , 2blog2mc\u22121 examples and delta equal to \u03b44m0 .\nUnder event K , the conditions of Lemma 4 hold, and by Lemma 11, RPX ,Y (Gt\u22121)( f \u2217) \u2264 R( f \u2217).\nWe simply apply Lemma 4 with these parameters to get A\u2032 (A in Lemma 4).\nA\u2032 = 4d ln( 16m0e\nd\u03b4/4m0 ) = 4d ln( 64m20e d\u03b4 ).\nThe fact that m/4\u2264 m0 \u2264 m/2 completes the proof.\nProof of Lemma 14 We know by Lemma 13 that there exist constants C1,C2 that depend only on ln(1\u03b4) and d, and are independent of m, s.t.\n\u03c3Active \u2264C1 lnm m +C2 \u221a lnm m \u00b7R( f \u2217).\nWe also know by the definition of Active-ILESS (Strategy 3), that it terminates when \u03c3Active is smaller than the given \u03b5. We will find m large enough s.t.\nC1 lnm m \u2264 \u03b5/2, (62)\nC2 \u221a lnm m \u00b7R( f \u2217)\u2264 \u03b5/2. (63)\nWe assume that \u03b5\u2264 1/e, as it is easy to find a proper m for \u03b5 > 1/e. Starting with Equation (62), we want to show that m = O(1\u03b5 ln( 1 \u03b5 )) satisfies it. Thus, we find k1 s.t.\nC1 ln(k1 1\u03b5 ln( 1 \u03b5 )) k1 1\u03b5 ln( 1 \u03b5 ) \u2264 \u03b5 2\n\u21d4 ln(k1 1\u03b5 \u00b7 ln( 1 \u03b5 )) ln(1\u03b5 ) \u2264 k1 2C1 .\nBounding the left-hand side of the equation for \u03b5\u2264 1/e gives us,\nln(k1 1\u03b5 \u00b7 ln( 1 \u03b5 ))\nln(1\u03b5 ) \u2264\nln(k1 1\u03b5 \u00b7 1 \u03b5 )\nln(1\u03b5 ) \u2264 2+ lnk1.\nWe need to find k1 that will satisfy\n2+ lnk1 \u2264 k1\n2C1 .\nk1 = 16C21 will work for C1 \u2265 1; otherwise, we take k1 = 10. We use the same procedure to show that m = O(R( f\n\u2217) \u03b52 ln( R( f \u2217) \u03b52 )) satisfies Equation (63). We\nrewrite the equation in the following way:\nlnm m \u2264 \u03b5\n2\n4C22R( f \u2217) , \u03b50.\nWe assume that \u03b50 \u2264 1/e (m = 4 holds otherwise) and find k2 s.t.\nln(k2 1\u03b50 ln( 1 \u03b50 ))\nk2 1\u03b50 ln( 1 \u03b50 )\n\u2264 \u03b50.\nAs before, we reduce the problem to finding k2 that satisfies\n2+ ln(k2)\u2264 k2.\nk2 = 4 suffices. We thus get that m = O( 1\u03b520 ln( 1\u03b520\n)) = O(R( f \u2217) \u03b52 ln( R( f \u2217) \u03b52 )) satisfies Equation (63). This implies that there exists a function m(1/\u03b5,R( f \u2217)) = O (\n1 \u03b5 ln( 1 \u03b5 )+ R( f \u2217) \u03b52 ln( R( f \u2217) \u03b52\n) that bounds the to-\ntal number of labels processed by Active-ILESS.\nLemma 19 [25] Let X1,X2, ...,Xn be independent Bernoulli trials with Pr[Xi = 1] = p, let X , \u2211ni=1 Xi, and let \u00b5 = EX. Then, for every \u03b1\u2265 0:\nPr(X < (1\u2212\u03b1)\u00b5)\u2264 exp(\u2212\u00b5\u03b12/2).\nProof This proof is taken from the work of John Canny [25]. For t > 0, we have\nPr(X < (1\u2212\u03b1)\u00b5) = Pr(exp(\u2212tX)> exp(\u2212t(1\u2212\u03b1)\u00b5)). (64)\nWe use Markov\u2019s inequality. For a nonnegative random variable X , and a > 0,\nPr(X \u2264 a)\u2264 E(X) a .\nWe apply the inequality for the right-hand side of Equation (64), to get\nPr(X < (1\u2212\u03b1)\u00b5)\u2264 E(exp(\u2212tX)) exp(\u2212t(1\u2212\u03b1)\u00b5) . (65)\nX1,X2, ...,Xn are independent and thus\nE(exp(\u2212tX)) = n\n\u220f i=1 E(exp(\u2212tXi)).\nFor each Xi E(exp(\u2212tXi)) = pe\u2212t +(1\u2212 p) = 1\u2212 p(1\u2212 e\u2212t).\nWe use the fact that 1\u2212 x < exp(\u2212x) for all x, with x = p(1\u2212 e\u2212t), to get\nE(exp(\u2212tXi))\u2264 exp(\u2212p(1\u2212 e\u2212t)),\nand conclude that\nE(exp(\u2212tX)) = n\n\u220f i=1\nE(exp(\u2212tXi))\u2264 n\n\u220f i=1\nexp ( \u2212p(1\u2212 e\u2212t) ) = exp ( n\n\u2211 i=1\np(e\u2212t \u22121) ) = exp ( \u00b5(e\u2212t \u22121) ) . (66)\nGoing back to Equation (65), we have,\nPr(X < (1\u2212\u03b1)\u00b5)\u2264 exp(\u00b5(e \u2212t \u22121))\nexp(\u2212t(1\u2212\u03b1)\u00b5) = exp\n( \u00b5(e\u2212t \u22121+ t\u2212 t\u03b1) ) . (67)\nWe choose t > 0 to make the right-hand side of the equation as small as possible. After derivation, we get that the best t is t = ln( 11\u2212\u03b1), and plugging it into Equation (67) gives us,\nPr(X < (1\u2212\u03b1)\u00b5) \u2264 exp (\n\u00b5(1\u2212\u03b1\u22121+ ln( 1 1\u2212\u03b1 )\u2212 ln( 1 1\u2212\u03b1\n)\u03b1) )\n= exp (\n\u00b5(\u2212\u03b1+ ln( 1 1\u2212\u03b1\n)(1\u2212\u03b1)) )\n=\n( e\u2212\u03b1\n(1\u2212\u03b1)1\u2212\u03b1\n)\u00b5 . (68)\nWe now simplify this bound to get the desired result. We know that (1\u2212\u03b1)1\u2212\u03b1 = e(1\u2212\u03b1)ln(1\u2212\u03b1), and by Taylor expansion\nln(1\u2212\u03b1) =\u2212\u03b1\u2212 \u03b1 2 2 \u2212 \u03b1 3 3 ...,\nwhich multiplied by (1\u2212\u03b1), gives us\n(1\u2212\u03b1)ln(1\u2212\u03b1) =\u2212\u03b1+ \u03b1 2 2 +positive terms >\u2212\u03b1+ \u03b1 2 2 . (69)\nPlugging (69) into Equation (68), we finally get,\nPr(X < (1\u2212\u03b1)\u00b5) \u2264 (\ne\u2212\u03b1\n(1\u2212\u03b1)1\u2212\u03b1 )\u00b5 = ( e\u2212\u03b1\ne(1\u2212\u03b1)ln(1\u2212\u03b1) )\u00b5 \u2264 ( e\u2212\u03b1\ne\u2212\u03b1+ \u03b12 2 )\u00b5 = e\u2212\u00b5\u03b1 2/2 (70)\nLemma 20 [25] Let X1,X2, ...,Xn be independent Poisson trials with Pr[Xi = 1] = p, let X ,\u2211ni=1 Xi, and let \u00b5 = EX. Then, for every \u03b1\u2265 2e\u22121:\nPr(X > (1+\u03b1)\u00b5)\u2264 2\u2212\u00b5\u03b1.\nProof Sketch This sketch is taken from the work of John Canny [25]. It is almost identical to the proof of Lemma 19.\nWe start by showing that\nPr(X > (1+\u03b1)\u00b5) \u2264 (\ne\u03b1\n(1+\u03b1)1+\u03b1\n)\u00b5 .\nFor every t > 0,\nPr(X > (1+\u03b1)\u00b5) = Pr[exp(tX)> exp(t(1+\u03b1)\u00b5)].\nAs we did in Lemma 19, we compute the Markov bound,\nPr(X > (1+\u03b1)\u00b5) \u2264 E(exp(tX)) exp(t(1+\u03b1)\u00b5) ,\nand use the fact that Xi are independent, just like in (66), to get that\nE(exp(tX))\u2264 exp ( \u00b5(et \u22121) ) .\nThus we get that\nPr(X > (1+\u03b1)\u00b5) \u2264 exp(\u00b5(e t \u22121))\nexp(t(1+\u03b1)\u00b5) = exp\n( \u00b5(et \u22121\u2212 t\u2212\u03b1t) ) .\nFrom deviation, we choose t = ln(1+\u03b1) to get\nPr(X > (1+\u03b1)\u00b5) \u2264 (\ne\u03b1\n(1+\u03b1)1+\u03b1\n)\u00b5 .\nFor \u03b1\u2265 2e\u22121:\nPr(X > (1+\u03b1)\u00b5) \u2264 (\ne\u03b1\n(1+\u03b1)1+\u03b1\n)\u00b5 \u2264 ( e\u03b1\n(2e)1+\u03b1\n)\u00b5 \u2264 ( e\u03b1\n(2e)\u03b1\n)\u00b5 = 2\u2212\u00b5\u03b1.\nLemma 21 Given that event K (see Definition 6.1) occurred, the radius of Batch-ILESS, as defined in Strategy 3, stage 4, satisfies\n\u03c3Active = O ( B m + \u221a B m \u00b7R( f \u2217) ) , (71)\nwhere B , 4d ln(8m 2e\nd\u03b4 ).\nProof Batch-ILESS simulates a run of Active-ILESS. Consider a run of Active-ILESSwith m0 examples and \u03b4 = \u03b40. The last iteration in which Gt has changed (relative to Gt\u22121) was iteration 2blog2 m0c , T . GT is calculated in exactly the same way as ILESS calculates its G under probability distribution PX ,Y (GT\u22121), when it is provided with T/2 examples, and \u03b402T as its delta. Assuming that event K occurred, we deduce that event E (see Definition 4.1) occurred as well. Therefore, Lemma 4 holds for the last iteration of Batch-ILESS.\nILESS operates in this run on T/2 labeled examples, and it holds that m0/4\u2264 T/2\u2264m0/2. The delta it uses in this run is \u03b402T > \u03b40 m0 , so by Lemma 4, we have\n\u03c3Active \u2264 6 B\nm0/4 +3\n\u221a B\nm0/4 \u00b7RPX ,Y (Gt\u22121)( f \u2217) = 24 B m0\n+6 \u221a\nB m0 \u00b7RPX ,Y (Gt\u22121)( f \u2217).\nTo finish the proof, we need to show that R( f \u2217)\u2265 RPX ,Y (Gt\u22121)( f \u2217). From Lemma 11, we know that when K occurs, any f \u2217 of the original distribution PX ,Y resides within Gt for all t. Thus, the true error of f \u2217 can only decrease under the revised distribution Gt\u22121( f \u2217).\nTheorem 22 Let F be a hypothesis class with VC-dimension d, and let PX ,Y be an unknown probability distribution. Assume that event K (see Definition 6.1) occurred. Then, for all f \u2217, the abstain rate is bounded by\n1\u2212\u03a6(Batch-ILESS)\u2264 \u03b8 f \u2217(R0) \u00b7R0,\nwhere\nR0 , 2 \u00b7R( f \u2217)+44 \u00b7 B m +12 \u00b7 \u221a B m \u00b7R( f \u2217).\nwhere B , 4d ln(8m 2e\nd\u03b4 ). This immediately implies (by definition) that\n1\u2212\u03a6(Batch-ILESS)\u2264 \u03b8(R0) \u00b7R0.\nProof Sketch The proof is very similar to the proof of Lemma 21. We observe the last modification of GT , and notice that the change was made according to a run of ILESS, on the implied probability distribution PX ,Y (GT\u22121). Then we simply activate Theorem 5 with the relevant parameters plugged into it.\nNote that by Lemma 11, all f \u2217 of the original distribution reside within Gt for all t, and thus, by Lemma 10, they are all true risk minimizers of PX ,Y (GT\u22121). This also implies that R( f \u2217) \u2265 RPX ,Y (Gt\u22121)( f\n\u2217) and thus can be used to bound Equation (30) of the original theorem that was proven for LESS. \u03b8 f is independent of PY |X for all f , and thus the change of the labels does not affect it."}], "references": [{"title": "On optimum recognition error and reject trade-off", "author": ["C. Chow"], "venue": "IEEE Trans. on Information Theory, vol. 16, pp. 41\u201336, 1970.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1970}, {"title": "Algorithmic Learning in a Random World", "author": ["V. Vovk", "A. Gammerman", "G. Shafer"], "venue": "New York: Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "On the foundations of noise-free selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 1605\u20131641, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Agnostic pointwise-competitive selective classification", "author": ["Y. Wiener", "R. El-Yaniv"], "venue": "Journal of AI Research, vol. 52, pp. 171\u2013201, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with rejection", "author": ["C. Cortes", "G. DeSalvo", "M. Mohri"], "venue": "International Conference on Algorithmic Learning Theory, pp. 67\u201382, Springer, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Agnostic selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Neural Information Processing Systems (NIPS), pp. 1665\u20131673, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Local complexities for empirical risk minimization", "author": ["P. Bartlett", "S. Mendelson", "P. Philips"], "venue": "COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 2004. 31", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A. Tsybakov"], "venue": "The Annals of Mathematical Statistics, vol. 32, pp. 135\u2013166, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML), pp. 353\u2013360, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithms for Active Learning", "author": ["D. Hsu"], "venue": "PhD thesis, Department of Computer Science and Engineering, School of Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Active learning using smooth relative regret approximations with applications", "author": ["N. Ailon", "R. Begleiter", "E. Ezra"], "venue": "25th Annual Conference on Learning Theory (COLT), 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A compression technique for analyzing disagreement-based active learning", "author": ["Y. Wiener", "S. Hanneke", "R. El-Yaniv"], "venue": "Journal of Machine Learning Research, vol. 16, pp. 713\u2013 745, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "On the version space compression set size and its applications", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Measures of Complexity, pp. 341\u2013357, Springer International Publishing, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pp. 65\u201372, ACM, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Activized learning: Transforming passive to active with improved label complexity", "author": ["S. Hanneke"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 5, pp. 1469\u20131587, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Theory of disagreement-based active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 7, no. 2-3, pp. 131\u2013309, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Surrogate losses in passive and active learning", "author": ["S. Hanneke", "L. Yang"], "venue": "arXiv:1207.3772, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D.J. Hsu", "C. Monteleoni"], "venue": "Advances in Neural Information Processing Systems 20, pp. 353\u2013360, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient and parsimonious agnostic active learning", "author": ["T.-K. Huang", "A. Agarwal", "D.J. Hsu", "J. Langford", "R.E. Schapire"], "venue": "Advances in Neural Information Processing Systems 28 (C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, eds.), pp. 2755\u20132763, Curran Associates, Inc., 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Active learning via perfect selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research, vol. 13, pp. 255\u2013279, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Theoretical Foundations of Selective Prediction", "author": ["Y. Wiener"], "venue": "PhD thesis, the Technion \u2014 Israel Institute of Technology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning, vol. 15, no. 2, pp. 201\u2013221, 1994.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "Advanced Lectures on Machine Learning, vol. 3176 of Lecture Notes in Computer Science, pp. 169\u2013207, Springer, 2003. 32", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Agnostic selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Neural Information Processing Systems (NIPS), 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Theory of Active Learning.", "author": ["S. Hanneke"], "venue": "http://www.stevehanneke.com,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "Lecture Notes in Computer Science, vol 4264. Springer, Berlin, Heidelberg, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems 18, pp. 235\u2013242, 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, vol. 28, pp. 133\u2013168, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient active learning of halfspaces: an aggressive approach", "author": ["A. Gonen", "S. Sabato", "S. Shalev-Shwartz"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 2583\u20132615, 2013. 33", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2].", "startOffset": 124, "endOffset": 130}, {"referenceID": 1, "context": "Introduction Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2].", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "Given a training sample consisting of m labeled instances, the learning algorithm is required to output a selective classifier [3], defined to be a pair ( f ,g), where f is a prediction function, chosen from some hypothesis class F , and g : X \u2192 {0,1} is a selection function, serving as a qualifier for f as follows: for any x, if g(x) = 1, the classifier predicts f (x), and otherwise it abstains.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "The selective classifier ( f ,g) is said to be pointwisecompetitive if, for each x with g(x) = 1, it must hold that f (x) = f \u2217(x) for all f \u2217 \u2208 F [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "The scenario of a predefined decision functions hypothesis class is investigated in [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "Pointwise-competitive selective classification (PCS) was first considered in the realizable case [3], for which a simple consistent selective strategy (CSS) was shown to achieve a bounded and monotonically increasing (with m) coverage in various non-trivial settings.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown.", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown.", "startOffset": 61, "endOffset": 67}, {"referenceID": 6, "context": "These bounds relied on the fact that the underlying probability distribution and the hypothesis class F will satisfy the so-called \u201c(\u03b21,\u03b22)-Bernstein property\u201d [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "The coverage bounds in [4, 6] are dependent on the parameters \u03b21,\u03b22.", "startOffset": 23, "endOffset": 29}, {"referenceID": 5, "context": "The coverage bounds in [4, 6] are dependent on the parameters \u03b21,\u03b22.", "startOffset": 23, "endOffset": 29}, {"referenceID": 6, "context": "This Bernstein property assumption (as presented in [7]), which allows for better concentration, can be problematic.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "Hanneke\u2019s disagreement coefficient [9] (see Definition 2.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].", "startOffset": 141, "endOffset": 152}, {"referenceID": 9, "context": "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].", "startOffset": 141, "endOffset": 152}, {"referenceID": 10, "context": "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].", "startOffset": 141, "endOffset": 152}, {"referenceID": 11, "context": "Note that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 12, "context": "Note that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 13, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 209, "endOffset": 217}, {"referenceID": 15, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 209, "endOffset": 217}, {"referenceID": 16, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 222, "endOffset": 226}, {"referenceID": 17, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 268, "endOffset": 272}, {"referenceID": 18, "context": "In [19], a computationally efficient algorithm for disagreement based AL.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "the red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows).", "startOffset": 56, "endOffset": 63}, {"referenceID": 9, "context": "the red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows).", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "We acquire the following definitions from [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "The disagreement set [9] and agreement set [3] w.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "The disagreement set [9] and agreement set [3] w.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "(5) In selective classification [3], the learning algorithm receives Sm and is required to output a selective classifier, defined to be a pair ( f ,g), where f \u2208 F is a classifier, and g : X \u2192 {0,1} is a selection function, serving as a qualifier for f as follows.", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Then, Hanneke\u2019s disagreement coefficient [9] of a classifier f \u2208 F with respect to the target distribution PX is", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "For more on the disagreement coefficient, and examples of probabilities distributions and hypothesis classes for which it is bounded, see [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "Convergence Bounds and LESS We use a uniform convergence bound from [18, 23].", "startOffset": 68, "endOffset": 76}, {"referenceID": 22, "context": "Convergence Bounds and LESS We use a uniform convergence bound from [18, 23].", "startOffset": 68, "endOffset": 76}, {"referenceID": 17, "context": "Lemma 1 ([18]) Let F be a hypothesis class with VC-dimension d.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "(14) Strategy 1 is the LESS algorithm of [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "In the original lemma from [18], there appears S(H ,n), the growth function.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We plug in Sauer\u2019s Lemma, S(H ,n)\u2264 ( em d ) d , into Lemma 1 from [18] to get our lemma.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "Remark 2 The original definition of pointwise-competitiveness from [4] requires a single f \u2217.", "startOffset": 67, "endOffset": 70}, {"referenceID": 15, "context": "In many cases, the disagreement coefficient, \u03b8(r), is bounded by a constant, or by O(polylog(1/r)) for all r > 0 (see [16]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "For example, it was shown in [12], that for linear separators under mixture of Gaussians, and for axis-aligned rectangles under product densities over Rk, \u03b8(r) is bounded by O(polylog(1/r)) for all r > 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "5, let X = [0,1], and F = { f1, f2} where", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "Let PX be the uniform distribution over [0,1].", "startOffset": 40, "endOffset": 45}, {"referenceID": 1, "context": "and for r in [2 ,1], \u2206B( f \u2217,r) r \u2264 1 1/2 = 2, (45)", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "and for r in [2 ,1], \u2206B( f \u2217,r) r \u2264 1 1/2 = 2, (45)", "startOffset": 13, "endOffset": 19}, {"referenceID": 9, "context": "Active-ILESS is very similar to Agnostic CAL [10], Algorithm 4.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "2 on page 36, and A2 [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "Moreover, while Agnostic CAL requires calculation of an ERM with many constraints (defined by the function LEARN in HSU\u2019s thesis), Active-ILESS requires a calculation of the ERM with only one constraint, as seen from the disbelief principle [24], already discussed in Section 4.", "startOffset": 241, "endOffset": 245}, {"referenceID": 9, "context": "In [10], Hsu introduced the agnostic CAL algorithm and showed (Theorem 4.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The leading term of this bound is R( f \u2217)2 \u03b52 , which is also the case for A 2 [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "This direction has been shown before in [9, 10] for agnostic CAL and A2.", "startOffset": 40, "endOffset": 47}, {"referenceID": 9, "context": "This direction has been shown before in [9, 10] for agnostic CAL and A2.", "startOffset": 40, "endOffset": 47}, {"referenceID": 15, "context": "As a preparation for the theorem, we present Lemma 17 (shown before in [16]), in which we introduce a small feature of the disagreement coefficient that will serve us later.", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "We found this useful bound in [26] (Theorem 5.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Our analysis proves that ILESS has sometimes significantly better rejection guarantees relative to the best known pointwisecompetitive selective strategy of [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 26, "context": "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.", "startOffset": 31, "endOffset": 43}, {"referenceID": 27, "context": "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.", "startOffset": 31, "endOffset": 43}, {"referenceID": 28, "context": "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.", "startOffset": 31, "endOffset": 43}], "year": 2017, "abstractText": "A selective classifier ( f ,g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A \u201cfast\u201d rejection rate is achieved if the rejection mass is bounded from above by \u00d5(1/m) where m is the number of labeled examples used to train the classifier (and \u00d5 hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke\u2019s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke\u2019s disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke\u2019s disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS.", "creator": "LaTeX with hyperref package"}}}