{"id": "1703.07015", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks", "abstract": "multivariate time series forecasting is an important machine learning problem across many domains, dramatically including predictions of volatile solar plant energy output, fuel electricity consumption, and traffic jam situation. temporal data arise in these real - world applications often involves assuming a mixture of long - term and transient short - term patterns, for which traditional simulation approaches such as autoregressive models and gaussian process may fail. in this paper, we proposed a novel deep learning empirical framework, called namely long - and not short - term time - series network ( lstnet ), to address this already open challenge. ibm lstnet uses the convolution neural network ( cnn ) engine to extract sequential short - term local dependency activation patterns among clustered variables, and the recurrent interaction neural network ( rnn ) to discover long - term patterns and trends. in our evaluation building on real - world data with physically complex mixtures of repetitive patterns, lstnet achieved significant performance improvements over again that of several state - of - the - art baseline methods.", "histories": [["v1", "Tue, 21 Mar 2017 00:33:36 GMT  (1198kb,D)", "https://arxiv.org/abs/1703.07015v1", null], ["v2", "Wed, 5 Jul 2017 01:24:10 GMT  (1204kb,D)", "http://arxiv.org/abs/1703.07015v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guokun lai", "wei-cheng chang", "yiming yang", "hanxiao liu"], "accepted": false, "id": "1703.07015"}, "pdf": {"name": "1703.07015.pdf", "metadata": {"source": "CRF", "title": "Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks", "authors": ["Guokun Lai", "Wei-Cheng Chang"], "emails": ["guokun@cs.cmu.edu", "wchang2@andrew.cmu.edu", "yiming@cs.cmu.edu", "hanxiaol@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Multivariate time series data are ubiquitous in our everyday life ranging from the prices in stock markets, the traffic flows on highways, the outputs of solar power plants, the temperatures across different cities, just to name a few. In such applications, users are often interested in the forecasting of the new trends or potential hazardous events based on historical observations on time series signals. For instance, a better route plan could be devised based on the predicted traffic jam patterns a few hours ahead, and a larger profit could be made with the forecasting of the near-future stock market.\nMultivariate time series forecasting often faces a major research challenge, that is, how to capture and leverage the dynamics dependencies among multiple variables. Specifically, real-world applications often entail a mixture of short-term and long-term repeating patterns, as shown in Figure 1 which plots the hourly occupancy rate of a freeway. Apparently, there are two repeating patterns, daily and weekly. The former portraits the morning peaks vs. evening peaks, while the latter reflects the workday and weekend patterns. A successful time series forecasting model should be capture both kinds of recurring patterns for accurate predictions. As another example, consider the task of predicting the output of a solar energy farm based on the measured solar radiation by massive sensors over different locations. The long-term patterns reflect the difference between days vs. nights, summer vs. winter, etc., and the short-term patterns reflect the effects of cloud movements, wind direction changes, etc. Again, without taking both kinds of recurrent patterns into account, accurate time series forecasting is not possible. However, traditional approaches such as the large body\nar X\niv :1\n70 3.\n07 01\n5v 2\n[ cs\n.L G\n] 5\nof work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically. Addressing such limitations of existing methods in time series forecasting is the main focus of this paper, for which we propose a novel framework that takes advantages of recent developments in deep learning research.\nDeep neural networks have been intensively studied in related domains, and made extraordinary impacts on the solutions of a broad range of problems. The recurrent neural networks (RNN) models [9], for example, have become most popular in recent natural language processing (NLP) research. Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].In the field of computer vision, as another example, convolution neural network (CNN) models [19, 21] have shown outstanding performance by successfully extracting local and shiftinvariant features (called \u201dshapelets\u201d sometimes) at various granularity levels from input images.\nDeep neural networks have also received an increasing amount of attention in time series analysis. A substantial portion of the previous work has been focusing on time series classification, i.e., the task of automated assignment of class labels to time series input. For instance, RNN architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories. RNN has also been applied to mobile data, for classifying the input sequences with respect to actions or activities [13]. CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.\nDeep neural networks have also been studied for time series forecasting, i.e., the task of using observed time series in the past to predict the unknown time series in a look-ahead horizon \u2013 the large the horizon, the harder the problem. Efforts in this direction range from the early work using\nnaive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].\nAlthough the aforementioned work have shed lights on how to use deep neural networks to improve time series analysis, none of them has offered good answers for the important questions below:\n\u2022 How can RNN (LSTM and GRU) and CNN be effectively combined for dynamic modeling of short-term and long-term dependency patterns in multi-variate time series data?\n\u2022 How much can the combined deep learning approach (CNN + RNN) improve the performance of representative autoregressive models?\n\u2022 Can we further combine the deep learning models (CNN + RNN) and traditional autoregressive models to achieve better performance than that of using each type of the models alone?\nAnswering the above questions are crucial for improving the state-of-the-art in time series forecasting, and is the main contribution we aim in this paper. Specifically, we propose a novel deep learning framework, namely Long- and Short-term Time-series Network (LSTNet), as illustrated in Figure 2. It leverages the strengths of both the convolutional layer to discover the local dependency patterns among multi-dimensional input variables, and the recurrent layer that captures complex long-term dependencies. A particular recurrent structure, namely Recurrent-skip, is designed for capturing very long-term dependence patterns and making the optimization easier as it utilizes the periodic property of the input time series signals. Finally, the LSTNet incorporates a traditional autoregressive linear model in parallel to the non-linear neural network part; which is similar to a highway component [29]. Adding the linear model to this framework we aim to address a potential weakness of the neural-network model, i.e., when/if the non-linear model is not sufficiently sensitive to the scale changes in input data, the linear model provides a better (more sensitive) alternative. Our evaluation results (Section 4.6) provide empirical evidence for this assertion.\nThe rest of this paper is organized as follows. Section 2 outlines the related background, including representative auto-regressive methods and Gaussian Process models. Section 3 describe our proposed LSTNet. Section 4 reports the evaluation results of our model in comparison with strong baselines on real-world datasets. Finally, we conclude our findings in Section 5."}, {"heading": "2 Related Background", "text": "One of the most prominent univariate time series models is the autoregressive integrated moving average (ARIMA) model. The popularity of the ARIMA model is due to its statistical properties as well as the well-known Box-Jenkins methodology [2] in the model selection procedure. ARIMA models are not only adaptive to various exponential smoothing techniques [25] but also flexible enough to subsume other types of time series models including autoregression (AR), moving average (MA) and Autoregressive Moving Average (ARMA). However, ARIMA models, including their variants for modeling long-term temporal dependencies [2], are rarely used in high dimensional multivariate time series forecasting due to their high computational cost.\nOn the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity. VAR models naturally extend AR models to the multivariate setting, which ignores the dependencies between output variables. Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model [27] for heavy-tail time series and structured VAR model [26] for better interpretations of the dependencies between high dimensional variables, and more. Nevertheless, the model capacity of VAR grows linearly over the temporal window size and quadratically over the number of variables. This implies, when dealing with long-term temporal patterns, the inherited large model is prone to overfitting. To alleviate this issue, [32] proposed to reduce the original high dimensional signals into lower dimensional hidden representations, then applied VAR for forecasting with a variety choice of regularization.\nTime series forecasting problems can also be treated as standard regression problems with timevarying parameters. It is therefore not surprising that various regression models with different loss functions and regularization terms are applied to time series forecasting tasks. For example, linear\nsupport vector regression (SVR) [4, 17] learns a max margin hyperplane based on the regression loss with a hyper-parameter controlling the threshold of prediction errors. Ridge regression is yet another example which can be recovered from SVR models by setting to zeros. Lastly, [22] applied LASSO models to encourage sparsity in the model parameters so that interesting patterns among different input signals could be manifest. These linear methods are practically more efficient for multivariate time series forecasting due to high-quality off-the-shelf solvers in the machine learning community. Nonetheless, like VARs, those linear models may fail to capture complex non-linear relationships of multivariate signals, resulting in an inferior performance at the cost of its efficiency.\nGaussian Processes (GP) is a non-parametric method for modeling distributions over a continuous domain of functions. This contrasts with models defined by a parameterized class of functions such as VARs and SVRs. GP can be applied to multivariate time series forecasting task as suggested in [28], and can be used as a prior over the function space in Bayesian inference. For example, [10] presented a fully Bayesian approach with the GP prior for nonlinear state-space models, which is capable of capturing complex dynamical phenomena. However, the power of Gaussian Process comes with the price of high computation complexity. A straightforward implementation of Gaussian Process for multivariate time-series forecasting has cubic complexity over the number of observations, due to the matrix inversion of the kernel matrix."}, {"heading": "3 Framework", "text": "In this section, we first formulate the time series forecasting problem, and then discuss the details of the proposed LSTNet architecture (Figure 2) in the following part. Finally, we introduce the objective function and the optimization strategy."}, {"heading": "3.1 Problem Formulation", "text": "In this paper, we are interested in the task of multivariate time series forecasting. More formally, given a series of fully observed time series signals Y = {y1,y2, . . . ,yT } where yt \u2208 Rn, and n is the variable dimension, we aim at predicting a series of future signals in a rolling forecasting fashion. That being said, to predict yT+h where h is the desirable horizon ahead of the current time stamp, we assume {y1,y2, . . . ,yT } are available. Likewise, to predict the value of the next time stamp yT+h+1, we assume {y1,y2, . . . ,yT ,yT+1} are available. We hence formulate the input matrix at time stamp T as XT = {y1,y2, . . . ,yT } \u2208 Rn\u00d7T . In the most of cases, the horizon of the forecasting task is chosen according to the demands of the environmental settings, e.g. for the traffic usage, the horizon of interest ranges from hours to a day; for the stock market data, even seconds/minutes-ahead forecast can be meaningful for generating returns.\nFigure 2 presents an overview of the proposed LSTnet architecture. The LSTNet is a deep learning framework specifically designed for multivariate time series forecasting tasks with a mixture of longand short-term patterns. In following sections, we introduce the building blocks for the LSTNet in detail."}, {"heading": "3.2 Convolutional Component", "text": "The first layer of LSTNet is a convolutional network without pooling, which aims to extract shortterm patterns in the time dimension as well as local dependencies between variables. The convolutional layer consists of multiple filters of width \u03c9 and height n (the height is set to be the same as the number of variables). The k-th filter sweeps through the input matrix X and produces\nhk = RELU(Wk \u2217X + bk) (1)\nwhere \u2217 denotes the convolution operation and the output hk would be a vector, and the RELU function is RELU(x) = max(0, x). We make each vector hk of length T by zero-padding on the left of input matrix X . The output matrix of the convolutional layer is of size dc \u00d7 T where dc denotes the number of filters."}, {"heading": "3.3 Recurrent Component", "text": "The output of the convolutional layer is simultaneously fed into the Recurrent component and Recurrent-skip component (to be described in subsection 3.4). The Recurrent component is a recurrent layer with the Gated Recurrent Unit (GRU) [6] and uses the RELU function as the hidden update activation function. The hidden state of recurrent units at time t is computed as,\nrt = \u03c3(xtWxr + ht\u22121Whr + br)\nut = \u03c3(xtWxu + ht\u22121Whu + bu)\nct = RELU(xtWxc + rt (ht\u22121Whc) + bc) ht = (1\u2212 ut) ht\u22121 + ut ct\n(2)\nwhere is the element-wise product, \u03c3 is the sigmoid function and xt is the input of this layer at time t. The output of this layer is the hidden state at each time stamp. While researchers are accustomed to using tanh function as hidden update activation function, we empirically found RELU leads to more reliable performance, through which the gradient is easier to back propagate."}, {"heading": "3.4 Recurrent-skip Component", "text": "The Recurrent layers with GRU [6] and LSTM [15] unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies. Due to gradient vanishing, however, GRU and LSTM usually fail to capture very long-term correlation in practice. We propose to alleviate this issue via a novel recurrent-skip component which leverages the periodic pattern in real-world sets. For instance, both the electricity consumption and traffic usage exhibit clear pattern on a daily basis. If we want to predict the electricity consumption at t o\u2019clock for today, a classical trick in the seasonal forecasting model is to leverage the records at t o\u2019clock in historical days, besides the most recent records. This type of dependencies can hardly be captured by off-theshelf recurrent units due to the extremely long length of one period (24 hours) and the subsequent optimization issues. Inspired by the effectiveness of this trick, we develop a recurrent structure with temporal skip-connections to extend the temporal span of the information flow and hence to ease the optimization process. Specifically, skip-links are added between the current hidden cell and the hidden cells in the same phase in adjacent periods. The updating process can be formulated as,\nrt = \u03c3(xtWxr + ht\u2212pWhr + br)\nut = \u03c3(xtWxu + ht\u2212pWhu + bu)\nct = RELU(xtWxc + rt (ht\u2212pWhc) + bc) ht = (1\u2212 ut) ht\u2212p + ut ct\n(3)\nwhere the input of this layer is the output of the convolutional layer, and p is the number of hidden cells skipped through. The value of p can be easily determined for datasets with clear periodic patterns (e.g. p = 24 for the hourly electricity consumption and traffic usage datasets), and has to be tuned otherwise. In our experiments, we empirically found that a well-tuned p can considerably boost the model performance even for the latter case. Furthermore, the LSTNet could be easily extended to contain variants of the skip length p.\nWe use a dense layer to combine the outputs of the Recurrent and Recurrent-skip components. The inputs to the dense layer include the hidden state of Recurrent component at time stamp t, denoted by hRt , and p hidden states of Recurrent-skip component from time stamp t\u2212 p+ 1 to t denoted by hSt\u2212p+1, h S t\u2212p+2 . . . , h S t . The output of the dense layer is computed as,\nhDt = W RhRt + p\u22121\u2211 i=0 WSi h S t\u2212i + b (4)\nwhere hDt is the prediction result of the neural network (upper) part in the Fig.2 at time stamp t."}, {"heading": "3.5 Autoregressive Component", "text": "Due to the non-linear nature of the Convolutional and Recurrent components, one major drawback of the neural network model is that the scale of outputs is not sensitive to the scale of inputs. Unfortunately, in specific real datasets, the scale of input signals constantly changes in a non-periodic manner, which significantly lowers the forecasting accuracy of the neural network model. A concrete example of this failure is given in Section 4.6. To address this deficiency, similar in spirit to the highway network [29], we decompose the final prediction of LSTNet into a linear part, which primarily focuses on the local scaling issue, plus a non-linear part containing recurring patterns. In the LSTNet architecture, we adopt the classical Autoregressive (AR) model as the linear component. Denote the forecasting result of the AR component as hLt \u2208 Rn, and the coefficients of the AR model as W ar \u2208 Rqar and bar \u2208 R, where qar is the size of input window over the input matrix. Note that in our model, all dimensions share the same set of linear parameters. The AR model is formulated as follows,\nhLt,i = qar\u22121\u2211 k=0 W ark yt\u2212k,i + b ar (5)\nThe final prediction of LSTNet is then obtained by by integrating the outputs of the neural network part and the AR component:\nY\u0302 t = h D t + h L t (6)\nwhere Y\u0302 t denotes the model\u2019s final prediction at time stamp t."}, {"heading": "3.6 Objective function", "text": "The squared error is the default loss function for many forecasting tasks, the corresponding optimization objective is formulated as,\nminimize \u0398 \u2211 t\u2208\u2126Train ||Y t \u2212 Y\u0302 t\u2212h||2F (7)\nwhere \u0398 denotes the parameter set of our model, \u2126Train is the set of time stamps used for training, || \u00b7 ||F is the Frobenius norm, and h is the horizon as mentioned in Section 3.1. The traditional linear regression model with the square loss function is named as Linear Ridge, which is equivalent to the vector autoregressive model with ridge regularization. However, experiments show that the Linear Support Vector Regression (Linear SVR) [30] dominates the Linear Ridge model in certain datasets. The only difference between Linear SVR and Linear Ridge is the objective function. The objective function for Linear SVR is,\nminimize \u0398\n1 2 ||\u0398||2F + C \u2211 t\u2208\u2126Train n\u22121\u2211 i=0 \u03bet,i\nsubject to |Y\u0302 t\u2212h,i \u2212 Y t,i| \u2264 \u03bet,i + , t \u2208 \u2126Train \u03bet,i \u2265 0\n(8)\nwhere C and are hyper-parameters. Motivated by the remarkable performance of the Linear SVR model, we incorporate its objective function in the LSTNet model as an alternative of the squared loss. For simplicity, we assume = 01, and the objective function above reduces to absolute loss (L1-loss) function as follows:\nminimize \u0398 \u2211 t\u2208\u2126Train n\u22121\u2211 i=0 |Y t,i \u2212 Y\u0302 t\u2212h,i| (9)\nIn the experiment section, we carefully examine the effectiveness of both objective functions defined in Equation 7 and Equation 9.\n1One could keep to make the objective function more faithful to the Linear SVR model without modifying the optimization strategy. We leave this for future study."}, {"heading": "3.7 Optimization Strategy", "text": "Our optimization strategy is the same as that in the traditional time series forecasting model. Supposing the input time series is Y = {y1,y2, . . . ,yT }, we define a tunable window size q, and reformulate the input at time stamp t as Xt = {yt\u2212q+1,yt\u2212q+2, . . . ,yt}. The problem then becomes a regression task with a set of feature-value pairs {Xt,yt+h}, and can be solved by Stochastic Gradient Decent (SGD) or its variants such as Adam [18]."}, {"heading": "4 Evaluation", "text": "We conducted extensive experiments with 8 methods (including our new methods) on 4 benchmark datasets for time series forecasting tasks."}, {"heading": "4.1 Methods for Comparison", "text": "The methods in our comparative evaluation are the follows.\n\u2022 AR stands for the autoregressive model, which is equivalent to the one dimensional VAR model. \u2022 LRidge is the vector autoregression (VAR) model with L2-regularization, which has been\nmost popular for multivariate time series forecasting. \u2022 LSVR is the vector autoregression (VAR) model with Support Vector Regression objective\nfunction [30] . \u2022 TRMF is the autoregressive model using temporal regularized matrix factorization by [32]. \u2022 GP is the Gaussian Process for time series modeling. [11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and\nautoregressive model. \u2022 LST-L1 is our proposed LSTNet model (Section 3) with the absolute loss objective func-\ntion. \u2022 LST-L2 is our proposed LSTNet model with the square loss objective function.\nFor the single output methods above such as AR, LRidge, LSVR and GP, we just trained n models independently, i.e., one model for each of the n output variables."}, {"heading": "4.2 Metrics", "text": "We used three conventional evaluation metrics defined as:\n\u2022 Root Relative Squared Error (RSE):\nRSE =\n\u221a\u2211 (i,t)\u2208\u2126Test(Yit \u2212 Y\u0302it)\n2\u221a\u2211 (i,t)\u2208\u2126Test(Yit \u2212mean(Y )) 2 (10)\n\u2022 Relative Absolute Error (RAE)\nRAE = \u2211 (i,t)\u2208\u2126Test |Yit \u2212 Y\u0302it|\u2211\n(i,t)\u2208\u2126Test |Yit \u2212mean(Y )| (11)\n\u2022 Empirical Correlation Coefficient (CORR)\nCORR = 1\nn n\u2211 i=1 \u2211 t ( Yit \u2212mean(Y i) )( Y\u0302it \u2212mean(Y\u0302 i) )\u221a\u2211 t ( Yit \u2212mean(Y i) )2( Y\u0302it \u2212mean(Y\u0302 i)\n)2 (12) where Y , Y\u0302 \u2208 Rn\u00d7T are ground true signals and system prediction signals, respectively. For RSE and RAE lower value is better, while for CORR higher value is better."}, {"heading": "4.3 Data", "text": "We used four benchmark datasets which are publicly available. Table 1 summarizes the corpus statistics.\n\u2022 Electricity2: The electricity consumption in kWh was recorded every 15 minutes from 2012 to 2014, for n = 321 clients. We converted the data to reflect hourly consumption;\n\u2022 Traffic3: A collection of 48 months (2015-2016) hourly data from the California Department of Transportation. The data describes the road occupancy rates (between 0 and 1) measured by different sensors on San Francisco Bay area freeways.\n\u2022 Solar-Energy4 : the solar power production records in the year of 2006, which is sampled every 10 minutes from 137 PV plants in Alabama State.\n\u2022 Exchange-Rate: the collection of the daily exchange rates of eight foreign countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore ranging from 1990 to 2016.\nAll datasets have been split into training set (60%), validation set (20%) and test set (20%) in chronological order. To facilitate future research in multivariate time series forecasting, we publicize all raw datasets and the one after preprocessing in the Github.5\nIn order to examine the existence of long-term and/or short-term repetitive patterns in time series data, we plot autocorrelation graph for some randomly selected variables from the four datasets in Figure 3. Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay defined below\nR(\u03c4) = E[(Xt \u2212 \u00b5)(Xt+\u03c4 \u2212 \u00b5)]\n\u03c32\nwhere Xt is the time series signals, \u00b5 is mean and \u03c32 is variance. In practice, we consider the empirical unbiased estimator to calculate the autocorrelation.\nWe can see in the graphs (a), (b) and (c) of Figure 3, there are repetitive patterns with high autocorrelation in the Electricity, Traffic and Solar-Energy datasets, but not in the Exchange-Rate dataset. Furthermore, we can observe a short-term daily pattern (in every 24 hours) and long-term weekly pattern (in every 7 days) in the graph of the Traffic dataset, which perfect reflect the expected regularity in highway traffic situations. On the other hand, in graph (d) of the Exchange-Rate dataset, we hardly see any repetitive long-term patterns, expect some short-term local continuity. These observations are important for our later analysis on the empirical results of different methods. That is, for the methods which can properly model and successfully leverage both short-term and long-term repetitive patterns in data, they should outperform well when the data contain such repetitive patterns (like in Electricity, Traffic and Solar-Energy). On the other hand, if the dataset does not contain such patterns (like in Exchange-Rate), the advantageous power of those methods may not lead a better performance than that of other less powerful methods. We will revisit this point in Section 4.7 with empirical justifications.\n2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 3http://pems.dot.ca.gov 4http://www.nrel.gov/grid/solar-power-data.html 5https://github.com/laiguokun/multivariate-time-series-data"}, {"heading": "4.4 Experimental Details", "text": "We conduct grid search over all tunable hyper-parameters on the held-out validation set for each method and dataset. Specifically, all methods share the same grid search range of the window size q ranging from {20, 21, . . . , 29} if applied. For LRidge and LSVR, the regularization coefficient \u03bb is chosen from {2\u221210, 2\u22128, . . . , 28, 210}. For GP, the RBF kernel bandwidth \u03c3 and the noise level \u03b1 are chosen from {2\u221210, 2\u22128, . . . , 28, 210}. For TRMF, the hidden dimension is chosen from {22, . . . , 26} and the regularization coefficient \u03bb is chosen from {0.1, 1, 10}. For LST-L1 and LSTL2, we adopted the training strategy described in Section 3.7. The hidden dimension of the Recurrent and Convolutional layer is chosen from {50, 100, 200}, and {20, 50, 100} for Recurrent-skip layer. The skip-length p of Recurrent-skip layer is set as 24 for the Traffic and Electricity dataset, and tuned range from 21 to 26 for the Solar-Energy and Exchange-Rate datasets. The regularization coefficient of the AR component is chosen from {0.1, 1, 10} to achieve the best performance. We perform dropout after each layer, except input and output ones, and the rate usually is set to 0.1 or 0.2. The Adam[18] algorithm is utilized to optimize the parameters of our model. We publicize the LSTNet code in the Github. 6"}, {"heading": "4.5 Main Results", "text": "Table 2 summarizes the evaluation results of all the methods (8) on all the test sets (4) in all the metrics (3). We set horizon = {3, 6, 12, 24}, respectively, which means the horizons was set from 3 to 24 hours for the forecasting over the Electricity and Traffic data, from 30 to 240 minutes over the Solar-Energy data, and from 3 to 24 days over the Exchange-Rate data. The larger the horizons, the harder the prediction tasks. The best result for each (data, metric) pair is highlighted in bold face in this table. The total count of the bold-faced results is 28 for LST-L1 (one version of the proposed LSTNet), 10 for LST-L2 (the other version of our LSTNet), 5 for AR, 4 for LRidge, and between 0 to 2 for the rest of the methods.\n6https://github.com/laiguokun/LSTNet\nClearly, LSTNet has the dominating performance on the first three datasets (Electricity, SolarEnergy and Traffic), especially in the settings with the large horizon, but slightly worse than AR and LRidge on the Exchange-Rate dataset. Why? Recall that in Section 4.3 and Figure 3 we used the autocorrelation curves of these datasets to show the existence of repetitive patterns in the Electricity, Solar-Energy and Traffic datasets but not in Exchange-Rate. The current results provide empirical evidence for the success of LSTNet models in modeling long-term and short-term dependency patterns when they do occur in data. Otherwise, LSTNet performed comparably with the better ones (AR and LRidge) among the representative baselines. The robustness of LSTNet is also due to its inclusion of the autoregressive model as a component, which we will discuss further in next section."}, {"heading": "4.6 Ablation Study", "text": "To examine the importance of each component in our framework, we conducted a set of ablation tests. Let us use the following notation for different settings in the examination.\n\u2022 LSTw/oskip: The LSTNet models (LST-L1 or LST-L2) without the Recurrent-skip component.\n\u2022 LSTw/oCNN: The LSTNet models (LST-L1 or LST-L2) without the Convolutional component.\n\u2022 LSTw/oAR: The LSTNet models (LST-L1 or LST-L2) without the AR component.\nThe test results measured using RSE are shown in Figure 57. Several observations from these results are worth highlighting:\n\u2022 The best result on each dataset is obtained with either LST-L1 or LST-L2; which of them is better depends on the dataset.\n\u2022 Removing the AR component (in LSTw/oAR) from the full model caused the most significant performance drops on most of the datasets, showing the crucial role of the AR component in general.\n\u2022 Removing the Skip and CNN components in (LSTw/oCNN or LSTw/oskip) caused big performance drops on some datasets but not all. All the components of LSTNet together leads to the robust performance of our approach on all the datasets.\n7We omit the results in RAE and CORR as they show similar comparison with respect to the relative performance among the methods.\nAs for why the AR component would have such an important role, our interpretation is that AR is generally robust to the sudden changes in data. To empirically validate this intuition we plot one dimension (one variable) of the time series signals in the electricity consumption dataset for the duration from 1 to 5000 hours in Figure 4, where the blue curve is the true data and the red curve is the system-forecasted signals. We can see that the true consumption suddenly increases around the 1000th hour, and that LST-L1 successfully captures this sudden change but LSTw/oAR fails to react properly. In other words, the neural-network component in LSTNet may not be sufficiently sensitive to random scale fluctuations in data (which is typical in Electricity data possibly due to random events for public holidays or temperature turbulence, etc.), while the simple linear AR model can make a proper adjustment in the forecasting.\nIn summary, this ablation study clearly justifies the efficiency of our architecture design. All components have contributed to the excellent and robust performance of LSTNet."}, {"heading": "4.7 Mixture of long- and short-term patterns", "text": "To illustrate the success of LSTNet in modeling the mixture of short-term and long-term recurring patterns in time series data, Figure6 compares the performance of LSTNet and VAR on an specific time series (one of the output variables) in the Traffic dataset. As discussed in Section 4.3, the Traffic data exhibit two kinds of repeating patterns, i.e. the daily ones and the weekly ones. We can see in Figure 6 that the true patterns (in blue) of traffic occupancy are very different on Fridays and Saturdays, and another on Sunday and Monday. However, the VAR model (part (a) of the figure) cannot learn such a distinction but predict the similar local patterns for both days instead. LSTNet, on the other hand (part (b) of the figure), successfully captures both the different repeating patterns on Mondays and Saturdays, and the local patterns within each day."}, {"heading": "5 Conclusion", "text": "In this paper, we presented a novel deep learning framework (LSTNet) for the task of multivariate time series forecasting. By combining the strengths of convolutional and recurrent neural networks and an autoregressive component, the proposed approach significantly improved the state-of-the-art results in time series forecasting on multiple benchmark datasets. With in-depth analysis and empirical evidence we show that LSTNet indeed successfully captures both short-term and long-term repeating patterns in data, and combines both linear and non-linear models for robust prediction. For future research we would like to extend the LSTNet framework for anomaly detection from time series data and for causality analysis based on the evolution of long- and short-term patterns."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Time series analysis: forecasting and control", "author": ["G.E. Box", "G.M. Jenkins", "G.C. Reinsel", "G.M. Ljung"], "venue": "John Wiley & Sons", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Distribution of residual autocorrelations in autoregressiveintegrated moving average time series models", "author": ["G.E. Box", "D.A. Pierce"], "venue": "Journal of the American statistical Association, 12  65(332):1509\u20131526", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1970}, {"title": "Support vector machine with adaptive parameters in financial time series forecasting", "author": ["L.-J. Cao", "F.E.H. Tay"], "venue": "IEEE Transactions on neural networks, 14(6):1506\u20131518", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Z. Che", "S. Purushotham", "K. Cho", "D. Sontag", "Y. Liu"], "venue": "arXiv preprint arXiv:1606.01865", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent networks and narma modeling", "author": ["J. Connor", "L.E. Atlas", "D.R. Martin"], "venue": "NIPS, pages 301\u2013308", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Nonlinear dynamic boltzmann machines for time-series prediction", "author": ["S. Dasgupta", "T. Osogami"], "venue": "AAAI-17. Extended research report available at goo. gl/Vd0wna", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Bayesian inference and learning in gaussian process state-space models with particle mcmc", "author": ["R. Frigola", "F. Lindsten", "T.B. Sch\u00f6n", "C.E. Rasmussen"], "venue": "Advances in Neural Information Processing Systems, pages 3156\u20133164", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian Time Series Learning with Gaussian Processes", "author": ["R. Frigola-Alcade"], "venue": "PhD thesis, PhD thesis, University of Cambridge", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series analysis", "author": ["J.D. Hamilton"], "venue": "volume 2. Princeton university press Princeton", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Deep", "author": ["N.Y. Hammerla", "S. Halloran", "T. Ploetz"], "venue": "convolutional, and recurrent models for human activity recognition using wearables. arXiv preprint arXiv:1604.08880", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["G. Hinton", "L. Deng", "D. Yu"], "venue": "E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u2013 97", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Hybrid neural network models for hydrologic time series forecasting", "author": ["A. Jain", "A.M. Kumar"], "venue": "Applied Soft Computing, 7(2):585\u2013592", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Financial time series forecasting using support vector", "author": ["K.-j. Kim"], "venue": "machines. Neurocomputing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal convolutional networks: A unified approach to action segmentation", "author": ["C. Lea", "R. Vidal", "A. Reiter", "G.D. Hager"], "venue": "Computer Vision\u2013ECCV 2016 Workshops, pages 47\u201354. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Forecasting macroeconomic time series: Lasso-based approaches and their forecast combinations with dynamic factor models", "author": ["J. Li", "W. Chen"], "venue": "International Journal of Forecasting, 30(4):996\u20131015", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to diagnose with lstm recurrent neural networks", "author": ["Z.C. Lipton", "D.C. Kale", "C. Elkan", "R. Wetzell"], "venue": "arXiv preprint arXiv:1511.03677", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "New introduction to multiple time series analysis", "author": ["H. L\u00fctkepohl"], "venue": "Springer Science & Business Media", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "General exponential smoothing and the equivalent arma process", "author": ["E. McKenzie"], "venue": "Journal of Forecasting, 3(3):333\u2013344", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1984}, {"title": "Estimating structured vector autoregressive model", "author": ["I. Melnyk", "A. Banerjee"], "venue": "arXiv preprint arXiv:1602.06606", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust estimation of transition matrices in high dimensional heavy-tailed vector autoregressive processes", "author": ["H. Qiu", "S. Xu", "F. Han", "H. Liu", "B. Caffo"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1843\u20131851", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Gaussian processes for time-series modelling", "author": ["S. Roberts", "M. Osborne", "M. Ebden", "S. Reece", "N. Gibson", "S. Aigrain"], "venue": "Phil. Trans. R. Soc. A, 371", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1984}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["V. Vapnik", "S.E. Golowich", "A. Smola"], "venue": "Support vector method for function approximation, regression estimation, and signal processing. Advances in neural information processing systems, pages 281\u2013287", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["J.B. Yang", "M.N. Nguyen", "P.P. San", "X.L. Li", "S. Krishnaswamy"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI), Buenos Aires, Argentina, pages 25\u201331", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Temporal regularized matrix factorization for highdimensional time series prediction", "author": ["H.-F. Yu", "N. Rao", "I.S. Dhillon"], "venue": "Advances in Neural Information Processing Systems, pages 847\u2013855", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Forecasting with artificial neural networks:: The state of the art", "author": ["G. Zhang", "B.E. Patuwo", "M.Y. Hu"], "venue": "International journal of forecasting, 14(1):35\u201362", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Time series forecasting using a hybrid arima and neural network model", "author": ["G.P. Zhang"], "venue": "Neurocomputing, 50:159\u2013175", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 11, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 21, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 31, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 33, "context": "of work in autoregressive methods [2, 12, 22, 32, 34] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically.", "startOffset": 34, "endOffset": 53}, {"referenceID": 8, "context": "The recurrent neural networks (RNN) models [9], for example, have become most popular in recent natural language processing (NLP) research.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 13, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 18, "context": "Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) [15] and the Gated Recurrent Unit (GRU) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 18, "context": "In the field of computer vision, as another example, convolution neural network (CNN) models [19, 21] have shown outstanding performance by successfully extracting local and shiftinvariant features (called \u201dshapelets\u201d sometimes) at various granularity levels from input images.", "startOffset": 93, "endOffset": 101}, {"referenceID": 20, "context": "In the field of computer vision, as another example, convolution neural network (CNN) models [19, 21] have shown outstanding performance by successfully extracting local and shiftinvariant features (called \u201dshapelets\u201d sometimes) at various granularity levels from input images.", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "For instance, RNN architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories.", "startOffset": 119, "endOffset": 126}, {"referenceID": 22, "context": "For instance, RNN architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories.", "startOffset": 119, "endOffset": 126}, {"referenceID": 12, "context": "RNN has also been applied to mobile data, for classifying the input sequences with respect to actions or activities [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.", "startOffset": 62, "endOffset": 74}, {"referenceID": 19, "context": "CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "CNN models have also been used in action/activity recognition [13, 20, 31], for the extraction of shift-invariant local patterns from input sequences as the features of classification models.", "startOffset": 62, "endOffset": 74}, {"referenceID": 6, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 17, "endOffset": 20}, {"referenceID": 15, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 43, "endOffset": 55}, {"referenceID": 32, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 43, "endOffset": 55}, {"referenceID": 33, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 43, "endOffset": 55}, {"referenceID": 2, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "naive RNN models [7] and the hybrid models [16, 33, 34] combining the use of ARIMA [3] and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 28, "context": "Finally, the LSTNet incorporates a traditional autoregressive linear model in parallel to the non-linear neural network part; which is similar to a highway component [29].", "startOffset": 166, "endOffset": 170}, {"referenceID": 1, "context": "The popularity of the ARIMA model is due to its statistical properties as well as the well-known Box-Jenkins methodology [2] in the model selection procedure.", "startOffset": 121, "endOffset": 124}, {"referenceID": 24, "context": "ARIMA models are not only adaptive to various exponential smoothing techniques [25] but also flexible enough to subsume other types of time series models including autoregression (AR), moving average (MA) and Autoregressive Moving Average (ARMA).", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "However, ARIMA models, including their variants for modeling long-term temporal dependencies [2], are rarely used in high dimensional multivariate time series forecasting due to their high computational cost.", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity.", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity.", "startOffset": 115, "endOffset": 126}, {"referenceID": 23, "context": "On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series [2, 12, 24] due to its simplicity.", "startOffset": 115, "endOffset": 126}, {"referenceID": 26, "context": "Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model [27] for heavy-tail time series and structured VAR model [26] for better interpretations of the dependencies between high dimensional variables, and more.", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model [27] for heavy-tail time series and structured VAR model [26] for better interpretations of the dependencies between high dimensional variables, and more.", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "To alleviate this issue, [32] proposed to reduce the original high dimensional signals into lower dimensional hidden representations, then applied VAR for forecasting with a variety choice of regularization.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "support vector regression (SVR) [4, 17] learns a max margin hyperplane based on the regression loss with a hyper-parameter controlling the threshold of prediction errors.", "startOffset": 32, "endOffset": 39}, {"referenceID": 16, "context": "support vector regression (SVR) [4, 17] learns a max margin hyperplane based on the regression loss with a hyper-parameter controlling the threshold of prediction errors.", "startOffset": 32, "endOffset": 39}, {"referenceID": 21, "context": "Lastly, [22] applied LASSO models to encourage sparsity in the model parameters so that interesting patterns among different input signals could be manifest.", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "GP can be applied to multivariate time series forecasting task as suggested in [28], and can be used as a prior over the function space in Bayesian inference.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "For example, [10] presented a fully Bayesian approach with the GP prior for nonlinear state-space models, which is capable of capturing complex dynamical phenomena.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The Recurrent component is a recurrent layer with the Gated Recurrent Unit (GRU) [6] and uses the RELU function as the hidden update activation function.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "The Recurrent layers with GRU [6] and LSTM [15] unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies.", "startOffset": 30, "endOffset": 33}, {"referenceID": 14, "context": "The Recurrent layers with GRU [6] and LSTM [15] unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies.", "startOffset": 43, "endOffset": 47}, {"referenceID": 28, "context": "To address this deficiency, similar in spirit to the highway network [29], we decompose the final prediction of LSTNet into a linear part, which primarily focuses on the local scaling issue, plus a non-linear part containing recurring patterns.", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "However, experiments show that the Linear Support Vector Regression (Linear SVR) [30] dominates the Linear Ridge model in certain datasets.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "The problem then becomes a regression task with a set of feature-value pairs {Xt,yt+h}, and can be solved by Stochastic Gradient Decent (SGD) or its variants such as Adam [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "\u2022 LSVR is the vector autoregression (VAR) model with Support Vector Regression objective function [30] .", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "\u2022 TRMF is the autoregressive model using temporal regularized matrix factorization by [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "[11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and autoregressive model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and autoregressive model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "[11, 28] \u2022 VAR-MLP is the model proposed in [34] that combines Multilayer Perception (MLP) and autoregressive model.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "The Adam[18] algorithm is utilized to optimize the parameters of our model.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these realworld applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Longand Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) to extract short-term local dependency patterns among variables, and the Recurrent Neural Network (RNN) to discover long-term patterns and trends. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. The dataset and experiment code both are uploaded to Github.", "creator": "LaTeX with hyperref package"}}}