{"id": "1703.07394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Deep Learning for Explicitly Modeling Optimization Landscapes", "abstract": "in all quantitative but invariably the most trivial optimization problems, the structure attributes of the solutions exhibit complex interdependencies between resolving the domain input parameters. decades of research with stochastic search techniques has shown the benefit of explicitly modeling complex the topological interactions between sets of parameters and interpreting the overall quality of the solutions discovered. we demonstrate a novel modelling method, based on learning deep networks, to model the global landscapes of optimization problems. to represent the search space concisely and interpret accurately, the deep networks must encode certain information about the underlying parameter interactions and their contributions to the quality of the solution. once the networks are trained, the networks are probed to publicly reveal parameter combinations with high expected performance with respect to attaining the optimization task. these estimates are used to initialize fast, randomized, local search algorithms, which does in turn expose more information about the search space that initially is subsequently used to refine the models. importantly we demonstrate the technique on proving multiple optimization problems that have arisen in underlying a variety of real - data world domains, including : box packing, graphics, job scheduling, layout and window compression. the functional problems include combinatoric search spaces, discontinuous and highly non - linear spaces, and span binary, higher - cardinality discrete, as well as continuous parameters. such strengths, limitations, and extensions of designing the approach mechanism are extensively discussed and demonstrated.", "histories": [["v1", "Tue, 21 Mar 2017 19:12:35 GMT  (526kb,D)", "http://arxiv.org/abs/1703.07394v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["shumeet baluja"], "accepted": false, "id": "1703.07394"}, "pdf": {"name": "1703.07394.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Explicitly Modeling Optimization Landscapes", "authors": ["Shumeet Baluja"], "emails": ["shumeet@google.com"], "sections": [{"heading": "1 Introduction to Optimization via Search Space Modeling", "text": "In the 1990s, a number of researchers [1] [2] [3] [4] independently started employing probabilistic models to guide heuristic stochastic based-search algorithms. The idea was simple, to use the knowledge of the search landscape that may be ascertained by analyzing the points encountered to guide where to look next. This sharply contrasted the procedure of many successful randomized hill-climbing algorithms that made small perturbations stochastically in hopes that a better solution was a close neighbor to the current best solution.\nIn the simplest instantiation, probabilistic methods explicitly maintain statistics about the search space by creating models of the good solutions found so far. These models are then sampled to generate the next query points to be evaluated. The sampled solutions are then used to update the model, and the cycle is continued. Probabilistic models for optimization were motivated with three goals: as a new optimization method, as a method to incorporate simple learning (Hebbian style probability updates) into hillclimbing, and as a method to explain how genetic algorithms [5] [6] [7] might work.\nA bit more detail on the third motivation is warranted: by maintaining a population of points (rather than a single point from which search is centered), genetic algorithms (GAs) can be viewed as creating implicit\nar X\niv :1\n70 3.\n07 39\n4v 1\n[ cs\n.N E\n] 2\n1 M\nar 2\nprobabilistic models of the solutions seen in the search. GAs attempt to implicitly capture dependencies between parameters and solution quality through the distribution of parameter settings contained in the population of solutions. Exploration proceeds by generating new samples to evaluate through a process of applying randomized \u201crecombination/crossover\u201d operators to pairs of high-performance members of the population. Because only the high-performance are selected for recombination, parameter settings found in the poor-performing solutions are not explored further. The recombination operator, which combines two \u201cparents\u201d candidate solutions by producing \u201cchildren\u201d that transfer sets of parameters from each parent into the children, serves to keep sets of parameters together (though in a randomized manner) in the next set of candidate solutions that are explored. In terms of sampling, this can be viewed as a simple approach to sampling the population\u2019s statistics. Though similar in intent, this implicit manner of sampling the distribution of solutions contrasts with the approach presented in this paper. Here, explicit steps are taken to model the parameters interdependencies that contribute to the quality of candidate solutions. The goal of this paper is succinctly stated as follows:\nThe primary goal of this paper is to demonstrate that while employing any searchalgorithm (hillclimbing, GA, simulated annealing, etc.), it is possible to simultaneously learn a deep-neural network based approximation of the evaluation function. Then, through the use of \u201cdeep-network inversion\u201d (employed as a method to sample the deep networks), intelligent perturbations of some/all of the samples generated by the search algorithm can be made. After these perturbations, the the new solutions should have an increased probability of higher scores.\nAs a secondary benefit, this modeling may also be used to make the evaluation more efficient by replacing it with the DNN\u2019s approximation of the evaluation function."}, {"heading": "1.1 Predecessors to Deep Modeling of Optimization Landscapes", "text": "Within the GA literature, one of the first attempts at using population level statistics was the \u201cBit-Based Simulated Crossover (BSC)\u201d operator [8]. Instead of combining pairs of solutions, population-level statistics were used to generate new solutions. The BSC operator worked as follows. For each bit position, the number of population members which contain a one in that bit position were counted. Each member\u2019s contribution was weighted by its fitness with respect to the target optimization function. The same process was used to count the number of zeros. Instead of using traditional crossover operators to generate new solutions, BSC generated new query points by stochastically assigning each bit\u2019s value by the probability of having seen that value in the previous population (the value specified by the weighted count). The important point to note is that BSC used the population statistics to generate new solutions.\nExtending the above idea and incorporating Hebbian learning, another early probabilistic optimization was the Population-Based Incremental Learning algorithm (PBIL) [1]. Rather than being based on population-genetics, the lineage for PBIL stems back to early reinforcement learning. PBIL is akin to a cooperative system of discrete learning automata in which the automata choose their actions independently, but all automata receive a common reinforcement dependent upon all their actions [9]. Unlike most previous studies of learning automata, which have commonly addressed optimization in noisy but very small environments, PBIL was used to explore large deterministic spaces.\nThe PBIL algorithm, most easily described with a binary alphabet, works as follows. The algorithm maintains a real-valued probability vector, specifying the probability of generating a 1 or a 0 in each bit position (just the first order statistics). The probability vector is sampled repeatedly to generate a set of new candidate solutions. These candidates are evaluated with respect to the optimization function, and the best solution is kept and all the other discarded. The probability vector is \u201cmoved\u201d towards the best solution through simple Hebbian-like updates. And the cycle is repeated. Note that the probabilistic model created in PBIL is extremely simple: There are no inter-parameter dependencies captured; each bit is modeled independently. The entire probability model is a single vector. Although this simple probabilistic model was used, PBIL was successful when compared to a variety of standard genetic algorithm and hillclimbing procedures on numerous benchmark and real-world problems. A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].\nThe most immediate improvements to PBIL are mechanisms that capture inter-parameter dependencies. Mutual Information Maximization for Input Clustering (MIMIC) [15] was one of the first to do this. MIMIC captured a heuristically chosen set of the pairwise dependencies between the solution parameters. From the top N% of all previously generated solutions, pair-wise conditional probabilities were calculated. MIMIC then used a greedy search to generate a chain in which each variable was conditioned on the previous variable. The first variable in the chain, X1, was chosen to be the variable with the lowest unconditional entropy, H(X1). When deciding which subsequent variable, Xi+1, to add to the chain, MIMIC selected the variable with the lowest conditional entropy, H(Xi+1|Xi). This extended the solely-unconditional model with PBIL to maintain a set of pair-wise dependencies. In [16], MIMIC\u2019s probabilistic model was extended to a larger class of dependency graphs: trees in which each variable is conditioned on at most one parent. As shown in 1968, this created the optimal tree-shaped network for a maximum-likelihood model of the data [17]. In experimental comparisons, MIMIC\u2019s chain-based probabilistic models typically performed significantly better than PBIL\u2019s simpler models. The tree-based graphs performed significantly better than MIMIC\u2019s chains.\nThe trend indicated that more accurate probabilistic models increased the probability of generating new candidate solutions in promising regions of the search space [18]. The natural extension to pair-wise modeling is modeling arbitrary dependencies. Bayesian networks are a popular method for efficiently representing dependencies [19] [20]. Bayesian networks are directed acyclic graphs in which each variable is represented by a vertex, and dependencies between variables are encoded as edges. Numerous researchers have taken the step of combining full Bayesian networks with stochastic search [21] [22] [23]. For an overview, see [24].\nAn alternate model building approach, termed STAGE, was presented in [25]. STAGE attempts to map a set of user-supplied features of the state space to a single value. This value represents the quality of solutions found thus far. The \u201cvalue function\u201d is used to select the next point from which to initialize search. Unlike the other algorithms described above, which attempt to automatically model the effects of parameter combinations on the overall solution quality, here hand-created features related to the optimization function are used. Other than the use of hand-crafted features, this approaches shares many of the important characteristics of the methods used in this paper.\nIn the next section, we describe the Deep-Opt algorithm and give details of how the probabilistic model is created and sampled. We also describe how the model is integrated with fast-search heuristics, following the work of [25] and [16]. In Section 3, we examine the performance of Deep-Opt on a wide variety of test problems. There are many interesting alternatives and extensions to the approach used here; experiments with three are presented in Section 4. The three alternatives help to increase the set of problems that can be tackled and address some of the limits of the approach as described in Section 2. Finally, a discussion of the results and suggestions for future work are presented in Section 5."}, {"heading": "2 Deep Learning for Search Space Modeling", "text": "Optimization with probabilistic modeling, at a high level, is simply explained in Figure 1. As shown in the figure, in previous work, we started with a large set of candidate solutions and evaluated them with respect to the objective function of the problem to be solved (e.g. the total tour length of the classic traveling salesman problem). The poor-performing candidate solutions were discarded. The set of remaining solutions, usually a small subset of the better performing members from the original set, were then modeled. The model was stochastically sampled, the solutions evaluated, and the process was continued.\nIn the interest of being concrete, a simple example of a probabilistic model is provided in Figure 2 using PBIL\u2019s independent-parameter model to represent 4 different populations 1. Once the model, PM , is created, PM is sampled to generate the candidate solutions to evaluate next. Why does this idea work? The solutions that are represented in PM are only those with high evaluations (the poor performing ones were discarded and never modeled), therefore, those that are subsequently generated by sampling the\n1Note that in a variety of previous studies, the solution-vectors were represented as a string of binary parameters as this worked well with the operators used with evolutionary algorithms.\nstatistics of multiple good solutions should be high-performing as well. A large number of solutions are generated, each is evaluated, and the lower-performing ones discarded, and the procedure is repeated 2.\nNote that although PBIL\u2019s probabilistic model is extremely simple, and no inter-parameter dependencies are modeled, as mentioned earlier, even this proved effective in many standard optimization tasks. Despite the successes, however, as shown in Figure 2 there are severe limitations: population sets B & D, although different, are represented with the same probabilistic vector; thereby demonstrating the inadequacy of the models and the need for more powerful representations of the candidate solution\u2019s statistics. Thus, models such as MIMIC & Optimal-Dependency-Trees (described in Section 1), in which pair-wise or higher-order dependencies were represented, were introduced. As problem complexity increased, these more flexible and powerful models improved the quality of the solutions generated.\nIn this paper, we use a neural network to create a mapping between the solutions sampled to that point and their score, as determined by the evaluation function. In the context of describing the algorithm, we also expose four of the largest differences between our approach and the approaches used earlier.\n(1) One of the primary differences is that by using a deep neural network, we do not a priori specify the form of the dependencies in the probabilistic model (e.g. pair-wise, or triplet combinations, etc.) Although the architecture of the neural network used for modeling is manually specified, the actual dependencies that the network encodes need not be the same form for all parameters, nor are they deeply tied to architecture of the network [26]. For all but one of the tests described in this paper, the neural network architecture will not change; a deep network is capable of modeling the necessary statistics.\n2Note that in the PM sampling process, a small amount of random mutation/perturbations are also introduced into the generated solutions to ensure that a heterogeneous set of solutions are explored.\n(2) Once the network has learned the mapping from input parameters to evaluation, a procedure for generating the new points to evaluate is necessary. This procedure is quite different than the straightforward sampling that was possible in the previous models such as those in PBIL or dependency trees. In the simplest model, PBIL (see Figure 2), samples were easily generated by a biased-random sample, where a \u20191\u2019 was generated in position p independently from any other bit, as specified by the real-value in position p in the probability vector. With models with dependencies, such as the dependency-trees [17], the sampling is simply conditioned on the variables on which the parameter is dependent (as specified in the tree-based model).\nWith neural networks, however, generating samples is more complex. It is based on the technique of \u201cnetwork inversion\u201d: given a trained network, network inversion uses standard back-propagation to modify the inputs rather than the network\u2019s weights. The inputs are modified to match the preset and clamped outputs. This method was first presented in [27] and has recently been popularized within the context of texture and style generation with neural networks [28] [29].\nWe use it as follows: We are given a trained network that maps the input parameters (scaled between 0.0 and 1.0) to their evaluation (also scaled between 0.0 and 1.0). First, all the weights of the network are frozen; they will not change for the sample generation process. Second, we clamp the output to the desired output \u2014 for maximization problems, we clamp the output 1.0; this indicates that we would like to generate solutions that are as good as the best ones seen so far.\nThe inputs are then initialized (either randomly or by other means such as perturbing the best solution seen thus far) and the network performs a forward propagation step and the error is measured at the output. The error metric is the standard Least-Mean Squares Error on the target output (which is clamped to 1.0).\nELMS = \u2211\ni\u2208outputs (targetsi \u2212 predictedi)2\nSince we pin the target to 1.0, and there is only a single output scaled between 0.0 and 1.0 (that represents the score of the candidate solution). This is simply:\nELMS = (1.0\u2212 predictedscore)2\nA process similar to standard training with stochastic-descent back-propagation (or any other network training procedure) is then used. However, unlike standard training, the errors are propagated all the way back to the inputs, and the inputs are modified \u2013 not the weights. As described in [27], the procedure addresses the following question: \u201cWhich input should be fed into the net to produce an output which approximates the given target vector T\u2019 (in our case the the target vector T is a simple scalar of 1.0). The error signal for the input i:\n\u03b4i = \u2212 \u03b4E\n\u03b4inputi\ntells the input units how to change (direction and magnitude) to decrease the error 3. In general, modest learning-rates for the gradient descent algorithm were found to work best for the network-inversion process (we used the Adam Optimizer [30] with learning-rate = 0.001). If the networks are trained well such that they successfully model the search landscape, then the set of input values found through this procedure will yield high-evaluation solutions when tested on the actual evaluation function.\n(3) Once the new candidate solutions are generated and evaluated, what is the next step? In the previous uses of probabilistic models (Figure 1), the low-performance candidate solutions were discarded and the high-performance solutions were kept. Interestingly, the actual evaluation of the high-performance\n3Whenever the units are changed outside the bounds of [0.0,1.0], they are clipped to be within the range. An alternative approach could have passed the activations through sigmoid functions; however, this would make setting values at the extrema (0.0 and 1.0) slower.\nsolutions was not used. In contrast, in our procedure, we create an explicit mapping from the candidate solution to its score 4. This reflects a fundamental difference in approaches: since an explicit mapping is created, it is not assumed that the model only represents good solutions; it represents both high and low quality solutions.\n(4) Finally, note that in contrast to many previous studies that represented the solution vectors as binary strings and modeled only the binary parameters, the deep-neural networks used in this study naturally model real-values. Extensions to binary and other discrete parameters will be described in Section 4."}, {"heading": "2.1 Integration with Fast Local Search Heuristics", "text": "In the simplest implementation, the candidate solutions generated by the network inversion are evaluated and the cycle is continued. Although this method will work, there are drawbacks. First, this is a slow process; training a full network to map the sampled points to their evaluations is an expensive procedure, as is sampling the network. Second, a post-processing step of local optimization, where small changes are made to the solutions generated, yields improvement to the solutions found. This is because both the interpolation and extrapolation capabilities of the trained networks are not perfect; there will be discrepancies between the estimated \u201cgoodness\u201d (evaluation) of a candidate solution and its actual evaluation.\nTwo early works in probabilistic model-based optimization [16] [25] suggested the use of the probabilistic models as methods to initialize faster local-search optimization techniques. This technique is used here. A very simple next-ascent stochastic hillclimbing (NASH) procedure is shown in Figure 3. It has repeatedly proven to work well in practice when used in conjunction with other optimization algorithms to perform local optimization, and also surprisingly well when used alone in a variety of scenarios\n4In our previous explorations of probabilistic models, some instantiations had weighted the contribution of each member of S by their relative score. Although this changes the models by specifying how well each samples is represented in the model, it does not create an explicit mapping from the parameters to their scores.\n[31] [32] [33] [34]. For the majority of the paper, we will use NASH as the underlying search process; the neural modeling will \u201cwrap-around\u201d NASH.\nImportantly, note that NASH is initialized with a single candidate solution. This candidate is perturbed until an equal or better solution is found. This fits well into the procedures described thus far: all of the candidate solutions evaluated by NASH can be added to the pool of solutions to be modeled, S. When the networks are trained and a next set of candidate solutions are generated, the single best solution from them is used to initialize the NASH algorithm. NASH proceeds as normal, again recording all the candidate solutions it evaluates \u2013 which are then use to augment S in the next time step, and the cycle continues.\nA visual description of the algorithm is shown in Figure 4."}, {"heading": "2.2 Putting it all Together", "text": "In this section, we describe some of the pragmatic considerations for deployment, and put all of the steps into step-by-step directions.\nThere are many possible ways to create and maintain S, the samples that are used for training the modeling networks. In our study, the size of S is kept constant throughout the run. Although a sufficiently large deep neural network should be able to represent the surface represented by all of the points seen, it would require both extra computation and not be valuable in practice. Overly precise models of low-evaluation areas are not necessary. To keep S a constant size, after every NASH run, the last 1000 unique solutions from the NASH run are added to S, and S is pruned back by removing the members that have been in S the longest (this implements a simple first-in-first-out scheme), as was suggested in previous studies [16]. If the algorithm is progressing correctly, the average score of the solutions present in S increases over time.\nThe full Deep-Opt algorithm is shown in Figure 5. With respect to the step \u201cTrain a Deep Neural Network\u201d, there are several points that should be noted. The use of a validation set is vital to good performance. In each training cycle, (100) samples are drawn by perturbing the best solution and added to the validation set. Then, after each step in training, the correlation between the actual evaluation of these samples and the network\u2019s predicted evaluation of these samples is measured. When the correlation begins to decrease, training is stopped and the next step of the algorithm begins. If the correlation is negative, training is restarted with random weights.\nThere are two implementation details not specified in Figure 5. In the first step, Line 1, several variants of creating the initial set, S, are possible. The simplest is, as shown, generating samples entirely randomly. Fully random sampling gives the broadest exploration of the search space. Alternatively, we could have performed a single NASH run and saved all the points explored. Using only one run, however, gives a poor representation of the global landscape because only a deep, single, path is explored. A third alternative is to sample a number of seed-points randomly and also explore their local neighborhoods by making small perturbations of the seed points. This gives a cursory indication of the local landscape around each of the seed points; empirically, this improved results on the problems tested. We use this initialization method throughout the paper. A full analysis was not conducted to measure the effects of alternate approaches.\nSecond, as can be seen, there are a large number of parameters and decisions made in the algorithm design. Our goal is to show that using a deep neural network is capable of modeling the search space, not necessarily advocating a particular network or set of parameters. Nonetheless, to make the study complete, we need to specify the networks we used. For the trials in this paper, two networks were used. The first, Deep-Opt-5, used a 5-fully-connected-layer network with 100 hidden units per layer. The second, Deep-Opt-10 used a 10-layer fully-connected network with 20 hidden units per layer and skip connections between every layer and its predecessors. Both networks have a single output - the estimate of the evaluation of the function being estimated. Weight decay was used in training (0.98). A variety of networks were tried, ranging from simple single layer networks to even deeper ones. As expected, the best performer was dependent on the problem, however these provided good results across multiple problems.\nFinally, the very careful reader may have noted that the effort to separate the steps in Lines 8-11 from those in lines 13-19. Later in the paper, we will show how lines 8-11 can be replaced with other procedures."}, {"heading": "2.3 Visualizing the Learning", "text": "Before turning our attention to the empirical tests in the next section, we present a motivating example to demonstrate how the local search algorithms utilize the models created. We examine a simple problem in which the evaluation is: evaluation(x) = \u221150 i=1(xi \u2217 sin(xi)). We instantiate this problem with 50 parameters that can take on values between [0,100]. Note that in this maximization problem, each parameter is independent. Though this is trivial problem, it serves to demonstrate the expected behavior.\nIf a NASH search is initialized with a solution vector that has low values for any of the parameters, there are many local optima on the way to the global optima. Nonetheless, because of the ease of the problem, even with no learning, some of the runs will find the global optima.\nIn Figure 6 (top-row), in the left column are the starting points for all 50 of the parameters before the NASH algorithm is run. As expected, they are randomly distributed across the input range. By the end of the NASH run (right column) many of the parameters are close to their optimal settings.\nThe middle row of Figure 6 shows the same (start/end configurations) for the 10th restart of NASH. Because NASH is still initialized randomly (no learning), we expect to see largely the same distribution of points in the beginning and the end. The same is shown in the bottom row, the 25th restart of NASH.\nNext, we repeat the same experiment with Deep-Opt. The only difference is in the initialization of the NASH algorithm. The samples that are generated in each NASH run are added to the set of solutions that are modeled by the neural network. From this neural network model, M new samples are drawn. The single best of the M is used to initialize the next NASH run. As expected, in Figure 7, the first run looks similar to the earlier case with no model. This is because there is no information in the model as yet. However, after the 10th NASH restart, (middle-row, Figure 7) there is a distinct difference in the starting values \u2013 many more are already in high-evaluation regions. The best solution found through sampling had many of its parameters in the right region of the search space. The likelihood of these reaching the global optimum is increased through NASH (right column). By the 25th run, this trend is even further evident. Sampling the model works as expected: the starting point for the hillclimbing is already in a better region - where more parameters are closer to the global optima.\nThough this problem is simple, it demonstrates how the modeling can improve the search results. Interestingly, early on in our studies, modeling sometimes led to poorer overall performance. Why? As the probabilistic model improved, exploration decreased \u2013 more samples started in a basin of attraction that led to the same local maxima as was seen previously. Improvement slowed because the updates to the model all happened with similar candidate solutions. The algorithm parameters, in particular the size of the sample set that was used for modeling, |S|, were tuned to the settings shown in Figure 5 to slow the convergence; this vastly improved performance. The effects of output scaling (discussed later in Section 4.2) are also relevant to this observation.\nThe (blue) points are the settings of each the 50 parameters in the beginning (left) and ending (right) of the NASH procedure (random perturbations of best seen-so-far). A histogram showing the distribution of the points is shown in red (this is shown because of the difficulty in seeing the overlapping points in this graph). Note that in the beginning of each run (left column), the points are uniformly distributed across the full input space. The results have vastly improved through hillclimbing, though not all points have made it to the global optimum. Top Row: NASH run #1, Middle Row: NASH run #10 (this is the HC run made after the 9th modeling step), Bottom Row: NASH run #25. Note about the graph: although each points represents one of the 50 parameters in the same single solution string, they are shown \u2019folded-over\u2019 onto the same graph. This is possible because each parameter is independent and is evaluated with respect to the same function."}, {"heading": "3 Empirical Results", "text": "In this section, we examine the benefits of the probabilistic model to select starting points for the NASH optimization procedure on a number of problems drawn from the literature and real-world needs. As described earlier, when the model is used, M samples are generated, the best of which is used to initialize NASH. To ensure that the model is actually providing useful information and that it is not merely the process of examining M samples before beginning a new run of NASH that is yielding the improved performance, three variants of NASH are explored. Though they vary in seemingly small implementation details, the effects on performance can be dramatic.\n\u2022 NASH-V1: This is exactly NASH shown in Figure 3. \u2022 NASH-V2: Before beginning the NASH run, M samples are randomly generated and evaluated.\nThe best one found from the M generated is used to initialize the NASH algorithm. No learning is used here. This variant is included to test whether just the process of generating multiple samples and selecting the best prior to starting NASH is enough to provide improvement to the final result \u2013 even with no modeling.\n\u2022 NASH-V3: Before beginning the NASH run, M samples are generated by making small perturbations to the best solution found in all the previous NASH runs. (The first NASH run is initialized randomly). Each of the M samples is then evaluated. The best sample found from the M is used to initialize the NASH algorithm. This variant tests whether the neural network models actually capture the shape of the search space, or whether they are only (inefficiently) forcing search around the best solutions seen so far.\nIt is interesting to note that on some of the problems detailed below, adding any of these heuristics will lead to a degradation in performance. For these problems, it is likely that the search space does not contain easily learnable trends\u2013 either because large portions are flat or pocked with local-optima, or that the information cannot be correctly modeled with the networks used here. This will be discussed with the problem descriptions. Using problem-specific hand-crafted features, such as done with STAGE [25], may help.\nThe termination condition for each NASH run was either (1) 10,000 evaluations were performed or (2) 500 evaluations were conducted with no-improvement. The latter indicated that the search might be trapped in a local maxima. All approaches were given a total of 500,000 evaluations. The number of NASH runs that were conducted within the 500,000 evaluations was dependent on the problem and how quickly/often the algorithm was unable to escape local optima."}, {"heading": "3.1 Noisy Evaluations", "text": "In the previous section, we used a simple Sines maximization problem as an illustrative example of how learning aids search. Due to the problem\u2019s simplicity, most search algorithms can perform well on it. However, with the introduction of noise, a clearer separation in performance emerges.\nIn this version of the problem, Noisy-Sines, the evaluation was modified to include significant uniformly distributed random noise. Uniformly chosen random noise between [0, 0.5] was added to the evaluation. For large parts of the search space, the overall evaluation is dominated by noise (note that if the same solution is evaluated twice, the noise is chosen independently each time).\nevaluation(x) = \u221150 i=1(xi \u2217 sin(xi)) 50.0 \u2217 100.0 + uniformNoise(0, 0.5) (1)\nThe performance of each algorithm is judged by the best solution found for the underlying objective function (as determined without the noise); no algorithm is privy to the underlying real function. The results are shown in Table 1. For each of the 5 algorithms tried (NASH-1,NASH-2,NASH-3, DeepOpt-5-Layers, Deep-Opt-10-Layers), the best evaluation averaged over trials is listed in the first row. The last two rows show the significance of the difference between the algorithm\u2019s performance and the performance of Deep-Opt-5-Layers and Deep-Opt-10-layers, respectively.\n3.2 Stable Marriage Reception-Party Seating\nThough this problem shares part of its name with the stable marriage problem, it is more akin to knapsack/packing problems. Real versions of this problem have arisen in topics as diverse as processor scheduling to intern and group seating layouts.\nIn a canonical version of this problem, G parties are invited to a formal-seated party, such as a wedding reception. Each party can have a variable size. Each member of the party must sit together at one of the T tables, which each have a capacity Ct. The additional twist to this problem is that each G has a preference with whom to sit with, expressed as a real value. The full preference matrix is |G \u00d7 G|. Preferences can be negative, and not constrained in magnitude. Further, it is not a requirement that each guest express a preference to every other guest, or even to any guests. Preferences may not be symmetric.\nThe goal is to find a seating assignment that (1) keeps the members of each group together, (2) does not seat people beyond the capacity of the table, (3) maximizes the summed happiness/preferences over all the tables. For the size of the problems explored here, the reception has 10 tables, each with capacity 12 people. Each guest\u2019s party is randomly chosen between 1 and 3 people. Preferences were expressed as a value between [-100, +100]. The number of groups, |G|, was set to 50. To encode the solution as a vector, each group was assigned T parameters, corresponding to each of T tables; there were a total of 500 (50\u00d7 10) parameters, (realV alueParameterg,t). At evaluation time, these 500 parameters were sorted from high to low. Based on the sorted list of realV alueParameterg,t assignments were made in order from highest to smallest of group g to table t. Note that the assignment occurred only if the group was (1) as yet unseated and (2) the table could hold the size of the group; otherwise that parameter was ignored and the next one processed. This encoding has the benefit of not only specifying each groups\u2019 preferences to tables, but also being able to encode \u201chow important\u201d it is that a particular group be assigned to a particular table.\n20 unique problems were created and tested with randomly generated, complete, |G \u00d7G| preference matrices. The random generation of problems led to an extremely large spread of final answers across problems. To summarize the results, we compared the five approaches, and gives the numbers of problems (out of 20) on which each algorithm obtained the highest evaluation (highest summed preferences at the tables, with all the constraints being met). The results are shown in Table 2. The next line of the table give the number of trials (out of 20) that Deep-Opt-5-layers outperformed the other 4 methods. The last line does the same for Deep-Opt-10-layers."}, {"heading": "3.3 Graph Bandwidth", "text": "Given a graph with V vertices and E edges, the graph bandwidth problem is to label the vertices of the graph with unique integers so that the difference between the labels between any two connected vertices is minimized. Formally, as described in [36], label the p vertices vi of a graph G with distinct integers f(vi) so that the quantity max{ |f(vi)\u2212 f(vj)| : vivj \u2208 E } is minimized (E is the edge set of G). More details of the complexity of this problem can be found in [37]. Interest in this problem stems from a variety of sources, including constraint satisfaction [38] and minimizing propagation delay in the layout of electronic cells.\nThe solution is encoded as follows: each vertex is assigned a real-valued parameter (full solution encoding of length |V |). The vertices are sorted according to their respective assigned values. The integers [1..V ] are then assigned to the vertices in their respective sort position. Once each vertex has an integer assignment, the maximum difference between the assignments of connected edges is returned. The results are shown in Table 3. This is a particularly difficult problem; ties are shown in parentheses."}, {"heading": "3.4 Graph-Based Constraint Satisfaction", "text": "Constraint Satisfaction has numerous real-world applications. We recently used it for resource allocation and job scheduling. Is is presented here in its simplest form.\nSimply stated, in this problem, there are P = 100 real-value parameters in the range [0,1.0]. These parameters are assigned to the vertices in a graph. The graph contains 2,000 randomly chosen, directed, edges which specify a constraint that the origination-node must hold a value greater than the destinationnode. The optimization problem is to assign values to the nodes such that as many of the 2,000 constraints are satisfied as possible. If the constraint is not met, the error is the absolute difference in the two values. The error, to be minimized, is summed over all constraints. The results are shown in Table 4."}, {"heading": "3.5 Graph-Based Discrete Constraint Satisfaction", "text": "In this variant of the previous graph-based constraint satisfaction problem, the exact same setup is used as in Section 3.4, however, each node may only take on 1 of 16 letters \u2013 A..P . In terms of the real-world application of job scheduling mentioned above, in this version of the problem, jobs can enter the system only at specific, synchronized times. This makes the problem closer to a selection problem (where one of the 16 values is selected for each of the nodes) as compared to the previous instantiation where a real value was assigned to each node.\nThough conceptually a small difference from the above encoding, discretization has enormous ramifications in the solution encoding. The simplest encoding is to use 100 real-valued outputs, one for each node, and divide the [0,1] region into 16 evenly spaced regions, each assigned to a single letter. However, for the reasons explained in Section 4.1, this encoding performs poorly. Instead, we use an encoding more amenable to selection problems and/or discrete-parameters; this encoding improves the performance of both Deep-Opt and as well as NASH alone.\nThe encoding used is similar to the Reception-Party-Seating task described in Section 3.2. Each vertex in the graph is assigned 16 real-valued parameters; each corresponding to a single letter A..P (In contrast, recall that with the encoding described in Section 3.4, each vertex was simply assigned 1 real-valued parameter). In each set of 16, the maximum value is found and the corresponding letter assigned to the vertex. In sum, for a 100 node graph, 1,600 parameters are used. Once the graph nodes are assigned values, the rest of the evaluation proceeds as described in Section 3.4."}, {"heading": "3.6 Two Dimensional Layout Problems", "text": "This section highlights the limitations of the Deep-Opt approach. A number of problems which broadly encompassed the task of two dimensional layout, did not improve significantly with search space modeling. Two of the problems are detailed here."}, {"heading": "3.6.1 Minimizing Crossings", "text": "The goal is to find a planar layout of a graph\u2019s nodes that minimizes edge crossings. See [39] for more details. In general, the edges can be drawn in any shape. For simplicity, in our implementation, the edges are drawn only with straight lines, this is termed the rectilinear crossing number.\nFor our tests, each node was represented with two parameters (x,y coordinates). Small graphs were tried with 25 nodes. This yielded a solution encoding of 50 real-values, which specified the coordinates of each point on a plane. Each graph had 50 randomly chosen connections. 20 randomly generated problem instantiations were attempted.\nOne of the interesting findings is that NASH-2 outperformed NASH-3. In most previous experiments, this was reversed. NASH-2 received a higher score in 12 out of the 20 problems (the scores, as measured by a standard t-test were statistically different with p = 0.96). Recall that NASH-2 initialized each hillclimbing run by first generating a small number of random candidate solutions and selecting the best one. In contrast, NASH-3 perturbs the current best solution to determine the best starting point for NASH. Although left for future exploration, it is worth investigating what insight this gives about the search space? If searching around the current best does not yield as good results as randomly starting over, does the search space have more or less optima, or are the local optima further spread apart, deep, etc? We leave the speculation of the ramifications of NASH-2 outperforming NASH-3 to future work. However, because the Deep-Opt can just as easily be applied to NASH-2 as NASH-3, for the experiments in this section, we used it to \u201cwrap\u201d NASH-2. Everything else, all of the parameters, etc., remained the same."}, {"heading": "3.6.2 Image Approximation via Triangle Covering", "text": "This is the only problem in the paper for which the parameters for NASH were changed to be optimized for this problem. Accordingly, Deep-Opt also used the same parameters. Had these parameters not been reset, neither NASH nor Deep-Opt would perform as well as shown here, with Deep-Opt under-performing NASH alone.\nIn this problem, there is an intensity target image (just black and white, no color information), I , that is N \u00d7 N pixels. The goals is to find T triangles and their intensities that approximate the image. Specifically, each triangle must specify three vertices between [0,N] in both X&Y and an intensity value. The triangles are drawn onto an initially empty canvas. The triangles may overlap; their intensities are additive. After all T triangles are drawn, the resulting image is scaled back into the appropriate space (0 .. 255 pixel intensities), and compared, pixel-wise, to the original image. The L2 distance is to be minimized. (The problem is set up as a maximization problem by taking the the reciprocal of the summed L2 distance.) This is a particularly difficult/interesting problem when T is small.\nTo set the parameters for NASH and Deep-Opt, we tested this problem with 50 triangles, trying to approximate an intensity based crop of \u201cThe Scream\u201d by Edvard Munch. Each triangle was encoded as 7 parameters: 6 for the (x,y) coordinates of three vertices and 1 for the intensity. With 50 triangles, there were a total of (50 \u00d7 7 =) 350 parameters in the solution encoding. Once the parameters were set, 3 other images, shown in Figure 8, were also attempted with the same parameter settings. The relative performance is given in Table 7. Note that the overall performance is virtually identical. This is averaged over 10 trials on each image."}, {"heading": "4 Extensions & Alternatives", "text": "In this section, we briefly describe three extensions and further tests to the Deep-Opt. Though the results are promising, they are preliminary."}, {"heading": "4.1 Discrete/Binary Parameters", "text": "Deep-Opt has been described with real-valued parameters. The input parameters, as well as the target output, are scaled to [0.0,1.0]. However, there are a wide variety of problems that employ discrete or binary parameters. Here, we present one method to tackle such problems.\nTo ground the discussion, let\u2019s examine a simple two dimensional checkerboard problem [16]. In this problem, there is a planar 15\u00d7 15 grid of binary digits. The goal is to set each digit such that it is the opposite of the digits in its 4 primary directions. There are two globally optimal solutions, but there there are also many locally optimal solutions such that any single bit-flip will not yield improved performance.\nFirst, we conducted an experiment to determine how Deep-Opt, as described to this point, would perform on this task. The solution encoding is 225 bits. In this instantiation, the maximum possible evaluation for this task is 676 (13*13*4); each of the bits in the 4 primary directions that are correctly set for the inner 13\u00d7 13 square contributes one point to the evaluation. The average Deep-Opt based solution quality was 537. For comparison, the three versions of NASH, without any learning, had average solution qualities of 655,655 and 668 (very close to the optimal), respectively. This difference between these and Deep-Opt is one of the largest witnessed in this entire study.\nWhy did this happen? In this initial attempt, the straight-forward method to interpret the real valued solution parameters as binary was used. If the real value was over 0.5, the bit was assigned to 1 and 0 otherwise. With this scheme, note that small changes to a parameter\u2019s real value often did not yield any change in solution string (unless the value was close to 0.5) or, thereby, to its evaluation. Therefore, many solution strings appeared to have the same evaluation, though they held different values as real-valued vectors.\nIdeally, if the bit position should be 1, we should favor solutions that have the real value associated with that bit position as far above 0.5 as possible (and the opposite for bit positions that should be 0). To do this, we use a technique similar to stochastic sigmoid units in training the neural network to model the search space (for a recent paper on this and related topics, see [40]).\nRecall that in the candidate generation phase (Lines 13-19 in Figure 5), we start with an input of a candidate solution vector, s. Over a number of iterations of back-driving the network, the inputs are modified by following the gradients needed to transform the candidate solution to one such that the network computes a value of 1.0 when the revised candidate solution is input. In the discrete version (referred to as Deep-Discrete-Opt), we instead treat each real valued parameter, p, p \u2208 s as a probability. We generate a small number of binary solutions strings (200) where each parameter in position-p has a probability of being assigned a 1.0 of p. All of these binary solutions are then fed to the network and the gradients computed for all of the solution strings (to move the network\u2019s evaluation of each of them closer to the target output of 1.0). Note that there is no need to evaluate each of these against the actual, real, evaluation function \u2013 only the network\u2019s output is measured to set the error signal. Although this procedure adds to the processing time, the result are vastly improved. Out of 10 trials, 9 had perfect evaluations of 676; overall the trials had an average of 673.4. This overcomes the previous limitations of the naive implementation of binarizing the real-value parameters.\nFor completeness, we also tried an alternative to this procedure to determine if just increasing the learning rate in the generation process would cause enough moves across the 0.5 boundary to achieve the same benefits. A variety of larger learning rates were tried (from 2\u00d7 to 40\u00d7 the standard learning rate used previously). In no case did the performance improve. The generation of binary strings outperformed all versions with only increased learning rates."}, {"heading": "4.2 The Role of Scaling Examples and Their Outputs", "text": "Deep-Opt, as used in this paper, works by back-driving the neural network inputs to change to cause the network to yield an output of 1.0 at its output node. Recall that the evaluations of the solutions generated in S are scaled, before each training session, to values between [0.0,1.0].\nScaling the evaluations of S to a fixed range leads to a subtle complexity. Note that a subset of the members of S change in every training cycle: as NASH completes a hillclimbing run, some of the newly found solutions are appended to S and other solutions from S removed. Often, in the early parts of search, the range of the actual evaluations present in S expands to include new, higher evaluation members while many of the randomly found initial members of S (with low evaluations) are still present. The opposite happens in the latter parts of search; the range of values contracts as the search focuses primarily on the better solutions and all the solutions have close (high) evaluations.\nBecause of this, note that in trainingCyclet, a member, s may have a real evaluation of X that is scaled to X\u2019 (when all the members of S are scaled between 0 and 1.0). In trainingCyclet+1, that same s, which has the same real evaluation of X, may be scaled to X\u201d, where X\u201d could be either greater or less than X\u2019. Because the networks continue their training in every cycle, this has not proven to be a problem as the change in evaluation seems to be quickly recovered; this may be because the relative ordering of the samples does not change. However, in the future, it will be interesting to try variants in which the re-scaling is not done. This may require engineering the setup of the problems so that they always output values in a fixed, a priori known, range.\nThis expansion and contraction dynamic process happens implicitly through the addition of new members of S as search progresses. Returning to the Sine problem, we can witness the convergence of the minimum and maximum values represented in S as new items are added to S (shown with a small |S| =5000); see Figure 9. The important trend is that the minimum and maximum values rapidly converge. This does not necessarily imply that all the candidate solutions in |S| are similar (though likely many are); it does mean that they are close in performance 5. The rate of this convergence corresponds to the exploitation vs. exploration trade-off.\nOne can imagine that instead of scaling the target outputs to values in the range [0.0,1.0], they were scaled to [0.0,Z], where Z < 1.0. In this approach, the highest scoring s \u2208 S will have a value of Z. When the network is back-driven, it is still driven to find solutions that produce a 1.0 in the output.\n5 Though difficult to see, towards the right of the graph, when new good solutions are found in later updates, the difference between the lines increases for a few iterations.\nSemantically, this attempts to create new solutions that are explicitly better than, not just equal to, those seen so far. The success of this approach is pinned on the network\u2019s successful extrapolation of the underlying search surface to regions of better performance. Many versions of this were tried, where Z was set to [0.2,0.5,0.8,0.9,0.95, and 0.99] in various experiments. Although the results are preliminary, setting the Z to 0.95 and higher had little effect on the results, when compared to setting Z = 1.0. Setting Z in the low range often hurt performance. Further exploration is left for future work."}, {"heading": "4.3 Alternative Underlying Search Algorithms", "text": "In this paper, we have coupled the use of deep-net modeling with an extremely simple, fast, localized search algorithm, NASH. It is important to also consider the possibility of using alternate search algorithms with the same modeling procedures. It is obvious how alternative search heuristics such as simulated annealing [41] and TABU search [42] can easily be substituted as they are often used to search neighborhoods around a single point in a manner similar to NASH. Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.\nRecall that, unlike NASH, genetic algorithms work from a population of points. Members of the population are created with recombination operators (crossover) that combine the elements of two or more candidate solutions. The newly created solutions are further randomly perturbed (mutated) to reveal the \u2019children\u2019 solutions that are the candidate solutions to evaluate next [5]. Numerous variations of GAs and task-specific operators are possible and have been explored in the research literature. Next, we perform a set of tests using a a simple-GA with the parameters shown in Table 8. These are typical of GAs used for static optimization problems in the literature6. To learn more about GAs, please see [5].\nWe test the GA with two population sizes (50 & 100) run for an equivalent number of function evaluations as all of the previous runs with hillclimbing (500,000). Additionally, as with the previous runs, the GA was restarted after 10,000 evaluations. In the standard GA, the initial population is comprised of candidate solutions that were randomly generated. In the Deep-Opt-GA, the initial population of candidate solutions is entirely generated from the back-driven neural network model.\nThis approach was tested on the same real-valued constraints problems described in Section 3.4. The results7 are shown in Table 9. Deep-Opt-GA outperformed the random initialization on all 20 instantiations attempted, for both sets of trials (with population size 50 and 100). The population size did not make a significant difference. Although not directly relevant to the effects of Deep-Opt, it is interesting to note how these results compare to the hillclimbing runs described in Section 3.4. Even the simplest NASH\n6It is likely that other operators and operator application rates may yield improved optimization algorithms for these problems. Our intent is more modest- simply to show that the Deep-Opt framework can be as easily wrapped around multiple-point search-based algorithms, such as GAs, as well as single-point search based algorithms, such as Hillclimbing.\n7To be consistent with the rest of the paper, the problem evaluation was inverted to make the minimization problem a maximization problem; hence, larger scores are better.\n(NASH-1), with a performance of 5206, did better than the GA models tested; though the GA models were consistently improved with learning. To summarize the findings in this section, using models to guide search can help even in search heuristics that operate from more than a single point \u2013 those that are population-based, such as genetic algorithms."}, {"heading": "5 Discussion and Future Work", "text": "We have presented a novel method to incorporate deep-learning with stochastic optimization. It is the next instantiation of intelligent model-based stochastic optimization and follows in the tradition of the probabilistic model based optimization approaches from the last two decades of research. An important aspect of this work is that a priori information about the problem is minimal in setting the form of the model. In this study, two multi-layer feed-forward networks with 5 and 10 hidden layers were used on all of the problems with no problem-specific modifications. With the judicious use of early-stopping in training, the potential downsides for overtraining were overcome.\nA consideration that we did not discuss in this paper is the speed of optimization. Between every hillclimbing run, a network is trained and sampled; this can be a time consuming procedure on the problems with the large solution-encoding (e.g. the discrete constraint satisfaction problem). As yet, we have not made attempts to optimize this; however, in the future, methods that employ fewer training steps between hillclimbing runs should be explored. It is likely that will achieve similarly positive results.\nMany of the advances from the rapidly evolving field of deep learning can be directly incorporated into this work (such as network shrinking, rapid training, regularization schemes, etc.), as continuously training networks is the core of the learning system. Outside of the deep-learning advancements, three avenues of immediate interest for future explorations are given below.\nFirst, the overarching goal of this paper was to concretely demonstrate the integration of neural network learning with optimization, not to promote this system as finalized optimization system. To make a finalized optimization system, further exploration of the algorithm\u2019s robustness and behavior is warranted. For example, there are many parameters in this system. In this study, we have found that the results are most sensitive to the size of |S| and to the decision of when to restart training the network from scratch \u2013 this happens when new samples are added to |S| and the network fails to accommodate them in learning (i.e. the error on the samples does not reduce). This may be because of the scaling issues mentioned in Section 4.2 and/or because the weights of the network may have grown too large. The beneficial effects of regularization may be especially pronounced in this application as networks are constantly being incrementally trained with changing data. Also, we have found a class of problems for which we have not observed a benefit of using the modeling (see Section 3.6). Are the problems too easy or too hard, or is an alternate representation needed?\nSecond, an alternative to using the network back-driving technique to generate candidate initialization points is to simply use the network as a proxy evaluation function for hillclimbing. In this approach, hillclimbing (NASH) is conducted directly on the model\u2019s output. After every perturbation, the new candidate-solution is passed through the network to measure it\u2019s estimated performance. This, unlike network back-driving, does not take advantage of the fact that the network is differentiable, and rather only uses it as a proxy for the real evaluation function. However, it may reveal parts of the search space that back-driving may not.\nThe third, and perhaps the most speculative, direction is to measure if there are transferable features that are learned between different instantiations of the same problem. For example, with respect to the triangle-covering problem presented in Section 3.6.2, once we have learned how to evaluate how well a set of triangles reproduces an image, is learning the evaluations for the next image easier? One can imagine that low level primitives of how to draw triangles, if they are indeed learned by the network, may be reusable. Even if there is little transference in the current implementation, exploring problem transference has enormous potential to make this system automatically more intelligent with time."}, {"heading": "Acknowledgments", "text": "The author would like to gratefully acknowledge Sergey Ioffe and Rif A. Saurous for their invaluable comments."}], "references": [{"title": "Population-based incremental learning. a method for integrating genetic search based function optimization and competitive learning", "author": ["S. Baluja"], "venue": "CMU-CS-94-163. Carnegie Mellon University, Dept. of Computer Science, Tech. Rep., 1994.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Topics in black-box combinatorial optimization", "author": ["A. Juels"], "venue": "University of California, Berkeley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "From recombination of genes to the estimation of distributions i. binary parameters", "author": ["H. M\u00fchlenbein", "G. Paass"], "venue": "International Conference on Parallel Problem Solving from Nature. Springer, 1996, pp. 178\u2013187.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "The compact genetic algorithm", "author": ["G.R. Harik", "F.G. Lobo", "D.E. Goldberg"], "venue": "IEEE transactions on evolutionary computation, vol. 3, no. 4, pp. 287\u2013297, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Genetic algorithms in search, optimization, and machine learning", "author": ["D.E. Goldberg"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1975}, {"title": "Genetic algorithms: a 30 year perspective", "author": ["K. De Jong"], "venue": "Perspectives on Adaptation in Natural and Artificial Systems, vol. 11, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Simulated crossover in genetic algorithms", "author": ["G. Syswerda"], "venue": "Foundations of Genetic Algorithms, vol. 2, pp. 239\u2013255, 1993.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning optimal discriminant functions through a cooperative game of automata", "author": ["M.A. Thathachar", "P.S. Sastry"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 17, no. 1, pp. 73\u201385, 1987.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Hill climbing with learning (an abstraction of genetic algorithm)", "author": ["V. Kvasnicka", "M. Pelik\u00e1n", "J. Pospichal"], "venue": "Neural Network World, 6. Citeseer, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Towards a theory of population based incremental learning", "author": ["M. Hohfeld", "G. Rudolph"], "venue": "Proceedings of the International Conference on Evolutionary Computation, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "A convergence proof for the population based incremental learning algorithm", "author": ["R. Rastegar", "A. Hariri", "M. Mazoochi"], "venue": "17th IEEE International Conference on Tools with Artificial Intelligence (ICTAI\u201905). IEEE, 2005, pp. 387\u2013391.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "The convergence behavior of the pbil algorithm: a preliminary approach", "author": ["C. Gonzalez", "J.A. Lozano", "P. Larra\u00f1aga"], "venue": "Artificial Neural Nets and Genetic Algorithms. Springer, 2001, pp. 228\u2013231.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Analyzing the population based incremental learning algorithm by means of discrete dynamical systems", "author": ["C. Gonz\u00e1lez", "J.A. Lozano", "P. Larranaga"], "venue": "Complex Systems, vol. 12, pp. 465\u2013479, 2000. 23", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Mimic: Finding optima by estimating probability densities", "author": ["J.S. De Bonet", "C.L. Isbell", "P. Viola"], "venue": "Advances in neural information processing systems, pp. 424\u2013430, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Fast probabilistic modeling for combinatorial optimization", "author": ["S. Baluja", "S. Davies"], "venue": "AAAI/IAAI. Madison, WI, USA, 1998, pp. 469\u2013476.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE transactions on Information Theory, vol. 14, no. 3, pp. 462\u2013467, 1968.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1968}, {"title": "Using optimal dependency-trees for combinatorial optimization", "author": ["S. Baluja", "S. Davies"], "venue": "International Conference on Machine Learning (ICML), 1997, pp. 30\u201338.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "A tutorial on learning with bayesian networks", "author": ["D. Heckerman"], "venue": "Innovations in Bayesian networks. Springer, 2008, pp. 33\u201382.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian networks", "author": ["J. Pearl", "S. Russell"], "venue": "Department of Statistics, UCLA, 2000.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Boa: The bayesian optimization algorithm", "author": ["M. Pelikan", "D.E. Goldberg", "E. Cant\u00fa-Paz"], "venue": "Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation-Volume 1. Morgan Kaufmann Publishers Inc., 1999, pp. 525\u2013532.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Bayesian optimization algorithm based on incremental model building", "author": ["J. Yao", "Y. Kong", "L. Yang"], "venue": "International Symposium on Intelligence Computation and Applications. Springer, 2015, pp. 202\u2013209.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Global optimization using bayesian networks", "author": ["R. Etxeberria", "P. Larranaga"], "venue": "Second Symposium on Artificial Intelligence (CIMAF-99). Habana, Cuba, 1999, pp. 332\u2013339.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning evaluation functions to improve optimization by local search", "author": ["J. Boyan", "A.W. Moore"], "venue": "Journal of Machine Learning Research, vol. 1, no. Nov, pp. 77\u2013112, 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "Inversion of multilayer nets", "author": ["A. Linden", "J. Kindermann"], "venue": "Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 425\u2013430.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "Texture synthesis using convolutional neural networks", "author": ["L. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 262\u2013270.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv preprint arXiv:1508.06576, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412.6980", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic hillclimbing as a baseline method for evaluating genetic algorithms", "author": ["A. Juels", "M. Wattenberg"], "venue": "Proceedings of the 1995 Conference on Neural Information Processing Systems (NIPS), vol. 8, 1996, p. 430.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1995}, {"title": "An empirical comparison of seven iterative and evolutionary function optimzation heuristics", "author": ["S. Baluja"], "venue": "Computer Science Department, Tech. Rep. CMU-CS-95-193, September 1995.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1995}, {"title": "When will a genetic algorithm outperform hill climbing", "author": ["M. Mitchell", "J.H. Holland", "S. Forrest"], "venue": "Advances in Neural Information Processing Systems 6, J. D. Cowan, G. Tesauro, and J. Alspector, Eds. Morgan-Kaufmann, 1994, pp. 51\u201358. [Online]. Available: http://papers.nips.cc/paper/ 836-when-will-a-genetic-algorithm-outperform-hill-climbing.pdf", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1994}, {"title": "Micro-auction based traffic light control: Responsive, local decision making", "author": ["M. Covell", "S. Baluja", "R. Sukthankar"], "venue": "IEEE Intelligent Transportation Systems Conference-2015, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "The bandwidth problem for graphs and matrices\u2014a survey", "author": ["P.Z. Chinn", "J. Chv\u00e1talov\u00e1", "A.K. Dewdney", "N.E. Gibbs"], "venue": "Journal of Graph Theory, vol. 6, no. 3, pp. 223\u2013254, 1982.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1982}, {"title": "Some applications of graph bandwidth to constraint satisfaction problems.", "author": ["R. Zabih"], "venue": "in AAAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["T. Raiko", "M. Berglund", "G. Alain", "L. Dinh"], "venue": "arXiv preprint arXiv:1406.2989, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science, vol. 220, no. 4598, pp. 671\u2013680, 1983.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1983}, {"title": "Tabu search-part i", "author": ["F. Glover"], "venue": "ORSA Journal on computing, vol. 1, no. 3, pp. 190\u2013206, 1989.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1989}, {"title": "Hill climbing beats genetic search on a boolean circuit synthesis problem of koza\u2019s", "author": ["K.J. Lang"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning, 1995, pp. 340\u2013343.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparing genetic algorithms, simulated annealing, and stochastic hillclimbing on timetabling problems", "author": ["P. Ross", "D. Corne"], "venue": "AISB Workshop on Evolutionary Computing. Springer, 1995, pp. 94\u2013102.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "When hillclimbers beat genetic algorithms in multimodal optimization", "author": ["F.G. Lobo", "M. Bazargani"], "venue": "Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation. ACM, 2015, pp. 1421\u20131422.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Genetic algorithms are not function optimizers", "author": ["K.A. De Jong"], "venue": "Foundations of genetic algorithms, vol. 2, pp. 5\u201317, 1993. 25", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "In the 1990s, a number of researchers [1] [2] [3] [4] independently started employing probabilistic models to guide heuristic stochastic based-search algorithms.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "In the 1990s, a number of researchers [1] [2] [3] [4] independently started employing probabilistic models to guide heuristic stochastic based-search algorithms.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "In the 1990s, a number of researchers [1] [2] [3] [4] independently started employing probabilistic models to guide heuristic stochastic based-search algorithms.", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "In the 1990s, a number of researchers [1] [2] [3] [4] independently started employing probabilistic models to guide heuristic stochastic based-search algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Probabilistic models for optimization were motivated with three goals: as a new optimization method, as a method to incorporate simple learning (Hebbian style probability updates) into hillclimbing, and as a method to explain how genetic algorithms [5] [6] [7] might work.", "startOffset": 249, "endOffset": 252}, {"referenceID": 5, "context": "Probabilistic models for optimization were motivated with three goals: as a new optimization method, as a method to incorporate simple learning (Hebbian style probability updates) into hillclimbing, and as a method to explain how genetic algorithms [5] [6] [7] might work.", "startOffset": 253, "endOffset": 256}, {"referenceID": 6, "context": "Probabilistic models for optimization were motivated with three goals: as a new optimization method, as a method to incorporate simple learning (Hebbian style probability updates) into hillclimbing, and as a method to explain how genetic algorithms [5] [6] [7] might work.", "startOffset": 257, "endOffset": 260}, {"referenceID": 7, "context": "Within the GA literature, one of the first attempts at using population level statistics was the \u201cBit-Based Simulated Crossover (BSC)\u201d operator [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "Extending the above idea and incorporating Hebbian learning, another early probabilistic optimization was the Population-Based Incremental Learning algorithm (PBIL) [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 8, "context": "PBIL is akin to a cooperative system of discrete learning automata in which the automata choose their actions independently, but all automata receive a common reinforcement dependent upon all their actions [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "A more theoretical analysis of PBIL can be found in [2][10][11][12][13][14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Mutual Information Maximization for Input Clustering (MIMIC) [15] was one of the first to do this.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "In [16], MIMIC\u2019s probabilistic model was extended to a larger class of dependency graphs: trees in which each variable is conditioned on at most one parent.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "As shown in 1968, this created the optimal tree-shaped network for a maximum-likelihood model of the data [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "The trend indicated that more accurate probabilistic models increased the probability of generating new candidate solutions in promising regions of the search space [18].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Bayesian networks are a popular method for efficiently representing dependencies [19] [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "Bayesian networks are a popular method for efficiently representing dependencies [19] [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Numerous researchers have taken the step of combining full Bayesian networks with stochastic search [21] [22] [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Numerous researchers have taken the step of combining full Bayesian networks with stochastic search [21] [22] [23].", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "Numerous researchers have taken the step of combining full Bayesian networks with stochastic search [21] [22] [23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "An alternate model building approach, termed STAGE, was presented in [25].", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "We also describe how the model is integrated with fast-search heuristics, following the work of [25] and [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "We also describe how the model is integrated with fast-search heuristics, following the work of [25] and [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": ") Although the architecture of the neural network used for modeling is manually specified, the actual dependencies that the network encodes need not be the same form for all parameters, nor are they deeply tied to architecture of the network [26].", "startOffset": 242, "endOffset": 246}, {"referenceID": 16, "context": "With models with dependencies, such as the dependency-trees [17], the sampling is simply conditioned on the variables on which the parameter is dependent (as specified in the tree-based model).", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "This method was first presented in [27] and has recently been popularized within the context of texture and style generation with neural networks [28] [29].", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "This method was first presented in [27] and has recently been popularized within the context of texture and style generation with neural networks [28] [29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "This method was first presented in [27] and has recently been popularized within the context of texture and style generation with neural networks [28] [29].", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": "As described in [27], the procedure addresses the following question: \u201cWhich input should be fed into the net to produce an output which approximates the given target vector T\u2019 (in our case the the target vector T is a simple scalar of 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "In general, modest learning-rates for the gradient descent algorithm were found to work best for the network-inversion process (we used the Adam Optimizer [30] with learning-rate = 0.", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "Two early works in probabilistic model-based optimization [16] [25] suggested the use of the probabilistic models as methods to initialize faster local-search optimization techniques.", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "Two early works in probabilistic model-based optimization [16] [25] suggested the use of the probabilistic models as methods to initialize faster local-search optimization techniques.", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "[31] [32] [33] [34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] [32] [33] [34].", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "[31] [32] [33] [34].", "startOffset": 10, "endOffset": 14}, {"referenceID": 32, "context": "[31] [32] [33] [34].", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "To keep S a constant size, after every NASH run, the last 1000 unique solutions from the NASH run are added to S, and S is pruned back by removing the members that have been in S the longest (this implements a simple first-in-first-out scheme), as was suggested in previous studies [16].", "startOffset": 282, "endOffset": 286}, {"referenceID": 23, "context": "Using problem-specific hand-crafted features, such as done with STAGE [25], may help.", "startOffset": 70, "endOffset": 74}, {"referenceID": 33, "context": "More details of the complexity of this problem can be found in [37].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "Interest in this problem stems from a variety of sources, including constraint satisfaction [38] and minimizing propagation delay in the layout of electronic cells.", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "The simplest encoding is to use 100 real-valued outputs, one for each node, and divide the [0,1] region into 16 evenly spaced regions, each assigned to a single letter.", "startOffset": 91, "endOffset": 96}, {"referenceID": 15, "context": "To ground the discussion, let\u2019s examine a simple two dimensional checkerboard problem [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 35, "context": "To do this, we use a technique similar to stochastic sigmoid units in training the neural network to model the search space (for a recent paper on this and related topics, see [40]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 36, "context": "It is obvious how alternative search heuristics such as simulated annealing [41] and TABU search [42] can easily be substituted as they are often used to search neighborhoods around a single point in a manner similar to NASH.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "It is obvious how alternative search heuristics such as simulated annealing [41] and TABU search [42] can easily be substituted as they are often used to search neighborhoods around a single point in a manner similar to NASH.", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 201, "endOffset": 205}, {"referenceID": 38, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 206, "endOffset": 210}, {"referenceID": 30, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 211, "endOffset": 215}, {"referenceID": 31, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 216, "endOffset": 220}, {"referenceID": 39, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 221, "endOffset": 225}, {"referenceID": 40, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 226, "endOffset": 230}, {"referenceID": 41, "context": "Although we will not delve into the general debate of the merits of these simple techniques with more sophisticated search techniques, as this has been an active area of discussion for several decades [31] [43] [32] [33] [44] [45] [46], it is interesting, to consider the use of the probabilistic models with very different search paradigms, such as genetic algorithms.", "startOffset": 231, "endOffset": 235}, {"referenceID": 4, "context": "The newly created solutions are further randomly perturbed (mutated) to reveal the \u2019children\u2019 solutions that are the candidate solutions to evaluate next [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 4, "context": "To learn more about GAs, please see [5].", "startOffset": 36, "endOffset": 39}], "year": 2017, "abstractText": "In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated.", "creator": "LaTeX with hyperref package"}}}