{"id": "1703.07476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Topic Identification for Speech without ASR", "abstract": "modern topic identification ( topic id ) systems for speech use automatic speech recognition ( asr ) signals to regularly produce speech transcripts, and perform supervised classification on such asr outputs. however, under resource - weighted limited conditions, the manually transcribed speech required to actively develop standard asr systems can be severely mentally limited or unavailable. in producing this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word - like or phoneme - like units, without depending on the supervised training of asr systems. moreover, using automatic phoneme - like tokenizations, we demonstrate that a convolutional neural network grammar based framework for learning spoken document representations provides competitive performance compared to a standard bag - of - sight words representation, as evidenced by comprehensive topic id evaluations run on both single - label and multi - label classification verification tasks.", "histories": [["v1", "Wed, 22 Mar 2017 00:37:33 GMT  (168kb,D)", "https://arxiv.org/abs/1703.07476v1", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v2", "Tue, 11 Jul 2017 17:11:15 GMT  (168kb,D)", "http://arxiv.org/abs/1703.07476v2", "5 pages, 2 figures; accepted for publication at Interspeech 2017"]], "COMMENTS": "5 pages, 2 figures, submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunxi liu", "jan trmal", "matthew wiesner", "craig harman", "sanjeev khudanpur"], "accepted": false, "id": "1703.07476"}, "pdf": {"name": "1703.07476.pdf", "metadata": {"source": "CRF", "title": "Topic Identification for Speech without ASR", "authors": ["Chunxi Liu", "Jan Trmal", "Matthew Wiesner", "Craig Harman", "Sanjeev Khudanpur"], "emails": ["khudanpur}@jhu.edu,", "craig@craigharman.net"], "sections": [{"heading": null, "text": "1. Introduction Topic identification (topic ID) on speech aims to identify the topic(s) for given speech recordings, referred to as spoken documents, where the topics are a predefined set of classes or labels. This task is typically formulated as a three-step process. First, speech is tokenized into words or phones by automatic speech recognition (ASR) systems [1], or by limited-vocabulary keyword spotting [2]. Second, standard text-based processing techniques are applied to the resulting tokenizations, and produce a vector representation for each spoken document, typically a bag-of-words multinomial representation, or a more compact vector given by probabilistic topic models [3, 4]. Finally, topic ID is performed on the spoken document representations by supervised training of classifiers, such as Bayesian classifiers and support vector machines (SVMs).\nHowever, in the first step, training the ASR system required for tokenization itself requires transcribed speech and pronunciations. In this paper, we focus on a difficult and realistic scenario where the speech corpus of a test language is annotated only with a minimal number of topic labels, i.e., no manual transcriptions or dictionaries for building an ASR system are available. We aim to exploit approaches that enable topic ID on speech without any knowledge of that language other than the topic annotations.\nIn this scenario, while previous work demonstrates that the cross-lingual phoneme recognizers can produce reasonable speech tokenizations [5, 6], the performance is highly dependent on the language and environmental condition (chan-\nThis work was partially supported by DARPA LORELEI Grant No \u00afHR0011-15-2-0024, NSF Grant No\n\u00af CRI-1513128, and IARPA Contract\nNo \u00af 2012-12050800010.\nnel, noise, etc.) mismatch between the training and test data. Therefore, we focus on unsupervised approaches that operate directly on the speech of interest. Raw acoustic featurebased unsupervised term discovery (UTD) is one such approach that aims to identify and cluster repeating word-like units across speech based around segmental dynamic time warping (DTW) [7, 8]. [9] shows that using the word-like units from UTD for spoken document classification can work well; however, the results in [9] are limited since the acoustic features on which UTD is performed are produced by acoustic models trained from the transcribed speech of its evaluation corpus. In this paper, we investigate UTD-based topic ID performance when UTD operates on language-independent speech representations extracted from multilingual bottleneck networks trained on languages other than the test language [10]. Another alternative to producing speech tokenizations without language dependency is the model-based approach, i.e., unsupervised learning of hidden Markov model (HMM) based phoneme-like units from untranscribed speech. We exploit the Variational Bayesian inference based acoustic unit discovery (AUD) framework in [11] that allows parallelized large-scale training. In topic ID tasks, such AUD-based systems have been shown to outperform other systems based on cross-lingual phoneme recognizers [6], and this paper aims to further investigate how the performance compares among UTD, AUD and ASR based systems.\nMoreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations. While UTD only identifies relatively long (0.5 \u2013 1 sec) repeated terms, AUD/ASR enables full-coverage segmentation of continuous speech into a sequence of units/words, and such a resulting temporal sequence enables another feature learning architecture based on convolutional neural networks (CNNs) [12]; instead of treating the sequential tokens as a bag of acoustic units or words, the whole token sequence is encoded as concatenated continuous vectors, and followed by convolution and temporal pooling operations that capture the local and global dependencies. Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16]. However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].\nFinally, incorporating the different tokenization and feature representation approaches noted above, we perform com-\nar X\niv :1\n70 3.\n07 47\n6v 2\n[ cs\n.C L\n] 1\n1 Ju\nl 2 01\n7\nprehensive topic ID evaluations on both single-label and multilabel spoken document classification tasks.\n2. Unsupervised tokenizations of speech"}, {"heading": "2.1. Unsupervised term discovery (UTD)", "text": "UTD aims to automatically identify and cluster repeated terms (e.g. words or phrases) from speech. To circumvent the exhaustive DTW-based search limited by O(n2) time [7], we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) [8], which permits search inO(n logn) time. We briefly describe the UTD procedures in ZRTools by four steps below, and full details can be found in [8].\n1. Construct the sparse approximate acoustic similarity matrices between pairs of speech utterances.\n2. Identify word repetitions via fast diagonal line search and segmental DTW.\n3. The resulting matches are used to construct an acoustic similarity graph, where nodes represent the matching acoustic segments and edges reflect DTW distances.\n4. Threshold the graph edges, and each connected component of the graph is a cluster of acoustic segments, which produces a corresponding term (word/phrase) category. Finally, the cluster of each discovered term category consists of a list of term occurrences.\nNote that in the third step above, the weight on each graph edge can be exact DTW-based similarity, or other similarity based on heuristics more than DTW distance. For example, we investigate an implementation in ZRTools, where a separate logistic regression model is used to rescore the similarity between identified matches by determining how likely the matching pair is the same underlying word/phrase and is not a filled pause (e.g. \u201cum-hum\u201d and \u201cyeah uh-huh\u201d in English). Filled pauses tend to be acoustically stationary with more phone repeats and thus would match throughout the acoustic similarity matrix, whereas a contentful word (without too many phone repeats) tend to concentrate around the main diagonal; thus, the features in logistic regression contain the numbers of matrix elements in diagonal bands in progressive steps away from the main diagonal. Feature weights are learned using a portion of transcribed speech with reference transcripts, and the resulting model can be used for language-independent rescoring."}, {"heading": "2.2. Acoustic unit discovery (AUD)", "text": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications. Specifically, a phone-loop model is formulated where each phoneme-like unit is modeled as an HMM with a Gaussian mixture model of output densities (GMM-HMM). Under the Dirichlet process framework, we consider the phone loop as an infinite mixture of GMM-HMMs, and the mixture weights are based on the stick-breaking construction of Dirichlet process. The infinite number of units in the mixture is truncated in practice, giving zero mixture weight to any unit beyond some large count. We treat such mixture of GMM-HMMs as a single unified HMM and thus the segmentation of the data is performed using standard forward-backward algorithm. Training is fully unsupervised and parallelized; after a fixed number of training iterations, we use Viterbi decoding algorithm to obtain acoustic unit tokenizations of the data."}, {"heading": "3. Learning document representations", "text": ""}, {"heading": "3.1. Bag-of-words representation", "text": "After we obtain the tokenizations of speech by either UTD or AUD, each spoken document is represented by a vector of unigram occurrence counts over discovered terms, or a vector of ngram counts over acoustic units, respectively. Each feature vector can be further scaled by inverse document frequency (IDF), producing a TF-IDF feature."}, {"heading": "3.2. Convolutional neural network-based representation", "text": "AUD enables full-coverage tokenization of continuous speech into a sequence of acoustic units, which we can exploit in a CNN-based framework to learn a vector representation for each spoken document. As shown in Figure 1, in an acoustic unit sequence a of lengthm, each unit ai, 1 \u2264 i \u2264 m, is encoded as a fixed dimensional continuous vector, and the whole sequence a is represented as a concatenated vector x. A shared convolutional feature transform T spans a fixed-sized n-gram window, n m, and slides over the whole sequence. Then the hidden feature layer h1 with nonlinearities consists of each feature vector h1i extracted from the shared convolutional window centered at each acoustic unit position i. Max-pooling is performed on top of each h1i , 1 \u2264 i \u2264 m, to obtain a fixed-dimensional vector representation for the whole sequence a, i.e., a vector representation of the whole spoken document, followed by another hidden layer h2 and a final output layer. Note that this framework needs supervision for training; e.g., the output layer can be a softmax function for single-label classification, and the whole model is trained with categorical cross-entropy loss.\nAlso, the vector representation of each unique acoustic unit can be randomly initialized, or pre-trained from other tasks. Specifically, we apply the skip-gram model of word2vec [22] to pre-train one embedding vector for each acoustic unit, based on the hierarchical softmax with Huffman codes."}, {"heading": "4. Supervised document classification", "text": ""}, {"heading": "4.1. Single-label classification", "text": "For the bag-of-words representation, we use a stochastic gradient descent (SGD) based linear SVM [20, 21] with hinge loss and L1/L2 norm regularization. For the CNN-based framework, we use a softmax function in the output layer for classification as described in Section 3.2."}, {"heading": "4.2. Multi-label classification", "text": "In the setting where each spoken document can be associated with multiple topics/labels, we proceed to perform a multi-label classification task. The baseline approach is the binary rele-\nvance method, which independently trains one binary classifier for each label, and the spoken document is evaluated by each classifier to determine if the respective label applies to it. Specifically, we use a set of SVMs (Section 4.1), one for each label, on the bag-of-words features.\nTo adapt the CNN-based framework for multi-label classification, we replace the softmax in the output layer with a set of sigmoid output nodes, one for each label, as shown in Figure 1. Since a sigmoid naturally provides output values between 0 and 1, we train the neural network (NN) to minimize the binary cross entropy loss defined as l(\u0398, (x, y)) = \u2212 \u2211K\nk=1(yk log ok + (1\u2212 yk) log(1\u2212 ok)), where \u0398 denotes the NN parameters, x is the feature vector of acoustic unit sequence, y is the target vector of labels, ok and yk are the output and the target for label k, and the number of unique labels isK.\n5. Experiments"}, {"heading": "5.1. Single-label classification", "text": ""}, {"heading": "5.1.1. Experimental setup", "text": "For our single-label topic classification experiments, we use the Switchboard Telephone Speech Corpus [23], a collection of two-sided telephone conversations. We use the same development (dev) and evaluation (eval) data sets as in [9, 10]. Each whole conversation has two sides and one single topic, and topic ID is performed on each individual-side speech (i.e., each side is seen as one single spoken document). In the 35.7 hour dev data, there are 360 conversation sides evenly distributed across six different topics (recycling, capital punishment, drug testing, family finance, job benefits, car buying), i.e., each topic has equal number of 60 sides. In the 61.6 hour eval data, there are another different six topics (family life, news media, public education, exercise/fitness, pets, taxes) evenly distributed across 600 conversation sides. Algorithm design choices are explored through experiments on dev data. We use manual segmentations provided by the Switchboard corpus to produce utterances with speech activity, and UTD and AUD are operating only on those utterances.\nFor UTD, we use the ZRTools [8] implementation with the default parameters except that, we use cosine similarity threshold \u03b4 = 0.5, and vary the diagonal median filter duration \u03ba over {0.6, 0.7}; we try both the exact DTW-based similarity and the rescored similarity as described in Section 2.1, and tune the similarity threshold (used to partition the graph edges) over {0.85, 0.88, 0.90, 0.92}. For AUD, the unsupervised training is performed only on the dev data (10 iterations); after training, we use the learned models to decode both dev and eval data set, and obtain the acoustic unit tokenizations. We use truncation level 200, which implies maximum 200 different acoustic units can be learned from the corpus. For each acoustic unit, we use a 3-state HMM with 2 Gaussians per state. For the stick-breaking construction of Dirichlet process, we vary the concentration parameter \u03b3 over {1.0, 10.0}, and other hyperparameters are the same as [11].\nThe acoustic features on which UTD and AUD operate are extracted using the same multilingual bottleneck (BN) network as described in [10] with Kaldi toolkit [24]. We conduct the multilingual BN training with 10 language collections (Assamese, Bengali, Cantonese, Haitian, Lao, Pashto, Tamil, Tagalog, Vietnamese and Zulu) \u2013 10 hours of transcribed speech per language. Complete specifications can be found in [10].\nFor SVM-based classification, we use the bag of discovered term unigrams, or bag of acoustic unit trigrams. On dev data, we\ntry using the features of raw counts or scaled by IDF, SVM regularization tuned overL1/L2 norm, regularization constant tuned over {0.001, 0.0001}, and SGD epochs tuned over {30, 50}. We further normalize each feature to L2 norm unit length. Each experiment is a run of 10-fold cross validation (CV) on the 360 conversation sides of dev data, or on the 600 sides of eval data, respectively. Note that our data size here is relatively small (only 360 or 600) and the SGD training may give high variance in the performance [25]. Therefore, to report classification accuracy for each configuration (when varying features or models), we repeat each CV experiment 5 times, where each experiment again is a run of 10-fold CV; then for each configuration, the mean and standard deviation of 5 experiments is reported.\nFor CNN-based classification, we use the same strategy to report classification accuracy, i.e., repeating experiments 5 times (where each time is a 10-fold CV) for each CNN configuration. Note that the respective 10 folds of both dev and eval data sets are fixed the same for all the SVM and CNN experiments. Additionally, for each 10-fold CV experiment, instead of training on 9 folds and testing on the remaining 1 fold as in SVM, we use 8 folds for CNN training, leave another 1 fold out as validation data; after training each CNN model for up to 100 epochs, the model with the best accuracy on the validation data is used for evaluation on the test set. The acoustic unit sequence (as CNN inputs) are zero-padded to the longest length in each dataset. We implemented the CNNs in Keras [26] with Theano [27] backend. CNN architectures are determined through experiments on dev data. For SGD training we use the Adadelta optimizer [28] and mini-batch size 18. The n-gram window size of each convolutional feature transform T is 7. The size of each hidden feature vector h1i (extracted from the transform T ) is 1024, with rectified linear unit (ReLU) nonlinearities. Thus, after max-pooling over time, we have a 1024- dimensional vector again, which then goes through another hidden layer h2 (also set as 1024-dimensional with ReLU) and finally into a softmax. Dropout [29] rate 0.2 is used at each layer.\nWhen we initialize the vector representation of each acoustic unit with a set of pre-trained vectors (instead of random initializations), we apply the skip-gram model of word2vec [22] to the acoustic unit tokenizations of each data set. We use the gensim implementation [30], which includes a vector space of embedding dimension 50 (tuned over {50, 80}), a skip-gram window of size 5, and SGD over 20 epochs."}, {"heading": "5.1.2. Results on Switchboard", "text": "Table 1 shows the topic ID results on Switchboard. For UTDbased classifications, we find that the default rescoring in ZRTools [8] which is designed to filter out the filled pauses produces comparable performance to the raw DTW similarity scores, but the rescoring can result in much faster connectedcomponent clustering (Section 2.1). Note that this rescoring model is estimated using a portion of transcribed Switchboard, but it is still a legitimate language-independent UTD approach while operating on languages other than English. While a diagonal median filter duration \u03ba of 0.6 or 0.7 gives similar results, \u03ba = 0.7 produces longer but fewer terms, giving more sparse feature representations. Therefore, we proceed with rescoring and \u03ba = 0.7 in the following UTD experiments (Section 5.2).\nFor AUD-based classifications, CNN without word2vec pre-training usually gives comparable results with SVM; however, using word2vec pre-training, CNN substantially outperforms the competing SVM in all cases. Also as the concentration parameter \u03b3 in AUD increases from 1.0 to 10.0 (yielding\nless concentrated distributions), we have more unique acoustic units in the tokenizations of both data sets, from 184 to 199, and \u03b3 = 10.0 usually produces better results than \u03b3 = 1.0."}, {"heading": "5.2. Multi-label classification", "text": ""}, {"heading": "5.2.1. Experimental setup", "text": "We further evaluate our topic ID performance on the speech corpora of three languages released by the DARPA LORELEI (Low Resource Languages for Emergent Incidents) Program. For each language there are a number of audio speech files, and each speech file is cut into segments of various lengths (up to 120 seconds). Each speech segment is seen as either in-domain or out-of-domain. In-domain data is defined as any speech segment relating to an incident or incidents, and in-domain data will fall into a set of domain-specific categories; these categories are known as situation types, or in-domain topics. There are 11 situation types: \u201cCivil Unrest or Wide-spread Crime\u201d, \u201cElections and Politics\u201d, \u201cEvacuation\u201d, \u201cFood Supply\u201d, \u201cUrgent Rescue\u201d, \u201cUtilities, Energy, or Sanitation\u201d, \u201cInfrastructure\u201d, \u201cMedical Assistance\u201d, \u201cShelter\u201d, \u201cTerrorism or other Extreme Violence\u201d, and \u201cWater Supply\u201d. We consider \u201cOut-ofdomain\u201d as the 12th topic label, so each speech segment either corresponds to one or multiple in-domain topics, or is \u201cOutof-domain\u201d. We use the average precision (AP, equal to the area under the precision-recall curve) as the evaluation metric, and report both the AP across the overall 12 labels, and the AP across 11 situation types, as shown in Table 2. For each configuration, only a single 10-fold CV result is reported, since we observe less variance in results here than in Switchboard. We have 16.5 hours in-domain data and 8.5 hours out-of-domain data for Turkish, 2.9 and 13.2 hours for Uzbek, and 7.7 and 7.2 hours for Mandarin. We use the same CNN architecture as on Switchboard but make the changes as described in Section 4.2. Also we use mini-batch size 30 and fix the training epochs as 100. All CNNs use word2vec pre-training. Additionally, we also implement another two separate topic ID baselines using the decoded word outputs from two supervised ASR systems, trained from 80 hours transcribed Babel Turkish speech [31] and about 170 hours transcribed HKUST Mandarin telephone speech (LDC2005T32 and LDC2005S15), respectively.\nTable 2: Multi-label topic ID average precision on LORELEI languages, with the number of speech segments in parentheses.\nDataset Feature Model Overall In-domain topics\nTurkish UTD SVM 0.583 0.531 AUD SVM 0.627 0.556 (2095) AUD CNN 0.641 0.564 ASR SVM 0.625 0.580\nUzbek UTD SVM 0.803 0.254 AUD SVM 0.791 0.203 (1416) AUD CNN 0.807 0.207\nMandarin UTD SVM 0.444 0.234 AUD SVM 0.436 0.220\n(724) AUD CNN 0.420 0.183 ASR SVM 0.461 0.261\nFigure 2: Average precision of in-domain situation types on Turkish when varying the number of folds used for training."}, {"heading": "5.2.2. Results on LORELEI datasets", "text": "As shown in Table 2, UTD-based SVMs are more competitive than AUD-based SVMs on the smaller corpora, i.e., Uzbek and Mandarin, while being less competitive on the larger corpus, Turkish. We further investigate this behavior on each individual language by varying the amount of training data; we split the data into 10 folds, and perform 10-fold CV 9 times, varying the number of folds for training from 1 to 9. As illustrated in Figure 2 for Turkish, as we use more folds for training, AUDbased system starts to be more competitive than UTD. Supervised ASR-based systems still give the best results in various cases, while UTD and AUD based systems give comparable performance.\nNote that CNN-based systems outperform SVMs on Turkish and Uzbek while losing on the smaller sized Mandarin, indicating more topic-labeled data is needed to enable competitive CNNs. This also indicates why CNNs on LORELEI corpora do not produce as large a gain over SVMs as on the larger sized Switchboard, since each 15-25 hour LORELEI corpus with 12 topic labels is a relatively small amount of data compared to the 35.7/61.6 hour Switchboard corpus with 6 labels."}, {"heading": "6. Concluding remarks", "text": "We have demonstrated that both UTD and AUD are viable technologies for producing effective tokenizations of speech that enable topic ID performance comparable to using standard ASR systems, while effectively removing the dependency on transcribed speech required by the ASR alternative. We find that when training data is severely limited the UTD-based classification is superior to AUD-based classification. As the amount of training data increases, performance improves across the board. Finally, with sufficient training data AUD-based CNNs with word2vec pre-training outperform AUD-based SVMs.\n7. References [1] T. J. Hazen, \u201cMCE Training Techniques for Topic Identification of\nSpoken Audio Documents,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 8, pp. 2451\u20132460, Nov 2011.\n[2] J. Wintrode and S. Khudanpur, \u201cLimited resource term detection for effective topic identification of speech,\u201d in Proc. ICASSP, 2014.\n[3] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent dirichlet allocation,\u201d Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.\n[4] C. May, F. Ferraro, A. McCree, J. Wintrode, D. Garcia-Romero, and B. Van Durme, \u201cTopic identification and discovery on text and speech,\u201d in Proc. EMNLP, 2015.\n[5] M.-h. Siu, H. Gish, A. Chan, W. Belfield, and S. Lowe, \u201cUnsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery,\u201d Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.\n[6] S. Kesiraju, R. Pappagari, L. Ondel, L. Burget, N. Dehak, S. Khudanpur, J. C\u030cernocky\u0300, and S. Gangashetty, \u201cTopic identification of spoken documents using unsupervised acoustic unit discovery,\u201d in Proc. ICASSP, 2017.\n[7] A. S. Park and J. R. Glass, \u201cUnsupervised pattern discovery in speech,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 186\u2013197, 2008.\n[8] A. Jansen and B. Van Durme, \u201cEfficient spoken term discovery using randomized algorithms,\u201d in Proc. ASRU, 2011, https: //github.com/arenjansen/ZRTools.\n[9] M. Dredze, A. Jansen, G. Coppersmith, and K. Church, \u201cNLP on spoken documents without ASR,\u201d in Proc. EMNLP, 2010.\n[10] C. Liu, J. Yang, M. Sun, S. Kesiraju, A. Rott, L. Ondel, P. Ghahremani, N. Dehak, L. Burget, and S. Khudanpur, \u201cAn empirical evaluation of zero resource acoustic unit discovery,\u201d in Proc. ICASSP, 2017.\n[11] L. Ondel, L. Burget, and J. C\u030cernocky\u0300, \u201cVariational inference for acoustic unit discovery,\u201d in Proc. SLTU, 2016.\n[12] R. Collobert and J. Weston, \u201cA unified architecture for natural language processing: Deep neural networks with multitask learning,\u201d in Proc. ICML, 2008.\n[13] P. Xu and R. Sarikaya, \u201cConvolutional neural network based triangular CRF for joint intent detection and slot filling,\u201d in Proc. ASRU, 2013.\n[14] C. Liu, P. Xu, and R. Sarikaya, \u201cDeep contextual language understanding in spoken dialogue systems.\u201d in Proc. INTERSPEECH, 2015.\n[15] Y. Kim, \u201cConvolutional neural networks for sentence classification,\u201d arXiv preprint arXiv:1408.5882, 2014.\n[16] X. Zhang, J. Zhao, and Y. LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d in Advances in neural information processing systems, 2015, pp. 649\u2013657.\n[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word representations in vector space,\u201d arXiv preprint arXiv:1301.3781, 2013.\n[18] J. Nam, J. Kim, E. L. Menc\u0131\u0301a, I. Gurevych, and J. Fu\u0308rnkranz, \u201cLarge-scale multi-label text classification - revisiting neural networks,\u201d in Proc. ECML-PKDD, 2014, pp. 437\u2013452.\n[19] C.-y. Lee and J. Glass, \u201cA nonparametric bayesian approach to acoustic model discovery,\u201d in Proc. ACL, 2012.\n[20] S. Shalev-Shwartz, Y. Singer, and N. Srebro, \u201cPegasos: Primal estimated sub-gradient solver for SVM,\u201d in Proc. ICML, 2007.\n[21] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.\n[22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d in Advances in neural information processing systems, 2013, pp. 3111\u20133119.\n[23] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Telephone speech corpus for research and development,\u201d in Proc. ICASSP, 1992.\n[24] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. ASRU, 2011.\n[25] Y. Zhang and B. Wallace, \u201cA sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification,\u201d arXiv preprint arXiv:1510.03820, 2015.\n[26] F. Chollet, \u201cKeras,\u201d https://github.com/fchollet/keras, 2015.\n[27] Theano Development Team, \u201cTheano: A Python framework for fast computation of mathematical expressions,\u201d arXiv e-prints, vol. abs/1605.02688, May 2016.\n[28] M. D. Zeiler, \u201cAdadelta: an adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012.\n[29] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv, 2012.\n[30] R. R\u030cehu\u030ar\u030cek and P. Sojka, \u201cSoftware Framework for Topic Modelling with Large Corpora,\u201d in Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 2010, pp. 45\u201350.\n[31] J. Trmal, G. Chen, D. Povey, S. Khudanpur, P. Ghahremani, X. Zhang, V. Manohar, C. Liu, A. Jansen, D. Klakow et al., \u201cA keyword search system using open source software,\u201d in Proc. SLT, 2014."}], "references": [{"title": "MCE Training Techniques for Topic Identification of Spoken Audio Documents", "author": ["T.J. Hazen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 8, pp. 2451\u20132460, Nov 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Limited resource term detection for effective topic identification of speech", "author": ["J. Wintrode", "S. Khudanpur"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Topic identification and discovery on text and speech", "author": ["C. May", "F. Ferraro", "A. McCree", "J. Wintrode", "D. Garcia-Romero", "B. Van Durme"], "venue": "Proc. EMNLP, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-h. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Topic identification of spoken documents using unsupervised acoustic unit discovery", "author": ["S. Kesiraju", "R. Pappagari", "L. Ondel", "L. Burget", "N. Dehak", "S. Khudanpur", "J. \u010cernock\u1ef3", "S. Gangashetty"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011, https: //github.com/arenjansen/ZRTools.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "NLP on spoken documents without ASR", "author": ["M. Dredze", "A. Jansen", "G. Coppersmith", "K. Church"], "venue": "Proc. EMNLP, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical evaluation of zero resource acoustic unit discovery", "author": ["C. Liu", "J. Yang", "M. Sun", "S. Kesiraju", "A. Rott", "L. Ondel", "P. Ghahremani", "N. Dehak", "L. Burget", "S. Khudanpur"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Variational inference for acoustic unit discovery", "author": ["L. Ondel", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. ICML, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep contextual language understanding in spoken dialogue systems.", "author": ["C. Liu", "P. Xu", "R. Sarikaya"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in neural information processing systems, 2015, pp. 649\u2013657.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale multi-label text classification - revisiting neural networks", "author": ["J. Nam", "J. Kim", "E.L. Menc\u0131\u0301a", "I. Gurevych", "J. F\u00fcrnkranz"], "venue": "Proc. ECML-PKDD, 2014, pp. 437\u2013452.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proc. ICML, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J. Godfrey", "E. Holliman", "J. McDaniel"], "venue": "Proc. ICASSP, 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B. Wallace"], "venue": "arXiv preprint arXiv:1510.03820, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 2010, pp. 45\u201350.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A keyword search system using open source software", "author": ["J. Trmal", "G. Chen", "D. Povey", "S. Khudanpur", "P. Ghahremani", "X. Zhang", "V. Manohar", "C. Liu", "A. Jansen", "D. Klakow"], "venue": "Proc. SLT, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "First, speech is tokenized into words or phones by automatic speech recognition (ASR) systems [1], or by limited-vocabulary keyword spotting [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "First, speech is tokenized into words or phones by automatic speech recognition (ASR) systems [1], or by limited-vocabulary keyword spotting [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "Second, standard text-based processing techniques are applied to the resulting tokenizations, and produce a vector representation for each spoken document, typically a bag-of-words multinomial representation, or a more compact vector given by probabilistic topic models [3, 4].", "startOffset": 270, "endOffset": 276}, {"referenceID": 3, "context": "Second, standard text-based processing techniques are applied to the resulting tokenizations, and produce a vector representation for each spoken document, typically a bag-of-words multinomial representation, or a more compact vector given by probabilistic topic models [3, 4].", "startOffset": 270, "endOffset": 276}, {"referenceID": 4, "context": "In this scenario, while previous work demonstrates that the cross-lingual phoneme recognizers can produce reasonable speech tokenizations [5, 6], the performance is highly dependent on the language and environmental condition (chan-", "startOffset": 138, "endOffset": 144}, {"referenceID": 5, "context": "In this scenario, while previous work demonstrates that the cross-lingual phoneme recognizers can produce reasonable speech tokenizations [5, 6], the performance is highly dependent on the language and environmental condition (chan-", "startOffset": 138, "endOffset": 144}, {"referenceID": 6, "context": "Raw acoustic featurebased unsupervised term discovery (UTD) is one such approach that aims to identify and cluster repeating word-like units across speech based around segmental dynamic time warping (DTW) [7, 8].", "startOffset": 205, "endOffset": 211}, {"referenceID": 7, "context": "Raw acoustic featurebased unsupervised term discovery (UTD) is one such approach that aims to identify and cluster repeating word-like units across speech based around segmental dynamic time warping (DTW) [7, 8].", "startOffset": 205, "endOffset": 211}, {"referenceID": 8, "context": "[9] shows that using the word-like units from UTD for spoken document classification can work well; however, the results in [9] are limited since the acoustic features on which UTD is performed are produced by acoustic models trained from the transcribed speech of its evaluation corpus.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] shows that using the word-like units from UTD for spoken document classification can work well; however, the results in [9] are limited since the acoustic features on which UTD is performed are produced by acoustic models trained from the transcribed speech of its evaluation corpus.", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "In this paper, we investigate UTD-based topic ID performance when UTD operates on language-independent speech representations extracted from multilingual bottleneck networks trained on languages other than the test language [10].", "startOffset": 224, "endOffset": 228}, {"referenceID": 10, "context": "We exploit the Variational Bayesian inference based acoustic unit discovery (AUD) framework in [11] that allows parallelized large-scale training.", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "In topic ID tasks, such AUD-based systems have been shown to outperform other systems based on cross-lingual phoneme recognizers [6], and this paper aims to further investigate how the performance compares among UTD, AUD and ASR based systems.", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 1, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 5, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 8, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 9, "context": "Moreover, after the speech is tokenized, these works [1, 2, 5, 6, 9, 10] are limited to using bag-of-words features as spoken document representations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 11, "context": "5 \u2013 1 sec) repeated terms, AUD/ASR enables full-coverage segmentation of continuous speech into a sequence of units/words, and such a resulting temporal sequence enables another feature learning architecture based on convolutional neural networks (CNNs) [12]; instead of treating the sequential tokens as a bag of acoustic units or words, the whole token sequence is encoded as concatenated continuous vectors, and followed by convolution and temporal pooling operations that capture the local and global dependencies.", "startOffset": 254, "endOffset": 258}, {"referenceID": 12, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 139, "endOffset": 147}, {"referenceID": 14, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 172, "endOffset": 180}, {"referenceID": 15, "context": "Such continuous space feature extraction frameworks have been used in various language processing tasks like spoken language understanding [13, 14] and text classification [15, 16].", "startOffset": 172, "endOffset": 180}, {"referenceID": 16, "context": "However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].", "startOffset": 556, "endOffset": 564}, {"referenceID": 15, "context": "However, three questions are worth investigating in our AUD-based setting: (i) if such a CNN-based framework can perform as well on noisy automatically discovered phoneme-like units as on orthographic words/characters, (ii) if pre-trained vectors of phonemelike units from word2vec [17] provide superior performance to random initialization as evidenced by the word-based tasks, and (iii) if CNNs are still competitive in low-resource settings of hundreds to two-thousand training exemplars, rather than the large/medium sized datasets as in previous work [15, 16].", "startOffset": 556, "endOffset": 564}, {"referenceID": 6, "context": "To circumvent the exhaustive DTW-based search limited by O(n) time [7], we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) [8], which permits search inO(n logn) time.", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "To circumvent the exhaustive DTW-based search limited by O(n) time [7], we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) [8], which permits search inO(n logn) time.", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "We briefly describe the UTD procedures in ZRTools by four steps below, and full details can be found in [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications.", "startOffset": 139, "endOffset": 142}, {"referenceID": 18, "context": "We exploit the nonparametric Bayesian AUD framework in [11] based on variational inference, rather than the maximum likelihood training in [5] which may oversimplify the parameter estimations, nor the Gibbs Sampling training in [19] which is not amenable to large scale applications.", "startOffset": 228, "endOffset": 232}, {"referenceID": 21, "context": "Specifically, we apply the skip-gram model of word2vec [22] to pre-train one embedding vector for each acoustic unit, based on the hierarchical softmax with Huffman codes.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "For the bag-of-words representation, we use a stochastic gradient descent (SGD) based linear SVM [20, 21] with hinge loss and L/L norm regularization.", "startOffset": 97, "endOffset": 105}, {"referenceID": 20, "context": "For the bag-of-words representation, we use a stochastic gradient descent (SGD) based linear SVM [20, 21] with hinge loss and L/L norm regularization.", "startOffset": 97, "endOffset": 105}, {"referenceID": 22, "context": "For our single-label topic classification experiments, we use the Switchboard Telephone Speech Corpus [23], a collection of two-sided telephone conversations.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "We use the same development (dev) and evaluation (eval) data sets as in [9, 10].", "startOffset": 72, "endOffset": 79}, {"referenceID": 9, "context": "We use the same development (dev) and evaluation (eval) data sets as in [9, 10].", "startOffset": 72, "endOffset": 79}, {"referenceID": 7, "context": "For UTD, we use the ZRTools [8] implementation with the default parameters except that, we use cosine similarity threshold \u03b4 = 0.", "startOffset": 28, "endOffset": 31}, {"referenceID": 10, "context": "0}, and other hyperparameters are the same as [11].", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "The acoustic features on which UTD and AUD operate are extracted using the same multilingual bottleneck (BN) network as described in [10] with Kaldi toolkit [24].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "The acoustic features on which UTD and AUD operate are extracted using the same multilingual bottleneck (BN) network as described in [10] with Kaldi toolkit [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Complete specifications can be found in [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Note that our data size here is relatively small (only 360 or 600) and the SGD training may give high variance in the performance [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "We implemented the CNNs in Keras [26] with Theano [27] backend.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "We implemented the CNNs in Keras [26] with Theano [27] backend.", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "For SGD training we use the Adadelta optimizer [28] and mini-batch size 18.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Dropout [29] rate 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "When we initialize the vector representation of each acoustic unit with a set of pre-trained vectors (instead of random initializations), we apply the skip-gram model of word2vec [22] to the acoustic unit tokenizations of each data set.", "startOffset": 179, "endOffset": 183}, {"referenceID": 29, "context": "We use the gensim implementation [30], which includes a vector space of embedding dimension 50 (tuned over {50, 80}), a skip-gram window of size 5, and SGD over 20 epochs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "For UTDbased classifications, we find that the default rescoring in ZRTools [8] which is designed to filter out the filled pauses produces comparable performance to the raw DTW similarity scores, but the rescoring can result in much faster connectedcomponent clustering (Section 2.", "startOffset": 76, "endOffset": 79}, {"referenceID": 30, "context": "Additionally, we also implement another two separate topic ID baselines using the decoded word outputs from two supervised ASR systems, trained from 80 hours transcribed Babel Turkish speech [31] and about 170 hours transcribed HKUST Mandarin telephone speech (LDC2005T32 and LDC2005S15), respectively.", "startOffset": 191, "endOffset": 195}], "year": 2017, "abstractText": "Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.", "creator": "LaTeX with hyperref package"}}}