{"id": "1703.07726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "\\$1 Today or \\$2 Tomorrow? The Answer is in Your Facebook Likes", "abstract": "in economics and psychology, delay discounting phenomenon is often used to characterize how individuals choose between a smaller immediate reward and a larger delayed reward. people with higher anticipating delay discounting rate ( ddr ) often choose smaller but more immediate rewards ( a \" today person \" ). in contrast, thus people with a lower discounting rate can often choose a very larger future anticipated rewards ( a \" tomorrow person \" ). since the ability to modulate the desire of immediate gratification for increased long term rewards plays an important role in our decision - influenced making, the aforementioned lower wait discounting rate often predicts better social, academic and health outcomes. in economic contrast, the substantially higher discounting rate is often associated with problematic behaviors such as alcohol / drug abuse, pathological identity gambling and serious credit card default. thus, research relying on understanding and moderating delay discounting has assessed the potential to produce substantial societal marginal benefits.", "histories": [["v1", "Wed, 22 Mar 2017 16:16:09 GMT  (348kb,D)", "https://arxiv.org/abs/1703.07726v1", null], ["v2", "Thu, 23 Mar 2017 05:27:59 GMT  (348kb,D)", "http://arxiv.org/abs/1703.07726v2", null], ["v3", "Fri, 24 Mar 2017 01:00:03 GMT  (348kb,D)", "http://arxiv.org/abs/1703.07726v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.SI", "authors": ["tao ding", "warren k bickel", "shimei pan"], "accepted": false, "id": "1703.07726"}, "pdf": {"name": "1703.07726.pdf", "metadata": {"source": "CRF", "title": "$1 Today or $2 Tomorrow? The Answer is in Your Facebook Likes", "authors": ["Tao Ding", "Warren K. Bickel", "Shimei Pan"], "emails": ["taoding01@umbc.edu", "wkbickel@vtc.vt.edu", "shimei@umbc.edu"], "sections": [{"heading": "1 Introduction", "text": "In economics and psychology, delay discounting is often used to characterize how individuals choose between a smaller immediate reward and a larger delayed reward [Bickel and Marsch, 2001]. People with higher delay discounting rate (DDR) often choose smaller but more immediate rewards (a \u201ctoday person\u201d). In contrast, people with a lower discounting rate often choose a larger future rewards (a \u201ctomorrow person\u201d). Since the ability to modulate the desire of immediate gratification for long term rewards plays an important role in our decision-making, lower discounting rate often predicts better social, academic and health outcomes [Mahalingam et al., 1992]. In contrast, higher discounting rate is often associated with problematic behaviors such as alcohol/drug abuse, pathological gambling and credit card default [Alessi and Petry, 2003; Kirby et al., 1999]. Thus, research on understanding and moderating delay discounting has the potential to produce substantial societal benefits [Bickel et al., 2011].\nRecently, social media analytics has become a powerful tool for studying human traits and behaviors. With the help of novel data mining and machine learning techniques, largescale social media analytics has generated deep insight into latent human traits and behaviors.\nContinuing this trend, in this study we apply large-scale social media analytics to study the relationship between one\u2019s\nsocial media behavior (e.g., Likes) and DDR. Likes are used by social media users (e.g., Facebook, Twitter and Instagram users) to express positive sentiment toward various targets such as products, movies, books, expressions, websites and people. Given the large variety of entities that can be liked (called \u201clike entities (LE)\u201d) and the large number of users, social media Likes represent one of the most generic digital footprints available today [Youyou et al., 2015]. Previous studies have demonstrated that social media Likes speak volumes about who we are. In addition to directly signaling interests and preferences, social media Likes are indicative of ethnicity, intelligence, and use of addictive substances [Kosinski et al., 2013].\nThe main contributions of this work include 1. This is the first large-scale study (e.g., we have tens of\nmillions of social media users in our dataset) that systematically investigates the relationship between a user\u2019s social media behavior and DDR. Our research results can shed new light on human delay discounting behavior and its role in human decision-making.\n2. We explore a comprehensive set of state-of-the-art unsupervised feature learning methods which can take advantage of a large amount of unannotated social media data. This is important since annotated ground truth DDR is expensive to obtain on a large scale.\n3. We build prediction models to predict DDR from Facebook Likes. We also evaluate the effectiveness of different feature learning methods in predicting DDR.\n4. Our study has revealed interesting patterns of the relationship between social media Likes and DDR."}, {"heading": "2 Related Work", "text": "There is a rich body of research in economics and behavior science that investigates the relationship between DDR and real-world human behaviors such as drug abuse [Bickel and Marsch, 2001], an obese [Weller et al., 2008], pathological gambling [Alessi and Petry, 2003], drinking [Field et al., 2007], smoking [Bickel et al., 1999] , addiction to the internet [Saville et al., 2010] and credit card default [Angeletos et al., 2001]. Such a study often involves a small number of participants (e.g. a few dozens or a few hundreds). Experiment data are often obtained using questionnaires, surveys or interviews. Statistical analysis such as correlation or regression analysis are often employed to study the relationship between different test variables (e.g., DDR and smoking habits).\nar X\niv :1\n70 3.\n07 72\n6v 3\n[ cs\n.A I]\n2 4\nM ar\n2 01\n7\nRecently, there is a surge of interests in using big data and social media analytics to study human behavior on a large scale. For example, social media analytics has been used to analyze and predict emotions [De Choudhury et al., 2013], personality [Youyou et al., 2015] and political learning [Kosinski et al., 2013]. Continuing this trend, in this work, we employ social media analytics to study the relationship between DDR and social media behavior. Our work is the first that focuses on applying social media analytics to DDR, a socially important behavior measure. Many of the existing systems rely on supervised machine learning and human-engineered features [Golbeck et al., 2011] . Since the trait and behavior ground truth are usually obtained via sophisticated psychometric tests, the number of training examples with behavior ground truth is often limited (e.g., a few thousand instances). With a small number of training data and a large number of features, the prediction models can overfit the training data easily. Thus it is important to take advantage of the availability of a large amount of unsupervised social media data. In this study, we have explored not only traditional dimension reduction techniques (e.g., SVD and LDA) but also a new generation of neural network-based feature learning methods to capture the structures in unsupervised social media data."}, {"heading": "3 Dataset", "text": "Our dataset was obtained by the myPersonality project between 2007 and 2012 [Kosinski et al., 2015]. myPersonality was a popular Facebook application that offered to its users psychometric tests and feedback on their scores. The data were gathered with an explicit opt-in consent for reuse for research purposes. To protect privacy, the data was also anonymized.\nOur current dataset includes the Facebook Likes of 11 million+ users. Overall, there are 9.9 million unique \u201clike entities (LE)\u201d and 1.8 billion user-like pairs. The average Likes per user is 161 and the average Likes per LE is 182. Figure (1a) and (1b) show the distributions of the number of Likes per user and the number of Likes received per LE. They both roughly follow a power law distribution.\nAmong the 11 million Facebook users with Likes, 6,330 have the DDR ground truth. In the following, we explain how the ground truth DDR was obtained."}, {"heading": "4 Ground Truth DDR", "text": "To measure DDR, previously different mathematical models such as an exponential and a hyperbolic discounting function were proposed to capture the relationship between discounting amount and length of delay to reward. So far, since a hyperbolic function captures the phenomenon that a person\u2019s discount rate typically decreases as the delay to the reward increases, it matches observed human discounting behavior better [Stillwell and Tunney, 2012]. The hyperbolic discount function is typically expressed as: V = A/(1+kD), whereA is the magnitude of a delayed reward, V is the current subjective value of that reward, and D is the delay to the delivery of the reward. The k parameter, often called the delay discount rate (DDR), varies with the steepness of an individual\u2019s discounting, with higher k indicating that delayed rewards lose their value more quickly than smaller k values.\nTo obtain delay discounting ground truth, users of the myPerosnality Facebook App were asked to complete a multi-item delay discounting questionnaire [Stillwell and Tunney, 2012]. Each user was presented 15 different immediate monetary rewards (e.g., $1000, $950, $900 ...). The user needs to choose between an immediate rewards and a delayed future reward. The delays were between 1 week and 5 years. The delayed future monetary rewards were $1000 for all delays, and $100 for 1 month. To compute a participant\u2019s hyperbolic DDR parameter k, first, an \u201cindifference value\u201d is calculated for each delay (\u201cindifference value\u201d is obtained when a user switches from a smaller immediate reward to a larger future rewards). Based on the hyperbolic function, k can be calculated as: k = (A \u2212 V )/V D where A is the delayed reward, which is $1000 or $100 in our example, D is the delay to the reward and V is the indifference value for the delay. Since the distribution of k is often found to be non-normal, the data were approximately normalized using the natural-log transformation [Stillwell and Tunney, 2012]. The final DDR of a person is the average of log10k over all the delays and all the future rewards."}, {"heading": "5 Individual User Likes and DDR", "text": "To understand the relationship between Facebook Likes and DDR, first we conducted a correlation analysis between each LE and DDR. Since we have a large number of unique LEs in our dataset, it is difficult to conduct a formal regression anal-\nysis. Instead, we use Pearson\u2019s correlation analysis to identify LEs that have significant correlation with DDR. For this study, we used the dataset with 6330 people who have both the Facebook Likes and their DDR ground truth. Our analysis has identified 2522 LEs that are significantly correlated with DDR (p < 0.05).\nTable 1 shows the Top 10 most positively/negatively correlated LEs. For those favored more by a \u201ctoday person\u201d, hip hop music/musicians dominate the list (50% of them). Adult animated sitcoms also have significant presence (20% of them). In contrast, LEs liked by a \u201ctomorrow person\u201d are more diverse, ranging from nerd comic sites (\u201cxkcd\u201d), fantasy novels (\u201cLord of the Rings\u201d), Sci-fi novels (\u201cEnders Game\u201d), indie rock band (\u201cBest Coast\u201d) to news media (\u201cNPR\u201d) and special field of study (\u201cethics\u201d).\nInterestingly, in behavior science, DDR is studied mostly in the context of addiction. Based on our results, although a fondness of LEs with direct links to addiction such as \u201cweed\u201d (cor = 0.03, p < 0.02), smoking (cor = 0.028, p < 0.03) and \u201cdrinking\u201d (cor = 0.05, p < 0.0001) are positively correlated with DDR, it seems that a person\u2019s taste of musics, books and TV shows may reveal more about his/her DDR."}, {"heading": "6 User Like Embedding (ULE)", "text": "Since the ground truth DDR can only be obtained via sophisticated psychometric tests [Stillwell and Tunney, 2012; Kirby et al., 1999], it is expensive to acquire DDR on a large scale. As a result, the number of training instances for supervised machine learning is often limited. On the other hand, the amount of unsupervised social media data is huge. To avoid model overfitting, we first use unsupervised feature\nlearning to learn structures in user Likes based only on unsupervised data . We call this process \u201cuser like embedding\u201d (ULE) since we transform user Likes from a high dimensional sparse vector space into a lower-dimensional dense embedding space. We also filter users and LEs with a small number of Likes. The threshold for user Likes is 50 and LE Likes is 800. After the filtering, our dataset contains 5,138,857 users with Likes, 253,980 unique LEs and 3508 people with both Likes and DDR. In total, we have studied eight methods."}, {"heading": "6.1 Feature Learning Methods", "text": "Singular Value Decomposition (SVD) is a mathematical technique that is frequently used for dimension reduction [De Lathauwer et al., 2000]. Given any m \u2217 n matrix A, the algorithm will find matrices U , V and W such that A = UWV T . Here U is an orthonormal m \u2217 n matrix, W is a diagonal n\u2217n metric and V is an orthonormal n\u2217n matrix. Dimensionality reduction is done by computing R = U \u2217Wr where Wr neglects all but the r largest singular values in the diagonal matrix W . In our study, A is a user-entity matrix wherem is the number of users and n is the number of unique LEs. Aij = 1 if useri likes LEj . Otherwise, it is 0.\nLatent Dirichlet Allocation (LDA) is a generative graphical model that allows sets of observations to be explained by unobserved latent groups [Blei et al., 2003]. In natural language processing, LDA is frequently used to learn a set of topics from a large number of documents. To apply LDA to our data, each individual LE is treated as a word token and all the LEs liked by the same person form a document. For each user, LDA outputs a multinomial distribution over a set of latent \u201cLike Topics\u201d. For example, a \u201cLike Topic\u201d about \u201chip hop music\u201d may include famous hip hop songs and mu-\nsicians. Autoencoder (AE) is a neural network-based method for self-taught learning [Hinton and Salakhutdinov, 2006]. It tries to learn an identity function so that the output is as close to the input as possible. Although an identity function seems a trivial function to learn, by placing additional constraints (e.g,, to make the number of neurons in the hidden layer much smaller than that of the input), we can still uncover structures in the data. Architecturally, the AE we used has one input layer, one hidden layer and one output layer. For each user, we construct a training instance (X,Y ) where the input vector X and output vector Y are the same. The size of X and Y is the total number of unique LEs in our dataset. Xi and Yi equal to 1 if the user likes LEi. Otherwise they are 0.\nULE with Continuous Bag of Word Model (U-CBOW) Continuous Bag of Word (CBOW) model is a neural networkbased method originally designed to learn dense vector representations for words [Mikolov et al., 2013]. The intuition behind the model is the Distributional Hypothesis, which states words that appear in the same context share semantic meanings. The model is traditionally trained to maximize the probability of predicting a target word given one or more context words. The model is frequently trained using either a hierarchical softmax function (HS) or negative sampling (NS) [Mikolov et al., 2013].\nTo apply CBOW to learn ULE, we treat each Facebook LE as a word token and all the LEs liked by the same person as a document. Given all the Likes from all the users, CBOW outputs a dense vector representation for each \u201cLike\u201d. Since each user may have multiple Likes, the final representation of ULE is the \u201caverage\u201d of all the individual like vectors. We have experimented with both HS and NS for inference and NS with a negative sampling rate of 10 consistently out-performs HS. Thus, in this paper, we only report the NS results.\nULE with Skipgram Model (U-SG) SG is similar to CBOW. Algorithmically, CBOW predicts a target word from one or more context words, while the SG does the inverse and predicts one or more context words from the target word. Similarly, to obtain ULE, we use \u201caverage\u201d to aggregate all the individual like vectors from the same user.\nParagraph Vector with Distributed Memory (P-DM) Given a sequence of tokens, P-DM can simultaneously learn a vector representation for each individual token and a vector for the entire sequence. P-DM was also originally designed for natural language processing [Le and Mikolov, 2014]. In P-DM, each sequence of words (e.g. a paragraph) is mapped\nto a sequence vector (e.g., paragraph vector) and each word is mapped to a unique word vector. The paragraph vector and one or more word vectors are aggregated to predict a target word in the context. In our study, given all the Likes of a user, P-DM learns a vector representation for each Like as well as a sequence vector for all the Likes. We use the sequence vector as the ULE in our study.\nParagraph Vector with Distributed Bag of Words (PDBOW) P-DBOW learns a global sequence vector to predict tokens randomly sampled from a sequence. Unlike P-DM, PDBOW only learns a vector for the entire sequence (e.g., an entire paragraph). It does not learn vectors for individual tokens (e.g., words). Neither does it use a local context window since the words for prediction are randomly sampled from the entire sequence. We use the sequence vector as the ULE in our study.\nULE with GloVe (U-GloVe) GloVe is an unsupervised learning algorithm originally designed to learn vector representations of words based on aggregated global word-word co-occurrence statistics from a text corpus [Pennington et al., 2014]. GloVe employs a global log bilinear regression model that combines the advantages of global matrix factorization with that of local context window-based methods. Since GloVe only outputs a vector for each token (e.g, a word or a \u201cLike\u201d), to obtain ULE, we also use \u201caverage\u201d to aggregate all the vectors of individual Likes from the same person.\nTo compare these methods, in terms of inference methods, some employ prediction-based technologies (e.g., CBOW, SG, P-DM, P-DBOW and AE) , others use count-based methods (e.g., SVD, LDA and GloVe). There are some empirical evidence indicating that prediction-based methods may have some advantage over count-based methods in feature learning [Baroni et al., 2014]. Moreover, to learn a ULE, some employ a two-stage process where they first learn a feature vector for individual Likes and then aggregate all the individual like vectors from the same user to form a ULE (e.g., UCBOW, U-SG, U-GloVe), while others directly learn a feature representation for all the Likes of a user (e.g., SVD, LDA, AE, P-DM, and P-DBOW). Since the 2-stage systems employ a simple vector aggregation function (e.g. \u201caverage\u201d), it may not be ideal because the feature vectors of different Likes maybe co-related. In addition, since some were originally designed for natural language processing, they often employ a local context window to account for the fact that words close to each other are more semantically relevant (e.g., U-CBOW, U-SG, U-GloVe and P-DM). For example, in U-CBOW, the\nalgorithm is designed to optimize its capability to predict a target word based on its neighboring words in the context window. In GloVe, the word-word occurrence is weighted by 1/d where d is the distance between two words. However, since Facebook Likes do not have timestamps in our dataset, the notion of \u201clocal context widow\u201d does not apply. Thus, algorithms that use local context window may not be able to fully capture the relations between all the Likes of a user. Finally, other than LDA, the meaning of the learned features is difficult to decode. Table 2 summarizes the differences between these methods."}, {"heading": "6.2 Experiment Settings", "text": "Except for AE and GloVe, we used the implementations in the Gensim Python library 1 to run the experiments. For AE, since a GPU implementation is more efficient than a CPU implementation, we used the Python Keras library with a Theano backend. For GloVe, we used the original C implementation by Pennington 2. Other than AE, which was run on a GPU machine with 4 Quadro M6000 GPU, 1 Intel Core i75930K CPU @ 3.50GHz and 123G RAM, all the other algorithms were run on a CPU machine with 40 Intel(R) Xeon(R) CPU E5-2630 @ 2.20GHz and 264G RAM.\nIn our experiments, we systematically varied the size of the output feature vectors (i.e., 50, 100, 300, and 500 dimensions). For models that used a local context window, we set the window size to 20. In terms of algorithm-specific parameters, for LDA, we used the default settings for the hyper parameters \u03b1 and \u03b2 (i.e. 1/k where k is the number of topics). For AE, both the epoch and batch size were set to 50. For CBOW, SG, and P-DM where individual like vectors were trained using NS, the negative sampling rate was set to 10. For GloVe, we used symmetric co-occurrence update and we set the iteration number to 50."}, {"heading": "6.3 Results: Correlation Analysis", "text": "To understand what has been uncovered during feature learning, we performed a preliminary analysis on the learned fea-\n1https://github.com/RaRe-Technologies/gensim 2https://github.com/hans/glove.py\ntures and their relations to DDR. Since only the meaning of the features learned by LDA can be understood intuitively, in the following, we focus on analyzing the \u201cLike Topics\u201d learned by LDA. In this analysis, the topic number was set to 500.\nAmong the 500 \u201cLike topics\u201d, 130 of them are significantly correlated with DDR based on Pearson\u2019s correlation analysis (p < 0.05). Table 3 shows a sample of the most positively and most negatively correlated \u201dLike Topics\u201d. Among the most positively related, Topic 141 and 369 contain the names of famous rappers such as 2Pac, Wiz Khalifa, Lil Wayne, Eminem and Jay-Z. Topic 431 includes mostly R&B musicians. Topic 430 contains various expressions/statements. Among the most negatively correlated \u201cLike topics\u201d, topic 159 is mostly about fantasy novels/movies (e.g., the Lord of the Rings). Topic 481 is about media companies (e.g., NPR), satire news (e.g., \u201cThe Daily Show\u201d, \u201cThe Colbert Report\u201d and \u201cthe Onion\u201d) and public figures (\u201cBarak Obama\u201d). Topic 405 includes mostly entertainers (e.g., George Takei) and topic 494 includes programs that focus on environmental and social issues (e.g., BBC earth)."}, {"heading": "7 Inferring Delay Discounting", "text": "We have implemented different prediction models to infer DDR from Facebook Likes. The experiments are designed to answer the following questions: (a) Is there any benefit of performing unsupervised feature learning? (b)Among the feature learning methods we have explored, which one is most effective in predicting DDR? (c) What is the impact of data and feature size on prediction? To answer (a), we compare the performance of the models that use the leaned ULE features with baseline systems that do not use feature learning. The baselines use supervised learning to directly predict DDR from raw data (i.e., individual Likes). To answer (b) we compare the prediction performance using features learned by the eight feature learning algorithms described above. To answer (c) we vary the number of users and the number of predicting features in the training data to demonstrate their impact on prediction performance. In addition, in all our models (including both the baseline and the systems using ULE), we\nadded two more statistical features about Likes: (1) the total number of Likes per user (2) the average Likes received by the LEs liked by a user.\nSince DDR is a continuous variable, we employ two Machine Learning methods that output numerical values: (1) Linear Regression with Lasso (LR-L) [Tibshirani, 1996] (b) Support Vector Regression (SVR) [Drucker et al., 1997]. LRL is a regression method that automatically performs both feature selection and regression. SVR is a version of the Support Vector Machine (SVM) for regression. In our experiments, we used the default parameter setting for SVR (C = 1, = 0.2 and kernel = RBF ). We used Pearson\u2019s correlation coefficient as the evaluation measure. All the results are based on 10-fold cross validation. Since in the experiments, LR-L and SVR have very similar performance. In the following, we only report the results from SVR.\nFigure 2a shows how the prediction performance changes with the data size. In general, more data means better performance. However, most of the systems achieved close to maximum performance with Likes from only 200,000 users. After that, the performance only improved slightly. Here, for clarity, we did not show results with more than 1 million users. Most lines become very flat after that. Moreover, other than AE, all the models trained with ULE performed significantly better than the baseline (The baseline is represented by the flat green line). In fact, four out of the eight models achieved more than 100% improvement over the baseline. Finally, among all the feature learning methods we tested, features from P-DBOW achieved the best performance. Possible explanations include (1) P-DBOW employs a neural networkbased prediction model (2) It uses a 1-stage process and the aggregation is done systematically via backpropogragion (3) It does not use a local context window, which is more appropriate for the Like data.\nFigure 2b shows the impact of the number of feature dimensions on system performance. Other than SVD which worked the best when the number is small (e.g. 50 or 100), all the other models worked the best when the feature dimension is large (e.g., either 300 or 500).\nIn addition, the two statistical features (i.e., number of Likes per user and the average popularity of the LEs liked by a user) were also useful. Models using all the features\nperformed better than those with the ULE features only."}, {"heading": "8 Discussions and Conclusions", "text": "In this study, we demonstrate that there are signals in social media Likes that can be used to infer DDR. Like many models of human behavior, DDR is very complex and can be influenced by many factors such as age and income [Green et al., 1996], personality [Hirsh et al., 2008], intelligence quotient (IQ) [Shamosh and Gray, 2008] and cultural differences [Du et al., 2002]. Since we didn\u2019t try to capture all the relevant factors in our model, the current goal is not to build a perfect DDR prediction system but to gain behavior insight into the relationship between them.\nTo conclude, in this paper, we describe the first large-scale study to investigate the relationship between a user\u2019s social media behavior (e.g., Facebook Likes) and DDR. We have explored a comprehensive set of unsupervised feature learning methods to leverage a large amount of unsupervised social media data. Our results have demonstrated the benefits of incorporating unsupervised feature learning. Four of our top models with feature learning perform twice as well as the baseline system. In addition, our study has revealed interesting patterns of the relationships between DDR and social media Likes. For example, Hip Hop music is liked more by a \u201ctoday person\u201d while sci-fi and fantasy novels are liked more by a \u201ctomorrow person\u201d. In addition, indirect signals such as your taste of musics, TV shows and books can reveal more about your DDR than more direct signals such as your interests in weed and smoking."}], "references": [{"title": "Pathological gambling severity is associated with impulsivity in a delay discounting procedure", "author": ["SM Alessi", "NM Petry"], "venue": "Behavioural Processes, 64(3):345\u2013354", "citeRegEx": "Alessi and Petry. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "The hyperbolic consumption model: Calibration", "author": ["George-Marios Angeletos", "David Laibson", "Andrea Repetto", "Jeremy Tobacman", "Stephen Weinberg"], "venue": "simulation, and empirical evaluation. The Journal of Economic Perspectives, 15(3):47\u201368,", "citeRegEx": "Angeletos et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Don\u2019t count", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL (1), pages 238\u2013247,", "citeRegEx": "Baroni et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward a behavioral economic understanding of drug dependence: delay discounting processes", "author": ["Warren K Bickel", "Lisa A Marsch"], "venue": "Addiction, 96(1):73\u201386,", "citeRegEx": "Bickel and Marsch. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Impulsivity and cigarette smoking: delay discounting in current", "author": ["Warren K Bickel", "Amy L Odum", "Gregory J Madden"], "venue": "never, and ex-smokers. Psychopharmacology, 146(4):447\u2013454,", "citeRegEx": "Bickel et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Remember the future: working memory training decreases delay discounting among stimulant addicts", "author": ["Warren K Bickel", "Richard Yi", "Reid D Landes", "Paul F Hill", "Carole Baxter"], "venue": "Biological psychiatry, 69(3):260\u2013265,", "citeRegEx": "Bickel et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Journal of machine Learning research", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3(Jan):993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "pages 3267\u20133276", "author": ["Munmun De Choudhury", "Scott Counts", "Eric Horvitz. Predicting postpartum changes in emotion", "behavior via social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"], "venue": "ACM,", "citeRegEx": "De Choudhury et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "SIAM journal on Matrix Analysis and Applications", "author": ["Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle. A multilinear singular value decomposition"], "venue": "21(4):1253\u20131278,", "citeRegEx": "De Lathauwer et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["Harris Drucker", "Christopher JC Burges", "Linda Kaufman", "Alex Smola", "Vladimir Vapnik"], "venue": "Support vector regression machines. Advances in neural information processing systems, 9:155\u2013161,", "citeRegEx": "Drucker et al.. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "The Psychological Record", "author": ["Wanjiang Du", "Leonard Green", "Joel Myerson. Cross-cultural comparisons of discounting delayed", "probabilistic rewards"], "venue": "52(4):479,", "citeRegEx": "Du et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Addiction", "author": ["Matt Field", "Paul Christiansen", "Jon Cole", "Andrew Goudie. Delay discounting", "the alcohol stroop in heavy drinking adolescents"], "venue": "102(4):579\u2013586,", "citeRegEx": "Field et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In CHI\u201911 extended abstracts on human factors in computing systems", "author": ["Jennifer Golbeck", "Cristina Robles", "Karen Turner. Predicting personality with social media"], "venue": "pages 253\u2013262. ACM,", "citeRegEx": "Golbeck et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal discounting in choice between delayed rewards: the role of age and income", "author": ["Leonard Green", "Joel Myerson", "David Lichtman", "Suzanne Rosen", "Astrid Fry"], "venue": "Psychology and aging, 11(1):79,", "citeRegEx": "Green et al.. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "science, 313(5786):504\u2013507", "citeRegEx": "Hinton and Salakhutdinov. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Journal of Experimental psychology: general", "author": ["Kris N Kirby", "Nancy M Petry", "Warren K Bickel. Heroin addicts have higher discount rates for delayed rewards than non-drug-using controls"], "venue": "128(1):78,", "citeRegEx": "Kirby et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Private traits and attributes are predictable from digital records of human behavior", "author": ["M. Kosinski", "D. Stillwell", "T. Graepel"], "venue": "Proceedings of the National Academy of Sciences, 110(15)", "citeRegEx": "Kosinski et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Facebook as a research tool for the social sciences: Opportunities", "author": ["Michal Kosinski", "Sandra C Matz", "Samuel D Gosling", "Vesselin Popov", "David Stillwell"], "venue": "challenges, ethical considerations, and practical guidelines. American Psychologist, 70(6):543,", "citeRegEx": "Kosinski et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 14", "author": ["Quoc V Le", "Tomas Mikolov. Distributed representations of sentences", "documents. In ICML"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Who can wait for the future? a personality perspective", "author": ["V. Mahalingam", "D. Stillwell", "M. Kosinski", "J. Rust", "A. Kogan"], "venue": "Social Psychological and Personality Science, 2(3):397\u2013425, June", "citeRegEx": "Mahalingam et al.. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "In Advances in neural information processing systems", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Psychological Record", "author": ["Bryan K Saville", "Amanda Gisbert", "Jason Kopp", "Carolyn Telesco. Internet addiction", "delay discounting in college students"], "venue": "60(2):273,", "citeRegEx": "Saville et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Delay discounting and intelligence: A metaanalysis", "author": ["Noah A Shamosh", "Jeremy R Gray"], "venue": "Intelligence, 36(4):289\u2013305,", "citeRegEx": "Shamosh and Gray. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Addiction", "author": ["David J Stillwell", "Richard J Tunney. Effects of measurement methods on the relationship between smoking", "delay reward discounting"], "venue": "107(5):1003\u20131012,", "citeRegEx": "Stillwell and Tunney. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Series B (Methodological)", "author": ["Robert Tibshirani. Regression shrinkage", "selection via the lasso. Journal of the Royal Statistical Society"], "venue": "pages 267\u2013288,", "citeRegEx": "Tibshirani. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Appetite", "author": ["Rosalyn E. Weller", "Edwin W. Cook III", "Kathy B. Avsar", "James E. Cox. Obese women show greater delay discounting than healthy-weight women"], "venue": "51(3):563 \u2013 569,", "citeRegEx": "Weller et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Computer-based personality judgments are more accurate than those made by humans", "author": ["W. Youyou", "M. Kosinski", "D. Stillwell"], "venue": "Proceedings Of The National Academy Of Sciences (PNAS)", "citeRegEx": "Youyou et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": ", substance abuse) [Kirby et al., 1999].", "startOffset": 19, "endOffset": 39}, {"referenceID": 3, "context": "In economics and psychology, delay discounting is often used to characterize how individuals choose between a smaller immediate reward and a larger delayed reward [Bickel and Marsch, 2001].", "startOffset": 163, "endOffset": 188}, {"referenceID": 19, "context": "Since the ability to modulate the desire of immediate gratification for long term rewards plays an important role in our decision-making, lower discounting rate often predicts better social, academic and health outcomes [Mahalingam et al., 1992].", "startOffset": 220, "endOffset": 245}, {"referenceID": 0, "context": "In contrast, higher discounting rate is often associated with problematic behaviors such as alcohol/drug abuse, pathological gambling and credit card default [Alessi and Petry, 2003; Kirby et al., 1999].", "startOffset": 158, "endOffset": 202}, {"referenceID": 15, "context": "In contrast, higher discounting rate is often associated with problematic behaviors such as alcohol/drug abuse, pathological gambling and credit card default [Alessi and Petry, 2003; Kirby et al., 1999].", "startOffset": 158, "endOffset": 202}, {"referenceID": 5, "context": "Thus, research on understanding and moderating delay discounting has the potential to produce substantial societal benefits [Bickel et al., 2011].", "startOffset": 124, "endOffset": 145}, {"referenceID": 27, "context": "Given the large variety of entities that can be liked (called \u201clike entities (LE)\u201d) and the large number of users, social media Likes represent one of the most generic digital footprints available today [Youyou et al., 2015].", "startOffset": 203, "endOffset": 224}, {"referenceID": 16, "context": "In addition to directly signaling interests and preferences, social media Likes are indicative of ethnicity, intelligence, and use of addictive substances [Kosinski et al., 2013].", "startOffset": 155, "endOffset": 178}, {"referenceID": 3, "context": "There is a rich body of research in economics and behavior science that investigates the relationship between DDR and real-world human behaviors such as drug abuse [Bickel and Marsch, 2001], an obese [Weller et al.", "startOffset": 164, "endOffset": 189}, {"referenceID": 26, "context": "There is a rich body of research in economics and behavior science that investigates the relationship between DDR and real-world human behaviors such as drug abuse [Bickel and Marsch, 2001], an obese [Weller et al., 2008], pathological gambling [Alessi and Petry, 2003], drinking [Field et al.", "startOffset": 200, "endOffset": 221}, {"referenceID": 0, "context": ", 2008], pathological gambling [Alessi and Petry, 2003], drinking [Field et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 11, "context": ", 2008], pathological gambling [Alessi and Petry, 2003], drinking [Field et al., 2007], smoking [Bickel et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 4, "context": ", 2007], smoking [Bickel et al., 1999] , addiction to the internet [Saville et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 22, "context": ", 1999] , addiction to the internet [Saville et al., 2010] and credit card default [Angeletos et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 1, "context": ", 2010] and credit card default [Angeletos et al., 2001].", "startOffset": 32, "endOffset": 56}, {"referenceID": 7, "context": "For example, social media analytics has been used to analyze and predict emotions [De Choudhury et al., 2013], personality [Youyou et al.", "startOffset": 82, "endOffset": 109}, {"referenceID": 27, "context": ", 2013], personality [Youyou et al., 2015] and political learning [Kosinski et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 16, "context": ", 2015] and political learning [Kosinski et al., 2013].", "startOffset": 31, "endOffset": 54}, {"referenceID": 12, "context": "Many of the existing systems rely on supervised machine learning and human-engineered features [Golbeck et al., 2011] .", "startOffset": 95, "endOffset": 117}, {"referenceID": 17, "context": "Our dataset was obtained by the myPersonality project between 2007 and 2012 [Kosinski et al., 2015].", "startOffset": 76, "endOffset": 99}, {"referenceID": 24, "context": "So far, since a hyperbolic function captures the phenomenon that a person\u2019s discount rate typically decreases as the delay to the reward increases, it matches observed human discounting behavior better [Stillwell and Tunney, 2012].", "startOffset": 202, "endOffset": 230}, {"referenceID": 24, "context": "To obtain delay discounting ground truth, users of the myPerosnality Facebook App were asked to complete a multi-item delay discounting questionnaire [Stillwell and Tunney, 2012].", "startOffset": 150, "endOffset": 178}, {"referenceID": 24, "context": "Since the distribution of k is often found to be non-normal, the data were approximately normalized using the natural-log transformation [Stillwell and Tunney, 2012].", "startOffset": 137, "endOffset": 165}, {"referenceID": 24, "context": "Since the ground truth DDR can only be obtained via sophisticated psychometric tests [Stillwell and Tunney, 2012; Kirby et al., 1999], it is expensive to acquire DDR on a large scale.", "startOffset": 85, "endOffset": 133}, {"referenceID": 15, "context": "Since the ground truth DDR can only be obtained via sophisticated psychometric tests [Stillwell and Tunney, 2012; Kirby et al., 1999], it is expensive to acquire DDR on a large scale.", "startOffset": 85, "endOffset": 133}, {"referenceID": 8, "context": "Singular Value Decomposition (SVD) is a mathematical technique that is frequently used for dimension reduction [De Lathauwer et al., 2000].", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "Latent Dirichlet Allocation (LDA) is a generative graphical model that allows sets of observations to be explained by unobserved latent groups [Blei et al., 2003].", "startOffset": 143, "endOffset": 162}, {"referenceID": 14, "context": "Autoencoder (AE) is a neural network-based method for self-taught learning [Hinton and Salakhutdinov, 2006].", "startOffset": 75, "endOffset": 107}, {"referenceID": 20, "context": "ULE with Continuous Bag of Word Model (U-CBOW) Continuous Bag of Word (CBOW) model is a neural networkbased method originally designed to learn dense vector representations for words [Mikolov et al., 2013].", "startOffset": 183, "endOffset": 205}, {"referenceID": 20, "context": "The model is frequently trained using either a hierarchical softmax function (HS) or negative sampling (NS) [Mikolov et al., 2013].", "startOffset": 108, "endOffset": 130}, {"referenceID": 18, "context": "P-DM was also originally designed for natural language processing [Le and Mikolov, 2014].", "startOffset": 66, "endOffset": 88}, {"referenceID": 21, "context": "ULE with GloVe (U-GloVe) GloVe is an unsupervised learning algorithm originally designed to learn vector representations of words based on aggregated global word-word co-occurrence statistics from a text corpus [Pennington et al., 2014].", "startOffset": 211, "endOffset": 236}, {"referenceID": 2, "context": "There are some empirical evidence indicating that prediction-based methods may have some advantage over count-based methods in feature learning [Baroni et al., 2014].", "startOffset": 144, "endOffset": 165}, {"referenceID": 25, "context": "Since DDR is a continuous variable, we employ two Machine Learning methods that output numerical values: (1) Linear Regression with Lasso (LR-L) [Tibshirani, 1996] (b) Support Vector Regression (SVR) [Drucker et al.", "startOffset": 145, "endOffset": 163}, {"referenceID": 9, "context": "Since DDR is a continuous variable, we employ two Machine Learning methods that output numerical values: (1) Linear Regression with Lasso (LR-L) [Tibshirani, 1996] (b) Support Vector Regression (SVR) [Drucker et al., 1997].", "startOffset": 200, "endOffset": 222}, {"referenceID": 13, "context": "Like many models of human behavior, DDR is very complex and can be influenced by many factors such as age and income [Green et al., 1996], personality [Hirsh et al.", "startOffset": 117, "endOffset": 137}, {"referenceID": 23, "context": ", 2008], intelligence quotient (IQ) [Shamosh and Gray, 2008] and cultural differences [Du et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 10, "context": ", 2008], intelligence quotient (IQ) [Shamosh and Gray, 2008] and cultural differences [Du et al., 2002].", "startOffset": 86, "endOffset": 103}], "year": 2017, "abstractText": "Delay discounting, a behavioral measure of impulsivity, is often used to quantify the human tendency to choose a smaller, sooner reward (e.g., $1 today) over a larger, later reward ($2 tomorrow). Delay discounting and its relation to human decision making is a hot topic in economics and behavior science since pitting the demands of long-term goals against short term desires is among the most difficult tasks in human decision making [Hirsh et al., 2008]. Previously, small-scale studies based on questionnaires were used to analyze an individual\u2019s delay discounting rate (DDR) and his/her realworld behavior (e.g., substance abuse) [Kirby et al., 1999]. In this research, we employ large-scale social media analytics to study DDR and its relation to people\u2019s social media behavior (e.g., Facebook Likes). We also build computational models to automatically infer DDR from Social Media Likes. Our investigation has revealed interesting results.", "creator": "LaTeX with hyperref package"}}}