{"id": "1703.07822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Information-theoretic Model Identification and Policy Search using Physics Engines with Application to Robotic Manipulation", "abstract": "we consider the problem of a robot learning evaluating the mechanical properties of objects through physical interaction with the object, and introduce a useful practical, data - efficient approach for ultimately identifying the motion models of these objects. the proposed method utilizes a physics engine, where the robot seeks to properly identify the inertial and friction parameters of the object by simulating its motion under different values of the parameters and identifying those that result in a simulation methodology which matches the observed underlying real motions. the dynamic problem is solved in a bayesian optimization framework. the same framework is used for both identifying the model of an object online domain and searching for a policy argument that would minimize precisely a given cost function according to the identified model. experimental results both in simulation and using a real robot indicate that the proposed method outperforms state - of - the - art model - free reinforcement learning approaches.", "histories": [["v1", "Wed, 22 Mar 2017 19:08:48 GMT  (7221kb,D)", "http://arxiv.org/abs/1703.07822v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["shaojun zhu", "rew kimmel", "abdeslam boularias"], "accepted": false, "id": "1703.07822"}, "pdf": {"name": "1703.07822.pdf", "metadata": {"source": "CRF", "title": "Information-theoretic Model Identification and Policy Search using Physics Engines with Application to Robotic Manipulation", "authors": ["Shaojun Zhu", "Andrew Kimmel", "Abdeslam Boularias"], "emails": ["abdeslam.boularias}@cs.rutgers.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nConsider the scenario shown in Figure 1, where a robot (Motoman) assists another robot (Baxter) that cannot reach its desired object. Due to the placements of the robots in the scene, the intersection of each robot\u2019s reachable workspace is empty, which restricts the robots from executing a \u201cdirect hand-off\u201d maneuver. In this case, the Motoman robot must exploit the dynamic physical properties of the object in order to \u201cslide\u201d it over to the Baxter robot. Ideally, this action would happen without intervention or assistance from an outside operative, such as a human.\nLearning the physical properties of an object and predicting its motion under physical interaction is a critical aspect of this challenge. If the robot simply executes a maximum velocity push on the object, the result could cause the object to leave the robot\u2019s workspace (i.e. falling off the table), which is undesirable as it would ruin the autonomous behavior of the system.\nThis paper proposes a data-efficient approach for motion prediction by utilizing a physics engine and learning the physical parameters through black-box Bayesian optimization. Specifically, the objective of the method is to predict the motion of an object when acted upon by a robotic hand. First, a real robot is used to perform some random pushing action with an object on a tabletop [1]. Both the initial and final configurations of the object and the hand are recorded. Instead of learning the object\u2019s motion explicitly, a Bayesian optimization technique is used to identify relevant physical parameters, such as mass and friction, through the physics\n1Shaojun Zhu, Andrew Kimmel, and Abdeslam Boularias are with the Department of Computer Science, Rutgers University, New Jersey, USA {shaojun.zhu, andrew.kimmel, abdeslam.boularias}@cs.rutgers.edu\nengine simulation. To predict the motion of the object under a new action, the learned parameters can be used to simulate the action in a physics engine. The results of this simulation can then be used by the robot to predict the effect of its action on the object. To solve the challenge in Figure 1, the same Bayesian optimization technique is used to search the optimal control policy for the robotic hand pushing the object."}, {"heading": "II. RELATED WORK", "text": "Several physics engines have been used for simulating dynamics of robots as well as the objects they interact with. Examples of popular physics engines frequently used in robotics include Bullet [2], MuJoCo [3], DART [4], PhysX [5], Havok [6], ODE [7], and GraspIt! [8]. A survey and a comparison of these tools are given in [9].\nData-driven system identification is a popular approach that is at the core of learning for control techniques. Examples of these techniques include model-based reinforcement learning for instance [10]. We focus here on works related to learning mechanical models of unknown objects. Several cognitive models that combine Bayesian inference with approximate knowledge of Newtonian physics have been proposed recently [11]\u2013[13]. These methods learn probabilistic models from noisy physical simulations. Nevertheless, these models are built to explain the learning of Newtonian physics in humans, rather than to be used for robotic manipulation, which typically requires a higher precision as well as faster\nar X\niv :1\n70 3.\n07 82\n2v 1\n[ cs\n.R O\n] 2\n2 M\nar 2\n01 7\nlearning and inference times. Two high-level approaches exist for solving physical interaction problems, which reside at two extremes of a spectrum. Model-based approaches [14]\u2013[18] rely on accurate models for objects and their properties. They are used within standard simulation, planning, and actuation control loops. A physics-based simulation was used in [14] for predicting the effects of pushing actions, but the authors considered only flat, well-separated objects on a smooth surface. A nonparametric approach was used in [16] for learning the outcome of pushing large objects (furniture). A Markov Decision Process (MDP) is used in [17] for modeling interactions between objects, however, only simulation results on pushing were reported in that work. Nevertheless, it is prohibitive to manually define perfect and accurate models that express all types of interactions a robot can experience in the real world. Other Bayesian model-based techniques, such as PILCO [19], have been proven efficient in utilizing a small amount of data for learning dynamical models and optimal policies. These techniques learn dynamical equations from scratch, unlike our method which assumes that the motion equations are known and provided by a physics engine, and instead concentrates on identifying only the inertial and friction properties of the objects.\nAnother alternative, which is becoming increasingly popular, addresses these challenges through end-to-end learning [1], [20]\u2013[31]. This involves the demonstration of many successful examples of physical interaction and learning the controls for solving a problem as a function of the sensing input. These approaches, however, usually require many physical experiments to effectively learn. The proposed method aims to be more data-efficient, and can quickly adapt online to minor changes in object dynamics. Furthermore, it is not clear for existing methods how uncertainty, a consequence of learning from a small number of data points, could be handled in a principled way. Note that there is a significant body of work on learning sliding models of objects using white-box optimization [15], [18], [32]. It is not clear, at the moment, if these methods would perform better than the proposed approach. A drawback of white-box methods is that they are often used only in simple setups, such as pushing planar objects [18]."}, {"heading": "III. PROPOSED APPROACH", "text": ""}, {"heading": "A. System Overview", "text": "To solve the problem of modeling mechanical properties of objects, this paper proposes an online learning approach to identify mass and sliding models of objects using Bayesian optimization. The goal is to allow the robot to use predefined models of objects, in the form of prior distributions, and to improve the accuracy of these models on the fly by interacting with the objects. This learning process must happen in real time since it takes place simultaneously with the physical interaction.\nFigure 2 shows an overview of the proposed approach. The first step consists of using a pre-trained object detector to detect the different objects present in the scene and estimate\ntheir poses by mapping them to a knowledge base of preexisting 3D mesh models. The proposed method augments the 3D mesh models of each object with the mechanical properties. These properties correspond to the object\u2019s mass, as well as the static and kinetic friction coefficients for each rigid subpart of a given object. Using a different model for each subpart of an object is crucial to modeling articulated objects. In this work, we focus on non-articulated objects. We divide the surface of an object into a regular grid and identify the friction parameters of each part of the grid. These properties are represented as a d-dimensional vector \u03b8. A prior distribution P0 on \u03b8 is used instead of a single value of \u03b8, since different instances of the same category usually have different mechanical properties.\nThe online learning algorithm takes as input a prior distribution Pt on the model parameters \u03b8. Pt is calculated based on an initial distribution P0 and a sequence of observations (x0, \u00b50, x1, \u00b51, . . . , xt\u22121, \u00b5t\u22121, xt), wherein xt is the 6D pose (position and orientation) of the manipulated object at time t and \u00b5t is a vector describing a force applied by the robot\u2019s fingertip on the object at time t. Applying a force \u00b5t results in changing the object\u2019s pose from xt to xt+1."}, {"heading": "B. Model Identification", "text": "Given a prior distribution Pt and a new observation (xt, \u00b5t+1, xt+1), a physics engine is used to estimate a posterior distribution Pt+1 on the model parameters \u03b8. We are currently using the Bullet physics engine [2]. The posterior distribution Pt+1 is obtained by simulating the effect of force \u00b5t+1 on the object under various values of parameters \u03b8 and observing the resulting positions x\u0302t+1. The goal is to identify the model parameters that make the outcome x\u0302t+1 of the simulation as close as possible to the actual observed outcome xt+1. In other terms, the following black-box optimization problem is solved:\n\u03b8\u2217 = arg min \u03b8 E(\u03b8) def = \u2016xt+1 \u2212 f(xt, \u00b5t, \u03b8)\u20162,\nwherein xt and xt+1 are the observed poses of the object at times t and t+ 1, \u00b5t is the force that moved the object from xt to xt+1, and f(xt, \u00b5t, \u03b8) = x\u0302t+1, the simulated pose at time t+ 1 after applying force \u00b5t in pose xt.\nThe model parameters \u03b8 can be limited to a discrete set, i.e. \u03b8 \u2208 {\u03b81, \u03b82, . . . , \u03b8n} def= \u0398. A naive approach of solving this problem consists of systematically simulating all the parameters \u03b8i in \u0398, simulating the effect of force \u00b5t on the object with parameters \u03b8i, and comparing the predicted pose f(xt, \u00b5t, \u03b8i) to the actual pose xt+1. However, this would be inefficient due to the size of \u0398, which is relatively large given that the dimension d of the parameter space is typically high. Furthermore, each individual simulation is also computationally expensive. It is therefore important to minimize the number of simulations while searching for the optimal parameters. Moreover, the optimization problem above is ill-posed, as is the case in all inverse problems. In other terms, there are multiple model parameters that\ncan explain an observed movement of an object. Instead of returning a single answer, the proposed algorithm returns a distribution Pt+1 on the set of possible parameters \u0398.\nThis paper formulates this challenge in a Bayesian optimization framework, which uses the Entropy Search technique presented in [33]. This work instead presents a more computationally efficient version of the Entropy Search technique, that we call Greedy Entropy Search and describe in the following.\nTo solve the aforementioned Bayesian optimization problem, the error function E must be learned from a minimum number of simulations, using a sequence of parameters \u03b81, \u03b8t, . . . , \u03b8k \u2208 \u0398. To choose these parameters efficiently, a belief about the actual error function is maintained. This belief is a probability measure p(E) over the space of all functions E : Rd \u2192 R. A Gaussian Process (GP) is used to represent the belief p, which is sequentially updated using the errors E(\u03b8i) computed from simulation using model parameters \u03b8i. Readers can find more details in textbooks on how Gaussian processes are updated from data and how to get the GP belief p on unknown function E from data points E(\u03b8i) [34]. The belief p is initialized at each time instance t using prior Pt, which represents the model distribution from the previous time-step.\nAfter simulating the object\u2019s motion with different model parameters \u03b81, \u03b8t, . . . , \u03b8k, p is updated using the computed simulation errors. p implicitly defines another distribution Pmin on the identity of the best model parameter \u03b8\u2217, which\ncan be used to select the next simulation parameter \u03b8k+1.\nPmin(\u03b8) def = P ( \u03b8 = arg min\n\u03b8i\u2208\u0398 E(\u03b8i) ) =\n\u222b E:Rd\u2192R p(E)\u03a0\u03b8i\u2208\u0398\u2212{\u03b8}H ( E(\u03b8i)\u2212 E(\u03b8) ) dE,\nwhere H is the Heaviside step function, i.e. H ( E(\u03b8i) \u2212\nE(\u03b8) ) = 1 if E(\u03b8i) \u2265 E(\u03b8) and H ( E(\u03b8i) \u2212 E(\u03b8) ) = 0 otherwise. Unlike p(E), the distribution of the simulation error E modeled as a Gaussian Process, the distribution Pmin does not have a closed-form expression. Therefore, Monte Carlo is used for estimating Pmin from samples of E(\u03b8i) for each \u03b8i \u2208 \u0398. Specifically, this process samples vectors containing the values that E takes, according to the learned Gaussian process, in each model parameter in \u0398. Pmin(\u03b8i) is estimated by counting the fraction of sampled vectors of the values of E where \u03b8i happens to have the lowest value.\nThe model parameter \u03b8 is chosen such that it has the highest contribution to the current entropy of Pmin, i.e. with the highest term \u2212Pmin(\u03b8) log ( Pmin(\u03b8) ) , as the next model parameter to evaluate in simulation. This method is referred to as the Greedy Entropy Search method because it aims to decrease the entropy of the belief Pmin. This process is repeated until the entropy of Pmin does not change much or until the simulation\u2019s time budget is consumed. After that, Pmin is used as the new belief Pt+1 on the model parameters. This new belief can then be utilized for planning an action \u00b5t+1 which will move the object to a new pose xt+1, after which the same process is repeated all over again."}, {"heading": "C. Policy Optimization", "text": "Given a distribution Pt on the model (e.g, friction parameters and mass), and cost function J : \u03c4 \u2192 R, where \u03c4 = (x0, \u00b50, x1, \u00b51, . . . , xH\u22121, \u00b5H\u22121, xH) is a trajectory of predicted object poses and applied forces, the robot needs to find a feedback control policy \u03c0\u03b7 that returns an action \u00b5t in pose xt of the object. Policy \u03c0\u03b7 is limited to a family of predefined policies (e.g, pushing directions) and parametrized by \u03b7 (e.g., end-effector velocity along a given pushing direction). Since the physics engine that we are using is deterministic, the transition model used by the physics engine is defined to be a function f that takes as input an initial pose x0 and a policy \u03c0\u03b7, a model parameter \u03b8 and returns a trajectory \u03c4 = f(x0, \u03c0\u03b7, \u03b8). We then search for a policy parameter \u03b7\u2217 defined as \u03b7\u2217 = arg min\u03b7 J(f(x0, \u03c0\u03b7, \u03b8)).\nTo solve this problem in real-time, only the most likely object model \u03b8\u2217 = arg max\u03b8\u2208\u0398 Pt(\u03b8) is used for finding the optimal policy parameter \u03b8\u2217. The policy parameter \u03b7 can be limited to a discrete set, i.e. \u03b7 \u2208 {\u03b71, \u03b72, . . . , \u03b7n} def= \u03a0. A naive approach of solving this problem consists of iterating over all the parameters \u03b7i in \u03a0, simulating a trajectory \u03c4i = f(x0, \u03c0\u03b7i , \u03b8\n\u2217) of the object using policy \u03c0\u03b7i , and selecting the policy parameter \u03b7i with the minimum cost J(\u03c4i). However, this would be computationally inefficient.\nWe therefore use the same Greedy Entropy Search method, presented in the previous section, for searching for the best policy parameter \u03b7\u2217 in real-time. This is achieved by noticing the analogy between model parameters \u03b8 and policy parameters \u03b7, and between the simulation error E(\u03b8) and the cost function J(f(x0, \u03c0\u03b7, \u03b8\u2217)). Hence, the same technique can be used for finding \u03b7\u2217 = arg min\u03b7 J(f(x0, \u03c0\u03b7, \u03b8)) where \u03b8 is known and \u03b7 is a variable."}, {"heading": "IV. EXPERIMENTS", "text": "In all experiments, we used Blender [35] which utilizes the Bullet [2] physics engine. PHYSIM 6DPose [36] was used to track the object and provide the initial and final poses of the object, through a RealSense depth camera mounted on the torso of the Motoman robot. Videos of the experiments can be found here: https://goo.gl/8Pi2Gu."}, {"heading": "A. Learning Physical Properties for Motion Prediction", "text": "1) Data Collection and Evaluation Metrics: In this preliminary experiment, a Reflex SF robotic hand mounted on the right arm of a Motoman SDA10F manipulator was used to randomly push a simple rigid object on a tabletop, as shown in Figure 3. We learn the object model parameters \u03b8 (mass and the friction coefficient) of an Expo eraser. During data collection, no human effort is needed to reset the scene since both the speed and pushing direction were controlled such that the object was always in the workspace of the robotic hand. Using the collected pushing data, the physical properties of the object were learned so as to predict the motion of the object under new actions. Fifteen random pushing actions were performed. Six actions were discarded due to inaccurate tracking caused by occlusions. Out of the remaining nine actions, six were used for training and the other three for testing. To measure the accuracy of the learned model, the error between the predicted location of the object and the observed end location was computed.\nAdditionally, a large scale planar push dataset [37] was also used to validate the proposed method. The dataset contained recorded poses of a planar object before and after being pushed by an ABB IRB 120 robot arm. The poses are recorded using the Vicon tracking system and are therefore more accurate.\n2) Results: We compared the results of the proposed Greedy Entropy Search method against Random Search in Figure 4. Random Search was performed by randomly sampling \u03b8 in the same parameter space as the Greedy Entropy Search. Both methods were run ten times, with the resulting mean and standard deviation of the training error reported. The results show that Greedy Entropy Search achieved lower error when predicting the results of new actions.\nThe prediction error is also reported as a function of the number of training samples. Figure 5a shows a comparison between the prediction errors of models trained with one sample, three samples and all six samples in. The results indicate that with more training samples, the average error decreases.\nThe proposed method was also tested using a large scale pushing dataset [37]. Specifically, we report the result using the rect1 shape on the abs surface. 200 samples were randomly\nselected and the result of 10-fold cross validation is shown in Figure4b. The proposed Greedy Entropy Search also achieved lower error than the Random Search baseline."}, {"heading": "B. Policy Optimization using the Motion Prediction Model", "text": "1) Setup: In this experiment, the task is to push the object to a fixed goal position from a start region. The setup is similar to IV-A, a Motoman manipulator pushing an Expo eraser using a Reflex hand. For each trial, we push the object twice towards the goal, as shown in Figure 6. In this experiment, the policy parameter \u03b7 is the push direction. 25 random actions are sampled and the action that can push the object closest to the goal position is selected to be executed.\n2) Results: We compare the pushing results using motion prediction model with two sets of parameters: one is learned using Greedy Entropy Search, the other is found using Random Search. Figure 7 shows that the model using Greedy Entropy Search enabled the robot to push the object to the 1cm vicinity of the goal position 7 out of 10 trials, while the one using Random Search only did it 4 times."}, {"heading": "C. High Speed Push Policy Optimization using Model Trained with Low Speed Push", "text": "So far, the actions were limited to low speed pushes so that the object was always in the reachable workspace of the\nrobot. In order to solve the challenge presented in Figure 1, however, higher speed push actions are needed. The friction between the object and the contact surface varies when the object moves at different speeds. We can collect data using higher speed push in a similar way to IV-A.1. However, this also means much more human resets will be needed, since the robot would push the object away from its workspace, sometimes even off the table. In this experiment, we avoid the human resets and aim to optimize high speed pushing policy using model trained with only low speed pushing data.\n1) Setup: In this experiment, the task is to push the bottle from one side to the other side of the table, which is about one meter away, as shown in Figure 8. We aim to find the optimal policy with parameter \u03b7 representing the pushing speed of the robotic hand. We collected random low speed pushing data in a similar way to IV-A.1, using a glucose bottle, without human reset.\nAfter being pushed, the object sometimes is no longer within the view of the RealSense camera on the torso of Motoman. Instead, the in-hand camera on Baxter robot was used to localize the final location of the object after it\u2019s being pushed. After learning the object model with parameters \u03b8 (mass and the friction coefficient), using the Greedy Entropy Search approach, optimal policy that can push the object closest to the goal position is selected. We compare our\napproach with a model-free reinforcement learning method: Policy learning by Weighting Exploration with the Returns (PoWER) [38]. PoWER iteratively optimizes a stochastic policy as an Expectation-Maximization(EM) problem, directly using real roll-outs results.\n2) Results: We report results from both simulation and real roll-outs. We evaluate: \u2022 The error between the final object location after being\npush and the desired goal location. \u2022 The number of times object falling off the table. Figure 10 and 11 show the result in simulation and with a real Motoman robot. In simulation, we randomly set groundtruth(GT) mass and friction parameters and perform roll-outs using the GT parameters. Both in simulation and with the real Motoman robot, the proposed method achieves both lower error and fewer object drops. We argue this is important in robot learning as we would like to minimize human efforts during the learning process in order to achieve autonomous robot learning. Notice that PoWER achieved smaller variance in real rollouts comparing to simulation. The probable reason for that is that because of sensing and actuation error in real roll-outs, PoWER tended to be over conservative in terms of pushing speed because of the object drops it made."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this paper, we presented a data-efficient online learning method for identifying mechanical properties of objects. The method leverages a physics engine through simulation and finds the optimal parameters that match the real roll-outs in a Bayesian optimization framework. The same framework is\nalso used for policy optimization. Experimental results, both in simulation and using a real robot, show that the method outperforms model-free reinforcement learning methods.\nAn important aspect of robot learning is how many real world roll-out data are enough to achieve a certain success rate. We are currently working on evaluating the model confidence by computing the expected success rate using the uncertainty of the model. In the future, finding efficient methods for handling model parameters of non-homogeneous objects is an interesting future direction that can help scaling to more complex environment. Furthermore, while this work only considered random exploratory actions, a more intelligent way of action sampling could help better exploring the action space. Additionally, it would be interesting to investigate combining the pre-trained deep models with online learning to achieve both high capability of generalization and data efficiency."}], "references": [{"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["Pulkit Agrawal", "Ashvin Nair", "Pieter Abbeel", "Jitendra Malik", "Sergey Levine"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Graspit!: A Versatile Simulator for Robotic Grasping", "author": ["Andrew Miller", "Peter K. Allen"], "venue": "IEEE Robotics and Automation Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ODE and physx", "author": ["Tom Erez", "Yuval Tassa", "Emanuel Todorov"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Inferring mass in complex scenes by mental", "author": ["J. Hamrick", "P.W. Battaglia", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "simulation. Cognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A compositional object-based approach to learning physical dynamics. Under review as a conference paper for ICLR, 2017", "author": ["M.B. Chang", "T. Ullman", "A. Torralba", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["P. Battaglia", "R. Pascanu", "M. Lai", "D.J. Rezende", "K. Koray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Physics-Based Grasp Planning Through Clutter", "author": ["Mehmet Dogar", "Kaijen Hsiao", "Matei Ciocarlie", "Siddhartha Srinivasa"], "venue": "In Robotics: Science and Systems VIII,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Stable pushing: Mechanics, controllability, and planning", "author": ["K.M. Lynch", "M.T. Mason"], "venue": "IJRR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Push-manipulation of Complex Passive Mobile Objects Using Experimentally Acquired Motion Models", "author": ["Tekin Merili", "Manuela Veloso", "H.Levent Akin"], "venue": "Autonomous Robots,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A Physics-Based Model Prior for Object-Oriented MDPs", "author": ["Jonathan Scholz", "Martin Levihn", "Charles L. Isbell", "David Wingate"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A convex polynomial force-motion model for planar sliding: Identification and application", "author": ["Jiaji Zhou", "Robert Paolini", "J. Andrew Bagnell", "Matthew T. Mason"], "venue": "In 2016 IEEE International Conference on Robotics and Automation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning", "author": ["M. Deisenroth", "C. Rasmussen", "D. Fox"], "venue": "In Robotics: Science and Systems (RSS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["S.M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["Katerina Fragkiadaki", "Pulkit Agrawal", "Sergey Levine", "Jitendra Malik"], "venue": "In ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning physics from dynamical scenes", "author": ["Tomer D. Ullman", "Andreas Stuhlm\u00fcller", "Noah D. Goodman", "Joshua B. Tenenbaum"], "venue": "In Proceedings of the Thirty-Sixth Annual Conference of the Cognitive Science Society,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Se3-nets: Learning rigid body motion using deep neural networks", "author": ["Arunkumar Byravan", "Dieter Fox"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Deep visual foresight for planning robot motion. ICRA 2017", "author": ["Chelsea Finn", "Sergey Levine"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "A comparative evaluation of approximate probabilistic simulation and deep neural networks as accounts of human physical scene understanding", "author": ["Renqiao Zhang", "Jiajun Wu", "Chengkai Zhang", "William T. Freeman", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "To fall or not to fall: A visual approach to physical stability prediction", "author": ["Wenbin Li", "Seyedmajid Azimi", "Ales Leonardis", "Mario Fritz"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning physical intuition of block towers by example", "author": ["Adam Lerer", "Sam Gross", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["Lerrel Pinto", "Dhiraj Gandhi", "Yuanfeng Han", "Yong-Lae Park", "Abhinav Gupta"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Visual stability prediction and its application to manipulation", "author": ["Wenbin Li", "Ales Leonardis", "Mario Fritz"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning, 2016", "author": ["Misha Denil", "Pulkit Agrawal", "Tejas D. Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Shape and pose recovery from planar pushing", "author": ["Kuan-Ting Yu", "John J. Leonard", "Alberto Rodriguez"], "venue": "In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Entropy Search for Information-Efficient Global Optimization", "author": ["Philipp Hennig", "Christian J. Schuler"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "More than a million ways to be pushed: A high-fidelity experimental data set of planar pushing", "author": ["Kuan-Ting Yu", "Maria Bauza", "Nima Fazeli", "Alberto Rodriguez"], "venue": "arXiv preprint arXiv:1604.04038,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Policy search for motor primitives in robotics", "author": ["Jens Kober", "Jan R Peters"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "First, a real robot is used to perform some random pushing action with an object on a tabletop [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Examples of popular physics engines frequently used in robotics include Bullet [2], MuJoCo [3], DART [4], PhysX [5], Havok [6], ODE [7], and GraspIt! [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "A survey and a comparison of these tools are given in [9].", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Examples of these techniques include model-based reinforcement learning for instance [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Several cognitive models that combine Bayesian inference with approximate knowledge of Newtonian physics have been proposed recently [11]\u2013[13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Several cognitive models that combine Bayesian inference with approximate knowledge of Newtonian physics have been proposed recently [11]\u2013[13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": "Model-based approaches [14]\u2013[18] rely on accurate models for objects and their properties.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "Model-based approaches [14]\u2013[18] rely on accurate models for objects and their properties.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "A physics-based simulation was used in [14] for predicting the effects of pushing actions, but the authors considered only flat, well-separated objects on a smooth surface.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "A nonparametric approach was used in [16] for learning the outcome of pushing large objects (furniture).", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "A Markov Decision Process (MDP) is used in [17] for modeling interactions between objects, however, only simulation results on pushing were reported in that work.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "Other Bayesian model-based techniques, such as PILCO [19], have been proven efficient in utilizing a small amount of data for learning dynamical models and optimal policies.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "Another alternative, which is becoming increasingly popular, addresses these challenges through end-to-end learning [1], [20]\u2013[31].", "startOffset": 116, "endOffset": 119}, {"referenceID": 13, "context": "Another alternative, which is becoming increasingly popular, addresses these challenges through end-to-end learning [1], [20]\u2013[31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 23, "context": "Another alternative, which is becoming increasingly popular, addresses these challenges through end-to-end learning [1], [20]\u2013[31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Note that there is a significant body of work on learning sliding models of objects using white-box optimization [15], [18], [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "Note that there is a significant body of work on learning sliding models of objects using white-box optimization [15], [18], [32].", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "Note that there is a significant body of work on learning sliding models of objects using white-box optimization [15], [18], [32].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "A drawback of white-box methods is that they are often used only in simple setups, such as pushing planar objects [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "This paper formulates this challenge in a Bayesian optimization framework, which uses the Entropy Search technique presented in [33].", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "Readers can find more details in textbooks on how Gaussian processes are updated from data and how to get the GP belief p on unknown function E from data points E(\u03b8i) [34].", "startOffset": 167, "endOffset": 171}, {"referenceID": 27, "context": "Additionally, a large scale planar push dataset [37] was also used to validate the proposed method.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "Greedy Entropy Search achieved lower error in both (a) the data collected with Motoman and (b) the planar push dataset [37].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "The proposed method was also tested using a large scale pushing dataset [37].", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "approach with a model-free reinforcement learning method: Policy learning by Weighting Exploration with the Returns (PoWER) [38].", "startOffset": 124, "endOffset": 128}], "year": 2017, "abstractText": "We consider the problem of a robot learning the mechanical properties of objects through physical interaction with the object, and introduce a practical, data-efficient approach for identifying the motion models of these objects. The proposed method utilizes a physics engine, where the robot seeks to identify the inertial and friction parameters of the object by simulating its motion under different values of the parameters and identifying those that result in a simulation which matches the observed real motions. The problem is solved in a Bayesian optimization framework. The same framework is used for both identifying the model of an object online and searching for a policy that would minimize a given cost function according to the identified model. Experimental results both in simulation and using a real robot indicate that the proposed method outperforms state-of-the-art model-free reinforcement learning approaches.", "creator": "LaTeX with hyperref package"}}}