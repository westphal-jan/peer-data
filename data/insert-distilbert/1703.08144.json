{"id": "1703.08144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Note Value Recognition for Piano Transcription Using Markov Random Fields", "abstract": "although this extended paper presents virtually a general statistical method for music transcription that operators can either estimate score processing times of note onsets and offsets from polyphonic midi performance signals. because performed note durations can deviate largely from score - indicated values, previous methods had address the problem of not being able to accurately estimate offset score times ( or note values ) and thus could only output independently incomplete string musical scores. based on observations that the pitch context and onset score event times are influential on the configuration of note values, we construct a hypothetical context - tree model that provides prior distributions of note values each using these parameters features and combine it with a performance model in the framework of markov random fields. evaluation results showed that our method reduces'the average error rate by around 40 percent compared to existing / simple methods. we also confirmed that, in our model, improving the audio score model plays a more important role than the performance model, \u201d and it automatically captures the voice structure by unsupervised learning.", "histories": [["v1", "Thu, 23 Mar 2017 17:07:14 GMT  (548kb,D)", "http://arxiv.org/abs/1703.08144v1", "12 pages, 15 figures, version submitted to IEEE/ACM TASLP"], ["v2", "Tue, 27 Jun 2017 22:26:42 GMT  (873kb,D)", "http://arxiv.org/abs/1703.08144v2", "13 pages, 16 figures, version accepted to IEEE/ACM TASLP"], ["v3", "Fri, 7 Jul 2017 13:10:15 GMT  (873kb,D)", "http://arxiv.org/abs/1703.08144v3", "13 pages, 16 figures, version accepted to IEEE/ACM TASLP, minor revision"]], "COMMENTS": "12 pages, 15 figures, version submitted to IEEE/ACM TASLP", "reviews": [], "SUBJECTS": "cs.AI cs.SD", "authors": ["eita nakamura", "kazuyoshi yoshii", "simon dixon"], "accepted": false, "id": "1703.08144"}, "pdf": {"name": "1703.08144.pdf", "metadata": {"source": "CRF", "title": "Note Value Recognition for Rhythm Transcription Using a Markov Random Field Model for Musical Scores and Performances of Piano Music", "authors": ["Eita Nakamura", "Simon Dixon"], "emails": ["enakamura@sap.ist.i.kyoto-u.ac.jp."], "sections": [{"heading": null, "text": "Index Terms\u2014Music transcription, symbolic music processing, statistical music language model, model for polyphonic musical scores, Markov random field.\nI. INTRODUCTION Music transcription is one of the most fundamental and challenging problems in music information processing [1], [2]. This problem, which involves conversion of audio signals into symbolic musical scores, can be divided into two subproblems, pitch analysis and rhythm transcription, which are often studied separately. Pitch analysis aims to convert the audio signals into the form of a piano roll, which can be represented as a MIDI signal, and multi-pitch analysis methods for polyphonic music have been extensively studied [3]\u2013[6]. Rhythm transcription, on the other hand, aims to convert a MIDI signal into a musical score by locating note onsets and offsets in a musical time direction (score time) [7]\u2013[16]. In order to track time-varying tempo, beat tracking is employed to locate beat positions in music audio signals [17]\u2013[21].\nAlthough most studies on rhythm transcription and beat tracking have focused on estimating onset score times, to obtain complete musical scores it is necessary to locate note offsets, or equivalently, identify note values. The configuration\nE. Nakamura is with the Graduate School of Informatics, Kyoto University, Kyoto 606-8501, Japan. He is supported by the JSPS research fellowship (PD). Electric address: enakamura@sap.ist.i.kyoto-u.ac.jp. This work was done while he was a visiting researcher at Queen Mary University of London.\nK. Yoshii is with the Graduate School of Informatics, Kyoto University, Kyoto 606-8501, Japan.\nS. Dixon is with the School of Electronic Engineering and Computer Science, Queen Mary University of London, London E1 4NS, UK.\nManuscript received XX, YY; revised XX, YY.\nof note values is especially important to describe the acoustic and interpretative nature of polyphonic music where there are multiple voices and the overlapping of notes produces different harmonies. Note value recognition has been addressed only in a few studies [10], [14] and the results of this study reveal that it is a non-trivial problem.\nThe difficulty of the problem arises from the fact that observed note durations in performances deviate largely from the score-indicated lengths so that the use of a prior (language) model for musical scores is crucial. Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23]. In particular, building a model at the symbolic level of musical notes (as opposed to the frame level of audio processing) that properly describes the multiple-voice structure while retaining computational tractability is a remaining problem.\nThe purpose of this paper is to investigate the problem of note value recognition using a statistical approach (Fig. 1). We formulate the problem as a post-processing step of estimating offset score times given onset score times obtained by rhythm transcription methods for note onsets. Firstly, we present results of statistical analyses and point out that the information of onset score times and the pitch context together with interdependence between note values provide clues for model construction. Secondly, we propose a Markov\nar X\niv :1\n70 3.\n08 14\n4v 1\n[ cs\n.A I]\n2 3\nM ar\n2 01\n7\nrandom field model that integrates a prior model for musical scores and a performance model that relates note values and actual durations (Sec. IV). To determine an optimal set of contexts/features for the score model from data, we develop a statistical learning method based on context-tree clustering [24]\u2013[26], which is an adaptation of statistical decision tree analysis. Finally, results of systematic evaluations of the proposed method and baseline methods are presented (Sec. V).\nThe contributions of this study are as follows. We formulate a statistical learning method to construct a highly predictive prior model for note values and quantitatively demonstrate its importance for the first time. The discussions cover simple methods and more sophisticated machine learning techniques and the evaluation results can serve as a reference for the state-of-the-art. Our problem is formulated in a general setting following previous studies on rhythm transcription and the method is applicable to a wide range of existing methods of onset rhythm transcription. Results of statistical analyses and learning in Secs. III and IV can also serve as a useful guide for research using other approaches such as rule-based methods and neural networks. Lastly, source code of our algorithms and evaluation tools is available from the accompanying web page [27] to facilitate future comparisons and applications."}, {"heading": "II. RELATED WORK", "text": "Before beginning the main discussion, let us review previous studies related to this paper.\nThere have been many studies on converting MIDI performance signals into a form of musical score. Older studies [7], [8] used rule-based methods and networks in attempts to model the process of human perception of musical rhythm. Since around 2000, various statistical models have been proposed to combine the statistical nature of note sequences in musical scores and that of temporal fluctuations in music performance. The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16]. The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos. Recently a merged-output HMM incorporating the multiple-voice structure has been proposed [16]. Temperley [14] proposed a score model similar to the metrical Markov model in which the hierarchical metrical structure is explicitly described. There are also studies that investigated probabilistic context-free grammar models [15].\nA recent study [16] reported results of systematic evaluation of (onset) rhythm transcription methods. Two data sets, polyrhythmic data and non-polyrhythmic data, were used and it was shown that HMM-based methods generally performed better than others and the merged-output HMM was most effective for polyrhythmic data. In addition to the accuracy of recognising onset beat positions, the metrical HMM has the advantage of being able to estimate metrical structure, i.e. the metre (duple or triple) and bar (or down beat) positions, and to avoid grammatically incorrect score representations that appeared in other HMMs.\nAs mentioned above, there have been only a few studies that discussed the recognition of note values in addition to onset score times. Takeda et al. [10] applied a similar method of estimating onset score times to estimating note values of monophonic performances and reported that the recognition accuracy dropped from 97.3% to 59.7% if rests are included. Temperley\u2019s Melisma Analyzer [14], based on a statistical model, outputs estimated onset and offset beat positions together with voice information for polyphonic music. There, offset score times are chosen from one of the following tactus beats according to some probabilities, or chosen as the onset position of the next note of the same voice. The recognition accuracy of note values has not been reported."}, {"heading": "III. PRELIMINARY OBSERVATIONS AND ANALYSES", "text": "In the following, we explain basic facts about the structure of polyphonic piano scores and discuss how it is important and non-trivial to recognise note values for such music based on observations and statistical analyses on music data. This provides motivations for the architecture of our model. Some terminology and notions used in this paper are also introduced."}, {"heading": "A. Structure of Polyphonic Musical Scores", "text": "To discuss recognition of note values in polyphonic piano music, we first explain the structure of polyphonic scores. The left-hand and right-hand parts are usually written in separate staffs and each staff can contain several voices, or streams of notes (Fig. 2(a)). In piano scores, each voice can contain chords and the number of voices can vary locally. Hereafter we use the word chords to indicate those within one voice. Except for rare cases of partial ties in chords, notes in a chord must have simultaneous onset and offset score times. This means that the offset score time of a note must be equal to or earlier than the onset score time of the next note/chord of the same voice. In the latter case, the note is followed by a rest. Such rests are rare [14] and thus the configuration of note values and the voice structure are inter-related.\nThe importance of voice structure in the description of note values can also be understood by comparing a polyphonic score with a reduced score obtained by putting all notes with simultaneous onsets into a chord and forming one \u2018big voice\u2019 without any rests as in Fig. 2(b). Since these two scores are the same in terms of onset score times, the differences are only in offset score times. One can see that appropriate voice structure is necessary to recover correct note values from the reduced\nscore. It can also be confirmed that note values are influential to realise the expected acoustic effect of polyphonic music. Because one can automatically obtain the reduced score given the onset score times, recovering the polyphonic score as in Fig. 2(a) from the reduced score as in Fig. 2(b) is exactly the aim of note value recognition."}, {"heading": "B. Distribution of Durations in Music Performances", "text": "A natural approach to recover note values from MIDI performances is finding those note values that best fit the actual note durations in the performances. In this paper, duration always means the time length measured in physical time, as opposed to a note value that is described in terms of score time/beat units. To relate durations to note values, one needs the (local) tempo that provides the conversion ratio. Although estimating tempos from MIDI performances is a nontrivial problem (see Sec. IV), let us suppose here they are given, for simplicity. Given a local tempo and a note value, one can calculate an expected duration, and conversely, one can estimate a note value given a local tempo and actual duration.\nFig. 3 shows distributions of the ratios of actual durations in performances and the durations expected from note values and tempos estimated from onset times (used performance data is described in Sec. IV-D). Because information of key-press and key-release times for each note and pedal movements can be obtained from MIDI signals, one can define the following two durations. The key-holding duration is the time interval between key-press and key-release times and the damperlifting duration is obtained by extending the offset time as long as the sustain/sostenuto pedal is held. As can be seen from the figure, both distributions have large variances and thus precise prediction of note values is impossible by using only the observed values. As mentioned previously [12], [14], this makes note value recognition a difficult problem and it has often been avoided in previous studies. Additionally, due to the large deviations of durations, most tempo estimation methods use only onset time information.\nA similar situation happens in speech recognition where the presence of acoustic variations and noise makes it difficult to extract symbolic text information by pure feature extraction. Similarly to using a prior language model, which was the key to improve the accuracy of speech recognition [28], a prior model for musical scores (score model) would be a key to solving our problem, which we seek in this paper."}, {"heading": "C. Hints for Constructing a Score Model", "text": "The simplest score model for note value recognition would be a discrete probability distribution over a set of note values. For example, one can consider the following 15 types of note values (where 1 = whole note, 1/4 = quarter note, etc.):{\n1 32 , 1 48 , 1 16 , 1 24 , 3 32 , 1 8 , 1 12 , 3 16 , 1 4 , 1 6 , 3 8 , 1 2 , 1 3 , 3 4 , 1 } . (1)\nThe distribution taken from a score data set (see Sec. IV-D) is shown in Fig. 5(a). Although the distribution has clear tendencies, it is not sufficiently sharp to compensate the large variance of the duration distributions. We will confirm that this simple model yields a poor recognition accuracy in Sec. V-B.\nHints for constructing a score model can be obtained by again observing the example in Fig. 2. It is observed that most notes in the reduced score have the same note values as in the original score, and even when they do not, the offset score times tend to correspond with one of the onset score times of following notes. To explain this more precisely in a statistical way, we define an onset cluster as the set of all notes with simultaneous onsets in the score and inter-onset note values (IONVs) as the intervals between onset score times of succeeding onset clusters (Fig. 4). As in the figure, for later convenience, we define IONVs for each note, even though they are same for all notes in an onset cluster. If one counts frequencies that each note value matches one of the first ten IONVs (or none of them), the result is as shown in Fig. 5(b). We see that the distribution has lower entropy than that in Fig. 5(a) and the probability that note values would be different from any of the first ten IONVs is small (3.50% in our data). This suggests that a more efficient search space for note values can be obtained by using the onset score time information.\nEven more predictive distributions of note values can be obtained by using the pitch information. This is because neighbouring notes (either horizontally or vertically) in a voice tend to have close pitches, as discussed in studies on voice separation [29]\u2013[31]. For example, if we select notes that have a note within five semitones in the next onset cluster, the distribution of note values in the space of IONVs becomes as in Fig. 5(c), reflecting the fact that inserted rests are rare. On the other hand, if we impose a condition of having a note with five semitones in the second next onset cluster but not having any notes within 14 semitones in the next cluster, then the distribution becomes as in Fig. 5(d), which reflects the fact that this condition implies that the note has an adjacent note in the same voice in the second next onset cluster. These results suggest on one side that pitch information together with onset\nscore time information can provide distributions of note values with more predictive ability and on the other side that those distributions are highly dependent on the pitch context.\nAlthough so far we have considered note values as independent distributions, their interdependence can also provide clues in estimating note values. One such interdependence can be inferred from the logical constraint of voice structure described in Sec. III-A. As chordal notes have the same note values and they also tend to have close pitches, notes with simultaneous onset score times and close pitches tend to have identical note values. This is another case where pitch information has influence on the distribution of note values."}, {"heading": "D. Summary of the Section", "text": "Here we summarise the findings in this section: \u2022 The voice structure and the configuration of note values\nare inter-related and the logical constraints for musical scores induce interdependence between note values. \u2022 Performed durations contain large deviations from those implied by the score and a score model is crucial to accurately estimate note values from performance signals. \u2022 Information about onset score times provides an efficient search space for note values through the use of IONVs. In particular, the probability that a note value falls into one of the first ten IONVs is quite high. \u2022 The distribution of note values is highly dependent on the pitch context, which would be useful for improving their predictability.\nIn the rest of this paper, we construct a computational model to incorporate these findings and examine by numerical experiments how they quantitatively influence the accuracy of note value recognition."}, {"heading": "IV. PROPOSED METHOD", "text": ""}, {"heading": "A. Problem Statement", "text": "For rhythm transcription, the input is a MIDI performance signal, represented as a sequence of pitches, onset times and offset times (pn, tn, toffn , t\u0304 off n ) N n=1 where n is an index for musical notes and N is the number of notes. As explained in Sec. III-B, we can define two offset times for each note, the key-release time and damper-drop time, denoted by toffn and t\u0304offn . The corresponding key-holding and damper-lifting duration will be denoted by dn = toffn \u2212 tn and d\u0304n = t\u0304offn \u2212 tn. The aim is to recognise the score times of the note onsets and offsets, which are denoted by (\u03c4n, \u03c4offn ) N n=1. In general, \u03c4n and \u03c4offn take values in the set of rational numbers in units of a\nbeat unit, say, the whole-note length. For example, \u03c41 = 0 and \u03c4off1 = 1/4 means that the first note is at the beginning of the score and has a quarter-note length. We use the following notations for sequences: d = (dn)Nn=1, \u03c4 off = (\u03c4offn ) N n=1, etc. We call the difference rn = \u03c4offn \u2212 \u03c4n the note value. In this paper, we consider the situation that the onset score times \u03c4 are given as estimations from conventional onset rhythm transcription algorithms. In addition, we assume that a local tempo vn, which gives a smoothed ratio of the time interval and score time interval at each note n, is given. Local tempos v = (vn)Nn=1 can be obtained from the sequences t and \u03c4 by applying some smoothing methods such as Kalman smoothing and local averaging, and typically they can be obtained as outputs of onset rhythm transcription algorithms.\nIn summary, we set up the problem of note value recognition as estimating the sequence \u03c4 off (or r) with inputs p,d, d\u0304, \u03c4 and v. For concreteness, in this paper, we mainly use as \u03c4 and v the outputs from a method based on a metrical HMM (Sec. IV-B), but our method is applicable as a post-processing step for any rhythm transcription method that outputs \u03c4 ."}, {"heading": "B. Estimation of Onset Score Times and Local Tempos", "text": "To estimate onset score times \u03c4 and local tempos v from a MIDI performance (p, t, toff , t\u0304off), we use a metrical HMM [9], which is one of the most accurate onset rhythm transcription methods (Sec. II). Here we briefly review the model.\nIn the metrical HMM, the probability P (\u03c4 ) of the score is generated from a Markov process on periodically defined beat positions denoted by (sn)Nn=1 with sn \u2208 {1, . . . , G} (G is a period of beats such as a bar). The sequence s is generated with the initial and transition probabilities as\nP (s) = P (s1) N\u220f n=2 P (sn|sn\u22121). (2)\nWe interpret sn as \u03c4n modulo G, or more explicitly, we obtain \u03c4 incrementally as follows:\n\u03c41 = s1, (3)\n\u03c4n+1 = \u03c4n + { sn+1 \u2212 sn, if sn+1 > sn; G+ sn+1 \u2212 sn, if sn+1 \u2264 sn.\n(4)\nThat is, if sn+1 \u2264 sn, we interpret that sn+1 indicates the beat position in the next bar. With this understood, P (\u03c4 ) is equivalent to P (s) as long as rn \u2264 G for all n. An extension is possible to allow note onset intervals larger than G [32].\nIn constructing the performance model, local tempo variables v are introduced to describe the indeterminacy and temporal variations of tempos. The probability P (t,v|\u03c4 ) is decomposed as P (t|\u03c4 ,v)P (v) and each factor is described with the following Gaussian Markov process:\nP (vn|vn\u22121) = N(vn; vn\u22121, \u03c32v), (5) P (tn+1|tn, \u03c4n+1, \u03c4n, vn)\n= N(tn+1; tn + (\u03c4n+1 \u2212 \u03c4n)vn, \u03c32t ) (6) where N( \u00b7 ;\u00b5,\u03a3) denotes a normal distribution with mean \u00b5 and variance \u03a3, and \u03c3v and \u03c3t are standard deviations representing the degree of tempo variations and onset time\nfluctuations, respectively. An initial distribution for v1 is described similarly as a Gaussian N(v1; vini, \u03c32v,ini).\nAn algorithm to estimate onset score times and local tempos can be obtained from the above model by maximising the posterior probability P (\u03c4 ,v|t) \u221d P (t,v|\u03c4 )P (\u03c4 ). Although a direct application of the Viterbi algorithm is impossible due to the presence of continuous and discrete latent variables, discretisation of the tempo variables (typically into \u223c50 steps) yields a good approximation as discussed previously [20], [32]. Note that this method does not use the pitch and offset information to estimate onset score times, which is typical in conventional onset rhythm transcription methods. Since the period G and rhythmic properties encrypted in P (sn|sn\u22121) are dependent on the metre, in practice it is effective to consider multiple metrical HMMs corresponding to different metres, such as duple metre and triple metre, and choose the one with the maximum posterior probability in the stage of inference."}, {"heading": "C. Markov Random Field Model", "text": "Here we describe our main model. As explained in Sec. III, it is essential to combine a score model that enables prediction of note values given the input information of onset score times and pitches and a performance model that relates note values to actual durations realised in music performances. To enable tractable inference and efficient parameter estimation, one should typically decompose each model into component models that involve a smaller number of stochastic variables.\nAs a framework to combine such component models, we consider the following Markov random field (MRF):\nP (r|p,d, d\u0304, \u03c4 ,v) \u221d exp [ \u2212 N\u2211 n=1 H1(rn; \u03c4 ,p)\u2212 \u2211 (n,m)\u2208N H2(rn, rm)\n\u2212 N\u2211 n=1 H3(rn; dn, d\u0304n, vn) ] . (7)\nHere H1 (called the context model) represents the prior model for each note value that depends on the onset score times and pitches, H2 (the interdependence model) represents the interdependence of neighbouring pairs of note values (N denotes the set of neighbouring note pairs specified later) and H3 (the performance model) represents the likelihood model. Each term can be interpreted as an energy function that has small values when the arguments have higher probabilities. The explicit forms of these functions are given as follows:\nH1 = \u2212\u03b21lnP (rn; \u03c4 ,p), (8) H2 = \u2212\u03b22lnP (rn, rm), (9) H3 = \u2212\u03b231lnP (dn; rn, vn)\u2212 \u03b232lnP (d\u0304n; rn, vn). (10)\nEach energy function is constructed with a negative log probability function multiplied by a positive weight. These weights \u03b21, \u03b22, \u03b231 and \u03b232 are introduced to represent the relative importance of the component models. For example, if we take \u03b21 = \u03b231 = \u03b232 = 1 and \u03b22 = 0, the model reduces to a Naive Bayes model with the durations considered as features. For other values of \u03b2 s, the model is no longer a generative model for the durations but still a generative model for the note\nvalues, which are the only unknown variables in our problem. In the following we explain the component models in detail. Learning parameters including \u03b2 s is discussed in Sec. IV-D.\n1) Context Model: The context model H1 describes a prior distribution for note values that is conditionally dependent on given onset score times and pitches. To construct this model, one should first specify the sample space of rn, or, the set of possible values that each rn can take. Based on the observations in Sec. III, we consider the first ten IONVs as possible values of rn. Since rn can take other values in reality, we also consider a formally defined value \u2018other\u2019, which represents all other values of rn. Let\n\u2126r(n) = {IONV(n, 1), . . . , IONV(n, 10), other} denote the sample space. Therefore P (rn; \u03c4 ,p) is considered as an 11-dimensional discrete distribution.\nAs we saw in Sec. III, the distribution P (rn; \u03c4 ,p) depends heavily on the pitch context. Based on our intuition that for each note n the succeeding notes with a close pitch are most influential on the voice structure, in this paper we use the feature vector cn = (cn(1), . . . , cn(10)) as a context of note n, where cn(k) denotes the unsigned pitch interval between note n and the closest pitch in its k-th next onset cluster. An example of the context is given in Fig. 6. Thus we have\nP (rn; \u03c4 ,p) = P (rn; cn(1), . . . , cn(10)). (11)\nWe remark that in general we can additionally consider different features (for example, metrical features) and our formulation in this section and in Sec. IV-D is valid independently of our particular choice of features.\nDue to the huge number of different contexts for notes, it is not practical to use Eq. (11) directly. With 88 pitches on a piano keyboard, each cn(k) can take 87 values and thus the right-hand side (RHS) of Eq. (11) has 11 \u00b7 8710 parameters (or slightly less free parameters after normalisation), which is computationally infeasible. (If one uses additional features, the number of parameters increases further.) To solve this problem, we use a context-tree model [24], [25], in which contexts are categorised according to a set of criteria that are represented as a tree (as in decision tree analysis) and all contexts in one category have the same probability distribution.\nFormally, a context-tree model is defined as follows. Here we consider a general context c = (c(1), . . . , c(F )), which is\nan F -dimensional feature vector. We assume that the set of possible values for c(f) is an ordered set for all f = 1, . . . , F and denote it by Rf . Let us denote the leaf nodes of a binary tree T by \u2202T . Each node \u03bd \u2208 T is associated with a set of contexts denoted by C\u03bd . In particular, for the root node 0 \u2208 T , C0 is the set of all contexts (R1\u00d7\u00b7 \u00b7 \u00b7\u00d7RF ). Each internal node \u03bd \u2208 T \\ \u2202T is associated with a criterion \u03b3(\u03bd) for selecting a subset of C\u03bd . A criterion \u03b3 = (f\u03b3 , \u03ba\u03b3) is defined as a pair of a feature dimension f\u03b3 \u2208 {1, . . . , F} and a cut \u03ba\u03b3 \u2208 Rf\u03b3 . The criterion divides a set of contexts C into two subsets as\nCL(\u03b3) = {c \u2208 C | c(f\u03b3) \u2264 \u03ba\u03b3}, (12) CR(\u03b3) = {c \u2208 C | c(f\u03b3) > \u03ba\u03b3}, (13)\nso that CL(\u03b3) \u2229 CR(\u03b3) = \u03c6 and CL(\u03b3) \u222a CR(\u03b3) = C. Now denoting the left and right child of \u03bd \u2208 T \\\u2202T by \u03bdL and \u03bdR, their sets of contexts are defined as C\u03bdL = C\u03bd\u2229CL0 (\u03b3(\u03bd)) and C\u03bdR = C\u03bd \u2229 CR0 (\u03b3(\u03bd)), which recursively defines a context tree (T, f, \u03ba) (Fig. 7). By definition, a context is associated to a unique leaf node: for all c \u2208 C0 there exists a unique \u03bb \u2208 \u2202T such that c \u2208 C\u03bb. We denote such a leaf by \u03bb(c). Finally, for each node \u03bd \u2208 T , a probability distribution Q\u03bd( \u00b7 ) is associated. Now we can define the probability PT ( \u00b7 ; c) as\nPT ( \u00b7 ; c) = Q\u03bb(c)( \u00b7 ). (14) The tuple T = (T, f, \u03ba,Q) defines a context-tree model.\nFor a context-tree model with L leaves, the number of parameters for the distribution of note values is now reduced to 11L. In general a model with a larger tree size has more ability to approximate Eq. (11) at the cost of an increasing number of model parameters. The next problem is to find the optimal tree size and the optimal criterion for each internal node. We will explain this in Sec. IV-D1.\n2) Interdependence Model: Although the distribution of note values in the context model is dependent on the pitch context, it is independently defined for each note value. As explained in Sec. III, interdependence of note values is also important since it arises from logical constraint on the voice structure. Such interdependence can be described with a joint probability of note values of a pair of notes in H2. As in the context model, we consider the set \u2126r as a sample space for note values so that the joint probability P (rn, rm) for notes n and m has 112 parameters.\nThe choice of the set of neighbouring note pairs N in Eq. (7) is most important for the interdependence model. In order to capture the voice structure we define N as\nN = {(n,m) | \u03c4n = \u03c4m, |pn \u2212 pm| \u2264 \u03b4nbh} (15) where \u03b4nbh is a parameter to define the vicinity of the pitch. The value of \u03b4nbh is determined from data (see Sec. IV-D4).\n3) Performance Model: The performance model is constructed with the probability of actual durations in performances given a note value and a local tempo. Since we can use two durations dn and d\u0304n, two distributions, P (dn; rn, vn) and P (d\u0304n; rn, vn), are considered for each note as in the RHS of Eq. (10). To regulate the effect of varying tempos and avoid the increase in the complexity of the model to handle possibly many types of note values, we consider distributions over normalised durations, d\u2032n = dn/(rnvn) and d\u0304 \u2032 n = d\u0304n/(rnvn), as we did in Sec. III. We therefore assume\nP (dn; rn, vn) = g(d \u2032 n) and P (d\u0304n; rn, vn) = g\u0304(d\u0304 \u2032 n) (16)\nwhere g and g\u0304 are one-dimensional probability distributions supported on positive real numbers.\nThe histograms corresponding to g and g\u0304 taken from performance data described in Sec. IV-D are illustrated in Fig. 3. One can recognise two (one) peak(s) for the distribution of normalised key-holding (damper-lifting) durations. Since theoretical forms of these distributions are unknown, we use as phenomenologically fitting distributions the following generalised inverse-Gaussian (GIG) distribution:\nGIG(x; a, b, p) = (a/b)p/2\n2Kp(2 \u221a ab) xp\u22121e\u2212(ax+b/x) (17)\nwhere a, b > 0 and p \u2208 R are parameters and Kp denotes the modified Bessel function of the second kind. The GIG distributions are supported on positive real numbers and include the gamma (a \u2192 0), inverse-gamma (b \u2192 0) and inverseGaussian (p = \u22121/2) distributions as special cases. Since a GIG distribution has only one peak, we use a mixture of GIG distributions to represent g. We parameterise g and g\u0304 as\ng(x) = w1GIG(x; a1, b1, p1) + w2GIG(x; a2, b2, p2), (18) g\u0304(x) = GIG(x; a3, b3, p3) (19)\nwhere w1 and w2 = 1 \u2212 w1 are mixture weights. Parameter values obtained from data are given in Sec. IV-D3."}, {"heading": "D. Model Learning", "text": "Similarly as the language model and the acoustic model for a speech recognition system are generally trained separately with different data, our three component models can be trained separately and combined afterwards to determine the optimal weights (the \u03b2 s). The context model and the interdependence model can be learned with musical score data and we used a dataset of 148 classical piano pieces (with 3.4\u00d7106 notes) by various composers1. On the other hand, the performance model requires performance data aligned with reference scores. The used data consisted of 180 performances (60 phrases \u00d7 3 different players) by various composers and various performers\n1The lists of used pieces for the score data and the performance data are available at the accompanying web page [27].\nthat are mostly collected from publicly available MIDI performances recorded in international piano competitions [33]. Due to the lack of abundant data, we used the same performance data for training and evaluation. Because the number of parameters for the performance model is small (ten independent parameters in g and g\u0304 and two weight parameters) and they are not fine-tunable, there should be little concern about overfitting here and most comparative evaluations in Sec. V are done with equal conditions. (See Sec. V-C for influence of varying weights.) To avoid overfitting, the score data and the performance data contained no overlapping musical pieces (at least in units of movements). Learning methods for the component models are described in the following sections and Sec. IV-D4 describes the optimisation of the \u03b2 s.\n1) Learning the Context Model: The context-tree model can be learned by growing the tree based on the maximum likelihood (ML) principle, which is called context-tree clustering. This is usually done by recursively splitting a node that minimises the likelihood [24]. Although it is not essentially new, we describe the learning method here for the readers\u2019 convenience because context-tree clustering is not commonly used in the field of music informatics and in articles for speech processing (where it is widely used) the notations are adapted for the case with Gaussian distributions, which is not ours.\nLet xi = (ri, ci) denote a sample extracted from score data, where i denotes a note in the score data, ri denotes an element in \u2126r(i) and ci denotes the context of note i. The set of all samples will be denoted by x = (xi)Ii=1. The log likelihood LT (x) of a context-tree model T = (T, f, \u03ba,Q) is given as\nLT (x) = I\u2211 i=1 lnPT (xi) = I\u2211 i=1 lnQ\u03bb(ci)(xi)\n= \u2211 \u03bb\u2208\u2202T \u2211 i: ci\u2208C\u03bb q\u03bb(xi) (20)\nwhere in the second line we decomposed the samples according to the criteria of the leaves and hereafter we denote q\u03bd( \u00b7 ) = lnQ\u03bd( \u00b7 ) for each node \u03bd. The parameters for each distribution Q\u03bd for node \u03bd \u2208 T are learned from the samples {xi|ci \u2208 C\u03bd} according to the ML method. We implicitly understand that all Q s are already learned in this way.\nGiven a context tree T (m) (one begins with a tree T (0) containing only the root node and proceeds m = 0, 1, 2, . . . as follows), one of the leaves \u03bb \u2208 \u2202T (m) is split according to some additional criterion \u03b3(\u03bb). Let us denote the expanded context-tree model by T (m)\u03bb . Since T (m) \u03bb is same as T (m) except for the new leaves \u03bbL and \u03bbR, the difference of log likelihoods \u2206L(\u03bb) = L\nT (m) \u03bb (x)\u2212 LT (m)(x) is given as\u2211 i: ci\u2208C\u03bbL q\u03bbL(xi) + \u2211 i: ci\u2208C\u03bbR q\u03bbR(xi)\u2212 \u2211 i: ci\u2208C\u03bb q\u03bb(xi). (21) Note that \u2206L(\u03bb) \u2265 0 since Q\u03bbL and Q\u03bbR have the ML. Now the leaf \u03bb\u2217 and the criterion \u03b3(\u03bb\u2217) that maximise \u2206L(\u03bb) are selected for growing the context tree: T (m+1) = T (m)\u03bb\u2217 .\nAccording to the above ML criterion, the context tree can be expanded to the point where all samples are completely separated by contexts, for which the model often suffers from overfitting. To avoid this and find an optimal tree size\naccording to the data, the minimal description length (MDL) criterion for model selection can be used [26], [34]. The MDL `M(x) for a model M with parameters \u03b8M is given as\n`M(x) = \u2212log2P (x; \u03b8\u0302M) + |M|\n2 log2I (22)\nwhere I is the length of x, |M| is the number of free parameters of model M and \u03b8\u0302M denotes the ML estimate of \u03b8M according to data x. Here, the first term in the RHS is the negative log likelihood, which in general decreases when the model\u2019s complexity increases. On the other hand, the second term increases when the number of model parameters increases. Thus a model that minimises the MDL is chosen by a trade off of the model\u2019s precision and complexity. The MDL criterion is justified by an information-theoretic argument [34].\nFor our context-tree model, each Q is an 11-dimensional discrete distribution and has ten free parameters, and therefore the increase of parameters by expanding a node is ten. Substituting this into Eq. (22), we find\n\u2206`(\u03bb\u2217) = `T (m+1)(x)\u2212 `T (m)(x) = \u2212\u2206L(\u03bb\u2217)/(ln 2) + (10/2)log2I. (23)\nIn summary, the context tree is expanded by splitting the optimal leaf \u03bb\u2217, up to a step where \u2206`(\u03bb\u2217) becomes positive.\nWith our score data of 3.4\u00d7 106 musical notes, the learned context tree had 132 leaves. A subtree is illustrated in Fig. 8 where the node ID is shown in square brackets and the labels 1, . . . , 10 in the distribution show those probabilities correspond to IONV(1), . . . , IONV(10) and the label 0 is assigned to the \u2018other\u2019. For example, one finds a distribution with a sharp peak at IONV(1) in node 2 whose contexts satisfy c(1) \u2264 2. This can be interpreted as follows: if note n has a pitch within 2 semitones in the next onset cluster, then it is highly probable that they are in the same voice and note n has rn = IONV(n, 1). On the other hand, the IONV(2) has the largest probability in node 7 (the distribution is the same one as in Fig. 5(d)) with contexts satisfying c(2) \u2264 5 and c(1) > 14, whose interpretation was explained in Sec. III-C. Similar interpretations can be made for node 11 and other nodes. These results show that the context tree tries to capture\nthe voice structure through the pitch context. As this is induced from data in an unsupervised way, it serves as an informationscientific confirmation that the voice structure has a strong influence on the configuration of note values.\n2) Learning the Interdependence Model: The interdependence model for each \u03b4nbh can be directly learned from score data: for all note pairs defined by Eq. (15), one obtains the joint probability of their note values. The obtained results for \u03b4nbh = 12 is shown in Fig. 9 where the same labels are used as in Fig. 8. The diagonal elements, which have the largest probability in each row and column, clearly reflect the constraint of chordal notes having the same note values.\nSince the interdependence model is by itself not as precise a generative model as the context model and these models are not independent, we optimise \u03b4nbh in combination with the context model. This is described in Sec. IV-D4, together with the optimisation of the weights. In preparation for this, we learned the joint probability for each of \u03b4nbh = 0, 1, . . . , 15.\n3) Learning the Performance Model: The parameters for the performance model in Eqs. (18) and (19) are learned from the distributions given in Fig. 3. We performed a grid search for minimising the squared fitting error for each distribution. The obtained values are the following:\na1 = 2.24\u00b1 0.02, b1 = 0.24\u00b1 0.01, p1 = 0.69\u00b1 0.01, a2 = 13.8\u00b1 0.1, b2 = 15.2\u00b1 0.1, p2 = \u22121.22\u00b1 0.04, w1 = 0.814\u00b1 0.004, w2 = 0.186\u00b1 0.004, a3 = 0.94\u00b1 0.01, b3 = 0.51\u00b1 0.01, p3 = 0.80\u00b1 0.01.\nThe fitting curves are illustrated as red curves and the two GIGs in g(x) are illustrated as green broken curves in Fig. 3.\n4) Optimisation of the Weights: Since the three component models for the MRF model in Eq. (7) are not independent, the weights \u03b2 should be obtained by simultaneous optimisation using performance data in general. However, since the amount of score data at hand is significantly larger than that of the performance data, we optimise the weights in a more efficient way. Namely, we first optimise \u03b21 and \u03b22 with the score data and then optimise \u03b231 and \u03b232 with the performance data (with fixed \u03b21 and \u03b22). When examining the influence of varying these weights in Sec. V-C, we will discuss that the influence of this sub-optimisation procedure is seemingly small.\nWe obtained the first two weights simultaneously with \u03b4nbh\nby the ML principle with the following results:\n\u03b2\u03021 = 0.965\u00b10.005, \u03b2\u03022 = 0.03\u00b10.005, \u03b4\u0302nbh = 12. (24) The result \u03b2\u03022 \u03b2\u03021 indicates that the interdependence model has little influence in the score model. Although it seems somewhat contradictory to the results in Sec. IV-D2 at first sight, we can understand this by noticing that both the context model and interdependence model make use of pitch proximity to capture the voice structure. The former model uses pitch proximity in the horizontal (time) direction and the latter model does so in the vertical (pitch) direction, and they have overlapping effects since whenever a note pair (say, note n and n\u2032) in an onset cluster have close pitches, they tend to share notes in succeeding onset clusters with close pitches (see e.g. the chords in the left-hand part in the score in Fig. 15). Thus note n and n\u2032 tend to obey similar distributions in the context model. Since the interdependence model is weaker in terms of predictive ability, this results in small \u03b2\u03022.\nWe optimised \u03b231 and \u03b232 according to the accuracy of note value recognition (more precisely, the average error rate defined in Sec. V-A) and the obtained values are as follows:\n\u03b2\u030231 = 0.21\u00b1 0.01, \u03b2\u030232 = 0.003\u00b1 0.001. (25) One can notice that \u03b2\u030232 \u03b2\u030231, which can be explained by the significantly larger variance of the distribution of damperlifting durations than that of key-holding durations in Fig. 3. On the other hand, the result that \u03b2\u030231 is considerably smaller than \u03b2\u03021 can be interpreted as that the score model has more importance for estimating note values (in our model). The effect of varying weights is examined in Sec. V-C.\nE. Inference Algorithm and Implementation\nWe can develop a note value recognition algorithm based on the maximisation of the probability in Eq. (7) with respect to r. As a search space, we consider \u2126r(n)\\{other} for each rn. Without H2, the probability is independent for each rn and the optimisation is straightforward. With H2, we should optimise those rn s connected in N simultaneously. Since there are only vertical interdependencies in our model, the optimisation can be done independently for each onset cluster. With J notes in an onset cluster, the set of candidate note values has size 10J . Occasionally J can be ten or more and the computation time can be too large. Since the probability of r having higher IONVs is small (Fig. 5(b)), cutoffs are placed on the order of IONVs when J > 6 in our implementation.\nOur implementation of the MRF model and the metrical HMM for onset rhythm transcription and tempo estimation is available [27]. A tempo estimation algorithm based on a Kalman smoother is also provided for applying our method to results of other onset rhythm transcriptions that do not include tempo information as output."}, {"heading": "V. EVALUATION", "text": ""}, {"heading": "A. Evaluation Measures", "text": "We first define evaluation measures used in our study. For each note n = 1, . . . , N , let rcn and r e n be the correct and\nestimated note values. Then the error rate E is defined as\nE = 1 N N\u2211 n=1 I(ren 6= rcn) (26)\nwhere I(C) is 1 if condition C is true and 0 otherwise. This measure does not take into account how close the estimation is to the correct value when they are not exactly equal. Alternatively one can consider the averaged \u2018distance\u2019 between the estimated and correct note values. As such a measure we define the following scale error S:\nS = exp [ 1\nN \u2211 n |ln(ren/rcn)| ] . (27)\nThe difference and average is defined in the logarithmic domain to avoid bias for larger note values. S is unity if all note values are correctly estimated, and for example, S = 2 if all estimations are doubled or halved from the correct values.\nBecause of the ambiguity of defining the beat unit, score times estimated by rhythm transcription methods often have doubled, halved or other scaled values [16], [35], which should not be treated as complete errors. To handle such scaling ambiguity, we normalise note values with the first IONV as\nr\u2032en = r e n/IONV e(n, 1), (28) r\u2032cn = r c n/IONV c(n, 1) (29)\nwhere IONVe(n, 1) and IONVc(n, 1) is the first IONV defined for the estimated and correct score, respectively. Scaleinvariant evaluation measures can be obtained by applying Eqs. (26) and (27) for r\u2032en and r \u2032c n ."}, {"heading": "B. Comparative Evaluations", "text": "In this section, we evaluate the proposed method, a previously studied method [14] and a simple model discussed in Sec. III on our data set and compare them in terms of the accuracy of note value recognition.\n1) Setup: To study the contribution of the component models of our MRF model, we evaluated the full model, a model without the interdependence model (\u03b22 = 0), a model without the performance model (\u03b231 = \u03b232 = 0) and an MRF model with a context model having no (or a trivial) context tree, all applied to the result of onset rhythm transcription by the metrical HMM. In addition, we evaluated a method based on a simple prior distribution on note values (Fig. 5(a)) combined with an output probability P (dn; rn, vn) in Eq. (16), which uses no information of onset score times. For comparison, we evaluated the Melisma Analyzer (version 2) [14], which is to our knowledge the only major method that can estimate onset and offset score times, and we also applied post-processing by the proposed method on the onset score times obtained by the Melisma Analyzer. The used data is described in Sec. IV-D.\n2) Results: The piece-wise average error rates and scale errors are shown in Fig. 10 where the mean (AVE) over all pieces and the standard error for the mean (corresponding to 1\u03c3 deviation in the t-test) are also given. Out of the 180 performances, only 115 performances were properly processed by the Melisma Analyzer and are depicted in the figure. In addition, 30.0% of the note values estimated by the method were zero and scale errors were calculated without these values.\nOne can see that the Melisma Analyzer and the simple model without using the onset score time information have high error rates and the proposed methods clearly outperformed them.\nThe distributions of note-wise scale errors r\u2032e/r\u2032c for incorrect estimations (r\u2032e/r\u2032c 6= 1) in Fig. 11 show that the Melisma Analyzer (simple model) more often estimates note values shorter (longer) than the correct ones. For the simple model, this is because it mostly relies on, other than a relatively weak prior distribution in Fig. 5(a), the distribution of key-holding durations in Fig. 3(a), which has the highest peak position lower than its mean. For the Melisma Analyzer, the short and zero note values arise because the method quantises the onset and (key-release) offset times into analysis frames of 50 ms. Whereas the comparison is not fair in that the Melisma Analyzer can potentially identify grace notes with zero note values, which our data did not contain and our method cannot recognise, the rate (30.0%) is considerably higher than their typical frequency in piano scores.\nAmong the different conditions for the proposed method, the full model had the best accuracy and the case with no context tree had significantly worse results, showing a clear effect of the context model. Compared to the full model, the average error rate for the model without the performance model was worse but within 1\u03c3 deviation and the average scale error was significantly worse, indicating that the performance model has an effect in approximating the estimated note values to the correct ones. On the other hand, results without the interdependence model were slightly worse but almost the same as the full model, which is because of the small \u03b2\u03022."}, {"heading": "C. Examining the Proposed Model", "text": "Here we examine the proposed model in greater depth. 1) Error Analyses: To examine the effect of the component models, let us look at the distribution of the estimated note values in the space of IONVs (Fig. 12). Note that the distribution for the ground truth is essentially the same as that in Fig. 5(b) but slightly different because the data is different and the onset clusters here are defined with the result of onset rhythm transcription by the metrical HMM.\nFirstly, the model without a context tree assigns the first IONV to note values with a high probability (> 98%), indicating that estimated results by the model are almost the same as for the one-voice representation in Fig. 2(b). This is consistent with the results in Fig. 11 that this model tends to estimate note values shorter than the correct values. Secondly, one can notice that the model without the performance model has a higher probability for the first IONV and smaller probabilities for most of the later IONVs compared with the full model. This suggests that the performance model uses the information of actual durations to correct (or better approximate) the estimated note values more frequently to larger values, leading to decreased scale errors. Finally, the proportion of errors corresponding to note values that are larger than IONV(10) is about 0.8%, indicating that the effect of enlarging the search space of note values by including higher IONVs is limited.\n2) Influence of the context-tree size and weights: Fig. 13 shows the average error rates and scale errors for various sizes of the context tree. The case with only one leaf (not shown in the figure) is the same as the case without a context tree\nexplained above. The errors rapidly decreased as the tree size increased for small numbers of leaves and but changed only slightly above 50 leaves. There was a gap between the error rates for the cases with 50 and 75 leaves, for which we do not have a good explanation. Far above the predicted value (132 leaves) by the MDL criterion, the errors tended to increase slightly, confirming that it is close to the optimal choice.\nFig. 14 shows the average error rate and scale error when varying the weights from the values in Eqs. (24) and (25). First, variations by increasing and decreasing the weights by 50% are within 1\u03c3 statistical significance, showing that the error rates are not very sensitive to these parameters. Second, the values \u03b2\u03021 and \u03b2\u03022, which were optimised based on ML using the score data, are found to be optimal with respect to the error rate. Finally, the similar shapes of the curves when fixing \u03b21/\u03b22 and fixing \u03b231/\u03b232 show that their relative values influences the results more than their absolute values in the examined region. The results together with the large-variance nature of the distributions of durations in Fig. 3 suggest\nInput piano roll (key-holding)\nInput piano roll (damper-lifting)\n\u2030 \u0153\u2039 \u0153# \u0153 \u0153 \u0153 \u0153 \u0153# \u0153 \u0153# \u0153# \u0153\u02d9 \u0153# \u0153\u2039 \u0153 \u0153 \u0153 \u0153 \u0153# \u0153 \u0153 \u0153 \u0153 \u0153\u02d9 \u0153# \u0153# \u0153 \u0153 \u0153# \u0153#\nEstimation by HMM + MRF (scaled by 4/3)\n&?\n&?\n20 21 22 23\n# # \u2039 # \u2039 # # # ### # # # # \u2039 # # # # # # # # # # # # #\n(sec)\n(sec)\n20 21 22 23\n# # \u2039 # \u2039 # # # ### # # # # \u2039 # # # # # # # # # # # # #\nPedal\nthat it is likely that more elaborate fitting functions for the performance model would not improve the results significantly and also that the sub-optimisation procedure for \u03b2 s described in Sec. IV-D4 did not deteriorate the results much.\n3) Example Result: Let us discuss an example2 in Fig. 15, which has a typical texture of piano music with the left-hand part having harmonising chords and the right-hand part having melodic notes, both of which have multiple voices inside. By comparing the performed durations to the score, we can see that overall the damper-lifting durations are closer to the scoreindicated durations for the left-hand notes and the key-holding durations are closer for the right-hand notes. This is because pianists tend to lift the pedal when harmonising chords change. This example shows that the two types of durations provide complementary information and one should not rely on one of them. On the other hand, for most notes, the offset score time matches to the onset score time of a succeeding note with a close pitch, which is what our context model describes.\nThe result by the MRF model shows that the model uses the score and performance models complementarily to find the optimal estimation. The correctly estimated half notes (as IONV(6)), A4 in the first bar and E5 in the second bar, have a close pitch in the next onset cluster and the incorrect estimates as IONV(1) are avoided by using the duration (and perhaps because of the existence of very close pitches at the sixth next onset clusters). On the other hand, the quarter-note F#4 and D#4 in the left-hand part in the second bar could not be correctly estimated probably because the voice makes a big leap here, closer notes in the right-hand part succeed them and the key-holding durations are short.\n2Sound files are available at the accompanying web page [27]."}, {"heading": "VI. CONCLUSION AND DISCUSSION", "text": "We discussed note value recognition of polyphonic piano music based on an MRF model combining the score model and the performance model. As suggested in the discussion in Sec. III and confirmed by evaluation results, performed durations can deviate greatly from the score-indicated lengths and thus the performance model aline has little predictive ability. The construction of the score model is then the key to solve the problem. We formulated a context-tree model that can learn highly predictive distributions of note values from data, using onset score times and the pitch context. It was demonstrated that this score model brings significant improvements on the recognition accuracy.\nRefinement of the score model is possible in a number of ways. Using more features for the context-tree model could improve the results. Using other feature-based model learning schemes such as deep neural networks are similarly possible. The refinement and extension of the search space for note values is another issue since the set of the first ten IONVs used in this study loses a certain proportion of solutions. The result that the context-tree model learned to capture the voice structure suggests that building a model with explicit voice structure is also interesting for creating generative models to reduce reliance on arbitrarily chosen features.\nTo apply this work, the construction of a complete polyphonic music transcription system from audio signals to musical scores is attractive. The framework developed in this study can be combined with existing multi-pitch analysers [3]\u2013[6] for this purpose. It is worth mentioning that the performance model should be trained on piano rolls obtained with these methods since the distribution of durations would differ from that of recorded MIDI signals. Extension of the model to correct audio transcription errors such as note insertions and deletions would also be of great importance."}, {"heading": "ACKNOWLEDGEMENT", "text": "We are grateful to David Temperley for providing source code for the Melisma Analyzer. E. Nakamura would like to thank Shinji Takaki for useful discussions about context-tree clustering. This work is in part supported by JSPS KAKENHI Nos. 24220006, 26280089, 26700020, 15K16054, 16H01744 and 16J05486, JST OngaCREST and OngaACCEL projects, and the long-term overseas research fund by the Telecommunications Advancement Foundation."}], "references": [{"title": "Automatic Music Transcription: Challenges and Future Directions,", "author": ["E. Benetos", "S. Dixon", "D. Giannoulis", "H. Kirchhoff", "A. Klapuri"], "venue": "J. Intelligent Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Adaptive Harmonic Spectral Decomposition for Multiple Pitch Estimation,", "author": ["E. Vincent", "N. Bertin", "R. Badeau"], "venue": "IEEE TASLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Polyphonic Piano Transcription Using Non-Negative Matrix Factorisation with Group Sparsity,", "author": ["K. O\u2019Hanlon", "M.D. Plumbley"], "venue": "Proc. ICASSP, pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Infinite Superimposed Discrete All-Pole Modeling for Source-Filter Decomposition of Wavelet Spectrograms,", "author": ["K. Yoshii", "K. Itoyama", "M. Goto"], "venue": "Proc. ISMIR, pp. 86\u201392,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "An End-to-End Neural Network for Polyphonic Piano Music Transcription,", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "IEEE/ACM TASLP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Mental Processes: Studies in Cognitive Science", "author": ["H. Longuet-Higgins"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "The Quantization of Musical Time: A Connectionist Approach,", "author": ["P. Desain", "H. Honing"], "venue": "Comp. Mus. J.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "A Hybrid Graphical Model for Rhythmic Parsing,", "author": ["C. Raphael"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Hidden Markov Model for Automatic Transcription of MIDI Signals,", "author": ["H. Takeda", "T. Otsuki", "N. Saito", "M. Nakai", "H. Shimodaira", "S. Sagayama"], "venue": "Proc. MMSP, pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "A Learning-Based Quantization: Unsupervised Estimation of the Model Parameters,", "author": ["M. Hamanaka", "M. Goto", "H. Asoh", "N. Otsu"], "venue": "Proc. ICMC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization,", "author": ["A. Cemgil", "B. Kappen"], "venue": "J. Artificial Intelligence Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Signal-to-Score Music Transcription Using Graphical Models,", "author": ["E. Kapanci", "A. Pfeffer"], "venue": "Proc. IJCAI, pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "A Unified Probabilistic Model for Polyphonic Music Analysis,", "author": ["D. Temperley"], "venue": "J. New Music Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Probabilistic Model of Two-Dimensional Rhythm Tree Structure Representation for Automatic Transcription of Polyphonic MIDI Signals,", "author": ["M. Tsuchiya", "K. Ochiai", "H. Kameoka", "S. Sagayama"], "venue": "Proc. APSIPA, pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices,", "author": ["E. Nakamura", "K. Yoshii", "S. Sagayama"], "venue": "IEEE/ACM TASLP, to appear,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Automatic Extraction of Tempo and Beat from Expressive Performances,", "author": ["S. Dixon"], "venue": "J. New Music Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Simultaneous Beat and Downbeat- Tracking Using a Probabilistic Framework: Theory and Large-Scale Evaluation,", "author": ["G. Peeters", "H. Papadopoulos"], "venue": "IEEE TASLP, vol. 19,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Inferring Metrical Structure in Music Using Particle Filters,", "author": ["F. Krebs", "A. Holzapfel", "A.T. Cemgil", "G. Widmer"], "venue": "IEEE/ACM TASLP, vol. 23,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Downbeat Tracking with Multiple Features and Deep Neural Networks,", "author": ["S. Durand", "J.P. Bello", "B. David", "G. Richard"], "venue": "Proc. ICASSP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Context-Free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms,", "author": ["H. Kameoka", "K. Ochiai", "M. Nakano", "M. Tsuchiya", "S. Sagayama"], "venue": "Proc. ISMIR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling,", "author": ["S. Raczynski", "E. Vincent", "S. Sagayama"], "venue": "IEEE TASLP, vol. 21,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Tree-Based State Tying for High Accuracy Acoustic Modelling", "author": ["S. Young", "J.J. Odell", "P. Woodland"], "venue": "Proc. Human Lang. Techno.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Contextual Additive Structure for HMM-Based Speech Synthesis,", "author": ["S. Takaki", "Y. Nankaku", "K. Tokuda"], "venue": "IEEE J. Selected Topics in Signal Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "MDL-Based Context-Dependent Subword Modeling for Speech Recognition", "author": ["K. Shinoda", "T. Watanabe"], "venue": "J. Acoustical Soc. Japan (E),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Note Value Recognition for Polyphonic Music", "author": ["E. Nakamura", "K. Yoshii", "S. Dixon"], "venue": "[Online]. Available: http://anonymous574868. github.io/demo.html, Accessed on: Mar", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "An Introduction to the Application of the Theory of Probabilistic Functions of a Markov Process to Automatic Speech Recognition,", "author": ["S. Levinson", "L. Rabiner", "M. Sondhi"], "venue": "The Bell Sys. Tech. J., vol. 62,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1983}, {"title": "Voice and Stream: Perceptual and Computational Modeling of Voice Separation,", "author": ["E. Cambouropoulos"], "venue": "Music Perception,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "HMM-Based Voice Separation of MIDI Performance,", "author": ["A. McLeod", "M. Steedman"], "venue": "J. New Music Res.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Merged-Output HMM for Piano Fingering of Both Hands,", "author": ["E. Nakamura", "N. Ono", "S. Sagayama"], "venue": "Proc. ISMIR, pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Rhythm Transcription of MIDI Performances Based on Hierarchical Bayesian Modelling of Repetition and Modification of Musical Note Patterns,", "author": ["E. Nakamura", "K. Itoyama", "K. Yoshii"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1946}, {"title": "Universal Coding, Information, Prediction, and Estimation", "author": ["J. Rissanen"], "venue": "IEEE TIT,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1984}, {"title": "On Tempo Tracking: Tempogram Representation and Kalman Filtering,", "author": ["A.T. Cemgil", "B. Kappen", "P. Desain", "H. Honing"], "venue": "J. New Music Res.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Music transcription is one of the most fundamental and challenging problems in music information processing [1], [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Pitch analysis aims to convert the audio signals into the form of a piano roll, which can be represented as a MIDI signal, and multi-pitch analysis methods for polyphonic music have been extensively studied [3]\u2013[6].", "startOffset": 207, "endOffset": 210}, {"referenceID": 4, "context": "Pitch analysis aims to convert the audio signals into the form of a piano roll, which can be represented as a MIDI signal, and multi-pitch analysis methods for polyphonic music have been extensively studied [3]\u2013[6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "Rhythm transcription, on the other hand, aims to convert a MIDI signal into a musical score by locating note onsets and offsets in a musical time direction (score time) [7]\u2013[16].", "startOffset": 169, "endOffset": 172}, {"referenceID": 14, "context": "Rhythm transcription, on the other hand, aims to convert a MIDI signal into a musical score by locating note onsets and offsets in a musical time direction (score time) [7]\u2013[16].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "In order to track time-varying tempo, beat tracking is employed to locate beat positions in music audio signals [17]\u2013[21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "Note value recognition has been addressed only in a few studies [10], [14] and the results of this study reveal that it is a non-trivial problem.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Note value recognition has been addressed only in a few studies [10], [14] and the results of this study reveal that it is a non-trivial problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 180, "endOffset": 183}, {"referenceID": 12, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 185, "endOffset": 189}, {"referenceID": 14, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 19, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 197, "endOffset": 201}, {"referenceID": 20, "context": "Because of its structure with overlapping multiple streams (voices), construction of a language model for polyphonic music is challenging and gathers increasing attention recently [6], [14], [16], [22], [23].", "startOffset": 203, "endOffset": 207}, {"referenceID": 21, "context": "To determine an optimal set of contexts/features for the score model from data, we develop a statistical learning method based on context-tree clustering [24]\u2013[26], which is an adaptation of statistical decision tree analysis.", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "To determine an optimal set of contexts/features for the score model from data, we develop a statistical learning method based on context-tree clustering [24]\u2013[26], which is an adaptation of statistical decision tree analysis.", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "Lastly, source code of our algorithms and evaluation tools is available from the accompanying web page [27] to facilitate future comparisons and applications.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "Older studies [7], [8] used rule-based methods and networks in attempts to model the process of human perception of musical rhythm.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Older studies [7], [8] used rule-based methods and networks in attempts to model the process of human perception of musical rhythm.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16].", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "The most popular approach is to use hidden Markov models (HMMs) [9]\u2013[12], [16].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "The score is described either as a Markov process on beat positions (metrical Markov model) [9], [11], [12] or a Markov model of notes (note Markov model) [10], and the performance model is often constructed as a state-space model with latent variables describing locally defined tempos.", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "Recently a merged-output HMM incorporating the multiple-voice structure has been proposed [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "Temperley [14] proposed a score model similar to the metrical Markov model in which the hierarchical metrical structure is explicitly described.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "There are also studies that investigated probabilistic context-free grammar models [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "A recent study [16] reported results of systematic evaluation of (onset) rhythm transcription methods.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "[10] applied a similar method of estimating onset score times to estimating note values of monophonic performances and reported that the recognition accuracy dropped from 97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Temperley\u2019s Melisma Analyzer [14], based on a statistical model, outputs estimated onset and offset beat positions together with voice information for polyphonic music.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "Such rests are rare [14] and thus the configuration of note values and the voice structure are inter-related.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "As mentioned previously [12], [14], this makes note value recognition a difficult problem and it has often been avoided in previous studies.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "As mentioned previously [12], [14], this makes note value recognition a difficult problem and it has often been avoided in previous studies.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "Similarly to using a prior language model, which was the key to improve the accuracy of speech recognition [28], a prior model for musical scores (score model) would be a key to solving our problem, which we seek in this paper.", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "This is because neighbouring notes (either horizontally or vertically) in a voice tend to have close pitches, as discussed in studies on voice separation [29]\u2013[31].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "This is because neighbouring notes (either horizontally or vertically) in a voice tend to have close pitches, as discussed in studies on voice separation [29]\u2013[31].", "startOffset": 159, "endOffset": 163}, {"referenceID": 7, "context": "To estimate onset score times \u03c4 and local tempos v from a MIDI performance (p, t, t , t\u0304), we use a metrical HMM [9], which is one of the most accurate onset rhythm transcription methods (Sec.", "startOffset": 113, "endOffset": 116}, {"referenceID": 29, "context": "An extension is possible to allow note onset intervals larger than G [32].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "Although a direct application of the Viterbi algorithm is impossible due to the presence of continuous and discrete latent variables, discretisation of the tempo variables (typically into \u223c50 steps) yields a good approximation as discussed previously [20], [32].", "startOffset": 251, "endOffset": 255}, {"referenceID": 29, "context": "Although a direct application of the Viterbi algorithm is impossible due to the presence of continuous and discrete latent variables, discretisation of the tempo variables (typically into \u223c50 steps) yields a good approximation as discussed previously [20], [32].", "startOffset": 257, "endOffset": 261}, {"referenceID": 21, "context": ") To solve this problem, we use a context-tree model [24], [25], in which contexts are categorised according to a set of criteria that are represented as a tree (as in decision tree analysis) and all contexts in one category have the same probability distribution.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": ") To solve this problem, we use a context-tree model [24], [25], in which contexts are categorised according to a set of criteria that are represented as a tree (as in decision tree analysis) and all contexts in one category have the same probability distribution.", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "1The lists of used pieces for the score data and the performance data are available at the accompanying web page [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "This is usually done by recursively splitting a node that minimises the likelihood [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "[2] 141317 (41.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "47%) [3] 72191 (21.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "18%) [4] 127228 (37.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[5] 67978 (19.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] 43510 (12.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "76%) [7] 24468 (7.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "18%) [8] 59250 (17.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "[9] 17238 (5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] 8860 (2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "6%) [11] 8378 (2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "45%) [12] 42012 (12.", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "according to the data, the minimal description length (MDL) criterion for model selection can be used [26], [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "according to the data, the minimal description length (MDL) criterion for model selection can be used [26], [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "The MDL criterion is justified by an information-theoretic argument [34].", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "Our implementation of the MRF model and the metrical HMM for onset rhythm transcription and tempo estimation is available [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Because of the ambiguity of defining the beat unit, score times estimated by rhythm transcription methods often have doubled, halved or other scaled values [16], [35], which should not be treated as complete errors.", "startOffset": 156, "endOffset": 160}, {"referenceID": 31, "context": "Because of the ambiguity of defining the beat unit, score times estimated by rhythm transcription methods often have doubled, halved or other scaled values [16], [35], which should not be treated as complete errors.", "startOffset": 162, "endOffset": 166}, {"referenceID": 12, "context": "In this section, we evaluate the proposed method, a previously studied method [14] and a simple model discussed in Sec.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "For comparison, we evaluated the Melisma Analyzer (version 2) [14], which is to our knowledge the only major method that can estimate onset and offset score times, and we also applied post-processing by the proposed method on the onset score times obtained by the Melisma Analyzer.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "mod el No co ntext tree Melis ma + MRF Melis ma [1 4] No o nset info.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "Melis ma [1 4] No o nset info.", "startOffset": 9, "endOffset": 14}, {"referenceID": 12, "context": "mod el No co ntext tree Melis ma + MRF M elism a [14 ]", "startOffset": 49, "endOffset": 54}, {"referenceID": 2, "context": "Melis ma [1 4] No o nset info.", "startOffset": 9, "endOffset": 14}, {"referenceID": 2, "context": "mod el No co ntext tree Melis ma + MRF Melis ma [1 4] No o nset info.", "startOffset": 48, "endOffset": 53}, {"referenceID": 24, "context": "2Sound files are available at the accompanying web page [27].", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "The framework developed in this study can be combined with existing multi-pitch analysers [3]\u2013[6] for this purpose.", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "The framework developed in this study can be combined with existing multi-pitch analysers [3]\u2013[6] for this purpose.", "startOffset": 94, "endOffset": 97}], "year": 2017, "abstractText": "This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.", "creator": "LaTeX with hyperref package"}}}