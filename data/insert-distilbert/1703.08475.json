{"id": "1703.08475", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching", "abstract": "catastrophic factor forgetting is a problem which refers to persons losing the information of the first task after training from the second task, in continual learning of neural networks. to resolve this problem, we propose the incremental moment matching ( imm ), which uses the bayesian neural network framework. imm assumes that the posterior distribution of parameters of neural networks is approximated with gaussian distribution approximation and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. to make assuming our gaussian range assumption reasonable, the imm procedure utilizes various transfer learning techniques including weight bench transfer, l2 - norm of old and new parameters, and a newly proposed variant hypothesis of dropout using old parameters. we analyze our methods on the mnist and cifar - ci 10 datasets, and then evaluate them on a real - world life - log dataset collected using google glass. experimental results show that imm produces some state - of - the - art performance in a variety of datasets.", "histories": [["v1", "Fri, 24 Mar 2017 15:43:39 GMT  (1187kb,D)", "http://arxiv.org/abs/1703.08475v1", null], ["v2", "Thu, 7 Sep 2017 13:01:46 GMT  (1290kb,D)", "http://arxiv.org/abs/1703.08475v2", "Selected for a spotlight presentation at NIPS, 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sang-woo lee", "jin-hwa kim", "jaehyun jun", "jung-woo ha", "byoung-tak zhang"], "accepted": true, "id": "1703.08475"}, "pdf": {"name": "1703.08475.pdf", "metadata": {"source": "META", "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching", "authors": ["Sang-Woo Lee", "Jin-Hwa Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "emails": ["<btzhang@bi.snu.ac.kr>."], "sections": [{"heading": "1. Introduction", "text": "Catastrophic forgetting is a fundamental challenge for artificial general intelligence based on neural networks. When models using stochastic gradient descent learn a new task, the models often forget the information of previous tasks (McCloskey & Cohen, 1989). This online multi-task learning is described as continual learning. Recently, this classical problem has resurfaced with the renaissance of deep learning research (Goodfellow et al., 2013; Kirkpatrick et al., 2017; Fernando et al., 2017).\nTo resolve this catastrophic forgetting problem, we propose the incremental moment matching (IMM). IMM uses the framework of Bayesian neural networks, which implies\n1Seoul National University, Seoul, South Korea 2Naver Labs, Naver Corp., Bundang, South Korea 3Surromind Robotics, Seoul, South Korea. Correspondence to: Byoung-Tak Zhang <btzhang@bi.snu.ac.kr>.\nPreliminary Work. Copyright 2017 by the authors.\nFigure 1. Geometric illustration of incremental moment matching (IMM). Mean-IMM simply averages the parameter of two neural networks, whereas mode-IMM tries to find a maximum of the mixture of Gaussian posteriors. For IMM to be reasonable, the search space of the loss function between the posterior means \u00b51 and \u00b52 should be reasonably smooth and convex-like. To find a \u00b52 which satisfies this condition of a smooth and convex-like path from \u00b51, we propose applying various transfer techniques for the IMM procedure.\nthat an uncertainty is assumed for each parameter in neural networks, and that the posterior distribution is calculated (Blundell et al., 2015). The dimension of the posterior distribution is the number of parameters in the neural networks. To overcome catastrophic forgetting, IMM approximates the mixture of Gaussian posterior with each component representing a single task, to one Gaussian distribution that represents a single combined task.\nIMM incrementally matches the mean and the covariance of posterior distributions based on the assumption that the posterior distributions of parameters are Gaussian distributions. In this approach, the final posterior distribution can be calculated in a closed form, with the moments of posterior distributions trained for the old and new task, respectively. We introduce two novel methods of moment matching. One is mean-IMM, which simply averages the parameters of two networks for old and new tasks as the minimization of KL-divergence between the approximated posterior distribution and the mixture of two Gaussian posteriors. The other is mode-IMM, which merges the parameters of two networks using a Laplacian approximation (MacKay, 1992) to approximate a mode of the mixture of two Gaussian posteriors, which represent the parameters of the two\nar X\niv :1\n70 3.\n08 47\n5v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\n01 7\nnetworks.\nIn general, it is too na\u0131\u0308ve to assume that the final posterior distribution for the whole task is Gaussian. To make our IMM work, the search space of the loss function between the posterior means needs to be smooth and convex-like. In other words, there should not exist high cost barriers between the means of the two networks for an old and a new task. To make our assumption of Gaussian distribution for neural network reasonable, we applied three main transfer learning techniques on the IMM procedure: weight transfer, L2-norm of the old and the new parameters, and our newly proposed variant of dropout using the old parameters. The whole procedure of IMM is illustrated in Figure 1.\nWe evaluate our approach mainly on three experiments. First, we analyze the search space of the loss function represented by convolutional neural networks (CNN) in the CIFAR-10 image recognition benchmark. The experimental results show that utilizing weight transfer on IMM is critical to making our assumption of the Gaussian posterior reasonable. Second, our approach is compared with existing models on the MNIST image recognition benchmark. Our models work as well as the comparative models in cases where the comparative models work well, however our models show reasonably successful results in cases where the comparative models fail. Finally, our proposed approach is evaluated on a real-world life-log dataset collected using Google Glass. We show that our mean-IMM model achieved similar performance to the comparative ensemble model without additional computational cost for learning and inference."}, {"heading": "2. Previous Work", "text": ""}, {"heading": "2.1. Catastrophic Forgetting", "text": "One of the major approaches for catastrophic forgetting is to use an ensemble of neural networks. When a new task arrives, the algorithm makes a new network, and shares the representation between the tasks (Lee et al., 2016; Rusu et al., 2016). This approach often improves the performance of the whole task and is practically used in continual learning. However, this approach has a complexity issue, especially in inference, because the number of networks increases with the number of new tasks that need to be learned.\nAnother approach studies methods using implicit distributed storage of information, in typical stochastic gradient descent (SGD) learning. These methods use the idea of dropout or maxout for distributively storing the information for each task by making use of the large capacity of the neural network (Srivastava et al., 2013). Unfortunately, most studies following this approach had limited success\nand failed to preserve performance on the old task when an extreme change to the environment occurred (Goodfellow et al., 2013). Alternatively, Fernando et al. (2017) proposed PathNet, which extends the idea of the ensemble approach for parameter reuse within a single network. In PathNet, a neural network has 10 \u223c 20 modules in each layer and 3 \u223c 4 modules are picked for one task in each layer by an evolutionary approach. This method alleviates the complexity issue of the ensemble approach to continual learning in a plausible way.\nRecently, the concept of applying a regularization function to the network trained by the old task to learning a new task has received attention. Learning without forgetting (LwF), one example of this approach, uses the pseudo-training data from the old task (Li & Hoiem, 2016). Before learning the new task, LwF puts the training data of the new task into the old network, and uses the output as pseudo-labels of the pseudo-training data. By optimizing both the pseudotraining data of the old task and the real data of the new task, LwF tries to prevent catastrophic forgetting. Their experimental result shows that the performance of LwF approximates joint batch learning if the number of instances in the new dataset is small. Elastic weight consolidation (EWC), another example of this approach, uses sequential Bayesian estimation to update neural networks for continual learning (Kirkpatrick et al., 2017). In EWC, the posterior distribution trained by the previous task is used to update the new prior distribution. This new prior is used for learning the new posterior distribution of the new task in a Bayesian manner. Updating the parameter of complex hierarchical models by sequential Bayesian estimation is not new (Ghahramani, 2000). Sequential Bayes was also used to learn topic models from stream data in Broderick et al. (2013)."}, {"heading": "2.2. Bayesian Neural Networks", "text": "Bayesian neural networks (BNN) assume an uncertainty for the whole parameter in neural networks so that the posterior distribution can be obtained (Blundell et al., 2015). Previous studies have argued that BNN regularize better than NN, and provides a confidence interval for the output estimation of each input instance. The whole study on the BNN, to the best of our knowledge, uses Gaussian distribution as the posterior of the parameters. In the Gaussian assumption, because tracking whole information of covariance matrix is too expensive, researchers usually use only the diagonal term for the covariance matrix, in which case, the posterior distribution is fully factorized for each parameter. However, the methods using full covariance was also suggested recently (Louizos & Welling, 2016). For estimating covariance matrix most of the studies use stochastic gradient variational Bayes (SGVB), where a sampled point from the posterior distribution by Monte Carlo is used in\nthe training phases (Kingma & Welling, 2013). Alternatively, Kirkpatrick et al. (2017) approximated the covariance matrix as an inverse of Fisher matrix. This approximation makes the computational cost of the inference of covariance matrix smaller when the update of covariance information is not needed in the training phase. Our method follows the approach using Fisher matrix."}, {"heading": "3. Incremental Moment Matching", "text": "In incremental moment matching (IMM), the moments of posterior distributions are matched in an incremental way. Similarly to our method, Bayesian moment matching is used for sum-product networks, a kind of deep hierarchical probabilistic model (Rashwan et al., 2016). Though sumproduct networks are usually not scalable to large datasets, their online learning method is useful, and it achieves similar performance to the batch learner. Our method using the moment matching focuses on continual learning, unlike the previous method, deals with significantly different statistics between tasks.\nIn our work, we use a Gaussian distribution to approximate the posterior distribution of parameters. We want to find the optimal parameter \u00b5\u22171:2 and \u03a3 \u2217 1:2 of the Gaussian approximation function q for the whole task, from the parameter of the first and second task, (\u00b51,\u03a31) and (\u00b52,\u03a32) of q1 and q2, respectively.\np1:2 \u2261 p(\u03b8|X1:2, y1:2) \u2248 q1:2 \u2261 q(\u03b8|\u00b51:2,\u03a31:2) (1) p1 \u2261 p(\u03b8|X1, y1) \u2248 q1 \u2261 q(\u03b8|\u00b51,\u03a31) (2) p2 \u2261 p(\u03b8|X2, y2) \u2248 q2 \u2261 q(\u03b8|\u00b52,\u03a32) (3)\nq1:2 denotes an approximation of the true posterior distribtion p1:2 for the whole task, q1 denotes an approximation of the true posterior distribtion p1 for the first task, and q2 denotes an approximation of true posterior distribtion p2 for the second task. \u03b8 denotes the vector of parameters. (X1, y1) and (X2, y2) are the training dataset of the first and second tasks, respectively.\nNext, we propose two algorithms for incremental moment matching."}, {"heading": "3.1. Mean-based Incremental Moment Matching (mean-IMM)", "text": "The first method is mean-IMM, which averages the parameters of two networks in each layer, using an mixing ratio \u03b1. The objective function of mean-IMM is to minimize the\nKL-divergence between q1:2 and the mixture of q1 and q2.\n\u00b5\u22171:2,\u03a3 \u2217 1:2 = argmin\n\u00b51:2,\u03a31:2\nKL(q1:2||(1\u2212 \u03b1)q1 + \u03b1q2) (4)\nAccording to Zhang and Kwok (2010), the solution is as follows:\n\u00b5\u22171:2 = (1\u2212 \u03b1)\u00b51 + \u03b1\u00b52 (5) \u03a3\u22171:2 =(1\u2212 \u03b1)(\u03a31 + (\u00b51 \u2212 \u00b5\u22171:2)(\u00b51 \u2212 \u00b5\u22171:2)T )+\n\u03b1(\u03a32 + (\u00b52 \u2212 \u00b5\u22171:2)(\u00b52 \u2212 \u00b5\u22171:2)T ) (6)\nNotice that covariance information is not needed for meanIMM, since calculating \u00b5\u22171:2 does not require \u03a31 nor \u03a32, \u00b51 and \u00b52 are sufficient to perform the task."}, {"heading": "3.2. Mode-based Incremental Moment Matching (mode-IMM)", "text": "The second method is mode-IMM, a variant of mean-IMM which uses the covariance information of the posterior of Gaussian distribution. Though mean-IMM minimizes KLdivergence for a mixture of Gaussian (MoG), usually, a weighted average of two mean vectors of Gaussian distributions is not a mode of MoG. In discriminative learning, the maximum of the distribution is of primary interest. Critical points of MoG with two clusters satisfy the following equations (Ray & Lindsay, 2005):\n\u03b8 =((1\u2212 \u03b1)\u03a3\u221211 + \u03b1\u03a3 \u22121 2 ) \u22121\n((1\u2212 \u03b1)\u03a3\u221211 \u00b51 + \u03b1\u03a3 \u22121 2 \u00b52),\n(7)\nwhere \u03b1 = q2q1+q2 . Note that \u03b1 is a function of \u03b8, so \u03b8 cannot be calculated in a closed-form from Equation 7. However, the optimal \u03b8 is in the set {\u03b8|\u03b8 = ((1 \u2212 \u03b1)\u03a3\u221211 + \u03b1\u03a3\u221212 ) \u22121((1\u2212 \u03b1)\u03a3\u221211 \u00b51 + \u03b1\u03a3 \u22121 2 \u00b52), 0 < \u03b1 < 1}, which motivates our mode-IMM method. See Appendix A for more detail.\nWe simply fix \u03b1 to 1/2 to approximate MoG with Laplacian approximation, in which the logarithm of the function is expressed by Taylor expansion (MacKay, 1992). Using Laplacian approximation, the MoG is approximated as follows:\nlog q1 + log q2 =\n\u2212 1 2 \u03b8T (\u03a3\u221211 + \u03a3 \u22121 2 )\u03b8 + (\u03a3 \u22121 1 \u00b51 + \u03a3 \u22121 2 \u00b52)\u03b8 + C\n(8)\nIn this assumption, the optimal solution is:\n\u00b5\u22171:2 = (\u03a3 \u22121 1 + \u03a3 \u22121 2 ) \u22121(\u03a3\u221211 \u00b51 + \u03a3 \u22121 2 \u00b52) (9)\n\u03a3\u22171:2 = (\u03a3 \u22121 1 + \u03a3 \u22121 2 ) \u22121 (10)\nHere, we assume diagonal covariance matrices, which means that there is no correlation among parameters. This diagonal assumption is useful, since it decreases the number of parameters for each covariance matrix from O(d2) toO(d), where d is the dimension of the parameters. Based on this, the \u00b5\u22171:2 is defined as follows:\n\u00b5\u22171:2,v = \u00b51,v/\u03c3\n2 1,v + \u00b52,v/\u03c3 2 2,v\n1/\u03c321,v + 1/\u03c3 2 2,v\n(11)\nv denotes an index of the parameter vector.\nFor covariance, we use the inverse of Fisher information matrix (Kirkpatrick et al., 2017; Pascanu & Bengio, 2013). The main idea of this approximation is that the square of gradients for parameters is a good indicator of its precision, which is the inverse of the variance. Fisher information matrix for the ith task, Fi is defined as:\nFi = E\n[ \u2202\n\u2202\u00b5i ln p(y|x, \u00b5i) \u00b7\n\u2202\n\u2202\u00b5i ln p(y|x, \u00b5i)T\n] , (12)\nwhere the probability of the expectation follows x \u223c \u03c0i and y \u223c p(y|x, \u00b5i), where \u03c0i denotes an empirical distribution of Xi."}, {"heading": "3.3. Comparative Models", "text": "We also compare the work of Kirkpatrick et al. (2017) to the results of our framework. The mechanism of EWC follows sequential Bayesian estimation. EWC maximizes the following terms by gradient descent.\nlog p1:2 \u2248 log p(y2|X2, \u03b8) + \u03bb \u00b7 log q1 + C\n= log p(y2|X2, \u03b8)\u2212 \u03bb\n2 (\u03b8 \u2212 \u00b51)T\u03a3\u221211 (\u03b8 \u2212 \u00b51) + C \u2032\n(13)\nIn EWC, \u03a3\u221211 is also approximated by the diagonal term of F1. Though it is a weakness to only use the diagonal term of the covariance matrix, this assumption works reasonably in the experiments in their paper.\nWe also compare the work of Li and Hoiem (2016). Although LwF does not explicitly assume Bayesian, the approach can be represented nonetheless as follows:\nlog p1:2 \u2248 log p(y2|X2, \u03b8) + \u03bb \u00b7 log p(y\u03021|X2, \u03b8) + log p(\u03b8) (14)\nWhere y\u03021 is the output from \u00b51 with input X2."}, {"heading": "4. Transfer Techniques for Incremental Moment Matching", "text": "In general, the loss function of neural networks is not convex. If the parameters of two neural networks initialized independently are averaged, it might perform poorly. This is because there might be high cost barriers between the two parameters (Goodfellow et al., 2014). However, we will show that various transfer learning technique can be used to ease this problem, and make the assumption of Gaussian distribution for neural networks reasonable. In this section, we introduce three practical techniques for IMM procedure, weight-transfer, L2-transfer, and drop-transfer."}, {"heading": "4.1. Weight-Transfer", "text": "The first and most important idea is weight-transfer. The parameters for new task \u00b52 are initialized with the parameters for previous task \u00b51 (Yosinski et al., 2014). Note that this weight-transfer technique was also applied to an incremental ensemble approach for continual learning (Lee et al., 2016). In our experiments, the use of weight-transfer was critical to the continual learning performance. For this reason, the experiments on IMM in this paper use the weight-transfer technique as default.\nThe weight-transfer technique is motivated by the geometrical property of neural networks discovered in the previous work (Goodfellow et al., 2014). They found that there is a straight path from the initial point to the solution without any high cost barrier, in various types of neural networks and datasets. This discovery may suggest that the weighttransfer from the previous task to the new task makes a smooth loss surface between two solutions for the tasks, so that, the optimal solution for both tasks lies on the interpolated point of the two solutions.\nTo empirically validate the concept of weight-transfer, we use the linear path analysis proposed by Goodfellow et al. (2014). We randomly chose 18,000 instances from the training dataset of CIFAR-10, and divided them into three subsets of 6,000 instances, respectively. These three subsets are used for sequential training by CNN models, parameterized by \u03b81, \u03b82, and \u03b83, respectively. Here, \u03b82 is initialized from \u03b81, and then, \u03b83 is initialized from \u03b82, in the same way as weight-transfer. In the analysis, each loss and accuracy is evaluated at a series of points \u03b8 = \u03b81 + \u03b1(\u03b82 \u2212 \u03b81) + \u03b2(\u03b83 \u2212 \u03b82), varying \u03b1 and \u03b2.\nThe result of the geometric analysis is shown in Figure 2. In the figure, the loss surface of the model on each online subset is nearly convex. Figure 2 shows that the parameter at 13 (\u03b81 + \u03b82 + \u03b83), which is the same as the solution of mean-IMM, performs better than any other reference points \u03b81, \u03b82, or \u03b83. However, when \u03b82 is not initialized by \u03b81, the convex-like shape disappears, since there is a high cost\nbarrier between the loss function of \u03b81 and \u03b82.\nWe also found that the distances between two parameters ||\u03b81 \u2212 \u03b82||2 with and without weight-transfer are similar to each other, when two models are trained by the same hyperparameters. This result implies that the mechanism of the weight-transfer is different to the strategy of simply making two parameters close together in a Euclidean space."}, {"heading": "4.2. L2-transfer", "text": "The second idea is L2-transfer, which is a variant of L2regularization. In L2-transfer, a regularization term of the distance between \u00b51 and \u00b52 is added to the following objective function for finding \u00b52, where \u03bb is a hyperparameter:\nlog p(y2|X2, \u00b52)\u2212 \u03bb \u00b7 ||\u00b51 \u2212 \u00b52||22 (15)\nNormally, L2-transfer is used in continual learning with large \u03bb, and can also be interpreted as a special case of EWC where the prior distribution is Gaussian with \u03bbI as a covariance matrix. Two studies (Li & Hoiem, 2016; Kirkpatrick et al., 2017) compare their proposed model to L2transfer in their transfer and continual learning.\nUnlike the previous usage of large \u03bb, we use small \u03bb for the IMM procedure. In other words, \u00b52 is first trained by Equation 15 with small \u03bb, and \u00b51 and \u00b52 are then merged to \u00b51:2 in our IMM. Since we want to make the loss surface between \u00b51 and \u00b52 smooth, and not to minimize the distance between \u00b51 and \u00b52. In convex optimization, the L2-regularizer makes the convex function strictly convex. Similarly, we hope L2-transfer with small \u03bb will help find a \u00b52 with a convex-like loss space between \u00b51 and \u00b52."}, {"heading": "4.3. Drop-transfer", "text": "The third idea is drop-transfer, which is a novel method devised in the paper. Drop-transfer is a variant of dropout where \u00b51 is the zero point of the dropout procedure. In training phase, the following \u00b5\u03022i is used for the weight vector corresponding to the ith node \u00b52i with dropout ratio p:\n\u00b5\u03022i =\n{ \u00b51i, if ith node is turned off\n1 1\u2212p \u00b7 \u00b52i \u2212 p 1\u2212p \u00b7 \u00b51i, otherwise\n(16)\nNotice that the expectation of \u00b5\u03022i is \u00b52i.\nThere are studies (Srivastava et al., 2014; Baldi & Sadowski, 2013) that interpreted dropout as an exponential ensemble of weak learners. In this perspective, as the marginalization of output distribution over whole weak learner is intractable, the parameters multiplied by the inverse of dropout rate are used for test phase in the procedure. In other words, the parameters of the weak learners are, in effect, simply averaged oversampled learners by dropout.\nIn the perspective of the concept of drop-transfer in our continual learning setting, we hypothesize that the dropout process makes the averaged point of two arbitrary sampled points using Equation 16, a good estimator. In other words, the performance of the averaged parameters is better than the performance of the single points before averaging."}, {"heading": "5. Experimental Results", "text": ""}, {"heading": "5.1. Disjoint MNIST Experiment", "text": "We first evaluate our models on the disjoint MNIST experiment (Srivastava et al., 2013). In this experiment, the MNIST dataset is divided into two datasets: the first dataset\nconsists of only digits {0, 1, 2, 3, 4} and the second dataset consists of the remaining digits {5, 6, 7, 8, 9}. The first dataset contains 51.39% of the test data, and the second dataset contains 48.61%. This implies that if the model incorrectly classifies every image of the first dataset, the accuracy can be at most 48.61%. Our task is 10-class joint categorization, unlike in the previous work, which considers two independent tasks of 5-class categorization. Therefore, our task is more difficult than the task of the previous work. In our experiment, all models only use one 10- way softmax output layer, except for LwF. As LwF does not originally consider having one output layer for two datasets, we make an ensemble model for LwF with the softmax output of the two last layers, otherwise, LwF did not reach a performance of 50%.\nWe evaluate the models both on the natural setting and the tuned setting. The natural setting refers to the most natural hyperparameter in the equation of each algorithm. For mean-IMM, it is most natural to evenly average two parameters and 1/2 is the most natural \u03b1 value in Equation 4. For EWC, 1 is the most natural \u03bb value in Equation 13. For L2-transfer, there is no natural hyperparameter value in Equation 15, so we need to choose \u03bb heuristically for each task and dataset.\nThe tuned setting refers to using heuristic hand-tuned hyperparameters. Tuned hyperparameter-setting is often used in practice. For example, when the model needs to learn from the new task after learning from the old task, a low learning rate or early stopping is used.\nHyperparam in Table 1 denotes hyperparameter mainly searched in the tuned setting. In the SGD, epoch per dataset for the second task corresponds to the hyperparameter. The unit is how much of the network is trained from the whole\ndata at once. In the LwF, L2-transfer, and EWC, \u03bb in Equations 13, 15, and 14 corresponds to the hyperparameter, respectively. In the mean-IMM and mode-IMM, \u03b1 in Equations 5 and 7 corresponds to the hyperparameter, respectively. In the drop-transfer, dropout ratio p in Equation 16 corresponds to the hyperparameter. The whole explained hyperparameter is devised to balance the information between the old and new tasks. If 1/(1 + \u03bb) = 0 or \u03b1 = 0, the final network of the algorithms is the same as the network for the first task. If 1/(1 + \u03bb) = 1 or \u03b1 = 1, the final network is the same as the network for the last task.\nWe used multi-layer perceptrons (MLP) with [784-800- 800-10] as the number of nodes, ReLU as the activation function, and vanilla SGD as the optimizer for all of the experiments. We set the epoch per dataset as 10, unless otherwise noted. The entire IMM model uses weight-transfer to smooth the loss surface of the model. Without weighttransfer, our IMM model does not work at all. In SGD, dropout is used as proposed in Goodfellow et al. (2013).\nTable 1 shows the experimental results from the disjoint MNIST experiment. Each accuracy was measured by repeating 10 experiments. In the tuned experiment, the performance of the IMM models exceeds 90%, and the performance increases more when more transfer techniques are applied. Among all the models, weight-transfer + L2transfer + drop-transfer + mode-IMM performs the best and at greater than 94%. However, the comparative models fail to reach greater than 90%. Existing regularizer including dropout does not help much for comparative models. In our experimental setting, the usual SGD-based optimizers always perform less than 50%, because the biases of the output layer for the old task are always pushed to large negative values, which implies that our task is extremely\nhard.\nBetween the two types of moment matching, mode-IMM works more robustly than mean-IMM. In the natural hyperparameter setting, mean-IMM performs worse when more transfer techniques are applied. This is because when the network is trained for the new task it is affected too largely by information from the first task. The Figure 3 also shows that mode-IMM is robust with \u03b1 and the optimal \u03b1 of mean-IMM is often not 1/2."}, {"heading": "5.2. Shuffled MNIST Experiment", "text": "The second experiment is the shuffled MNIST experiment (Goodfellow et al., 2013; Kirkpatrick et al., 2017). In this experiment, the first dataset is the same as the original MNIST dataset. However, in the second dataset, the input pixels of all images are shuffled with a fixed, random permutation. Therefore, the difficulty of the two datasets is the same, though a different solution is required for each dataset. In previous work, EWC reaches the performance level of the batch learner, and it is argued that EWC overcomes catastrophic forgetting in some domains. In our set-\nting, the training data is divided into sets of 30,000 instances each. The experimental details are similar to the disjoint MNIST experiment, except all models are allowed to use dropout regularization. In mean-IMM, mode-IMM, and SGD, natural hyperparameter is used, where for EWC, an optimal \u03bb is searched.\nTable 2 shows the experimental results from the shuffled MNIST experiment. The result shows that our IMM paradigm does work and performs similarly to EWC where it is known that EWC performs well. We found that dropout regularization in the task makes both our models and comparative models perform better. The performance of meanIMM again does not reach the performance level of modeIMM. Figure 4 illustrates the performance of mean-IMM and mode-IMM. The result shows mode-IMM preserves the performance of two tasks. The performance of SGD can reach to near 97% in the tuned setting with a low learning rate and early stopping. However, we write 95.97% in the table according to the result from the natural setting because we want to emphasize that performance decreases with the usual learning rate and without early stopping.\nKirkpatrick et al. (2017) interpreted that the Fisher matrix F as weight importance in explaining their EWC model. However, this assumption does not always hold. In the shuffled MNIST experiment, their assumption on weight importance holds, since a large number of pixels always has a value of zero. For these areas, the corresponding element of the Fisher matrix is also zero. Therefore, EWC does work by allowing weights to change, which are not used in the previous task. On the other hand, mode-IMM also works by selectively balancing between two weights using variance information. However, these assumptions on weight importance do not hold in the disjoint MNIST\nexperiment. The most important weight in the disjoint MNIST experiment is the bias term in the output layer. Nevertheless, these elements of the Fisher matrix are not guaranteed to get high value nor can be used to balance the class distribution between the first and second task. We believe that using only the diagonal term for the covariance matrix in Bayesian neural networks is too na\u0131\u0308ve in general and that it is why EWC failed in the disjoint MNIST experiment. We think this could be alleviated in future work by using a more complex prior, such as a matrix Gaussian distribution while assuming correlations between nodes in the network (Louizos & Welling, 2016)."}, {"heading": "5.3. Lifelog Dataset", "text": "We evaluate the proposed mean-IMM on the Lifelog dataset. The Lifelog dataset consists of the 660,000 seconds of egocentric video stream data, collected over 46 days, from three participants, using Google Glass (Lee et al., 2016). Three class categories, location, sub-location, and activity, are labeled on each frame of video. The dataset consists of 10 days of training data and 4 days of test data in order of time for each participant respectively. In the framework of Lee et al. (2016), the network can be updated every day, but a new network can be made for the 3rd, 7th, and 10th day, with training data of 3, 4, and 3 days, respectively. Following this framework, our network is made in the 3rd, 7th, and 10th day, and then merged to previously trained networks. We used AlexNet pretrained by the ImageNet dataset (Krizhevsky et al., 2012). For mean-IMM, we do not calculate the covariance matrix nor the Fisher matrix. This prevents the computational cost of our contin-\nual learning from increasing.\nThe Experimental results on the Lifelog dataset are in Table 3. The performance of models without mean-IMM is from Lee et al. (2016). In Table 3, online fine-tuning and last-layer fine-tuning use one network, whereas dualmemory architeture (DMA), na\u0131\u0308ve incremental bagging, and incremental bagging w/ transfer use many networks. Our mean-IMM performs similar to the state-of-the-art ensemble model, although we only use one network."}, {"heading": "6. Conclusion", "text": "Catastrophic forgetting is a fundamental challenge for continual learning tasks. To solve this problem, we introduced incremental moment matching (IMM) and corresponding transfer techniques that improve the performance of IMM. We discussed not only how moment matching works but also how three transfer techniques in the paper make our Gaussian assumption reasonable. Three experimental results showed that IMM achieves state-of-the-art performance in a variety of datasets. It was also shown that our IMM procedure performs better, by applying the methods proposed in the previous works, including dropout regularization and L2-transfer. We believe our IMM could also be applied together with other methods, including EWC and PathNet, to further boost the performance. Using our continual learning setting, we discovered geometrical properties and a Bayesian perspective of modern deep neural networks."}, {"heading": "Acknowledgments", "text": "Sang-Woo Lee would like to thank Jiseob Kim, Min-Oh Heo, Christina Baek, and Heidi Tessmer for helpful comments and editing."}, {"heading": "APPENDIX A. Modes in the Mixture of Gaussian", "text": "According to Ray and Lindsay (2005), all the critical points \u03b8 of a mixture of Gaussian (MoG) with two components are in one curve as the following equation with 0 < \u03b1 < 1.\n\u03b8 = ((1\u2212 \u03b1)\u03a3\u221211 + \u03b1\u03a3 \u22121 2 ) \u22121((1\u2212 \u03b1)\u03a3\u221211 \u00b51 + \u03b1\u03a3 \u22121 2 \u00b52) (17)\nThe proof is as follows. Imagine two Gaussian distribution q1 and q2, such as in Equations 2 and 3, respectively.\nq1 \u2261 q1(\u03b8;\u00b51,\u03a31) = 1\u221a\n(2\u03c0)d|\u03a31| exp\n( \u22121\n2 (\u03b8 \u2212 \u00b51)T\u03a3\u221211 (\u03b8 \u2212 \u00b51)\n) (18)\nq2 \u2261 q2(\u03b8;\u00b52,\u03a32) = 1\u221a\n(2\u03c0)d|\u03a32| exp\n( \u22121\n2 (\u03b8 \u2212 \u00b52)T\u03a3\u221211 (\u03b8 \u2212 \u00b52)\n) (19)\nd is the dimension of the Gaussian distribution. Mixture of two Gaussian q1 and q2 with the equal mixing ratio (i.e., 1:1) is q1/2 + q2/2. The derivation of the MoG is as follows:\n\u2202(q1/2 + q2/2) \u2202\u03b8 = \u2212q1 2 (\u03a3\u221211 (\u03b8 \u2212 \u00b51))\u2212 q2 2 (\u03a3\u221212 (\u03b8 \u2212 \u00b52)) = 0 (20)\nIf we set Equation 20 to 0, to find all critical points, the following equation holds:\n\u03b8 = (q1\u03a3 \u22121 1 + q2\u03a3 \u22121 2 ) \u22121(q1\u03a3 \u22121 1 \u00b51 + q2\u03a3 \u22121 2 \u00b52) (21)\nWhen set \u03b1 to q2q1+q2 , Equation 17 holds.\nFor MoG with two components in d dimension, the number of modes can be at most d+ 1 (Ray & Ren, 2012). Therefore, it is hard to find all mode in high-dimensional Gaussian in general.\nThe property of critical points of a MoG with two components can be extended to the case of k components. The following equation holds:\n\u03b8 = ( k\u2211 i=1 \u03b1i\u03a3 \u22121 i ) \u22121( k\u2211 i=1 \u03b1i\u03a3 \u22121 i \u00b5i), (22)\nwhere 0 < \u03b1i < 1 for all i and \u2211 i \u03b1i = 1. There is no tight upper bound on the number of modes of MoG in general. There is a guess that, for all d, k \u2265 1, the upper bound is d+ k \u2212 1 choose d (Ame\u0301ndola et al., 2017)."}, {"heading": "APPENDIX B. Example Algorithms of Incremental Moment Matching", "text": "Algorithm 1 Mean-IMM with weight-transfer Input: data {(X1, y1),...,(X\u221e, y\u221e)}, weight w0 Output: wt repeat wt\u2217 \u2190 wt\u22121 Train(wt\u2217, Xt, yt) with L(wt\u2217, Xt, yt) +R(wt\u2217) wt \u2190 \u03b1wt\u2217 + (1\u2212 \u03b1)wt\u22121\nuntil forever\nAlgorithm 2 Mode-IMM with weight-transfer, L2-transfer, and Fisher matrix Input: data {(X1, y1),...,(X\u221e, y\u221e)}, weight w0 Output: wt repeat wt\u2217 \u2190 wt\u22121 Train(wt\u2217, Xt, yt) with L(wt\u2217, Xt, yt) +R(wt\u2217, wt\u22121\u2217) Ft\u2217 \u2190 CalculateFisherMatrix(wt\u2217, Xt, yt) wt \u2190 f(\u03b1Ft\u2217, wt\u2217, (1\u2212 \u03b1)Ft\u22121, wt\u22121) Ft \u2190 g(Ft\u2217, Ft\u22121, \u03b1)\nuntil forever\nTwo moment matching methods: mean-IMM and mode-IMM, and three transfer learning techniques: weight-transfer, L2transfer, and drop-transfer, can be combined to make a variety of continual learning algorithms. Among these combinations, two examples, mean-IMM with weight-transfer and mode-IMM with weight-transfer and L2-transfer, are described in Algorithm 1 and Algorithm 2, respectively. In Algorithm 2, functions f and g come from Equations 7 and 10, respectively."}], "references": [{"title": "Maximum number of modes of gaussian mixtures", "author": ["Am\u00e9ndola", "Carlos", "Engstr\u00f6m", "Alexander", "Haase", "Christian"], "venue": "arXiv preprint arXiv:1702.05066,", "citeRegEx": "Am\u00e9ndola et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Am\u00e9ndola et al\\.", "year": 2017}, {"title": "Understanding dropout", "author": ["Baldi", "Pierre", "Sadowski", "Peter J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Baldi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2013}, {"title": "Weight uncertainty in neural network", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Streaming variational bayes", "author": ["Broderick", "Tamara", "Boyd", "Nicholas", "Wibisono", "Andre", "Wilson", "Ashia C", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Broderick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2013}, {"title": "Pathnet: Evolution channels gradient descent in super neural networks", "author": ["Fernando", "Chrisantha", "Banarse", "Dylan", "Blundell", "Charles", "Zwols", "Yori", "Ha", "David", "Rusu", "Andrei A", "Pritzel", "Alexander", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1701.08734,", "citeRegEx": "Fernando et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2017}, {"title": "Online variational bayesian learning", "author": ["Ghahramani", "Zoubin"], "venue": "In NIPS workshop on Online Learning,", "citeRegEx": "Ghahramani and Zoubin.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani and Zoubin.", "year": 2000}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["Goodfellow", "Ian J", "Mirza", "Mehdi", "Xiao", "Da", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.6211,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Goodfellow", "Ian J", "Vinyals", "Oriol", "Saxe", "Andrew M"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning without forgetting", "author": ["Li", "Zhizhong", "Hoiem", "Derek"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Structured and efficient variational deep learning with matrix gaussian posteriors", "author": ["Louizos", "Christos", "Welling", "Max"], "venue": "arXiv preprint arXiv:1603.04733,", "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "A practical bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["McCloskey", "Michael", "Cohen", "Neal J"], "venue": "Psychology of learning and motivation,", "citeRegEx": "McCloskey et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCloskey et al\\.", "year": 1989}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Online and distributed bayesian moment matching for parameter learning in sum-product networks", "author": ["Rashwan", "Abdullah", "Zhao", "Han", "Poupart", "Pascal"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rashwan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rashwan et al\\.", "year": 2016}, {"title": "The topography of multivariate normal mixtures", "author": ["Ray", "Surajit", "Lindsay", "Bruce G"], "venue": "Annals of Statistics,", "citeRegEx": "Ray et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2005}, {"title": "On the upper bound of the number of modes of a multivariate normal mixture", "author": ["Ray", "Surajit", "Ren", "Dan"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Ray et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Compete to compute", "author": ["Srivastava", "Rupesh K", "Masci", "Jonathan", "Kazerounian", "Sohrob", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Simplifying mixture models through function approximation", "author": ["Zhang", "Kai", "Kwok", "James T"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Recently, this classical problem has resurfaced with the renaissance of deep learning research (Goodfellow et al., 2013; Kirkpatrick et al., 2017; Fernando et al., 2017).", "startOffset": 95, "endOffset": 169}, {"referenceID": 4, "context": "Recently, this classical problem has resurfaced with the renaissance of deep learning research (Goodfellow et al., 2013; Kirkpatrick et al., 2017; Fernando et al., 2017).", "startOffset": 95, "endOffset": 169}, {"referenceID": 2, "context": "that an uncertainty is assumed for each parameter in neural networks, and that the posterior distribution is calculated (Blundell et al., 2015).", "startOffset": 120, "endOffset": 143}, {"referenceID": 18, "context": "These methods use the idea of dropout or maxout for distributively storing the information for each task by making use of the large capacity of the neural network (Srivastava et al., 2013).", "startOffset": 163, "endOffset": 188}, {"referenceID": 6, "context": "Unfortunately, most studies following this approach had limited success and failed to preserve performance on the old task when an extreme change to the environment occurred (Goodfellow et al., 2013).", "startOffset": 174, "endOffset": 199}, {"referenceID": 4, "context": "Alternatively, Fernando et al. (2017) proposed PathNet, which extends the idea of the ensemble approach for parameter reuse within a single network.", "startOffset": 15, "endOffset": 38}, {"referenceID": 3, "context": "Sequential Bayes was also used to learn topic models from stream data in Broderick et al. (2013).", "startOffset": 73, "endOffset": 97}, {"referenceID": 2, "context": "Bayesian neural networks (BNN) assume an uncertainty for the whole parameter in neural networks so that the posterior distribution can be obtained (Blundell et al., 2015).", "startOffset": 147, "endOffset": 170}, {"referenceID": 14, "context": "Similarly to our method, Bayesian moment matching is used for sum-product networks, a kind of deep hierarchical probabilistic model (Rashwan et al., 2016).", "startOffset": 132, "endOffset": 154}, {"referenceID": 7, "context": "This is because there might be high cost barriers between the two parameters (Goodfellow et al., 2014).", "startOffset": 77, "endOffset": 102}, {"referenceID": 19, "context": "The parameters for new task \u03bc2 are initialized with the parameters for previous task \u03bc1 (Yosinski et al., 2014).", "startOffset": 88, "endOffset": 111}, {"referenceID": 7, "context": "The weight-transfer technique is motivated by the geometrical property of neural networks discovered in the previous work (Goodfellow et al., 2014).", "startOffset": 122, "endOffset": 147}, {"referenceID": 6, "context": "To empirically validate the concept of weight-transfer, we use the linear path analysis proposed by Goodfellow et al. (2014). We randomly chose 18,000 instances from the training dataset of CIFAR-10, and divided them into three subsets of 6,000 instances, respectively.", "startOffset": 100, "endOffset": 125}, {"referenceID": 18, "context": "We first evaluate our models on the disjoint MNIST experiment (Srivastava et al., 2013).", "startOffset": 62, "endOffset": 87}, {"referenceID": 6, "context": "Model Explanation of Natural Tuned Hyperparam Hyperparam Accuracy Hyperparam Accuracy SGD (Goodfellow et al., 2013) epoch per dataset 10 47.", "startOffset": 90, "endOffset": 115}, {"referenceID": 6, "context": "In SGD, dropout is used as proposed in Goodfellow et al. (2013).", "startOffset": 39, "endOffset": 64}, {"referenceID": 6, "context": "Model Accuracy SGD (Goodfellow et al., 2013) 96.", "startOffset": 19, "endOffset": 44}, {"referenceID": 6, "context": "The second experiment is the shuffled MNIST experiment (Goodfellow et al., 2013; Kirkpatrick et al., 2017).", "startOffset": 55, "endOffset": 106}], "year": 2017, "abstractText": "Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.", "creator": "LaTeX with hyperref package"}}}