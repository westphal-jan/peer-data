{"id": "1703.09137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Where to put the Image in an Image Caption Generator", "abstract": "finally when a neural language model model is used for caption generation, encoding the image information can be fed to the neural network either by directly incorporating it in a recurrent neural network - - conditioning the language model by injecting image features - - or in a layer following the recurrent neural network - - conditioning the language model by merging the image features. while merging implies that visual features only are bound at the end of the caption generation process, injecting can bind the visual features at a variety stages. in this paper we already empirically show that late binding is superior to early binding in terms of different evaluation metrics. this suggests that structurally the different modalities ( visual and linguistic ) for caption generation should not mutually be jointly encoded by the rnn ; rather, the multimodal integration should be delayed to a subsequent stage. furthermore, this suggests that recurrent neural networks evolution should not be viewed as actually generating text, but secondly only as encoding it for prediction problems in a subsequent layer.", "histories": [["v1", "Mon, 27 Mar 2017 15:13:49 GMT  (116kb,D)", "http://arxiv.org/abs/1703.09137v1", "under review, 29 pages, 5 figures, 6 tables"]], "COMMENTS": "under review, 29 pages, 5 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.CV", "authors": ["marc tanti", "albert gatt", "kenneth p camilleri university of malta)"], "accepted": false, "id": "1703.09137"}, "pdf": {"name": "1703.09137.pdf", "metadata": {"source": "CRF", "title": "Where to put the Image in an Image Caption Generator\u2217", "authors": ["Marc Tanti", "Albert Gatt"], "emails": ["marc.tanti.06@um.edu.mt", "albert.gatt@um.edu.mt", "kenneth.camilleri@um.edu.mt"], "sections": [{"heading": null, "text": "\u2217The research work disclosed in this publication is partially funded by the Endeavour Scholarship Scheme (Malta). Scholarships are part-financed by the European Union - European Social Fund (ESF) - Operational Programme II Cohesion Policy 2014-2020 Investing in human capital to create more opportunities and promote the well-being of society.\nar X\niv :1\n70 3.\n09 13\n7v 1"}, {"heading": "1 Introduction", "text": "Image caption generation1 is the task of generating a natural language description of the content of an image (Bernardi et al., 2016). One way to do this is to use a neural language model, a neural network that generates a sentence word by word. These work by using a recurrent neural network (RNN) that predicts the next word in the sentence based on its prefix or \u2018history\u2019. This predicted word can then be appended to the previous prefix in order to predict the word after that, and so on, until a whole sentence is generated. A simple neural language model can be extended into an image caption generator by conditioning predictions on image features. In other words, the language model takes as input not only the prefix, but also the image being captioned. This raises the question: At which stage should image information be introduced into the language model?\nRecent work on image captioning has answered this question in different ways, suggesting different views of the relationship between image and text in the caption generation task. To our knowledge, however, these different models and architectures have not been systematically compared. Yet, the question of where image information should feature in captioning is at the heart of a broader set of questions concerning how language can be grounded in perceptual information, questions which have been addressed by philosophers (Harnad, 1990) and AI practitioners (Roy, 2005).\nAs we will show in more detail in Section 2, differences in the way caption generation architectures treat image features can be characterised in terms of two distinct sets of decisions:\nConditioning by injecting versus conditioning by merging : A language model can be conditioned by injecting the image (see Figure 1a). In these \u2018inject\u2019 architectures, the image vector (usually derived from the activation values of a hidden layer in a convolutional neural network) is injected into the RNN, for example by treating the image vector on a par with a \u2018word\u2019 and including it as part of the caption prefix. The RNN is trained to vectorise the image-caption mixture in such a way that this vector can be used to predict the next word. On the other hand, a language model can be conditioned by merging the image (see Figure 1b). In the case of \u2018merge\u2019 architectures, the image is left out of the RNN subnetwork, such that the RNN handles only the caption prefix, that is, handles only purely linguistic information. After the prefix has been vectorised, the image vector is then merged with the prefix vector in a separate \u2018multimodal layer\u2019 which comes after the RNN subnetwork. Merging can be done by, for example, adding the two vectors together elementwise. In this case, the RNN is trained to only vectorise the prefix and the mixture is handled in a subsequent feedforward layer.\n1Throughout this paper we refer to textual descriptions of images as captions, although technically a caption is text that complements an image with extra information that is not available from the image. Specifically, the descriptions we talk about are \u2018concrete\u2019 and \u2018conceptual\u2019 image descriptions (Hodosh et al., 2013).\nIn short, if the image somehow influences the state of an RNN that is also processing words then it is being injected, otherwise it is being merged.\nEarly versus late inclusion of image features : As the foregoing description suggests, merge architectures tend to incorporate image features somewhat late in the generation process, that is, after processing the whole caption prefix. In the case of inject architectures, there is a broader range of possibilities, from injecting the image before processing any of the words in the caption prefix, to injecting the image after processing the last word of the prefix. Section 2 describes these options in more detail. The main point, however, is that it is possible to conceive of models in which visual information influences linguistic choices at different stages.\nThe main contribution of this paper is to present a systematic comparison of the different ways in which the \u2018conditioning\u2019 of linguistic choices based on visual information can be carried out, studying their implications for caption generator architectures. Thus, rather than seeking new results that improve on the state of the art, we seek to determine, based on an exhaustive evaluation of inject and merge architectures on a common dataset, where image features are best placed in the caption generation and image retrieval process.2\nFrom a scientific perspective, such a comparison would be useful for shedding light on the way language can be grounded in vision. Should images and text be intermixed throughout the process, or should they initially be kept separate before being combined in some multimodal layer? Many papers speak of RNNs as \u2018generating\u2019 text. Is this the case or are RNNs better viewed as encoders which\n2All the code used in our experiments is available at https://github.com/mtanti/ where-image\nvectorise a prefix so that the next neural network layer can predict the next word? Answers to these questions would help inform theories of how caption generation can be performed.\nFrom an engineering perspective, insights into the relative performance of different models could provide rules of thumb for selecting an architecture for image captioning, possibly for other tasks as well. This would make it easier to develop new architectures and new ways to perform caption generation.\nThe remainder of this paper is structured as follows. We first give an overview of related work, focusing in particular on the architectures used for caption generation. Section 3 discusses the architectures we compare, followed by a description of the data and experiments in Section 4. Results are presented and discussed in Section 5. We conclude with some general discussion and directions for future work."}, {"heading": "2 Background", "text": "In this section we discuss a number of recent image caption generation models, with emphasis on how the image conditions the neural language model, based on the distinction between inject and merge architectures illustrated in Figure 1. Before we discuss these models, we first outline the view of RNNs as used for language modelling that will be taken in the remainder of this paper. This serves as useful background to understanding the purpose of the illustrations used throughout."}, {"heading": "2.1 Ways of viewing recurrent neural networks", "text": "Traditionally, neural language models are depicted as in Figure 2a, where strings are thought of as being continuously generated. A new word is generated after each time step, with the RNN\u2019s state being combined with the last generated word in order to generate the next word. We refer to this as the \u2018continuous view\u2019. In this paper, we adopt a slightly different \u2013 though functionally identical\n\u2013 perspective on RNNs and their role in language models. This is illustrated in Figure 2b. We propose to think of the RNN in terms of a series of discontinuous snapshots over time, with each word being generated from the entire prefix of previous words and with the RNN\u2019s state being reinitialised each time. We refer to this as the \u2018discontinuous view\u2019.\nThe discontinuous perspective brings to light some similarities between RNNs as used in language modelling and the encoding RNNs in sequence-to-sequence models, for example in neural machine translation systems (Sutskever et al., 2014). In order to translate a source sentence into a target sentence, the whole source sentence is first encoded into a fixed vector which is then mapped into a translation in the target language. In language modelling, instead of translating a sentence into another sentence, we\u2019re \u2018translating\u2019 the prefix of a sentence into a possible next word."}, {"heading": "2.2 Types of architectures", "text": "In Section 1, we made a high-level distinction between architectures that merge linguistic and image features in a multimodal layer, and those that inject image features directly into the caption prefix encoding process. As we noted, merge architectures tend to incorporate image features relatively late, after linguistic strings have been encoded. By contrast, the inject architecture has been instantiated in a number of different ways in the literature which bind image features at different times, illustrated in Figure 3 and described below:\n\u2022 Init-inject: The RNN\u2019s initial state is set to be the image vector (or a vector derived from the image vector). This is an early binding architecture.\n\u2022 Pre-inject: The first input to the RNN is the image vector (or a vector derived from the image vector). The word vectors of the caption prefix come later. The image vector is thus treated as a first word in the prefix. This is an early binding architecture.\n\u2022 Par-inject: The image vector (or a vector derived from the image vector) serves as input to the RNN in parallel with the word vectors of the caption prefix, such that either (a) the RNN takes two separate inputs; or (b) the word vectors are combined with the image vector into a single input before being passed to the RNN. The image vector doesn\u2019t need to be exactly the same for each word, nor does it need to be included with every word. This is a mixed binding architecture.\n\u2022 Post-inject: This is a theoretical possibility, though no work actually adopts this architecture, to our knowledge. In this case, the last input to the RNN is the image vector (or a vector derived from the image vector) and is preceded by the word vectors of the caption prefix. The image vector is thus treated as the \u2018final word\u2019 in the prefix, a possibility that is probably easier to envisage under a discontinuous view of RNNs (Figure 2b). Post-inject architectures are mentioned here in view of our in-\nclusion of them in the experiments reported below. This is a late binding architecture.\nWith these distinctions in mind, we next discuss a selection of recent contributions, placing them in the context of this classification. Table 1 provides a summary of a wider selection of published architectures."}, {"heading": "2.2.1 Init-inject architectures", "text": "Architectures conforming to the init-inject model treat the image vector as the initial state of an RNN. These include (Devlin et al., 2015), who use a gated recurrent unit (GRU) model (Chung et al., 2014), that was initialised with an image vector. This was compared with a maximum entropy model (ME) that maps a bag of visual words extracted from an image onto the most likely complete caption.\nIn a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework a\u0301 la (Sutskever et al., 2014).\n(Wang et al., 2016) combine a simple RNN and an LSTM in one architecture, keeping them independent until their vectors are mixed together by weighted sum. This was motivated by the observation that the simple RNN and LSTM get better scores in different evaluation metrics.\n(Liu et al., 2016) describe two systems, both of which are optimized by discrete optimisation on caption quality metrics directly which are CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). During learning, the system\u2019s error is measured by generating (sampling) a number of continuations of every prefix in the training set into a whole caption and measuring the average quality of the full generated captions. This quality is used to guide optimisation. One of the two systems described performs init-inject on an image vector."}, {"heading": "2.2.2 Pre-inject architectures", "text": "Pre-inject models treat the image as though it were the first word in the prefix during training. (Vinyals et al., 2015) use an LSTM language model in this way, finding that pre-injection gives better results than par-injection (described below). Their hypothesis is that this is because par-injection makes the neural network overfit to image noise, which is always present with every word in the prefix.\nRather than injecting a low-level image vector, (Wu et al., 2015) opt for a higher-level approach by extracting attributes from an image and passing those as an input to the caption generator. This attributes vector is passed to the RNN by pre-injection.\n(Krause et al., 2016) generate paragraph-length captions in two stages. First, an LSTM which incorporates the image at every time step generates a sequence of vectors representing sentence topics; these constitute the input to another LSTM which generates the caption, by injecting the sentence vector with the caption prefix to predict the next word in the caption.\n(Rennie et al., 2016) describe two systems, an attention model and a nonattention model, both of which use discrete optimisation in order to optimize CIDEr directly. It was found that optimizing CIDEr directly is the best way to improve other metrics as well. During learning, the system\u2019s error is measured by generating (sampling) full captions and measuring the quality of the captions. This quality is used to guide optimisation. The non-attention model pre-injects the image vector.\n(Yao et al., 2016) describe five systems which mix image vectors and image attributes in different ways. Three of the systems work as follows: (a) preinjecting the attributes alone (b) injecting the image as a first word and the attributes as a second, (c) injecting the attributes as a first word and the image as a second. This last one gives the best performance of all five in terms of METEOR (Banerjee and Lavie, 2005)."}, {"heading": "2.2.3 Par-inject architectures", "text": "Par-injection combines image features and word features together when passing them to the RNN. For example, in two papers, Chen and colleagues (Chen and Zitnick, 2014, 2015) approach caption generation using two simple RNNs in parallel. One RNN is trained to predict the image vector given the caption prefix while the other RNN is trained to predict the next word given the caption prefix, the image vector, and the first RNN. The first RNN \u2018remembers\u2019 visual information about the caption prefix; this provides a more stable memory than a simple RNN\u2019s state.\nSimilarly, (Donahue et al., 2015) describes an image and video captioning system that puts two LSTMs in series after each other. However, it was found that injecting the image into the second (later) LSTM works better than injecting it into the first.\n(Karpathy and Fei-Fei, 2015) generate captions for multiple subregions within an image. The caption generation works by using a simple RNN that uses parinjection, but only with the first word, that is, only the first word is combined with the image. Everywhere else the image vector is treated as if it is an allzeros vector. It was found that this works better than par-injection with every word.\n(Zhou et al., 2016) also use par-injection, but the first word that the image is combined with is an all-zeros vector, that is, they use both pre-injection and par-injection. The image vector\u2019s elements are attended to differently for each word in the prefix, which is then par-injected into the RNN.\nOne of the two systems of (Rennie et al., 2016) has already been described in Section 2.2.2. The attention model performs par-inject of the attended image vector into the cell memory of an LSTM. Attention was found to improve the quality of the system."}, {"heading": "2.2.4 Merge architectures", "text": "Rather than combining image features together with linguistic features from within the RNN, merge architectures delay their combination until after the caption prefix has been vectorised. Among the exponents of this approach is the work of (Kiros et al., 2014a), who uses a log-bilinear language model (LBL) (Mnih and Hinton, 2007) to convert a caption prefix into a single vector which is later merged with the image vector in order to determine the next word in the caption.\nIn later work, (Kiros et al., 2014b) describe a caption generator that can be conditioned either on images or on full captions. This is because both image and caption vectors are embedded into a common multimodal space which maximises the similarity between corresponding images and captions. An image or caption vector is then merged with the output of a LBL in order to predict the next word in a prefix. To assist with the generation process, the expected parts of speech are passed to the LBL as well.\nSimilar to Kiros, (Mao et al., 2014, 2015a) use a multimodal layer, where an image vector is merged with a vector from a simple RNN. (Mao et al., 2015a) do a basic comparison between inject and merge architectures and find that merging works better than injecting. (Mao et al., 2015b) describe a subsequent development, where the vocabulary of a caption generator can be increased after training by only learning the new words\u2019 embeddings whilst leaving the rest of the network intact.\n(Hendricks et al., 2016) also uses a multimodal layer that merges the image vector with the caption prefix vector produced using an LSTM. This architecture keeps the image out of the LSTM by necessity, as the system is designed to be trained on images and sentences separately, to enable generalisation to objects not found in the training set. Hence, an object recogniser and a language model are trained separately. Their outputs are combined into a single vector which is used to predict the next word in the caption prefix. This makes it possible to improve the performance of each module separately using non-captioned images and free text."}, {"heading": "2.2.5 Mixed models", "text": "There is a growing amount of work that has explored a combination of different inject and/or merge strategies, most of which are attention-based models. This is the case in the work of (Xu et al., 2015), who describe an attention-based caption generator. Here, an LSTM is initialised using the full image. The attended image is passed to the LSTM by par-injection, giving rise to a new state. This new state is then merged with the attended image again in order to determine the next word. Thus, the architecture incorporates aspects of the init-inject, par-inject and merge strategies.\n(Lu et al., 2016) describe an attention-based model that also determines the importance that should be given to an image during generation. If a caption needs to generate the compound word \u201cstop sign\u201d, then only \u201cstop\u201d requires access to the image and \u201csign\u201d can be deduced linguistically. The attended image is merged whilst the full image is par-injected.\n(Yang et al., 2016) describe a generic attention-based encoder-decoder system that was used for image captioning and source code commenting. The attended image is par-injected whilst a mixture of the attended and full image is init-injected.\n(You et al., 2016) describe an attention-based caption generator that initinjects the full image whilst using attributes from the image to perform attention both at the word level (par-inject) and at the RNN level (merge).\nOne of the two systems of (Liu et al., 2016) has already been described in Section 2.2.1. The other system performs init-inject of the image vector and parinject of the image attributes. The addition of attributes did not significantly improve the system.\nThree of the five systems of (Yao et al., 2016) have already been described in Section 2.2.2. The other two systems perform pre-inject of the image attributes and par-inject of the image vector or pre-inject of the image vector and parinject of the image attributes. This last system performs the best of all five in terms of CIDEr."}, {"heading": "2.3 Summary and outlook", "text": "While the literature on caption generation now provides a rich range of models and comparative evaluations, there is as yet very little explicit systematic comparison between the performance of the architectures surveyed above, each of which represents a different way of conditioning the prediction of language sequences on visual information. Work that has tested both par-inject and pre-inject, such as (Vinyals et al., 2015), reports that pre-inject works better. The work of (Mao et al., 2015a) compares inject and merge architectures and concludes that merge is better than inject. However Mao et al.\u2019s comparison between architectures is a relatively tangential part of their overall evaluation, and is based only on the BLEU metric (Papineni et al., 2002).\nAnswering the question of which architecture is best is difficult because different architectures perform differently on different evaluation measures, as shown for example by (Wang et al., 2016), who compared simple RNNs and LSTMs. Although the state of the art systems in caption generation all use inject type architectures, it is also the case that they are more complex systems than the published merge architectures and so it is not fair to conclude that inject is better than merge on a survey of the literature alone.\nIn what follows, we present a systematic comparison between all the different architectures discussed above, using both simple RNNs and LSTMs. We perform these evaluations using a common dataset and a variety of quality metrics, covering (a) the predictive capacity of the caption generator; (b) the quality of the generated captions; (c) the linguistic diversity of the generated captions; and (d) the networks\u2019 capabilities to find the most relevant image given a caption. This last metric is included by way of comparison: many caption generators have been shown to serve a dual function, for both image description and retrieval. By including retrieval among our evaluation methods, we seek to shed light on whether, from an architectural perspective, the same conclusions can be drawn as for captioning."}, {"heading": "3 Architectures", "text": "In this section we go over the different architectures that are evaluated in this paper. A diagram illustrating the three main architecture types is shown in\nFigure 4. We start with a description of a general, schematic architecture in Section 3.1, followed by a description of how each evaluated architecture modifies this schema in Section 3.2. Finally, we give a formal description of the important details of the architectures in Section 3.3."}, {"heading": "3.1 General architecture", "text": "This section describes the basis from which all evaluated architectures are derived. It is a scaled-down version of the architecture described in (Vinyals et al., 2015). This architecture was chosen for its simplicity whilst still being the best performing system in the 2015 MSCOCO image captioning challenge.3\nWord embeddings Word embeddings, that is, the vectors that represent known words prior to being fed to the RNN, consist of 256-element vectors that have been randomly initialised. No precompiled vector embeddings such as word2vec (Mikolov et al., 2013) were used. Instead, the embeddings are trained as part of the neural network in order to learn the best representations of words.\nRecurrent neural network The purpose of the RNN is to take a prefix of embedded words (with image vector in inject architectures) and produce a single vector that represents the sequence. The size of this vector is 256. Two types\n3http://mscoco.org/dataset/#captions-leaderboard\nof RNNs were used: the simple RNN described in (Mao et al., 2015a) and the LSTM described in (Vinyals et al., 2015). Apart from enabling a comparison of different RNN types overall, this strategy accounts for using an RNN that is reported to work well in a merge architecture (Mao et al., 2015a) and another RNN that is reported to work well in an inject architecture (Vinyals et al., 2015).\nImage Prior to training, all images were vectorised using the activation values of the penultimate layer of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which is trained to perform object recognition and returns a 4,096-element vector. The convolutional neural network is not influenced by the caption generation training. During training, a layer of the neural network (with no activation function) compresses this vector into a 256 element vector (the same size as word vectors). It is this 256-element vector that is referred to as the \u2018image vector\u2019 in our experiments.\nOutput Once the image and the caption prefix have been vectorised and mixed into a single vector, the next step is to use them to predict the next word in the caption. This is done by passing the mixed vector through a softmax layer that predicts the probability of each possible 7,415 next word being the word that comes after the prefix.\nRegularisation In order to reduce overfitting, dropout (Srivastava et al., 2014) with a dropout rate of 0.5 was applied on the 4,096-element image input and the word embeddings. Preliminary experiments showed that applying dropout to the input vectors works better than applying dropout to intermediate layers."}, {"heading": "3.2 Evaluated architectures", "text": "This section describes each architecture that is evaluated in our experiments. There is one language model architecture (langmodel), three merge architectures (merge), and four inject architectures (inject), each of which can either use a simple RNN (srnn) or an LSTM (lstm) as a recurrent neural network. In total we therefore evaluate (8\u00d7 2) 16 different architectures."}, {"heading": "3.2.1 The language model architecture", "text": "The language model architecture (langmodel) is a simple unconditioned imageless language model that predicts the next word in a caption given a prefix only. It is used as a baseline against which to measure the effectiveness of different ways of including an image in an architecture."}, {"heading": "3.2.2 The merge architectures", "text": "The merge architectures merge the prefix vector with the image vector before passing the result to the output layer. We consider three ways to merge the image vector with the prefix vector:\n\u2022 merge-concat : The image vector and prefix vector are concatenated into a single vector that has a length equal to the sum of the length of the two vectors. Whilst this method is intuitive, we are mindful of the extra parameters this architecture needs to have in order to process a larger layer.\n\u2022 merge-add : The image vector and prefix vector are added together elementwise into a single vector that has a length equal to the original vectors (image vector and prefix vector have an equal length).\n\u2022 merge-mult : The image vector and prefix vector are multiplied together elementwise into a single vector that has a length equal to the original vectors (image vector and prefix vector have an equal length)."}, {"heading": "3.2.3 The inject architectures", "text": "The inject architecture injects the image vector into the RNN as if it were part of the caption prefix. As noted in Section 2, there are four ways to inject the image into the RNN, which are realised in our experiments in the following way:\n\u2022 inject-init : The image vector is treated as an initial state for the RNN. After initialising the RNN, the vectors in the prefix are then fed to the RNN as usual. Every other architecture in our experiments uses the allzeros vector as an initial state.\n\u2022 inject-pre : The image vector is used as the first \u2018word\u2019 in the caption prefix. This makes the image vector the first thing that the RNN will see.\n\u2022 inject-par : The image vector is added (elementwise) to every word vector in the caption prefix in order to make the RNN take a mixed word-image vector. Every word would have the exact same image vector added to it.\n\u2022 inject-post : The image vector is used as the last \u2018word\u2019 in the caption prefix. This makes the image vector the last thing that the RNN will see before predicting each next word."}, {"heading": "3.3 Formal details", "text": "This section gives a more formal description of the evaluated architectures. As a matter of notation, we treat vectors as being horizontal.\nThe simple RNN, which is the same one used in (Mao et al., 2015a), is defined as\nsn = ReLU(xn + sn\u22121Wss + bs) (1)\nwhere xn is the n th input, sn is the hidden state after n inputs, s0 is the initial state, Wss is the state-to-state weight matrix (which is square), bs is the state bias vector, and ReLU refers to the Rectified Linear Unit function, which is defined as\nReLU(x) = max(0, x) (2)\nThe LSTM model, which is the one used in (Vinyals et al., 2015), is defined as\nin = sig(xnWxi + sn\u22121Wsi + bi) (3)\nfn = sig(xnWxf + sn\u22121Wsf + bf ) (4)\ncn = fn cn\u22121 + in tanh(xnWxc + sn\u22121Wsc + bc) (5) on = sig(xnWxo + sn\u22121Wso + bo) (6)\nsn = on cn (7)\nwhere xn is the n th input, sn is the hidden state after n inputs, s0 is the initial state, cn is the cell state after n inputs, c0 is the all-zeros vector (even when init-inject is used), in is the input gate after n inputs, fn is the forget gate after n inputs, on is the output gate after n inputs, in is the input gate after n inputs, W\u03b1\u03b2 is the weight matrix between \u03b1 and \u03b2, b\u03b1 is the bias vector for \u03b1, and is the elementwise vector multiplication operator. In the above, \u2018sig\u2019 refers to the sigmoid function which is defined as:\nsig(x) = 1\n1 + e\u2212x (8)\nOnly the last hidden state of the RNN is used in the rest of the neural network. Both the state vector (in the simple RNN and LSTM) and the cell vector (in the LSTM) are of size 256.\nThe feedforward layers used for the image and output are normal dense layers which are defined as\nz = xW + b (9)\nwhere z is the result vector, x is the input vector, W is the weight matrix, b is the bias vector.\nThe result vector can then be passed through an activation function, such as softmax, which is defined as\nsoftmax(x)i = exi\u2211 j e xj\n(10)\nwhere softmax(x)i refers to the i th element of the output vector."}, {"heading": "4 Experiments", "text": "This section describes the experiments conducted in order to compare the performance of the different architectures described in the previous section. A summary of all the parameters and configurations is given in Table 2."}, {"heading": "4.1 Dataset", "text": "Images The dataset used for all experiments was the version of Flickr30k4 (Young et al., 2014) distributed by (Karpathy and Fei-Fei, 2015)5, a set of 31,014 images taken from Flickr combined with five manually written captions per image. The provided dataset is split into a training, validation, and test set of 29,000, 1,014, and 1,000 images respectively. The images are already vectorised into 4,096-element vectors via the activations of layer \u2018fc7\u2019 (the penultimate layer) of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which was trained for object recognition on the ImageNet dataset (Deng et al., 2009). These vectors were normalised to unit-length vectors prior to training.\nVocabulary The known vocabulary consists of all the words in the captions training set that occur at least 5 times (capital letters are normalised to lowercase before constructing the vocabulary). This gives us a total of 7,413 known words. These words are used both as inputs, which are embedded and fed to\n4We used Flickr30k instead of the larger MSCOCO (Lin et al., 2014) in order to reduce training time as we had to evaluate many different architectures.\n5http://cs.stanford.edu/people/karpathy/deepimagesent/\nthe RNN, and as outputs, which are given probabilities by the softmax. Apart from these content tokens, some other special tokens were used:\n\u2022 PAD: ignored by the RNN and used to make all caption prefixes in the training set of equal length (used only with the input words);\n\u2022 START: used to mark the beginning of the caption prefix and useful for predicting the first word in the caption (used only with the input words);\n\u2022 END: used to mark the end of the caption and useful for predicting the end of the caption (used only with the output words);\n\u2022 UNKNOWN: used to replace and represent any word which is not in the known vocabulary (used with both the input and output words)."}, {"heading": "4.2 Training set", "text": "In Section 2, we noted that it is possible to adopt a continuous or a discontinuous perspective on an RNN. For the experiments reported here, adopting the latter perspective turns out to be beneficial, in that it naturally enables a unified treatment of all the models under consideration. In particular, training a postinject model requires us to inject the image at every stage of training on a given prefix.\nTherefore, throughout our experiments, captions are broken down into separate prefixes of increasing length and each prefix is treated as a separate entry in the training set. The training goal is to maximize the individual probability of each word in all the captions (including the END token), given the image and caption prefix.\nThe training set produced from the dataset consists of triples \u3008I, C0...n, Cn+1\u3009, where I is an image described by caption C, C0...n is the caption prefix with n words (plus START token), and Cn+1 is the next word (which can be the END token). Each caption-image pair is broken down into all the caption\u2019s prefixes together with the word following the prefix and corresponding image. All prefixes start with a START token (including the empty prefix with the first word as a next word) and are padded so that all of the sequences are of the same length (equal to the longest caption in the training set plus the START token). An example is given in Figure 5. Before each training epoch, the rows are all shuffled and then divided into minibatches. We allow prefixes of the same caption to be placed in different minibatches."}, {"heading": "4.3 Learning", "text": "We did not focus too much on optimizing hyperparameters as it is difficult to find a set of hyperparameters that benefits all architectures at once and we did not want one architecture to have an advantage over the others. For this reason we used default or generally recommended settings were possible.\nCost function As a cost function, the sum of the crossentropy of all \u3008I, C0...n, Cn+1\u3009 triples (symbols defined in Section 4.2) in the training set minibatch were used. This means that the neural networks are trained to minimise the crossentropy of their probability predictions on the training set captions. The crossentropy is defined as follows:\ncrossentropy(P, I, C0...n, Cn+1) = \u2212 ln (P (Cn+1|C0...n, I)) (11)\nwhere P is the output of the neural network being trained which is the probability of a particular word being the next word in a caption prefix.\nOptimisation Neural network learning is performed using Adam (P. Kingma and Ba, 2014) as an optimisation method. All hyperparameters were left as suggested in the original paper: \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999, and = 10 \u22128.\nGradient control In order to avoid gradient explosion (Pascanu et al., 2012), all gradients in the gradient descent algorithm were clipped to be within the range \u00b15 as suggested by (Karpathy and Fei-Fei, 2015).\nMinibatches Minibatches of size 500 were used. Minibatches consist of prefixes rather than sentences (see Figure 5).\nStopping criteria Each training session lasted at most 20 epochs. The same cost function that is used on the training set is also used on the validation set so that only the weights that gave the smallest validation cost will be evaluated on the test set. An early stopping patience of two was used such that if the cost stops decreasing for two epochs on the validation set then training is stopped.\nInitialisation For weight initialisation, all biases were set to zero and all feed forward weights were randomly set using xavier initialisation (Glorot and Bengio, 2010), including the word embeddings. Xavier initialisation sets the weights of a layer to random numbers sampled from a normal distribution with\nmean equal to zero and standard deviation equal to \u221a\n2 ni+no where ni is the\nnumber of inputs to the layer and no is the number of outputs from the layer. The recurrent weights of the RNNs (square matrices that process the state) were initialised using orthogonal weights."}, {"heading": "4.4 Evaluation metrics", "text": "To evaluate the different architectures, the test set captions (which are shared among all experiments) are used to measure the architectures\u2019 quality using metrics that fall into four classes, described below."}, {"heading": "4.4.1 Probability metrics", "text": "These are metrics that quantify how well the architectures are at predicting probabilities of the caption words given their corresponding image and prefix. We report the perplexity of the model given the test captions. The predicted perplexity of a sentence is calculated as:\nperplexity(P,C, I) = 2H(P,C,I) (12)\nH(P,C, I) = \u2212 1 |C| |C|\u2211 n=0 log2 P (Cn+1|C0...n, I) (13)\nwhere P is the trained neural network that gives the probability of a particular word being the next word in a caption prefix, C is a caption with |C| words, I is an image described by caption C, and H is the entropy function. Note that Cn is the nth word in C and C0...n are the first n words in C (with START token).\nIn order to aggregate the caption perplexity of the entire test set of captions into a single number, we report the arithmetic mean, geometric mean, and median of all the caption\u2019s scores."}, {"heading": "4.4.2 Generation metrics", "text": "These metrics quantify the quality of the generated captions by measuring the degree of overlap between generated captions and those in the test set. We use the MSCOCO evaluation code6 which measures the standard evaluation metrics BLEU-(1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin and Och, 2004). Captions were generated using beam search with a beam width of 40 and a clipped maximum length of 50 words.\n6https://github.com/tylin/coco-caption"}, {"heading": "4.4.3 Diversity metrics", "text": "Apart from measuring the caption similarity to the ground truth we also measure the diversity of the vocabulary used in the generated captions. This is intended to shed light on the extent to which captions are \u2018stereotyped\u2019, that is, the extent to which a model re-uses (sub-)strings from case to case, irrespective of the input image.\nAs a limiting case, consider a caption generator which always outputs the same caption. This has the lowest possible diversity. In order to quantify this we measure the percentage of known vocabulary words used in all generated captions and the entropy of the unigram and bigram frequencies in all the generated captions together, which is calculated as:\ndiversity(P, F ) = \u2212 |F |\u2211 i=1 Pi(F ) log2 Pi(F ) (14)\nPi(F ) = Fi\u2211|F | j=1 Fj\n(15)\nwhere F is the frequency distribution over unigrams or bigrams, with |F | different unigrams or bigrams, and Pi is the maximum likelihood estimate probability of encountering unigram or bigram i. Note that Fn is the frequency of the n th unigram or bigram. Entropy gives a measure of how uniform the frequency distributions are (with higher entropy for more uniform distributions). The more uniform, the more likely that each unigram or bigram was used in equal proportion, rather than using the same few words for the majority of the time, hence the greater the variety of words used."}, {"heading": "4.4.4 Retrieval metrics", "text": "As noted in Section 2, we include image retrieval performance among our evaluation measures in view of the frequent practice in the literature of evaluating image captioning models bidirectionally, for both generation and retrieval. In the present case, we are mainly interested in whether the various architectures under consideration are ranked the same in the two tasks.\nRetrieval metrics are metrics that quantify how well the architectures are at retrieving the correct image given a caption. A conditioned language model can be used for retrieval by measuring the degree of relevance each image has to the given caption. Relevance is measured as the probability of the whole caption given the image (by multiplying together each word\u2019s probability). Different images will give different probabilities for the same caption. The more probable the caption is, the more relevant the image is.\nWe use the standard R@n recall measures (Hodosh et al., 2013), and report recall at 1, 5, and 10. Recall at n is the percentage of captions whose correct image is ranked at position n or less in the list of images sorted by relevance."}, {"heading": "5 Results", "text": "Three runs of each experiment were performed and the mean together with the standard deviation (reported in parentheses) of the different evaluation measures over the three runs is reported. For each run, the initial weight settings, minibatch selections, and dropout selections are different since these are randomly determined. Everything else is identical across runs.\nFor comparison, the architectures used in (Mao et al., 2015a) and (Vinyals et al., 2015) are also included in the tables below. Both of these approaches have been influential in the literature; furthermore, they are explicitly defined and hence can be easily reimplemented. For the purposes of the present experiments, they were reimplemented using the same layers and layer sizes as in the original; however, to ensure a fair comparison we have trained and tested them in the same way as the rest of the architectures considered here. This means that we have used the same initialisation, regularisation, training method, etc. For this reason, the results reported below for the Mao and Vinyals architectures occasionally differ from those originally reported by the authors."}, {"heading": "5.1 Data", "text": "Table 3 shows the results of the evaluation using probability metrics, which consist of different ways to aggregate perplexities, against the captions in the test set.\nTable 4 shows metrics that measure the quality of generated captions. Note that the imageless language model here generates one caption, which is the most probable caption overall, and uses it for all images.\nTable 5 shows the extent to which models exploit a significant proportion of the available vocabulary. We estimate the proportion of the available vocabulary (unigram or bigram) used by a model. For comparison, we include the proportions for human captions in the test set, considering both an overall proportion based on all five captions per test set image (human-all) and a proportion based on only the first caption from the five available human captions (human-one).\nFinally, Table 6 shows the results of evaluating the models based on a \u2018reversal\u2019 of the caption generation process, considering image retrieval based on a caption in the test set. Note that, since the imageless language model cannot be applied for retrieval, it is not included in this table."}, {"heading": "5.2 Discussion", "text": "If we take the late binding architectures, merge and post-inject, and the early binding architectures, init-inject and pre-inject, as two groups, then there is a clearly discernible pattern for both the models using a simple RNN and those using an LSTM: given the same RNN type, late binding architectures perform better than early binding architectures with mixed binding architectures (parinject) floating somewhere in the middle. This holds across the board, for all\nevaluation metrics, including retrieval. This result is somewhat surprising, since the LSTM is a reimplementation of the one described by (Vinyals et al., 2015), which is optimized for a pre-inject architecture. In fact there doesn\u2019t seem to be any evaluation criterion where early binding architectures have an advantage over late binding ones. Even if we ignore merge-concat, which has more weights than other architectures, merge-add is always better than inject-par which is almost always better than inject-pre.\nMerge by multiplication seems to suffer on perplexity, but performs well on the other measures. It also has the highest standard deviation in median perplexity (across the three runs). This suggests that one possible reason for its relatively poor ranking on perplexity is due to a high degree of variation in output probability across runs.\nThe diversity metrics show that all architectures have a similar word frequency distribution (although simple RNN early binding architectures are relatively more skewed). Nevertheless, the statistics on vocabulary usage are very telling. It seems that even though humans use at least 32% of the known vocabulary to describe the test set images, none of the evaluated systems used more than 7%. We interpret this as meaning that neural caption generators require seeing a word in the training set very often in order to learn how to use it. From a methodological perspective, this further implies that setting an even higher frequency threshold below which words are mapped to the UNKNOWN word (the current experiments set the threshold at five), would be feasible and would make relatively little difference to the results. Furthermore, this also explains why the standard deviation of the generation measures is so low compared to other measures: the caption generators were conservatively using a handful of words, resulting in a relatively low degree of variation in the captions.\nIn the concluding section, we turn to a more thorough interpretation of these results. Our conclusion, however, must be that models in which image features are included early in the generation process perform poorly, relative to those models based on injecting or merging image features later. It appears,\nthen, that visual information disrupts the processing of linguistic information, as evidenced by the better performance of merge and post-inject models, compared to pre- and par-inject. A better strategy seems to be to encode visual and linguistic information separately prior to the creation of a mixed, or multimodal, representation."}, {"heading": "6 Conclusion", "text": "This paper presented a systematic evaluation of a number of variations on architectures for image caption generation and retrieval. The primary focus was on the distinction between what we have termed \u2018inject\u2019 and \u2018merge\u2019 architectures. The former type of model mixes image and language information, training an RNN to vectorise an image-language prefix mixture. By contrast, merge architectures maintain a separation between an RNN subnetwork, which encodes a linguistic string, and the image vector, merging them late in the process, prior to a prediction module. These models are therefore compatible with approaches to image caption generation using a \u2018multimodal\u2019 layer (Kiros et al., 2014b,a, Mao et al., 2014, 2015a, Hendricks et al., 2016, Song and Yoo, 2016). A related, though distinct question we addressed concerns the stage at which image\ninformation is incorporated in the generation process, with inject architectures permitting a variety of strategies for early or late insertion.\nWhile both types of architectures have been exploited in the literature, the inject architecture has been more popular. Yet, there has been little systematic evaluation of its advantages compared to merge. Our experiments show that the late binding architectures such as merge and post-inject are superior to early and mixed binding architectures such as init-inject, pre-inject, and parinject, in any of the configurations used in the experiments. This is the case whether the models are evaluated on the basis of perplexity, n-gram overlap against the test set, or vocabulary diversity (though all models turn out to be highly conservative on the latter set of measures). Late binding architectures also perform better in image retrieval tasks.\nTwo main conclusions can be drawn from this work. First, our results show that treating image features on a par with linguistic information, and \u2018mixing\u2019 them in the sequence processing part of the model, is not optimal. Rather, as in the late binding models, it seems better to separate the linguistic encoding and image features, merging them at the end. In short, in multimodal tasks\nwhere language is being grounded in visual information, the latter should not be treated as \u2018another word\u2019. Another explanation for why late binding should be better than early binding is because by treating the images as words, the RNN is effectively having to deal with a much larger vocabulary, since the vocabulary would include all the images in the training set apart from all the different words. This can compromise performance. In general it seems that introducing \u2018extra\u2019 information to the RNN disrupts the RNN\u2019s encoding process, which can be seen from the fact that par-inject architectures perform worse than post-inject.\nA second set of conclusions is related to our view of the function of RNNs. As noted in Section 2, we postulate two perspectives, a continuous and a discontinuous view. The latter makes one important feature of RNNs explicit: at any time step, the RNN is encoding a prefix, and the result is used to predict the next element in the sequence. Thus, the RNN is not really \u2018generating text\u2019; were the RNN truly generating the text, then it would need to know which image it was generating text for, as in an early binding model, but this seems to degrade performance compared to the late binding alternative. Against this background, it becomes clearer why late binding architectures outperform early binding ones: prediction in these architectures is based on a language prefix and an image, which are put together prior to the prediction stage. In sum, it is better to keep the RNN exclusively for encoding linguistic information, that is, interpreting sequences for other layers in the neural network. It would seem that making the RNN do more than simply encoding a linguistic sequence is not ideal.\nThe work presented here opens up some avenues for future research. As we noted at the outset, there are some similarities between our view of deep learning architectures and previous work on neural machine translation models (Sutskever et al., 2014, Bahdanau et al., 2014). In future work, we hope to investigate whether conditioning by merge is also the best method of conditioning the sentence generator in a language translation model or a model for other sequence-to-sequence tasks, such as question answering. This would also shed light on the similarities and differences between a range of NLP tasks, as shown by other work on sequence-to-sequence modelling (Sutskever et al., 2014).\nFurthermore, by keeping language and image information separate, merge architectures lend themselves to potentially greater portability and ease of training. For example, it should be possible in principle to take the parameters of the RNN and embedding layers of a general text language model and transfer them to the corresponding layers in a caption generator. This would reduce training time as it would avoid learning the RNN weights and the embedding weights of the caption generator from scratch. As understanding of deep learning architectures evolves in the NLP community, one of our goals should be to maximise the degree of transferability among model components."}], "references": [{"title": "SPICE: Semantic Propositional Image Caption Evaluation, pages 382\u2013398", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "Springer International Publishing, Cham.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, volume 29, pages 65\u201372.", "citeRegEx": "Banerjee and Lavie,? 2005", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "Journal of Artificial Intelligence Research, 55:409\u2013442.", "citeRegEx": "Bernardi et al\\.,? 2016", "shortCiteRegEx": "Bernardi et al\\.", "year": 2016}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CoRR, abs/1411.5654.", "citeRegEx": "Chen and Zitnick,? 2014", "shortCiteRegEx": "Chen and Zitnick", "year": 2014}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Chen and Zitnick,? 2015", "shortCiteRegEx": "Chen and Zitnick", "year": 2015}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J.", "K. Li", "L. Fei-Fei"], "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition. Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Language Models for Image Captioning: The Quirks and What Works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "CoRR, abs/1505.01809.", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Aistats, volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "The symbol grounding problem", "author": ["S. Harnad"], "venue": null, "citeRegEx": "Harnad,? \\Q1990\\E", "shortCiteRegEx": "Harnad", "year": 1990}, {"title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "K. Saenko", "T. Darrell"], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Hendricks et al\\.,? 2016", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Image Representations and New Domains in Neural Image Captioning", "author": ["J. Hessel", "N. Savva", "M.J. Wilber"], "venue": "CoRR, abs/1508.02091.", "citeRegEx": "Hessel et al\\.,? 2015", "shortCiteRegEx": "Hessel et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47(1):853\u2013899. Flickr8k.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Karpathy and Fei.Fei,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Proceedings of The 31st International Conference on Machine Learning, page 595603.", "citeRegEx": "Kiros et al\\.,? 2014a", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014b", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A hierarchical approach for generating descriptive image paragraphs", "author": ["J. Krause", "J. Johnson", "R. Krishna", "L. Fei-Fei"], "venue": "ArXiv.", "citeRegEx": "Krause et al\\.,? 2016", "shortCiteRegEx": "Krause et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Lin", "C.-Y.", "F.J. Och"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics - ACL \u201904. Association for Computational Linguistics (ACL).", "citeRegEx": "Lin et al\\.,? 2004", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y.", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision \u2013 ECCV 2014, pages 740\u2013755. Springer Nature.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Optimization of image description metrics using policy gradient methods", "author": ["S. Liu", "Z. Zhu", "N. Ye", "S. Guadarrama", "K. Murphy"], "venue": "CoRR, abs/1612.00370.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Knowing when to look: Adaptive attention via A visual sentinel for image captioning", "author": ["J. Lu", "C. Xiong", "D. Parikh", "R. Socher"], "venue": "CoRR, abs/1612.01887. 26", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Describing images by feeding LSTM with structural words", "author": ["S. Ma", "Y. Han"], "venue": "2016 IEEE International Conference on Multimedia and Expo (ICME). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Ma and Han,? 2016", "shortCiteRegEx": "Ma and Han", "year": 2016}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR.", "citeRegEx": "Mao et al\\.,? 2015a", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile. Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Mao et al\\.,? 2015b", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning - ICML \u201907. Association for Computing Machinery (ACM).", "citeRegEx": "Mnih and Hinton,? 2007", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Simplified LSTM unit and search space probability exploration for image description", "author": ["O. Nina", "A. Rodriguez"], "venue": "2015 10th International Conference on Information, Communications and Signal Processing (ICICS). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Nina and Rodriguez,? 2015", "shortCiteRegEx": "Nina and Rodriguez", "year": 2015}, {"title": "Image description through fusion based recurrent multi-modal learning", "author": ["R.M. Oruganti", "S. Sah", "S. Pillai", "R. Ptucha"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Oruganti et al\\.,? 2016", "shortCiteRegEx": "Oruganti et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Understanding the exploding gradient problem", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Computing Research Repository (CoRR) abs/1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Selfcritical sequence training for image captioning", "author": ["S.J. Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": "CoRR, abs/1612.00563.", "citeRegEx": "Rennie et al\\.,? 2016", "shortCiteRegEx": "Rennie et al\\.", "year": 2016}, {"title": "Semiotic schemas: A framework for grounding language in action and perception", "author": ["D. Roy"], "venue": "Artificial Intelligence, 167(1-2):170\u2013205. 27", "citeRegEx": "Roy,? 2005", "shortCiteRegEx": "Roy", "year": 2005}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman,? 2014", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Multimodal representation: Kneser-ney smoothing/skip-gram based neural language model", "author": ["M. Song", "C.D. Yoo"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Song and Yoo,? 2016", "shortCiteRegEx": "Song and Yoo", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 27, pages 3104\u20133112. Curran Associates, Inc.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team,? 2016", "shortCiteRegEx": "Team", "year": 2016}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A parallel-fusion RNN-LSTM architecture for image caption generation", "author": ["M. Wang", "L. Song", "X. Yang", "C. Luo"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Image captioning with an intermediate attributes layer. CoRR, abs/1506.01144", "author": ["Q. Wu", "C. Shen", "A. van den Hengel", "L. Liu", "A.R. Dick"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, volume abs/1502.03044, page 20482057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Encode, review, and decode: Reviewer module for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "R. Salakhutdinov", "W.W. Cohen"], "venue": "CoRR, abs/1605.07912.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Boosting image captioning with attributes", "author": ["T. Yao", "Y. Pan", "Y. Li", "Z. Qiu", "T. Mei"], "venue": "CoRR, abs/1611.01646. 28", "citeRegEx": "Yao et al\\.,? 2016", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Institute of Electrical and Electronics Engineers (IEEE).", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Image caption generation with text-conditional semantic attention", "author": ["L. Zhou", "C. Xu", "P. Koch", "J.J. Corso"], "venue": "CoRR, abs/1606.04621.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Image caption generation is the task of generating a natural language description of the content of an image (Bernardi et al., 2016).", "startOffset": 109, "endOffset": 132}, {"referenceID": 11, "context": "Yet, the question of where image information should feature in captioning is at the heart of a broader set of questions concerning how language can be grounded in perceptual information, questions which have been addressed by philosophers (Harnad, 1990) and AI practitioners (Roy, 2005).", "startOffset": 239, "endOffset": 253}, {"referenceID": 36, "context": "Yet, the question of where image information should feature in captioning is at the heart of a broader set of questions concerning how language can be grounded in perceptual information, questions which have been addressed by philosophers (Harnad, 1990) and AI practitioners (Roy, 2005).", "startOffset": 275, "endOffset": 286}, {"referenceID": 15, "context": "Specifically, the descriptions we talk about are \u2018concrete\u2019 and \u2018conceptual\u2019 image descriptions (Hodosh et al., 2013).", "startOffset": 96, "endOffset": 117}, {"referenceID": 40, "context": "The discontinuous perspective brings to light some similarities between RNNs as used in language modelling and the encoding RNNs in sequence-to-sequence models, for example in neural machine translation systems (Sutskever et al., 2014).", "startOffset": 211, "endOffset": 235}, {"referenceID": 8, "context": "These include (Devlin et al., 2015), who use a gated recurrent unit (GRU) model (Chung et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 6, "context": ", 2015), who use a gated recurrent unit (GRU) model (Chung et al., 2014), that was initialised with an image vector.", "startOffset": 52, "endOffset": 72}, {"referenceID": 24, "context": "In a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework \u00e1 la (Sutskever et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 14, "context": "In a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework \u00e1 la (Sutskever et al.", "startOffset": 166, "endOffset": 200}, {"referenceID": 40, "context": "In a related vein, (Ma and Han, 2016) first extract a sequence of attributes from an image, then map these to a caption using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), in an encoder-decoder framework \u00e1 la (Sutskever et al., 2014).", "startOffset": 239, "endOffset": 263}, {"referenceID": 44, "context": "(Wang et al., 2016) combine a simple RNN and an LSTM in one architecture, keeping them independent until their vectors are mixed together by weighted sum.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "(Liu et al., 2016) describe two systems, both of which are optimized by discrete optimisation on caption quality metrics directly which are CIDEr (Vedantam et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 42, "context": ", 2016) describe two systems, both of which are optimized by discrete optimisation on caption quality metrics directly which are CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al.", "startOffset": 135, "endOffset": 158}, {"referenceID": 0, "context": ", 2015) and SPICE (Anderson et al., 2016).", "startOffset": 18, "endOffset": 41}, {"referenceID": 4, "context": "(Chen and Zitnick, 2014) X (Chen and Zitnick, 2015) X (Devlin et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "(Chen and Zitnick, 2014) X (Chen and Zitnick, 2015) X (Devlin et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 8, "context": "(Chen and Zitnick, 2014) X (Chen and Zitnick, 2015) X (Devlin et al., 2015) X (Donahue et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": ", 2015) X (Donahue et al., 2015) X (Hendricks et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 12, "context": ", 2015) X (Hendricks et al., 2016) X (Hessel et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 13, "context": ", 2016) X (Hessel et al., 2015) X (Karpathy and Fei-Fei, 2015) X (Kiros et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": ", 2015) X (Karpathy and Fei-Fei, 2015) X (Kiros et al.", "startOffset": 10, "endOffset": 38}, {"referenceID": 17, "context": ", 2015) X (Karpathy and Fei-Fei, 2015) X (Kiros et al., 2014a) X (Kiros et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 18, "context": ", 2014a) X (Kiros et al., 2014b) X (Krause et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 19, "context": ", 2014b) X (Krause et al., 2016) X (Liu et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 22, "context": ", 2016) X (Liu et al., 2016)\u2020 X (Liu et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 22, "context": ", 2016)\u2020 X (Liu et al., 2016)\u2020 X X (Lu et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 23, "context": ", 2016)\u2020 X X (Lu et al., 2016) X X (Ma and Han, 2016) X (Mao et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 24, "context": ", 2016) X X (Ma and Han, 2016) X (Mao et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 27, "context": ", 2016) X X (Ma and Han, 2016) X (Mao et al., 2014) X (Mao et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 25, "context": ", 2014) X (Mao et al., 2015a) X (Mao et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 26, "context": ", 2015a) X (Mao et al., 2015b) X (Nina and Rodriguez, 2015) X (Oruganti et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 30, "context": ", 2015b) X (Nina and Rodriguez, 2015) X (Oruganti et al.", "startOffset": 11, "endOffset": 37}, {"referenceID": 31, "context": ", 2015b) X (Nina and Rodriguez, 2015) X (Oruganti et al., 2016) X (Rennie et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 35, "context": ", 2016) X (Rennie et al., 2016)\u2020 X (Rennie et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 35, "context": ", 2016)\u2020 X (Rennie et al., 2016)\u2020 X (Song and Yoo, 2016) X (Vinyals et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 38, "context": ", 2016)\u2020 X (Song and Yoo, 2016) X (Vinyals et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 43, "context": ", 2016)\u2020 X (Song and Yoo, 2016) X (Vinyals et al., 2015) X (Wang et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 44, "context": ", 2015) X (Wang et al., 2016) X (Wu et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 45, "context": ", 2016) X (Wu et al., 2015) X (Xu et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 46, "context": ", 2015) X (Xu et al., 2015) X X X (Yang et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 47, "context": ", 2015) X X X (Yang et al., 2016) X X (Yao et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 48, "context": ", 2016) X X (Yao et al., 2016)\u2020 X (Yao et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 48, "context": ", 2016)\u2020 X (Yao et al., 2016)\u2020 X X (You et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 49, "context": ", 2016)\u2020 X X (You et al., 2016) X X X (Zhou et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 51, "context": ", 2016) X X X (Zhou et al., 2016) X", "startOffset": 14, "endOffset": 33}, {"referenceID": 43, "context": "(Vinyals et al., 2015) use an LSTM language model in this way, finding that pre-injection gives better results than par-injection (described below).", "startOffset": 0, "endOffset": 22}, {"referenceID": 45, "context": "Rather than injecting a low-level image vector, (Wu et al., 2015) opt for a higher-level approach by extracting attributes from an image and passing those as an input to the caption generator.", "startOffset": 48, "endOffset": 65}, {"referenceID": 19, "context": "(Krause et al., 2016) generate paragraph-length captions in two stages.", "startOffset": 0, "endOffset": 21}, {"referenceID": 35, "context": "(Rennie et al., 2016) describe two systems, an attention model and a nonattention model, both of which use discrete optimisation in order to optimize CIDEr directly.", "startOffset": 0, "endOffset": 21}, {"referenceID": 48, "context": "(Yao et al., 2016) describe five systems which mix image vectors and image attributes in different ways.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "This last one gives the best performance of all five in terms of METEOR (Banerjee and Lavie, 2005).", "startOffset": 72, "endOffset": 98}, {"referenceID": 9, "context": "Similarly, (Donahue et al., 2015) describes an image and video captioning system that puts two LSTMs in series after each other.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "(Karpathy and Fei-Fei, 2015) generate captions for multiple subregions within an image.", "startOffset": 0, "endOffset": 28}, {"referenceID": 51, "context": "(Zhou et al., 2016) also use par-injection, but the first word that the image is combined with is an all-zeros vector, that is, they use both pre-injection and par-injection.", "startOffset": 0, "endOffset": 19}, {"referenceID": 35, "context": "One of the two systems of (Rennie et al., 2016) has already been described in Section 2.", "startOffset": 26, "endOffset": 47}, {"referenceID": 17, "context": "Among the exponents of this approach is the work of (Kiros et al., 2014a), who uses a log-bilinear language model (LBL) (Mnih and Hinton, 2007) to convert a caption prefix into a single vector which is later merged with the image vector in order to determine the next word in the caption.", "startOffset": 52, "endOffset": 73}, {"referenceID": 29, "context": ", 2014a), who uses a log-bilinear language model (LBL) (Mnih and Hinton, 2007) to convert a caption prefix into a single vector which is later merged with the image vector in order to determine the next word in the caption.", "startOffset": 55, "endOffset": 78}, {"referenceID": 18, "context": "In later work, (Kiros et al., 2014b) describe a caption generator that can be conditioned either on images or on full captions.", "startOffset": 15, "endOffset": 36}, {"referenceID": 25, "context": "(Mao et al., 2015a) do a basic comparison between inject and merge architectures and find that merging works better than injecting.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "(Mao et al., 2015b) describe a subsequent development, where the vocabulary of a caption generator can be increased after training by only learning the new words\u2019 embeddings whilst leaving the rest of the network intact.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "(Hendricks et al., 2016) also uses a multimodal layer that merges the image vector with the caption prefix vector produced using an LSTM.", "startOffset": 0, "endOffset": 24}, {"referenceID": 46, "context": "This is the case in the work of (Xu et al., 2015), who describe an attention-based caption generator.", "startOffset": 32, "endOffset": 49}, {"referenceID": 23, "context": "(Lu et al., 2016) describe an attention-based model that also determines the importance that should be given to an image during generation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 47, "context": "(Yang et al., 2016) describe a generic attention-based encoder-decoder system that was used for image captioning and source code commenting.", "startOffset": 0, "endOffset": 19}, {"referenceID": 49, "context": "(You et al., 2016) describe an attention-based caption generator that initinjects the full image whilst using attributes from the image to perform attention both at the word level (par-inject) and at the RNN level (merge).", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "One of the two systems of (Liu et al., 2016) has already been described in Section 2.", "startOffset": 26, "endOffset": 44}, {"referenceID": 48, "context": "Three of the five systems of (Yao et al., 2016) have already been described in Section 2.", "startOffset": 29, "endOffset": 47}, {"referenceID": 43, "context": "Work that has tested both par-inject and pre-inject, such as (Vinyals et al., 2015), reports that pre-inject works better.", "startOffset": 61, "endOffset": 83}, {"referenceID": 25, "context": "The work of (Mao et al., 2015a) compares inject and merge architectures and concludes that merge is better than inject.", "startOffset": 12, "endOffset": 31}, {"referenceID": 33, "context": "\u2019s comparison between architectures is a relatively tangential part of their overall evaluation, and is based only on the BLEU metric (Papineni et al., 2002).", "startOffset": 134, "endOffset": 157}, {"referenceID": 44, "context": "Answering the question of which architecture is best is difficult because different architectures perform differently on different evaluation measures, as shown for example by (Wang et al., 2016), who compared simple RNNs and LSTMs.", "startOffset": 176, "endOffset": 195}, {"referenceID": 43, "context": "It is a scaled-down version of the architecture described in (Vinyals et al., 2015).", "startOffset": 61, "endOffset": 83}, {"referenceID": 28, "context": "No precompiled vector embeddings such as word2vec (Mikolov et al., 2013) were used.", "startOffset": 50, "endOffset": 72}, {"referenceID": 25, "context": "of RNNs were used: the simple RNN described in (Mao et al., 2015a) and the LSTM described in (Vinyals et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 43, "context": ", 2015a) and the LSTM described in (Vinyals et al., 2015).", "startOffset": 35, "endOffset": 57}, {"referenceID": 25, "context": "Apart from enabling a comparison of different RNN types overall, this strategy accounts for using an RNN that is reported to work well in a merge architecture (Mao et al., 2015a) and another RNN that is reported to work well in an inject architecture (Vinyals et al.", "startOffset": 159, "endOffset": 178}, {"referenceID": 43, "context": ", 2015a) and another RNN that is reported to work well in an inject architecture (Vinyals et al., 2015).", "startOffset": 81, "endOffset": 103}, {"referenceID": 37, "context": "Image Prior to training, all images were vectorised using the activation values of the penultimate layer of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which is trained to perform object recognition and returns a 4,096-element vector.", "startOffset": 164, "endOffset": 194}, {"referenceID": 39, "context": "Regularisation In order to reduce overfitting, dropout (Srivastava et al., 2014) with a dropout rate of 0.", "startOffset": 55, "endOffset": 80}, {"referenceID": 25, "context": "The simple RNN, which is the same one used in (Mao et al., 2015a), is defined as", "startOffset": 46, "endOffset": 65}, {"referenceID": 43, "context": "The LSTM model, which is the one used in (Vinyals et al., 2015), is defined as", "startOffset": 41, "endOffset": 63}, {"referenceID": 50, "context": "6 Data set Flickr30k (Young et al., 2014) Vocabulary all tokens in training set occurring 5 times or more Cost function sum cross-entropy Optimisation Adam (P.", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "5 Stopping criteria early stopping patience of 2 epochs with maximum of 20 epochs Gradient control elementwise gradient clipping of \u00b15 Initialisation biases - zeros; feedforward weights - xavier (Glorot and Bengio, 2010); recurrent weights - orthogonal Generation beam search with beam width being 40 and a maximum length of 50 words", "startOffset": 195, "endOffset": 220}, {"referenceID": 50, "context": "Images The dataset used for all experiments was the version of Flickr30k (Young et al., 2014) distributed by (Karpathy and Fei-Fei, 2015), a set of 31,014 images taken from Flickr combined with five manually written captions per image.", "startOffset": 73, "endOffset": 93}, {"referenceID": 16, "context": ", 2014) distributed by (Karpathy and Fei-Fei, 2015), a set of 31,014 images taken from Flickr combined with five manually written captions per image.", "startOffset": 23, "endOffset": 51}, {"referenceID": 37, "context": "The images are already vectorised into 4,096-element vectors via the activations of layer \u2018fc7\u2019 (the penultimate layer) of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which was trained for object recognition on the ImageNet dataset (Deng et al.", "startOffset": 179, "endOffset": 209}, {"referenceID": 7, "context": "The images are already vectorised into 4,096-element vectors via the activations of layer \u2018fc7\u2019 (the penultimate layer) of the VGG OxfordNet 19-layer convolutional neural network (Simonyan and Zisserman, 2014), which was trained for object recognition on the ImageNet dataset (Deng et al., 2009).", "startOffset": 276, "endOffset": 295}, {"referenceID": 21, "context": "4We used Flickr30k instead of the larger MSCOCO (Lin et al., 2014) in order to reduce training time as we had to evaluate many different architectures.", "startOffset": 48, "endOffset": 66}, {"referenceID": 34, "context": "Gradient control In order to avoid gradient explosion (Pascanu et al., 2012), all gradients in the gradient descent algorithm were clipped to be within the range \u00b15 as suggested by (Karpathy and Fei-Fei, 2015).", "startOffset": 54, "endOffset": 76}, {"referenceID": 16, "context": ", 2012), all gradients in the gradient descent algorithm were clipped to be within the range \u00b15 as suggested by (Karpathy and Fei-Fei, 2015).", "startOffset": 112, "endOffset": 140}, {"referenceID": 10, "context": "Initialisation For weight initialisation, all biases were set to zero and all feed forward weights were randomly set using xavier initialisation (Glorot and Bengio, 2010), including the word embeddings.", "startOffset": 145, "endOffset": 170}, {"referenceID": 33, "context": "We use the MSCOCO evaluation code which measures the standard evaluation metrics BLEU-(1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.", "startOffset": 96, "endOffset": 119}, {"referenceID": 2, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.", "startOffset": 16, "endOffset": 42}, {"referenceID": 42, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin and Och, 2004).", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "We use the standard R@n recall measures (Hodosh et al., 2013), and report recall at 1, 5, and 10.", "startOffset": 40, "endOffset": 61}, {"referenceID": 25, "context": "For comparison, the architectures used in (Mao et al., 2015a) and (Vinyals et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 43, "context": ", 2015a) and (Vinyals et al., 2015) are also included in the tables below.", "startOffset": 13, "endOffset": 35}, {"referenceID": 43, "context": "This result is somewhat surprising, since the LSTM is a reimplementation of the one described by (Vinyals et al., 2015), which is optimized for a pre-inject architecture.", "startOffset": 97, "endOffset": 119}, {"referenceID": 40, "context": "This would also shed light on the similarities and differences between a range of NLP tasks, as shown by other work on sequence-to-sequence modelling (Sutskever et al., 2014).", "startOffset": 150, "endOffset": 174}], "year": 2017, "abstractText": "When a neural language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in a recurrent neural network \u2013 conditioning the language model by injecting image features \u2013 or in a layer following the recurrent neural network \u2013 conditioning the language model by merging the image features. While merging implies that visual features are bound at the end of the caption generation process, injecting can bind the visual features at a variety stages. In this paper we empirically show that late binding is superior to early binding in terms of different evaluation metrics. This suggests that the different modalities (visual and linguistic) for caption generation should not be jointly encoded by the RNN; rather, the multimodal integration should be delayed to a subsequent stage. Furthermore, this suggests that recurrent neural networks should not be viewed as actually generating text, but only as encoding it for prediction in a subsequent", "creator": "TeX"}}}