{"id": "1703.09179", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Transfer learning for music classification and regression tasks", "abstract": "\u25cf in this paper, \u201c we present a transfer learning approach for music classification and regression tasks. we propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of seemingly multiple sound layers in a trained convolutional network. we show publicly that how this convnet feature can serve as a general - purpose music representation. in the experiment, a convnet is trained for music tagging and physically then transferred for many music - related classification and musical regression tasks as well as an audio - related vocal classification task. in experiments, the convnet feature outperforms the baseline mfcc feature in all tasks and many reported approaches of parallel aggregating mfccs and low - and high - level music scoring features.", "histories": [["v1", "Mon, 27 Mar 2017 16:48:03 GMT  (3787kb,D)", "https://arxiv.org/abs/1703.09179v1", "16 pages, single column, NOT iclr submission"], ["v2", "Thu, 29 Jun 2017 15:58:38 GMT  (130kb,D)", "http://arxiv.org/abs/1703.09179v2", "To appear at 18th International Society of Music Information Retrieval (ISMIR) Conference, 2017"], ["v3", "Sat, 15 Jul 2017 13:36:05 GMT  (130kb,D)", "http://arxiv.org/abs/1703.09179v3", "18th International Society of Music Information Retrieval (ISMIR) Conference, Suzhou, China, 2017"], ["v4", "Wed, 13 Sep 2017 16:20:26 GMT  (130kb,D)", "http://arxiv.org/abs/1703.09179v4", "18th International Society of Music Information Retrieval (ISMIR) Conference, Suzhou, China, 2017"]], "COMMENTS": "16 pages, single column, NOT iclr submission", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM cs.SD", "authors": ["keunwoo choi", "gy\\\"orgy fazekas", "mark sandler", "kyunghyun cho"], "accepted": false, "id": "1703.09179"}, "pdf": {"name": "1703.09179.pdf", "metadata": {"source": "CRF", "title": "TRANSFER LEARNING FOR MUSIC CLASSIFICATION AND REGRESSION TASKS", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In the field of machine learning, transfer learning is often defined as re-using parameters that are trained on a source task for a target task, aiming to transfer knowledge between the domains. A common motivation for transfer learning is the lack of sufficient training data in the target task. When using a neural network, by transferring pretrained weights, the number of trainable parameters in the target-task model can be significantly reduced, enabling effective learning with a smaller dataset.\nA popular example of transfer learning is semantic image segmentation in computer vision, where the network utilises rich information, such as basic shapes or prototypical templates of objects, that were captured when trained for image classification [37]. Another example is pretrained word embeddings in natural language processing. Word embedding, a vector representation of a word, can be trained on large datasets such as Wikipedia [35] and adopted to other tasks such as sentiment analysis [27].\nc\u00a9 Keunwoo Choi, Gyo\u0308rgy Fazekas, Mark Sandler, Kyunghyun Cho. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Keunwoo Choi, Gyo\u0308rgy Fazekas, Mark Sandler, Kyunghyun Cho. \u201cTransfer learning for music classification and regression tasks\u201d, 18th International Society for Music Information Retrieval Conference, Suzhou, China, 2017. This work has been part funded by FAST IMPACt EPSRC Grant EP/L019981/1 and the European Commission H2020 research and innova- tion grant AudioCommons (688382). Mark Sandler acknowledges the sup- port of the Royal Society as a recipient of a Wolfson Research Merit Award. Kyunghyun Cho thanks the support by eBay, TenCent, Facebook, Google and NVIDIA.\nThere have been several works on transfer learning in Music Information Retrieval (MIR). Hamel et al. proposed to directly learn music features using linear embedding [57] of mel-spectrogram representations and genre/similarity/tag labels [20]. Oord et al. outlines a large-scale transfer learning approach, where a multi-layer perceptron is combined with the spherical K-means algorithm [16] trained on tags and play-count data [54]. After training, the weights are transferred to perform genre classification and auto-tagging with smaller datasets. In music recommendation, Choi et al. used the weights of a convolutional neural network for feature extraction in playlist generation [10], while Liang et al. used a multi-layer perceptron for feature extraction of content-aware collaborative filtering [29]."}, {"heading": "2. TRANSFER LEARNING FOR MUSIC", "text": "In this section, our proposed transfer learning approach is described. A convolutional neural network (convnet) is designed and trained for a source task, and then, the network with trained weights is used as a feature extractor for target tasks. The schematic of the proposed approach is illustrated in Figure 1."}, {"heading": "2.1 Convolutional Neural Networks for Music Tagging", "text": "We choose music tagging as a source task because i) large training data is available and ii) its rich label set covers various aspects of music, e.g., genre, mood, era, and instrumentations. In the source task, a mel-spectrogram (X), a two-dimensional representation of music signal, is used as the input to the convnet. The mel-spectrogram is selected since it is psychologically relevant and computationally efficient. It provides a mel-scaled frequency representation which is an effective approximation of human auditory perception [36] and typically involves compressing the frequency axis of short-time Fourier transform representation (e.g., 257/513/1025 frequency bins to 64/96/128 Mel-frequency bins). In our study, the number of melbins is set to 96 and the magnitude of mel-spectrogram is mapped to decibel scale (log10 X), following [8] since it is also shown to be crucial in [7].\nIn the proposed system, there are five layers of convolutional and sub-sampling in the convnet as shown in Figure 1. This convnet structure with 2-dimensional 3\u00d73 kernels and 2-dimensional convolution, which is often called Vggnet [44], is expected to learn hierarchical time-frequency\nar X\niv :1\n70 3.\n09 17\n9v 4\n[ cs\n.C V\n] 1\n3 Se\np 20\n17\npatterns. This structure was originally proposed for visual image classification and has been found to be effective and efficient in music classification 1 [11]."}, {"heading": "2.2 Representation Transfer", "text": "In this section, we explain how features are extracted from a pre-trained convolutional network. In the remainder of the paper, this feature is referred to as pre-trained convnet feature, or simply convnet feature.\nIt is already well understood how deep convnets learn hierarchical features in visual image classification [58]. By convolution operations in the forward path, lower-level features are used to construct higher-level features. Subsampling layers reduce the size of the feature maps while adding local invariance. In a deeper layer, as a result, the features become more invariant to (scaling/location) distortions and more relevant to the target task.\nThis type of hierarchy also exists when a convnet is trained for a music-related task. Visualisation and sonification of convnet features for music genre classification has shown the different levels of hierarchy in convolutional layers [13], [9].\nSuch a hierarchy serves as a motivation for the proposed transfer learning. Relying solely on the last hidden layer may not maximally extract the knowledge from a pretrained network. For example, low-level information such as tempo, pitch, (local) harmony or envelop can be captured in early layers, but may not be preserved in deeper layers due to the constraints that are introduced by the network structure: aggregating local information by discarding less-relevant information in subsampling. For the same reason, deep scattering networks [6] and a convnet for mu-\n1 For more recent information on kernel shapes for music classification, please see [40].\nsic tagging introduced in [28] use multi-layer representations.\nBased on this insight, we propose to use not only the activations of the final hidden layer but also the activations of (up to) all intermediate layers to find the most effective representation for each task. The final feature is generated by concatenating these features as demonstrated in Figure 1, where all the five layers are concatenated to serve as an example.\nGiven five layers, there are \u22115\nn=1 5Cn = 31 strategies of layer-wise combination. In our experiment, we perform a nearly exhaustive search and report all results. We designate each strategy by the indices of layers employed. For example, a strategy named \u2018135\u2019 refers to using a 32 \u00d7 3 = 96-dimensional feature vector that concatenates the first, third, and fifth layer convnet features.\nDuring the transfer, average-pooling is used for the 1st\u2013 4th layers to reduce the size of feature maps to 1\u00d71 as illustrated in Figure 1. Averaging is chosen instead of max pooling because it is more suitable for summarising the global statistics of large regions, as done in the last layer in [30]. Max-pooling is often more suitable for capturing the existence of certain patterns, usually in small and local regions 2 .\nLastly, there have been works suggesting randomweights (deep) neural networks including deep convnet can work well as a feature extractor [22] [59] (Not identical, but a similar approach is transferring knowledge from an irrelevant domain, e.g., visual image recognition, to music task [19].) We report these results from random convnet features and denote it as random convnet feature. Assessing performances of random convnet feature will help to clarify the contributions of the pre-trained knowledge transfer versus the contributions of the convnet structure and nonlinear high-dimensional transformation."}, {"heading": "2.3 Classifiers and Regressors of Target Tasks", "text": "Variants of support vector machines (SVMs) [45, 50] are used as a classifier and regressor. SVMs work efficiently in target tasks with small training sets, and outperformed K-nearest neighbours in our work for all the tasks in a preliminary experiment. Since there are many works that use hand-written features and SVMs, using SVMs enables us to focus on comparing the performances of features."}, {"heading": "3. PREPARATION", "text": ""}, {"heading": "3.1 Source Task: Music Tagging", "text": "In the source task, 244,224 preview clips of the Million Song Dataset [5] are used (201,680/12,605/25,940 for training/validation/test sets respectively) with top-50 last.fm tags including genres, eras, instrumentations, and moods. Mel-spectrograms are extracted from music signals in real-time on the GPU using Kapre [12]. Binary cross-entropy is used as the loss function during training.\n2 Since the average is affected by zero-padding which is applied to signals that are shorter than 29 seconds, those signals are repeated to create 29-second signals. This only happens in Task 5 and 6 in the experiment.\nThe ADAM optimisation algorithm [25] is used for accelerating stochastic gradient descent. The convnet achieves 0.849 AUC-ROC score (Area Under Curve - Receiver Operating Characteristic) on the test set. We use the Keras [14] and Theano [51] frameworks in our implementation."}, {"heading": "3.2 Target Tasks", "text": "Six datasets are selected to be used in six target tasks. They are summarised in Table 1. \u2022 Task 1: The Extended ballroom dataset consists of spe-\ncific Ballroom dance sub-genres.\n\u2022 Task 2: The Gtzan genre dataset has been extremely popular, although some flaws have been found [48].\n\u2022 Task 3: The dataset size is smaller than the others by an order of magnitude.\n\u2022 Task 4: Emotion predition on the arousal-valence plane. We evaluate arousal and valence separately. We trim and use the first 29-second from the 45-second signals.\n\u2022 Task 5. Excerpts are subsegments from tracks with binary labels (\u2018vocal\u2019 and \u2018non-vocal\u2019). Many of them are shorter than 29s. This dataset is provided for benchmarking frame-based vocal detection while we use it as a pre-segmented classification task, which may be easier than the original task.\n\u2022 Task 6: This is a non-musical task. For example, the classes include air conditioner, car horn, and dog bark. All excerpts are shorter than 4 seconds."}, {"heading": "3.3 Baseline Feature and Random Convnet Feature", "text": "As a baseline feature, the means and standard deviations of 20 Mel-Frequency Cepstral Coefficients (MFCCs), and their first and second-order derivatives are used. In this paper, this baseline feature is called MFCCs or MFCC vectors. MFCC is chosen since it has been adopted in many music information retrieval tasks and is known to provide a robust representation. Librosa [34] is used for MFCC extraction and audio processing.\nThe random convnet feature is extracted using the identical convnet structure of the source task and after random weights initialisation with a normal distribution [21] but without a training."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Configurations", "text": "For Tasks 1-4, the experiments are done with 10-fold crossvalidation using stratified splits. For Task 5, pre-defined\ntraining/validation/test sets are used. The experiment on Task 6 is done with 10-fold cross-validation without replacement to prevent using the sub-segments from the same recordings in training and validation. The SVM parameters are optimised using grid-search based on the validation results. Kernel type/bandwidth of radial basis function and the penalty parameter are selected from the ranges below:\n\u2022 Kernel type: [linear, radial] \u2013 Bandwidth \u03b3 in radial basis function :\n[1/23, 1/25, 1/27, 1/29, 1/211, 1/213, 1/Nf ]\n\u2022 Penalty parameter C : [0.1, 2.0, 8.0, 32.0]\nA radial basis function is exp(\u2212\u03b3|x\u2212 x\u2032|2), and \u03b3 and Nf refer to the radial kernel bandwidth and the dimensionality of feature vector respectively. With larger C, the penalty parameter or regularisation parameter, the loss function gives more penalty to misclassified items and vice versa. We use Scikit-learn [38] for these target tasks. The code for the data preparation, experiment, and visualisation are available on GitHub 3 ."}, {"heading": "4.2 Results and Discussion", "text": "Figure 2 shows a summary of the results. The scores of the i) best performing convnet feature, ii) concatenating \u201812345\u2019 4 convnet feature and MFCCs, iii) MFCC feature, and iv) state-of-the-art algorithms for all the tasks.\nIn all the six tasks, the majority of convnet features outperforms the baseline feature. Concatenating MFCCs\n3 https://github.com/keunwoochoi/transfer_ learning_music\n4 Again, \u201812345\u2019 refers to the convnet feature that is concatenated from 1st\u20135th layers. For another example, \u2018135\u2019 means concatenating the features from first, third, and fifth layers.\nwith \u201812345\u2019 convnet feature usually does not show improvement over a pure convnet feature except in Task 6, audio event classification. Although the reported state-ofthe art is typically better, almost all methods rely on musical knowledge and hand-crafted features, yet our features perform competitively. An in-depth look at each task is therefore useful to provide insight.\nIn the following subsections, the details of each task are discussed with more results presented from (almost) exhaustive combinations of convnet features as well as random convnet features at all layers. For example, in Figure 3, the scores of 28 different convnet feature combinations are shown with blue bars. The narrow, grey bars next to the blue bars indicate the scores of random convnet features. The other three bars on the right represent the scores of the concatenation of \u201812345\u2019 + MFCC feature, MFCC feature, and the reported state-of-the-art methods respectively. The rankings within the convnet feature combinations are also shown in the bars where top-7 and lower-7 are highlighted.\nWe only briefly discuss the results of random convnet features here. The best performing random convnet features do not outperform the best-performing convnet features in any task. In most of the combinations, convnet features outperformed the corresponding random convnet features, although there are few exceptions. However, random convnet features also achieved comparable or even better scores than MFCCs, indicating i) a significant part of the strength of convnet features comes from the network structure itself, and ii) random convnet features can be useful especially if there is not a suitable source task.\n4.2.1 Task 1. Ballroom Genre Classification\nFigure 3 shows the performances of different features for Ballroom dance classification. The highest score is achieved using the convnet feature \u2018123\u2019 with 86.7% of accuracy. The convnet feature shows good performances, even outperforming some previous works that explicitly use rhythmic features.\nThe result clearly shows that low-level features are crucial in this task. All of the top-7 strategies of convnet feature include the second layer, and 6/7 of them include the first layer. On the other hand, the lower-7 are [\u20185\u2019, \u20184\u2019, \u20183\u2019, \u201845\u2019, \u201835\u2019, \u20182\u2019, \u201825\u2019], none of which includes the first layer. Even \u20181\u2019 achieves a reasonable performance (73.8%).\nThe importance of low-level features is also supported by known properties of this task. The ballroom genre labels are closely related to rhythmic patterns and tempo [32] [49]. However, there is no label directly related to tempo in the source task. Moreover, deep layers in the proposed structure are conjectured to be mostly invariant to tempo. As a result, high-level features from the fourth and fifth layers poorly contribute to the task relative to those from the first, second, and third layers.\nThe state-of-the-art algorithm which is also the only algorithm that used the same dataset due to its recent release uses 2D scale transform, an alternative representation of music signals for rhythm-related tasks [33], and\nreports 94.9% of weighted average recall. For additional comparisons, there are several works that use the Ballroom dataset [18]. This has 8 classes and it is smaller in size than the Extended Ballroom dataset (13 classes). Laykartsis and Lerch [31] combines beat histogram and timbre features to achieve 76.7%. Periodicity analysis with SVM classifier in Gkiokas et al. [17] respectively shows 88.9%/85.6 - 90.7%, before and after feature selection.\n4.2.2 Task 2. Gtzan Music Genre Classification\nFigure 4 shows the performances on Gtzan music genre classification. The convnet feature shows 89.8% while the concatenated feature and MFCCs respectively show only 78.1% and 66.0% of accuracy. Although there are methods that report accuracies higher than 94.5%, we set 94.5% as the state-of-the-art score following the dataset analysis in [48], which shows that the perfect score cannot surpass 94.5% considering the noise in the Gtzan dataset.\nAmong a significant number of works that use the Gtzan music genre dataset, we describe four methods in more detail. Three of them use an SVM classifier, which enables us to focus on the comparison with our feature. Arabi and Lu [1] is most similar to the proposed convnet features in a way that it combines low-level and high-level features and shows a similar performance. Beniya et al. [4] and Huang et al. [23] report the performances with many low-level features before and after applying feature selection algorithms. Only the latter outperforms the proposed method and only after feature selection.\n\u2022 Arabi and Lu [1] uses not only low-level features such as {spectral centroid/flatness/roll-off/flux}, but also highlevel musical features such as {beat, chord distribution and chord progressions}. The best combination of the features shows 90.79% of accuracy.\n\u2022 Beniya et al. [4] uses a particularly rich set of statistics such as {mean, standard deviation, skewness, kurtosis,\ncovariance} of many low-level features including {RMS energy, attack, tempo, spectral features, zero-crossing, MFCC, dMFCC, ddMFCC, chromagram peak and centroid}. The feature vector dimensionality is reduced by MRMR (max-relevance and min-redundancy) [39] to obtain the highest classification accuracy of 87.9%.\n\u2022 Huang et al. [23] adopts another feature selection algorithm, self-adaptive harmony search [55]. The method uses statistics such as {mean, standard deviation} of many features including {energy , pitch, and timbral features} and their derivatives. The original 256- dimensional feature achieved 84.3% of accuracy which increases to 92.2% and 97.2% after feature selection.\n\u2022 Reusing AlexNet [26], a pre-trained convnet for visual image recognition achieved 78% of accuracy [19]. In summary, the convnet feature achieves better performance than many approaches which use extensive music feature sets without feature selection as well as some of the approaches with feature selection. For this task, it turns out that combining features from all layers is the best strategy. In the results, \u201812345\u2019, \u20182345\u2019, and \u20181234\u2019 are three best configurations, and all of the top-7 scores are from those strategies that use more than three layers. On the contrary, all lower-7 scores are from those with only 1 or 2 layers. This is interesting since the majority (7/10) of the target labels already exists in source task labels, by which it is reasonable to assume that the necessary information can be provided only with the last layer for those labels. Even in such a situation, however, low-level features contribute to improving the genre classification performance 5 .\nAmong the classes of target task, classical and disco, reggae do not exist in the source task classes. Based on this, we consider two hypotheses, i) the performances of those three classes may be lower than the others, ii) lowlevel features may play an important role to classify them since high-level feature from the last layer may be biased to the other 7 classes which exist in the source task. However, both hypotheses are rebutted by comparing the performances for each genres with convnet feature \u20185\u2019 and\n5 On the contrary, in Task 5 - music emotion classification, high-level feature plays a dominant role (see Section 4.2.4).\n\u201812345\u2019 as in Figure 5. First, with \u20185\u2019 convnet feature, classical shows the highest accuracy while both disco and reggae show accuracies around the average accuracy reported over the classes. Second, aggregating early-layer features affects all the classes rather than the three omitted classes. This suggests that the convnet features are not strongly biased towards the genres that are included in the source task and can be used generally for target tasks with music different from those genres.\n4.2.3 Task 3. Gtzan Speech/music Classification\nFigure 6 shows the accuracies of convnet features, baseline feature, and state-of-the-art [47] with low-level features including MFCCs and sparse dictionary learning for Gtzan music/speech classification. A majority of the convnet feature combinations achieve 100% accuracy. MFCC features achieve 99.2%, but the error rate is trivial (0.8% is one sample out of 128 excerpts).\nAlthough the source task is only about music tags, the pre-trained feature in any layer easily solved the task, suggesting that the nature of music and speech signals in the dataset is highly distinctive.\n4.2.4 Task 4. Music Emotion Prediction\nFigure 7 shows the results for music emotion prediction (Task 4). The best performing convnet features achieve 0.633 and 0.415 r2 scores on arousal and valence axes respectively.\nOn the other hand, the state-of-the-art algorithm reports 0.704 and 0.500 r2 scores using music features with a recurrent neural network as a classifier [56] that uses 4,777 audio features including many functionals (such as quantiles, standard deviation, mean, inter peak distances) of 12 chroma features, loudness, RMS Energy, zero crossing rate, 14 MFCCs, spectral energy, spectral roll-off, etc.\nFor the prediction of arousal, there is a strong dependency on the last layer feature. All top-7 performances are from the feature vectors that include the fifth layer. The first layer feature also seems important, since all of the top5 strategies include the first and fifth layer features. For\nvalence prediction, the third layer feature seems to be the most important one. The third layer is included in all of the top-6 strategies. Moreover, \u20183\u2019 strategy was found to be best performing among strategies with single layer feature.\nTo summarise the results, the predictions of arousal and valence rely on different layers, for which they should be optimised separately. In order to remove the effect of the choice of a classifier and assess solely the effect of features, we compare our approach to the baseline method of [56] which is based on the same 4,777 features with SVM, not a recurrent neural network. The baseline method achieves .541 and .320 r2 scores respectively on arousal and valence, both of which are lower than those achieved by using the proposed convnet feature. This further confirms the effectiveness of the proposed convnet features.\n4.2.5 Task 5. Vocal/non-vocal Classification Figure 8 presents the performances on vocal/non-vocal classification using the Jamendo dataset [41]. There is no known state-of-the-art result, as the dataset is usually used for frame-based vocal detection/segmentation. Presegmented Excerpt classification is the task we formulate in this paper. For this dataset, the fourth layer plays the most important role. All the 14 combinations that include the fourth layer outperformed the other 14 strategies without the fourth layer.\n4.2.6 Task 6. Acoustic Event Detection\nFigure 9 shows the results on acoustic event classification using Urbansound8K dataset [42]. Since this is not a music-related task, there are no common tags between the source and target tasks, and therefore the final-layer feature is not expected to be useful for the target task.\nThe strategy of concatenating \u201812345\u2019 convnet features and MFCCs yields the best performance. Among convnet features, \u20182345\u2019, \u201812345\u2019, \u2018123\u2019, and \u2018234\u2019 achieve good accuracies. In contrast, those with only one\nor two layers do not perform well. We were not able to observe any particular dependency on a certain layer.\nSince the convnet features are trained on music, they do not outperform a dedicated convnet trained for the target task. The state-of-the-art method is based on a deep convolutional neural network with data augmentation [43]. Without augmenting the training data, the accuracy of convnet in the same work is reported to be 74%, which is still higher than our best result (71.4%). 6\nThe convnet feature still shows better results than conventional audio features, demonstrating its versatility even for non-musical tasks. The method in [42] with {minimum, maximum, median, mean, variance, skewness, kurtosis} of 25 MFCCs and {mean and variance} of the first and second MFCC derivatives (225-dimensional feature) achieved only 68% accuracy using the SVM classifier. This is worse than the performance of the best performing convnet feature.\nIt is notable again that unlike in the other tasks, concatenating convnet feature and MFCCs results in an improvement over either a convnet feature or MFCCs (71.4%). This suggests that they are complementary to each other in this task."}, {"heading": "5. CONCLUSIONS", "text": "We proposed a transfer learning approach using deep learning and evaluated it on six music information retrieval and audio-related tasks. The pre-trained convnet was first trained to predict music tags and then aggregated features from the layers were transferred to solve genre classification, vocal/non-vocal classification, emotion prediction, speech/music classification, and acoustic event classification problems. Unlike the common approach in transfer learning, we proposed to use the features from every convolutional layers after applying an average-pooling to reduce their feature map sizes.\nIn the experiments, the pre-trained convnet feature showed good performance overall. It outperformed the baseline MFCC feature for all the six tasks, a feature that is very popular in music information retrieval tasks because it gives reasonable baseline performance in many tasks. It also outperformed the random-weights convnet features for all the six tasks, demonstrating the improvement by pre-training on a source task. Somewhat surprisingly, the performance of the convnet feature is also very competitive with state-of-the-art methods designed specifically for each task. The most important layer turns out to differ from task to task, but concatenating features from all the layers generally worked well. For all the five music tasks, concatenating MFCC feature onto convnet features did not improve the performance, indicating the music information in MFCC feature is already included in the convnet feature. We believe that transfer learning can alleviate the data sparsity problem in MIR and can be used for a large number of different tasks.\n6 Transfer learning targeting audio event classification was recently introduced in [2, 3] and achieved a state-of-the-art performance."}, {"heading": "6. REFERENCES", "text": "[1] Arash Foroughmand Arabi and Guojun Lu. Enhanced polyphonic music genre classification using high level features. In Signal and Image Processing Applications (ICSIPA), 2009 IEEE International Conference on, pages 101\u2013106. IEEE, 2009.\n[2] Relja Arandjelovic\u0301 and Andrew Zisserman. Look, listen and learn. arXiv preprint arXiv:1705.08168, 2017.\n[3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. In Advances in Neural Information Processing Systems, pages 892\u2013900, 2016.\n[4] Babu Kaji Baniya, Joonwhoan Lee, and Ze-Nian Li. Audio feature reduction and analysis for automatic music genre classification. In Systems, Man and Cybernetics (SMC), 2014 IEEE International Conference on, pages 457\u2013462. IEEE, 2014.\n[5] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida, pages 591\u2013596. University of Miami, 2011.\n[6] Joan Bruna and Ste\u0301phane Mallat. Invariant scattering convolution networks. IEEE transactions on pattern analysis and machine intelligence, 35(8):1872\u20131886, 2013.\n[7] Keunwoo Choi, Gyo\u0308rgy Fazekas, Kyunghyun Cho, and Mark Sandler. The effects of noisy labels on deep convolutional neural networks for music classification. arXiv:1706.02361, 2017.\n[8] Keunwoo Choi, Gyo\u0308rgy Fazekas, and Mark Sandler. Automatic tagging using deep convolutional neural networks. In The 17th International Society of Music Information Retrieval Conference, New York, USA. International Society of Music Information Retrieval, 2016.\n[9] Keunwoo Choi, Gyo\u0308rgy Fazekas, and Mark Sandler. Explaining deep convolutional neural networks on music classification. arXiv preprint arXiv:1607.02444, 2016.\n[10] Keunwoo Choi, Gyo\u0308rgy Fazekas, and Mark Sandler. Towards playlist generation algorithms using rnns trained on within-track transitions. In Workshop on Surprise, Opposition, and Obstruction in Adaptive and Personalized Systems (SOAP), Halifax, Canada, 2016, 2016.\n[11] Keunwoo Choi, Gyo\u0308rgy Fazekas, Mark Sandler, and Kyunghyun Cho. Convolutional recurrent neural networks for music classification. In 2017 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2017.\n[12] Keunwoo Choi, Deokjin Joo, and Juho Kim. Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras. In Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML, 2017.\n[13] Keunwoo Choi, Jeonghee Kim, Gyo\u0308rgy Fazekas, and Mark Sandler. Auralisation of deep convolutional neural networks: Listening to learned features. In International Society of Music Information Retrieval (ISMIR), Late-Breaking/Demo Session, Malaga, Spain. International Society of Music Information Retrieval, 2015.\n[14] Franc\u0327ois Chollet. Keras: Deep learning library for theano and tensorflow. https://github.com/fchollet/keras, 2015.\n[15] Djork-Arne\u0301 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\n[16] Adam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural Networks: Tricks of the Trade, pages 561\u2013580. Springer, 2012.\n[17] Aggelos Gkiokas, Vassilis Katsouros, and Gyo\u0308rgy Carayannis. Towards multi-purpose spectral rhythm features: An application to dance style, meter and tempo estimation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(11):1885\u20131896, 2016.\n[18] Fabien Gouyon, Simon Dixon, Elias Pampalk, and Gerhard Widmer. Evaluating rhythmic descriptors for musical genre classification. In 25th AES International Conference, London, UK, 2004.\n[19] Grzegorz Gwardys and Daniel Grzywczak. Deep image features in music information retrieval. International Journal of Electronics and Telecommunications, 60(4):321\u2013326, 2014.\n[20] Philippe Hamel, Matthew EP Davies, Kazuyoshi Yoshii, and Masataka Goto. Transfer learning in mir: Sharing learned latent representations for music audio classification and similarity. Curitiba, Brazil, 2013.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.\n[22] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: a new learning scheme of feedforward neural networks. In IEEE International Joint Conference on Neural Networks, volume 2, pages 985\u2013990. IEEE, 2004.\n[23] Yin-Fu Huang, Sheng-Min Lin, Huan-Yu Wu, and YuSiou Li. Music genre classification based on local feature selection using a self-adaptive harmony search algorithm. Data & Knowledge Engineering, 92:60\u201376, 2014.\n[24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[25] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[27] Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In International Conference on Machine Learning, volume 14, pages 1188\u20131196, 2014.\n[28] Jongpil Lee and Juhan Nam. Multi-level and multiscale feature aggregation using pre-trained convolutional neural networks for music auto-tagging. arXiv preprint arXiv:1703.01793, 2017.\n[29] Dawen Liang, Minshu Zhan, and Daniel PW Ellis. Content-aware collaborative music recommendation using pre-trained neural networks. In Conference of the International Society for Music Information Retrieval (ISMIR 2015), pages 295\u2013301. Malaga, Spain, 2015.\n[30] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\n[31] Athanasios Lykartsis and Alexander Lerch. Beat histogram features for rhythm-based musical genre classification using multiple novelty functions. In Proceedings of the 16th ISMIR Conference, pages 434\u2013440, 2015.\n[32] Ugo Marchand and Geoffroy Peeters. The extended ballroom dataset. Conference of the International Society for Music Information Retrieval (ISMIR 2016) latebreaking session, 2016.\n[33] Ugo Marchand and Geoffroy Peeters. Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description. In Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on, pages 1\u20136. IEEE, 2016.\n[34] Brian McFee, Matt McVicar, Colin Raffel, Dawen Liang, Oriol Nieto, Eric Battenberg, Josh Moore, Dan Ellis, Ryuichi YAMAMOTO, Rachel Bittner, Douglas Repetto, Petr Viktorin, Joa\u0303o Felipe Santos, and Adrian Holovaty. librosa: 0.4.1, October 2015.\n[35] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[36] Brian CJ Moore. An introduction to the psychology of hearing. Brill, 2012.\n[37] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1717\u20131724, 2014.\n[38] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n[39] Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy. IEEE Transactions on pattern analysis and machine intelligence, 27(8):1226\u20131238, 2005.\n[40] Jordi Pons and Xavier Serra. Designing efficient architectures for modeling temporal features with convolutional neural networks. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2017.\n[41] Mathieu Ramona, Gae\u0308l Richard, and Bertrand David. Vocal detection in music with support vector machines. In Acoustics, Speech and Signal Processing (ICASSP), 2008 IEEE International Conference on, pages 1885\u2013 1888, March 31 - April 4 2008.\n[42] J. Salamon, C. Jacoby, and J. P. Bello. A dataset and taxonomy for urban sound research. In 22st ACM International Conference on Multimedia (ACM-MM\u201914), Orlando, FL, USA, Nov. 2014.\n[43] Justin Salamon and Juan Pablo Bello. Deep convolutional neural networks and data augmentation for environmental sound classification. IEEE Signal Processing Letters, 2017.\n[44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[45] Alex J Smola and Bernhard Scho\u0308lkopf. A tutorial on support vector regression. Statistics and computing, 14(3):199\u2013222, 2004.\n[46] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya Sha, and Yi-Hsuan Yang. 1000 songs for emotional analysis of music. In Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia, pages 1\u20136. ACM, 2013.\n[47] M Srinivas, Debaditya Roy, and C Krishna Mohan. Learning sparse dictionaries for music and speech classification. In Digital Signal Processing (DSP), 2014 19th International Conference on, pages 673\u2013675. IEEE, 2014.\n[48] Bob L Sturm. The gtzan dataset: Its contents, its faults, their effects on evaluation, and its future use. arXiv preprint arXiv:1306.1461, 2013.\n[49] Bob L Sturm et al. Revisiting priorities: Improving mir evaluation practices. In Proc. 17th International Society for Music Information Retrieval Conference (ISMIR\u201916), New York, NY, USA, 2016.\n[50] Johan AK Suykens and Joos Vandewalle. Least squares support vector machine classifiers. Neural processing letters, 9(3):293\u2013300, 1999.\n[51] The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fre\u0301de\u0301ric Bastien, Justin Bayer, Anatoly Belikov, et al. Theano: A python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.\n[52] George Tzanetakis. Gtzan musicspeech. availabe online at http://marsyas.info/download/data sets, 1999.\n[53] George Tzanetakis and Perry Cook. Musical genre classification of audio signals. Speech and Audio Processing, IEEE transactions on, 10(5):293\u2013302, 2002.\n[54] Aa\u0308ron Van Den Oord, Sander Dieleman, and Benjamin Schrauwen. Transfer learning by supervised pretraining for audio-based music classification. In Conference of the International Society for Music Information Retrieval (ISMIR 2014), 2014.\n[55] Chia-Ming Wang and Yin-Fu Huang. Self-adaptive harmony search algorithm for optimization. Expert Systems with Applications, 37(4):2826\u20132837, 2010.\n[56] Felix Weninger, Florian Eyben, and Bjorn Schuller. On-line continuous-time music mood regression with deep recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 5412\u20135416. IEEE, 2014.\n[57] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annotation. International Joint Conference on Artificial Intelligence, 2011.\n[58] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818\u2013833. Springer, 2014.\n[59] Yujun Zeng, Xin Xu, Yuqiang Fang, and Kun Zhao. Traffic sign recognition using extreme learning classifier with deep convolutional features. In The 2015 international conference on intelligence science and big data engineering (IScIDE 2015), Suzhou, China, 2015."}], "references": [{"title": "Enhanced polyphonic music genre classification using high level features", "author": ["Arash Foroughmand Arabi", "Guojun Lu"], "venue": "In Signal and Image Processing Applications (ICSIPA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Look, listen and learn", "author": ["Relja Arandjelovi\u0107", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1705.08168,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Soundnet: Learning sound representations from unlabeled video", "author": ["Yusuf Aytar", "Carl Vondrick", "Antonio Torralba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Audio feature reduction and analysis for automatic music genre classification", "author": ["Babu Kaji Baniya", "Joonwhoan Lee", "Ze-Nian Li"], "venue": "In Systems, Man and Cybernetics (SMC),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "In Proceedings of the 12th International Society for Music Information Retrieval Conference, October", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Invariant scattering convolution networks", "author": ["Joan Bruna", "St\u00e9phane Mallat"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The effects of noisy labels on deep convolutional neural networks for music classification", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Kyunghyun Cho", "Mark Sandler"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "In The 17th International Society of Music Information Retrieval Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Explaining deep convolutional neural networks on music classification", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "arXiv preprint arXiv:1607.02444,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Towards playlist generation algorithms using rnns trained on within-track transitions", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "In Workshop on Surprise, Opposition, and Obstruction in Adaptive and Personalized Systems (SOAP), Halifax,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras", "author": ["Keunwoo Choi", "Deokjin Joo", "Juho Kim"], "venue": "In Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Auralisation of deep convolutional neural networks: Listening to learned features", "author": ["Keunwoo Choi", "Jeonghee Kim", "Gy\u00f6rgy Fazekas", "Mark Sandler"], "venue": "In International Society of Music Information Retrieval (ISMIR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Keras: Deep learning library for theano and tensorflow", "author": ["Fran\u00e7ois Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning feature representations with k-means", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Towards multi-purpose spectral rhythm features: An application to dance style, meter and tempo estimation", "author": ["Aggelos Gkiokas", "Vassilis Katsouros", "Gy\u00f6rgy Carayannis"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Evaluating rhythmic descriptors for musical genre classification", "author": ["Fabien Gouyon", "Simon Dixon", "Elias Pampalk", "Gerhard Widmer"], "venue": "In 25th AES International Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Deep image features in music information retrieval", "author": ["Grzegorz Gwardys", "Daniel Grzywczak"], "venue": "International Journal of Electronics and Telecommunications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Transfer learning in mir: Sharing learned latent representations for music audio classification and similarity", "author": ["Philippe Hamel", "Matthew EP Davies", "Kazuyoshi Yoshii", "Masataka Goto"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["Guang-Bin Huang", "Qin-Yu Zhu", "Chee-Kheong Siew"], "venue": "In IEEE International Joint Conference on Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Music genre classification based on local feature selection using a self-adaptive harmony search algorithm", "author": ["Yin-Fu Huang", "Sheng-Min Lin", "Huan-Yu Wu", "Yu- Siou Li"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Multi-level and multiscale feature aggregation using pre-trained convolutional neural networks for music auto-tagging", "author": ["Jongpil Lee", "Juhan Nam"], "venue": "arXiv preprint arXiv:1703.01793,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "Content-aware collaborative music recommendation using pre-trained neural networks", "author": ["Dawen Liang", "Minshu Zhan", "Daniel PW Ellis"], "venue": "In Conference of the International Society for Music Information Retrieval (ISMIR", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Beat histogram features for rhythm-based musical genre classification using multiple novelty functions", "author": ["Athanasios Lykartsis", "Alexander Lerch"], "venue": "In Proceedings of the 16th ISMIR Conference,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "The extended ballroom dataset. Conference of the International Society for Music Information Retrieval (ISMIR 2016) latebreaking session, 2016", "author": ["Ugo Marchand", "Geoffroy Peeters"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description", "author": ["Ugo Marchand", "Geoffroy Peeters"], "venue": "In Machine Learning for Signal Processing (MLSP),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "An introduction to the psychology of hearing", "author": ["Brian CJ Moore"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Maxime Oquab", "Leon Bottou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Designing efficient architectures for modeling temporal features with convolutional neural networks", "author": ["Jordi Pons", "Xavier Serra"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Vocal detection in music with support vector machines", "author": ["Mathieu Ramona", "Ga\u00ebl Richard", "Bertrand David"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "A dataset and taxonomy for urban sound research", "author": ["J. Salamon", "C. Jacoby", "J.P. Bello"], "venue": "In 22st ACM International Conference on Multimedia (ACM-MM\u201914),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Deep convolutional neural networks and data augmentation for environmental sound classification", "author": ["Justin Salamon", "Juan Pablo Bello"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A tutorial on support vector regression", "author": ["Alex J Smola", "Bernhard Sch\u00f6lkopf"], "venue": "Statistics and computing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "1000 songs for emotional analysis of music", "author": ["Mohammad Soleymani", "Micheal N Caro", "Erik M Schmidt", "Cheng-Ya Sha", "Yi-Hsuan Yang"], "venue": "In Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Learning sparse dictionaries for music and speech classification", "author": ["M Srinivas", "Debaditya Roy", "C Krishna Mohan"], "venue": "In Digital Signal Processing (DSP),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "The gtzan dataset: Its contents, its faults, their effects on evaluation, and its future use", "author": ["Bob L Sturm"], "venue": "arXiv preprint arXiv:1306.1461,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Revisiting priorities: Improving mir evaluation practices", "author": ["Bob L Sturm"], "venue": "In Proc. 17th International Society for Music Information Retrieval Conference (IS- MIR\u201916),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Least squares support vector machine classifiers", "author": ["Johan AK Suykens", "Joos Vandewalle"], "venue": "Neural processing letters,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["The Theano Development Team", "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Musical genre classification of audio signals", "author": ["George Tzanetakis", "Perry Cook"], "venue": "Speech and Audio Processing, IEEE transactions on,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}, {"title": "Transfer learning by supervised pretraining for audio-based music classification", "author": ["A\u00e4ron Van Den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In Conference of the International Society for Music Information Retrieval", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Self-adaptive harmony search algorithm for optimization", "author": ["Chia-Ming Wang", "Yin-Fu Huang"], "venue": "Expert Systems with Applications,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}, {"title": "On-line continuous-time music mood regression with deep recurrent neural networks", "author": ["Felix Weninger", "Florian Eyben", "Bjorn Schuller"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European conference on computer vision,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Traffic sign recognition using extreme learning classifier with deep convolutional features", "author": ["Yujun Zeng", "Xin Xu", "Yuqiang Fang", "Kun Zhao"], "venue": "In The 2015 international conference on intelligence science and big data engineering (IScIDE", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "A popular example of transfer learning is semantic image segmentation in computer vision, where the network utilises rich information, such as basic shapes or prototypical templates of objects, that were captured when trained for image classification [37].", "startOffset": 251, "endOffset": 255}, {"referenceID": 32, "context": "Word embedding, a vector representation of a word, can be trained on large datasets such as Wikipedia [35] and adopted to other tasks such as sentiment analysis [27].", "startOffset": 102, "endOffset": 106}, {"referenceID": 26, "context": "Word embedding, a vector representation of a word, can be trained on large datasets such as Wikipedia [35] and adopted to other tasks such as sentiment analysis [27].", "startOffset": 161, "endOffset": 165}, {"referenceID": 53, "context": "proposed to directly learn music features using linear embedding [57] of mel-spectrogram representations and genre/similarity/tag labels [20].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "proposed to directly learn music features using linear embedding [57] of mel-spectrogram representations and genre/similarity/tag labels [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "outlines a large-scale transfer learning approach, where a multi-layer perceptron is combined with the spherical K-means algorithm [16] trained on tags and play-count data [54].", "startOffset": 131, "endOffset": 135}, {"referenceID": 50, "context": "outlines a large-scale transfer learning approach, where a multi-layer perceptron is combined with the spherical K-means algorithm [16] trained on tags and play-count data [54].", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": "used the weights of a convolutional neural network for feature extraction in playlist generation [10], while Liang et al.", "startOffset": 97, "endOffset": 101}, {"referenceID": 28, "context": "used a multi-layer perceptron for feature extraction of content-aware collaborative filtering [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 33, "context": "It provides a mel-scaled frequency representation which is an effective approximation of human auditory perception [36] and typically involves compressing the frequency axis of short-time Fourier transform representation (e.", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "In our study, the number of melbins is set to 96 and the magnitude of mel-spectrogram is mapped to decibel scale (log10 X), following [8] since it is also shown to be crucial in [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "In our study, the number of melbins is set to 96 and the magnitude of mel-spectrogram is mapped to decibel scale (log10 X), following [8] since it is also shown to be crucial in [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 41, "context": "This convnet structure with 2-dimensional 3\u00d73 kernels and 2-dimensional convolution, which is often called Vggnet [44], is expected to learn hierarchical time-frequency ar X iv :1 70 3.", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "Exponential linear unit (ELU) is used as an activation function in all convolutional layers [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "In all the convolutional layers, the kernel sizes are (3, 3), numbers of channels N is 32, and Batch normalisation is used [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "This structure was originally proposed for visual image classification and has been found to be effective and efficient in music classification 1 [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 54, "context": "It is already well understood how deep convnets learn hierarchical features in visual image classification [58].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Visualisation and sonification of convnet features for music genre classification has shown the different levels of hierarchy in convolutional layers [13], [9].", "startOffset": 150, "endOffset": 154}, {"referenceID": 8, "context": "Visualisation and sonification of convnet features for music genre classification has shown the different levels of hierarchy in convolutional layers [13], [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "For the same reason, deep scattering networks [6] and a convnet for mu-", "startOffset": 46, "endOffset": 49}, {"referenceID": 37, "context": "1 For more recent information on kernel shapes for music classification, please see [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 27, "context": "sic tagging introduced in [28] use multi-layer representations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "Lastly, there have been works suggesting randomweights (deep) neural networks including deep convnet can work well as a feature extractor [22] [59] (Not identical, but a similar approach is transferring knowledge from an irrelevant domain, e.", "startOffset": 138, "endOffset": 142}, {"referenceID": 55, "context": "Lastly, there have been works suggesting randomweights (deep) neural networks including deep convnet can work well as a feature extractor [22] [59] (Not identical, but a similar approach is transferring knowledge from an irrelevant domain, e.", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": ", visual image recognition, to music task [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 42, "context": "Variants of support vector machines (SVMs) [45, 50] are used as a classifier and regressor.", "startOffset": 43, "endOffset": 51}, {"referenceID": 47, "context": "Variants of support vector machines (SVMs) [45, 50] are used as a classifier and regressor.", "startOffset": 43, "endOffset": 51}, {"referenceID": 4, "context": "In the source task, 244,224 preview clips of the Million Song Dataset [5] are used (201,680/12,605/25,940 for training/validation/test sets respectively) with top-50 last.", "startOffset": 70, "endOffset": 73}, {"referenceID": 11, "context": "Mel-spectrograms are extracted from music signals in real-time on the GPU using Kapre [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "Ballroom dance genre classification Extended ballroom [32] 4,180 Accuracy 13 T2.", "startOffset": 54, "endOffset": 58}, {"referenceID": 49, "context": "Genre classification Gtzan genre [53] 1,000 Accuracy 10 T3.", "startOffset": 33, "endOffset": 37}, {"referenceID": 43, "context": "Emotion prediction EmoMusic (45-second) [46] 744 Coefficient of determination (r) N/A (2-dimensional) T5.", "startOffset": 40, "endOffset": 44}, {"referenceID": 38, "context": "Vocal/non-vocal classification Jamendo [41] 4,086 Accuracy 2 T6.", "startOffset": 39, "endOffset": 43}, {"referenceID": 39, "context": "Audio event classification Urbansound8K [42] 8,732 Accuracy 10", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "The ADAM optimisation algorithm [25] is used for accelerating stochastic gradient descent.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "We use the Keras [14] and Theano [51] frameworks in our implementation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 48, "context": "We use the Keras [14] and Theano [51] frameworks in our implementation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 45, "context": "\u2022 Task 2: The Gtzan genre dataset has been extremely popular, although some flaws have been found [48].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "The random convnet feature is extracted using the identical convnet structure of the source task and after random weights initialisation with a normal distribution [21] but without a training.", "startOffset": 164, "endOffset": 168}, {"referenceID": 35, "context": "We use Scikit-learn [38] for these target tasks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "The ballroom genre labels are closely related to rhythmic patterns and tempo [32] [49].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "The ballroom genre labels are closely related to rhythmic patterns and tempo [32] [49].", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "The state-of-the-art algorithm which is also the only algorithm that used the same dataset due to its recent release uses 2D scale transform, an alternative representation of music signals for rhythm-related tasks [33], and 12 34 5 12 34 23 45 12 3 12 4 12 5 13 4 13 5 14 5 23 4 23 5 24 5 34 5 12 13 14 15 23 24 25 34 35 45 1 2 3 4 5 co nc at mf cc So TA .", "startOffset": 214, "endOffset": 218}, {"referenceID": 17, "context": "For additional comparisons, there are several works that use the Ballroom dataset [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Laykartsis and Lerch [31] combines beat histogram and timbre features to achieve 76.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "[17] respectively shows 88.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "5% as the state-of-the-art score following the dataset analysis in [48], which shows that the perfect score cannot surpass 94.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Arabi and Lu [1] is most similar to the proposed convnet features in a way that it combines low-level and high-level features and shows a similar performance.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "[4] and Huang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[23] report the performances with many low-level features before and after applying feature selection algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 Arabi and Lu [1] uses not only low-level features such as {spectral centroid/flatness/roll-off/flux}, but also highlevel musical features such as {beat, chord distribution and chord progressions}.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "[4] uses a particularly rich set of statistics such as {mean, standard deviation, skewness, kurtosis,", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "The feature vector dimensionality is reduced by MRMR (max-relevance and min-redundancy) [39] to obtain the highest classification accuracy of 87.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "[23] adopts another feature selection algorithm, self-adaptive harmony search [55].", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[23] adopts another feature selection algorithm, self-adaptive harmony search [55].", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "\u2022 Reusing AlexNet [26], a pre-trained convnet for visual image recognition achieved 78% of accuracy [19].", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "\u2022 Reusing AlexNet [26], a pre-trained convnet for visual image recognition achieved 78% of accuracy [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 44, "context": "Figure 6 shows the accuracies of convnet features, baseline feature, and state-of-the-art [47] with low-level features including MFCCs and sparse dictionary learning for Gtzan music/speech classification.", "startOffset": 90, "endOffset": 94}, {"referenceID": 52, "context": "500 r scores using music features with a recurrent neural network as a classifier [56] that uses 4,777 audio features including many functionals (such as quantiles, standard deviation, mean, inter peak distances) of 12 chroma features, loudness, RMS Energy, zero crossing rate, 14 MFCCs, spectral energy, spectral roll-off, etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 52, "context": "In order to remove the effect of the choice of a classifier and assess solely the effect of features, we compare our approach to the baseline method of [56] which is based on the same 4,777 features with SVM, not a recurrent neural network.", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "Figure 8 presents the performances on vocal/non-vocal classification using the Jamendo dataset [41].", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "Figure 9 shows the results on acoustic event classification using Urbansound8K dataset [42].", "startOffset": 87, "endOffset": 91}, {"referenceID": 40, "context": "The state-of-the-art method is based on a deep convolutional neural network with data augmentation [43].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "The method in [42] with {minimum, maximum, median, mean, variance, skewness, kurtosis} of 25 MFCCs and {mean and variance} of the first and second MFCC derivatives (225-dimensional feature) achieved only 68% accuracy using the SVM classifier.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "6 Transfer learning targeting audio event classification was recently introduced in [2, 3] and achieved a state-of-the-art performance.", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "6 Transfer learning targeting audio event classification was recently introduced in [2, 3] and achieved a state-of-the-art performance.", "startOffset": 84, "endOffset": 90}], "year": 2017, "abstractText": "In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as lowand high-level music features.", "creator": "LaTeX with hyperref package"}}}