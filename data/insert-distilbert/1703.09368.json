{"id": "1703.09368", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Learning and inference in knowledge-based probabilistic model for medical diagnosis", "abstract": "based on a weighted knowledge graph to represent first - order knowledge and combining it with a probabilistic evaluation model, we propose a methodology for the creation of a medical knowledge network ( mkn ) in medical diagnosis. when a set of symptoms is activated centrally for a specific patient, we can generate a ground medical knowledge network composed of symptom nodes and possible potential disease nodes. by incorporating a boltzmann machine into the potential function of a markov network, separately we investigated the joint probability distribution of the mkn. in order to deal with numerical symptoms, a multivariate inference model is presented that uses conditional probability. in addition, designing the weights for the knowledge graph were efficiently learned from manually annotated chinese electronic medical records ( cemrs ). used in our experiments, we found numerically that the optimum choice of the quality of disease node and the unique expression of candidate symptom variable can improve the effectiveness of medical cryptic diagnosis. our experimental results comparing a markov logic network and the logistic regression algorithm on an actual cemr database indicate that our feedback method holds promise and that mkn can facilitate studies of intelligent diagnosis.", "histories": [["v1", "Tue, 28 Mar 2017 01:51:34 GMT  (1212kb)", "http://arxiv.org/abs/1703.09368v1", "32 pages, 8 figures"]], "COMMENTS": "32 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jingchi jiang", "chao zhao", "yi guan", "qiubin yu"], "accepted": false, "id": "1703.09368"}, "pdf": {"name": "1703.09368.pdf", "metadata": {"source": "CRF", "title": "Learning and inference in knowledge-based probabilistic model for medical diagnosis", "authors": ["Jingchi Jiang", "Chao Zhao", "Yi Guan", "Qiubin Yu"], "emails": ["guanyi@hit.edu.cn", "jiangjingchi0118@163.com", "hitsa.zc@gmail.com", "yuqiubin6695@163.com", "guanyi@hit.edu.cn", "jiangjingchi0118@163.com", "hitsa.zc@gmail.com", "yuqiubin6695@163.com"], "sections": [{"heading": null, "text": "Jingchi Jianga, Chao Zhaoa, Yi Guana,*, Qiubin Yub\na School of Computer Science and Technology, Harbin Institute of Technology, Harbin"}, {"heading": "150001, China", "text": "b Medical Record Room, The 2nd Affiliated Hospital of Harbin Medical University,\nHarbin 150086, China\n* Correspondence address: Yi Guan, School of Computer Science and Technology,\nHarbin Institute of Technology, Comprehensive Building 803, Harbin Institute of Technology, Harbin 150001, China. Tel.: +86-186-8674-8550.\nE-mail addresses: guanyi@hit.edu.cn (Y. Guan), jiangjingchi0118@163.com (J.C. Jiang), hitsa.zc@gmail.com (C. Zhao), yuqiubin6695@163.com (Q.B. Yu).\nLearning and inference in knowledge-based probabilistic model for medical diagnosis\nJingchi Jianga, Chao Zhaoa, Yi Guana,* Qiubin Yub\na School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China b Medical Record Room, The 2nd Affiliated Hospital of Harbin Medical University, Harbin 150086, China\n* Correspondence address: Yi Guan, School of Computer Science and Technology, Harbin\nInstitute of Technology, Comprehensive Building 803, Harbin Institute of Technology, Harbin\n150001, China. Tel.: +86-186-8674-8550.\nE-mail addresses: guanyi@hit.edu.cn (Y. Guan), jiangjingchi0118@163.com (J.C. Jiang), hitsa.zc@gmail.com (C. Zhao), yuqiubin6695@163.com (Q.B. Yu).\nAbstract\nBased on a weighted knowledge graph to represent first-order knowledge and combining it with a probabilistic model, we propose a methodology for the creation of a medical knowledge\nnetwork (MKN) in medical diagnosis. When a set of symptoms is activated for a specific patient, we can generate a ground medical knowledge network composed of symptom nodes\nand potential disease nodes. By Incorporating a Boltzmann machine into the potential function\nof a Markov network, we investigated the joint probability distribution of the MKN. In order to deal with numerical symptoms, a multivariate inference model is presented that uses\nconditional probability. In addition, the weights for the knowledge graph were efficient ly learned from manually annotated Chinese Electronic Medical Records (CEMRs). In our\nexperiments, we found numerically that the optimum choice of the quality of disease node and\nthe expression of symptom variable can improve the effectiveness of medical diagnosis. Our experimental results comparing a Markov logic network and the logistic regression algorithm\non an actual CEMR database indicate that our method holds promise and that MKN can facilitate studies of intelligent diagnosis.\nKeywords: Probabilistic model; First-order knowledge; Markov network; Gradient descent; Markov logic network"}, {"heading": "1. Introduction", "text": "The World Health Organization (WHO) reports that 422 million adults have diabetes, and\n1.5 million deaths are directly attributed to diabetes each year [1]. Additionally, the number of\ndeaths caused by cardiovascular diseases (CVDs) and cancer is estimated to be 17.5 million and 8.2 million, respectively [2]. The WHO report on cancer shows that new cases of cancer\nwill increase by 70 percent over the next two decades. In the face of this situation, researchers\nhave begun to pay more attention to health care. According to existing studies, more than 30 percent of cancer deaths could be prevented by early diagnosis and appropriate treatment [3].\nBecause an accurate diagnosis contributes to a proper choice of treatment and subsequent cure, medical diagnosis plays a great role in the improvement of health care. Consequently, a means\nto provide an effective intelligent diagnostic method to assist clinicians by reducing costs and\nimproving the accuracy of diagnosis has been a critical goal in efforts to enhance the patient medical service environment.\nClassification is one of the most widely researched topics in medical diagnosis. The\ngeneral model classifies a set of symptom data into one of several predefined categories of\ndisease for cases of medical diagnosis. A decision tree [4-5] is a classic algorithm in the medical\nclassification domain, one that uses the information entropy method; however, it is sensitive to inconsistencies in the data. The support vector machine [6-8] has a solid theoretical basis for\nthe classification task; because of its efficient selection of features, it has higher predictive accuracy than decision trees. Bayesian networks [9-10], which are based on Bayesian theory\n[11-12], describe the dependence relationship between the symptom variables and the disease\nvariables; these can be used in medical diagnosis. Other diagnostic models include neural\nnetworks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20]. Each\nof these is designed with a distinct methodology for addressing diagnosis problems.\nExisting studies have mainly focused on exploring effective methods for improving the\naccuracy of disease classification. However, these methods often ignore the importance of the application of domain knowledge. Although Markov logic network [21] is a probabilist ic\ninferential model based on the first-order logic rules, it only applies to binary features which is\nagainst the numerical characteristic of symptom. In this paper, we focus on combining medical knowledge with a novel probabilistic model to assist clinicians in making intelligent diagnoses.\nWe conducted our investigation as follows:\n(1) In order to obtain medical knowledge from Chinese Electronic Medical Records\n(CEMRs), we adopted techniques for the recognition of named entities and entity relationships.\nBy mapping named entities and entity relationships into sentences, we built a medical knowledge base consisting of a set of rules in first-order logic.\n(2) We mapped the first-order knowledge base into a knowledge graph. This graph is\ncomposed of first-order predications (nodes) and diagnostic relationships among predications\n(edges). Furthermore, the graph can also be an intuitive reflection of the inferential structure of\nthe knowledge.\n(3) We developed a novel probabilistic model for medical diagnosis that is based on\nMarkov network theory. For adapting to the requirements of multivariate feature in medical diagnosis, we incorporated a Boltzmann machine into the potential function of a Markov\nnetwork. It can simultaneously model both binary and numerical indexes of symptoms. The\nmathematical derivation of learning and inference is rigorously deduced.\n(4) By a numerical comparison with other diagnostic models for CEMRs, we found that\nour probabilistic model is more effective for diagnosing several diseases according to the measures of precision for the first 10 results (P@10), recall for the first 10 results (R@10), and\naverage discounted cumulative gain (DCG-AVG).\nThe rest of this paper is organized as follows. In Section 2, we introduce Chinese Electronic\nMedical Records and the knowledge graph. In Section 3, we review the fundamentals of\nMarkov networks and Markov logic networks. In Section 4, the knowledge-based probabilist ic model based on Markov networks is proposed; then, we demonstrate the mathematica l\nderivation of learning and inference. In Section 5, we further evaluate the effectiveness and accuracy of our probabilistic model for medical diagnosis. Finally, we conclude this paper and\ndiscuss directions for future work in Section 6."}, {"heading": "2. Knowledge extraction and knowledge representation", "text": ""}, {"heading": "2.1. Chinese Electronic Medical Records", "text": "Electronic medical records (EMRs) [22] are a systematized collection of patient health\ninformation in a digital format. As the crucial carrier of recorded medical activity, EMRs\ncontain significant medical knowledge [23-24]. Therefore, for this study we adopted Chinese\nElectronic Medical Records (CEMRs) in free-form text as the primary source of medical knowledge. These CEMRs, which have had protected health information (PHI) [25] removed,\ncome from the Second Affiliated Hospital of Harbin Medical University, and we have obtained the usage rights for research. These CEMRs mainly include five kinds of free-form text:\ndischarge summary, progress note, complaints of the patient, disease history of the patient, and\nthe communication log. Considering the abundance of medical knowledge and the difficulty of\nChinese text processing, we chose the discharge summary and the progress note as the source\nfor knowledge extraction. The structures of the discharge summary and progress note are shown in Figs. 1 and 2, respectively."}, {"heading": "2.2. Corpus", "text": "The recognition of named entities [26] and entity relationships [27] is an important aspect\nin the extraction of medical knowledge from CEMRs. Referencing the medical concept\nannotation guideline and the assertion annotation guideline given by Informatics for Integrating Biology and the Bedside (i2b2) [28], we have drawn on the guidelines for CEMRs [29] and\nmanually annotated the named entity and entity relationship of 992 CEMRs as the resource of medical knowledge. For this diagnostic task, this study only kept \u201csymptom\u201d entities, \u201cdisease\u201d\nentities, and the \u201cindication\u201d relationship. The \u201cindication\u201d relationship holds when the related\n\u201csymptom\u201d indicates that the patient suffers from the related \u201cdisease.\u201d In addition, there are three modifiers for \u201csymptom\u201d entities, namely present, absent, and possible."}, {"heading": "2.3. Knowledge graph", "text": "The medical knowledge obtained from the 992 CEMRs can be comprehended as a set of\nfirst-order logic rules among \u201csymptom\u201d entities, \u201cdisease\u201d entities, and \u201cindicat ion\u201d\nrelationships. The reliability of medical knowledge corresponds to the probability of the \u201cindication\u201d relationship. By gathering all the annotated \u201cindication\u201d relationships, a medical\nknowledge base may be constructed. However, the medical knowledge base lacks the connectivity of real-world knowledge. In order to capture this medical knowledge more\nintuitively, we build a more comprehensive knowledge graph consisting of \u201cdisease\u201d and\n\u201csymptom\u201d entities as nodes and the \u201cindication\u201d relationships as edges. As the reliability of medical knowledge increases, the corresponding edge\u2019s weight gradually grows. The topology\nof the knowledge graph is shown in Fig. 3.\nThe nodes in the knowledge graph are divided into two different colors according to the\ntype of entity, the red nodes and the green nodes representing \u201csymptom\u201d entities and \u201cdisease\u201d\nentities, respectively. This graph contains 173 kinds of disease and 508 kinds of symptom. As\na whole, 1069 pieces of knowledge are embodied in the knowledge graph. 3. Markov logic networks (MLNs)\nAs a uniform framework of statistical relational learning, a Markov logic network (MLN)\ncombines first-order logic with a probability graph model for solving problems of complexity\nand uncertainty. From a probability-and-statistics point of view, MLN is based on the\nmethodology of Markov networks (MNs) [30]. From a first-order-logic point of view, it can briefly present the uncertainty rules and can tolerate incomplete and contradictory problems in\nthe knowledge areas. 3.1. Markov networks and first-order logic\nA Markov network, which is a model for the joint distribution of a set of variables 1 2( , ,..., )nX X X X   , provides the theoretical basis for a Markov logic network. The\nMarkov network is composed of an undirected graph G and a set of potential functions k . The joint distribution of the Markov network is given as\n{ } 1 ( ) ( )k k\nk\nP X x x Z   \n(1)\nwhere { }kx is the state of the kth clique. Z , known as the normalization function, is given by { }( )k kx kZ x   . The most widely used method for approximate inference in MN is Markov chain Monte Carlo (MCMC), and Gibbs sampling in particular. Another popular inference method in MN is the sum\u2013product algorithm.\nA medical knowledge base is a set of rules in first-order logic. Rules are composed using\nfour types of symbols: constants, variables, functions, and predicates. A term is any expression representing an object. An atom is a predicate symbol applied to a tuple of terms. A ground\natom is an atomic rule, all of whose arguments are ground terms. A possible world assigns a truth value to each possible ground atom."}, {"heading": "3.2. Markov logic networks in medical diagnosis", "text": "A Markov logic network can be considered as a template for generating Markov networks.\nGiven different sets of constants, it will generate different Markov networks. According to the\ndefinition of a Markov network, the joint distribution of a Markov logic network is given by\n( )\n{ }\n1 1 ( ) exp( ( )) ( ) i n x\ni i i i\ni i\nP X x n x x Z Z     \n(2)\nwhere ( )in x is the number of true groundings of the ith rule iR in constants x ; i is the weight for iR ; { }ix is the state of the atoms appearing in iR ; and { }( ) i i ix e   . Because MLN only focuses on binary features, the constants x are discrete values and {0,1}x .\nTo apply MLN in medical diagnosis, the atom is considered as the medical entity. When\nthe medical entity presents an explicit condition for a patient, the corresponding atom of this\nentity in MLN is assigned the value 1; otherwise, 0. By this mapping method, we are able to convert medical knowledge into the binary rules of the MLN. As medical knowledge\naccumulates, an MLN will be built, and the maximum probability model of MLN can be used for medical diagnosis. Given a series of symptoms, the risk probability for a specific disease is\ncalculated by\narg max ( | ) arg max ( , )i i y y i P y x n x y \n(3)\nBecause of the higher complexity of calculating ( , )in x y , the problem of maximum probability can be transformed into a satisfiability problem, for which a set of variables is searched to maximize the number of rules satisfied."}, {"heading": "4. Methodology", "text": "Although MLN can be used for medical diagnosis, it is only suitable for binary rules. The\nreason is that the values of ( , )in x y are uncountable when x is a continuous variable. Thus, MLN is inefficient for multivariate rules. In the health care field, the indexes of symptoms are often expressed in numeric form. Thus, the existing MLN methodology has some obvious\nshortcomings for numeric-based diagnosis. This section addresses that problem. By changing\nthe form of expression of the potential function, we can incorporate the continuous variable x into the joint distribution of MN, enabling the conditional probability model for inference to\nbe deduced via Boltzmann machine, and the learning model for calculating the weight for each rule is proposed."}, {"heading": "4.1. Medical knowledge network (MKN)", "text": "Based on the previously mentioned knowledge graph, we propose a model for handling\nnumeric-based diagnosis, combining the knowledge graph with the theoretical basis of Markov\nnetworks. The novel theoretical framework is named the medical knowledge network (MKN). Definition 1. A medical knowledge network L is a set of pairs ( , )i iR  where iR is the medical knowledge in first-order logic and i is the reliability of iR . Together with a finite set of constants 1 2{ , ,..., }nC c c c , it defines a ground medical knowledge network ,L CM as follows: 1. ,L CM contains one multivariate node for each possible grounding of each medical entity appearing in L. The value of the node is the quantified indicator of the symptom entity or\ndisease entity. 2. ,L CM contains one weight for each piece of medical knowledge. This weight is the i associated with iR in L.\nIn a Markov network, a potential function is a nonnegative real-valued function of the state\nof the corresponding clique. Therefore, the potential function of MKN can also be regarded as\nthe state of a clique, which is composed of one or more \u201cindication\u201d relationships. Incorporating the quantified indicator of each entity into the potential function is an important\nstep for numeric-based diagnosis. From statistical physics, we can express the potential function as an energy function [31], rewriting ( )D as\n( ) exp( ( ))D D   (4)\nwhere ( )D is often called an energy function. The set D is the state of the \u201csymptom\u201d entity and \u201cdisease\u201d entity. Then, the expression ( )D is interpreted in terms of an\nunrestricted Boltzmann machine [32], which is the one of the earliest types of Markov network.\nThe energy function associated with the \u201cindication\u201d relationship is defined by a particula r ly\nsimple parametric form,\n( ) ( , )ij i j ij i jD x x x x    \n(5)\nwhere ix and jx represent the value of the \u201csymptom\u201d entity and the \u201cdisease\u201d entity, respectively, and ij is the contribution of the energy function. According to Eq. (1), the joint distribution is defined as follows:\n1 1 ( ) ( ) exp( ( ))\n1 exp ( )\n1 exp\ni i\ni i\ni\ni i\ni\ni\ni i r R r R\ns d i r r\ni r R\ns d i r r\ni r R\nP X x D D Z Z\nx x Z\nx x Z\n \n\n\n \n\n\n   \n                     \n \n\n\n(6)\nIn general, the energy function of an unrestricted Boltzmann machine contains a set of parameters iu that encode individual node potentials. These activated individual variables will stress the effect of the \u201csymptom\u201d entity in the energy function. The rewritten probability\nformula is given as\n1 ( ) exp ( )s\ni i iri\ni\ns d s i r r rx i\nr R\nP X x x x u x Z \n             \n(7)\nAs can be seen, when a \u201csymptom\u201d entity is activated, the factor of the corresponding individual node potential will be considered a major component of the model; this is exactly\nconsistent with a clinical diagnosis that is based on symptoms. In this paper, we adopt the Gaussian potential function (GPF) as the individual node potential, which is expressed as\n2( )\n1\ns dx x jri\nd ri j\nn\nx x j\nd\nu m e  \n           \n(8)\nwhere s dx x jrid represents the distance between node\ni\ns rx and its neighboring node d jx in the\nknowledge graph. The influence factor  is used for controlling the influence range of each node and d jx m is the quality of node d jx . The final probability model is defined as the following distribution:\n2( )\n1\n1 ( ) exp\ns dx x jri\nd i i ij\ni\nn s d s i r r rx i j\nr R\nd\nP X x x x m e x Z\n \n \n             \n       \n \n(9)\nUsing Definition 1 and the deduced joint distribution, Algorithm 1 provides the procedure\nfor building a medical knowledge network.\nAlgorithm 1. Construction of medical knowledge network\nInput: ListEMR: a list of the electronic medical records for training. Output: Network: a medical knowledge network. Begin 1: Initialize the lists of nodes Setnode and list of edges Setedge in MKN. 2: Extract the entities and relationships from the ListEMR \u2192 Rules = {rule1,\nrule2, \u2026, rulen}.\n3: Initialize the weights for Rules by a fixed value \u03c9. 4: for rulei \u2208 Rules do 5: Initialize the symptom node Nodesymptom and disease node Nodedisease. 6: Parse the symptom predicate and the disease predicate from rulei \u2192\nNodesymptom and Nodedisease.\n7: Setnode \u2190 Nodesymptom. 8: Setnode \u2190 Nodedisease. 9: Define the relationship Edgei between Nodesymptom and Nodedisease. 10: Add Edgei to Setedge, and assign \u03c9 as the weight of Edgei. 11: end for 12: Function PageRank(Setnode, Setedge) end 13: After calculating the PageRank of all nodes, the MKN is built: Setnode, Setedge\n\u2192 Network.\n14: return Network.\nThrough traversing the rules and parsing the predicates, a medical knowledge network can\nbe implemented. The PageRank function is used as the quality of each node. To characterize the reliability of medical knowledge, we set up a fixed value \u03c9 for the initial network."}, {"heading": "4.2. Inference", "text": "Medical inference can answer the two common generic clinical questions: \u201cWhat is the\nprobability that rule 1R holds given rule 2R ?\u201d and \u201cWhat is the probability of disease 1D given the symptom vector 1S ?\u201d In response to the first problem, we can answer by computing the conditional probability as\n1 2\n2\n1 2 1 2 ,\n1 2 ,\n2 ,\n,\n,\n( | , , ) ( | , )\n( | )\n( | )\n( | )\n( | )\nR R\nR\nL C\nL C\nL C\nL Cx\nL Cx\nP R R L C P R R M\nP R R M\nP R M\nP X x M\nP X x M\n \n\n \n\n    \n\n\n\n(10)\nThe set iR  is the set of rules where iR holds, and ,( | )L CP x M is given by Eq. (9). Through the free combinations of pairs of disconnected atoms in MKN, some new rules will be derived\nby inference. When the probability of a new rule exceeds a certain threshold, it can be concluded that the new rule is reliable under the current base. Rule inference not only helps\nenrich the knowledge base but is also a self-learning mechanism for the MKN.\nThe second inference question is what is usually meant by \u201cdisease diagnosis.\u201d On the\ncondition that the patient has a given symptom vector, we can predict the risk probability for a\nspecific disease. This can be classified as a typical problem of conditional probability. The risk probability of disease y can be calculated by\n( )\n1\n( ) ( ) 0 1\n1 1\n( | )\nexp\nexp exp\nx yr ji\ni i j i\ni l\nx y x yr j r ji i\ni i j i i i j i\ni l\nl l\nd n\ni r r y r\ni j r R\nd d n n\ni r r y r i r r y r\ni j j r R\nP Y y B b\nx y m e x\nx y m e x x y m e x\n\n \n\n \n\n   \n  \n \n                \n                                \n \n   i l i r R         \n(11)\nwhere lR is the set of ground rules in which disease y appears, and lb is the Markov\nblanket of y . The Markov blanket of a node is the minimal set of nodes that renders it\nindependent of the remaining network; this is simply the set of that node\u2019s neighbors in the knowledge graph. Corresponding to the ith ground rule, ir y is the value (0 or 1) of disease y . In contrast with the MLN diagnostic model, MKN avoids the complexity problem of ( , )in x y and incorporates the quantitative value of symptom ir x into the diagnostic model. The detailed diagnostic algorithm is shown in Algorithm 2.\nAlgorithm 2. Disease diagnostic algorithm based on MKN\nInputs: Rules: a set of rules with the learned weights \u03c9. PR: a set of PageRank values for the nodes in MKN.\nEvidences: a set of ground atoms with known values for a specific patient. Query: a set of ground atoms with unknown disease values.\nOutput: Result: a diagnosis result for the specific patient. Begin 1: for diseasei \u2208 Query do 2: Initialize the probability Proactivated with the activated diseasei. 3: Initialize the probability Proinactivated with the inactivated diseasei. 4: //Activating the disease atoms in Network. 5: for rulej \u2208 Rules do 6: Proinactivated += \ud835\udf14\ud835\udc57 \u2219 \ud835\udc60\ud835\udc66\ud835\udc5a\ud835\udc5d\ud835\udc61\ud835\udc5c\ud835\udc5a\ud835\udc57 \u2219 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc57 + \ud835\udc43\ud835\udc45\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc57 \u2219 \ud835\udc60\ud835\udc66\ud835\udc5a\ud835\udc5d\ud835\udc61\ud835\udc5c\ud835\udc5a\ud835\udc57 \ud835\udc38\u2044 . 7: if \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc56 \u2208 \ud835\udc5f\ud835\udc62\ud835\udc59\ud835\udc52\ud835\udc57 8: Activate the atom of diseasej. 9: end if 10: Proactivated += \ud835\udf14\ud835\udc57 \u2219 \ud835\udc60\ud835\udc66\ud835\udc5a\ud835\udc5d\ud835\udc61\ud835\udc5c\ud835\udc5a\ud835\udc57 \u2219 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc57 + \ud835\udc43\ud835\udc45\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc57 \u2219 \ud835\udc60\ud835\udc66\ud835\udc5a\ud835\udc5d\ud835\udc61\ud835\udc5c\ud835\udc5a\ud835\udc57 \ud835\udc38\u2044 . 11: end for 12: \ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc56 = \ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 ) (\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51) + \ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 ))\u2044 . 13: end for 14: Function Sort(Result) end 15: return Result.\nFollowing the Eq. (11), we propose a disease diagnostic algorithm based on MKN. To\nprovide a reliable diagnosis, we need calculate the risk of each disease. According to the\nevidences, Algorithm 2 can generate a list of potential disease which is sorted by diagnost ic possibility."}, {"heading": "4.3. Learning", "text": "A learning model is proposed for the calculation of the weight for each piece of medical\nknowledge from the Chinese Electronic Medical Records. In this study, we adopted the\ngradient descent method. Assuming independence among diseases, the learning model first calculates the joint probability distribution of a disease vector:\n * ,\n1\n( ) | m\nl l L C\nl P Y y P Y y M     \n(12)\nwhere m is the dimension of disease vector y . By Eq. (12), the derivative of the log-\nlikelihood function with respect to the weight for the ith rule is\n \n \n*\n,\n1\n,\n1\nlog ( ) log |\nlog |\nm\nl l L C\nli i\nm\nl l L C\nl i\nP Y y P Y y M\nP Y y M\n \n\n \n\n\n\n    \n \n  \n\n\n\n(13)\nThe calculation of  ,log |l l L C i P Y y M \n \n will be a stubborn problem. Therefore, we try\nto construct the derivative of the log-likelihood. From Eq. (9), we know that the normaliza t ion\nfunction Z can be expressed as\n( )\n1\nexp\nx yr ji\ni i j i\ni\nd n\ni r r y ry i j\nr R\nZ x y m e x \n \n \n                     \n(14)\nThen, we have the pseudo-log-likelihood of Eq. (9) and its gradient:\n( )\n,\n1\nlog ( | ) log\nx yr ji\ni i j i\ni\nd n\nL C i r r y r\ni j r R\nP Y y M x y m e x Z \n \n         \n     \n \n(15)\n( )\n,\n1\n,\n1 log ( | ) exp\n( | )\nx yr ji\ni i i i j i i\ni\ni i i i\nd n\nL C r r i r r y r ri ry i ji\nr R\nr r L C r ry\nP Y y M x y x y m e x x y Z\nx y P Y y M x y\n\n\n\n \n\n \n\n\n                          \n   \n  \n\n(16)\nwhere  is the set of all possible values of y , and ,( | )L CP Y y M can be given by Eq. (9). By bringing Eq. (16) into Eq. (13), the derivative of the log-likelihood with respect to the weight for the ith rule can be naturally calculated. We get the final expression\n * ,\n1\nlog ( ) ( | ) i i i i\nm\nr r L C r ry li P Y y x y P Y y M x y  \n     \n  \n(17)\nAfter finite iterations, i is calculated with the learning rate  .\n*\n, , 1 1log ( ) |i t i t t i P Y y      \n   \n\n(18)\nThe detailed procedure for the weight learning model is presented in Algorithm 3.\nAlgorithm 3. Learning algorithm for MKN\nInputs: Network: an MKN with vector \u03c9 of fixed weights. Evidences: a set of ground atoms with known values. Output: Weights: a learned weight vector. Begin 1: Initialize the weight vector Weights. 2: for weighti \u2208 Weights do //weighti represents the weight for the ith rule 3: while t From 1 To 100 do 4: for evidencej \u2208 Evidences do //evidence set for the jth patient 5: Extract the blanket of the ith rule \u2192 blanketi. 6: //Mapping evidencej to blanketi 7: \ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc5d\ud835\udc52 += \ud835\udc60\ud835\udc56\ud835\udc57 \u2219 \ud835\udc51\ud835\udc56\ud835\udc57 \u2212 \u2211 [\ud835\udc43\ud835\udf14(\ud835\udc4b\ud835\udc59 = \ud835\udc65 \u2032|\ud835\udc4f\ud835\udc59\ud835\udc4e\ud835\udc5b\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc56) \u2219 \ud835\udc60\ud835\udc56\ud835\udc57 \u2219 \ud835\udc51\ud835\udc56\ud835\udc57 ]\ud835\udc65\u2032 \u2208\ud835\udc4b . 8: //sij represents the symptom value of evidencej for the ith rule 9: //dij represents the disease value of evidencej for the ith rule 10: end for 11: \ud835\udf14\ud835\udc56,\ud835\udc61 = \ud835\udf14\ud835\udc56,\ud835\udc61\u22121 + \ud835\udf02 \u2219 \ud835\udc60\ud835\udc59\ud835\udc5c\ud835\udc5d\ud835\udc52. 12: end while 13: weighti \u2190 \ud835\udf14\ud835\udc56,\ud835\udc61. 14: end for 15: return Weights.\nIn summary, we adopt the log-likelihood function and the gradient descent method to learn\nthe weight vector. Fortunately, the gradient of pseudo-log-likelihood can be calculated by the\njoint probability distribution in finite time. Mapping the evidence to its Markov blanket is also\nto improve the time-effectiveness of learning algorithm."}, {"heading": "5. Experiments and discussion", "text": "In order to verify the effectiveness of the medical knowledge network, we conducted\nexperiments using actual CEMRs. Based on the knowledge graph concept described in Section 2.2, we built an MKN for medical diagnosis. We chose the manually annotated 992 CEMRs\nwith the help of medical professionals and only kept the discharge summary and the progress\nnote as the source of knowledge. In the annotating process, we classified the entities into five categories: disease, type of disease, symptom, test, and treatment; only the disease entity and\nthe symptom entity were extracted to complete the diagnostic task. Additionally, owing to the lack of numerical indexes for symptoms in our CEMRs, we adopted modifiers for the symptom,\nnamely present, possible, and absent, to represent the symptom variable x, corresponding to 2,\n1, and 0, respectively. Although the modifier of the symptom is not a continuous variable, a multivariate version of MKN also has theoretical significance.\nAfter the MKN was constructed, we randomly selected 300 untagged CEMRs as the test\ncorpus, and conditional random fields (CRFs) were used to automatically recognize the disease\nentities and symptom entities. Based on the symptom entities on each CEMR, we inferred the\ndiagnosis result and ascertained whether there was consistency between the diagnosed disease and the actual disease.\nThe description and analysis of the experiments are mainly concerned with three aspects:\nthe parameter analysis, the weight learning, and the relative effectiveness of MKN and the\nother methods compared.\n5.1. Parameter analysis\nIn this section, we focus on the optimum choices for the parameter values. In Eq. (11),\nr ji x yd is the distance between ir x and its neighboring node jy in the knowledge graph.\nTherefore, we define 1 r ji x yd  . The influence factor  represents the control range of each node. If a symptom atom and a disease atom appear in a common rule, they have an interaction with each other, and the two atoms in each rule can be represented as two adjacent nodes in the\nknowledge graph. Since we naturally assume that the symptom node only affects the nearest\nconnected disease node, we set 1  .\nThe terms jy m , which is the quality of the disease node, and ir x , which is the symptom\nvariable in the ith rule, are both uncertainty parameters. The selection of expressions for jy m and ir x will affect the accuracy of MKN in diagnosing disease. To begin, we experimented with three classical measures for jy m : PageRank, degree, and betweenness centrality, and we used discounted cumulative gain (DCG) [33] as the indicator to measure the accuracy of the diagnosis result. The DCG score can be calculated by\n1\n2 2log\nP i\nP i i\nrel DCG rel\n\n \n(19)\nwhere irel represents the relevance of the ith disease in the diagnosis result; a correct diagnosis is 1, whereas a misdiagnosis is 0. The variable P is the number of diagnosis results, which in this study was 10.\nAs structural differences between the discharge summary and the progress note can lead\nto different numbers of symptom for the same patient, two experiments were used to distinguish between them. Fig. 4 shows the DCG scores (y-axes) plotted against the serial numbers of 40\ndischarge summaries and 260 progress notes (x-axes) for the three measures of quality for the disease node.\nAlthough the results show that the curves of the DCG scores are irregular, the DCG score\nis 1 in most cases. From the DCG descriptions, the reason is that most of the CEMRs have only one actual disease, and our model ranks this disease at the top of the diagnosis result. We also\nobserve that the effectiveness of the PageRank-based MKN is better than the other methods for\nboth the discharge summary and the progress note.\nIn order to describe the diagnosis result more directly, we adopt a second measure, R@10,\nwhich is the recall for the first 10 results. If m actual diseases appear in a CEMR and the MKN returns n of them, then the R@10 is given by\n@10 0 , 10 n R n m n\nm    \n(20)\nFig. 5 shows the distributions of R@10 for discharge summaries and progress notes, with\nthe blue, green, and red bars showing the results using PageRank, betweenness centrality, and\ndegree, respectively. Under PageRank, the recall for nearly half the records is 1.0. By contrast, the results for betweenness centrality and degree are unsatisfactory because they have higher\nproportions with a recall of 0.0 and lower proportions with 1.0. Considering both factors DCG\nand R@10, we conclude that PageRank is more suitable to use as the quality of disease node\njy m .\nThe second uncertainty parameter is ir x , which is the quantitative value of the symptom.\nAlthough the discrete modifier of the symptom could be employed as the representation of ir x in this paper, it would not be the best choice for processing continuous values in the future. If the continuous value of a symptom is used directly as ir x , the problem of normalization across different symptoms will be an important factor that could cause undesirable results. Therefore, we seek a representation of ir x that not only satisfies the requirements for discrete values but also might be suitable for continuous values. The sigmoid function is a typical normaliza t ion\nmethod. However, the domain of the sigmoid function does not match the value range for symptoms. For the diagnostic task, we designed an improved sigmoid function to express ir x , the quantitative value of symptom. The improved sigmoid function is defined as follows:\n2( )\n2 ( ) 1\n1 normal x x S x e     \n(21)\nwhere x is the value of the symptom, and normalx is the normal value, corresponding to absent (and represented by 0) in this paper. By the characteristic of a sigmoid function, we can\nmap the symptom variable to a normalization interval, which is 0 ( ) 1S x  .\nTo further answer what kinds of representation of ir x can improve the accuracy of the\ndiagnosis results, we continued with experiments comparing the sigmoid function, our\nimproved sigmoid, and discrete modifiers of the symptom. The experimental results are shown in Figs. 6 and 7.\nFollowing the same evaluation criteria, the results shown in Fig. 7 indicate that the\nperformance of the modifier-based variable is consistent with that of the improved sigmo id\nfunction, whereas in Fig. 6 the performance of the improved sigmoid function is shown to be\na little better than that of the other methods. Hence, we conclude that the improved sigmo id\nfunction can not only handle the continuous symptom variable, but also performs well in the discrete field. In summary, considering MKN\u2019s ability to migrate between continuous variables\nand discrete variables, the improved sigmoid function should be employed as the expression of symptom variable ir x ."}, {"heading": "5.2. Weight learning", "text": "In this section, we make a credibility assumption: If a ground atom is in the knowledge\nbase, it is assumed to be true; otherwise, it is false. In other words, the inference of the MKN\ndepends completely on the existing medical knowledge. To test the effectiveness of the learning method, we compared four types of weighting, including constant weighting, MLN-\nbased weighting, nonnegative MLN-based weighting, and MKN-based weighting. We divided\nMLN-based weight learning into typical weighting and nonnegative weighting. Because negative weights would be generated by MLN, violating the credibility assumption of medical\nknowledge, we rewrote the learning program \u201cTuffy,\u201d [34] which is an open-source MLN inference engine, to ensure that the learned weights would be nonnegative. When a negative\nweight is learned, our solution is to replace the negative weight with the current minimum\npositive weight at each iteration. In addition, we experimented using different constants as weights to check whether it might influence the diagnosis results. Tables 1 and 2 summarize\nthe results for the discharge summaries and the progress notes, respectively, showing P@10, precision for the first 20 results (P@20), R@10, and average DCG.\nTable 1 Analysis of effectiveness of weight learning for discharge summaries.\nIndex\nWeight Type P@10 P@20 R@10 DCG-AVG Constant Weight of 0.5 0.875 0.9 0.62 1.06 Constant Weight of 1 0.875 0.9 0.62 1.06 MLN Weight 0.8056 0.8611 0.5233 0.822 Positive MLN Weight 0.8485 0.9091 0.51 0.8402 MKN Weight 0.9 0.95 0.67 1.0983\nP@10 = precision for first 10 results; P@20 = precision for first 20 results; R@10 = recall for first 10 results; DCG-AVG = average discounted cumulative gain. MLN = Markov logic network; MKN = medical knowledge network.\ndemonstrating that the diagnosis results are not at all influenced by the weights\u2019 being equally\nadjusted. Furthermore, the positive MLN-based weighting is better than the typical MLN-based weighting by all evaluation criteria, whether from discharge summaries or progress notes. This\nindicates that the results of diagnosis are significantly improved by positivizing the negative ly\nweighted knowledge, further demonstrating the significance of negatively weighted knowledge for the reliability of our medical knowledge. Finally, we experimentally conclude that the\nweight learning methods in order of effectiveness are MKN-based, constant, positive MLNbased, and MLN-based.\n5.3. Comparison with other algorithms\nAfter determining the uncertainty parameters and the type of weights, we compared three\ndiagnostic systems: MLN, MKN, and the logistic regression algorithm (LR). Fig. 8 shows the DCG curves and the distribution of R@10 using all CEMRs. MKN is clearly more accurate\nthan the other methods, demonstrating the promise of this approach. According to the DCG scores, LR performs well in some CEMRs but very poorly in others; its recall values are\nuniformly poor. Although LR is used in diagnosing single diseases in some studies, it hardly\napplies to diagnosis of multiple diseases, especially when a patient\u2019s symptoms are sparse. Compared with LR, MLN performs better in the DCG, even surpassing MKN for some records.\nHowever, there is a greater difference between MLN and MKN in R@10. We believe that the theoretical mechanism of MLN being based on rough binary logic is the main cause of the poor\neffect; the binary atoms cannot precisely capture the degree of seriousness of the symptoms.\nAs a result of this defect, the actual top diseases cannot be ranked in the top 10 to the extent possible, although these diseases are indeed detected, which is reflected in the DCG. On the\nother hand, this testifies to the advantage of MKN\u2019s multivariate atoms for medical diagnosis.\nFurther, we calculated the average DCG and R@10 values for each of the three\nalgorithms, which are shown in Table 3.\nare still misdiagnosed completely. The most likely reason is that the medical knowledge base created from 992 annotated CEMRs is minuscule. With the accumulation of medical\nknowledge, we believe that the usefulness of MKN as an intelligent system will continue to develop."}, {"heading": "6. Conclusion and future work", "text": "In this paper, we have presented a knowledge-based probabilistic model for medical\ndiagnosis. By extracting medical knowledge from Chinese Electronic Medical Records, a\nknowledge graph was constructed, which is composed of \u201cdisease\u201d nodes and \u201csymptom\u201d nodes. Building on the theory of Markov networks and Markov logic networks, we developed\na novel probabilistic model, called the medical knowledge network. In order to address the\nproblem of numeric-based diagnosis, the model applies the energy function of Boltzmann machines as the potential function. Then, the mathematical derivation process of learning and\ninference were rigorously deduced. In contrast to a Markov logic network, the medical\nknowledge network adopts ternary rules or even continuously numeric rules, not being limited\nto binary rules. In experiments, PageRank and our improved sigmoid function were applied as the quality of disease node and the expression of the symptom variable, respectively. Empirica l\ntests with actual records illustrate that MKN can improve diagnostic accuracy. Through comparisons with other algorithms, the effectiveness and promise of MKN were also\ndemonstrated.\nMKN is a knowledge-based inference model applicable to many AI problems, but leaves\nample space for the future. Directions for future work fall into three main areas:\nKnowledge base: We plan to annotate more records and structure more medical knowledge to investigate the application of MKN in a variety of domains.\nInference: We plan to test the effectiveness of the numeric rules after the test data have been\nsatisfied, identifying and exploiting the possibility of inference throughout the knowledge base. Learning: We plan to develop algorithms for learning and replace the pseudo-log-likelihood\nfunction, study dynamic approaches to weight learning, and build MKNs from sparse data and incomplete data."}, {"heading": "Acknowledgements", "text": "The Chinese Electronic Medical Records used in this paper were provided by the Second\nAffiliated Hospital of Harbin Medical University. We would like to thank the reviewers for their detailed reviews and insightful comments, which have helped to improve the quality of\nthis paper."}], "references": [{"title": "Decision tree classifiers for automated medical diagnosis", "author": ["A.T. Azar", "S.M. El-Metwally"], "venue": "Neural Computing and Applications. 23(7-8) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensemble decision tree classifier for breast cancer data", "author": ["D. Lavanya", "K.U. Rani"], "venue": "International Journal of Information Technology Convergence and Services. 2(1) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector machines with genetic fuzzy feature transformation for biomedical data classification, Inf Sci", "author": ["Y.C.T.Bo. Jin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A decision support system to improve medical diagnosis using a combination of k-medoids clustering based attribute weighting and SVM", "author": ["M. Peker"], "venue": "Journal of medical systems. 40(5) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "P", "author": ["D. Vassis", "B.A. Kampouraki"], "venue": "Belsis, et al., Using neural networks and SVMs for automatic medical diagnosis: a comprehensive review, INTERNATIONAL CONFERENCE ON INTEGRATED INFORMATION (IC-ININFO). 1644(1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A method for root cause analysis with a Bayesian belief network and fuzzy cognitive map, Expert Systems with Applications", "author": ["Y.Y. Wee", "W.P. Cheah", "S.C. Tan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "W", "author": ["A.C. Constantinou", "N. Fenton"], "venue": "Marsh, et al., From complex questionnaire and interviewing data to intelligent Bayesian network models for medical decision support , Artificial intelligence in medicine. 67(1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature selection and classification model construction on type 2 diabetic patients\u2019 data", "author": ["Y. Huang", "P. McCullagh", "N. Black", "R. Harper"], "venue": "Artif. Intell. Med. 41(3) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "J", "author": ["X. Liu", "R. Lu"], "venue": "Ma, et al., Privacy-preserving patient-centric clinical decision support system on naive Bayesian classification, IEEE journal of biomedical and health informatics. 20(2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Breast cancer diagnosis using genetically optimized neural network model", "author": ["A. Bhardwaj", "A. Tiwari"], "venue": "Expert Systems with Applications. 42(1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["F. Amato"], "venue": "L\u00f3pez, E.M. Pe\u00f1a-M\u00e9ndez, et al., Artificial neural networks in medical diagnosis, Journal of applied biomedicine. 11(2) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Intelligent heart disease prediction system using data mining techniques", "author": ["S. Palaniappan", "R. Awang"], "venue": "IEEE/ACS International Conference on Computer Systems and Applications . ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "A", "author": ["J.A. Sanz", "M. Galar"], "venue": "Jurio, et al., Medical diagnosis of cardiovascular diseases using an interval-valued fuzzy rule-based classification system, Applied Soft Computing. 20(1) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Application of Fuzzy Logic for Decision-Making in Medical Expert Systems", "author": ["N.A. Korenevskiy"], "venue": "Biomedical Engineering. 49(1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolutionary computation in medicine: an overview", "author": ["C.A. Pena-Reyes", "M. Sipper"], "venue": "Artificial Intelligence in Medicine. 19(1) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Medical Diagnosis using Soft Computing Techniques: A Review", "author": ["A. Jain"], "venue": "Internationa l Journal of Artificial Intelligence and Knowledge Discovery. 5(3) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["W.P. Goh", "X. Tao"], "venue": "Zhang, et al., Decision support systems for adoption in dental clinics : a survey, Knowledge-Based Systems. 104(1) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning. 62(1-2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Archetype sub-ontology: Improving constraint-based clinical knowledge model in electronic health records", "author": ["A.K. Sari", "W. Rahayu", "M. Bhatt"], "venue": "Knowledge-Based Systems. 26(1) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "A hybrid knowledge-based approach to supporting the medical prescription for general practitioners: Real case in a Hong Kong Learning and inference in knowledge-based probabilistic model for medical diagnosis", "author": ["S.L. Ting", "S.K. Kwok", "A.H.C. Tsang"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Evaluating the state-of-the-art in automatic deidentification", "author": ["\u00d6. Uzuner", "Y. Luo", "P. Szolovits"], "venue": "Journal of the American Medical Informatics Association. 14(5) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploiting Wikipedia as external knowledge for named entity recognition", "author": ["J. Kazama", "K. Torisawa"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "D", "author": ["M. Song", "W.C. Kim"], "venue": "Lee, et al., PKDE4J: Entity and relation extraction for public knowledge discovery, Journal of biomedical informatics. 57(1) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Beitrag zur theorie des ferromagnetismus", "author": ["E. Ising"], "venue": "Zeitschrift f\u00fcr Physik A Hadrons and Nuclei. 31(1) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1925}, {"title": "Optimal perceptual inference", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1983}, {"title": "A simple and efficient sampling method for estimating AP and NDCG", "author": ["E. Yilmaz", "E. Kanoulas", "J.A. Aslam"], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "A decision tree [4-5] is a classic algorithm in the medical classification domain, one that uses the information entropy method; however, it is sensitive to inconsistencies in the data.", "startOffset": 16, "endOffset": 21}, {"referenceID": 1, "context": "A decision tree [4-5] is a classic algorithm in the medical classification domain, one that uses the information entropy method; however, it is sensitive to inconsistencies in the data.", "startOffset": 16, "endOffset": 21}, {"referenceID": 2, "context": "The support vector machine [6-8] has a solid theoretical basis for the classification task; because of its efficient selection of features, it has higher predictive accuracy than decision trees.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "The support vector machine [6-8] has a solid theoretical basis for the classification task; because of its efficient selection of features, it has higher predictive accuracy than decision trees.", "startOffset": 27, "endOffset": 32}, {"referenceID": 4, "context": "The support vector machine [6-8] has a solid theoretical basis for the classification task; because of its efficient selection of features, it has higher predictive accuracy than decision trees.", "startOffset": 27, "endOffset": 32}, {"referenceID": 5, "context": "Bayesian networks [9-10], which are based on Bayesian theory [11-12], describe the dependence relationship between the symptom variables and the disease variables; these can be used in medical diagnosis.", "startOffset": 18, "endOffset": 24}, {"referenceID": 6, "context": "Bayesian networks [9-10], which are based on Bayesian theory [11-12], describe the dependence relationship between the symptom variables and the disease variables; these can be used in medical diagnosis.", "startOffset": 18, "endOffset": 24}, {"referenceID": 7, "context": "Bayesian networks [9-10], which are based on Bayesian theory [11-12], describe the dependence relationship between the symptom variables and the disease variables; these can be used in medical diagnosis.", "startOffset": 61, "endOffset": 68}, {"referenceID": 8, "context": "Bayesian networks [9-10], which are based on Bayesian theory [11-12], describe the dependence relationship between the symptom variables and the disease variables; these can be used in medical diagnosis.", "startOffset": 61, "endOffset": 68}, {"referenceID": 9, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 16, "endOffset": 23}, {"referenceID": 10, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 16, "endOffset": 23}, {"referenceID": 11, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 16, "endOffset": 23}, {"referenceID": 12, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 42, "endOffset": 49}, {"referenceID": 13, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 42, "endOffset": 49}, {"referenceID": 14, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 80, "endOffset": 87}, {"referenceID": 15, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 80, "endOffset": 87}, {"referenceID": 16, "context": "4 networks (NN) [13-15], fuzzy logic (FL) [16-17], and genetic algorithms (GAs) [18-20].", "startOffset": 80, "endOffset": 87}, {"referenceID": 17, "context": "Although Markov logic network [21] is a probabilist ic inferential model based on the first-order logic rules, it only applies to binary features which is against the numerical characteristic of symptom.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "As the crucial carrier of recorded medical activity, EMRs contain significant medical knowledge [23-24].", "startOffset": 96, "endOffset": 103}, {"referenceID": 19, "context": "As the crucial carrier of recorded medical activity, EMRs contain significant medical knowledge [23-24].", "startOffset": 96, "endOffset": 103}, {"referenceID": 20, "context": "These CEMRs, which have had protected health information (PHI) [25] removed, come from the Second Affiliated Hospital of Harbin Medical University, and we have obtained the usage rights for research.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "The recognition of named entities [26] and entity relationships [27] is an important aspect", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "The recognition of named entities [26] and entity relationships [27] is an important aspect", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "From a probability-and-statistics point of view, MLN is based on the methodology of Markov networks (MNs) [30].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "From statistical physics, we can express the potential function as an energy function [31], rewriting ( ) D \uf066 as ( ) exp( ( )) D D \uf066 \uf065 \uf03d \uf02d (4) where ( ) D \uf065 is often called an energy function.", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "unrestricted Boltzmann machine [32], which is the one of the earliest types of Markov network.", "startOffset": 31, "endOffset": 35}, {"referenceID": 26, "context": "used discounted cumulative gain (DCG) [33] as the indicator to measure the accuracy of the diagnosis result.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": null, "creator": "Microsoft\u00ae Word 2013"}}}