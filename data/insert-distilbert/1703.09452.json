{"id": "1703.09452", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "SEGAN: Speech Enhancement Generative Adversarial Network", "abstract": "current speech enhancement techniques operate naturally on the spectral domain and / or exploit some higher - level feature. the majority of them tackle a limited number sets of noise conditions and they rely on first - order statistics. to mainly circumvent these issues, deep networks are being increasingly properly used, thanks, to their ability to learn complex functions from large example sets. in this work, we propose via the use of generative adversarial input networks for speech resolution enhancement. in contrast to current techniques, we presently operate at the waveform level, training the model end - to - end, and successively incorporate 28 speakers and 40 different noise conditions into the same model, or such realizing that model parameters are shared across them. nevertheless we specifically evaluate the partially proposed model using an independent, unseen test set with two speakers and 20 explicitly alternative noise conditions. if the enhanced samples confirm the viability of the proposed model, data and both objective and subjective evaluations confirm the effectiveness value of it. with that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further innovative speech - centric design choices to improve their performance.", "histories": [["v1", "Tue, 28 Mar 2017 08:39:06 GMT  (514kb,D)", "http://arxiv.org/abs/1703.09452v1", null], ["v2", "Fri, 21 Apr 2017 12:37:03 GMT  (514kb,D)", "http://arxiv.org/abs/1703.09452v2", "5 pages, 4 figures"], ["v3", "Fri, 9 Jun 2017 11:34:06 GMT  (514kb,D)", "http://arxiv.org/abs/1703.09452v3", "5 pages, 4 figures, accepted in INTERSPEECH 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SD", "authors": ["santiago pascual", "antonio bonafonte", "joan serr\\`a"], "accepted": false, "id": "1703.09452"}, "pdf": {"name": "1703.09452.pdf", "metadata": {"source": "CRF", "title": "SEGAN: Speech Enhancement Generative Adversarial Network", "authors": ["Santiago Pascual", "Antonio Bonafonte", "Joan Serr\u00e0"], "emails": ["santi.pascual@upc.edu,", "antonio.bonafonte@upc.edu", "joan.serra@telefonica.com"], "sections": [{"heading": "1. Introduction", "text": "Speech enhancement tries to improve the intelligibility and quality of speech contaminated by additive noise [1]. Its main applications are related to improving the quality of mobile communications in noisy environments. However, we also find important applications related to hearing aids and cochlear implants, where enhancing the signal before amplification can significantly reduce discomfort and increase intelligibility [2]. Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].\nClassic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10]. Neural networks have been also applied to speech enhancement since the 80s [11, 12]. Recently, the denoising auto-encoder architecture [13] has been widely adopted. However, recurrent neural networks (RNNs) are also used. For instance, the recurrent denoising autoencoder has shown significant performance exploiting the temporal context information in embedded signals. Most recent approaches apply long short-term memory RNNs to the denoising task [4, 14]. In [15] and [16], noise features are estimated and included in the input features of deep neural networks. The use of dropout, post-filtering, and perceptually motivated metrics are shown to be effective.\nMost of the current systems are based on the short-time Fourier analysis/synthesis framework [1]. They only modify the spectrum magnitude, as it is often claimed that short-time phase\nis not important for speech enhancement [17]. However, further studies [18] show that significant improvements of speech quality are possible especially when a clean phase spectrum is known. In 1988, Tamura et al. [11] proposed a deep network that worked directly on the raw audio waveform, but they used feed-forward layers that worked frame-by-frame (60 samples) on a speaker-dependent and isolated-word database.\nA recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22]. As far as we are concerned, GANs have not yet been applied to any speech generation nor enhancement task, so this is the first approach to use the adversarial framework to generate speech signals.\nThe main advantages of the proposed speech enhancement GAN are:\n\u2022 It provides a quick enhancement process. No causality is required and, hence, no recursive operation like in RNNs.\n\u2022 It works end-to-end, with the raw audio. Therefore, no hand-crafted features are extracted and, with that, no explicit assumptions about the raw data are done.\n\u2022 It learns from different speakers and noise types, and incorporates them together into the same shared parametrization. This makes the system simple and generalizable in those dimensions.\nIn the following, we give an overview of GANs (Sec. 2). Next, we describe the proposed model (Sec. 3) and its experimental setup (Sec. 4). We finally report the results (Sec. 5) and discuss some conclusions (Sec. 6)."}, {"heading": "2. Generative Adversarial Networks", "text": "GANs [19] are generative models that learn to map samples z from some prior distribution Z to samples x from another distribution X , which is the one of the training examples (e.g., images, audio, etc.). The component within the GAN structure that performs the mapping is called the generator (G), and its main task is to learn an effective mapping that can imitate the real data distribution to generate novel samples related to those of the training set. Importantly, G does so not by memorizing input-output pairs, but by mapping the data distribution characteristics to the manifold defined in our prior Z .\nThe way in which G learns to do the mapping is by means of an adversarial training, where we have another component, called the discriminator (D). D is typically a binary classifier, and its inputs are either real samples, coming from the dataset that G is imitating, or fake samples, made up by G. The adversarial characteristic comes from the fact that D has to classify the samples coming from X as real, whereas the samples\nar X\niv :1\n70 3.\n09 45\n2v 1\n[ cs\n.L G\n] 2\n8 M\nar 2\n01 7\ncoming from G, X\u0302 , have to be classified as fake. This leads to G trying to fool D, and the way to do so is that G adapts its parameters such that D classifies G\u2019s output as real. During back-propagation, D gets better at finding realistic features in its input and, in turn, G corrects its parameters to move towards the real data manifold described by the training data (Fig. 1). This adversarial learning process is formulated as a minimax game between G and D, with the objective\nmin G max D V (D,G) = Ex\u223cpdata(x) [logD(x)]+\n+ Ez\u223cpz(z) [log (1\u2212D (G (z)))] . (1)\nWe can also work with a conditioned version of GANs, where we have some extra information in G and D to perform mapping and classification (see [20] and references therein). In that case, we may add some extra input xc, from which we would change the objective function to\nmin G max D V (D,G) = Ex\u223cpdata(x,xc) [logD(x, xc)]+\n+ Exc\u223cpdata(xc),z\u223cpz(z) [log (1\u2212D (G (z, xc)))] . (2)\nThere have been recent improvements in the GAN methodology to stabilize training and increase the quality of the generated samples in G, because the classic approach suffered from vanishing gradients due to the sigmoid cross-entropy loss function that was used to compute the cost. To solve this, we employ the least-squares GAN (LSGAN) approach [21], which substitutes the cost function by the least-squares function with binary coding (1 is real, 0 is fake). With this, the formulation in Eq. 2 changes to\nmin D\nVLSGAN(D) = 1\n2 Ex\u223cpdata(x,xc)[(D(x, xc)\u2212 1) 2]+\n+ 1\n2 Exc\u223cpdata(xc),z\u223cpz(z)[D(G(z, xc)) 2]\n(3)\nmin G\nVLSGAN(G) = 1\n2 Ex\u223cpdata(xc),z\u223cpz(z)[(D(G(z, xc))\u2212 1) 2].\n(4)"}, {"heading": "3. Speech Enhancement GAN", "text": "The enhancement problem is defined so that we have an input noisy signal x\u0303 and we want to clean it to obtain the enhanced signal x\u0302. We propose to do so with a speech enhancement GAN\n(SEGAN). In our case, the G network performs the enhancement. Its inputs are the noisy speech signal x\u0303 together with the latent representation z, and its output is the enhanced version x\u0302 = G(x\u0303). We design G to be fully convolutional, so that there are no fully connected layers at all. This enforces the network to focus on temporally-close correlations in the input signal and throughout the whole layering process. Furthermore, it reduces the number of training parameters and therefore training time.\nThe G network is structured similarly to an auto-encoder (Fig. 2). In the encoding stage, the input signal is projected and compressed through a number of strided convolutional layers followed by PReLUs [23], getting a convolution result out of every N steps of the filter . We choose strided convolutions as they were shown to be more stable for GAN training than other pooling approaches [22]. Decimation is done until we get a condensed representation, called the thought vector c, which gets concatenated with the latent vector z. The encoding process is reversed in the decoding stage by means of fractionalstrided transposed convolutions (sometimes called deconvolutions), followed again by PReLUs.\nThe G network also features skip connections, connecting each encoding layer to its homologous decoding layer, and bypassing the compression performed in the middle of the model (Fig. 2). This is done because the input and the output of the model share the same underlying structure, which is that of the natural speech, and many low level details could be lost to reconstruct the speech waveform properly if we force all the information to flow through the compression bottleneck. Skip connections directly pass the fine-grained information of the waveform to the decoding stage. In addition, they offer a better training behavior, as the gradients can flow deeper through the whole structure without suffering much vanishing [24].\nAn important feature of G is its end-to-end structure, so that it processes raw speech sampled at 16 kHz, getting rid of any\nintermediate transformations to extract acoustic features (contrasting to many common pipelines). In this type of model, we have to be careful with typical regression losses like mean absolute error or mean squared error, as noted in the raw speech generative model WaveNet [25]. These losses work under strong assumptions on how our output distribution is shaped and, therefore, impose important modeling limitations, like not allowing multi-modal distributions and biasing the predictions towards an average of all the possible predictions. Our solution to overcome these limitations is to use the generative adversarial setting. This way, D is in charge of transmitting information to G of what is real and what is fake in the data it is getting, such that G can slightly correct its output waveform towards the realistic distribution, getting rid of the noisy signals as those are signaled to be fake. In this sense, D can be understood as learning some sort of loss detailing the output distribution corrections required for G\u2019s output to look real.\nIn preliminary experiments, we found it convenient to add a secondary component to the loss of G in order to minimize the distance between its generations and the clean examples. To measure such distance, we chose the L1 norm, as it has been proven to be effective in the image manipulation domain [20, 26]. This way, we let the adversarial component to add more fine-grained and realistic results. The magnitude of theL1 norm is controlled by a new hyper-parameter \u03bb, so that the G loss in Eq. 4 becomes\nmin G\nVLSGAN(G) = 1\n2 Ex\u223cpdata(xc),z\u223cpz(z)[(D(G(z, xc))\u2212 1) 2]+\n+ \u03bb \u2016G(z, x\u0303)\u2212 x\u20161. (5)"}, {"heading": "4. Experimental Setup", "text": ""}, {"heading": "4.1. Data Set", "text": "To evaluate the effectiveness of the SEGAN, we resort to the data set by Valentini et al. [27]. We choose it because it is open and available1, and because the amount and type of data fits our purposes for this work: generalizing on many types of noise for many different speakers. The data set is a selection of 30 speakers from the Voice Bank corpus [28]: 28 are included in the train set and 2 in the test set.\nTo make the noisy training set, a total of 40 different conditions are considered [27]: 10 types of noise (2 artificial and 8 from the Demand database [29]) with 4 signal-to-noise ratio (SNR) each (15, 10, 5, and 0 dB). There are around 10 different sentences in each condition per training speaker. To make the test set, a total of 20 different conditions are considered [27]: 5 types of noise (all from the Demand database) with 4 SNR each (17.5, 12.5, 7.5, and 2.5 dB). There are around 20 different sentences in each condition per test speaker. Importantly, the test set is totally unseen by (and different from) the training set, using different speakers and conditions."}, {"heading": "4.2. SEGAN Setup", "text": "The model is trained for 86 epochs with RMSprop [30] and a learning rate of 0.0002, using an effective batch size of 400. We structure the training examples in two pairs (Fig. 3): the real pair, composed of a noisy signal and a clean signal (x\u0303 and x), and the fake pair, composed of a noisy signal and an enhanced signal (x\u0303 and x\u0302). To adequate the data set files to our waveform\n1http://dx.doi.org/10.7488/ds/1356\ngeneration purposes, we down-sample the original utterances from 48 kHz to 16 kHz. During train, we extract chunks of waveforms with a sliding window of approximately one second of speech (16384 samples) every 500 ms (50% overlap). During test, we basically slide the window with no overlap through the whole duration of our test utterance and concatenate the results at the end of the stream.\nRegarding the \u03bbweight of ourL1 regularization, after some experimentation, we set it to 100 for the whole training. We initially set it to 1, but we observed that the G loss was two orders of magnitude under the adversarial one, so the L1 had no practical effect on the learning. Once we set it to 100, we saw a minimization behavior in the L1 and an equilibrium behavior in the adversarial one. As the L1 got lower, the quality of the output samples increased, which we hypothesize helped G being more effective in terms of realistic generation.\nRegarding the architecture, G is composed of 22 onedimensional strided convolutional layers of filter width 31 and strides of N = 2. The amount of filters per layer increases so that the depth gets larger as the width (duration of signal in time) gets narrower. The resulting dimensions per layer, being it samples \u00d7 feature maps, is 16384\u00d71, 8192\u00d716, 4096\u00d732, 2048\u00d732, 1024\u00d764, 512\u00d764, 256\u00d7128, 128\u00d7128, 64\u00d7256, 32\u00d7256, 16\u00d7512, and 8\u00d71024. There, we sample the noise samples z from our prior 8\u00d71024-dimensional normal distribution N (0, I). As mentioned, the decoder stage of G is a mirroring of the encoder with the same filter widths and the same amount of filters per layer. However, skip connections and the addition of the latent vector make the number of feature maps in every layer to be doubled.\nThe network D follows the same one-dimensional convolutional structure as G\u2019s encoder stage, and it fits to the conventional topology of a convolutional classification network. The differences are that (1) it gets two input channels of 16384 samples, (2) it uses LeakyReLU non-linearities with \u03b1 = 0.3, and (3) in the last activation layer there is a one-dimensional convolution layer with one filter of width one that does not downsample the hidden activations (1\u00d71 convolution). The latter (3) reduces the amount of parameters required for the final classification neuron, which is fully connected to all hidden activations with a linear behavior. This means that we reduce the amount of required parameters in that fully connected component from 8\u00d7 1024 = 8192 to 8, and the way in which the 1024 channels are merged is learnable in the parameters of the convolution.\nAll the project is developed with the TensorFlow deep learning framework [31], and the code is available at https: //github.com/santi-pdp/segan. We refer to this resource for further fine-grained details of our implementation. A sample of the enhanced speech audios is provided at http: //veu.talp.cat/segan."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Objective Evaluation", "text": "As a first assessment of the performance of our approach, we compute the following objective measures to evaluate the quality of the enhanced speech over the 824 files of the test set. All measures are defined as predictors of the mean opinion score (MOS) that would be obtained in subjective tests, and all go from 1 (bad quality/high distortion/intrusive noise) to 5 (excellent quality/no degradation/not noticeable noise). All metrics compare the enhanced signal with the clean reference. They have been computed using the implementation included in [1], and available at the publisher website2.\n\u2022 PESQ: Perceptual evaluation of speech quality, using the wide-band version recommended in ITU-T P.862.2 [32].\n\u2022 CSIG: MOS rating of the signal distortion attending only to the speech signal [33].\n\u2022 CBAK: MOS rating of the intrusiveness of background noise [33].\n\u2022 COVL: MOS rating of the overall effect [33].\nTable 1 shows the results of these metrics. To have a comparative reference, the table also shows the results of these metrics when applied directly to the noisy signals and to signals filtered using the Wiener algorithm based on a priori SNR estimation [34], as provided in [1]. It can be observed how SEGAN gets slightly better PESQ and COVL than the noisy signal and the Wiener-enhanced signal. SEGAN reduces less noise than Wiener (CBAK), but produces less distortion in the speech signal (CSIG), achieving better overall results (COVL)."}, {"heading": "5.2. Subjective Evaluation", "text": "To get a better assessment, a perceptual test has been carried out to compare SEGAN with the noisy signal and the Wiener baseline. For that, 20 sentences were selected from the test set. As the database does not indicate the amount and type of noise for each file, the selection was done by listening to some of the provided noisy files, trying to balance different noise types. Most of the files have low SNR, but a few with high SNR were also included.\nA total of 19 listeners were presented with the 20 sentences in a randomized order. For each sentence, three versions were\n2https://www.crcpress.com/downloads/K14513/ K14513_CD_Files.zip\npresented, also in random order: noisy signal, Wiener-enhanced signal, and SEGAN-enhanced signal. For each signal, the listener rated the overall quality, using a scale from 1 to 5. In the description of the 5 categories, they were instructed to pay attention to both the signal distortion and the noise intrusiveness (e.g., 5=excellent: very natural speech with no degradation and not noticeable noise). Listeners could listen to each signal as many times as they wanted, and were asked to pay attention to the comparative rate of the three signals.\nThe results are shown in Table 2. It can be observed how SEGAN is preferred over both the noisy signal and the Wiener baseline. However, as there is a large variation in the SNR of the noisy signal, the MOS range, for all the systems, is very large, and the difference between Wiener and SEGAN is not significant. However, as the listeners compared all the systems at same time, it is possible to compute the comparative MOS (CMOS) by subtracting the MOS of the two systems being compared. Fig. 4 shows the CMOS when comparing SEGAN with respect to the noisy or the Wiener-enhanced signals. We can see how the signals generated by SEGAN are preferred over both. More specifically, SEGAN is preferred over the original (noisy) signal in 62% of the cases, while the noisy signal is preferred in 12.1% of the cases (no preference in 25.9% of the cases). With respect to the Wiener system, SEGAN is preferred in 47.8% of cases and Wiener is preferred in 24% of the cases (no preference in 28.2% of the cases)."}, {"heading": "6. Conclusions", "text": "In this work, an end-to-end speech enhancement method has been implemented within the generative adversarial framework. The model works as an encoder-decoder fully-convolutional structure, which makes it fast to operate for denoising waveform chunks. The results show that, not only the method is viable, but it can also represent an effective alternative to current approaches. Possible future work involves the exploration of better convolutional structures and the inclusion of perceptual weightings in the adversarial training, so that we reduce possible high frequency artifacts that might be introduced by the current model. Further experiments need to be done to compare SEGAN with other competitive approaches."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the project TEC2015-69266-P (MINECO/FEDER, UE)."}, {"heading": "8. References", "text": "[1] P. C. Loizou, Speech Enhancement: Theory and Practice, 2nd ed.\nBoca Raton, FL, USA: CRC Press, Inc., 2013.\n[2] L.-P. Yang and Q.-J. Fu, \u201cSpectral subtraction-based speech enhancement for cochlear implant patients in background noise,\u201d The Journal of the Acoustical Society of America, vol. 117, no. 3, pp. 1001\u20131004, 2005.\n[3] D. Yu, L. Deng, J. Droppo, J. Wu, Y. Gong, and A. Acero, \u201cA minimum-mean-square-error noise reduction algorithm on melfrequency cepstra for robust speech recognition,\u201d in Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4041\u20134044.\n[4] A. L. Maas, Q. V. Le, T. M. O\u2019Neil, O. Vinyals, P. Nguyen, and A. Y. Ng, \u201cRecurrent neural networks for noise reduction in robust asr.\u201d in Interspeech, 2012, pp. 22\u201325.\n[5] J. Ortega-Garcia and J. Gonzalez-Rodriguez, \u201cOverview of speech enhancement techniques for automatic speaker recognition,\u201d in Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, vol. 2, Oct 1996, pp. 929\u2013932 vol.2.\n[6] M. Berouti, R. Schwartz, and J. Makhoul, \u201cEnhancement of speech corrupted by acoustic noise,\u201d in Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP \u201979., vol. 4, Apr 1979, pp. 208\u2013211.\n[7] J. Lim and A. Oppenheim, \u201cAll-pole modeling of degraded speech,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, no. 3, pp. 197\u2013210, Jun 1978.\n[8] Y. Ephraim, \u201cStatistical-model-based speech enhancement systems,\u201d Proceedings of the IEEE, vol. 80, no. 10, pp. 1526\u20131555, Oct 1992.\n[9] M. Dendrinos, S. Bakamidis, and G. Carayannis, \u201cSpeech enhancement from noise: A regenerative approach,\u201d Speech Communication, vol. 10, no. 1, pp. 45\u201357, 1991.\n[10] Y. Ephraim and H. L. Van Trees, \u201cA signal subspace approach for speech enhancement,\u201d IEEE Transactions on speech and audio processing, vol. 3, no. 4, pp. 251\u2013266, 1995.\n[11] S. Tamura and A. Waibel, \u201cNoise reduction using connectionist models,\u201d in Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 1988, pp. 553\u2013556.\n[12] S. Parveen and P. Green, \u201cSpeech enhancement with missing data techniques using recurrent neural networks,\u201d in Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2004, pp. 733\u2013736.\n[13] X. Lu, Y. Tsao, S. Matsuda, and C. Hori, \u201cSpeech enhancement based on deep denoising autoencoder.\u201d in Interspeech, 2013, pp. 436\u2013440.\n[14] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le Roux, J. R. Hershey, and B. Schuller, \u201cSpeech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR,\u201d in Proc. of the Int. Conf. on Latent Variable Analysis and Signal Separation, 2015, pp. 91\u201399.\n[15] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. on Audio, Speech and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.\n[16] A. Kumar and D. Florencio, \u201cSpeech enhancement in multiplenoise conditions using deep neural networks,\u201d in Proc. of the Int. Speech Communication Association Conf. (INTERSPEECH), 2016, pp. 3738\u20133742.\n[17] D. Wang and J. Lim, \u201cThe unimportance of phase in speech enhancement,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 30, no. 4, pp. 679\u2013681, Aug 1982.\n[18] K. Paliwal, K. Wo\u0301jcicki, and B. Shannon, \u201cThe importance of phase in speech enhancement,\u201d Speech Communication, vol. 53, no. 4, pp. 465 \u2013 494, 2011. [Online]. Available: http://www. sciencedirect.com/science/article/pii/S0167639310002086\n[19] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d in Advances in neural information processing systems, 2014, pp. 2672\u20132680.\n[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-toimage translation with conditional adversarial networks,\u201d ArXiv: 1611.07004, 2016.\n[21] X. Mao, Q. Li, H. Xie, R. Y. K. Lau, and Z. Wang, \u201cLeast squares generative adversarial networks,\u201d ArXiv: 1611.04076, 2016.\n[22] A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised representation learning with deep convolutional generative adversarial networks,\u201d arXiv preprint arXiv:1511.06434, 2015.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDelving deep into rectifiers: Surpassing human-level performance on imagenet classification,\u201d in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.\n[24] \u2014\u2014, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.\n[25] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d CoRR abs/1609.03499, 2016.\n[26] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, \u201cContext encoders: Feature learning by inpainting,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2536\u20132544.\n[27] C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi, \u201cInvestigating rnn-based speech enhancement methods for noiserobust text-to-speech,\u201d in 9th ISCA Speech Synthesis Workshop, pp. 146\u2013152.\n[28] C. Veaux, J. Yamagishi, and S. King, \u201cThe voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\u201d in Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE), 2013 International Conference. IEEE, 2013, pp. 1\u20134.\n[29] J. Thiemann, N. Ito, and E. Vincent, \u201cThe diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings,\u201d The Journal of the Acoustical Society of America, vol. 133, no. 5, pp. 3591\u20133591, 2013.\n[30] T. Tieleman and G. Hinton, \u201cLecture 6.5-RMSprop: divide the gradient by a running average of its recent magnitude,\u201d COURSERA: Neural Networks for Machine Learning 4, 2, 2012.\n[31] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., \u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems,\u201d arXiv preprint arXiv:1603.04467, 2016.\n[32] P.862.2: Wideband extension to Recommendation P.862 for the assessment of wideband telephone networks and speech codecs, ITU-T Std. P.862.2, 2007.\n[33] Y. Hu and P. C. Loizou, \u201cEvaluation of objective quality measures for speech enhancement,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 229\u2013238, Jan 2008.\n[34] P. Scalart and J. V. Filho, \u201cSpeech enhancement based on a priori signal to noise estimation,\u201d in 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, vol. 2, May 1996, pp. 629\u2013632 vol. 2."}], "references": [{"title": "Speech Enhancement: Theory and Practice, 2nd ed", "author": ["P.C. Loizou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Spectral subtraction-based speech enhancement for cochlear implant patients in background noise", "author": ["L.-P. Yang", "Q.-J. Fu"], "venue": "The Journal of the Acoustical Society of America, vol. 117, no. 3, pp. 1001\u20131004, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A minimum-mean-square-error noise reduction algorithm on melfrequency cepstra for robust speech recognition", "author": ["D. Yu", "L. Deng", "J. Droppo", "J. Wu", "Y. Gong", "A. Acero"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4041\u20134044.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural networks for noise reduction in robust asr.", "author": ["A.L. Maas", "Q.V. Le", "T.M. O\u2019Neil", "O. Vinyals", "P. Nguyen", "A.Y. Ng"], "venue": "in Interspeech,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Overview of speech enhancement techniques for automatic speaker recognition", "author": ["J. Ortega-Garcia", "J. Gonzalez-Rodriguez"], "venue": "Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, vol. 2, Oct 1996, pp. 929\u2013932 vol.2.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Enhancement of speech corrupted by acoustic noise", "author": ["M. Berouti", "R. Schwartz", "J. Makhoul"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP \u201979., vol. 4, Apr 1979, pp. 208\u2013211.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1979}, {"title": "All-pole modeling of degraded speech", "author": ["J. Lim", "A. Oppenheim"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, no. 3, pp. 197\u2013210, Jun 1978.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1978}, {"title": "Statistical-model-based speech enhancement systems", "author": ["Y. Ephraim"], "venue": "Proceedings of the IEEE, vol. 80, no. 10, pp. 1526\u20131555, Oct 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Speech enhancement from noise: A regenerative approach", "author": ["M. Dendrinos", "S. Bakamidis", "G. Carayannis"], "venue": "Speech Communication, vol. 10, no. 1, pp. 45\u201357, 1991.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "A signal subspace approach for speech enhancement", "author": ["Y. Ephraim", "H.L. Van Trees"], "venue": "IEEE Transactions on speech and audio processing, vol. 3, no. 4, pp. 251\u2013266, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Noise reduction using connectionist models", "author": ["S. Tamura", "A. Waibel"], "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 1988, pp. 553\u2013556.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Speech enhancement with missing data techniques using recurrent neural networks", "author": ["S. Parveen", "P. Green"], "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2004, pp. 733\u2013736.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech enhancement based on deep denoising autoencoder.", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "in Interspeech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B. Schuller"], "venue": "Proc. of the Int. Conf. on Latent Variable Analysis and Signal Separation, 2015, pp. 91\u201399.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE/ACM Trans. on Audio, Speech and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement in multiplenoise conditions using deep neural networks", "author": ["A. Kumar", "D. Florencio"], "venue": "Proc. of the Int. Speech Communication Association Conf. (INTERSPEECH), 2016, pp. 3738\u20133742.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The unimportance of phase in speech enhancement", "author": ["D. Wang", "J. Lim"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 30, no. 4, pp. 679\u2013681, Aug 1982.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1982}, {"title": "The importance of phase in speech enhancement", "author": ["K. Paliwal", "K. W\u00f3jcicki", "B. Shannon"], "venue": "Speech Communication, vol. 53, no. 4, pp. 465 \u2013 494, 2011. [Online]. Available: http://www. sciencedirect.com/science/article/pii/S0167639310002086", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, 2014, pp. 2672\u20132680.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Image-toimage translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "ArXiv: 1611.07004, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y.K. Lau", "Z. Wang"], "venue": "ArXiv: 1611.04076, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["\u2014\u2014"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2536\u20132544.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigating rnn-based speech enhancement methods for noiserobust text-to-speech", "author": ["C. Valentini-Botinhao", "X. Wang", "S. Takaki", "J. Yamagishi"], "venue": "9th ISCA Speech Synthesis Workshop, pp. 146\u2013152.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 0}, {"title": "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database", "author": ["C. Veaux", "J. Yamagishi", "S. King"], "venue": "Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE), 2013 International Conference. IEEE, 2013, pp. 1\u20134.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "The diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings", "author": ["J. Thiemann", "N. Ito", "E. Vincent"], "venue": "The Journal of the Acoustical Society of America, vol. 133, no. 5, pp. 3591\u20133591, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-RMSprop: divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURS- ERA: Neural Networks for Machine Learning 4, 2, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation of objective quality measures for speech enhancement", "author": ["Y. Hu", "P.C. Loizou"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 229\u2013238, Jan 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Speech enhancement based on a priori signal to noise estimation", "author": ["P. Scalart", "J.V. Filho"], "venue": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, vol. 2, May 1996, pp. 629\u2013632 vol. 2.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Speech enhancement tries to improve the intelligibility and quality of speech contaminated by additive noise [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "However, we also find important applications related to hearing aids and cochlear implants, where enhancing the signal before amplification can significantly reduce discomfort and increase intelligibility [2].", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].", "startOffset": 136, "endOffset": 145}, {"referenceID": 3, "context": "Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].", "startOffset": 136, "endOffset": 145}, {"referenceID": 4, "context": "Speech enhancement has also been successfully applied as a preprocessing stage in speech recognition and speaker identification systems [3, 4, 5].", "startOffset": 136, "endOffset": 145}, {"referenceID": 5, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 148, "endOffset": 155}, {"referenceID": 9, "context": "Classic speech enhancement methods are spectral subtraction [6], Wiener filtering [7], statistical model-based methods [8], and subspace algorithms [9, 10].", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Neural networks have been also applied to speech enhancement since the 80s [11, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Neural networks have been also applied to speech enhancement since the 80s [11, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "Recently, the denoising auto-encoder architecture [13] has been widely adopted.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "Most recent approaches apply long short-term memory RNNs to the denoising task [4, 14].", "startOffset": 79, "endOffset": 86}, {"referenceID": 13, "context": "Most recent approaches apply long short-term memory RNNs to the denoising task [4, 14].", "startOffset": 79, "endOffset": 86}, {"referenceID": 14, "context": "In [15] and [16], noise features are estimated and included in the input features of deep neural networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [15] and [16], noise features are estimated and included in the input features of deep neural networks.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "Most of the current systems are based on the short-time Fourier analysis/synthesis framework [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "They only modify the spectrum magnitude, as it is often claimed that short-time phase is not important for speech enhancement [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "However, further studies [18] show that significant improvements of speech quality are possible especially when a clean phase spectrum is known.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "[11] proposed a deep network that worked directly on the raw audio waveform, but they used feed-forward layers that worked frame-by-frame (60 samples) on a speaker-dependent and isolated-word database.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 296, "endOffset": 308}, {"referenceID": 20, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 296, "endOffset": 308}, {"referenceID": 21, "context": "A recent breakthrough in the deep learning generative modeling field are generative adversarial networks (GANs) [19], which have achieved a good level of success in the computer vision field to generate realistic images and generalize well to pixel-wise, complex (high-dimensional) distributions [20, 21, 22].", "startOffset": 296, "endOffset": 308}, {"referenceID": 18, "context": "GANs [19] are generative models that learn to map samples z from some prior distribution Z to samples x from another distribution X , which is the one of the training examples (e.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "We can also work with a conditioned version of GANs, where we have some extra information in G and D to perform mapping and classification (see [20] and references therein).", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "To solve this, we employ the least-squares GAN (LSGAN) approach [21], which substitutes the cost function by the least-squares function with binary coding (1 is real, 0 is fake).", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "In the encoding stage, the input signal is projected and compressed through a number of strided convolutional layers followed by PReLUs [23], getting a convolution result out of every N steps of the filter .", "startOffset": 136, "endOffset": 140}, {"referenceID": 21, "context": "We choose strided convolutions as they were shown to be more stable for GAN training than other pooling approaches [22].", "startOffset": 115, "endOffset": 119}, {"referenceID": 23, "context": "In addition, they offer a better training behavior, as the gradients can flow deeper through the whole structure without suffering much vanishing [24].", "startOffset": 146, "endOffset": 150}, {"referenceID": 24, "context": "In this type of model, we have to be careful with typical regression losses like mean absolute error or mean squared error, as noted in the raw speech generative model WaveNet [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "To measure such distance, we chose the L1 norm, as it has been proven to be effective in the image manipulation domain [20, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 25, "context": "To measure such distance, we chose the L1 norm, as it has been proven to be effective in the image manipulation domain [20, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The data set is a selection of 30 speakers from the Voice Bank corpus [28]: 28 are included in the train set and 2 in the test set.", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "To make the noisy training set, a total of 40 different conditions are considered [27]: 10 types of noise (2 artificial and 8 from the Demand database [29]) with 4 signal-to-noise ratio (SNR) each (15, 10, 5, and 0 dB).", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "To make the noisy training set, a total of 40 different conditions are considered [27]: 10 types of noise (2 artificial and 8 from the Demand database [29]) with 4 signal-to-noise ratio (SNR) each (15, 10, 5, and 0 dB).", "startOffset": 151, "endOffset": 155}, {"referenceID": 26, "context": "To make the test set, a total of 20 different conditions are considered [27]: 5 types of noise (all from the Demand database) with 4 SNR each (17.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "The model is trained for 86 epochs with RMSprop [30] and a learning rate of 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "All the project is developed with the TensorFlow deep learning framework [31], and the code is available at https: //github.", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "They have been computed using the implementation included in [1], and available at the publisher website.", "startOffset": 61, "endOffset": 64}, {"referenceID": 31, "context": "\u2022 CSIG: MOS rating of the signal distortion attending only to the speech signal [33].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "\u2022 CBAK: MOS rating of the intrusiveness of background noise [33].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "\u2022 COVL: MOS rating of the overall effect [33].", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "To have a comparative reference, the table also shows the results of these metrics when applied directly to the noisy signals and to signals filtered using the Wiener algorithm based on a priori SNR estimation [34], as provided in [1].", "startOffset": 210, "endOffset": 214}, {"referenceID": 0, "context": "To have a comparative reference, the table also shows the results of these metrics when applied directly to the noisy signals and to signals filtered using the Wiener algorithm based on a priori SNR estimation [34], as provided in [1].", "startOffset": 231, "endOffset": 234}], "year": 2017, "abstractText": "Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.", "creator": "LaTeX with hyperref package"}}}