{"id": "1703.09891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "LabelBank: Revisiting Global Perspectives for Semantic Segmentation", "abstract": "semantic segmentation requires a detailed labeling of distributed image pixels by object category. information derived from local image patches is necessary to describe the detailed shape of individual objects. however, this geographic information is ambiguous and captures can result in noisy labels. global map inference spectra of image content can instead capture the general semantic concepts present. we advocate that holistic linguistic inference of image concepts provides valuable information for processing detailed pixel labeling. we all propose use a generic framework to leverage holistic information in the form of a labelbank for pixel - level segmentation.", "histories": [["v1", "Wed, 29 Mar 2017 05:58:21 GMT  (5228kb,D)", "http://arxiv.org/abs/1703.09891v1", "Pre-prints"]], "COMMENTS": "Pre-prints", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["hexiang hu", "zhiwei deng", "guang-tong zhou", "fei sha", "greg mori"], "accepted": false, "id": "1703.09891"}, "pdf": {"name": "1703.09891.pdf", "metadata": {"source": "CRF", "title": "LabelBank: Revisiting Global Perspectives for Semantic Segmentation", "authors": ["Hexiang Hu", "Zhiwei Deng", "Simon Fraser", "Guang-Tong Zhou", "Fei Sha", "Greg Mori"], "emails": ["hexiang.frank.hu@gmail.com", "zhiweid@sfu.ca", "zhouguangtong@gmail.com", "feisha@usc.edu", "mori@cs.sfu.ca"], "sections": [{"heading": null, "text": "We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches."}, {"heading": "1. Introduction", "text": "Great progress has been made in visual recognition. In tasks ranging from image classification to object detection, algorithms rival human performance in certain conditions. Semantic segmentation, labeling each pixel to depict semantic elements by detailed shapes and contours, is arguably a requisite element of full visual understanding of a scene. For applications such as robotics, autonomous driving, or other scene understanding endeavours, accurate delineation of object contours is necessary for success.\nHowever, detailed semantic segmentation is challenging \u2013 there exists significant ambiguity in fine-scale image\n\u2217equal contribution.\npatches that can result in noisy semantic segmentation outputs. The main focus of this paper is utilizing holistic information to filter noisy low-level semantic segmentation (see Figure 1). We term this holistic representation LabelBank, specifically defined as a continuous vector of confidences of which objects are likely to be present in an image.\nThis holistic LabelBank can be derived from a variety of sources. Akin to image classification, one could directly infer object content from global visual features describing an image (c.f. the seminal ObjectBank work [24]). Further, this information can be extracted more generally \u2013 in many scenarios we have additional meta data such as sentences\n1\nar X\niv :1\n70 3.\n09 89\n1v 1\n[ cs\n.C V\n] 2\n9 M\ndescribing an image or tag-style labels regarding image attributes. We demonstrate that our framework can be utilized in a range of settings, from purely visual information (no additional data beyond the image pixels) to situations where additional meta data are present.\nWe instantiate our ideas within a single framework for inferring the LabelBank representation and utilizing it for semantic segmentation. This framework can be used in common with a variety of state-of-the-art semantic segmentation networks. State-of-the-art methods for semantic segmentation leverage the successes of Convolutional Neural Networks (CNNs) [23]. CNNs have transformed the field of image classification, especially since the development of AlexNet [20]. There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc. Semantic segmentation utilizes these network structures, combined with dense output structures to label image pixels by semantic categories. A representative work is the Fully Convolutional Network (FCN) [34] that leverages skip features of CNNs to produce a detailed pixel labeling. Another example is the DeepLab [4] framework, which augments FCN with dilated convolution [40], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and obtains state-of-art semantic segmentation performance.\nCommon among these previous semantic segmentation methods is a focus on (layers of) low-level pixel analysis leading to semantic segmentation. State-of-the-art techniques combine this with graphical model-style techniques (CRF) and pooling structures to obtain high accuracy. The role of these additional components is to smooth out the noisy pixel labelings that result from the direct CNN analysis. As a complementary approach, we advocate for a holistic inference of LabelBank that globally suggests category labels that are likely to be present in the image.\nTo make use of the LabelBank representation in semantic segmentation, we leverage it to filter out false-positive pixel predictions \u2013 if the holistic information in the LabelBank suggests a semantic category is unlikely to be present, then pixels should be unlikely to be predicted as that label. We utilize these observations to propose a framework that unifies a LabelBank inference process and a detailed semantic segmentation process via a holistic filtering process. Our framework is generic and flexible enough to leverage different data sources / architectures in LabelBank inference and can integrate state-of-the-art semantic segmentation networks. For example, the LabelBank can be derived from cues ranging from image appearance to attributes to textual descriptions. The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4]. Finally, our holistic filtering leverages the information in the LabelBank to guide segmentation by refining noisy pixel predictions.\nContribution. We summarize our main contributions as: \u2022 First, we develop LabelBank to guide semantic segmen-\ntation, where LabelBank is a holistic representation of image content that can be derived from various sources. \u2022 Second, we construct holistic filtering that enables us to filter out false-positive pixel predictions under the guidance of LabelBank. \u2022 Third, we propose a neural network framework for semantic segmentation. We implement approaches for inferring LabelBank and conducting semantic segmentation, which facilitate the flow of global image information to pixel segmentation. Our framework is general, and could be incorporated into a variety of CNN-based semantic segmentation architectures. \u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27]. Experimental results show that the proposed LabelBank-based framework can be used to improve a variety of state-of-the-art semantic segmentation approaches."}, {"heading": "2. Related Work", "text": "Semantic segmentation. The success of CNNs in object recognition has led to renewed attention on semantic segmentation. A representative work is the Fully Convolutional Network (FCN) [34] that uses skip features of CNNs for detailed pixel labeling. FCN combines multi-level feature descriptors to leverage coarse-to-fine local pixel information. In a recent advance, the atrous convolution is introduced by Chen et al. [4] as a technique to retain a large field of view while keeping fewer trainable weights in semantic segmentation networks. The same method termed as dilated convolution was also pursued by Yu and Koltun [40] by a cascading series of dilated convolution layers.\nAnother line of work pushes on refining the detailed shapes and contours of semantic segmentation. A fully connected CRF is proposed by Kra\u0308henbu\u0308hl and Koltun [19] as an efficient dense pixel modeling method. The fully connected pairwise CRF is adopted by Chen et al. [4] and Zheng et al. [41] on top of FCNs as a further refinement. These methods have achieved considerable improvement on the semantic segmentation task. Different from the abovementioned methods, our work leverages holistic LabelBank to filter out false-positive pixel predictions. Global-local information fusion. It has been shown that visual understanding benefits from exploiting and leveraging information of varying granularity. Deng et al. [8] modeled hierarchical and exclusive relations among semantic categories. Jain et al. [17] developed recurrent neural network structures for spatio-temporal inference. Hu et al. [16] proposed a neural graph inference model to propagate information among multiple levels of visual classes, in-\ncluding coarse labels, fine-grained categories and attributes. Amer et al. [1] adopted the and-or graph structure to reason about human activities at multiple levels of granularity. Gkioxari et al. [10] exploited contextual cues to improve action recognition.\nIn the realm of semantic segmentation, He et al. [15] proposed to use multi-scale CRFs to capture features at various image resolutions for semantic segmentation. Approaches for modeling object instances and their segmentations have also been developed [21, 39]. Another example of this line improves instance-aware semantic segmentation with object detection and classification [6, 7]. In contrast, we exploit holistic LabelBank for semantic segmentation, and do not require instance-level annotations."}, {"heading": "3. Framework", "text": "We now present our framework that leverages a holistic LabelBank in semantic segmentation. Recall that the LabelBank is defined as a continuous vector of confidences for the presence of semantic object categories in an image.\nFigure 2 provides an overview of our framework. In detail, it is composed of three components. First, we have a holistic inference process that takes varied information sources to reason about the LabelBank representation of an image. Second, we have a detailed semantic segmentation process to conduct preliminary semantic segmentation on the image to generate a segmentation map. Finally, we have a holistic filtering process that leverages the inferred LabelBank to filter out false-positive pixel predictions in the preliminary semantic segmentation results.\nOur framework is generic and can flexibly incorporate various data sources / architectures in LabelBank inference, leveraging different semantic segmentation networks. For example, the LabelBank can be derived from a variety of data sources, ranging from purely visual appearance to the cases where additional meta data are available, such as sentences describing image content and tag-like labels on image attributes. The LabelBank inference architecture also\nvaries depending on the available data sources. Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4]. In our implementation, we slightly modified FCN and DilatedNet for improved performance, by replacing the linear pixel classifier with a non-linear two-layer CNN. Due to space limitations, we present the details in the appendix.\nWe report our experimental results in Section 4 to verify the generalizability and effectiveness of our framework. In what follows, we first describe our holistic filtering process in Section 3.1, and then present our exemplar implementations of LabelBank inference in Section 3.2."}, {"heading": "3.1. Holistic Filtering", "text": "Holistic filtering is a key component in our framework. It uses the LabelBank representation derived from the holistic inference process to actively filter out false-positive pixel predictions in the segmentation map generated by the semantic segmentation process. Note that a segmentation map is typically organized as a matrix of confidences for assigning each semantic category label on each image pixel. Our idea is to use the LabelBank to recommend labels for pixel predictions \u2013 if LabelBank suggests a semantic label is unlikely to be present, then pixels should be unlikely to be predicted as that label as well.\nTo implement the idea, we weight the segmentation map predictions on each pixel by the LabelBank confidences. The weighting is done by a multiplication of both LabelBank and segmentation confidences transformed in a sigmoidal space. In the sigmoidal space, unlikely labels tend to receive low confidences close to 0, and likely labels tend to have high confidences close to 1. Therefore, the final confidence after the multiplication is high only if both the LabelBank and segmentation confidences are high. The final confidence is low whenever either the LabelBank confidence or the segmentation confidence is low. The detailed holistic filtering process is illustrated in Figure 2.\nFormally, we denote the LabelBank representation of an\nimage by a vector c \u2208 Rk\u00d71\u00d71, where k is the total number of semantic labels of interest. Each element of c indicates the confidence of observing the corresponding semantic category in the image. We also denote S \u2208 Rk\u00d7h\u00d7w as the segmentation map generated by the semantic segmentation process, where each element of S stores the confidence of observing a semantic category at the corresponding image location. Note that the segmentation map size h \u00d7 w is typically smaller than the original image size H \u00d7W , due to the pooling or down-sampling operations employed by most semantic segmentation networks (e.g., FCN [34] and DilatedNet [40, 4]).\nWe first map the LabelBank c and the segmentation map S to a sigmoidal space via:\nc\u03c3 = [ \u03c3(cl) ]l=k l=1 , S\u03c3 = [ \u03c3(Sl,i,j) ]l=k,i=h,j=w l=1,i=1,j=1 , (1)\nwhere \u03c3(x) = 11+e\u2212x is the sigmoid function. To weight each location of the segmentation map by the LabelBank, we replicate c\u03c3 to obtain an expanded LabelBank C\u03c3 \u2208 Rk\u00d7h\u00d7w of the same size as the segmentation map. Note that the confidence vector of the expanded LabelBank at each location is a copy of the LabelBank, i.e.,\nC\u03c3:,i,j = c \u03c3, \u22001 \u2264 i \u2264 h, 1 \u2264 j \u2264 w. (2)\nWe then conduct element-wise multiplication of the expanded LabelBank and the segmentation map to filter out false-positive predictions in the segmentation map. The refined segmentation map is computed as:\nSr = C\u03c3 S\u03c3. (3)\nIn Sr, a location receives a high confidence on a label only if both the LabelBank and the original pixel prediction are highly confident of predicting that label.\nFinally, we apply a logit function (i.e., the inverse sigmoid function) on each element of the refined segmentation map, so that the final confidences share the same value domain as the original segmentation map S. Formally, the LabelBank filtered segmentation map is derived as:\nSf = [ `(Srl,i,j) ]l=k,i=h,j=w l=1,i=1,j=1 , (4)\nwhere `(x) = log( x1\u2212x ) is the logit function. Note that all operations in our holistic filtering process are differentiable for gradient back-propagation to enable end-to-end training.\nAs Sf is typically sized smaller than the original image, we apply an up-sampling operation to generate a full semantic segmentation map. The up-sampling is simply done by bi-linear interpolation (following [4]) to increase the resolution to the original image size. We could also switch the order of up-sampling and holistic filtering in the pipeline to leverage LabelBank to refine the full segmentation map. This results in slightly better empirical performance, but increases the computational cost significantly. We keep upsampling after holistic filtering for the sake of efficiency."}, {"heading": "3.2. LabelBank Inference", "text": "The LabelBank can be inferred from a variety of data sources. A straightforward way is to use the image itself, i.e., visual appearance. We present two exemplar visual inference architectures in Sections 3.2.1 and 3.2.2, but other architectures are possible and can be easily adopted in our framework. Moreover, the LabelBank can also be derived from commonly available image meta data, such as tagbased labels on image attributes or sentences describing the image content. We implement sample architectures in Section 3.2.3 for this purpose. Furthermore, we have also experimented with combined visual appearance and meta data for a more accurate LabelBank representation.\nTo design the architectures, we would like to emphasize the following two principles. First, we prefer a process that takes an image and / or its meta data as input and produces the LabelBank representation as output, where the gradient can be back-propagated through for end-to-end training. Second, the architectures should not require any additional supervision beyond the readily available pixel labels in training data. The pixel labels have already depicted the semantic objects in the images, and we should make use of them in LabelBank inference."}, {"heading": "3.2.1 SPP for Visual Appearance", "text": "Following the design principles, we first implement a spatial pyramid pooling (SPP) based architecture for visual inference. This is illustrated in the top part of Figure 3.\nThe SPP architecture is motivated by the success of spatial pyramid pooling for image recognition [13]. It first employs a feature network (i.e., low-level layers of convolution and pooling) to extract a feature map on an image. Then it applies spatial pyramid pooling on the feature map, followed by a multi-layer perception to predict the LabelBank.\nFor training purposes, we obtain ground-truth LabelBank from the ground-truth pixel labels \u2013 an image is labeled by a semantic category if there is at least one pixel labeled with that category. We adopt a sigmoid activation\nlayer on the output of the multi-layer perception, and evaluate a categorical cross-entropy loss."}, {"heading": "3.2.2 DC for Visual Appearance", "text": "We further develop a dense cropping (DC) based architecture for visual inference. This is shown in the bottom part of Figure 3. The DC architecture is motivated by the fact that a semantic object rarely takes up the entire image, but instead a portion of an image. Thus, we conduct location-aware predictions that inspect densely cropped image windows.\nFor implementation, we first apply the same feature network as the SPP architecture. Then we densely crop the feature map to obtain features on image windows for locationaware predictions. We implement a two-layer CNN structure as the prediction model (details in the appendix). Each location-aware prediction results in a k-dimensional vector that describes the confidences for each semantic category to appear in the corresponding image window. Finally, we compose the LabelBank representation by a max-pooling over the location-aware predictions.\nWe impose a loss on the location-aware predictions. Specifically, the ground-truth labels on each image window are derived from the ground-truth pixel labels \u2013 an image window is labeled by a semantic category as long as the image window has at least one pixel labeled with that category. We first apply a sigmoid activation layer on the output of the location-aware predictions, and then evaluate a categorical cross-entropy loss."}, {"heading": "3.2.3 OHE and W2V Embedding for Meta Data", "text": "We also propose an embedding based architecture to infer LabelBank from meta data. This architecture first embeds meta data in a feature space. Specifically, depending on the type of the meta data, we could apply one-hot encoding for attributes (OHE architecture), or word2vec-style representation [30, 32] for sentence descriptions (W2V architecture). With the embedded features, we then apply a multilayer perception to predict the LabelBank of an image.\nWe train by imposing a loss on the final LabelBank predictions. The ground-truth LabelBank and loss are computed the same way as done in our SPP architecture."}, {"heading": "3.2.4 Training", "text": "Our framework is end-to-end trainable, because gradient back-propagation is enabled in the above LabelBank inference processes, preliminary semantic segmentation (FCN and DilatedNet), as well as holistic filtering (Section 3.1).\nWe jointly optimize both LabelBank inference loss and semantic segmentation loss in training. We have described the LabelBank inference losses in the above subsections. The segmentation loss is enforced on the full segmentation map obtained from holistic filtering. Following the standard setting of semantic segmentation [34, 4], we first apply a softmax activation layer to the full segmentation map,\nand then compute a categorical cross-entropy loss for the segmentation task. The balance between the two losses is controlled by a constant multiplier so that both losses have similar order of magnitude. To further clarify our implementation details, we will publicly release our code."}, {"heading": "4. Experiments", "text": "In this section, we conduct experiments to verify the effectiveness and generalizability of our framework. We examine a variety of settings for LabelBank inference and semantic segmentation. We present the experiments from two perspectives. First, we study the variants of LabelBank inference in Section 4.2, using different data sources and architectures. Second, we study the variants in our semantic segmentation process in Section 4.3, using FCN and DilatedNet. Before diving into the details, we first describe the common experimental settings in Section 4.1."}, {"heading": "4.1. Settings", "text": "Datasets. We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27]. All these datasets contain abundant contextual labels on pixels, and thus are challenging for semantic segmentation. Table 1 summarizes the five datasets. The appendix elaborates on the details of these datasets.\nBaselines. A direct baseline is to use the semantic segmentation process only, and ignore LabelBank inference and holistic filtering. As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN+ and DilatedNet+. Evaluation Protocols. Following [34], we evaluate four common performance metrics for semantic segmentation. The metrics are variations on pixel accuracy and region intersection over union (IU). Specifically, we denote by nij the number of pixels of category i predicted to belong to category j, ti = \u2211 j nij the total number of pixels of category i, and k the total number of categories. We evaluate:\n\u2022 Pixel Accuracy (pAcc): \u2211 i nii/ \u2211 i ti\n\u2022 Mean Accuracy (mAcc): (1/k) \u2211 i nii/ti\n\u2022 Mean IU (mIU): (1/k) \u2211 i nii/ ( ti + \u2211 j nji \u2212 nii ) \u2022 Frequency Weighted IU (fwIU):\n( \u2211 k tk) \u22121\u2211 i tinii/ ( ti + \u2211 j nji \u2212 nii ) In all the result tables, we highlight the best results in red and boldfaced, and the second best in blue and underline.\nFine Print. We provide the detailed data augmentation, feature networks, training strategies and qualitative visualizations in the appendix."}, {"heading": "4.2. Study on the LabelBank Inference Process", "text": "In this section, we evaluate variations on LabelBank inference, with different data sources and architectures. Specifically, we have SPP and DC for visual appearance, OHE for attributes, and W2V for textual descriptions. Here we fix our semantic segmentation network as FCN+, and show that our framework is effective and generic with respect to various LabelBank inference processes."}, {"heading": "4.2.1 Inference from Visual Appearance", "text": "We first instantiate our framework to infer LabelBank from image visual appearance, using SPP and DC. This setting requires no additional meta data, and is directly comparable with existing semantic segmentation approaches. We show that our framework achieves favorable performance (the results on NYUDv2 and SIFT-Flow are deferred to the appendix due to space limit). PASCAL-Context. We have compared our methods (SPP + FCN+ and DC + FCN+) with the baseline FCN+, as well as existing methods in the literature. The results are reported in Table 2. It clearly shows that our methods obtain better performance over all the compared methods. This verifies the effectiveness of our framework. Table 2 also validates the utility of LabelBank based holistic filtering \u2013 it boosts FCN+ substantially.\nIt is worth noting that DC+ FCN+ outperforms the current state-of-the-art method, DeepLab + COCO + CRF [4]. However, we did not use extra training data (e.g. the COCO dataset) and domain adaptation to obtain a better model. We applied neither CRFs to smooth the segmentation results, nor multi-scale test to refine segmentation at various image granularities. Therefore, we predict that our performance could be further boosted using these techniques. ADE20K. We follow the settings of [42] in our experiments. We train and test our frameworks on re-sized images\nof 384\u00d7384 pixels. To evaluate the performance against the ground-truth annotations, we re-size the resultant segmentation from 384\u00d7384 back to the original image size. This experiment enables us to directly compare with those reported in [42]. Note that this setting may lead to the loss of image details and object aspect ratios, thus yielding suboptimal performance. We summarize the results in Table 3. It shows that our DC + FCN+ achieves the best performance in all four metrics, when compared with the baseline FCN+ and existing approaches in literature. We also studied another setting using the original image sizes for training and testing, where the overall performance improves with high-resolution inputs. We present these results in the appendix. COCO-Stuff. We follow the standard setting of [3] in our experiments. The results are presented in Table 4. Again, it shows that DC + FCN+ outperforms all existing methods and the FCN+ baseline considerably. Discussion. The above experiments show that DC + FCN+ can achieve state-of-the-art performance. SPP + FCN+ is slightly worse, but still achieves reasonable results (especially on PASCAL-Context). DC works better than SPP because DC is an ensemble-like method that combines location-aware predictions. For the rest of the experiments, we use DC as the default architecture for inferring LabelBank from visual appearance."}, {"heading": "4.2.2 Inference from Meta Data", "text": "Next we conduct experiments to infer LabelBank from meta data, where we leverage image attributes and textual de-\nscriptions to help discovering holistic concepts in images. Specifically, we extract image attributes on ADE20K and COCO-Stuff. We rely on the provided taxonomies of the semantic object categories in the two datasets. For each object category of an image, we assign attributes to be its ancestor hypernyms. The image attributes are collected as the union of attributes for all semantic categories present in the image. We then apply the OHE architecture to infer the LabelBank representation.\nTextual descriptions are available on COCO-Stuff, and we apply our W2V architecture for LabelBank inference. In detail, we first perform GloVe embedding [32] on individual words, and then obtain a feature vector of the textual description by averaging the embeddings of all words. Advanced embedding techniques (e.g., skip-thought vector [18] or paragraph vector [22]) are applicable, and we leave these for future exploration.\nThe results are provided in Tables 3 and 4. Our methods, OHE + FCN+ and W2V + FCN+, provide a clear improvement over the baseline FCN+. It again verifies the generalizability of our framework for LabelBank inference."}, {"heading": "4.2.3 Combining Meta Data and Visual Appearance", "text": "Furthermore, we could use meta data and visual appearance together for a more accurate inference of the LabelBank representation. In our implementation, we simply append the OHE/W2V embedded features to each location of the feature map of our DC architecture. This results in the OHE + DC + FCN+ and W2V + DC + FCN+ methods, and we report their performance in Tables 3 and 4.\nIt clearly shows that OHE + DC + FCN+ and W2V + DC + FCN+ achieve the best performance on ADE20K and COCO-Stuff. Also note that combining meta data and visual appearance together generates better LabelBank than using individual data sources \u2013 OHE + DC + FCN+ outperforms OHE + FCN+ and DC + FCN+ on the two datasets."}, {"heading": "4.3. Study on the Semantic Segmentation Process", "text": "Here we verify the generalizability of our framework over different semantic segmentation processes. We use DC for LabelBank inference, and instantiate the segmentation process with FCN+ and DilatedNet+.\nThe comparative results are shown in Table 5. Please refer to Tables 2, 3 and 4 for a complete comparison with ex-\nisting approaches in literature. As shown, DC + FCN+ and DC + DilatedNet+ consistently improve over their baseline counterparts FCN+ and DilatedNet+ (except on SIFT-Flow, DC + FCN+ performs slightly worse than FCN+). This validates that our framework is general and can improve stateof-the-art CNN-based semantic segmentation networks."}, {"heading": "5. Analysis", "text": "In this section, we conduct empirical studies to further understand our framework. We asked the following two questions: (i) what is the maximum performance gain we could achieve if we have perfect LabelBank inference? and (ii) how could we refine the current LabelBank inference to further improve the performance? We provide answers and insights on these two questions in the following."}, {"heading": "5.1. Oracle LabelBank", "text": "Our experiments in Section 4 show that semantic segmentation benefits from the guidance of the LabelBank. However, by no means would we claim that our LabelBank inference is perfect. Therefore, an interesting question to raise is: how well could our framework perform if we had perfect LabelBank inference?\nWe answer this question by replacing our LabelBank inference process with a static process that returns \u201coracle\u201d LabelBank. The oracle LabelBank can be simply derived from the ground-truth pixel labels, by taking the value of infinity if a semantic class is present in the image, and negative infinity otherwise. We keep the rest of the framework intact, and train it end-to-end towards optimizing the segmentation loss (see Section 3.2.4 for details).\nWe compare FCN+, DC + FCN+, and Oracle + FCN+ in Figure 4. It clearly shows that Oracle + FCN+ using perfect\nLabelBank boosts the semantic segmentation performance significantly \u2013 for example, the mIU values are improved to 63.61% on PASCAL-Context and 54.26% on ADE20K. We believe that Oracle + FCN+ provides the upper-bound on performance we could possibly achieve with an FCN+ segmentation network with our framework. We would gradually approach this upper-bound as we obtain a better and better LabelBank inference process."}, {"heading": "5.2. Noisy LabelBank", "text": "Certainly we could never assume oracle LabelBank in real scenarios. For example, when applying a threshold of 0 on the LabelBank confidences derived by DC + FCN+, we observe 90.93% recall and 46.75% precision on PASCALContext images, and 69.07% recall and 47.59% precision on ADE20K images. We believe that precision and recall are two key measures on the goodness of the LabelBank representation. So here we study the impact of LabelBank precision and recall on the semantic segmentation performance, and provide insight on how to refine the current inference process for further performance gain.\nWe have evaluated our framework with various precision and recall settings on the LabelBank. We start with the oracle LabelBank, and contaminate it to certain precision and recall levels. Specifically, we degrade the precision by adding in more and more noisy image labels, and degrade the recall by removing more and more ground-truth image labels. We experimented with a FCN+ based semantic segmentation network, and leveraged the noisy LabelBank in holistic filtering. Due to computational resource limitations, we did not retrain the FCN+ networks (as we have done for Oracle + FCN+). Detailed experimental setup is provided in the supplementary material.\nFigure 5 plots the mean IU grid. It is shown that recall matters much more than precision \u2013 the mean IU decreases more significantly with respect to the degradation of recall\nthan precision. Note that this observation is obtained with noisy binary labels in the LabelBank, and may not apply directly to continuous LabelBank confidences. However, it does suggest a promising direction for future development in LabelBank inference \u2013 it could be beneficial to push hard on recalling ground-truth image labels."}, {"heading": "6. Conclusion", "text": "This paper motivates the use of a holistic LabelBank representation for semantic segmentation. We have presented a generic framework consisting of three components: LabelBank inference, semantic segmentation, and holistic filtering. The LabelBank inference process derives a holistic LabelBank representation of an image from various data sources and inference architectures. The semantic segmentation applies state-of-the-art CNN-based networks to generate a preliminary segmentation map. Finally, the holistic filtering process refines segmentation results by leveraging the LabelBank information to filter out false-positive pixel predictions. Experiments on benchmark semantic segmentation datasets show the effectiveness of the proposed framework. We believe that our solution is general and could be applied to many other applications, for example, improving object detection with holistic LabelBank, etc."}, {"heading": "Appendix A. Implementation Details", "text": "In this section, we explain the detailed implementation of FCN+ and DilatedNet+ in Section A.1, the two-layer CNN used in the DC architecture in Section A.2, multi-layer perceptron (MLP) in Section A.3, and our feature network in Section A.4.\nA.1. FCN+ and DilatedNet+\nThe semantic segmentation process is responsible for generating a preliminary segmentation map for the holistic filtering process to refine under the guidance of LabelBank. As mentioned in Section 3, we have slightly modified FCN [34] and DilatedNet [4] to serve as our semantic segmentation process, i.e., FCN+ and DilatedNet+. We show the detailed network architectures in Figure 6, and describe the details in the following.\nFCN+ has a similar structure as FCN [34]: it first applies a feature network to obtain a feature map on an image, then uses skip features to generate three segmentation maps of various down-sampling rates (8s, 16s and 32s), then fuses them together, and finally up-samples to the original image size for semantic segmentation. The only modification is that FCN+ applies our designed non-linear pixel classifier (instead of the linear pixel classifier in FCN) to generate a segmentation map from the input feature map.\nThe non-linear pixel classifier leverages non-linearity and dilated convolution to model pixel labeling. It consists of a dilated convolutional layer [40, 4] to aggregate contextual information, a ReLU layer for non-linearity, as well as a one-by-one convolutional layer to predict the segmentation map. In our experiments, we apply 3 by 3 convolutional kernels in the dilated convolutional layer, using a dilation rate of 2 under 512 channels.\nOur DilatedNet+ is derived from DilatedNet [4] with the same modification \u2013 we apply our non-linear pixel classifier on the feature map (obtained after dilated convolutions) to generate the segmentation map. We kept the rest of the structure the same as [4].\nWe show an empirical comparison between the original FCN/DilatedNet and our FCN+/DilatedNet+ in Table 6. For a fair comparison, we apply the same feature network, i.e., 16-layer VGGNet [36], for all methods. Our networks\nobtained slightly better performance than the original methods due to the usage of the non-linear pixel classifier.\nA.2. Two-Layer CNN in the DC Architecture\nAs mentioned in Section 3.2.2 and Figure 3, we use a two-layer CNN to infer the LabelBank representation from densely cropped image windows. The two-layer CNN is structured as follows. The first is a dilated convolutional layer. We use k\u00d7k convolutional kernels with a dilation rate of r under d channels. In our experiments, we empirically set the patch size as 224, k = 3, r = 2 and d = 512. We apply ReLU activation on its outputs for non-linearity. The second layer is a one-by-one convolutional layer to predict the current window\u2019s LabelBank representation. We then max-pool the location-aware predictions to compose the final LabelBank representation for the image.\nA.3. Multi-Layer Perceptron (MLP)\nWe apply the same MLP structure in our SPP, OHE and W2V architectures for LabelBank inference. It first employs a fully-connected layer to map the input vector to (2048) hidden units, then applies a ReLU activation function, and finally leverages another fully-connected layer to generate the output representation, i.e., the k-dimensional LabelBank.\nA.4. Feature Network\nWhen inferring the LabelBank from visual appearance, our SPP and DC architectures both employ a feature network (i.e., low-level layers of convolution and pooling) to extract a feature map on a given image. In our experiments, we empirically build the feature network from the 152-layer ResNet [14] by removing the top fully-connected layers and keeping the convolutional and pooling layers.\nAlso note that there is a feature network in our semantic segmentation process (i.e., FCN+ and DilatedNet+). In our experiments, we share the same feature network across LabelBank inference and semantic segmentation for computational efficiency and feature generalizability.\nWith the shared feature network, our framework looks similar to multi-task learning of classification and segmentation. However, we point out that the key to success lies\nin the LabelBank based holistic filtering. We provide empirical evidence to support this in Section E.1. Furthermore, we also conduct experiments in Section E.2 to show that our framework is flexible and can take various feature networks."}, {"heading": "Appendix B. Training Strategy", "text": "Optimization. We follow the practice of FCN [34] to train our framework \u2013 optimizing the objective by stochastic gradient descent with a small batch size (e.g., 1) and a large momentum (e.g., 0.99). We train for approximately 60 epochs and choose the best models through validation. Data augmentation. It has been shown that data augmentation is a practical technique to boost semantic segmentation performance. In our experiments, we have applied horizontal flipping as well as scale augmentation when training our networks. For scale augmentation, we randomly pick a scale factor in the set {0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3} to scale each image."}, {"heading": "Appendix C. Experimental Datasets", "text": "PASCAL-Context is an extension of the PASCAL VOC 2010 dataset, with detailed pixel-wise annotations [31]. The semantic labels include both objects and stuff present in the image. Following [31, 34], we evaluate our network on the most frequent 59 classes alongside one background class. The training and testing sets contain 4,998 and 5,105 im-\nages, respectively.\nADE20K is another dataset with densely annotated objects and stuff. We learn our model on the 20,210 training images, and report performance on the 2,000 validation images. We did not evaluate on its 5,000 test images as the ground-truth annotations are not publicly available. Following [42], we select the top 150 semantic classes ranked by their total pixel ratios, including 35 object classes and 115 stuff classes. The pixels from the 150 classes occupy 92.75% of all pixels in the dataset.\nCOCO-Stuff is a recent densely annotated dataset [3] with images sampled from COCO [26]. There are 80 thing and 91 stuff classes. Following the standard train/test split, we train on 9,000 images and test on 1,000 images.\nNYUDv2 is an RGB-D dataset on indoor scenes collected using Microsoft Kinect. It has 1,449 RGB-D images, with pixel-wise labels that have been coalesced into 40 semantic classes by Gupta et al. [11]. We experiment with the standard split of 795 training images and 654 testing images.\nSIFT-Flow dataset contains 2,688 images thoroughly annotated by LabelMe users [27, 28] with 33 semantic pixel labels (e.g., mountain, sun, bridge, etc). We use the same split as [27, 28] \u2013 2,488 images for training and 200 images for testing."}, {"heading": "Appendix D. Additional Results", "text": "Due to the space limit in our main paper, we defer our experimental results on SIFT-Flow and NYUDv2 here. These two datasets have no additional meta data beyond the image visual appearance. So here we compare our SPP + FCN+ and DC + FCN+ with the baseline FCN+ as well as stateof-the-art approaches. The details are described as follows.\nD.1. SIFT-Flow\nThe results are reported in Table 7, which shows that our methods perform the best over all the compared methods. Note that the current state-of-the-art FCN-8s [34] leverages the available pixel-wise geometric labels (i.e., horizontal, vertical and sky) as extra supervision, whereas our methods do not use them. These observations show the utility of our framework in semantic segmentation.\nIt is worth mentioning that a competitive method on the SIFT-Flow dataset is proposed by Lin et al. [25], achieving 88.1% pAcc, 53.4% mAcc and 44.9% mIU. However, this method up-samples training and testing images by a factor of two, and benefits from the high-resolution images to obtain superior performance. On the other hand, all the other methods including ours do not up-sample images, and thus are not directly comparable with [25].\nD.2. NYUDv2\nWe provide the evaluation results in Table 8. It is shown that our methods achieve the best performance, especially with DC + FCN+.\nOur methods only use the color images for training, ignoring the depth information. DC + FCN+ still improves 5.46% mIU over the state-of-the-art method, FCN-32s + RGB + HHA [34], which utilizes both the color and depth information. It again verifies the effectiveness of the proposed framework."}, {"heading": "Appendix E. Ablation Studies on Feature Network", "text": "In this section, we conduct ablation studies on our feature network. First, in Section E.1, we compare our framework with multi-task learning that also uses a shared feature network. Second, in Section E.2, we evaluate the flexibility of our framework against various feature networks.\nE.1. Comparison with Multi-Task Learning\nIn our framework implementation, we share the feature network across the visual appearance based LabelBank inference and the semantic segmentation process. The resultant framework has a similar structure as multi-task learning of classification and segmentation. However, we emphasize that our performance gain mainly comes from the LabelBank based holistic filtering, not the generic features learned from the shared feature network. We conduct experiments to support this in the following.\nWe have evaluated a standard multi-task learning method, where we have a classification branch in parallel with a semantic segmentation branch, with a shared feature network but without LabelBank based holistic filtering. We instantiate the classification and segmentation branches with DC and FCN+, respectively. The results on PASCALContext are reported in Table 9, which shows no significant performance gain with the multi-task training over the baseline FCN+. In contrast, our DC + FCN+ outperforms Multi-Task FCN+ and FCN+ substantially. It validates the critical usage of our LabelBank based holistic filtering for semantic segmentation.\nIt is also interesting to note that Multi-Task FCN+ has exactly the same network capacity (i.e., number of parameters) as our DC + FCN+, but it performs worse than ours. It shows that increasing network capacity might not necessarily increase performance. Instead, it is our LabelBank based holistic filtering that boosts semantic segmentation.\nE.2. Feature Network Variants\nWe have also conducted experiments to study the flexibility of our framework in taking various feature networks. In detail, we have tried the 16-layer VGGNet [36] and the 152-layer ResNet [14] respectively as our feature network. We evaluate our methods (i.e., DC + FCN+ and DC + DilatedNet+) and our baselines (i.e., FCN+ and DilatedNet+) accordingly. The comparative results on the PASCAL-Context dataset are reported in Table 10.\nThe table clearly shows that our methods improve over the baselines, using either VGGNet or ResNet. It verifies that our framework is flexible and can adopt various feature networks to improve semantic segmentation. This flexibility enables our framework to take advantage of the continual improvement in CNN architectures. Furthermore, it is beneficial to use ResNet as our feature network as it consistently outperforms its VGGNet counterpart. This is reasonable since ResNet employs a deeper structure than VGGNet to capture rich image features."}, {"heading": "Appendix F. Settings for Noisy LabelBank Experiments", "text": "In Section 5.2, we experimented with various precision and recall settings on LabelBank. The detailed setup is as follows.\nTo degrade the precision, we have added in np \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 10} noisy labels per image. Similarly, to degrade the recall, we have removed nr \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 10} groundtruth labels per image (if there are fewer than nr groundtruth labels in an image, we just remove them all). Note that np and nr can be set as fractional values to further refine the numerical accuracy of precision and recall. For example, setting nr = 2.3 means that we will first remove 2 (the integer part of nr) ground-truth labels on each image, and then randomly pick up 30% (the decimal part of nr) of the images to remove one more ground-truth label each. With this trick, we also tried nr \u2208 {0.2, 0.4, 0.6, 0.8} in our experiment. We conduct holistic filtering for semantic segmentation with an exhaustive grid search of np and nr values. Note that each combination of np and nr produces\nLabelBank with certain precision and recall of ground-truth labels (by averaging over all images). The mean IU grid is plotted in Figure 6."}, {"heading": "Appendix G. Visualizations", "text": "We select sample images from PASCAL-Context and ADE20K, and visualize the semantic segmentation results in Figures 7 and 8. Here we use the DC architecture to infer LabelBank from image visual appearance. As a comparison with existing methods, we have also provided the FCN [34] results on PASCAL-Context and the DilatedNet [42] results on ADE20K. The qualitative comparison verifies the utility of LabelBank for semantic segmentation \u2013 it helps to filter out false-positive pixel predictions via the holistic filtering process."}], "references": [{"title": "Cost-sensitive top-down/bottom-up inference for multiscale activity recognition", "author": ["M.R. Amer", "D. Xie", "M. Zhao", "S. Todorovic", "S.-C. Zhu"], "venue": "ECCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Higher order conditional random fields in deep neural networks", "author": ["A. Arnab", "S. Jayasumana", "S. Zheng", "P.H.S. Torr"], "venue": "ECCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "COCO-Stuff: Thing and stuff classes in context", "author": ["H. Caesar", "J. Uijlings", "V. Ferrari"], "venue": "arXiv preprint arXiv:1612.03716,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A. Yuille"], "venue": "arXiv Preprint, abs/1606.00915,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "ICCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional feature masking for joint object and stuff segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale object classification using label relation graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "ECCV,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE T-PAMI, 35(8):1915\u20131929,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Contextual action recognition with R*CNN", "author": ["G. Gkioxari", "R. Girshick", "J. Malik"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual organization and recognition of indoor scenes from RGB-D images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning rich features from RGB-D images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbel\u00e1ez", "J. Malik"], "venue": "ECCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiscale conditional random fields for image labeling", "author": ["X. He", "R.S. Zemel", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structured inference neural networks with label relations", "author": ["H. Hu", "G.-T. Zhou", "Z. Deng", "Z. Liao", "G. Mori"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Structural- RNN: Deep learning on spatio-temporal graphs", "author": ["A. Jain", "A.R. Zamir", "S. Savarese", "A. Saxena"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient inference in fully connected CRFs with Gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "OBJ CUT", "author": ["M.P. Kumar", "P.H.S. Torr", "A. Zisserman"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Object Bank: A high-level image representation for scene classification and semantic feature sparsification", "author": ["L.-J. Li", "H. Su", "E.P. Xing", "L. Fei-Fei"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I. Reid", "A. van den Hengel"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "L.C. Zitnick"], "venue": "ECCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "IEEE T-PAMI, 33(12):2368\u20132382,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "SIFT flow: Dense correspondence across scenes and its applications", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "IEEE T-PAMI, 33(5):978\u2013994,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "ParseNet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "ICLR Workshop,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P.H.O. Pinheiro", "R. Collobert"], "venue": "ICML,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["E. Shelhamer", "J. Long", "T. Darrell"], "venue": "IEEE T-PAMI,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Action bank: A high-level representation of activity in video", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding things: Image parsing with regions and per-exemplar detectors", "author": ["J. Tighe", "S. Lazebnik"], "venue": "CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous object detection and segmentation by boosting local shape feature based classifier", "author": ["B. Wu", "R. Nevatia"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "ICCV,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic understanding of scenes through the ADE20K dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "arXiv Preprint, abs/1608.05442,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "the seminal ObjectBank work [24]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "State-of-the-art methods for semantic segmentation leverage the successes of Convolutional Neural Networks (CNNs) [23].", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "CNNs have transformed the field of image classification, especially since the development of AlexNet [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 35, "context": "There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc.", "startOffset": 105, "endOffset": 109}, {"referenceID": 36, "context": "There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "There have been many follow-up CNN architectures to further boost image classification, including VGGNet [36], Google Inception [37], ResNet [14], etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "A representative work is the Fully Convolutional Network (FCN) [34] that leverages skip features of CNNs to produce a detailed pixel labeling.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "Another example is the DeepLab [4] framework, which augments FCN with dilated convolution [40], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and obtains state-of-art semantic segmentation performance.", "startOffset": 31, "endOffset": 34}, {"referenceID": 39, "context": "Another example is the DeepLab [4] framework, which augments FCN with dilated convolution [40], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and obtains state-of-art semantic segmentation performance.", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4].", "startOffset": 100, "endOffset": 104}, {"referenceID": 39, "context": "The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4].", "startOffset": 120, "endOffset": 127}, {"referenceID": 3, "context": "The semantic segmentation process can be implemented using state-of-the-art approaches, such as FCN [34] and DilatedNet [40, 4].", "startOffset": 120, "endOffset": 127}, {"referenceID": 30, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 41, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 135, "endOffset": 138}, {"referenceID": 34, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 147, "endOffset": 151}, {"referenceID": 26, "context": "\u2022 Finally, we evaluate our proposed framework on standard semantic segmentation datasets: PASCAL-Context [31], ADE20K [42], COCO-Stuff [3], NYUDv2 [35] and SIFT-Flow [27].", "startOffset": 166, "endOffset": 170}, {"referenceID": 33, "context": "A representative work is the Fully Convolutional Network (FCN) [34] that uses skip features of CNNs for detailed pixel labeling.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "[4] as a technique to retain a large field of view while keeping fewer trainable weights in semantic segmentation networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 39, "context": "The same method termed as dilated convolution was also pursued by Yu and Koltun [40] by a cascading series of dilated convolution layers.", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "A fully connected CRF is proposed by Kr\u00e4henb\u00fchl and Koltun [19] as an efficient dense pixel modeling method.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "[4] and Zheng et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[41] on top of FCNs as a further refinement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] modeled hierarchical and exclusive relations among semantic categories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] developed recurrent neural network structures for spatio-temporal inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed a neural graph inference model to propagate information among multiple levels of visual classes, in-", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "replicate (k) [0, 1] LabelBank: c\u03c3", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "[0, 1] expanded LabelBank: C\u03c3", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "(k, h, w) [0, 1] LabelBank Inference", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "(k, h, w) [0, 1] element-wise mul&plica&on", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "[1] adopted the and-or graph structure to reason about human activities at multiple levels of granularity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] exploited contextual cues to improve action recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed to use multi-scale CRFs to capture features at various image resolutions for semantic segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Approaches for modeling object instances and their segmentations have also been developed [21, 39].", "startOffset": 90, "endOffset": 98}, {"referenceID": 38, "context": "Approaches for modeling object instances and their segmentations have also been developed [21, 39].", "startOffset": 90, "endOffset": 98}, {"referenceID": 5, "context": "Another example of this line improves instance-aware semantic segmentation with object detection and classification [6, 7].", "startOffset": 116, "endOffset": 122}, {"referenceID": 6, "context": "Another example of this line improves instance-aware semantic segmentation with object detection and classification [6, 7].", "startOffset": 116, "endOffset": 122}, {"referenceID": 33, "context": "Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4].", "startOffset": 150, "endOffset": 154}, {"referenceID": 39, "context": "Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4].", "startOffset": 169, "endOffset": 176}, {"referenceID": 3, "context": "Furthermore, our semantic segmentation process is also generic, and can be implemented with state-of-the-art CNN-based network architectures like FCN [34] or DilatedNet [40, 4].", "startOffset": 169, "endOffset": 176}, {"referenceID": 33, "context": ", FCN [34] and DilatedNet [40, 4]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 39, "context": ", FCN [34] and DilatedNet [40, 4]).", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": ", FCN [34] and DilatedNet [40, 4]).", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": "The up-sampling is simply done by bi-linear interpolation (following [4]) to increase the resolution to the original image size.", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "The SPP architecture is motivated by the success of spatial pyramid pooling for image recognition [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "Specifically, depending on the type of the meta data, we could apply one-hot encoding for attributes (OHE architecture), or word2vec-style representation [30, 32] for sentence descriptions (W2V architecture).", "startOffset": 154, "endOffset": 162}, {"referenceID": 31, "context": "Specifically, depending on the type of the meta data, we could apply one-hot encoding for attributes (OHE architecture), or word2vec-style representation [30, 32] for sentence descriptions (W2V architecture).", "startOffset": 154, "endOffset": 162}, {"referenceID": 33, "context": "Following the standard setting of semantic segmentation [34, 4], we first apply a softmax activation layer to the full segmentation map, and then compute a categorical cross-entropy loss for the segmentation task.", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "Following the standard setting of semantic segmentation [34, 4], we first apply a softmax activation layer to the full segmentation map, and then compute a categorical cross-entropy loss for the segmentation task.", "startOffset": 56, "endOffset": 63}, {"referenceID": 30, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 69, "endOffset": 73}, {"referenceID": 41, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 98, "endOffset": 101}, {"referenceID": 34, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "We evaluate our framework on five benchmark datasets: PASCAL-Context [31], ADE20K [42], COCOStuff [3], NYUDv2 [35], and SIFT-Flow [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 15, "endOffset": 19}, {"referenceID": 41, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 91, "endOffset": 94}, {"referenceID": 34, "context": "Pascal-Context [31] 59 + bg 4,998/5,105/ADE20K [42] 150 + bg 20,210/2,000/5,000 COCO-Stuff [3] 171 + bg 9,000/1,000/NYUDv2 [35] 40 795/654/-", "startOffset": 123, "endOffset": 127}, {"referenceID": 26, "context": "SIFT-Flow [27] 33 2488/200/Table 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN and DilatedNet.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN and DilatedNet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "As mentioned above, we modified FCN [34] and DilatedNet [40, 4] to be our segmentation networks, and refer them to FCN and DilatedNet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 33, "context": "Following [34], we evaluate four common performance metrics for semantic segmentation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "CFM [6] - - 34.", "startOffset": 4, "endOffset": 7}, {"referenceID": 33, "context": "4 FCN-8s [34] 67.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "03 CRF-RNN [41] - - 39.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "3 DeepLab [4] - - 39.", "startOffset": 10, "endOffset": 13}, {"referenceID": 28, "context": "6 ParseNet [29] - - 40.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "4 BoxSup [5] - - 40.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "5 HO-CRF [2] - - 41.", "startOffset": 9, "endOffset": 12}, {"referenceID": 24, "context": "3 Context [25] 71.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "3 DeepLab + COCO [4] - - 44.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "7 DeepLab + COCO + CRF [4] - - 45.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "It is worth noting that DC+ FCN outperforms the current state-of-the-art method, DeepLab + COCO + CRF [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 41, "context": "We follow the settings of [42] in our experiments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "SegNet [42] 71.", "startOffset": 7, "endOffset": 11}, {"referenceID": 41, "context": "84 SegNet Cascade [42] 71.", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "05 FCN-8s [42] 71.", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "33 DilatedNet [42] 73.", "startOffset": 14, "endOffset": 18}, {"referenceID": 41, "context": "14 DilatedNet Cascade [42] 74.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "FCN [3] 52.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "7 DeepLab [3] 57.", "startOffset": 10, "endOffset": 13}, {"referenceID": 41, "context": "This experiment enables us to directly compare with those reported in [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "We follow the standard setting of [3] in our experiments.", "startOffset": 34, "endOffset": 37}, {"referenceID": 31, "context": "In detail, we first perform GloVe embedding [32] on individual words, and then obtain a feature vector of the textual description by averaging the embeddings of all words.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": ", skip-thought vector [18] or paragraph vector [22]) are applicable, and we leave these for future exploration.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": ", skip-thought vector [18] or paragraph vector [22]) are applicable, and we leave these for future exploration.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixellevel segmentation. We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.", "creator": "LaTeX with hyperref package"}}}