{"id": "1703.10344", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "wikipedia entity pages are a valuable source of information for direct consumption and for knowledge - base - construction, update and maintenance. facts in these entity pages are just typically supported by references. recent studies show that as much as + 20 \\ % of the global references are from online news sources. however, many entity pages are incomplete even if relevant information is already available consistently in existing scientific news articles. even for the already present references, there is often a delay between the news article publication time and the reference time. in this work, we therefore look at wikipedia through the lens of news and propose a novel news - article suggestion task to improve news coverage in expanding wikipedia, and generally reduce on the lag of newsworthy references. occasionally our work finds direct application, as a precursor, to wikipedia page generation and knowledge - base acceleration tasks that rely on relevant and high quality input sources.", "histories": [["v1", "Thu, 30 Mar 2017 07:56:42 GMT  (765kb,D)", "http://arxiv.org/abs/1703.10344v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.SI", "authors": ["besnik fetahu", "katja markert", "avishek anand"], "accepted": false, "id": "1703.10344"}, "pdf": {"name": "1703.10344.pdf", "metadata": {"source": "CRF", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "authors": ["Besnik Fetahu", "Katja Markert", "Avishek Anand"], "emails": ["fetahu@L3S.de", "markert@L3S.de", "anand@L3S.de", "Permissions@acm.org."], "sections": [{"heading": null, "text": "We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the salience and relative authority of entities, and the novelty of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93% in the article-entity suggestion stage and upto 84% for the article-section placement. Finally, we compare our approach against competitive baselines and show significant improvements.\nCategories and Subject Descriptors H3.3 [Information Systems]: Information Storage and Retrieval\u2014Information Search and Retrieval"}, {"heading": "1. INTRODUCTION", "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. c\u00a9 2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00. DOI: http://dx.doi.org/10.1145/2806416.2806531.\nWikipedia is the largest source of open and collaboratively curated knowledge in the world. Introduced in 2001, it has evolved into a reference work with around 5m pages for the English Wikipedia alone. In addition, entities and event pages are updated quickly via collaborative editing and all edits are encouraged to include source citations, creating a knowledge base which aims at being both timely as well as authoritative. As a result, it has become the preferred source of information consumption about entities and events1. Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].\nHowever, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone,\n1Wikipedia is one of the Top 10 viewed page sites and the top reference site according to Alexa Internet ranking www. alexa.com.\nar X\niv :1\n70 3.\n10 34\n4v 1\n[ cs\n.I R\n] 3\n0 M\nar 2\n01 7\nrespectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure 1) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages [10].\nTo remedy these problems, it is important to identify information sources that contain novel and salient facts to a given entity page. However, not all information sources are equal. The online presence of major news outlets is an authoritative source due to active editorial control and their articles are also a timely container of facts. In addition, their use is in line with current Wikipedia editing practice, as is shown in [10] that almost 20% of current citations in all entity pages are news articles. We therefore propose news suggestion as a novel task that enhances entity pages and reduces delay while keeping its pages authoritative.\nExisting efforts to populate Wikipedia [18] start from an entity page and then generate candidate documents about this entity using an external search engine (and then postprocess them). However, such an approach lacks in (a) reproducibility since rankings vary with time with obvious bias to recent news (b) maintainability since document acquisition for each entity has to be periodically performed. To this effect, our news suggestion considers a news article as input, and determines if it is valuable for Wikipedia. Specifically, given an input news article n and a state of Wikipedia, the news suggestion problem identifies the entities mentioned in n whose entity pages can improve upon suggesting n. Most of the works on knowledge base acceleration [2, 1, 8], or Wikipedia page generation [18] rely on high quality input sources which are then utilized to extract textual facts for Wikipedia page population. In this work, we do not suggest snippets or paraphrases but rather entire articles which have a high potential importance for entity pages. These suggested news articles could be consequently used for extraction, summarization or population either manually or automatically \u2013 all of which rely on high quality and relevant input sources.\nWe identify four properties of good news recommendations: salience, relative authority, novelty and placement. First, we need to identify the most salient entities in a news article. This is done to avoid pollution of entity pages with only marginally related news. Second, we need to determine whether the news is important to the entity as only the most relevant news should be added to a precise reference work. To do this, we compute the relative authority of all entities in the news article: we call an entity more authoritative than another if it is more popular or noteworthy in the real world. Entities with very high authority have many news items associated with them and only the most relevant of these should be included in Wikipedia whereas for entities of lower authority the threshold for inclusion of a news article will be lower. Third, a good recommendation should be able to identify novel news by minimizing redundancy coming from multiple news articles. Finally, addition of facts is facilitated if the recommendations are fine-grained, i.e.,\nrecommendations are made on the section level rather than the page level (placement).\nApproach and Contributions. We propose a two-stage news suggestion approach to entity pages. In the first stage, we determine whether a news article should be suggested for an entity, based on the entity\u2019s salience in the news article, its relative authority and the novelty of the article to the entity page. The second stage takes into account the class of the entity for which the news is suggested and constructs section templates from entities of the same class. The generation of such templates has the advantage of suggesting and expanding entity pages that do not have a complete section structure in Wikipedia, explicitly addressing long-tail and trunk entities. Afterwards, based on the constructed template our method determines the best fit for the news article with one of the sections.\nWe evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages. Given the Wikipedia snapshot at a given year (in our case [2009-2014]), we suggest news articles that might be cited in the coming years. The existing news references in the entity pages along with their reference date act as our ground-truth to evaluate our approach. In summary, we make the following contributions.\n\u2022 we propose a two-stage news suggestion approach for Wikipedia entity pages.\n\u2022 we adopt and address the problem of determining whether a news article should be referenced to an entity considering the entity salience, relative authority and novelty of the article for the entity page.\n\u2022 we are able to place articles in a specific section of the entity page. Through section templates, we address the problems of entities with a limited section structure by class-based generalization i.e. we can expand entity pages with sections that come from entities of a similar class.\n\u2022 an extensive evaluation on 351,982 news articles and 73,734 entity pages, using their state for the years [2009-2013]."}, {"heading": "2. RELATED WORK", "text": "As we suggest a new problem there is no current work addressing exactly the same task. However, our task has similarities to Wikipedia page generation and knowledge base acceleration. In addition, we take inspiration from Natural Language Processing (NLP) methods for salience detection.\nWikipedia Page Generation is the problem of populating Wikipedia pages with content coming from external sources. Sauper and Barzilay [18] propose an approach for automatically generating whole entity pages for specific entity classes. The approach is trained on already-populated entity pages of a given class (e.g. \u2018Diseases\u2019) by learning templates about the entity page structure (e.g. diseases have a treatment section). For a new entity page, first, they extract documents via Web search using the entity title and the section title as a query, for example \u2018Lung Cancer \u2019+\u2018Treatment \u2019. As already discussed in the introduction, this has problems with reproducibility and maintainability. However, their main focus is on identifying the best paragraphs extracted from the collected documents. They rank the paragraphs via an optimized supervised perceptron model for finding the most representative paragraph that is the least similar to paragraphs in other sections. This paragraph is then included in the newly generated entity page. Taneva and Weikum [21] propose an approach that constructs short summaries for the long tail. The summaries are called \u2018gems\u2019 and the size of a \u2018gem\u2019 can be user defined. They focus on generating summaries that are novel and diverse. However, they do not consider any structure of entities, which is present in Wikipedia.\nIn contrast to [18] and [21], we actually focus on suggesting entire documents to Wikipedia entity pages. These are authoritative documents (news), which are highly relevant for the entity, novel for the entity and in which the entity is salient. Whereas relevance in Sauper and Barzilay is implicitly computed by web page ranking we solve that problem by looking at relative authority and salience of an entity, using the news article and entity page only. As Sauper and Barzilay concentrate on empty entity pages, the problem of novelty of their content is not an issue in their work whereas it is in our case which focuses more on updating entities. Updating entities will be more and more important the bigger an existing reference work is. Both the approaches in [18] and [21] (finding paragraphs and summarization) could then be used to process the documents we suggest further. Our concentration on news is also novel.\nKnowledge Base Acceleration. In this task, given specific information extraction templates, a given corpus is analyzed in order to find worthwhile mentions of an entity or snippets that match the templates. Balog [2, 1] recommend news citations for an entity. Prior to that, the news articles are classified for their appropriateness for an entity, where as features for the classification task they use entity, document, entity-document and temporal features. The best performing features are those that measure similarity between an entity and the news document. West et al. [25] consider the problem of knowledge base completion, through question answering and complete missing facts in Freebase based on templates, i.e. Frank Zappa bornIn Baltymore, Maryland.\nIn contrast, we do not extract facts for pre-defined templates but rather suggest news articles based on their relevance to an entity. In cases of long-tail entities, we can suggest to add a novel section through our abstraction and generation of section templates at entity class level.\nEntity Salience. Determining which entities are prominent or salient in a given text has a long history in NLP, sparked by the linguistic theory of Centering [23]. Salience has been used in pronoun and co-reference resolution [15], or to predict which entities will be included in an abstract\nof an article [8]. Frequent features to measure salience include the frequency of an entity in a document, positioning of an entity, grammatical function or internal entity structure (POS tags, head nouns etc.). These approaches are not currently aimed at knowledge base generation or Wikipedia coverage extension but we postulate that an entity\u2019s salience in a news article is a prerequisite to the news article being relevant enough to be included in an entity page. We therefore use the salience features in [8] as part of our model. However, these features are document-internal \u2014 we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority, news authority and novelty that measure the relations between several entities, between entity and news article as well as between several competing news articles."}, {"heading": "3. PROBLEM DEFINITION AND APPROACH OUTLINE", "text": ""}, {"heading": "3.1 Terminology and Problem Definition", "text": "We are interested in named entities mentioned in documents. An entity e can be identified by a canonical name, and can be mentioned differently in text via different surface forms. We canonicalize these mentions to entity pages in Wikipedia, a method typically known as entity linking. We denote the set of canonicalized entities extracted and linked from a news article n as \u03d5(n). For example, in Figure 2, entities are canonicalized into Wikipedia entity pages (e.g. Odisha is canonicalized to the corresponding article2). For a collection of news articles N, we further denote the resulting set of entities by E = \u222an\u2208N{ei}.\nInformation in an entity page is organized into sections and evolves with time as more content is added. We refer to the state of Wikipedia at a time t as Wt and the set of sections for an entity page e as its entity profile Se(t). Unlike news articles, text in Wikipedia could be explicitly linked to entity pages through anchors. The set of entities explicitly referred in text from section s \u2208 Se(t) is defined as \u03b3(s). Furthermore, Wikipedia induces a category structure over its entities, which is exploited by knowledge bases like YAGO (e.g. Barack Obama isA Person). Consequently, each entity page belongs to one or more entity categories or classes c. Now we can define our news suggestion problem below:\nDefinition 1 (News Suggestion Problem). Given a set of news articles N = {n1, . . . , nk} and set of Wikipedia entity pages E = {e1, . . . , em} (from Wt) we intend to suggest a news article n published at time ti > t to entity page e and additionally to the most relevant section for the entity page s \u2208 Se(t)."}, {"heading": "3.2 Approach Overview", "text": "We approach the news suggestion problem by decomposing it into two tasks:\n1. AEP : Article\u2013Entity placement\n2. ASP : Article\u2013Section placement\nIn this first step, for a given entity-news pair \u3008n, e\u3009, we determine whether the given news article n \u2208 N should be 2http://en.wikipedia.org/wiki/Odisha\nsuggested (we will refer to this as \u2018relevant\u2019 ) to entity e \u2208 E. To generate such \u3008n, e\u3009 pairs, we perform the entity linking process, \u03d5(n), for n.\nThe article\u2013entity placement task (described in detail in Section 4.1) for a pair \u3008n, e\u3009 outputs a binary label (either \u2018non-relevant\u2019 or \u2018relevant\u2019 ) and is formalized in Equation 1.\nAEP : \u3008e, n\u3009 \u2192 {0, 1}, \u2200e \u2208 \u03d5(n) \u2227 n \u2208 N (1)\nIn the second step, we take into account all \u2018relevant\u2019 pairs \u3008n, e\u3009 and find the correct section for article n in entity e, respectively its profile Se(t) (see Section 4.2). The article\u2013 section placement task, determines the correct section for the triple \u3008n, e, Se(t)\u3009, and is formalized in Equation 2.\nASP : \u3008e, n, Se(t)\u3009 \u2192 {s1, . . . , sk}, s \u2208 Se(t) (2)\nIn the subsequent sections we describe in details how we approach the two tasks for suggesting news articles to entity pages."}, {"heading": "4. NEWS ARTICLE SUGGESTION", "text": "In this section, we provide an overview of the news suggestion approach to Wikipedia entity pages (see Figure 2). The approach is split into two tasks: (i) article-entity (AEP) and (ii) article-section (ASP) placement. For a Wikipedia snapshotWt and a news corpus N, we first determine which news articles should be suggested to an entity e. We will denote our approach for AEP by Fe. Finally, we determine the most appropriate section for the ASP task and we denote our approach with Fs.\nIn the following, we describe the process of learning the functions Fe and Fs. We introduce features for the learning process, which encode information regarding the entity salience, relative authority and novelty in the case of AEP task. For the ASP task, we measure the overall fit of an article to the entity sections, with the entity being an input from AEP task. Additionally, considering that the entity profiles Se(t) are incomplete, in the case of a missing section we suggest and expand the entity profiles based on section templates generated from entities of the same class c (see Section 4.2.1)."}, {"heading": "4.1 Article\u2013Entity Placement", "text": "In this step we learn the function Fe to correctly determine whether n should be suggested for e, basically a binary classification model (0=\u2018non-relevant\u2019 and 1=\u2018relevant\u2019 ). Note that we are mainly interested in finding the relevant pairs in this task. For every news article, the number of disambiguated entities is around 30 (but n is suggested for only two of them on average). Therefore, the distribution of \u2018non-relevant\u2019 and \u2018relevant\u2019 pairs is skewed towards the earlier, and by simply choosing the \u2018non-relevant\u2019 label we can achieve a high accuracy for Fe. Finding the relevant pairs is therefore a considerable challenge.\nAn article n is suggested to e by our function Fe if it fulfills the following properties. The entity e is salient in n (a central concept), therefore ensuring that n is about e and that e is important for n. Next, given the fact there might be many articles in which e is salient, we also look at the reverse property, namely whether n is important for e. We do this by comparing the authority of e (which is a measure of popularity of an entity, such as its frequency of mention in a whole corpus) with the authority of its cooccurring entities in \u03d5(n), leading to a feature we call relative\nauthority. The intuition is that for an entity that has overall lower authority than its co-occurring entities, a news article is more easily of importance.3 Finally, if the article we are about to suggest is already covered in the entity profile Se(t), we do not wish to suggest redundant information, hence the novelty. Therefore, the learning objective of Fe should fulfill the following properties. Table 1 shows a summary of the computed features for Fe.\n4.1.1 Salience-based features Baseline Features. As discussed in Section 2, a variety\nof features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick [8]. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in [8] gives details.\nRelative Entity Frequency. Although frequency of mention and positional features play some role in baseline features, their interaction is not modeled by a single feature nor do the positional features encode more than sentence position. We therefore suggest a novel feature called relative entity frequency, \u03a6(e, n), that has three properties.: (i) It rewards entities for occurring throughout the text instead of only in some parts of the text, measured by the number of paragraphs it occurs in (ii) it rewards entities that occur more frequently in the opening paragraphs of an article as we model \u03a6(e, n) as an exponential decay function. The decay corresponds to the positional index of the news paragraph. This is inspired by the news-specific discourse structure that tends to give short summaries of the most important facts and entities in the opening paragraphs. (iii) it compares entity frequency to the frequency of its co-occurring mentions as the weight of an entity appearing in a specific paragraph, normalized by the sum of the frequencies of other entities in\n3This is why people occurring infrequently in the news keep any press cutting mentioning them.\n\u03d5(n).\n\u03a6(e, n) = |p(e, n)| |p(n)| \u2211 p\u2208p(n)  tf(e, p)\u2211 e\u2032 6=e tf(e\u2032, p)  1 p\n(3)\nwhere, p represents a news paragraph from n, and with p(n) we indicate the set of all paragraphs in n. The frequency of e in a paragraph p is denoted by tf(e, p). With |p(e, n)| and |p(n)| we indicate the number of paragraphs in which entity e occurs, and the total number of paragraphs, respectively.\n4.1.2 Authority-based features Relative Authority. In this case, we consider the com-\nparative relevance of the news article to the different entities occurring in it. As an example, let us consider the meeting of the Sudanese bishop Elias Taban4 with Hillary Clinton5. Both entities are salient for the meeting. However, in Taban\u2019s Wikipedia page, this meeting is discussed prominently with a corresponding news reference6, whereas in Hillary Clinton\u2019s Wikipedia page it is not reported at all. We believe this is not just an omission in Clinton\u2019s page but mirrors the fact that for the lesser known Taban the meeting is big news whereas for the more famous Clinton these kind of meetings are a regular occurrence, not all of which can be reported in what is supposed to be a selection of the most important events for her. Therefore, if two entities co-occur, the news is more relevant for the entity with the lower a priori authority.\nThe a priori authority of an entity (denoted by \u0393(e)) can be measured in several ways. We opt for two approaches: (i) probability of entity e occurring in the corpus N, and (ii) authority assessed through centrality measures like PageRank [16]. For the second case we construct the graph G = (V,E) consisting of entities in E and news articles in N as vertices. The edges are established between n and entities in \u03d5(n), that is \u3008n \u2192 \u03d5(n)\u3009, and the out-links from e, that is \u3008e\u2192 \u03b3(s(t\u2212 1))\u3009 (arrows present the edge direction).\nStarting from a priori authority, we proceed to relative authority by comparing the a priori authority of co-occurring entities in \u03d5(n). We define the relative authority of e as the proportion of co-occurring entities e\u2032 \u2208 \u03d5(n) that have a higher a priori authority than e (see Equation 4.\n\u0393\u0302(e|\u03d5(n)) = 1 |\u03d5(n)| \u2211\ne\u2032\u2208\u03d5(n) 1\u0393(e\u2032)>\u0393(e) (4)\nAs we might run the danger of not suggesting any news articles for entities with very high a priori authority (such as Clinton) due to the strict inequality constraint, we can relax the constraint such that the authority of co-occurring entities is above a certain threshold.\nNews Domain Authority. The news domain authority addresses two main aspects. Firstly, if bundled together with the relative authority feature, we can ensure that dependent on the entity authority, we suggest news from authoritative sources, hence ensuring the quality of suggested articles. The second aspect is in a news streaming scenario where multiple news domains report the same event \u2014 ideally only articles coming from authoritative sources would fulfill the conditions for the news suggestion task. 4http://en.wikipedia.org/wiki/Elias_Taban 5http://en.wikipedia.org/wiki/Hillary_Clinton 6http://tinyurl.com/mshf7j2\nThe news domain authority is computed based on the number of news references in Wikipedia coming from a particular news domain D. This represents a simple prior that a news article n is from domain D in corpus N. We extract the domains by taking the base URLs from the news article URLs.\n4.1.3 Novelty-based features An important feature when suggesting an article n to an\nentity e is the novelty of n w.r.t the already existing entity profile Se(t\u22121). Studies [3] have shown that on comparable collections to ours (TREC GOV2) the number of duplicates can go up to 17%. This figure is likely higher for major events concerning highly authoritative entities on which all news media will report.\nGiven an entity e and the already added news references Nt\u22121 = {n1, . . . , nk} up to year t \u2212 1, the novelty of nk+1 at year t is measured by the KL divergence between the language model of nk+1 and articles in Nt\u22121. We combine this measure with the entity overlap of nk+1 and n\n\u2032 \u2208 Nt\u22121. The novelty value of nk+1 is given by the minimal divergence value. Low scores indicate low novelty for the entity profile Se(t).\nN (n|e) = min n\u2032\u2208Nt\u22121\n{ \u03bb \u00b7DKL ( \u03b8(n\u2032)||\u03b8(n) ) +\n(1\u2212 \u03bb) \u00b7 jaccard ( \u03d5(n\u2032), \u03d5(n) )} (5)\nwhere DKL is the KL divergence of the language models (\u03b8(n) and \u03b8(n\u2032)), whereas \u03bb is the mixing weight (\u03bb = {0, . . . , 1}) between the language models DKL and the entity overlap in n and n\u2032."}, {"heading": "4.2 Article\u2013Section Placement", "text": "We model the ASP placement task as a successor of the AEP task. For all the \u2018relevant\u2019 news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections \u2018Early Life\u2019, \u2018Presidency\u2019, \u2018Family and Personal Life\u2019 etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an \u2018Accidents\u2019 section before this year\u2019s disaster, which was the first in the history of the airline.\nEven if sections are missing for certain entities, similar sections usually occur in other entities of the same class (e.g. other airlines had disasters and therefore their pages have an accidents section). We exploit such homogeneity of section structure and construct templates that we use to expand entity profiles. The learning objective for Fs takes into account the following properties:\n1. Section-templates: account for incomplete section structure for an entity profile Se(t) by constructing\nsection templates S\u0302c from an entity class c\n2. Overall fit: measures the overall fit of a news article to sections in the section templates S\u0302c\n4.2.1 Section-Template Generation Given the fact that entity profiles are often incomplete,\nwe construct section templates for every entity class. We group entities based on their class c and construct section\ntemplates S\u0302c. For different entity classes, e.g. Person and Location, the section structure and the information represented in those section varies heavily. Therefore, the section templates are with respect to the individual classes in our experimental setup (see Figure 3).\nS\u0302c = {s1, . . . , sk}, \u2200Se(t) \u2208 E \u2227 e typeOf c (6)\nGenerating section templates has two main advantages. Firstly, by considering class-based profiles, we can overcome the problem of incomplete individual entity profiles and thereby are able to suggest news articles to sections that do not yet exist in a specific entity Se(t). The second advantage is that we are able to canonicalize the sections, i.e. \u2018Early Life\u2019 and \u2018Early Life and Childhood\u2019 would be treated similarly.\nTo generate the section template S\u0302c, we extract all sections from entities of a given type c at year t. Next, we cluster the entity sections, based on an extended version of k\u2013means clustering [12], namely x\u2013means clustering introduced in Pelleg et al. which estimates the number of clusters efficiently [17]. As a similarity metric we use the cosine similarity computed based on the tf\u2013idf models of the sections. Using the x\u2013means algorithm we overcome the requirement to provide the number of clusters k beforehand. x\u2013means extends the k\u2013means algorithm, such that a user only specifies a range [Kmin, Kmax] that the number of clusters may reasonably lie in.\n4.2.2 News-section fit The learning objective of Fs is to determine the overall fit\nof a news article n to one of the sections in a given section\ntemplate S\u0302c. The template is pre-determined by the class of the entity for which the news is suggested as relevant by Fe. In all cases, we measure how well n fits each of the sections s \u2208 S\u0302c(t \u2212 1) as well as the specific entity section s\u2032 \u2208 Se(t\u22121). The section profiles in S\u0302c(t\u22121) represent the aggregated entity profiles from all entities of class c at year t\u2212 1.\nTo learn Fs we rely on a variety of features that consider several similarity aspects as shown in Table 2. For the sake of simplicity we do not make the distinction in Table 2 between the individual entity section and class-based section similarities, se(t\u22121) and s(t\u22121), respectively. Bear in mind\nthat an entity section se might be present at year t but not at year t \u2212 1 (see for more details the discussion on entity profile expansion in Section 6.2.4).\nTopic. We use topic similarities to ensure (i) that the content of n fits topic-wise with a specific section text and (ii) that it has a similar topic to previously referred news articles in that section. In a pre-processing stage we compute the topic models for the news articles, entity sections Se(t\u22121) and the aggregated class-based sections in S\u0302c. The topic models are computed using LDA [5]. We only computed a single topic per article/section as we are only interested in topic term overlaps between article and sections. We distinguish two main features: the first feature measures the overlap of topic terms between n and the entity section se(t\u2212 1) and s(t\u2212 1) \u2208 S\u0302c, and the second feature measures the overlap of the topic model of n against referred news articles in Nt\u22121 at time t\u2212 1.\nSyntactic. These features represent a mechanism for conveying the importance of a specific text snippet, solely based on the frequency of specific POS tags (i.e. NNP, CD etc.), as commonly used in text summarization tasks. Following the same intuition as in [18], we weigh the importance of articles by the count of specific POS tags. We expect that for different sections, the importance of POS tags will vary. We measure the similarity of POS tags in a news article against the section text. Additionally, we consider bi-gram and tri-gram POS tag overlap. This exploits similarity in syntactical patterns between the news and section text.\nLexical. As lexical features, we measure the similarity of n against the entity section text se(t\u2212 1) and the aggregate section text s(t \u2212 1). Further, we distinguish between the overall similarity of n and that of the different news paragraphs (p(n) which denotes the paragraphs of n up to the 5th paragraph). A higher similarity on the first paragraphs represents a more confident indicator that n should be suggested to a specific section s. We measure the similarity based on two metrics: (i) the KL-divergence between the computed language models and (ii) cosine similarity of the corresponding paragraph text p(n) and section text.\nEntity-based. Another feature set we consider is the overlap of named entities and their corresponding entity classes. For different entity sections, we expect to find a particular set of entity classes that will correlate with the section, e.g. \u2018Early Life\u2019 contains mostly entities related to family, school, universities etc.\nFrequency. Finally, we gather statistics about the number of entities, paragraphs, news article length, top\u2013k enti-\nties and entity classes, and the frequency of different POS tags. Here we try to capture patterns of articles that are usually cited in specific sections."}, {"heading": "5. DATASETS AND PRE-PROCESSING", "text": ""}, {"heading": "5.1 Evaluation Plan", "text": "In this section we outline the evaluation plan to verify the effectiveness of our learning approaches. To evaluate the news suggestion problem we are faced with two challenges.\n\u2022 What comprises the ground truth for such a task ?\n\u2022 How do we construct training and test splits given that entity pages consists of text added at different points in time ?\nConsider the ground truth challenge. Evaluating if an arbitrary news article should be included in Wikipedia is both subjective and difficult for a human if she is not an expert. An invasive approach, which was proposed by Barzilay and Sauper [18], adds content directly to Wikipedia and expects the editors or other users to redact irrelevant content over a period of time. The limitations of such an evaluation technique is that content added to long-tail entities might not be evaluated by informed users or editors in the experiment time frame. It is hard to estimate how much time the added content should be left on the entity page. A more non-invasive approach could involve crowdsourcing of entity and news article pairs in an IR style relevance assessment setup. The problem of such an approach is again finding knowledgeable users or experts for long-tail entities. Thus the notion of relevance of a news recommendation is challenging to evaluate in a crowd setup.\nWe take a slightly different approach by making an assumption that the news articles already present in Wikipedia entity pages are relevant. To this extent, we extract a dataset comprising of all news articles referenced in entity pages (details in Section 5.2). At the expense of not evaluating the space comprising of news articles absent in Wikipedia, we succeed in (i) avoiding restrictive assumptions about the quality of human judgments, (ii) being invasive and polluting Wikipedia, and (iii) deriving a reusable test bed for quicker experimentation.\nThe second challenge of construction of training and test set separation is slightly easier and is addressed in Section 5.4."}, {"heading": "5.2 Datasets", "text": "The datasets we use for our experimental evaluation are directly extracted from the Wikipedia entity pages and their revision history. The generated data represents one of the contributions of our paper.7 The datasets are the following:\nEntity Classes. We focus on a manually predetermined set of entity classes for which we expect to have news coverage. The number of analyzed entity classes is 27, including 73, 734 entities with at least one news reference. The entity classes were selected from the DBpedia class ontology. Figure 3 shows the number of entities per class for the years (2009-2014).\n7http://l3s.de/~fetahu/cikm2015/data/\nNews Articles. We extract all news references from the collected Wikipedia entity pages.8 The extracted news references are associated with the sections in which they appear. In total there were 411, 673 news references, and after crawling we end up with 351, 982 successfully crawled news articles. The details of the news article distribution, and the number of entities and sections from which they are referred are shown in Table 3.\nArticle-Entity Ground-truth. The dataset comprises of the news and entity pairs \u3008n, e\u3009 \u2192 {0, 1}. News-entity pairs are relevant if the news article is referenced in the entity page. Non-relevant pairs (i.e. negative training examples) consist of news articles that contain an entity but are not referenced in that entity\u2019s page. If a news article n is referred from e at year t, the features are computed taking into account the entity profiles at year Se(t\u2212 1).\nArticle-Section Ground-truth. The dataset consists of the triple \u3008n, e, s\u3009, where s \u2208 S\u0302c, where we assume that \u3008n, e\u3009 has already been determined as relevant. We therefore have a multi-class classification problem where we need to determine the section of e where n is cited. Similar to the article-entity ground truth, here too the features compute the similarity between n, Se(t\u2212 1) and S\u0302c(t\u2212 1)."}, {"heading": "5.3 Data Pre-Processing", "text": "We POS-tag the news articles and entity profiles Se(t) with the Stanford tagger [22]. For entity linking the news articles, we use TagMe![9] with a confidence score of 0.3. On a manual inspection of a random sample of 1000 disambiguated entities, the accuracy is above 0.9. On average, the number of entities per news article is approximately 30. For entity linking the entity profiles, we simply follow the anchor text that refers to Wikipedia entities."}, {"heading": "5.4 Train and Testing Evaluation Setup", "text": "8A news reference in Wikipedia is denoted by the template {cite type=\u2018news\u2019 | url=\u2018\u2019}\nWe evaluate the generated supervised models for the two tasks, AEP and ASP, by splitting the train and testing instances. It is important to note that for the pairs \u3008n, e\u3009 and the triple \u3008n, e, S\u0302c\u3009, the news article n is referenced at time t by entity e, while the features take into account the entity profile at time t \u2212 1. This avoids any \u2018overlapping\u2019 content between the news article and the entity page, which could affect the learning task of the functions Fe and Fs. Table 4 shows the statistics of train and test instances. We learn the functions at year t and test on instances for the years greater than t. Please note that we do not show the performance for year 2014 as we do not have data for 2015 for evaluation."}, {"heading": "6. RESULTS AND DISCUSSION", "text": ""}, {"heading": "6.1 Article\u2013Entity Placement", "text": "Here we introduce the evaluation setup and analyze the results for the article\u2013entity (AEP) placement task. We only report the evaluation metrics for the \u2018relevant\u2019 news-entity pairs. A detailed explanation on why we focus on the \u2018relevant\u2019 pairs is provided in Section 4.1.\n6.1.1 Evaluation Setup Baselines. We consider the following baselines for this\ntask.\n\u2022 B1. The first baseline uses only the salience-based features by Dunietz and Gillick [8].\n\u2022 B2. The second baseline assigns the value relevant to a pair \u3008n, e\u3009, if and only if e appears in the title of n.\nLearning Models. We use Random Forests (RF) [6].9 We learn the RF on all computed features in Table 1. The optimization on RF is done by splitting the feature space into multiple trees that are considered as ensemble classifiers. Consequently, for each classifier it computes the margin function as a measure of the average count of predicting the correct class in contrast to any other class. The higher the margin score the more robust the model.\nMetrics. We compute precision P, recall R and F1 score for the relevant class. For example, precision is the number of news-entity pairs we correctly labeled as relevant compared to our ground truth divided by the number of all news-entity pairs we labeled as relevant.\n6.1.2 Approach Effectiveness The following results measure the effectiveness of our ap-\nproach in three main aspects: (i) overall performance of Fe 9Our emphasis in this paper is not a comparison of learning models but of course other classifiers can be used for this task.\nand comparison to baselines, (ii) robustness across the years, and (iii) optimal model for the AEP placement task.\nPerformance. Figure 4 shows the results for the years 2009 and 2013, where we optimized the learning objective with instances from year t and evaluate on the years ti > t (see Section 5.4).10 The results show the precision\u2013recall curve. The red curve shows baseline B1 [8], and the blue one shows the performance of Fe. The curve shows for varying confidence scores (high to low) the precision on labeling the pair \u3008e, n\u3009 as \u2018relevant\u2019. In addition, at each confidence score we can compute the corresponding recall for the \u2018relevant\u2019 label. For high confidence scores on labeling the news-entity pairs, the baseline B1 achieves on average a precision score of P=0.50, while Fe has P=0.93. We note that with the drop in the confidence score the corresponding precision and recall values drop too, and the overall F1 score for B1 is around F1=0.2, in contrast we achieve an average score of F1=0.67.\nIt is evident from Figure 4 that for the years 2009 and 2013, Fe significantly outperforms the baseline B1. We measure the significance through the t-test statistic and get a p-value of 2.2e\u221216. The improvement we achieve over B1 in absolute numbers, \u2206P=+0.5 in terms of precision for the years between 2009 and 2014, and a similar improvement in terms of F1 score. The improvement for recall is \u2206 R=+0.4. The relative improvement over B1 for P and F1 is almost 1.8 times better, while for recall we are 3.5 times better. In Table 5 we show the overall scores for the evaluation metrics for B1 and Fe. Finally, for B2 we achieve much poorer performance, with average scores of P=0.21, R=0.20 and F1=0.21.\nRobustness. In Table 5, we show the overall performance for the years between 2009 and 2013. An interesting observation we make is that we have a very robust performance and the results are stable across the years. If we consider the experimental setup, where for year t = 2009 we optimize the learning objective with only 74k training instances and evaluate on the rest of the instances, it achieves a very good performance. We predict with F1=0.68 the remaining 469k instances for the years t \u2208 (2009, 2014].\nThe results are particularly promising considering the fact that the distribution between our two classes is highly skewed. On average the number of \u2018relevant\u2019 pairs account for only around 4\u22126% of all pairs. A good indicator to support such a statement is the kappa (denoted by \u03ba) statistic. \u03bameasures agreement between the algorithm and the gold standard on both labels while correcting for chance agreement (often expected due to extreme distributions). The \u03ba scores for B1 across the years is on average 0.19, while for Fe we achieve a score of 0.65 (the maximum score for \u03ba is 1).\n10We only show the first year 2009 and the last year 2013, since the difference to the other years is marginal.\n6.1.3 Feature Analysis In Figure 5 we show the impact of the individual feature\ngroups that contribute to the superior performance in comparison to the baselines. Relative entity frequency from the salience feature, models the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of Fe with relative entity frequency from the salience feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. However, if both feature groups are considered separately, they significantly outperform the baseline B1."}, {"heading": "6.2 Article-Section Placement", "text": "Here we show the evaluation setup for ASP task and discuss the results with a focus on three main aspects, (i) the overall performance across the years, (ii) the entity class specific performance, and (iii) the impact on entity profile expansion by suggesting missing sections to entities based on the pre-computed templates.\n6.2.1 Evaluation Setup Baselines. To the best of our knowledge, we are not\naware of any comparable approach for this task. Therefore, the baselines we consider are the following:\n\u2022 S1: Pick the section from template S\u0302c with the highest lexical similarity to n: S1= argmaxs\u2208S\u0302c(t\u22121)\u3008n, e, s\u3009\n\u2022 S2: Place the news into the most frequent section in S\u0302c\nLearning Models. We use Random Forests (RF) [6] and Support Vector Machines (SVM) [7]. The models are optimized taking into account the features in Table 2. In contrast to the AEP task, here the scale of the number of instances allows us to learn the SVM models. The SVM model is optimized using the \u2212 SV R loss function and uses the Gaussian kernels.\nMetrics. We compute precision P as the ratio of news\nfor which we pick a section s from S\u0302c and s conforms to the one in our ground-truth (see Section 5.2). The definition of recall R and F1 score follows from that of precision.\n6.2.2 Overall Article-Section Performance Figure 6 shows the overall performance and a comparison\nof our approach (when Fs is optimized using SVM) against the best performing baseline S2. With the increase in the number of training instances for the ASP task the performance is a monotonically non-decreasing function. For the year 2009, we optimize the learning objective of Fs with around 8% of the total instances, and evaluate on the rest. The performance on average is around P=0.66 across all classes. Even though for many classes the performance is already stable (as we will see in the next section), for some classes we improve further. If we take into account the years between 2010 and 2012, we have an increase of \u2206P=0.17, with around 70% of instances used for training and the remainder for evaluation. For the remaining years the total improvement is \u2206P=0.18 in contrast to the performance at year 2009.\nOn the other hand, the baseline S1 has an average precision of P=0.12. The performance across the years varies slightly, with the year 2011 having the highest average precision of P=0.13. Always picking the most frequent section as in S2, as shown in Figure 6, results in an average precision of P=0.17, with a uniform distribution across the years."}, {"heading": "6.2.3 Article-Section Performance per Entity Class", "text": "Here we show the performance of Fs decomposed for the\ndifferent entity classes. Specifically we analyze the 27 classes in Figure 3. In Table 6, we show the results for a range of years (we omit showing all years due to space constraints). For illustration purposes only, we group them into four main classes ({ Person, Organization, Location, Event}) and into the specific sub-classes shown in the second column in\nTable 6. For instance, the entity classes OfficeHolder and Politician are aggregated into Person\u2013Politics.\nIt is evident that in the first year the performance is lower in contrast to the later years. This is due to the fact that as we proceed, we can better generalize and accurately determine the correct fit of an article n into one of the sections\nfrom the pre-computed templates S\u0302c. The results are already stable for the year range (2009, 2012]. For a few Person sub-classes, e.g. Politics, Entertainment, we achieve an F1 score above 0.9. These additionally represent classes with a sufficient number of training instances for the years [2009, 2012]. The lowest F1 score is for the Criminal and Television classes. However, this is directly correlated with the insufficient number of instances.\nThe baseline approaches for the ASP task perform poorly. S1, based on lexical similarity, has a varying performance for different entity classes. The best performance is achieved for the class Person - Politics, with P=0.43. This highlights the importance of our feature choice and that the ASP cannot be considered as a linear function, where the maximum similarity yields the best results. For different entity classes different features and combination of features is necessary. Considering that S2 is the overall best performing baseline, through our approach Fs we have a significant improvement of over \u2206P=+0.64.\nThe models we learn are very robust and obtain high accuracy, fulfilling our pre-condition for accurate news suggestions into the entity sections. We measure the robustness of Fs through the \u03ba statistic. In this case, we have a model with roughly 10 labels (corresponding to the number of sec-\ntions in a template S\u0302c). The score we achieve shows that our model predicts with high confidence with \u03ba = 0.64.\n6.2.4 Entity Profile Expansion The last analysis is the impact we have on expanding en-\ntity profiles Se(t) with new sections. Figure 7 shows the ratio of sections for which we correctly suggest an article n\nto the right section in the section template S\u0302c(t). The ratio here corresponds to sections that are not present in the entity profile at year t \u2212 1, that is s /\u2208 Se(t \u2212 1). However, given the generated templates S\u0302c(t\u2212 1), we can expand the entity profile Se(t\u2212 1) with a new section at time t. In details, in the absence of a section at time t, our model trains well on similar sections from the section template S\u0302c(t\u2212 1), hence we can predict accurately the section and in this case suggest its addition to the entity profile. With time, it is obvious that the expansion rate decreases at later years as the entity profiles become more \u2018complete\u2019.\nThis is particularly interesting for expanding the entity profiles of long-tail entities as well as updating entities with real-world emerging events that are added constantly. In many cases such missing sections are present at one of the entities of the respective entity class c. An obvious case is the example taken in Section 4.1, where the \u2018Accidents\u2019 is rather common for entities of type Airline. However, it is nonexistent for some specific entity instances, i.e Germanwings airline.\nThrough our ASP approach Fs, we are able to expand both long-tail and trunk entities. We distinguish between the two types of entities by simply measuring their section text length. The real distribution in the ground truth (see Section 5.2) is 27% and 73% are long-tail and trunk entities, respectively. We are able to expand the entity profiles for both cases and all entity classes without a significant difference, with the only exception being the class Creative Work, where we expand significantly more trunk entities."}, {"heading": "7. CONCLUSION AND FUTURE WORK", "text": "In this work, we have proposed an automated approach for the novel task of suggesting news articles to Wikipedia entity pages to facilitate Wikipedia updating. The process consists of two stages. In the first stage, article\u2013entity placement, we suggest news articles to entity pages by considering three main factors, such as entity salience in a news article, relative authority and novelty of news articles for an entity page. In the second stage, article\u2013section placement, we determine the best fitting section in an entity page. Here, we remedy the problem of incomplete entity section profiles by constructing section templates for specific entity classes. This allows us to add missing sections to entity pages. We carry out an extensive experimental evaluation on 351,983 news articles and 73,734 entities coming from 27 distinct entity classes. For the first stage, we achieve an overall performance with P=0.93, R=0.514 and F1=0.676, outperforming our baseline competitors significantly. For the second stage, we show that we can learn incrementally to determine the correct section for a news article based on section templates. The overall performance across different classes is P=0.844, R=0.885 and F1=0.860.\nIn the future, we will enhance our work by extracting facts from the suggested news articles. Results suggest that the news content cited in entity pages comes from the first para-\ngraphs. However, challenging task such as the canonicalization and chronological ordering of facts, still remain.\nAcknowledgements. This work is funded by the ERC Advanced Grant ALEXANDRIA (grant no. 339233)."}, {"heading": "8. REFERENCES", "text": "[1] K. Balog and H. Ramampiaro. Cumulative citation\nrecommendation: classification vs. ranking. In 36th ACM SIGIR, Dublin, Ireland, 2013, pages 941\u2013944.\n[2] K. Balog, H. Ramampiaro, N. Takhirov, and K. N\u00f8rv\u030aag. Multi-step classification approaches to cumulative citation recommendation. In OAIR, Lisbon, Portugal, 2013, pages 121\u2013128.\n[3] Y. Bernstein and J. Zobel. Redundant documents and search effectiveness. In 14th ACM CIKM, pages 736\u2013743, New York, USA, 2005.\n[4] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and S. Hellmann. DBpedia - a crystallization point for the web of data. J. Web Sem., 7(3), Sept. 2009.\n[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993\u20131022, Mar. 2003.\n[6] L. Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001.\n[7] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. ACM TIST, 2(3):27, 2011.\n[8] J. Dunietz and D. Gillick. A new entity salience task with millions of training examples. In 14th EACL, Gothenburg, Sweden, pages 205\u2013209, 2014.\n[9] P. Ferragina and U. Scaiella. Fast and accurate annotation of short texts with wikipedia pages. IEEE Software, 29(1):70\u201375, 2012.\n[10] B. Fetahu, A. Anand, and A. Anand. How much is wikipedia lagging behind news? In WebSci \u201915, Oxford, UK, 2015.\n[11] J. Hoffart, M. A. Yosef, I. Bordino, H. Fu\u0308rstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum. Robust disambiguation of named entities in text. In 2011 EMNLP, Stroudsburg, PA, USA, 2011.\n[12] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu. An efficient k-means clustering algorithm: Analysis and implementation. IEEE Trans. Pattern Anal. Mach. Intell., 24(7):881\u2013892, 2002.\n[13] R. Kaptein, P. Serdyukov, A. De Vries, and J. Kamps. Entity ranking using wikipedia as a pivot. In 19th ACM CIKM, New York, USA, 2010.\n[14] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled data. In 47th ACL and the 4th AFNLP, pages 1003\u20131011, Stroudsburg, PA, USA, 2009.\n[15] V. Ng. Supervised noun phrase coreference research: The first fifteen years. In 48th ACL, 2010, Uppsala, Sweden, pages 1396\u20131411.\n[16] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. 1999.\n[17] D. Pelleg, A. W. Moore, et al. X-means: Extending k-means with efficient estimation of the number of clusters. In ICML, pages 727\u2013734, 2000.\n[18] C. Sauper and R. Barzilay. Automatically generating wikipedia articles: A structure-aware approach. In 47th ACL, 2009, Singapore, pages 208\u2013216.\n[19] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A core of semantic knowledge. In 16th WWW, New York, USA, 2007.\n[20] M. Surdeanu, D. McClosky, J. Tibshirani, J. Bauer, A. X. Chang, V. I. Spitkovsky, and C. D. Manning. A simple distant supervision approach for the tac-kbp slot filling task. In Text Analysis Conference 2010 Workshop.\n[21] B. Taneva and G. Weikum. Gem-based entity-knowledge maintenance. In 22nd ACM CIKM, pages 149\u2013158, New York, USA, 2013.\n[22] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL, pages 173\u2013180, Stroudsburg, USA, 2003.\n[23] M. A. Walker, A. K. Joshi, and E. F. Prince. Centering theory in discourse. Oxford University Press, 1998.\n[24] P. Wang and C. Domeniconi. Building semantic kernels for text classification using wikipedia. In 14th ACM SIGKDD, New York, USA, 2008.\n[25] R. West, E. Gabrilovich, K. Murphy, S. Sun, R. Gupta, and D. Lin. Knowledge base completion via search-based question answering. In 23rd WWW, Seoul, Korea, pages 515\u2013526, 2014."}], "references": [{"title": "Cumulative citation recommendation: classification vs. ranking", "author": ["K. Balog", "H. Ramampiaro"], "venue": "In 36th ACM SIGIR, Dublin,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "N\u00f8rv\u030aag. Multi-step classification approaches to cumulative citation recommendation", "author": ["K. Balog", "H. Ramampiaro", "N. Takhirov"], "venue": "In OAIR, Lisbon, Portugal,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Redundant documents and search effectiveness", "author": ["Y. Bernstein", "J. Zobel"], "venue": "14th ACM CIKM, pages 736\u2013743, New York, USA,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "DBpedia a crystallization point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "J. Web Sem., 7(3), Sept.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., 3:993\u20131022, Mar.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM TIST, 2(3):27,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A new entity salience task with millions of training examples", "author": ["J. Dunietz", "D. Gillick"], "venue": "14th EACL, Gothenburg, Sweden, pages 205\u2013209,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate annotation of short texts with wikipedia pages", "author": ["P. Ferragina", "U. Scaiella"], "venue": "IEEE Software, 29(1):70\u201375,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "How much is wikipedia lagging behind news", "author": ["B. Fetahu", "A. Anand"], "venue": "In WebSci \u201915,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Robust disambiguation of named entities in text", "author": ["J. Hoffart", "M.A. Yosef", "I. Bordino", "H. F\u00fcrstenau", "M. Pinkal", "M. Spaniol", "B. Taneva", "S. Thater", "G. Weikum"], "venue": "2011 EMNLP, Stroudsburg, PA, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 24(7):881\u2013892,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Entity ranking using wikipedia as a pivot", "author": ["R. Kaptein", "P. Serdyukov", "A. De Vries", "J. Kamps"], "venue": "19th ACM CIKM, New York, USA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "47th ACL and the 4th AFNLP, pages 1003\u20131011, Stroudsburg, PA, USA,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised noun phrase coreference research: The first fifteen years", "author": ["V. Ng"], "venue": "In 48th ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters", "author": ["D. Pelleg", "A.W. Moore"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Automatically generating wikipedia articles: A structure-aware approach", "author": ["C. Sauper", "R. Barzilay"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Yago: A core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "16th WWW, New York, USA,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "A simple distant supervision approach for the tac-kbp slot filling task", "author": ["M. Surdeanu", "D. McClosky", "J. Tibshirani", "J. Bauer", "A.X. Chang", "V.I. Spitkovsky", "C.D. Manning"], "venue": "In Text Analysis Conference 2010 Workshop", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Gem-based entity-knowledge maintenance", "author": ["B. Taneva", "G. Weikum"], "venue": "22nd ACM CIKM, pages 149\u2013158, New York, USA,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "NAACL, pages 173\u2013180, Stroudsburg, USA,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Centering theory in discourse", "author": ["M.A. Walker", "A.K. Joshi", "E.F. Prince"], "venue": "Oxford University Press,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Building semantic kernels for text classification using wikipedia", "author": ["P. Wang", "C. Domeniconi"], "venue": "14th ACM SIGKDD, New York, USA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Knowledge base completion via search-based question answering", "author": ["R. West", "E. Gabrilovich", "K. Murphy", "S. Sun", "R. Gupta", "D. Lin"], "venue": "23rd WWW, Seoul, Korea, pages 515\u2013526,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 104, "endOffset": 107}, {"referenceID": 23, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 187, "endOffset": 191}, {"referenceID": 12, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 208, "endOffset": 212}, {"referenceID": 19, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 237, "endOffset": 245}, {"referenceID": 13, "context": "Moreso, this knowledge is harvested and utilized in building knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].", "startOffset": 237, "endOffset": 245}, {"referenceID": 9, "context": "In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "In addition, their use is in line with current Wikipedia editing practice, as is shown in [10] that almost 20% of current citations in all entity pages are news articles.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "Existing efforts to populate Wikipedia [18] start from an entity page and then generate candidate documents about this entity using an external search engine (and then postprocess them).", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "Most of the works on knowledge base acceleration [2, 1, 8], or Wikipedia page generation [18] rely on high quality input sources which are then utilized to extract textual facts for Wikipedia page population.", "startOffset": 49, "endOffset": 58}, {"referenceID": 0, "context": "Most of the works on knowledge base acceleration [2, 1, 8], or Wikipedia page generation [18] rely on high quality input sources which are then utilized to extract textual facts for Wikipedia page population.", "startOffset": 49, "endOffset": 58}, {"referenceID": 7, "context": "Most of the works on knowledge base acceleration [2, 1, 8], or Wikipedia page generation [18] rely on high quality input sources which are then utilized to extract textual facts for Wikipedia page population.", "startOffset": 49, "endOffset": 58}, {"referenceID": 17, "context": "Most of the works on knowledge base acceleration [2, 1, 8], or Wikipedia page generation [18] rely on high quality input sources which are then utilized to extract textual facts for Wikipedia page population.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Sauper and Barzilay [18] propose an approach for automatically generating whole entity pages for specific entity classes.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "Taneva and Weikum [21] propose an approach that constructs short summaries for the long tail.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "In contrast to [18] and [21], we actually focus on suggesting entire documents to Wikipedia entity pages.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "In contrast to [18] and [21], we actually focus on suggesting entire documents to Wikipedia entity pages.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "Both the approaches in [18] and [21] (finding paragraphs and summarization) could then be used to process the documents we suggest further.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Both the approaches in [18] and [21] (finding paragraphs and summarization) could then be used to process the documents we suggest further.", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "Balog [2, 1] recommend news citations for an entity.", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Balog [2, 1] recommend news citations for an entity.", "startOffset": 6, "endOffset": 12}, {"referenceID": 24, "context": "[25] consider the problem of knowledge base completion, through question answering and complete missing facts in Freebase based on templates, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Determining which entities are prominent or salient in a given text has a long history in NLP, sparked by the linguistic theory of Centering [23].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "Salience has been used in pronoun and co-reference resolution [15], or to predict which entities will be included in an abstract of an article [8].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Salience has been used in pronoun and co-reference resolution [15], or to predict which entities will be included in an abstract of an article [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "We therefore use the salience features in [8] as part of our model.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "Features set of features as proposed by Dunietz and Gillick [8]", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "We reimplemented the ones in Dunietz and Gillick [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "Table 2 in [8] gives details.", "startOffset": 11, "endOffset": 14}, {"referenceID": 15, "context": "We opt for two approaches: (i) probability of entity e occurring in the corpus N, and (ii) authority assessed through centrality measures like PageRank [16].", "startOffset": 152, "endOffset": 156}, {"referenceID": 2, "context": "Studies [3] have shown that on comparable collections to ours (TREC GOV2) the number of duplicates can go up to 17%.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "Next, we cluster the entity sections, based on an extended version of k\u2013means clustering [12], namely x\u2013means clustering introduced in Pelleg et al.", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "which estimates the number of clusters efficiently [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "The topic models are computed using LDA [5].", "startOffset": 40, "endOffset": 43}, {"referenceID": 17, "context": "Following the same intuition as in [18], we weigh the importance of articles by the count of specific POS tags.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "An invasive approach, which was proposed by Barzilay and Sauper [18], adds content directly to Wikipedia and expects the editors or other users to redact irrelevant content over a period of time.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "We POS-tag the news articles and entity profiles Se(t) with the Stanford tagger [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "For entity linking the news articles, we use TagMe![9] with a confidence score of 0.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "The first baseline uses only the salience-based features by Dunietz and Gillick [8].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "We use Random Forests (RF) [6].", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "The red curve shows baseline B1 [8], and the blue one shows the performance of Fe.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "We use Random Forests (RF) [6] and Support Vector Machines (SVM) [7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "We use Random Forests (RF) [6] and Support Vector Machines (SVM) [7].", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the salience and relative authority of entities, and the novelty of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93% in the article-entity suggestion stage and upto 84% for the article-section placement. Finally, we compare our approach against competitive baselines and show significant improvements.", "creator": "LaTeX with hyperref package"}}}