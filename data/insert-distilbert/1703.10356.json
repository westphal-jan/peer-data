{"id": "1703.10356", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Simplified End-to-End MMI Training and Voting for ASR", "abstract": "an hybrid of a hidden markov model ( hmm ) and a deep neural network ( dnn ) is considered. defining end - to - end training systems using gradient descent is suggested, similarly to employing the training exercises of connectionist information temporal classification ( ctc ). we use a maximum a - posteriori ( map ) criterion designed with assuming a simple language evolution model in the training class stage, and a standard hmm decoder algorithms without approximations. recognition results are presented using speech databases. our method compares favorably to ctc in terms of performance, robustness and quality of alignments.", "histories": [["v1", "Thu, 30 Mar 2017 08:40:19 GMT  (876kb,D)", "http://arxiv.org/abs/1703.10356v1", "Submitted to Interspeech 2017"], ["v2", "Sun, 16 Jul 2017 15:12:39 GMT  (1184kb,D)", "http://arxiv.org/abs/1703.10356v2", null]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["lior fritz", "david burshtein"], "accepted": false, "id": "1703.10356"}, "pdf": {"name": "1703.10356.pdf", "metadata": {"source": "CRF", "title": "End-to-End MAP Training of a Hybrid HMM-DNN Model", "authors": ["Lior Fritz", "David Burshtein"], "emails": ["lior.fritz@gmail.com,", "burstyn@eng.tau.ac.il"], "sections": [{"heading": "1. Introduction", "text": "A major advantage of connectionist temporal classification (CTC) [1] over a hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) [2] lies in the simplicity of the training and its scalability. However, there are some issues with CTC. First, exact decoding is computationally intractable, and one needs to use some approximation [1]. In addition, CTC does not excel in providing a good alignment between the input and output sequences, posing challenges in some applications [3, 4].\nWe suggest a novel training method of an HMM-DNN hybrid. Similarly to CTC, we use end-to-end training. However, unlike CTC, our derivation is formulated using the maximum a-posteriori (MAP) approach with a simple language model (LM) in the training stage. The resulting objective function is similar to [5] with some differences. First, we derive our method from first principles as a MAP estimate. Second, we allow the integration of a scalable LM into the objective function. Decoding can be performed using a standard HMM decoder, without approximations. We use the weighted finite state transducer (WFST) approach [6] and discuss some implementation issues. Speech recognition results on the Wall Street Journal (WSJ) corpus [7] compare favorably with CTC. One of the advantages of the method is its robustness and the fact that it can be used to provide reliable alignment between the input and output sequences."}, {"heading": "2. The Model", "text": "Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10]. A blank label is inserted between words, and we reserve two states for sentence start and end. The transcript (template),\n\u0393, of the sentence is its sequence of labels. As an example, consider the sentence \u201cis he\u201d and assume a character-based modeling. Then the transcript is \u0393 = {start, blank, i, s, blank, h, e, blank, end}.\nIn fact, the sentence transcript, \u0393, defines a left to right HMM, where transitions are only allowed between neighboring states in the transcript, or from a state to itself (self transition). That is,\nP (\u0393k | \u0393j) = { p\u0393j (0) if k = j p\u0393j (1) if k = j + 1 0 otherwise\n(1)\nand pj(0) + pj(1) = 1 for all j. Figure 1 describes the character-based HMM defined by the sentence \u201cis he\u201d.\nTo formulate a MAP estimation scheme we need to define a probabilistic LM for strings s = (s0, s1, . . . , sT ). This model also defines the distribution of the transcript random variable, \u0393. As an optimal solution, one would use the ground-truth LM used in the decoding stage, yet this would be computationally infeasible. As an alternative solution, we suggest a degenerated model of labels bi-grams. We have found that this model is sufficient for obtaining state-of-the-art results. Given that at time t\u22121 the process is at state c, we remain at c with probability pc(0), and make a transition to any other state c\u0302 with probability pc(1)q(c, c\u0302). That is,\npc,c\u0302 =P (St = c\u0302 | St\u22121 = c) = {\npc(0) if c\u0302 = c pc(1) \u00b7 q(c, c\u0302) otherwise (2)\nWe assume q(c, c) = 0. For a characters based labeling this constraint can be dealt with either by using the solution suggested in [5], or by defining additional characters that represent consecutive identical characters (e.g. \u2018ll\u2019, \u2018mm\u2019). For phonetic based labeling we add a blank label between consecutive identical phonemes.\nLet {ot} be the sequence of acoustic feature vectors representing some sentence. Define a time-extended feature vector, o\u0303t\n\u2206= (ot\u2212F , . . . ,ot, . . . ,ot+F ), for some integer F . We assume a left to right HMM for the sequence {o\u0303t}. The underlying hidden state (label) sequence is {st}, where st \u2208 {0, 1, . . . , L\u22121} and L is the total number of possible states. The joint probability of o\u0303 \u2206={o\u0303t}Tt=1\nar X\niv :1\n70 3.\n10 35\n6v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 7\nand s \u2206= {st}Tt=0 is given by\nP (o\u0303, s) = T\u220f t=1 pst\u22121,st T\u220f t=1 P (o\u0303t | st) (3)\nSuppose also that some neural network (NN) recognizer produces the values yt,st \u2261 yt,st,\u03b8, which are estimates of 1 logP (st | o\u0303t), for t = 1, 2, . . . , T . Here \u03b8 are the parameters of the NN. Note that \u2211L\u22121 l=0 e\nyt,l = 1 for all t. By Bayes\u2019 law, (3) can be rewritten as,\nP (o\u0303, s) = A ( T\u220f t=1 pst\u22121,st ) \u00b7 exp { T\u2211 t=1 (yt,st \u2212 \u03c9st ) } (4) where \u03c9l \u2206= logP (l) is the logarithm of the prior proba-\nbility of the state l, and A = \u220fT t=1 P (o\u0303t).\nWe train our model using a database of N pairs (\u0393n, o\u0303n), n = 1, 2, . . . , N , where o\u0303n is the n-th observation sequence, and \u0393n is its associated transcript. First, the values of the probability transition matrix q(c, c\u0302) can be estimated from a training text using\nq(c, c\u0302) = Nc,c\u0302/Nc (5)\nwhere Nc,c\u0302 (Nc, respectively) is the number of occurrences of label pairs c, c\u0302 (labels, c) in the text.\nTo train the remaining model parameters we use the MAP criterion. The goal is to maximize the loglikelihood of the transcript given its observation sequence. That is, the MAP estimate, \u03a8MAP, to the parameter vector, \u03a8 = { \u03b8, {pj(0), pj(1), \u03c9j}j\u2208{0,1,...,L\u22121} } , is given by\n\u03a8MAP = argmax \u03a8 N\u2211 n=1 logP\u03a8 (\u0393n | o\u0303n) (6)\nlogP (\u0393 | o\u0303) = logP (o\u0303,\u0393)\u2212 logP (o\u0303) (7) Since the term A in (4) cancels out in (7), in the sequel we set A = 1 without loss of generality.\nOur criterion (6)-(7) is similar to the one in [5]. However, our criterion is derived as a MAP estimate. Also, in [5] the values of the transition probabilities between all labels have to be trained, whereas in our model, the probabilities q(c, c\u0302) can be pre-trained, so that only pc(0) for all labels c need to be trained from the acoustics. This also allows convenient integration with higher-level LM.\nOnce the model has been trained, Decoding is preformed with a standard LM instead of the simple bi-gram LM used for training. Thus we can apply a standard HMM decoder as we explain later in more detail."}, {"heading": "2.1. Training Procedure", "text": "To apply (6), we need to compute P (o\u0303) and P (o\u0303,\u0393). By (4) and since, as explained, we may set A = 1, the first term, P (o\u0303), is computed by summing over all possible state sequences, as follows,\nP (o\u0303) = \u2211\ns ( T\u220f t=1 pst\u22121,st ) \u00b7 exp { T\u2211 t=1 yt,st } (8)\n1The base of all the logarithms in this paper is e\nwhere we denote yt,st \u2206= yt,st \u2212 \u03c9st . The transition probabilities are given by (2). To compute P (o\u0303,\u0393) we use,\nP (o\u0303,\u0393) = \u2211\ns\u2208S(\u0393) ( T\u220f t=1 pst\u22121,st ) \u00b7 exp { T\u2211 t=1 yt,st } (9)\nwhere S(\u0393) are all state sequences (s0, s1, . . . , sT ) which are consistent with the given transcript, \u0393.\nTo compute P (o\u0303,\u0393) and its derivatives efficiently, first denote\n\u03b1t(k) \u2206= P (T (S0, . . . , St) \u2208 (\u03930, . . . ,\u0393k) (10)\n, o\u03031, . . . , o\u0303t)\n\u03b2t(k) \u2206= P (T (St, . . . , ST ) \u2208 (\u0393k, . . . ,\u0393K) (11)\n, o\u0303t+1, . . . , o\u0303T | St = \u0393k)\nwhere T (S0, . . . , St) is the transcript of the state sequence up to time t and where \u03930, . . . ,\u0393k is the k + 1 prefix of \u0393. T (St, . . . , ST ) is the transcript of the state sequence from t to T and \u0393k, . . . ,\u0393K is the K \u2212 k + 1\nsuffix of \u0393. Then P (o\u0303,\u0393) = K\u2211 k=0 \u03b1t(k)\u03b2t(k) for any t.\nNow, \u03b1t(k) and \u03b2t(k) can be computed efficiently using the following time recursions,\n\u03b1t(k) = (\u03b1t\u22121(k)p\u0393k (0) (12) +\u03b1t\u22121(k \u2212 1)p\u0393k\u22121(1)q(\u0393k\u22121,\u0393k) ) eyt,\u0393k\n\u03b2t(k) =\u03b2t+1(k)p\u0393k (0)e yt+1,\u0393k (13)\n+ \u03b2t+1(k + 1)p\u0393k (1)q(\u0393k,\u0393k+1)e yt+1,\u0393k+1\nOur training algorithm is a stochastic gradient descent (SGD) that operates on mini-batches. To compute the gradient with respect to any element, \u03b8i, of the parameter vector, \u03b8, we use\n\u2202P (o\u0303,\u0393) \u2202\u03b8i = \u2211 t,k \u2202P (o\u0303,\u0393) \u2202yt,k \u2202yt,k \u2202\u03b8i\n(14)\nWe calculate \u2202yt,k \u2202\u03b8i using the back-propagation algorithm. In addition, by (9),\n\u2202P (o\u0303,\u0393) \u2202yt,r\n= \u2211\nk : \u0393k=r\n\u03b1t(k)\u03b2t(k) (15)\n\u2202P (o\u0303,\u0393) \u2202\u03c9r = \u2212 T\u2211 t=1 \u2211 k : \u0393k=r \u03b1t(k)\u03b2t(k) (16)\n\u2202P (o\u0303,\u0393) \u2202pr(0) = T\u2211 t=1 \u2211 k : \u0393k=r \u03b1t\u22121(k)\u03b2t(k) exp { yt,\u0393k } (17)\n\u2202P (o\u0303,\u0393) \u2202pr(1) = T\u2211 t=1 \u2211 k : \u0393k=r \u03b1t\u22121(k)q(\u0393k,\u0393k+1) (18)\n\u00b7 \u03b2t(k + 1) exp { yt,\u0393k+1 } Note that the values eyt,l are constrained to be valid probabilities. To satisfy this constraint we use the standard approach of a softmax layer at the output of the\nNN. Similarly, pj(0), pj(1) and \u03c9j are constrained to be valid probabilities. To satisfy this constraint, we first define unconstrained transition probabilities and priors and pass them through a softmax function that produces valid transition probabilities and priors. The derivatives of the unconstrained parameters can be calculated using the chain rule, taking into account the softmax derivatives and (15)-(18).\nTo compute P (o\u0303) and its derivatives efficiently, first denote\n\u03b1\u0303t(c) \u2206= P (o\u03031, o\u03032, . . . , o\u0303t, St = c) (19)\n\u03b2\u0303t(c) \u2206= P (o\u0303t+1, o\u0303t+2, . . . , o\u0303T | St = c) (20) Then P (o\u0303) = \u2211 c \u03b1\u0303t(c)\u03b2\u0303t(c) for any t. Now, \u03b1\u0303t(c) and \u03b2\u0303t(c) can be computed efficiently using,\n\u03b1\u0303t(c) = ( \u03b1\u0303t\u22121(c)pc(0) (21)\n+ \u2211 c\u03026=c \u03b1\u0303t\u22121(c\u0302)pc\u0302(1)q(c\u0302, c) ) eyt,c\n\u03b2\u0303t(c) =\u03b2\u0303t+1(c)pc(0)eyt+1,c (22) + \u2211 c\u03026=c \u03b2\u0303t+1(c\u0302)pc(1)q(c, c\u0302)eyt+1,c\u0302\nAlso,\n\u2202P (o\u0303) \u2202yt,r = \u03b1\u0303t(r)\u03b2\u0303t(r) (23)\n\u2202P (o\u0303) \u2202\u03c9r = \u2212 T\u2211 t=1 \u03b1\u0303t(r)\u03b2\u0303t(r) (24)\n\u2202P (o\u0303) \u2202pr(0) = T\u2211 t=1 \u03b1\u0303t\u22121(r)\u03b2\u0303t(r) exp { yt,r }\n(25)\n\u2202P (o\u0303) \u2202pr(1) = T\u2211 t=1 \u2211 c6=r \u03b1\u0303t\u22121(r)q(r, c)\u03b2\u0303t(c) exp { yt,c } (26)"}, {"heading": "2.2. Decoding", "text": "To decode we can use standard HMM decoding techniques without approximations. To integrate our model with a LM, we use the WFST approach. A WFST is a finite-state machine, in which each transition has an input symbol, an output symbol and a weight. Our WFST is implemented with the FST library OpenFST [11], and is based on the decoding method of EESEN [10].\nWe first build separate WFSTs for the LM (grammar), the lexicon and the HMM labels. An example for the grammar WFST, G, with two possible sentences is shown in Figure 2a. The lexicon WFST, L, encodes sequences of lexicon units (e.g. phonemes or characters) to words. It enforces the occurrence of the blank label between words, and, for phoneme labeling, also between identical labels. An example is shown in Figure 2b. The HMM WFST, H, maps a sequence of frame-level labels into a single lexicon unit. It allows occurrences of repetitive labels with the proper weighting. An example is shown in Figure 2c. Finally, we compose these graphs into a single graph. The inputs to this graph are the outputs of the neural network, normalized by the priors.\n0 1cats:cats/1.0\n2 like:lik e/0.5\n4\nplaying:playing/1.0\n3\nhate:hate/0.5 water: water/ 1.0\n(a) The G WFST exemplified on the sentences: \u201ccats like playing\u201d and \u201ccats hate water\u201d. The caption on each arc represents: input symbol : output symbol / weight.\n0 1 bl: 2 K:cats 3 AE: 4 T: 5 S:\nbl:\n(b) The L WFST exemplified on the word \u201ccats\u201d using phonemes. The symbol represents no input/output."}, {"heading": "2.3. Numerical Issues", "text": "For HMMs and CTC, the calculations of the forward and backward variables are usually performed in log-scale, as in [9,10]. Another possibility is to use the normalization technique in [12]. In our implementation it was necessary to combine both approaches, with some modification of the method in [12], as we now describe. For each time frame, t, we first compute the forward (backward, respectively) variables according to the original recursions in log-scale, and then subtract the maximal value of the forward (backward) variables at t from each forward (backward) variable. A simpler variant of the above is to perform the subtraction only every certain time units.\nFor simplicity, we describe our method in the linear scale, for P (o\u0303) and its derivatives. The adaptation to the calculation of P (o\u0303,\u0393) and its derivatives is straightforward. Denote \u03b1\u0303\u2217t\n\u2206= max c \u03b1\u0303t(c) and \u03b2\u0303\u2217t \u2206= max c \u03b2\u0303t(c).\nEffectively, the normalized forward and backward variables are given by,\n\u03b1t(c) = \u03b1\u0303t(c) t\u220f\nu=1 \u03b1\u0303\u2217u , \u03b2t(c) = \u03b2\u0303t(c) T\u220f u=t \u03b2\u0303\u2217u\n(27)\nDefining P t(o\u0303) , \u2211\nc \u03b1t(c)\u03b2t(c), we have P t(o\u0303) = \u2211 c \u03b1\u0303t(c) t\u220f\nu=1 \u03b1\u0303\u2217u \u03b2\u0303t(c) T\u220f u=t \u03b2\u0303\u2217u = P (o\u0303) t\u220f u=1 \u03b1\u0303\u2217u T\u220f u=t \u03b2\u0303\u2217u\n(28)\nUsing (23)-(24) yields (the derivatives with respect to pr(0) and pr(1) can be obtained similarly using (25)- (26)),\n\u2202 logP (o\u0303) \u2202yt,r = \u03b1t(r)\u03b2t(r) P t(o\u0303)\n(29)\n\u2202 logP (o\u0303) \u2202\u03c9r = \u2212 T\u2211 t=1 \u03b1t(r)\u03b2t(r) P t(o\u0303)\n(30)"}, {"heading": "2.4. Implementation", "text": "We implemented our model in Tensorflow [13]. Our loss function is implemented in C++, where the samples in the mini-batch are calculated in parallel on different cores of the CPU. We have integrated the feature extraction procedures from EESEN (which uses Kaldi\u2019s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN. Since the computation time required by the NN is dominant, the training process takes only 5%-8% more time when using our loss function in comparison to CTC."}, {"heading": "3. Experiments", "text": "We conducted experiments on the Wall Street Journal (WSJ) corpus [7]. The training data consists of 81 hours of transcribed speech. We use almost the same training process and NN architecture as in [10]. We extract 95% of the training set for training, and 5% for cross validation. The inputs of the NN are 40-dimensional filterbank with delta and delta-delta coefficients. The features are normalized by mean and variance on the speaker basis. We operate on a phonemes-based system, with 72 labels. The utterances in the training set are sorted by their lengths. The mini-batch size is set to 30. We use the ADAM optimizer [17] with an initial learning rate of 0.001. We use gradient clipping [18] with a value of 50. The learning rate decay and the stopping criteria are determined based on the validation WER (optimizing for each method). We test our model on the eval92 and dev93 test sets. The acoustic model scores are scaled down by a factor of 0.5-0.9, and the optimal value is chosen. We use the standard pruned trigram LM of WSJ, as in [10]. The bi-gram LM used in training is obtained using the training set text, and the large WSJ LM training set text. We use a RNN architecture of 4 layers of 320 bi-directional LSTM cells [19] without peephole connections [20].\nTable 1 shows the obtained WER on the validation, eval92 and dev93 sets. We compare our model (HMMMAP) to the CTC model under the same conditions. Decoding with CTC is performed using the TLG WFST and the prior normalization method in [10]. Our CTC model obtains comparable results with the ones reported in [10]. We see a consistent improvement in WER of HMM-MAP compared to CTC. Figure 3 shows the validation WER vs. epoch of CTC, and our model using three different LMs during training: bi-grams, uni-grams and uniform. It demonstrates the importance of using a reasonable LM during training. Figure 4 shows the obtained alignments on some utterance from the training set. It demonstrates a significantly improved alignment of HMM-MAP compared to CTC, which is due to the peaky output distributions of CTC."}, {"heading": "4. Conclusions", "text": "We have presented a novel end-to-end MAP training method of a HMM-DNN hybrid. The model compares favorably to CTC on the WSJ corpus. We also believe our method is more robust in the following sense. In [21], using CTC, it was found that the network\u2019s prediction of the blank label was too dominant during decoding. The authors suggested a blank-prediction-diminishing method, by subtracting a high prior from the network\u2019s blank prediction. In [10] a different priors normalization method was proposed, which also diminishes the blank predictions. However, the formulation of the CTC model does not justify priors normalization. In [22] it has been shown that the priors subtraction method in [10] might even degrade the WER in some cases.\nIn future work, we plan to further examine the effect of the LM used in the training process, and apply our model to other sequence-to-sequence tasks, where it may be important to provide good alignment between the input and output sequences."}, {"heading": "5. Acknowledgements", "text": "This research was supported by the Yitzhak and Chaya Weinstein Research Institute for Signal Processing. A Tesla K40c GPU used in this research was donated by the NVIDIA Corporation."}, {"heading": "6. References", "text": "[1] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber,\n\u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[3] Y. Wang, X. Deng, S. Pu, and Z. Huang, \u201cResidual convolutional CTC networks for automatic speech recognition,\u201d arXiv preprint arXiv:1702.07793, 2017.\n[4] R. Sanabria, F. Metze, and F. De La Torre, \u201cRobust end-to-end deep audiovisual speech recognition,\u201d arXiv preprint arXiv:1611.06986, 2016.\n[5] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2letter: an end-to-end convnet-based speech recognition system,\u201d Neural Information Processing Systems (NIPS). [Online]. Available: https://arxiv.org/abs/1609.03193\n[6] M. Mohri, F. Pereira, and M. Riley, \u201cWeighted finitestate transducers in speech recognition,\u201d Computer Speech & Language, vol. 16, no. 1, pp. 69\u201388, 2002.\n[7] D. B. Paul and J. M. Baker, \u201cThe design for the Wall Street Journal-based CSR corpus,\u201d in Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357\u2013362.\n[8] R. L. Rabiner, \u201cA tutorial on hidden Markov models and selected applications in speech recognition,\u201d Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.\n[9] A. Y. Hannun, C. Case, J. Casper, B. C. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, \u201cDeep speech: scaling up end-to-end speech recognition,\u201d CoRR, vol. abs/1412.5567, 2014. [Online]. Available: http://arxiv.org/abs/1412.5567\n[10] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding,\u201d in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), December 2015.\n[11] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri, \u201cOpenFst: A general and efficient weighted finite-state transducer library. Implementation and Application of Automata,\u201d 2007.\n[12] L. R. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition. PTR Prentice Hall, 1993.\n[13] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., \u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems,\u201d arXiv preprint arXiv:1603.04467, 2016.\n[14] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe Kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.\n[15] Y. Miao, \u201cKaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn,\u201d arXiv preprint arXiv:1401.6984, 2014.\n[16] V. Renkens, \u201cKaldi with TensorFlow Neural Net,\u201d https: //github.com/vrenkens/tfkaldi, 2016.\n[17] D. Kingma and J. Ba, \u201cADAM: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[18] R. Pascanu, T. Mikolov, and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d International Conference on Machine Learning (ICML), vol. 28, pp. 1310\u20131318, 2013.\n[19] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.\n[20] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, \u201cLearning precise timing with LSTM recurrent networks,\u201d Journal of machine learning research, vol. 3, no. Aug, pp. 115\u2013143, 2002.\n[21] H. Sak, A. Senior, K. Rao, O. Irsoy, A. Graves, F. Beaufays, and J. Schalkwyk, \u201cLearning acoustic frame labeling for speech recognition with recurrent neural networks,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4280\u20134284.\n[22] N. Kanda, X. Lu, and H. Kawai, \u201cMaximum a posteriori based decoding for CTC acoustic models,\u201d Interspeech 2016, pp. 1868\u20131872, 2016."}], "references": [{"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Residual convolutional CTC networks for automatic speech recognition,", "author": ["Y. Wang", "X. Deng", "S. Pu", "Z. Huang"], "venue": "arXiv preprint arXiv:1702.07793,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Robust end-to-end deep audiovisual speech recognition,", "author": ["R. Sanabria", "F. Metze", "F. De La Torre"], "venue": "arXiv preprint arXiv:1611.06986,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Weighted finitestate transducers in speech recognition,", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Computer Speech & Language,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "The design for the Wall Street Journal-based CSR corpus,", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition,", "author": ["R.L. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Deep speech: scaling up end-to-end speech recognition,", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B.C. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "CoRR, vol. abs/1412.5567,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "EESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding,", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["C. Allauzen", "M. Riley", "J. Schalkwyk", "W. Skut", "M. Mohri"], "venue": "Implementation and Application of Automata,\u201d", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Fundamentals of Speech Recognition", "author": ["L.R. Rabiner", "B.-H. Juang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems,", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "The Kaldi speech recognition toolkit,", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1925}, {"title": "Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn,", "author": ["Y. Miao"], "venue": "arXiv preprint arXiv:1401.6984,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Kaldi with TensorFlow Neural Net,", "author": ["V. Renkens"], "venue": "https: //github.com/vrenkens/tfkaldi,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "ADAM: A method for stochastic optimization,", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks,", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Long short-term memory,", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Learning precise timing with LSTM recurrent networks,", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "Journal of machine learning research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks,", "author": ["H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Maximum a posteriori based decoding for CTC acoustic models,", "author": ["N. Kanda", "X. Lu", "H. Kawai"], "venue": "Interspeech 2016,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Introduction A major advantage of connectionist temporal classification (CTC) [1] over a hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) [2] lies in the simplicity of the training and its scalability.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Introduction A major advantage of connectionist temporal classification (CTC) [1] over a hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) [2] lies in the simplicity of the training and its scalability.", "startOffset": 159, "endOffset": 162}, {"referenceID": 0, "context": "First, exact decoding is computationally intractable, and one needs to use some approximation [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "In addition, CTC does not excel in providing a good alignment between the input and output sequences, posing challenges in some applications [3, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 3, "context": "In addition, CTC does not excel in providing a good alignment between the input and output sequences, posing challenges in some applications [3, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 4, "context": "We use the weighted finite state transducer (WFST) approach [6] and discuss some implementation issues.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Speech recognition results on the Wall Street Journal (WSJ) corpus [7] compare favorably with CTC.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "The Model Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "The Model Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10].", "startOffset": 254, "endOffset": 261}, {"referenceID": 8, "context": "The Model Consider a sentence where each word is represented by a left to right hidden Markov model (HMM) [8] whose states are the basic elements of the word (termed labels), such as the character spelling or the phonetic transcription of the word as in [9, 10].", "startOffset": 254, "endOffset": 261}, {"referenceID": 9, "context": "Our WFST is implemented with the FST library OpenFST [11], and is based on the decoding method of EESEN [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "Our WFST is implemented with the FST library OpenFST [11], and is based on the decoding method of EESEN [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Numerical Issues For HMMs and CTC, the calculations of the forward and backward variables are usually performed in log-scale, as in [9,10].", "startOffset": 132, "endOffset": 138}, {"referenceID": 8, "context": "Numerical Issues For HMMs and CTC, the calculations of the forward and backward variables are usually performed in log-scale, as in [9,10].", "startOffset": 132, "endOffset": 138}, {"referenceID": 10, "context": "Another possibility is to use the normalization technique in [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "In our implementation it was necessary to combine both approaches, with some modification of the method in [12], as we now describe.", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "Model WER% validation eval92 dev93 CTC, EESEN [10] 7.", "startOffset": 46, "endOffset": 50}, {"referenceID": 8, "context": "87 Hybrid HMM-DNN [10] 7.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "We implemented our model in Tensorflow [13].", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "We have integrated the feature extraction procedures from EESEN (which uses Kaldi\u2019s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN.", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "We have integrated the feature extraction procedures from EESEN (which uses Kaldi\u2019s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN.", "startOffset": 102, "endOffset": 110}, {"referenceID": 14, "context": "We have integrated the feature extraction procedures from EESEN (which uses Kaldi\u2019s scripts [14]) and [15, 16], the NN procedures from Tensorflow and the WFSTs decoding procedures from EESEN.", "startOffset": 102, "endOffset": 110}, {"referenceID": 5, "context": "We conducted experiments on the Wall Street Journal (WSJ) corpus [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "We use almost the same training process and NN architecture as in [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "We use the ADAM optimizer [17] with an initial learning rate of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "We use gradient clipping [18] with a value of 50.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "We use the standard pruned trigram LM of WSJ, as in [10].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "We use a RNN architecture of 4 layers of 320 bi-directional LSTM cells [19] without peephole connections [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "We use a RNN architecture of 4 layers of 320 bi-directional LSTM cells [19] without peephole connections [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "Decoding with CTC is performed using the TLG WFST and the prior normalization method in [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Our CTC model obtains comparable results with the ones reported in [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "In [21], using CTC, it was found that the network\u2019s prediction of the blank label was too dominant during decoding.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [10] a different priors normalization method was proposed, which also diminishes the blank predictions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [22] it has been shown that the priors subtraction method in [10] might even degrade the WER in some cases.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [22] it has been shown that the priors subtraction method in [10] might even degrade the WER in some cases.", "startOffset": 64, "endOffset": 68}], "year": 2017, "abstractText": "An hybrid of a hidden Markov model (HMM) and a deep neural network (DNN) is considered. End-to-end training using gradient descent is suggested, similarly to the training of connectionist temporal classification (CTC). We use a maximum a-posteriori (MAP) criterion with a simple language model in the training stage, and a standard HMM decoder without approximations. Recognition results are presented using speech databases. Our method compares favorably to CTC in terms of performance, robustness and quality of alignments.", "creator": "LaTeX with hyperref package"}}}