{"id": "1703.10579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Evaluating Complex Task through Crowdsourcing: Multiple Views Approach", "abstract": "with the popularity of massive open online courses, grading through crowdsourcing has become a prevalent approach towards large scale classes. however, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters carries a restriction of insufficient knowledge of the workers from the crowd. due to knowledge limitation of the crowd as graders, grading based on partial perspectives becomes a one big challenge for evaluating complex tasks through crowdsourcing. especially for those tasks which not only need specific knowledge for grading, but also should be graded as practically a whole instead of grades being decomposed into smaller and simpler subtasks. we propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. aggregation correction algorithm based on graders variances are used to systematically combine the grades for each view. we also detect bias patterns posterior of the graders, and debias them regarding each view of the task. bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. the proposed approach is analyzed on a typical synthetic data set. we show that our model gives more accurate results compared to the grading approaches using without different views and debiasing algorithm.", "histories": [["v1", "Thu, 30 Mar 2017 17:25:47 GMT  (722kb)", "http://arxiv.org/abs/1703.10579v1", "8 pages, 13 figures, the paper is accepted by ICCSE 2016"]], "COMMENTS": "8 pages, 13 figures, the paper is accepted by ICCSE 2016", "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["lingyu lyu", "mehmed kantardzic"], "accepted": false, "id": "1703.10579"}, "pdf": {"name": "1703.10579.pdf", "metadata": {"source": "CRF", "title": "Evaluating Complex Task through Crowdsourcing: Multiple Views Approach", "authors": ["Lingyu Lyu", "Mehmed Kantardzic"], "emails": ["l0lv0002@louisville.edu", "mehmed.kantardzic@louisville.edu"], "sections": [{"heading": null, "text": "(MOOCs), grading through crowdsourcing has become a prevalent approach towards large scale classes. However, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters a restriction of insufficient knowledge of the workers from the crowd. Due to knowledge limitation of the crowd graders, grading based on partial perspectives becomes a big challenge for evaluating complex tasks through crowdsourcing. Especially for those tasks which not only need specific knowledge for grading, but also should be graded as a whole instead of being decomposed into smaller and simpler sub-tasks. We propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. Aggregation algorithm based on graders\u2019 variances are used to combine the grades for each view. We also detect bias patterns of the graders, and de-bias them regarding each view of the task. Bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. The proposed approach is analyzed on a synthetic data set. We show that our model gives more accurate results compared to the grading approaches without different views and de-biasing algorithm.\nKeywords\u2014complex task; crowdsourcing; view; bias pattern; de-\nbias; Vancouver algorithm\nI. INTRODUCTION\nGrades and comments from instructors are very important to evaluate the level of the students\u2019 understanding and provide guidance for future studying process. Traditional ways of grading by instructors or teaching assistants are becoming a big challenge for large classes such as massive open online classes (MOOCs), which are distributed on platforms such as Udacity, Coursera and EdX [1], and the grading processes are really time consuming and tedious. As an example, there were around 100,000 students signed up for the online machine learning course offered on the Coursera platform. To evaluate students\u2019 work, a scalable way for grading is required for such large scale classes. Due to the restriction of un-scalability of the traditional way, grading through crowdsourcing has been developed as one of the prevalent ways to solve the problem.\nSince Amazon launched Mechanical Turk in 2005, crowdsourcing has become a powerful mechanism for completing large scale tasks which involve human intelligence computing. The tasks in MTurk range from labeling images with keywords to writing product descriptions. However, the types of tasks accomplished through MTurk have typically been limited to those that are low in complexity, independent, and require little time and cognitive effort to complete [15]. In contrast to\nthe typical tasks posted on MTurk, there are many tasks that are complex and need specialized skills to deal with in the real world. Students\u2019 essays or research papers are examples of complex tasks which are different from traditional MTurk\u2019s tasks.\nTo explore the use of crowdsourcing in complex task grading which is not decomposable into sub-tasks, consider for example the usual experience of giving course evaluations. The evaluations are given by students to reflect the success of the teachers\u2019 teaching skills for the whole semester. It actually is also a task of grading. Usually, every student in one class is asked to give course evaluation at the end of the semester. It includes questions such as: did the instructor explain concepts correctly; did the instructor provide clear constructive feedback; was the course effectively organized, etc. After each question, the evaluation scale is provided for the student to pick from. This evaluation process of instructors\u2019 success in teaching the course is a grading task through crowdsourcing, where students are used as a crowd. In order to give the evaluation for the course, it is necessary to provide grades considering the full semester, instead of decomposing the task into weekly or monthly based evaluation. Another issue arises during the evaluating process is that different students would grade based on their own perspectives and experiences without any guidance information. To ensure perspective uniformity, those instructive questions are provided, which are also called views, for the students grade. After getting grades for each view, an overall grade could be obtained by aggregating the view grades. Derived from this idea, we propose the approach of crowdsourcing grading process through different views for complex tasks such as evaluating programming projects or research papers.\nThe idea makes use of different views, which are diverse grading perspectives worked as instructive information to provide uniformity among graders. These different views are specified by experts before distributing the tasks to the graders for evaluation. We describe the graders used in the complex task evaluation as experienced domain grader, which means they have some knowledge about the domain of the assignment, but at the same time are different from experts in the field. Insufficient knowledge of the graders may lead to the existence of bias in the grading process regarding each view. By detecting the bias behavior of the graders, it is possible to de-bias them statistically, and significantly improve the quality of the grading process.\nThere are several concepts used in this work, and they are defined as follows:\nExperts \u2013 Those who have enough knowledge and ability to have best understanding of the complex task, e.g. instructors, who specified and assigned the complex assignment for students, are considered as experts.\nExperienced domain grader \u2013 Those who have some knowledge about the domain of the assignment, but at the same time are different from experts that they are not as knowledgeable and experienced in the field. For example, the students, who have taken and successfully finished the course, could be counted as an experienced domain grader for corresponding course assignment.\nComplex task \u2013 The task which cannot be simply decomposed into smaller sub-tasks, but instead is able to define different views for it. As an example, grading essays or computer science projects can be viewed as a complex task.\nObserved view grade \u2013 Observed view grade is the grade given by for every view by each grader without any processing.\nConsensus view grade \u2013 The grade we get for each view after applying aggregating algorithms or some other processing algorithms, which used to combine observed view grades from different graders for the same submission.\nOverall grade \u2013 The grade we get after combing all the view grades according to the instructions of the expert. It is the grade for every submission instead of each view.\nTrue grade \u2013 The grade which could reflects the true quality of the submission of the complex assignment. True grades are the grades given by experts.\nWe present in this paper the framework for crowdsourcing a grading process for complex tasks. Two main contributions of our approach are: extending the Vancouver algorithm [2], proposed by L. de Alfaro, and are applied for different views of complex tasks defined by an expert. Applying de-biasing process to discover biased graders, and the biases are removed from the final grades.\nThe paper is organized as follows: Section II gives the literature review of current methods used for grading by crowdsourcing. Section III describes the proposed Vancouverbased method with de-biasing. Section IV shows our experiment results. Section V presents comparison of the out-come of this work with other methodologies.\nII. PREVIOUS WORK"}, {"heading": "2.1 Literature Review", "text": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25]. As stated in [24], \u201cstudents are trained to be competent reviewers and are then given the responsibility of providing their classmates with personalized feedback on expository writing assignments\u201d. Most of the peer evaluation systems proposed nowadays are similar to this concept. In [1], three statistical models are developed for estimating true grades in peer grading. The models put prior distributions over latent variables such as true scores, grader\u2019s reliability and bias. Gibbs sampling [13] and expectation\nmaximization (EM) [6-12] is then used as approximate inference approach to estimate the true score. A system called CrowdGrader is developed in [2] to explore the use of peer feedback in grading assignments. It introduced a Vancouver algorithm (derived from [14]) which relies on a reputation system to aggregate the peer reviewing grades. [23] applied Bayesian data analysis to model a computer supported peer review process in a legal class. In [24], a web-based peer review program called Calibrated Peer Review (CPR) system is introduced to improve student learning. Peer reviewing experiments have shown promise, but it may also cause problems. As what Richard Smith proposed in his paper, \u201cthe practice of peer review is based on faith in its effects, rather on facts\u201d [3]. Suppose all the students in one class try to give good grades to each other, then with the absence of the true grades provided by the instructor or teaching assistant used for comparison, the consensus grades would be the submissions\u2019 grades. However, these grades actually cannot give correct estimation of the true grades.\nMany complex tasks, including complex grading tasks, are difficult to be done by crowdsourcing through MTurk due to its typical limitations. In [15] a framework was proposed to solve the problem by breaking down the complex task into a sequence of subtasks. Then subtasks are done through MTurk. This idea inspired by MapReduce [16] systems consists of three steps: partition, map and reduce. [17-19] presented the similar approaches to address the problem encountered while crowdsourcing the solutions to complex tasks. In [17], crowdsourcing is used as a novel approach to grade the math through Internet, by splitting the expert task to non-expert. Divide-and-conquer algorithm, which depicts the \u2018decompose, solve and recompose\u2019 structure, is proposed in [18] to solve general problem via crowdsourcing. In [19], a system called PlateMate is introduced to crowdsource nutritional analysis from photographs via MTurk. Foods in each image are identified and measured separately. Although crowdsourcing a complex task by decomposing it into subtasks offers tradeoff between cost and performance, the problem arises when the complex task is undecomposable. The framework is no longer available when the complex task should be solved as a whole.\nMostly, bias is unavoidable when collecting the data from crowdsourcing. Bias may be caused by personal preference, systematic misleading, and lack of interest [26]. Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28]. In [26], a Bayesian model, named as Bayesian Bias Mitigation for Crowdsourcing (BBMC), is proposed to capture the sources of bias. Authors in [27] introduced and evaluated probabilistic models that can detect and correct task-dependent biases in crowdsourcing automatically. In [1], statistical models have been presented to infer the graders\u2019 biases in peer reviewing process."}, {"heading": "2.2 Vancouver Algorithm", "text": "The Vancouver algorithm [2] measures each student\u2019s grading accuracy, by comparing the grades assigned by the student with the grades given to the same submission by other students in the crowd. It gives more weight to the input of students with higher measured accuracy.\nThe Vancouver algorithm is based on the fact of variance minimization principle. This principle said that we could weigh the input provided by student i in proportion to 1/vi, where vi is grader i\u2019s variance. The algorithm proceeds in iterative fashion, using consensus grades to estimate the grading variance of each user, and using the information on user variance to compute more precise consensus grades. The approach may be formalized as: We denote by U the set of students, and by S the set of items to be graded (the submissions). We let G = (T, E) be the graph encoding the review relation, where T = S \u222a U and S \u2229 U = \u2205, and where (i, j) \u2208 E iff j reviewed i; for (i, j) \u2208 E, we let gij be the grade assigned by j to i. We denote by \u2202t the 1-neighborhood of a node t \u2208 T.\nThe algorithm proceeds by updating estimates vj of the variance of user j \u2208 U, and estimates ci of the consensus grade of item i \u2208 S, and estimates vi of the variance with which ci is known. To produce these estimates, the algorithm relies on messages m = (l, x, v) consisting of a source l \u2208 S \u222a U, of a value x, and of a variance v. We denote by Mi, Mj the lists of messages associated with item i \u2208 T or user j \u2208 U. Given a set M of messages, we indicate by:\n\ud835\udc38(\ud835\udc40) = \u2211 \ud835\udc65/\ud835\udc63(\ud835\udc59,\ud835\udc65,\ud835\udc63)\u2208\ud835\udc40\n\u2211 1/\ud835\udc63(\ud835\udc59,\ud835\udc65,\ud835\udc63)\u2208\ud835\udc40 (1)\n\ud835\udc63\ud835\udc4e\ud835\udc5f(\ud835\udc40) = (\u2211 1\n\ud835\udc63(\ud835\udc59,\ud835\udc65,\ud835\udc63)\u2208\ud835\udc40 )\u22121 (2)\nThe best estimator \ud835\udc38(M) we can obtain from M, and its variance \ud835\udc63\ud835\udc4e\ud835\udc5f(M).\nIII. CROWDSOURCING THE GRADING PROCESS OF COMPLEX TASKS THROUGH DIFFERENT VIEWS\nThe Vancouver algorithm is extended in order to aggregate the observed view grades into consensus view grades. Section 3.1 describes the modified approach. Bias is analyzed for graders regarding each view, for the complex task here. Section 3.2 depicts how the bias is evaluated through bias patterns, and the way to de-bias the graders. Section 3.3 gives the details of the crowdsourcing process."}, {"heading": "3.1 Modified Vancouver Algorithm", "text": "After defining multiple views for the grading tasks, each view will be considered as one separate aggregation task, and the\nmodified Vancouver algorithm is applied to iteratively estimate the consensus grades for views. The extension of Vancouver algorithm is still based on the same principle of Vancouver algorithm, the main difference is that multiple views are taken into account.\nThe details of the modified Vancouver approach can be seen in Algorithm 1. S is the set of submissions need to be graded, and U is the set of graders. \ud835\udc40\ud835\udc56 , \ud835\udc40\ud835\udc57 are the lists of messages associated with submission \ud835\udc56 and grader \ud835\udc57. The set of views and observed view grades are used as the input for the algorithm, as well as the review graph. Instead of propagating only one variance as in basic Vancouver algorithm, different variances are considered for multiple views of each grader in the extended approach. After getting consensus view grades, it is required to combine the view grades into overall grade for each submission. The combining methods depends on the instructions of the experts: One way is just simply add the view grades up to get the overall consensus grade for the submission. Another way could be given the weights of each view of the assignment from the expert, aggregating the view grades in proportion to their weights. The extended Vancouver algorithm incorporates the consensus view grades combining steps as in lines 40-45 in Algorithm 1. Here, we assume each view is given a weight associated to it, thus the consensus grade for each submission can be obtained by adding up view grades times their weight."}, {"heading": "3.2 De-biasing", "text": "Bias patterns may be identified by analyzing the graders\u2019 fluctuation around true grade, for each view of the complex task. Bias is then reduced from each view grade for the biased graders with detected patterns.\nThe bias pattern is defined as: for all the submissions graded by the grader g, if most of his/her grades for view v is consistently higher (or lower) than the true grades of the view, then we say the g has a bias pattern on v for the assignment. There are two different types of bias pattern: a) most of the grader\u2019s scores of the view are higher than the true score, we call this pattern as \u201cpositive bias pattern\u201d, and b) graders\u2019 scores are lower than the true score, which we call \u201cnegative bias pattern\u201d. Figure 1 gives an example for each of these two bias patterns. X-axis represents different submissions graded by the grade (for one view) and y-axis is the score scale. For the specific grader,\n_______________________________________________________________\nAlgorithm 1 Modified Vancouver Algorithm\nInput: A review graph G = ((S \u222a U), E) such that |\u2202t| > 1 for all t \u2208 S \u222a U, along with {gij[view]}(i,j)\u2208E, and number of iterations K > 0, views set V and v\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 \ud835\udc49. Output: Estimates \ud835\u0302\udc5e\ud835\udc56[\ud835\udc63] for i \u2208 S. 1: {Initialization} 2: for all i \u2208 S do 3: for all view \u2208 V do 4: Mi[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] := {(j, \ud835\udc54\ud835\udc56\ud835\udc57[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] , 1) | (i, j) \u2208 E}. 5: end for 6: end for 7: for iteration k = 1, 2, . . . , K do 8: {Propagation from submissions} 9: for all j \u2208 U do 10: for all \ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 V do 11: Mj[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] := \u2205 12: end for 13: end for 14: for all i \u2208 S do 15: for all j \u2208 \u2202i do 16: for all \ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 V do 17: Let M\u2212j[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] = {(\ud835\udc57 ,, \ud835\udc65, \ud835\udc63) \u2208 Mi[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] | \ud835\udc57 , \u2260 \ud835\udc57} in Mj[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] := Mj[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] \u222a (i, E(M\u2212j[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] ), var(M\u2212j[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] )) 18: end for 19: end for 20: {Propagation from graders} 21: for all i \u2208 S do 22:\n22: for all \ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 V do 23: Mi[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] := \u2205 24: end for 25: end for 26: for all j \u2208 U do 27: for all i \u2208 \u2202j do 28: for all \ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 V do 29: Let M\u2212i[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] = {\ud835\udc56, , (\ud835\udc65 \u2212 \ud835\udc54\ud835\udc56 ,\ud835\udc57) 2 , \ud835\udc63) | (\ud835\udc56, , \ud835\udc65, \ud835\udc63) \u2208 Mj, \ud835\udc56,\u2260 \ud835\udc56 } in Mi := Mi \u222a (j, \ud835\udc54\ud835\udc56\ud835\udc57, E(M\u2212j )) 30: end for 31: end for 32: end for 33: end for 34: {Final Aggregation} 35: for all i \u2208 S do 36: for all \ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 V do 37: \ud835\u0302\udc5e\ud835\udc56[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] := E(Mi) 38: end for 39: end for 40: for all \ud835\udc56 \u2208 \ud835\udc46 do 41: \ud835\u0302\udc5e\ud835\udc56: = 0 42: for all \ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \u2208 \ud835\udc49 do 43: \ud835\u0302\udc5e\ud835\udc56: = \ud835\u0302\udc5e\ud835\udc56 + \ud835\u0302\udc5e\ud835\udc56[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] \u00d7 \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61[\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64] 44: end for 45: end for\nhis/her grades given by \u2018+\u2019 sign are consistently higher (or lower) than true grades represented by small red circles.\nWe propose the approach to detect and measure bias pattern as the percentage of values that lie within a width of two standard deviations around the mean of the differences of the true grades and observed grades. Below we present the detailed way how we recognize the bias pattern.\nSuppose there are n different submissions graded by grader g, for each of the views of the assignment v. True grades are given as T, where ti \u2208T is the true grade of submission i for this view. O is the set of observed grades from g, where oi \u2208 O is the\ngrade given by g to ith submission for v. Denote diff(ti, oi) as the difference for each pair of ti and oi. Calculate:\n\ud835\udf07 = 1\n\ud835\udc5b \u2211 \ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53(\ud835\udc61\ud835\udc56 , \ud835\udc5c\ud835\udc56)\n\ud835\udc5b \ud835\udc56=1 (3)\n\ud835\udf0e2 = \ud835\udc38((\ud835\udc42 \u2212 \ud835\udf07)2) (4)\n(\ud835\udf07 \u2212 2\ud835\udf0e, \ud835\udf07 + 2\ud835\udf0e) (5)\nInterval (5) represents the band around the mean of the differences of the true grades and observed grades with a width of two standard deviations. In statistics, we could make sure approximately 95% difference values would fall into the range as stated in (5). That means, if \ud835\udf07 \u2212 2\ud835\udf0e as calculated from equation (3) and (4) is greater than 0, then we could say 95% of the difference values (\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53(\ud835\udc61\ud835\udc56 , \ud835\udc5c\ud835\udc56)) are positive. In other words, 95% of the grader\u2019s grades for view v are higher than the true grades. We state that this grader has a positive bias pattern with 95% confidence for view v. Similarly, if the result of \ud835\udf07 + 2\ud835\udf0e is negative, we have 95% confidence that the grader has a negative bias pattern for view v. Grades from the graders with negative bias pattern or positive bias pattern are then de-biased to give a better performance before applying aggregating algorithm.\nThe way to de-bias the observed view grades provided by the biased graders varies on different occasions. The approaches for de-biasing might be subtracting: a) min(\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53(\ud835\udc61\ud835\udc56, \ud835\udc5c\ud835\udc56)); b) max(\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53(\ud835\udc61\ud835\udc56, \ud835\udc5c\ud835\udc56)); c) median of (\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53(\ud835\udc61\ud835\udc56, \ud835\udc5c\ud835\udc56)) or d) average of (\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53(\ud835\udc61\ud835\udc56, \ud835\udc5c\ud835\udc56)) from the observed view grades. The best method would be determined based on experimental heuristics."}, {"heading": "3.3 Grading Complex Task through Crowdsourcing", "text": "This research develops a crowdsourcing methodology for grading complex tasks which will result in consensus grades as close as possible to expert grades. By defining different views for the complex task by expert, the approach provides uniformity among the graders crowd. The framework of the approach is presented in Fig. 2 and explained as follows:\n1) Define different views for the complex task: Different\nviews are first defined for the complex task by experts. As an\nexample, while grading a research paper, the instructor may\ndefine views for the assignment such as a) comprehensiveness;\nb) enough quotes; c) examples, and inferences; d) technical\nstrength; e) data sets selection and f) presentation, etc. It is also\nrequired that an expert gives the scale for each view of the\nassignment such as 0 to 5 for background information, 0 to 10\nto technical strength, and 0 to 3 to data sets selection, etc.\n2) Distribute the submissions to graders: each submission\nshould be reviewed and graded by several graders, and each\ngrader is able to review and grade several submissions. The\nexperienced domain graders are assumed in the proposed\napproach. In addition, we assume that each grader must review\na minimum number of submissions to ensure that each\nsubmission would have enough reviews. The problem may\noccur in real world applications: if there are graders who do not\ndo their reviews, which will lead to insufficient number of\nreviews for some submissions. An online algorithm is proposed\nin [2] to solve the defect. The algorithm dynamically estimates\nthe probability that each review task will be completed, on the\nbasis of previous history, and assigns review tasks to achieve\nuniform coverage.\n3) Collect grades for all submissions: After reviewing each\nsubmission, graders need to evaluate and give grades for each\nview of the submission.\n4) Aggregate the grades: The modified Vancouver algorithm which implements a reputation system, which consists of\ngraders\u2019 variance and biases, for students is used in this research to aggregate the grades for each view from step 3. This algorithm\nensures that higher accuracy leads to higher reputation, and\ntherefore to higher influence on the consensus grades [2].\nEach view is considered as one separate aggregation task, and the Vancouver algorithm is applied to iteratively estimate the consensus grade for these views.\n5) De-bias each graders\u2019 grade for different views: Bias\npatterns are first identified for each view of every grader using\nthe approach presented in section 3.1. True grades provided by\nexperts are used to compare with the grades given by the\ngraders. The bias patterns are detected from comparisons. Bias\nis reduced from the view grades afterwards by subtracting\nmin(diff(ti, oi)) from the observed grades for a given biased grader. After de-biasing the view grades, the aggregation\nalgorithm in step 4 is applied again on the de-biased data set to\nget the consensus grades for each view of the submission.\n6) Final overall grades for each submission: After applying\nthe aggregation algorithm, each view of every submission will\nfinally get its own consensus grade. The overall grade for each\nsubmission is obtained through summation of the view grades.\nIV. EXPERIMENTAL ANALYSIS"}, {"heading": "4.1 Data Set", "text": "The method is evaluated on synthetic data. In order to compare the proposed approach with the methodology applied in [2] called Vancouver, we used the same synthetic data set. 50 graders and 50 submissions were considered, where each grader was reviewing 6 submissions. To give better understanding of the process, we define 2 different views for the assignment. For each view, we assumed: the true quality qi of each item i has normal distribution with standard deviation 1. Each grader j had a characteristic variance vj, and we let the grade qij assigned by j to i be equal to qi + \u2206ij , where qi is the true quality, and \u2206ij has normal distribution with mean 0 and variance vj . We assumed that the variances {vj}j\u2208U of the graders were distributed according to a Gamma distribution with scale 0.4, and shape factors k = 2, 3."}, {"heading": "4.2 Models", "text": "Three different models are analyzed, and they are denoted as: AVG, DM1 and DM2.\nAVG model represents the average model. It just simply averages the grades for each submission received to get the consensus grade. It acts as a baseline model.\nDM1 is the model with different views defined by an expert, but without the step of bias pattern recognition. Thus, there is no de-biasing applied for the observed grades. DM1 is basically Vancouver method in [2], but with extension to different views.\nDM2 model uses both different views of assignments and de-biasing the graders. Different views are defined for the complex assignment before getting the grades from graders. After getting the observed grades, bias patterns are recognized, de-biasing is applied, and finally overall consensus grades are obtained."}, {"heading": "4.3 Evaluation Metrics", "text": "Our goal is to get consensus grades as close as possible to the true grades given by an expert. As a result, coefficient correlation (\u03c1), standard deviation (\ud835\udf0e), and root mean square error (RMSE) are calculated between true grades and consensus grades as the evaluation metrics. The metrics are calculated as follows:\n\ud835\udf0c\ud835\udc4c,\ud835\u0302\udc4c = \ud835\udc36\ud835\udc42\ud835\udc49(\ud835\udc4c,\ud835\u0302\udc4c )\n\ud835\udf0e\ud835\udc4c\ud835\udf0e\ud835\u0302\udc4c (6)\n\ud835\udc36\ud835\udc42\ud835\udc49(\ud835\udc4c, \ud835\u0302\udc4c) = \ud835\udc38((\ud835\udc4c \u2212 \ud835\udf07\ud835\udc4c)(\ud835\u0302\udc4c \u2212 \ud835\udf07\ud835\u0302\udc4c)) (7)\nwhere \ud835\udf0c\ud835\udc4c,\ud835\u0302\udc4c is the coefficient correlation of random variables \ud835\udc4c(true grades) and \ud835\u0302\udc4c (consensus grades), \ud835\udf0e\ud835\udc4c, \ud835\udf0e\ud835\u0302\udc4c are the standard deviation of \ud835\udc4c and \ud835\u0302\udc4c, \ud835\udf07\ud835\udc4c, \ud835\udf07\ud835\u0302\udc4c are the mean of \ud835\udc4c and \ud835\u0302\udc4c, \ud835\udc36\ud835\udc42\ud835\udc49(\ud835\udc4c, \ud835\u0302\udc4c)\nis the covariance of \ud835\udc4c and \ud835\u0302\udc4c.\n\ud835\udc45\ud835\udc40\ud835\udc46\ud835\udc38 = \u221a \u2211 (\ud835\u0302\udc66\ud835\udc61\u2212\ud835\udc66)\n2\ud835\udc5b \ud835\udc61=1\n\ud835\udc5b (8)\nwhere \ud835\u0302\udc66\ud835\udc61 is the estimated value for true grade and \ud835\udc66 is the true value."}, {"heading": "4.4 Experimental Results", "text": "The synthetic data is simulated through 100 runs and the coefficient correlation, standard deviation and RMSE are then reported as the average over these 100 runs. Figure 3 shows the evaluation metrics calculated for the view1 of first 20 simulation runs, with shape factor k = 2 of the Gamma distribution, for the three models. To present the difference between DM1 and DM2, we show a more detailed figure in Fig. 4. The result of view2 is quite similar to view1. The results are summarized in Table 1. Specifically, the overall outcome is calculated from aggregation the 100 runs instead of averaging the results from the final views\u2019 outcome.\nIn order to better understand the influence of the bias patterns to the results, we run the following experiment: Increasing the percentage of biased graders (graders who have bias patterns) for each of the view. We increase the number of biased graders for view 1 from 9 to 24, and view 2 from 12 to 28. DM1 and DM2 model are applied to get the consensus grades. We then calculate the metrics for evaluation. Table 2 shows the results."}, {"heading": "4.5 Detailed Experimental Results Analysis and Discussion", "text": "As the results show in Table 1, all three metrics we selected are improved for each view after applying DM1 compared to baseline \u2013 AVG model. Thus, the overall results of DM1 model improved 20.3% in contrast to AVG model for correlation when k=2. After de-biasing, which means applying DM2, there is further improvement. As an example, if standards deviation has 70% improvement comparing DM1 with AVG, and 10% improvement comparing DM2 with DM1, then 87% improvement is given comparing DM2 with AVG model. In\nconclusion, DM2 model gave the best performance for all three evaluation metrics comparing with AVG baseline model and DM1.\nFigure 5 and 6 present the percentage of improvement of the results comparing the three different models, which is calculated from Table 1. Figure 5(a) and 6(a) show the enhancement obtained for all three metrics from comparing DM1 model with baseline AVG model, for shape factor equals to 2 and 3 respectively. Figure 5(b) and 6(b) give percentage of improvement of the accuracy after comparing DM2 with DM1 (for k = 2, 3). It could be seen clearly in figure 5(a) and 6(a) that, no matter for each view or overall grades, significant improvement can be obtained by applying DM1 in contrast with AVG model. However, further improvement on the accuracy is able to achieve by using DM2 comparing with DM1. Thus, DM2 gives the best performance.\nBy defining different views and taking into account graders\u2019 bias pattern for each view, we could provide significant gains in accuracy. To prove the importance of de-biasing the graders, we increase the number of biased graders per view in our data set. Table 2 shows the results of three selected metrics results and percentage of improvement comparing DM2 (with de-biasing) with DM1 (without de-biasing). As results presented, the more biased graders, the better improvement we could get by using the DM2 model. The reason is that more graders with a bias pattern means less of them make random error, and as a result, more accurate grades could be obtained after de-biasing the graders.\nWe compared our method with the Vancouver algorithm in [2] (Alfaro and Shavlovsky, 2014). The Vancouver algorithm is essentially DM1 model, except for that no different views are defined. Thus, the overall results in Table 1 is the approximation of the outcome of Vancouver algorithm. The improvement of metrics of overall grades in Figure 5(b) and 6(b) presents the enhancement on accuracy comparing DM2 model with Vancouver algorithm. Our results show that DM2 yields the\nbetter results.\nIn research [1], by developing the algorithm for estimating the true grades through statistical models, the authors were able to reduce the RMSE error on their prediction of ground truth by 31% to 33% comparing to baseline model. However, our experimental results show that we are able to reduce RMSE by 67% from 1.820 to 0.605 on overall grades by applying the proposed framework.\nV. CONCLUSIONS AND FUTURE WORKS\nIn this paper, we presented a novel framework for grading un-decomposable, complex tasks through crowdsourcing. The key innovations include: 1) Extending the Vancouver algorithm to multiple views. 2) De-bias the graders for each view of the task. By defining multiple views for the complex task, uniformity is ensured among the graders. Bias pattern is then detected for each view of the grader, and then we de-bias them in order to improve the grading accuracy. Experimental results indicate that our model DM2 outperform the baseline AVG model and DM1 which doesn\u2019t include de-bias process.\nTo further justify and improve the proposed model, real world complex problems will be formulated, collected, and graded by experienced domain graders. Grading tasks in this work is performed in batch mode. However, a time domain model might reveal the relevance and influences between previous and future grading processes. For example, graders\u2019 bias pattern might be reused if some of the graders are persistent for different grading tasks. In addition, the expert background of the graders could also be taken into account while weighting the observed grades for consensus grades. For instance, while\nincorporating computer science students as graders for grading Java programming assignments, graders who have experience on Java coding are probably more reliable than those who do not. Finally, instead of randomly choosing graders for tasks in this work, we plan to come up with algorithms to take into account the selection of graders with higher accuracy and reliability, as well as optimization of the number of graders for specific complex tasks."}], "references": [{"title": "Tuned models of peer assessment in MOOCs", "author": ["C. Piech", "J. Huang", "Z. Chen", "C. Do", "A. Ng", "D. Koller"], "venue": "ArXiv preprint arXiv:1307-2579,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "CrowdGrader: A tool for crowd-sourcing the evaluation of homework assignments", "author": ["L. Alfaro", "M. Shavlovsky"], "venue": "Proceedings of the 45th ACM technical symposium on Computer science education:", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Peer review: A flawed process at the heart of the science and journals", "author": ["S. Smith"], "venue": "J R Soc Med. Apr;", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Dear colleague letter: Information to principal investigators (pis) planning to submit proposals to the sensors and sensing systems (sss) program", "author": ["N.S. Foundation"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Telescope time without tears: A distributed approach to peer review", "author": ["M. Merrifield", "D. Saari"], "venue": "Astronomy & Geophysics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Maximum likelihood estimation of observer errorrates using the EM algorithm", "author": ["A. Dawid", "A. Skene"], "venue": "Applied Statistics:", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1979}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological):", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Inferring ground truth from subjective labelling of venus images", "author": ["P. Smyth", "U. Fayyad", "M. Burl", "P. Perona", "P. Baldi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Learning with multiple labels", "author": ["R. Jin", "Z. Ghahramani"], "venue": "In Advances in neural information processing systems:", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in neural information processing", "author": ["J. Whitehill", "T.-F. Wu", "J. Bergsma", "J. Movellan", "P. Ruvolo"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning from crowds", "author": ["V. Raykar", "S. Yu", "L. Zhao", "G. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE PAMI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1984}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D. Karger", "S. Oh", "D. Shah"], "venue": "In Proc. of the 25th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "CrowdForge: Crowdsourcing complex work", "author": ["A. Kittur", "B. Smus", "S. Khamkar", "R.E. Kraut"], "venue": "24th annual ACM symposium on User interface software and technology:", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Map Reduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Crowdsourcing general computation", "author": ["H. Zhang", "E. Horvitz", "R.C. Miller", "D.C. Parkes"], "venue": "Proceedings of ACM CHI Conference on Human Factors,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Platemate: crowdsourcing nutritional analysis from food photographs", "author": ["J. Noronha", "E. Hysen", "H. Zhang", "K.Z. Gajos"], "venue": "Proc. UIST", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Some Thoughts on Grading Systems and Grading Practices", "author": ["J. Terwilliger"], "venue": "Teacher Training in Measurement and Assessment Skills:", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Grades as valid measures of academic achievement of classroom learning. Clearinghouse: A Journal of Educational Strategies", "author": ["J. Ellen"], "venue": "Issues and Ideas,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Toward ai-enhanced computer-supported peer review in legal education", "author": ["K. Ashley", "I. Goldin"], "venue": "In 24th International Conference on Legal Knowledge and Information Systems (JURIX),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Calibrated peer review - a writing and critical-thinking instructional tool", "author": ["A.A. Russell"], "venue": "Teaching Tips: Innovations in Undergraduate Science Instruction,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "The impact of self-and peer-grading on student learning", "author": ["P.M. Sadler", "E. Good"], "venue": "Educational assessment,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Bayesian bias mitigation for crowdsourcing", "author": ["Wauthier", "M. Jordan"], "venue": "In In Proc. of NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Identifying and accounting for taskdependent bias in crowdsourcing", "author": ["E. Kamar", "A. Kapoor", "E. Horvitz"], "venue": "Third AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Leveraging in-batch annotation bias for crowdsourced active learning", "author": ["H. Zhuang", "J. Young"], "venue": "Proc. ACM WSDM: 243\u2013252. Shanghai,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Traditional ways of grading by instructors or teaching assistants are becoming a big challenge for large classes such as massive open online classes (MOOCs), which are distributed on platforms such as Udacity, Coursera and EdX [1], and the grading processes are really time consuming and tedious.", "startOffset": 227, "endOffset": 230}, {"referenceID": 14, "context": "However, the types of tasks accomplished through MTurk have typically been limited to those that are low in complexity, independent, and require little time and cognitive effort to complete [15].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "Two main contributions of our approach are: extending the Vancouver algorithm [2], proposed by L.", "startOffset": 78, "endOffset": 81}, {"referenceID": 18, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 68, "endOffset": 75}, {"referenceID": 19, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 68, "endOffset": 75}, {"referenceID": 0, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 221, "endOffset": 226}, {"referenceID": 1, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 221, "endOffset": 226}, {"referenceID": 20, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 226, "endOffset": 233}, {"referenceID": 21, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 226, "endOffset": 233}, {"referenceID": 22, "context": "To relieve the grading burden of the experts in the traditional way [20-22], especially with the popularity of open online classes such as MOOCs, peer reviewing has been proposed as a new way to address the grading tasks [1-2][23-25].", "startOffset": 226, "endOffset": 233}, {"referenceID": 21, "context": "As stated in [24], \u201cstudents are trained to be competent reviewers and are then given the responsibility of providing their classmates with personalized feedback on expository writing assignments\u201d.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "In [1], three statistical models are developed for estimating true grades in peer grading.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 6, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 7, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 9, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 10, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 11, "context": "Gibbs sampling [13] and expectation maximization (EM) [6-12] is then used as approximate inference approach to estimate the true score.", "startOffset": 54, "endOffset": 60}, {"referenceID": 1, "context": "A system called CrowdGrader is developed in [2] to explore the use of peer feedback in grading assignments.", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "It introduced a Vancouver algorithm (derived from [14]) which relies on a reputation system to aggregate the peer reviewing grades.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "[23] applied Bayesian data analysis to model a computer supported peer review process in a legal class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In [24], a web-based peer review program called Calibrated Peer Review (CPR) system is introduced to improve student learning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "As what Richard Smith proposed in his paper, \u201cthe practice of peer review is based on faith in its effects, rather on facts\u201d [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "In [15] a framework was proposed to solve the problem by breaking down the complex task into a sequence of subtasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "This idea inspired by MapReduce [16] systems consists of three steps: partition, map and reduce.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "[17-19] presented the similar approaches to address the problem encountered while crowdsourcing the solutions to complex tasks.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[17-19] presented the similar approaches to address the problem encountered while crowdsourcing the solutions to complex tasks.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "Divide-and-conquer algorithm, which depicts the \u2018decompose, solve and recompose\u2019 structure, is proposed in [18] to solve general problem via crowdsourcing.", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "In [19], a system called PlateMate is introduced to crowdsource nutritional analysis from photographs via MTurk.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Bias may be caused by personal preference, systematic misleading, and lack of interest [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28].", "startOffset": 108, "endOffset": 115}, {"referenceID": 24, "context": "Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28].", "startOffset": 108, "endOffset": 115}, {"referenceID": 25, "context": "Many researches have included biases analysis in crowdsourcing models to improve the approximation accuracy [26-28].", "startOffset": 108, "endOffset": 115}, {"referenceID": 23, "context": "In [26], a Bayesian model, named as Bayesian Bias Mitigation for Crowdsourcing (BBMC), is proposed to capture the sources of bias.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Authors in [27] introduced and evaluated probabilistic models that can detect and correct task-dependent biases in crowdsourcing automatically.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "In [1], statistical models have been presented to infer the graders\u2019 biases in peer reviewing process.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "The Vancouver algorithm [2] measures each student\u2019s grading accuracy, by comparing the grades assigned by the student with the grades given to the same submission by other students in the crowd.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "An online algorithm is proposed in [2] to solve the defect.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "This algorithm ensures that higher accuracy leads to higher reputation, and therefore to higher influence on the consensus grades [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "In order to compare the proposed approach with the methodology applied in [2] called Vancouver, we used the same synthetic data set.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "DM1 is basically Vancouver method in [2], but with extension to different views.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "We compared our method with the Vancouver algorithm in [2] (Alfaro and Shavlovsky, 2014).", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "In research [1], by developing the algorithm for estimating the true grades through statistical models, the authors were able to reduce the RMSE error on their prediction of ground truth by 31% to 33% comparing to baseline model.", "startOffset": 12, "endOffset": 15}], "year": 2016, "abstractText": "With the popularity of massive open online courses (MOOCs), grading through crowdsourcing has become a prevalent approach towards large scale classes. However, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters a restriction of insufficient knowledge of the workers from the crowd. Due to knowledge limitation of the crowd graders, grading based on partial perspectives becomes a big challenge for evaluating complex tasks through crowdsourcing. Especially for those tasks which not only need specific knowledge for grading, but also should be graded as a whole instead of being decomposed into smaller and simpler sub-tasks. We propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. Aggregation algorithm based on graders\u2019 variances are used to combine the grades for each view. We also detect bias patterns of the graders, and de-bias them regarding each view of the task. Bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. The proposed approach is analyzed on a synthetic data set. We show that our model gives more accurate results compared to the grading approaches without different views and de-biasing algorithm. Keywords\u2014complex task; crowdsourcing; view; bias pattern; debias; Vancouver algorithm", "creator": "Microsoft\u00ae Word 2013"}}}