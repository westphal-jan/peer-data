{"id": "1704.00103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "abstract": "we describe a common method to produce a standardized network where current methods such as deepfool have great difficulty producing adversarial samples. our construction necessarily suggests some insights into how deep networks work. we provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that allowing our method is hard to defeat using several non standard networks and datasets. we use our method to produce a system that can independently reliably detect whether an image is telling a picture of a real scene or not. our system applies to images accurately captured with depth level maps ( rgbd images ) and checks if a pair of image and depth map is consistent. yet it relies on the relative difficulty rate of producing naturalistic virtual depth maps for arbitrary images in video post processing. we demonstrate that providing our system is robust to adversarial examples built from currently known attacking filtering approaches.", "histories": [["v1", "Sat, 1 Apr 2017 02:12:40 GMT  (3783kb,D)", "http://arxiv.org/abs/1704.00103v1", null], ["v2", "Tue, 15 Aug 2017 05:59:38 GMT  (3740kb,D)", "http://arxiv.org/abs/1704.00103v2", "Accepted to ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jiajun lu", "theerasit issaranon", "david forsyth"], "accepted": false, "id": "1704.00103"}, "pdf": {"name": "1704.00103.pdf", "metadata": {"source": "CRF", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "authors": ["Jiajun Lu", "Theerasit Issaranon", "David Forsyth"], "emails": ["daf}@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "Adversarial examples are images with tiny, imperceptible perturbations that fool a classifier into predicting the wrong labels with high confidence. x denotes the input to some classifier, which is a natural example and has label l. A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l. This is interesting, because ||a(x)\u2212 x ||2 is so small that we would expect a(x) to be labelled l.\nAdversarial examples are a persistent problem of classification neural networks, and of many other classification schemes. Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18]. Adversarial examples are important for practical reasons, because one can construct physical adversarial examples, suggesting that neural networks in current status are unusable in some image classification applications (e.g. imagine a small physical modification that could reliably get a stop sign classified as a go faster sign [24]). Adversarial examples are important for conceptual reasons too, because an explanation of why adversarial examples are easy to construct could cast some light on the inner life\nof neural networks. The absence of theory means it is hard to defend against adversarial examples (for example, distillation was proposed as a defense [25], but was later shown to not work [2]).\nAdversarial example constructions (e.g., line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks. An important puzzle is that networks that generalize very well remain susceptible to adversarial examples [29]. Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26]. Some network architectures appear to be robust to adversarial examples [13], which still need more empirical verification. At least some adversarial attacks appear to apply to many distinct networks [18].\nWe denote the probability distribution of examples by P (X). At least in the case of vision, P (X) has support on some complicated subset of the input space, which is known\n1\nar X\niv :1\n70 4.\n00 10\n3v 1\n[ cs\n.C V\n] 1\nA pr\n2 01\n7\nas the \u201cmanifold\u201d of \u201creal images\u201d. Nguyen et al. show how to construct examples that appear to be noise, but are confidently classified as objects [22]. This construction yields a(x) lies outside the support of P (X), so the classifier\u2019s labeling is unreliable because it has not seen such examples. However, most adversarial examples \u201clook like\u201d images to humans, such as figure 5 in [29], so they are likely to lie within the support of P (X).\nOne way to build a network that is robust to adversarial examples is to train networks with enhanced training data (adding adversarial samples [17]); this approach faces difficulties, because the dimension of the images and features in networks means an unreasonable quantity of training data is required. Alternatively, we can build a network that detects and rejects an adversarial sample. Metzen et al. show that, by attaching a detection subnetwork that observes the state of the original classification network, one can tell whether it has been presented with an adversarial example or not [16]. However, because the gradients of their detection subnetwork are quite well behaved, the joint system can be attacked easily (in our experiments; [16] does not provide any analysis for attacking on both detectors and classifiers). We also show their detection subnetwork is also easily fooled by adversarial samples produced by attacking methods which are not used in detector training process.\nOur method focuses on codes produced by quantizing individual ReLUs in particular layers of the classification network (\u201cpatterns of activation\u201d), and proceed from the hypothesis:\nHypothesis 1 Adversarial attacks work by producing different patterns of activation in late stage ReLUs to those produced by natural examples.\nThese patterns lie outside the family for which the softmax layer would be reliable. This hypothesis suggests that: (a) the presence of an adversarial example can be detected (as in Metzen et al. [16]); (b) such detectors can be made very difficult to defeat (unlike Metzen et al. [16]; section 3); (c). such detectors should be good at generalization for different adversarial attacks (unlike Metzen et al. [16]); (d) transfer attacks work because an example that generates unfamiliar patterns in one network tends to generate unfamiliar patterns in other networks too; (e) transfer attacks could be defended as well (section 3).\nContributions: Section 2 describes our SafetyNet architecture, which consists of the original classifier network and a detector that rejects adversarial examples. A type I attack on SafetyNet consists of a standard adversarial example crafted to be (a) similar to a natural image; (b) misclassified by the original network. A type II attack consists of an example that is crafted to be (a) similar to a natural image; (b) misclassified; and (c) not rejected by SafetyNet.\nWe show that SafetyNet is robust to both types of attacks and generalize well. Concealing the gradients is highly effective for SafetyNet, and it produces a black box that is strongly resistant to the best attacks we have been able to construct. This is in sharp contrast to all other known methods [24, 16].\nIn section 3, we demonstrate SceneProof, a robust and reasonably effective proof that an image is an image of a real scene (a \u201creal\u201d image; contrast a \u201cfake\u201d image, which is not an image of a real scene). We identify images of real scenes by checking a match between the image and a depth map, which is hard to manipulate. We show that SceneProof is (a) accurate and (b) strongly resistant to attacks that try to get manipulated scenes identified as authentic scenes.\nIn section 4, we propose a model that explains why our approach works, and it also demonstrates that SafetyNet is difficult to attack in principle."}, {"heading": "2. SafetyNet: Spotting Adversarial Examples", "text": "SafetyNet consists of the original classifier, and an adversary detector which looks at the internal state of the later layers in the original classifier, as in Figure 1. If the adversary detector declares that an example is adversarial, then the sample is rejected."}, {"heading": "2.1. Detecting Adversarial Examples", "text": "The adversary detector needs to be hard to attack. We force an attacker to solve a hard discrete optimization problem. For a layer of ReLUs at a high level in the classification network, we quantize each ReLU at some set of thresholds to generate a discrete code (binarized code in the case of one threshold). Our hypothesis 1 suggests that different code patterns appear for natural examples and adversarial examples. We use an adversary detector that compares a code produced at test time with a collection of examples, meaning that an attacker must make the network produce a code that is acceptable to the detector (which is hard; section 3). The adversary detector in SafetyNet uses an RBF-SVM on binary or ternary codes (activation patterns) to find adversarial examples.\nWe denote a code by c. The RBF-SVM classifies by\nf(c) = N\u2211 i \u03b1iyi exp(\u2212||c\u2212 ci||2/2\u03c32) + b (1)\nIn this objective function, when \u03c3 is small, the detector produces essentially no gradient unless the attacking code c is very close to a positive example ci. Our quantization process makes the detector more robust and the gradients even harder to get. Experiments show that this form of gradient obfuscation is quite robust, and that confusing the detector is very difficult without access to the RBF-SVM, and still\ndifficult even when access is possible. Experiments in section 3 and theory in section 4 confirms that the optimization problem is hard."}, {"heading": "2.2. Attacking Methods", "text": "We use the following standard and strong attacks [2], with various choice of hyper-parameters, to test the robustness of the systems. Each attack searches for a nearby a(x) which changes the class of the example and does not create visual artifacts. We use these methods to produce both type I attack (fool the classifier) and type II attack (fool the classifier and sneak past the detector).\nFast Sign method: Goodfellow et al [9] described this simple method. The applied perturbation is the direction in image space which yields the highest increase of the linearized cost under l\u221e norm. It uses a hyper-parameter to govern the distance between adversarial and original image.\nIterative methods: Kurakin et al. [14] introduced an iteration version of the fast sign method, by applying it several times with a smaller step size \u03b1 and clipping all pixels after each iteration to ensure that results stay in the neighborhood of the original image. We apply two versions of this method, one where the neighborhood is in L\u221e norm and another where it is in L2 norm.\nDeepFool method: Moosavi-Dezfooli et al. [19] introduced the DeepFool adversary, which is able to choose which class an example is switched to. DeepFool iteratively perturbs an image xadv0 , linearizes the classifier around x adv n and finds the closest class boundary. The minimal step according to the lp distance from xadvn to traverse this class boundary is determined and the resulting point is used as xadvn+1. The algorithm stops once x adv n+1 changes the class of the actual classifier. We use a powerful L2 version of DeepFool.\nTransfer method: Papernot et al. [24] described a way to attack a black-box network. They generated adversarial samples using another accessible network, which performs the same task, and used these adversarial samples to attack the black-box network. This strategy has been notably reliable."}, {"heading": "2.3. Type I Attacks Are Detected", "text": "Accuracy: Our SafetyNet can detect adversarial samples with high accuracy on CIFAR-10 [12] and ImageNet1000 [4]. For classification networks, we used a 32-layer ResNet [10] for CIFAR-10 and a VGG19 network [28] for ImageNet-1000. Figures 2 shows the detection accuracy of our Binarized RBF-SVM detector on the x5 layer of ResNet for Cifar10 and on the fc7 layer of VGG19 trained for ImageNet-1000. Adversarial samples are generated by Iterative-L2, Iterative-Linf, DeepFool-L2 and FastSign methods. Figure 2 compares our RBF-SVM detection results with the detector subnetwork results of [16]. The RoC\nfor our detector for Cifar-10 and ImageNet-1000 appears in Figure 3.\nOur results show: When our detector is tested on the same adversary as it is trained on, its performance is similar to the detector subnetwork, even though our detector works on quantized activation patterns while the detector subnetwork works on original continuous activation patterns. DeepFool is a strong attack. Increasing the number of categories in the problem makes it easier for DeepFool to produce an undetected adversarial example, likely because it becomes easier to exploit local classification errors without producing strange ReLU activations. If DeepFool is required to produce a label outside the top-5 for the original example, the attack is much weaker.\nGeneralization across attacks: Generally, a detector cannot know at training time what attacks will occur at test time. We test generalization across attacks by training a detector on one class of attack, then testing with other classes of attack. Figure 2 shows that our RBF-SVM generalizes across attacks more reliably than a detector subnetwork. We believe this is because the representation presented to the RBF-SVM has been aggressively summarized (by quantization), so that the classifier is not distracted by subtle but irrelevant features. Another example that questions the generalization ability of neural networks is the JPEG example in Table 7. Networks trained on normal quality JPEG images fails significantly on low quality JPEG test images."}, {"heading": "2.4. Rejecting by Classification Confidence", "text": "Our experiments demonstrate that there is a trade-off between classification confidence and detection easiness for adversarial examples. Adversarial examples with high confidence in wrong classification labels tend to have more abnormal activation patterns, so they are easier to be detected by detectors. While adversarial examples with low classification confidence in wrong labels are harder to be detected. For example, attacks like DeepFool add small and just enough perturbations to change the classification label, so these adversarial examples are sometimes hard to detect. However, these adversarial examples could not assign high classification confidence to the wrong label. If they perform more iterations and increase the wrong class classification confidence, our detector could detect them much easier.\nExperiments also show that Type II attacks on our quantized SVM detector together with the classifier produce adversarial examples with low confidence. All these experiments mean that we can use classification confidence as a detection criteria, and it could help us increase the detector\u2019s detection ability and decrease the potential to be attacked by Type II attacks.\nThe classification confidence in our experiments is measured by the ratio of the example\u2019s second highest classification confidence to the highest classification confidence. For\nexample, if an image has 60% probability to be a dog and 15% probability to be a cat, our classification confidence is 0.25. We reject examples with classification confidence ratio bigger than a threshold, which means the classifier is unsure about the classification.\nThe classification confidence rejection results for non attack images and various Type II attack adversarial examples are included in Table 2 for Cifar-10 and Table 3 for ImageNet-1000. Both tables show that rejecting by classification confidence rejects few non attack images while\nhugely increase the rejection of Type II attack adversarial examples. The benefits of rejecting by classification confidence is also demonstrated in the Type II attacks section."}, {"heading": "2.5. Type II Attacks fail", "text": "A type II attack involves a search for an adversarial example that will be (a) mislabelled and (b) not detected. We perform the gradient descent based Type II attacks for Cifar10 and ImageNet-1000 with SVM detector, and compare to detection subnetwork [16]. Metzen et al. [16] only investigated type I attacks and has not investigated type II attacks on the detection subnetwork. Because the gradients of detection subnetwork are better formed, it should be easier to attack with Type II gradient descent attacks.\nIn our experiments for Cifar-10 and ImageNet-1000, we use different gradient descent based Type II attacks (L0, L2, Fast, DeepFool and top-5 DeepFool) to attack the detector and classifier at the same time. In the later SceneProof part, gradient descent based Type II attacks on SceneProof dataset use L2 LBFGS method.\nThe summary for Type II attacks on Cifar-10 could be found in Table 4. The numbers reported in the table are the percentages of adversarial examples that are both misclassified and undetected (lower is better). Without classification confidence rejection, quantized SVM detector and detection subnetwork perform similar under Type II attacks for L0, L2 and Fast methods, and quantized SVM detector performs significantly better under DeepFool Type II attacks. With classification confidence rejection, quantized SVM detector is very hard to attack and performs better than detection subnetwork on almost all attacking methods. The classification confidence rejection increases at maximum 7% false rejection on non attack images. The detailed percentages of Type II attacks on Cifar-10 could be found in Table 10.\nThe summary for Type II attacks on ImageNet-1000 could be found in Table 5.The table arrangement is same to Table 4, and DeepFool5 is top-5 DeepFool attack. Quantized SVM detector consistently performs better than detection subnetwork for various attacking methods and for\nboth with classification confidence rejection and without. It\u2019s very difficult to perform Type II attacks on quantized SVM detector with rejection. The classification confidence rejection increases at maximum 10% false rejection on non attack images. The detailed percentages of Type II attacks on ImageNet-1000 could be found in Table 11."}, {"heading": "3. SceneProof", "text": "We would like Alice to be able to prove to Bob that her photo is real without the intervention of a team of experts, and we\u2019d like Bob to have high confidence in the proof. This proof needs to operate at large scales (i.e. anyone could produce a proof while taking a picture), and automatically.\nCurrent best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]). Such analyses are difficult to conduct at large scales or automatically. We construct a proof by capturing an RGBD image. The proof of realness is achieved by a classifier that checks both image and depth and determines whether they are consistent. Such a system works if (a) the classifier is acceptably\naccurate (i.e. it can determine whether the pair is real or not accurately); (b) it can detect a variety of adversarial manipulations of depth or image or both (i.e. type I attacks fail) ; and (c) type II attacks generally fail. We achieve this by using the SafetyNet architecture.\nWe are mainly concerned with attacks that make \u201cfake\u201d images to be labeled \u201creal\u201d. Natural attacks on our system are: produce a depth map for an RGB image using some regression method to obtain an RGBD image (regression); manipulate RGBD image by inserting new objects; take an RGBD image labeled \u201cfake\u201d and manipulate it to be labeled \u201creal\u201d (type I adversarial); take an RGBD image labeled \u201cfake\u201d and manipulate it to be labeled \u201creal\u201d in a way that fools SafetyNet\u2019s adversary detector (type II adversarial). There is a wide range of available regression/adversarial attacks, and our system needs to be robust to various methods which might be used to prepare the regression/adversarial attack.\nReal test data is easily obtained. We use the raw Kinect captures of LivingRoom and Bedroom from NYU v2 dataset [20]. However, fake data requires care. To evalu-\nate generalization over different attacks, we omit some \u201cregression\u201d methods from the training data and use them only in test. \u201cRegression\u201d methods used in both train and test are: random swaps of depth and image planes; single image predicted depth [5]; rectangle cropped region insertion and random shifted or scaled misaligned depth and image. \u201cRegression\u201d methods used only in test are: all zero depth values; nearest neighbor down-sample and up-sampled images and depths; low quality JPEG compressed images and depths; Middlebury stereo RGBD dataset [27] and Sintel RGBD dataset [1](which should be classified \u201cfake\u201d because they are renderings). Refer to Figure 4 for dataset and attacks.\nType I attacks on SafetyNet fail: Type I attacks on SceneProof using a familiar adversary (i.e. one used to train the detector) fail. We report results for two detectors A (applied to fc7 of VGG19) and B (applied to fc6 of VGG19) in Table 6. Type I attacks on SceneProof using an unfamiliar adversary (i.e. one not used to train the detector) generally fail. We report results for two detectors A (applied to fc7 of VGG19) and B (applied to fc6 of VGG19) in Table 7.\nA type II attack must both fool the classifier and sneak past the detector. We distinguish between two conditions. In non-blackbox case, the internals of the SafetyNet system is accessible to the attacker. Alternatively, the network may be a black box, with internal states and gradients concealed. In this case, attackers must probe with inputs and gather\noutputs, or build another approximate network as in [24]. Type II attacks on accessible SafetyNet fail: a type II attack involves a search for an adversarial example that will be (a) mislabelled and (b) not detected. This search is made difficult by the quantization procedure and by the narrow basis functions in the RBF-SVM, so we smooth the quantization operation and the RBF-SVM kernel operation. Smoothing is essential to make the search tractable, but can significantly misapproximate SafetyNet (which is what makes attacks hard). Our smoothing attack uses a sigmoid function with parameter \u03bb to simulate the quantization process. We also help the search process by increasing the size of the RBF parameter \u03c3 to form smoother gradients. Even after smoothing the objective function, attacks tend to fail, likely because it is hard to make an effective tradeoff between easy search and approximation. Table 8 includes Type I and Type II, blackbox and non-blackbox attacking results on SceneProof dataset. Our SafetyNet is the most robust architecture to various attacks.\nType II attacks on black box SafetyNet fail: Assume the state of SafetyNet is concealed. We follow [23, 18] by building attacks on various alternative networks, then transferring these network\u2019s adversarial samples. These attacks fail for our SafetyNet, refer to Table 8. In contrast to SafetyNet, the detector subnetwork of [16] is generally susceptible to type II attacks in both blackbox and non-blackbox settings. This is because of quantization process and detec-\ntion subnetwork\u2019s classification boundary problem [18]."}, {"heading": "4. Theory: Bars and P-domains", "text": "We construct one possible explanation for adversarial examples that successfully explains (a) the phenomenology and (b) why SafetyNet works. We have a network with N layers of ReLU\u2019s, and study y(k)i (x), the values at the output of the k\u2019th layer of ReLUs. This is a piecewise linear function of x. Such functions break up the input space into cells, at whose boundaries the piecewise linear function changes (i.e. is only C0). Now assume that for some y(k)i (x) there exist p-domains (union of cells) D in the input space such that: (a) there are no or few examples in the p-domain; (b) the measure of D under P (X) is small; (c) |y(k)i (x) | is large inside D and small outside D. We will always use the term \u201cp-domain\u201d to refer to domains with these properties. We think that the total measure of all p-domains under P (X) is small.\nBy construction, ReLU networks can represent such pdomains. We construct a p-domain using a basis function with small support. R(u) denote a ReLU applied to u. We\nhave basic bar function \u03c6.\n\u03c6(x; i, s, ) = 1  R((xi \u2212 s) + )\u22122R((xi \u2212 s))+ R((xi \u2212 s)\u2212 )  where \u03c6 has support when |xi \u2212 s | < and has peak value 1. For an index set I with cardinality #I and vectors s, , we write bar function b as\nb(x; I, s, ) = R( (\u2211 i\u2208I \u03c6(x; i, si, i)\u2212#I + 1 ) )\nwhere b has support when ||xI \u2212 sI ||1 < I . Figure 5 illustrates these functions. It is clear that a CNN can encode bars and weighted sums of bars, and that for at least k \u2265 2 every y(k)i could in principle be a bar function. Appropriate choices of s, and I choose the location and support of the bar and so can produce bars which have low measure under P (X). Now the functions presented to the softmax layer are a linear combination of the y(N)i (x). This means that with choice of weight and parameters, a bar can appear at this level, and create a p-domain.\nWe expect such p-domains to have several important properties. Adversarial fertility: P-domains can be used to make adversarial examples by choosing a point in a pdomain close to x. Because there are no or few examples in the p-domain, the loss may not cause the classifier to control the maximum value attained by y(k)i (x) in\nthis p-domain; and the large range of values inside the p-domain can be used to change the values in layers upstream of k, by moving the example around the p-domain. Generalization-neutral: The requirement that p-domains have small measure in P (X) means that both train and test examples are highly unlikely to lie in p-domains. A system with p-domains could generalize well without being immune to adversarial examples. Some subset of pdomains are likely findable by LBFGS. Consider the gradient of y(N)i (x) with respect to x in two cells separated by a boundary, where some ReLU changes state, weight decay encourages a relatively small change in gradient over these boundaries. If cells neighboring a p-domain have no or few examples in them, we can expect that the gradient change within cell is small too and a second order approximation of y(N)i (x) could be reliable. We also expect cells to be small, so search and entering a p-domain are possible and requires crossing multiple cell boundaries, which\nmeans many changes in ReLU activation. This argument suggests p-domains present odd patterns of ReLU activation, particularly in p-domains where some of the y(k)i (x) are large in the absence of examples.\nWhy p-domains could exist: As Zhang et al. point out, the number of training examples available to a typical modern network is small compared to the relative capacity of deep networks [30]. For example, excellent training error is obtainable for randomly chosen image labels [30]. We expect that y(N)i (x) will have a number of cells that is exponential in the dimension of x, ensuring that the vast majority of cells lack any example. However, the weight decay term is not sufficient to ensure that y(N)i is zero in these cells. Overshoot by stochastic gradient descent, caused by poor scaling in the loss, is the likely reason that y(N)i (x) has support in these cells. Szegedy et al. demonstrate that, in practice, ReLU layers can have large norm as linear operators, despite weight decay (see [29], sec. 4.3), so large values in p-domains are plausible. This large norm is likely to be the result of overshoot. Recall that the value of y(N)i (x) is determined by the product of numerous weights, so in some locations in x, the value of y(N)i could be large, which is a result of multiple layer norms interacting poorly.\nAn alternative to attacking by search using smoothed RBF gradients is as follows. One might pass an example through the main classifier, determine what code it had, then seek an adversarial example that produces that code (and so must fool the RBF-SVM). We sketch a proof that the optimization problem is extremely difficult. Choose some threshold t > 0. We use bt(u) for the function that binarizes its argument with t. Assume we have at least one unit y (k) i that encodes a weighted sum of bar functions. We wish to create an adversarial example a(x\u2217) that (a) meets criteria for being adversarial and (b) ensures that bt(y (k) i (a)) takes a prescribed value (either one or zero). The feasible set for this constraint can be disconnected (e.g. a sum of the bump functions of Figure 5 (right)), and so need not be convex, implying that the optimization problem is intractable. As a simple example, the following constraint set is disconnected for < 1/2\n{x | bt(b(x; 1,0, ) + b(x; 1,1, )) = 1} ."}, {"heading": "5. Discussion", "text": "We have described a method to produce a classifier that identifies and rejects adversarial examples. Our SafetyNet is able to reject adversarial examples that come from attacking methods not seen in training data. We have shown that it is hard to produce an example that (a) is mislabeled and (b) is not detected as adversarial by SafetyNet. We have sketched one possible reason that SafetyNet works, and is hard to attack. Many interesting problems are opened by\nour work, and we provides lots of insights into the mechanism that neural network works.\nSaferNet: There might be some better architecture than our SafetyNet, whose objective function is harder to optimize. The ideal case would be an architecture that forces the attacker to solve a hard discrete optimization problem which does not naturally admit smoothing.\nNeural network pruning: Our work suggests that networks behave poorly for input space regions where no data has been seen. We speculate that this behavior could be discouraged by a post-training pruning process, which removes neurons, paths or activation patterns not touched by training data.\nExplicit management of overshoot during training: we have explained adversarial examples using p-domains, which is the result of poor damping of weights during training. We speculate that constructing adversarial examples during training, by identifying locations where this damping problem occurs and exploiting structural insights into network behavior, could control the adversarial sample problem (rather than just using adversarial examples as training data)."}, {"heading": "6. Supporting Materials", "text": ""}, {"heading": "6.1. SceneProof Dataset", "text": "Our SceneProof dataset is processed from NYU Depth v2 raw captures, Sintel Synthetic RGBD dataset and Middlebury Stereo dataset. The dataset is split into part I and part II. Part I contains NYU natural image & depth pairs, along with manipulated unnatural scenes (swap depth, insert region, predicted depth, scale & shift depth), refer to Figure 6. It is used to train our classifier and work as test data part I. Part II contains unnatural scenes manipulated by other methods (set depth channel to zero, down sample and then up-sample both RGBD channels, aggressively compress the JPG RGBD images), and image & depth pairs from synthetic dataset and stereo dataset, refer to Figure 7. Part II is used as test data part II to test the generalization ability of our SceneProof network, and check the reactions of our detectors to unseen unnatural inputs. A good detector need to tend to reject unfamiliar data type, which does not exist in training data, because it is hard for classifier to do right classifications on unseen data types. In real application scenarios, it needs to be a human computer hybrid system where computer provides suspicious cases and human makes final decisions. Table 9 includes the dataset constitution, and we plan to release the dataset for academia usages."}, {"heading": "6.2. Type II Attacks on Cifar-10 and ImageNet-1000", "text": "In this section, we include the detailed percentages of Type II attacks on Cifar-10 could be found in Table 10, and\nthe detailed percentages of Type II attacks on ImageNet1000 could be found in Table 11."}], "references": [{"title": "A naturalistic open source movie for optical flow evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": "A. Fitzgibbon et al. (Eds.), editor, European Conf. on Computer Vision (ECCV), Part IV, LNCS 7577, pages 611\u2013625. Springer-Verlag, Oct.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in neural information processing systems, pages 2366\u20132374,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Exposing photo manipulation with inconsistent reflections", "author": ["H. Farid"], "venue": "ACM Trans. Graph., 31(1):4,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Photo forensics", "author": ["H. Farid"], "venue": "MIT Press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "How to detect faked photos", "author": ["H. Farid"], "venue": "American Scientist,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Exposing photo manipulation with inconsistent shadows", "author": ["E. Kee", "J.F. O\u2019Brien", "H. Farid"], "venue": "ACM Transactions on Graphics (ToG),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Dense associative memory is robust to adversarial inputs", "author": ["D. Krotov", "J.J. Hopfield"], "venue": "arXiv preprint arXiv:1701.00939,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Y. Liu", "X. Chen", "C. Liu", "D. Song"], "venue": "arXiv preprint arXiv:1611.02770,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "On detecting adversarial perturbations", "author": ["J.H. Metzen", "T. Genewein", "V. Fischer", "B. Bischoff"], "venue": "arXiv preprint arXiv:1702.04267,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "S.-i. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Universal adversarial perturbations", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": "arXiv preprint arXiv:1610.08401,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574\u20132582,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["P.K. Nathan Silberman", "Derek Hoiem", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427\u2013436,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Security and Privacy (SP), 2016 IEEE Symposium on, pages 582\u2013597. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Transferability in machine learning: from phenomena to blackbox attacks using adversarial samples", "author": ["N. Papernot", "P.D. McDaniel", "I.J. Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "High-resolution stereo datasets with subpixel-accurate ground truth", "author": ["D. Scharstein", "H. Hirschm\u00fcller", "Y. Kitajima", "G. Krathwohl", "N. Ne\u0161i\u0107", "X. Wang", "P. Westling"], "venue": "German Conference on Pattern Recognition, pages 31\u201342. Springer,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "ICLR 2016,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 12, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 17, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 22, "context": "A variety of constructions [9, 14, 19, 24] can generate an adversarial example a(x) to make the classifier label it m 6= l.", "startOffset": 27, "endOffset": 42}, {"referenceID": 27, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 43, "endOffset": 54}, {"referenceID": 19, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 43, "endOffset": 54}, {"referenceID": 1, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 43, "endOffset": 54}, {"referenceID": 16, "context": "Adversarial examples are easy to construct [29, 21, 3], and there are even universal adversarial perturbations [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "imagine a small physical modification that could reliably get a stop sign classified as a go faster sign [24]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "The absence of theory means it is hard to defend against adversarial examples (for example, distillation was proposed as a defense [25], but was later shown to not work [2]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 7, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 33, "endOffset": 36}, {"referenceID": 27, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": ", line search along the gradient [9]; LBFGS on an appropriate cost [29]; DeepFool [19]) all rely on the gradient of the network, but it is known that using the gradient of another similar network is sufficient [24], so concealing the gradient does not work as a defense for current networks.", "startOffset": 210, "endOffset": 214}, {"referenceID": 27, "context": "An important puzzle is that networks that generalize very well remain susceptible to adversarial examples [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26].", "startOffset": 122, "endOffset": 134}, {"referenceID": 13, "context": "Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26].", "startOffset": 122, "endOffset": 134}, {"referenceID": 24, "context": "Another important puzzle is that examples that are adversarial for one network tend to be adversarial for another as well [29, 15, 26].", "startOffset": 122, "endOffset": 134}, {"referenceID": 11, "context": "Some network architectures appear to be robust to adversarial examples [13], which still need more empirical verification.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "At least some adversarial attacks appear to apply to many distinct networks [18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "show how to construct examples that appear to be noise, but are confidently classified as objects [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "However, most adversarial examples \u201clook like\u201d images to humans, such as figure 5 in [29], so they are likely to lie within the support of P (X).", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "One way to build a network that is robust to adversarial examples is to train networks with enhanced training data (adding adversarial samples [17]); this approach faces difficulties, because the dimension of the images and features in networks means an unreasonable quantity of training data is required.", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "show that, by attaching a detection subnetwork that observes the state of the original classification network, one can tell whether it has been presented with an adversarial example or not [16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "However, because the gradients of their detection subnetwork are quite well behaved, the joint system can be attacked easily (in our experiments; [16] does not provide any analysis for attacking on both detectors and classifiers).", "startOffset": 146, "endOffset": 150}, {"referenceID": 14, "context": "[16]); (b) such detectors can be made very difficult to defeat (unlike Metzen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16]; section 3); (c).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16]); (d) transfer attacks work because an example that generates unfamiliar patterns in one network tends to generate unfamiliar patterns in other networks too; (e) transfer attacks could be defended as well (section 3).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This is in sharp contrast to all other known methods [24, 16].", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "This is in sharp contrast to all other known methods [24, 16].", "startOffset": 53, "endOffset": 61}, {"referenceID": 7, "context": "Fast Sign method: Goodfellow et al [9] described this simple method.", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "[14] introduced an iteration version of the fast sign method, by applying it several times with a smaller step size \u03b1 and clipping all pixels after each iteration to ensure that results stay in the neighborhood of the original image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] introduced the DeepFool adversary, which is able to choose which class an example is switched to.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] described a way to attack a black-box network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Accuracy: Our SafetyNet can detect adversarial samples with high accuracy on CIFAR-10 [12] and ImageNet1000 [4].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "Accuracy: Our SafetyNet can detect adversarial samples with high accuracy on CIFAR-10 [12] and ImageNet1000 [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "For classification networks, we used a 32-layer ResNet [10] for CIFAR-10 and a VGG19 network [28] for ImageNet-1000.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "For classification networks, we used a 32-layer ResNet [10] for CIFAR-10 and a VGG19 network [28] for ImageNet-1000.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "Figure 2 compares our RBF-SVM detection results with the detector subnetwork results of [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "To facilitate comparison, we follow the conventions of [16], plotting the success of the adversary (i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "A: Results for the detection subnetwork on CIFAR-10 from [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "C: Results for SafetyNet and the detection subnetwork (cnn) of [16] on CIFAR-10, where the detector was trained on L\u221e attack and tested on other attacking methods; SafetyNet generalizes better than detection subnetwork to different adversarial attacking methods.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "We cannot compare to the detection subnetwork of [16], because they do not provide results for ImageNet-1000.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "We perform the gradient descent based Type II attacks for Cifar10 and ImageNet-1000 with SVM detector, and compare to detection subnetwork [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "[16] only investigated type I attacks and has not investigated type II attacks on the detection subnetwork.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 149, "endOffset": 155}, {"referenceID": 5, "context": "Current best methods to identify fake images require careful analysis of vanishing points [8], illumination angles [6], and shadows [11] (reviews in [8, 7]).", "startOffset": 149, "endOffset": 155}, {"referenceID": 18, "context": "We use the raw Kinect captures of LivingRoom and Bedroom from NYU v2 dataset [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "\u201cRegression\u201d methods used in both train and test are: random swaps of depth and image planes; single image predicted depth [5]; rectangle cropped region insertion and random shifted or scaled misaligned depth and image.", "startOffset": 123, "endOffset": 126}, {"referenceID": 25, "context": "\u201cRegression\u201d methods used only in test are: all zero depth values; nearest neighbor down-sample and up-sampled images and depths; low quality JPEG compressed images and depths; Middlebury stereo RGBD dataset [27] and Sintel RGBD dataset [1](which should be classified \u201cfake\u201d because they are renderings).", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "\u201cRegression\u201d methods used only in test are: all zero depth values; nearest neighbor down-sample and up-sampled images and depths; low quality JPEG compressed images and depths; Middlebury stereo RGBD dataset [27] and Sintel RGBD dataset [1](which should be classified \u201cfake\u201d because they are renderings).", "startOffset": 237, "endOffset": 240}, {"referenceID": 22, "context": "In this case, attackers must probe with inputs and gather outputs, or build another approximate network as in [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "We follow [23, 18] by building attacks on various alternative networks, then transferring these network\u2019s adversarial samples.", "startOffset": 10, "endOffset": 18}, {"referenceID": 16, "context": "We follow [23, 18] by building attacks on various alternative networks, then transferring these network\u2019s adversarial samples.", "startOffset": 10, "endOffset": 18}, {"referenceID": 14, "context": "In contrast to SafetyNet, the detector subnetwork of [16] is generally susceptible to type II attacks in both blackbox and non-blackbox settings.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "8% Sintel RGBD [1] 27.", "startOffset": 15, "endOffset": 18}, {"referenceID": 25, "context": "4% Middlebury RGBD [27] 24.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "tion subnetwork\u2019s classification boundary problem [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "The table compares a VGG19 network (Ori) with the detection subnetwork of [16] (Subnet), and two variants of SafetyNet (Det A, where we have an RBF-SVM on fc7; and Det ABC, where we have an RBF-SVM on each of fc7, fc6 and pool5, and declare an adversary when any detector responds).", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "This is because of quantization process and our SafetyNet works like an example matcher, while detection subnetwork suffers from classification boundary problem [18].", "startOffset": 161, "endOffset": 165}, {"referenceID": 28, "context": "point out, the number of training examples available to a typical modern network is small compared to the relative capacity of deep networks [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "For example, excellent training error is obtainable for randomly chosen image labels [30].", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "demonstrate that, in practice, ReLU layers can have large norm as linear operators, despite weight decay (see [29], sec.", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat using several standard networks and datasets. We use our method to produce a system that can reliably detect whether an image is a picture of a real scene or not. Our system applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our system is robust to adversarial examples built from currently known attacking approaches.", "creator": "LaTeX with hyperref package"}}}