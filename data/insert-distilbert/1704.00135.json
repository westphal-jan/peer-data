{"id": "1704.00135", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Topic modeling of public repositories at scale using names in source code", "abstract": "programming constructed languages themselves have a limited number of reserved keywords and character based tokens that define the language specification. however, programmers have a rich use of one natural language within entering their code through comments, text literals and naming entities. and the programmer defined names that can be found concurrently in source code are a rich source of information to build a high functional level understanding of the general project. the goal of funding this paper is to apply topic modeling to names used accidentally in over 13. 6 million repositories and perceive the inferred topics. one possibility of the problems in such formal a study presentation is the occurrence of duplicate repositories not officially marked as forks ( obscure forks ). we show how to address it using naturally the same identifiers which are extracted best for topic modeling.", "histories": [["v1", "Sat, 1 Apr 2017 08:16:20 GMT  (120kb,D)", "http://arxiv.org/abs/1704.00135v1", "11 pages"], ["v2", "Sat, 20 May 2017 08:29:00 GMT  (350kb,D)", "http://arxiv.org/abs/1704.00135v2", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.PL cs.CL", "authors": ["vadim markovtsev", "eiso kant"], "accepted": false, "id": "1704.00135"}, "pdf": {"name": "1704.00135.pdf", "metadata": {"source": "CRF", "title": "Topic modeling of public repositories at scale using names in source code", "authors": ["Vadim Markovtsev"], "emails": ["vadim@sourced.tech", "eiso@sourced.tech"], "sections": [{"heading": null, "text": "We open with a discussion on naming in source code, we then elaborate on our approach to remove exact duplicate and fuzzy duplicate repositories using Locality Sensitive Hashing on the bag-of-words model and then discuss our work on topic modeling; and finally present the results from our data analysis together with open-access to the source code, tools and datasets.\nIndex Terms\u2014programming, open source, source code, software repositories, git, GitHub, topic modeling, ARTM, locality sensitive hashing, MinHash, open dataset, data.world.\nI. INTRODUCTION\nThere are more than 18 million non-empty public repositories on GitHub which are not marked as forks. This makes GitHub the largest version control repository hosting service. It has become difficult to explore such a large number of projects and nearly impossible to classify them. One of the main sources of information that exists about public repositories is their code.\nTo gain a deeper understanding of software development it is important to understand the trends among open-source projects. Bleeding edge technologies are often used first in open source projects and later employed in proprietary solutions when they become stable enough1. An exploratory analysis of open-source projects can help to detect such trends and provide valuable insight for industry and academia.\nSince GitHub appeared the open-source movement has gained significant momentum. Historically developers would manually register their open-source projects in software digests. As the number of projects dramatically grew, those lists became very hard to update; as a result they became more fragmented and started exclusively specializing in narrow\n1Notable examples include the Linux OS kernel, the PostgreSQL database engine, the Apache Spark cluster-computing framework and the Docker containers.\ntechnological ecosystems. The next attempt to classify open source projects was based on manually submitted lists of keywords. While this approach works [1], it requires careful keywords engineering to appear comprehensive, and thus not widely adopted by the end users in practice. GitHub introduced repository tags in January 2017 which is a variant of manual keywords submission.\nThe present paper describes how to conduct fully automated topic extraction from millions of public repositories. It scales linearly with the overall source code size and has substantial performance reserve to support the future growth. We propose building a bag-of-words model on names occurring in source code and applying proven Natural Language Processing algorithms to it. Particularly, we describe how \u201dWeighted MinHash\u201d algorithm [2] helps to filter fuzzy duplicates and how an Additive Regularized Topic Model (ARTM) [3] can be efficiently trained. The result of the topic modeling is a nearly-complete open source projects classification. It reflects the drastic variety in open source projects and reflect multiple features. The dataset we work with consists of approx. 18 million public repositories retrieved from GitHub in October 2016.\nThe rest of the paper is organised as follows: Section II reviews prior work on the subject. Section III elaborates on how we turn software repositories into bags-of-words. Section IV describes the approach to efficient filtering of fuzzy repository clones. Section V covers the building of the ARTM model with 256 manually labeled topics. Section VI presents the achieved topic modeling results. Section VII lists the opened datasets we were able to prepare. Finally, section VIII presents a conclusion and suggests improvements to future work."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Academia", "text": "There was an open source community study which presented statistics about manually picked topics in 2005 by J. Xu et.al. [4].\nBlincoe et.al. [5] studied GitHub ecosystems using reference coupling over the GHTorrent dataset [6] which contained 2.4 million projects. This research employs an alternative topic modeling method on source code of 13.6 million projects.\nar X\niv :1\n70 4.\n00 13\n5v 1\n[ cs\n.P L\n] 1\nA pr\n2 01\n7\nInstead of using the GHTorrent dataset we\u2019ve prepared open datasets from almost all public repositories on GitHub to be able to have a more comprehensive overview.\nM. Lungi [7] conducted an in-depth study of software ecosystems in 2009, the year when GitHub appeared. The examples in this work used samples of approx. 10 repositories. And the proposed discovery methods did not include Natural Language Processing.\nThe problem of the correct analysis of forks on GitHub has been discussed by Kalliamvakou et.al. [8] along with other valuable concerns.\nTopic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23]. In the aforementioned works, the scope of the research was focused on individual projects.\nThe usage of topic modeling [24] focused on improving software maintenance and was evaluated on 4 software projects. Concepts were extracted using a corpus of 24 projects in [25]. Giriprasad Sridhara et.al. [26], Yang and Tan [27], Howard et.al. [28] considered comments and/or names to find semantically similar terms; Haiduc and Marcus [29] researched common domain terms appearing in source code. The presented approach in this papers reveals similar and domain terms, but leverages a much significantly larger dataset of 13.6 million repositories.\nBajracharya and Lopes [30] trained a topic model on the year long usage log of Koders, one of the major commercial code search engines. The topic categories suggested by Bajracharya and Lopes share little similarity with the categories described in this paper since the input domain is much different.\nB. Industry\nTo our knowledge, there are few companies which maintain a complete mirror of GitHub repositories. source{d} [31] is focused on doing machine learning on top of the collected source code. Software Heritage [32] strives to become web.archive.org for open source software. SourceGraph [33] processes source code references, internal and external, and created a complete reference graph for projects written in Golang.\nlibraries.io [34] is not GitHub centric but rather processes the dependencies and metadata of open source packages fetched from a number of repositories. It analyses the dependency graph (at the level of projects, while SourceGraph analyses at the level of functions)."}, {"heading": "III. BUILDING THE BAG-OF-WORDS MODEL", "text": "This section follows the way we convert software repositories into bags-of-words, the model which stores each project as a multiset of its identifiers, ignoring the order while maintaining multiplicity.\nFor the purpose of our analysis we choose to use the latest version of the master branch of each repository. And treat each repository as a single document. An improvement for further research would be to use the entire history of each repository including unique code found in each branch."}, {"heading": "A. Preliminary Processing", "text": "Our first goal is to process each repository to identify which files contain source code, and which files are redundant for our purpose. GitHub has an open-source machine learning based library named linguist [35] that identifies the programming language used within a file based on its extension and contents. We modified it to also identify vendor code and automatically generated files. The first step in our preprocessing is to run linguist over each repository\u2019s master branch. From 11.94 million repositories we end up with 402.6 million source files in which we have high confidence it is source code written by a developer in that project. Identifying the programming language used within each file is important for the next step, the names extraction, as it determines the programming language parser."}, {"heading": "B. Extracting Names", "text": "Source code highlighting is a typical task for professional text editors and IDE\u2019s. There have been several open source libraries created to tackle this task. Each works by having a grammar file written per programming language which contains the rules. Pygments [36] is a high quality communitydriven package for Python which supports more than 400 programming languages and markups. According to Pygments, all source code tokens are classified across the following categories: comments, escapes, indentations and generic symbols, reserved keywords, literals, operators, punctuation and names.\nLinguist and Pygments have different sets of supported languages. Linguist stores it\u2019s list at master/lib/linguist/languages.yml and the similar Pygments list is stored as pygments.lexers.LEXERS. Each has nearly 400 items and the intersection is approximately 200 programming languages (\u201dprogramming\u201d Linguist\u2019s item type). The languages common to Linguist and Pygments which were chosen are listed in appendix A. In this research we apply Pygments to the 402.6 million source files to extract all tokens which belong to the type Token.Name."}, {"heading": "C. Processing names", "text": "The next step is to process the names according to naming conventions. As an example class FooBarBaz adds three words to the bag: foo, bar and baz, or int wdSize should add two: wdsize and size. Fig. 1 is the full listing of the function written in Python 3.4+ which splits identifiers.\nIn this step each repository is saved as an sqlite database file which contains a table with: programming language, name extracted and frequency of occurance in that repository. The total number of unique names that were extracted were 17.28 million."}, {"heading": "D. Stemming names", "text": "It is common to stem names when creating a bag-of-words in NLP. Since we are working with natural language that is predominantly English we have applied the Snowball stemmer [37] from the Natural Language Toolkit (NLTK) [38]. The stemmer was applied to names which were >6 characters long. In further research a diligent step would be to compare results with and without stemming of the names, and also to predetermine the language of the name (when available) and apply stemmers in different languages.\nThe length of words on which stemming was applied was chosen after the manual observation that shorter identifiers tend to collide with each other when stemmed and longer identifiers need to be normalized. Fig. 2 represents the distribution of identifier lengths in the dataset:\nIt can be seen that the most common name length is 6. Fig. 3 is the plot of the number of unique words in the dataset depending on the stemming threshold:\nWe observe the breaking point at length 5. The vocabulary size linearly grows starting with length 6. Having manually inspected several collisions on smaller thresholds, we came to\nthe conclusion that 6 corresponds to the best trade-off between collisions and word normalization.\nThe Snowball algorithm was chosen based on the comparative study by Jivani [39]. Stems not being real words are acceptable but it is critical to have minimum over-stemming since it increases the number of collisions. The total number of unique names are 16.06 million after stemming.\nTo be able to efficiently pre-process our data we used Apache Spark [40] running on 64 4-core nodes which allowed us to process repositories in parallel in less than 1 day.\nHowever, before training a topic model one has to exclude near-duplicate repositories. In many cases GitHub users include the source code of existing projects without preserving the commit history. For example, it is common for web sites, blogs and Linux-based firmwares. Those repositories contain very little original changes and may introduce frequency noise into the overall names distribution. This paper suggests the\nway to filter the described fuzzy duplicates based on the bag of words model built on the names in the source code."}, {"heading": "IV. FILTERING NEAR-DUPLICATE REPOSITORIES", "text": "There were more than 70 million GitHub repositories in October 2016 by our estimation. Approx. 18 million were not marked as forks. Nearly 800,000 repositories were de facto forks but not marked correspondingly by GitHub. That is, they had the same git commit history with colliding hashes. Such repositories may appear when a user pushes a cloned or imported repository under his or her own account without using the GitHub web interface to initiate a fork.\nWhen we remove such hidden forks from the initial 18 million repositories, there still remain repositories which are highly similar. A duplicate repository is sometimes the result of a git push of an existing project with a small number of changes. For example, there are a large number of repositories with the Linux kernel which are ports to specific devices. In another case, repositories containing web sites were created using a cloned web engine and preserving the development history. Finally, a large number of github.io repositories are the same. Such repositories contain much text content and few identifiers which are typically the same (HTML tags, CSS rules, etc.).\nFiltering out such fuzzy forks speeds up the future training of the topic model and reduces the noise. As we obtained a bag-of-words for every repository, the naive approach would be to measure all pairwise similarities and find cliques. But first we need to define what is the similarity between bags-ofwords."}, {"heading": "A. Weighted MinHash", "text": "Suppose that we have two dictionaries - key-value mappings with unique keys and values indicating non-negative \u201dweights\u201d of the corresponding keys. We would like to introduce a similarity measure between them. The Jaccard Similarity between dictionaries A = {i : ai}, i \u2208 I and B = {j : bj}, j \u2208 J is defined as\nJ =\n\u2211 k\u2208K\nmin(ak, bk)\u2211 k\u2208K max(ak, bk) ,K = I \u222a J (1)\nwhere ak = 0, k /\u2208 I and bk = 0, k /\u2208 J . If the weights are binary, this formula is equivalent to the common Jaccard Similarity definition.\nThe same way as MinHash is the algorithm to find similar sets in linear time, Weighted MinHash is the algorithm to find similar dictionaries in linear time. Weighted MinHash was introduced by Ioffe in [2]. We have chosen it in this paper because it is very efficient and allows execution on GPUs instead of large CPU clusters. The proposed algorithm depends on the parameter K which adjusts the resulting hash length.\n1. for k in range(K): 1.1. Sample rk, ck \u223c Gamma(2, 1) - Gamma distri-\nbution (their PDF is P (r) = re\u2212r), and \u03b2k \u223c Uniform(0, 1).\n1.2. Compute\ntk = b lnSk rk + \u03b2kc (2) yk = e rk(tk\u2212\u03b2k) (3) zk = yke rk (4) ak = ck zk (5)\n2. Find k\u2217 = arg mink ak and return the samples (k\u2217, tk\u2217). Thus given K and supposing that the integers are 32- bit we obtain the hash with size 8K bytes. Samples from Gamma(2, 1) distribution can be efficiently calculated as r = \u2212 ln(u1u2) where u1, u2 \u223c Uniform(0, 1) - uniform distribution between 0 and 1.\nWe developed the MinHashCUDA [41] library and Python native extension which is the implementation of Weighted MinHash algorithm for NVIDIA GPUs using CUDA [42]. There were several engineering challenges with that implementation which are unfortunately out of the scope of this paper. We were able to hash all 10 million repositories with hash size equal to 128 in less than 5 minutes using MinHashCUDA and 2 NVIDIA Titan X Pascal GPU cards."}, {"heading": "B. Locality Sensitive Hashing", "text": "Having calculated all the hashes in the dataset, we can perform Locality Sensitive Hashing. We define several hash tables, each for it\u2019s own sub-hash which depends on the target level of false positives. Same elements will appear in the same bucket; union of the bucket sets across all the hash tables for a specific sample yields all the similar samples. Since our goal is to determine the sets of mutually similar samples, we should consider the set intersection instead.\nWe used the implementation of Weighted MinHash LSH from Datasketch [43]. It is designed after the corresponding algorithm in Mining of Massive Datasets [44]. LSH takes a single parameter - the target Weighted Jaccard Similarity value (\u201dthreshold\u201d). MinHash LSH puts every repository in a number of separate hash tables which depend on the threshold and the hash size. We used the default threshold 0.9 in our experiments which ensures a low level of dissimilarity within a hash table bin.\nAlgorithm 5 describes the fuzzy duplicates detection pipeline. Step 6 discards less than 0.5% of all the sets and aims at reducing the number of false positives. The bins size distribution after step 5 is depicted on Fig. 4 - it is clearly seen that the majority of the bins has the size 2. Step 6 uses Weighted Jaccard similarity threshold 0.8 instead of 0.9 to be sensitive to evident outliers exclusively.\nTable I reveals how different hash sizes influence on the resulting number of fuzzy clones:\nAlgorithm 5 results in approximately 467,000 sets of fuzzy duplicates with overall 1.7 million unique repositories. Each repository appears in two sets on average. The examples of fuzzy duplicates are listed in appendix B. The detection algorithm works especially well for static web sites which share the same JavaScript libraries.\nAfter the exclusion of the fuzzy duplicates, we finish dataset processing and pass over to training of the topic model. The total number of unique names has now reduced by 2.06 million to 14 million unique names. To build a meaningful dataset, names with occurrence of less than Tf = 20 were excluded from the final vocabulary. 20 was chosen on the frequency histogram shown on Fig. 6 since it is the drop-off point.\nAfter this exclusion, there are now 2 million unique names,\nwith an average size of a bag-of-words of 285 per repository and Fig. 7 displays the heavy-tailed bag size distribution."}, {"heading": "V. TRAINING THE ARTM TOPIC MODEL", "text": "This section revises ARTMs and describes how the training of the topic model was performed. We have chosen ARTM instead of other topic modeling algorithms since it has the most efficient parallel CPU implementation in bigARTM according to our benchmarks."}, {"heading": "A. Additive Regularized Topic Model", "text": "Suppose that we have a topic probabilistic model of the collection of documents D which describes the occurrence of terms w in document d with topics t:\np(w|d) = \u2211 t\u2208T p(w|t)p(t|d). (6)\nHere p(w|t) is the probability of the term w to belong to the topic t, p(t|d) is the probability of the topic t to belong to the document d, thus the whole formula is just an expression of the total probability, accepting the hypothesis of conditional independence: p(w|d, t) = p(w|t). Terms belong to the vocabulary W , topics are taken from the set T which is simply the series of indices [1, 2, . . . nt].\nWe\u2019d like to solve the problem of recovering p(w|t) and p(t|d) from the given set of documents {d \u2208 D : d = {w1 . . . wnd}}. We normally assume p\u0302(w|d) = ndwnd , ndw being the number of times term w occurred in document d, but this implies that all the terms are equally important which is not always true. \u201dImportance\u201d here means some measure which negatively correlates with the overall frequency of the term. Let us denote the recovered probabilities as p\u0302(w|t) = \u03c6wt and p\u0302(t|d) = \u03b8td. Thus our problem is the stochastic matrix decomposition which is not correctly stated:\nndw nd \u2248 \u03a6 \u00b7\u0398 = (\u03a6S)(S\u22121\u0398) = \u03a6\u2032 \u00b7\u0398\u2032. (7)\nThe stated problem can be solved by applying maximum likelihood estimation:\u2211\nd\u2208D \u2211 w\u2208d ndw ln \u2211 t \u03c6wt\u03b8td \u2192 max \u03a6,\u0398\n(8)\nupon the conditions\n\u03c6wt > 0; \u2211 w\u2208W \u03c6wt = 1; \u03b8td > 0; \u2211 t\u2208T \u03b8td = 1. (9)\nThe idea of ARTM is to naturally introduce regularization as one or several extra additive members:\u2211\nd\u2208D \u2211 w\u2208d ndw ln \u2211 t \u03c6wt\u03b8td +R(\u03a6,\u0398)\u2192 max \u03a6,\u0398 . (10)\nSince this is a simple summation, one can combine a series of regularizers in the same objective function. For example, it is possible to increase \u03a6 and \u0398 sparsity or to make topics less correlated. Well-known LDA model [45] can be reproduced as ARTM too.\nThe variables \u03a6 and \u0398 can be effectively calculated using the iterative expectation maximization algorithm [46]. Many ready to be used ARTM regularizers are already implemented in the BigARTM open source project [47]."}, {"heading": "B. Training", "text": "Vorontsov shows in [3] that ARTM is trained best if the regularizers are activated sequentially, with a lag relative to each other. For example, first EM iterations are performed without any regularizers at all and the model reaches target perplexity, then \u03a6 and \u0398 sparsity regularizers are activated and the model optimizes for those new members in the objective function while not increasing the perplexity. Finally other advanced regularizers are appended and the model minimizes the corresponding members while leaving the old ones intact.\nWe apply only \u03a6 and \u0398 sparsity regularizers in this paper. Further research is required to leverage others. We\nexperimented with the training of ARTM on the source code identifiers from III and observed that the final perplexity and sparsity values do not change considerably on the wide range of adjustable meta-parameters. The best training metaparameters are given in Table II.\nWe chose 256 topics merely because it is time intensive to label them and 256 was the largest amount we could label. The traditional ways of determining the optimal number of topics using e.g. Elbow curves are not applicable to our data. We cannot consider topics as clusters since a typical repository corresponds to several topics and increasing the number of topics worsens the model\u2019s generalization and requires the dedicated topics decorrelation regularizer. The overall number of iterations equals 18. The convergence plot is shown on Fig. 8.\nThe achieved quality metric values are as given in Table III. On the average, a single iteration took 40 minutes to complete on our hardware. We used BigARTM in a Linux environment on 16-core (32 threads) Intel(R) Xeon(R) CPU E5-2620 v4 computer with 256 GB of RAM. BigARTM supports the parallel training and we set the number of workers\nto 30. The peak memory usage was approximately 32 GB. It is possible to relax the hardware requirements and speed up the training if the model size is reduced. If we set the frequency threshold Tf to a greater value, we can dramatically reduce the input data size with the risk of loosing the ability of the model to generalize."}, {"heading": "C. Converting the repositories to the topics space", "text": "Let Rt be the matrix of repositories in the topics space of size R\u00d7T , Rn be the sparse matrix representing the dataset of size R\u00d7N and Tn be the matrix representing the trained topic model of size N \u00d7 T . We perform the matrix multiplication to get the repository embeddings:\nRt = Rn \u00d7 Tn. (11)\nWe further normalize each row of the matrix by L2 metric:\nRnormedt = rowwise Rt \u2016Rt\u20162 . (12)\nThe sum along every column of this matrix indicates the significance of each topic. Fig. 9 shows the distribution of this measure."}, {"heading": "VI. TOPIC MODELING RESULTS", "text": "The employed topic model is unable to summarize the topics the same way humans do. It is possible to interpret some topics based on the most significant words, some based on relevant repositories, but many require manual supervision with the careful analysis of most relevant names and repositories. This supervision is labour intensive and the single topic normally takes up to 30 minutes to summarize with proper confidence. 256 topics required several man-days to complete the analysis.\nAfter a careful analysis, we sorted the labelled topics into the following groups: \u2022 Concepts (41) - general, broad and abstract. The most\ninteresting group. It includes scientific terms, facts about the world and the society.\n\u2022 Human languages (10) - it appeared that one can determine programmer\u2019s approximate native language looking at his code, thanks to the stem bias. \u2022 Programming languages (33) - not so interesting since this is the information we already have after linguist classification. Programming languages usually have a standard library of classes and functions which is imported/included into most of the programs, and the corresponding names are revealed by our topic modeling. Some topics are more narrow than a programming language. \u2022 General IT (72) - the topics which could appear in Concepts if had an expressive list of key words but do not. The repositories are associated by the unique set of names in the code without any special meaning. \u2022 Technologies (87) - devoted to some specific, potentially narrow technology or product. Often indicates an ecosystem or community around the technology. \u2022 Games (13) - related to video games. Includes specific gaming engines.\nThe complete topics list is in appendix C. The example topic labelled \u201dMachine Learning, Data Science\u201d is shown in appendix D.\nIt can be observed that some topics are dual and need to be splitted. That duality is a sign that the number of topics should be bigger. At the same time, some topics appear twice and need to be de-correlated, e.g. using the \u201ddecorrelation\u201d ARTM regularizer. Simple reduction or increase of the number of topics however do not solve those problems, we found it out while experimenting with 200 and 320 topics."}, {"heading": "VII. RELEASED DATASETS", "text": "We generated several datasets which were extracted from our internal 100 TB GitHub repository storage. We incorporated them on data.world [48], the recently emerged \u201dGitHub for data scientists\u201d, each has the description, the origin note and the format definition. They are accessed at data.world/vmarkovtsev. Besides, the datasets are uploaded to Zenodo and have DOI. They are listed in Table VII."}, {"heading": "VIII. CONCLUSION AND FUTURE WORK", "text": "Topic modeling of GitHub repositories is an important step to understanding software development trends and open source\ncommunities. We built a repository processing pipeline and applied it to more than 18 million public repositories on GitHub. Using developed by us open source tool MinHashCUDA we were able to remove 1.6 million fuzzy duplicate repositories from the dataset. The preprocessed dataset with source code names as well as other datasets are open and the presented results can be reproduced. We trained ARTM on the resulting dataset and manually labelled 256 topics. The data processing and model training are possible to perform using a single GPU card and a moderately sized Apache Spark cluster. The topics covered a broad range of projects but there were repeating and dual ones. The chosen number of topics was enough for general exploration but not enough for the complete description of the dataset.\nFuture work may involve experimentation with clustering the repositories in the topic space and comparison with clusters based on dependency or social graphs [49]."}, {"heading": "APPENDIX A", "text": "PARSED LANGUAGES\nabap abl actionscript ada agda ahk alloy antlr apl applescript arduino as3 aspectj aspx-vb autohotkey autoit awk b3d bash batchfile befunge blitzbasic blitzmax bmax boo bplus brainfuck bro bsdmake c c# c++ ceylon cfc cfm chapel chpl cirru clipper clojure cmake cobol coffeescript\ncoldfusion common lisp component pascal console coq csharp csound cucumber cuda cython d dart delphi dosbatch dylan ec ecl eiffel elisp elixir elm emacs erlang factor fancy fantom fish fortran foxpro fsharp gap gas genshi gherkin glsl gnuplot go golo gosu groovy haskell haxe\nhy i7 idl idris igor igorpro inform 7 io ioke j isabelle jasmin java javascript jsp julia kotlin lasso lassoscript lean lhaskell lhs limbo lisp literate agda literate haskell livescript llvm logos logtalk lsl lua make mako mathematica matlab mf minid mma modelica modula-2 monkey\nmoocode moonscript mupad myghty nasm nemerle nesc newlisp nimrod nit nix nixos nsis numpy obj-c obj-c++ obj-j objectpascal ocaml octave ooc opa openedge pan pascal pawn perl php pike plpgsql posh povray powershell progress prolog puppet pyrex python qml robotframework\nr racket ragel rb rebol red redcode ruby rust sage salt scala\nscheme scilab shell shen smali smalltalk smarty sml sourcepawn splus squeak stan\nstandard ml\nsupercollider swift tcl tcsh thrift typescript vala vb.net verilog vhdl\nvim winbatch x10 xbase xml+genshi xml+kid xquery xslt xtend zephir"}, {"heading": "APPENDIX B EXAMPLES OF FUZZY DUPLICATE REPOSITORIES", "text": "A. Linux kernel\n\u2022 1406/linux-0.11 \u2022 yi5971/linux-0.11 \u2022 love520134/linux-0.11 \u2022 wfirewood/source-linux-0.11 \u2022 sunrunning/linux-0.11 \u2022 Aaron123/linux-0.11 \u2022 junjee/linux-0.11 \u2022 pengdonglin137/linux-0.11 \u2022 yakantosat/linux-0.11\nB. Tutorials\n\u2022 dcarbajosa/linuxacademy-chef \u2022 jachinh/linuxacademy-chef \u2022 flarotag/linuxacademy-chef \u2022 qhawk/linuxacademy-chef \u2022 paul-e-allen/linuxacademy-chef\nC. Web applications 1\n\u2022 choysama/my-django-first-blog \u2022 mihuie/django-first \u2022 PubMahesh/my-first-django-app \u2022 nickmalhotra/first-django-blog \u2022 Jmeggesto/MyFirstDjango \u2022 atlwendy/django-first \u2022 susancodes/first-django-app \u2022 quipper7/DjangoFirstProject \u2022 phidang/first-django-blog\nD. Web applications 2\n\u2022 iggitye/omrails \u2022 ilrobinson81/omrails \u2022 OCushman/omrails \u2022 hambini/One-Month-Rails \u2022 Ben2pop/omrails \u2022 chrislorusso/omrails \u2022 arjunurs/omrails \u2022 crazystingray/omrails \u2022 scorcoran33/omrails \u2022 Joelf001/Omrails"}, {"heading": "APPENDIX C COMPLETE LIST OF LABELLED TOPICS", "text": "A. Concepts\n1) 2D geometry 2) 3D geometry 3) Arithmetic 4) Audio 5) Bitcoin 6) Card Games 7) Chess; Hadoop # 8) Classical mechanics\n(physics) 9) Color Manipula-\ntion/Generation 10) Commerce, ordering 11) Computational Physics 12) Date and time 13) Design patterns; HTML\nparsing 14) Email * 15) Email * 16) Enumerators, Mathemat-\nical Expressions 17) Finance and trading 18) Food (eg. pizza, cheese,\nbeverage), Calculator 19) Genomics 20) Geolocalization, Maps 21) Graphs\n22) Hexademical numbers 23) Human 24) Identifiers 25) Language names;\nJavaFX # 26) Linear Algebra; Opti-\nmization 27) Machine Learning, Data\nScience 28) My 29) Parsing 30) Particle physics 31) Person Names (Ameri-\ncan) 32) Personal Information 33) Photography, Flickr 34) Places, transportation,\ntravel 35) Publishing; Flask # 36) Space and solar system 37) Sun and moon 38) Trade 39) Trees, Binary Trees 40) Video; movies 41) Word Term\nB. Human languages\n42) Chinese 43) Dutch 44) French * 45) French * 46) German\n47) Portuguese * 48) Portuguese * 49) Spanish * 50) Spanish * 51) Vietnamese\nC. Programming languages\n52) Assembler 53) Autoconf 54) Clojure 55) ColdFusion * 56) ColdFusion * 57) Common LISP 58) Emacs LISP 59) Emulated assembly 60) Go 61) HTML 62) Human education sys-\ntem 63) Java AST and bytecode 64) libc 65) Low-level PHP\n66) Lua * 67) Lua * 68) Makefiles 69) Mathematics: proofs, sets 70) Matlab 71) Object Pascal 72) Objective-C 73) Perl 74) Python 75) Python, ctypes 76) Ruby 77) Ruby with language ex-\ntensions 78) SQL 79) String Manipulation in C\n80) Verilog/VHDL 81) Work, money, employ-\nment, driving, living\n82) x86 Assembler * 83) x86 Assembler * 84) XPCOM\nD. General IT\n85) 3-char identifiers 86) Advertising (Facebook,\nAd Engines, Ad Blockers, AdMob) 87) Animation 88) Antispam; PHP forums 89) Antivirus; database ac-\ncess # 90) Barcodes; browser en-\ngines # 91) Charting 92) Chat; messaging 93) Chinese web 94) Code analysis and gen-\neration 95) Computer memory and\ninterfaces 96) Console, terminal, COM 97) CPU and kernel 98) Cryptography 99) Date and time picker 100) DB Sharding, MongoDB sharding 101) Design patterns; formal architecture 102) DevOps 103) Drawing * 104) Drawing * 105) Forms (UI) 106) Glyphs; X11 and FreeType # 107) Grids and tables 108) HTTP auth 109) iBeacons 110) Image Manipulation 111) Image processing 112) Intel SIMD, Linear Algebra # 113) IO operations 114) Javascript selectors 115) JPEG and PNG 116) Media Players 117) Metaprogramming 118) Modern JS frontend (Bower, Grunt, Yeoman) 119) Names starting with \u201cm\u201d 120) Networking 121) OAuth; major web services # 122) Observer design pattern\n123) Online education; Moodle 124) OpenGL * 125) Parsers and compilers 126) Plotting 127) Pointers 128) POSIX Shell; VCS # 129) Promises and deferred execution; Angular # 130) Proof of concept 131) RDF and SGML parsing 132) Request and Response 133) Requirements and dependencies 134) Sensors; DIY devices 135) Sockets C API 136) Sockets, Networking 137) Sorting and searching 138) SQL database 139) SQL DB, XML in PHP projects 140) SSL 141) Strings 142) Testing with mocks 143) Text editor UI 144) Threads and concurrency 145) Typing suggestions and dropdowns 146) UI 147) Video player 148) VoIP 149) Web Media, Arch Packages # 150) Web posts 151) Web testing; crawling 152) Web UI 153) Wireless 154) Working with buffers 155) XML (SAX, XSL) 156) XMPP 157) .NET 158) Android Apps 159) Android UI 160) Apache Libraries for BigData 161) Apache Thrift 162) Arduino, AVR 163) ASP.NET * 164) ASP.NET *\nE. Technologies\n165) Backbone.js 166) Chardet (Python) 167) Cocos2D 168) Comp. vision; OpenCV 169) Cordova 170) CPython 171) Crumbs; cake(PHP) 172) cURL 173) DirectDraw 174) DirectX 175) Django Web Apps, CMS 176) Drupal 177) Eclipse SWT 178) Emacs configs 179) Emoji and Dojo # 180) Facebook; Parse SDK # 181) ffmpeg 182) FLTK 183) Fonts 184) FPGA, Verilog 185) FreeRTOS (Embedded) 186) Glib 187) Ionic framework, Cordova 188) iOS Networking 189) iOS Objective-C API 190) iOS UI 191) Jasmine tests, JS exercises, exercism # 192) Java GUI 193) Java Native Interface 194) Java web servers 195) Javascript AJAX,\nJavascript DOM manipulation\n196) Joomla 197) JQuery 198) jQuery Grid 199) Lex, Yacc compiler 200) libav / ffmpeg 201) Linear algebra libraries 202) Linux Kernel, Linux Wireless 203) Lodash 204) MFC Desktop Applica-\ntions\n205) Minecraft mods 206) Monads 207) OpenCL 208) OpenGL * 209) PHP sites written by\nnon-native English people\n210) PIC32 211) Portable Document Format 212) Puppet 213) Pusher.com Apps 214) Python packaging 215) Python scientific stack 216) Python scrapers 217) Qt * 218) Qt * 219) React 220) ROS (Robot Operating System) 221) Ruby On Rails Apps 222) SaltStack 223) Shockwave Flash 224) Spreadsheets (Excel) 225) Spreadsheets with PHP 226) SQLite 227) STL, Boost 228) STM32 229) Sublime Extensions 230) Symphony, Doctrine; NLP # 231) U-boot 232) Vim Extensions 233) Visual Basic, MSSQL 234) Web scraping 235) WinAPI 236) Wordpress * 237) Wordpress * 238) Wordpress-like frontend 239) Working with PDF in PHP 240) wxWidgets 241) Zend framework 242) zlib * 243) zlib *"}, {"heading": "F. Games", "text": "244) 3D graphics and Unity 245) Fantasy Creatures\n* Repeating topic with different key words, see section VI. # Dual topic, see section VI.\n246) Games 247) Hello World, Games 248) Minecraft 249) MMORPG 250) Pokemon 251) Puzzle games\n252) RPG, fantasy 253) Shooters (SDL) 254) Unity Engine 255) Unity3D Games 256) Web Games"}, {"heading": "APPENDIX D KEY WORDS AND REPOSITORIES BELONGING TO TOPIC #27 (MACHINE LEARNING, DATA SCIENCE)", "text": "Rank Word Rank Repository 0.313115 plot 1.000000 jingxia/kaggle yelp 0.303456 numpy 0.999998 Carreau/spicy 0.273759 plt 0.999962 zck17388/test 0.187565 figur 0.999719 jonathanekstrand/Python ... 0.181307 zeros 0.999658 skendrew/astroScanr 0.169696 matplotlib 0.999543 southstarj/YCSim 0.166166 dtype 0.999430 parteekDhream/statthermo... 0.165236 fig 0.999430 axellundholm/FMN050 0.159658 ylabel 0.999361 soviet1977/PriceList 0.153094 xlabel 0.999354 connormarrs/3D-Rocket-... 0.146327 subplot 0.999282 Holiver/matplot 0.144736 shape 0.999103 wetlife/networkx 0.132792 pyplot 0.999034 JingshiPeter/CS373 0.124264 scipy 0.998969 marialeon/los4mas2 0.120666 axis 0.998385 acnz/Project 0.110212 arang 0.998138 khintz/GFSprob 0.110049 mean 0.998123 claralusan/test aug 18 0.096037 reshap 0.997822 amcleod5/PythonPrograms 0.093182 range 0.997662 ericqh/deeplearning 0.084059 ylim 0.997567 laserson/stitcher 0.082812 linspac 0.996786 hs-jiang/MCA python 0.081260 savefig 0.996327 DianaSplit/Tracking 0.080978 xlim 0.995153 SivaGabbi/Ipython-Noteb... 0.080325 axes 0.994776 prov-suite/prov-sty 0.077891 legend 0.992801 bmoerker/EffectSizeEstim... 0.076858 bins 0.992558 natalink/machine learning 0.076140 panda 0.992324 olehermanse/INF1411-El... 0.076043 astyp 0.991026 fonnesbeck/scipy2014 tut... 0.075235 pylab 0.990514 fablab-paderborn/device-... 0.073265 ones 0.989586 mqchau/dataAnalysis 0.072214 xrang 0.988722 acemaster/Image-Process... 0.072196 len 0.988514 henryoswald/Sin-Plot 0.069818 float 0.987874 npinto/virtualenv-bootstrap 0.065453 linewidth 0.987039 ipashchenko/test datajoy 0.065453 linalg 0.986802 Ryou-Watanabe/practice 0.065322 norm 0.986272 mirthbottle/datascience-... 0.064042 hist 0.986039 hglabska/doktorat 0.062975 label 0.985837 parejkoj/yaledemo 0.061608 sum 0.985246 grajasumant/python 0.060443 cmap 0.985173 aaronspring/Scientific-Py... 0.059155 scatter 0.985043 asesana/plots 0.058877 fontsiz 0.984838 Sojojo83/SJFirstRepository 0.057343 self 0.983808 Metres/MetresAndThtu... 0.057328 none 0.983393 e-champenois/pySurf 0.056908 true 0.983170 pawel-kw/dexy-latex-ex... 0.056292 xtick 0.983074 keiikegami/envelopetheorem 0.051978 figsiz 0.982967 msansa/test 0.051359 sigma 0.982904 qdonnellan/spyder-examples 0.050785 ndarray 0.981725 qiuwch/PythonNotebook... 0.050586 sqrt 0.981156 rescolo/getdaa"}], "references": [{"title": "Improved consistent sampling, weighted minhash and l1 sketching", "author": ["S. Ioffe"], "venue": "Proceedings of the 2010 IEEE International Conference on Data Mining, ICDM \u201910, (Washington, DC, USA), pp. 246\u2013255, IEEE Computer Society, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Additive regularization of topic models", "author": ["K. Vorontsov", "A. Potapenko"], "venue": "Machine Learning, vol. 101, no. 1, pp. 303\u2013323, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The open source software community structure", "author": ["J. Xu", "S. Christley", "G. Madey"], "venue": "Proceedings of the North American Association for Computation Social and Organization Science, NAACSOS \u201905, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Ecosystems in github and a method for ecosystem identification using reference coupling", "author": ["K. Blincoe", "F. Harrison", "D. Damian"], "venue": "Proceedings of the 12th Working Conference on Mining Software Repositories, MSR \u201915, (Piscataway, NJ, USA), pp. 202\u2013207, IEEE Press, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The ghtorrent dataset and tool suite", "author": ["G. Gousios"], "venue": "Proceedings of the 10th Working Conference on Mining Software Repositories, MSR \u201913, (Piscataway, NJ, USA), pp. 233\u2013236, IEEE Press, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Reverse Engineering Software Ecosystems", "author": ["M. Lungu"], "venue": "PhD thesis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "The promises and perils of mining github", "author": ["E. Kalliamvakou", "G. Gousios", "K. Blincoe", "L. Singer", "D.M. German", "D. Damian"], "venue": "Proceedings of the 11th Working Conference on Mining Software Repositories, MSR 2014, (New York, NY, USA), pp. 92\u2013101, ACM, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring topic models in software engineering data analysis: A survey", "author": ["X. Sun", "X. Liu", "B. Li", "Y. Duan", "H. Yang", "J. Hu"], "venue": "2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD), pp. 357\u2013362, May 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Using topic models to support software maintenance", "author": ["S. Grant", "J.R. Cordy", "D.B. Skillicorn"], "venue": "2012 16th European Conference on Software Maintenance and Reengineering, pp. 403\u2013408, March 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Examining the relationship between topic model similarity and software maintenance", "author": ["S. Grant", "J.R. Cordy"], "venue": "2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE), pp. 303\u2013307, Feb 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Explaining software defects using topic models", "author": ["T.-H. Chen", "S.W. Thomas", "M. Nagappan", "A.E. Hassan"], "venue": "Proceedings of the 9th IEEE Working Conference on Mining Software Repositories, MSR \u201912, (Piscataway, NJ, USA), pp. 189\u2013198, IEEE Press, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated concept location using independent component analysis", "author": ["S. Grant", "J.R. Cordy", "D. Skillicorn"], "venue": "2008 15th Working Conference on Reverse Engineering, pp. 138\u2013142, Oct 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining concepts from code with probabilistic topic models", "author": ["E. Linstead", "P. Rigor", "S. Bajracharya", "C. Lopes", "P. Baldi"], "venue": "Proceedings of the Twenty-second IEEE/ACM International Conference on Automated Software Engineering, ASE \u201907, (New York, NY, USA), pp. 461\u2013464, ACM, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "An application of latent dirichlet allocation to analyzing software evolution", "author": ["E. Linstead", "C. Lopes", "P. Baldi"], "venue": "2008 Seventh International Conference on Machine Learning and Applications, pp. 813\u2013818, Dec 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling the evolution of topics in source code histories", "author": ["S.W. Thomas", "B. Adams", "A.E. Hassan", "D. Blostein"], "venue": "Proceedings of the 8th Working Conference on Mining Software Repositories, MSR \u201911, (New York, NY, USA), pp. 173\u2013182, ACM, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Using latent semantic analysis to identify similarities in source code to support program understanding", "author": ["J.I. Maletic", "A. Marcus"], "venue": "Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000, pp. 46\u201353, 2000.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic software clustering via latent semantic analysis", "author": ["J.I. Maletic", "N. Valluri"], "venue": "14th IEEE International Conference on Automated Software Engineering, pp. 251\u2013254, Oct 1999.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Semantic clustering: Identifying topics in source code", "author": ["A. Kuhn", "S. Ducasse", "T. G\u0131\u0301rba"], "venue": "Inf. Softw. Technol., vol. 49, pp. 230\u2013243, Mar. 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Mining software repositories using topic models", "author": ["S.W. Thomas"], "venue": "Proceedings of the 33rd International Conference on Software Engineering, ICSE \u201911, (New York, NY, USA), pp. 1138\u20131139, ACM, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating source code summarization techniques: Replication and expansion", "author": ["B.P. Eddy", "J.A. Robinson", "N.A. Kraft", "J.C. Carver"], "venue": " 2013 21st International Conference on Program Comprehension (ICPC), pp. 13\u201322, May 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving topic model source code summarization", "author": ["P.W. McBurney", "C. Liu", "C. McMillan", "T. Weninger"], "venue": "Proceedings of the 22Nd International Conference on Program Comprehension, ICPC 2014, (New York, NY, USA), pp. 291\u2013294, ACM, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Itmviz: Interactive topic modeling for source code analysis", "author": ["A.M. Saeidi", "J. Hage", "R. Khadka", "S. Jansen"], "venue": "Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension, ICPC \u201915, (Piscataway, NJ, USA), pp. 295\u2013298, IEEE Press, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Msr4sm: Using topic models to effectively mining software repositories for software maintenance tasks", "author": ["X. Sun", "B. Li", "H. Leung", "B. Li", "Y. Li"], "venue": "Inf. Softw. Technol., vol. 66, pp. 1\u201312, Oct. 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic extraction of a wordnet-like identifier network from software", "author": ["V. Prince", "C. Nebut", "M. Dao", "M. Huchard", "J.-R. Falleri", "M. Lafourcade"], "venue": "International Conference on Program Comprehension, vol. 00, pp. 4\u201313, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying word relations in software: A comparative study of semantic similarity tools", "author": ["G. Sridhara", "L. Pollock", "E. Hill", "K. Vijay-Shanker"], "venue": "International Conference on Program Comprehension, vol. 00, pp. 123\u2013132, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Inferring semantically related words from software context", "author": ["J. Yang", "L. Tan"], "venue": "Proceedings of the 9th IEEE Working Conference on Mining Software Repositories, MSR \u201912, (Piscataway, NJ, USA), pp. 161\u2013170, IEEE Press, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatically mining software-based, semantically-similar words from commentcode mappings", "author": ["M.J. Howard", "S. Gupta", "L. Pollock", "K. Vijay-Shanker"], "venue": "Proceedings of the 10th Working Conference on Mining Software Repositories, MSR \u201913, (Piscataway, NJ, USA), pp. 377\u2013 386, IEEE Press, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "On the use of domain terms in source code", "author": ["S. Haiduc", "A. Marcus"], "venue": "International Conference on Program Comprehension, vol. 00, pp. 113\u2013 122, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining search topics from a code search engine usage log", "author": ["S. Bajracharya", "C. Lopes"], "venue": "Proceedings of the 2009 6th IEEE International Working Conference on Mining Software Repositories, MSR \u201909, (Washington, DC, USA), pp. 111\u2013120, IEEE Computer Society, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Snowball: A language for stemming algorithms", "author": ["M.F. Porter"], "venue": "2001.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "A comparative study of stemming algorithms", "author": ["A.G. Jivani"], "venue": "vol. 2 (6) of IJCTA, pp. 1930\u20131938, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1930}, {"title": "Scalable parallel programming with cuda", "author": ["J. Nickolls", "I. Buck", "M. Garland", "K. Skadron"], "venue": "Queue, vol. 6, pp. 40\u201353, Mar. 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "ekzhu/datasketch.\u201d https://github.com/ekzhu/datasketch. Appeared in 2015-03", "author": ["E. Zhu"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, Mar. 2003.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2003}, {"title": "The expectation maximization algorithm", "author": ["F. Dellaert"], "venue": "tech. rep., Georgia Institute of Technology, 2002.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "On clusters in open source ecosystems", "author": ["S. Syed", "S. Jansen"], "venue": "2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Particularly, we describe how \u201dWeighted MinHash\u201d algorithm [2] helps to filter fuzzy duplicates and how an Additive Regularized Topic Model (ARTM) [3] can be efficiently trained.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "Particularly, we describe how \u201dWeighted MinHash\u201d algorithm [2] helps to filter fuzzy duplicates and how an Additive Regularized Topic Model (ARTM) [3] can be efficiently trained.", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] studied GitHub ecosystems using reference", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "coupling over the GHTorrent dataset [6] which contained 2.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Lungi [7] conducted an in-depth study of software ecosystems in 2009, the year when GitHub appeared.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "[8] along with other valuable concerns.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 157, "endOffset": 161}, {"referenceID": 11, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 220, "endOffset": 224}, {"referenceID": 14, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 226, "endOffset": 230}, {"referenceID": 15, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 264, "endOffset": 268}, {"referenceID": 16, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 332, "endOffset": 336}, {"referenceID": 17, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 338, "endOffset": 342}, {"referenceID": 18, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 344, "endOffset": 348}, {"referenceID": 19, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 362, "endOffset": 366}, {"referenceID": 20, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 368, "endOffset": 372}, {"referenceID": 21, "context": "Topic modeling of source code has been applied to a variety of problems reviewed in [9]: improvement of software maintenance [10], [11], defects explanation [12], concept analysis [13], [14], software evolution analysis [15], [16], finding similarities and clones [17], clustering source code and discovering the internal structure [18], [19], [20], summarizing [21], [22], [23].", "startOffset": 374, "endOffset": 378}, {"referenceID": 22, "context": "The usage of topic modeling [24] focused on improving software maintenance and was evaluated on 4 software projects.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Concepts were extracted using a corpus of 24 projects in [25].", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "[26], Yang and Tan [27], Howard et.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26], Yang and Tan [27], Howard et.", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "[28] considered comments and/or names to find semantically similar terms; Haiduc and Marcus [29] researched common domain terms appearing in source code.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] considered comments and/or names to find semantically similar terms; Haiduc and Marcus [29] researched common domain terms appearing in source code.", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "Bajracharya and Lopes [30] trained a topic model on the year long usage log of Koders, one of the major commercial code search engines.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "Since we are working with natural language that is predominantly English we have applied the Snowball stemmer [37] from the Natural Language Toolkit (NLTK) [38].", "startOffset": 110, "endOffset": 114}, {"referenceID": 30, "context": "The Snowball algorithm was chosen based on the comparative study by Jivani [39].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Weighted MinHash was introduced by Ioffe in [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 31, "context": "We developed the MinHashCUDA [41] library and Python native extension which is the implementation of Weighted MinHash algorithm for NVIDIA GPUs using CUDA [42].", "startOffset": 155, "endOffset": 159}, {"referenceID": 32, "context": "We used the implementation of Weighted MinHash LSH from Datasketch [43].", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "Well-known LDA model [45] can be reproduced as ARTM too.", "startOffset": 21, "endOffset": 25}, {"referenceID": 34, "context": "The variables \u03a6 and \u0398 can be effectively calculated using the iterative expectation maximization algorithm [46].", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Vorontsov shows in [3] that ARTM is trained best if the regularizers are activated sequentially, with a lag relative to each other.", "startOffset": 19, "endOffset": 22}, {"referenceID": 35, "context": "Future work may involve experimentation with clustering the repositories in the topic space and comparison with clusters based on dependency or social graphs [49].", "startOffset": 158, "endOffset": 162}], "year": 2017, "abstractText": "Programming languages themselves have a limited number of reserved keywords and character based tokens that define the language specification. However, programmers have a rich use of natural language within their code through comments, text literals and naming entities. The programmer defined names that can be found in source code are a rich source of information to build a high level understanding of the project. The goal of this paper is to apply topic modeling to names used in over 13.6 million repositories and perceive the inferred topics. One of the problems in such a study is the occurrence of duplicate repositories not officially marked as forks (obscure forks). We show how to address it using the same identifiers which are extracted for topic modeling. We open with a discussion on naming in source code, we then elaborate on our approach to remove exact duplicate and fuzzy duplicate repositories using Locality Sensitive Hashing on the bag-of-words model and then discuss our work on topic modeling; and finally present the results from our data analysis together with open-access to the source code, tools and datasets.", "creator": "LaTeX with hyperref package"}}}