{"id": "1704.01314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF", "abstract": "we present a character - based model for joint bit segmentation and pattern pos tagging for chinese. the bidirectional rnn - crf architecture for general sequence tagging implementations is also adapted and applied with novel vector representations of chinese characters that capture rich contextual information and involve lower - than - character level features. the proposed model is extensively evaluated and compared with a state - of - the - art metadata tagger respectively on ctb5, ctb9 and ud chinese. the experimental results indicate that our model is accurate and robust representation across datasets in different sizes, genres and sequence annotation schemes. likewise we obtain state - of - the - art performance on ctb5, achieving 94. 38 f1 - score for joint segmentation and consistent pos tagging.", "histories": [["v1", "Wed, 5 Apr 2017 08:58:44 GMT  (31kb)", "https://arxiv.org/abs/1704.01314v1", "8 pages plus 2 pages references and 1 page appendix, 3 figures, submitted to EMNLP 2017"], ["v2", "Thu, 6 Apr 2017 22:53:37 GMT  (31kb)", "http://arxiv.org/abs/1704.01314v2", "8 pages plus 2 pages references and 1 page appendix, 3 figures, submitted to EMNLP 2017"], ["v3", "Tue, 12 Sep 2017 09:29:15 GMT  (32kb)", "http://arxiv.org/abs/1704.01314v3", "10 pages plus 1 page appendix, 3 figures, IJCNLP 2017"]], "COMMENTS": "8 pages plus 2 pages references and 1 page appendix, 3 figures, submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yan shao", "christian hardmeier", "j\\\"org tiedemann", "joakim nivre"], "accepted": false, "id": "1704.01314"}, "pdf": {"name": "1704.01314.pdf", "metadata": {"source": "CRF", "title": "Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF", "authors": ["Yan Shao"], "emails": ["joakim.nivre}@lingfil.uu.se", "jorg.tiedemann@helsinki.fi"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n01 31\n4v 3\n[ cs\n.C L\n] 1\n2 Se\np 20\n17\njoint segmentation and POS tagging for Chinese. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and sub-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our model is accurate and robust across datasets in different sizes, genres and annotation schemes. We obtain stateof-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and POS tagging."}, {"heading": "1 Introduction", "text": "Word segmentation and part-of-speech (POS) tagging are core steps for higher-level natural language processing (NLP) tasks. Given the raw text, segmentation is applied at the very first step and POS tagging is performed on top afterwards. As by convention the words in Chinese are not delimited by spaces, segmentation is non-trivial, but its accuracy has a significant impact on POS tagging. Moreover, POS tags provide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008).\nPOS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a\njoint model which predicts the combinatory labels of segmentation boundaries and POS tags at the character level. Joint segmentation and POS tagging becomes a standard character-based sequence tagging problem and therefore the general machine learning algorithms for structured prediction can be applied.\nThe bidirectional recurrent neural network (RNN) using conditional random fields (CRF) (Lafferty et al., 2001) as the output interface for sentence-level optimisation (BiRNN-CRF) achieves state-of-the-art accuracies on various sequence tagging tasks (Huang et al., 2015; Ma and Hovy, 2016) and outperforms the traditional linear statistical models. RNNs with gated recurrent cells, such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014) are capable of capturing long dependencies and retrieving rich global information. The sequential CRF on top of the recurrent layers ensures that the optimal sequence of tags over the entire sentence is obtained.\nIn this paper, we model joint segmentation and POS tagging as a fully character-based sequence tagging problem via predicting the combinatory labels. The BiRNN-CRF architecture is adapted and applied. The Chinese characters are fed into the neural networks as vector representations. In addition to utilising the pre-trained character embeddings, we propose a concatenated ngram-representation of the characters. Furthermore, sub-character level information, namely radicals and orthographical features extracted by convolutional neural networks (CNNs), are also incorporated and tested. Three datasets of different sizes, genres and with different annotation schemes are employed for evaluation. Our model is thoroughly evaluated and compared with the joint segmentation and POS tagging model in ZPar\n(Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our proposed model outperforms ZPar on all the datasets in terms of accuracy.\nThe main contributions of this work include: 1. We apply the BiRNN-CRF model for general sequence tagging to joint segmentation and POS tagging for Chinese and achieve state-of-the-art accuracy. The experimental results show that our tagger is robust and accurate across datasets of different sizes, genres and annotation schemes. 2. We propose a novel approach for vector representations of characters that leads to substantial improvements over the baseline model. 3. Additional improvements are obtained via exploring the feasibility of utilising sub-character level information. 4. We provide an open-source implementation of our method along with pre-trained character embeddings.1"}, {"heading": "2 Model", "text": ""}, {"heading": "2.1 Neural Network Architecture", "text": "Our baseline model is an adaptation of BiRNNCRF. As illustrated in Figure 1, the Chinese characters are represented as vectors and fed into the bidirectional recurrent layers. The character representations will be described in detail in the following sections. For the recurrent layer, we employ GRU as the basic recurrent unit as it has similar functionalities but fewer parameters compared to LSTM (Chung et al., 2014). Dropout (Srivastava et al., 2014) is applied to the outputs of the bidirectional recurrent layers. The outputs are concatenated and passed to the first-order chain CRF layer. The optimal sequence of the combinatory labels is predicted at the end. There is a post processing step to retrieve both segmentation and POS tags from the combinatory tags."}, {"heading": "2.2 Tagging Scheme", "text": "Following the work of Kruengkrai et al. (2009a), the employed tags indicating the word boundaries are B, I, E, S representing a character at the beginning, inside, end of a word or as a singlecharacter word. The CRF layer models conditional scores over all possible combinatory labels given the input characters. Incorporating the transition scores between the successive labels, the op-\n1 https://github.com/yanshao9798/tagger\n(too) (hot)(summer)\ntimal sequence can be obtained efficiently via the Viterbi algorithm both for training and decoding.\nThe time complexity for the Viterbi algorithm is linear with respect to the sentence length n as O(k2n), where k is constant and equals to the total number of combinatory labels. The efficiency can be improved if we reduce k. For some POS tags, combining them with the full boundary tags is redundant. For instance, only the functional word \u7684 can be tagged as DEG in Chinese Treebank (Xue et al., 2005). Since it is a single-character word, combinatory tags of B-DEG, I-DEG, and E-DEG never occur in the experimental data and should therefore be pruned to reduce the search space. Similarly, if the maximum length of words under a given POS tag is two in the training data, we prune the corresponding label."}, {"heading": "2.3 Character Representations", "text": "We propose three different approaches to effectively represent Chinese characters as vectors for the neural network."}, {"heading": "2.3.1 Concatenated N-gram", "text": "The prevalent character-based neural models assume that larger spans of text, such as words and\nn-grams, can be represented by the sequence of characters that they consist of. For example, the vector representation Vm,n of a span cm,n is obtained by passing the vector representations vi of the characters ci to a functions f as:\nVm,n = f(vm, vm+1, ..., vn) (1)\nwhere f is usually an RNN (Ling et al., 2015) or a CNN (dos Santos and Zadrozny, 2014).\nIn this paper, instead of completely relying on the BiRNN to extract contextual features from context-free character representations, we encode rich local information in the character vectors via employing the incrementally concatenated n-gram representation as demonstrated in Figure 2. In the example, the vector representation of the pivot character \u592a in the given context is the concatenation of the context-free vector representation Vi,i of \u592a itself along with Vi\u22121,i of the bigram \u5929\u592a as well as Vi\u22121,i+1 of the trigram \u5929\u592a\u70ed.\nInstead of constructing the vector representation Vm,n of an n-gram cm,n from the character representations as in Equation 1, Vm,n in different orders, such as Vi,i, Vi\u22121,i, and Vi\u22121,i+1, are randomly initialised separately. We use a single special vector to represent all the unknown n-grams per order. The n-grams in different orders are then concatenated incrementally to form up the vector representations of a Chinese character in the given context, which is passed further to the recurrent layers. As shown in Figure 2, the neighbouring characters on both sides of the pivot character are taken into account."}, {"heading": "2.3.2 Radicals and Orthographical Features", "text": "Chinese characters are logograms. As opposed to alphabetical languages, there is rich information\nencrypted in the graphical components. For instance, the Chinese characters that share the same part \u9485 (gold) are all somewhat related to metals, such as \u94f6 (silver), \u94c1 (iron), \u9488 (needle) and so on. The shared part \u9485 is known as the radical, which functions as a semantic indicator. Hence, we investigate the effectiveness of using the information below the character level for our task.\nRadicals are first represented as randomly initialised vectors and concatenated as parts of the character representations. Radicals are traditionally used as indices in Chinese dictionaries. In our approach, they are retrieved via the unicode representation of Chinese characters as the characters that share the same radical are grouped together. They are organised in consistent with the categorisation in Kangxi Dictionary (\u5eb7\u7199\u5b57\u5178), in which all the Chinese characters are grouped under 214 different radicals. We only employ the radicals of the common characters in the unicode range of (U+4E00, U+9FFF). For the characters out of the range and the non-Chinese characters, we use a single special vector as their radical representations.\nAdditionally, instead of presuming that only radicals encode sub-character level information, we use convolutional neural networks (CNNs) to extract graphical features from scratch by regarding the Chinese characters as pictures and feed their pixels as the input. As illustrated in Figure 3, there are two convolutional layers, both followed by a max-pooling layer. The output of the second max-pooling layer is reshaped and passed to a regular fully-connected layer. Dropout is applied to the output of the fully-connected layer. The output is then concatenated as parts of the character representation. The CNNs are trained jointly with the main network."}, {"heading": "2.3.3 Pre-trained Character Embeddings", "text": "The context-free vector representations of single characters introduced in section 2.3.1 can be replaced by pre-trained character embeddings retrieved from large corpora. We employ GloVe (Pennington et al., 2014) to train our character embeddings on Wikipedia2 and the freely available Sogou News Corpora (SogouCS).3 We use randomly initialised vectors as the representations of the characters that are not in the embedding vocabulary. Pre-trained embeddings for higher-order n-grams are not employed in this paper."}, {"heading": "2.4 Ensemble Decoding", "text": "At the final decoding phase, we use ensemble decoding, a simple averaging technique, to mitigate the deviations led by random weight initialisation of the neural network. For the chain CRF decoder, the final sequence of the combinatory tags y is obtained via the conditional scores S(yi|xi) and the transition scores T (yi, yj) given the input sequence x. Instead of computing the optimal sequence with respect to the scores returned by a single model, both the conditional scores and transition scores are averaged over four models with identical parameter settings that are trained independently:\ny\u2217 = argmax y\u2208L(x)\np(y|x; \u00af{S}, \u00af{T}) (2)\nEnsemble decoding is only applied to the best performing model according to the feature experiments at the final testing phase in this paper."}, {"heading": "3 Implementation", "text": "Our neural networks are implemented using the TensorFlow 1.2.0 library (Abadi et al., 2016). We group the sentences with similar lengths into the same buckets and the sentences in the same bucket are padded to the same length accordingly. We construct sub-computational graphs respectively for each bucket. The training and tagging speed of our neural network on GPU devices can be drastically improved thanks to the bucket model. The training time is proportional to both the size of the training set and the number of POS tags.\nTable 1 shows the adopted hyper-parameters. We use one set of parameters for all the experiments on different datasets. The weights of the\n2https://dumps.wikimedia.org/ 3http://www.sogou.com/labs/resource/cs.php\nneural networks, including the randomly intialised embeddings, are initialised using the scheme introduced in Glorot and Bengio (2010). The network is trained with the error back-propagation algorithm. All the embeddings are fine-tuned during training by back-propagating gradients. Adagrad (Duchi et al., 2011) with mini-batches is employed for optimisation with the initial learning rate \u03b70 = 0.1, which is updated with a decay rate \u03c1 = 0.05 as \u03b7t = \u03b70 \u03c1(t\u22121)+1 , where t is the index of the current epoch.\nThe model is optimised with respect to the performance on the development sets. F1-scores of both segmentation (F1Seg) and joint POS tagging (F1Seg&Tag) are employed as F1Seg \u2217F1Seg&Tag to measure the performance of the model after each epoch during training. In our experiments, the models are trained for 30 epochs. To ensure that the weights are well optimised, we only adopt the best epoch after the model is trained at least for 5 epochs."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "We employ three different datasets for our experiments, namely Chinese Treebank (Xue et al., 2005) 5.0 (CTB5) and 9.0 (CTB9) along with the Chinese section in Universal Dependencies (UD Chinese) (Nivre et al., 2016) of version 1.4.\nCTB5 is the most employed dataset for joint segmentation and POS tagging in previous research. It is composed of newswire data. We follow the conventional split of the dataset as in Jiang et al. (2008); Kruengkrai et al. (2009a);\nZhang and Clark (2010). CTB9 consists of source texts in various genres, CTB5 is a subset of it. We split CTB9 by referring to the partition of CTB7 in Wang et al. (2011). We extend the training, development and test sets from CTB5 by adding 80% of the new data in CTB9 to training and 10% each to development and test. The doublechecked files are all placed in the test set. The detailed splitting information can be found in Table 10 in Appendix. UD Chinese has both universal and language-specific POS tags. They are not predicted jointly in this paper. For the sake of convenience, we refer the universal tags as UD1 and the language-specific ones as UD2 in the following sessions. To make the model benefit from the pre-trained character embeddings, we convert the texts in UD Chinese from traditional Chinese into simplified Chinese.\nTable 2 shows brief statistics of the employed datasets in numbers of words. The out-ofvocabulary (OOV) words are counted regardless of the POS tags. We can see that the size of UD Chinese is much smaller and it has a notably higher OOV rate than the two CTB datasets."}, {"heading": "4.2 Experimental Results", "text": "Both segmentation (Seg) and joint segmentation and POS tagging (Seg&Tag) are evaluated in our experiments.4 We employ word-level recall (R), precision (P) and F1-score (F) as the evaluation metrics. A series of feature experiments are carried out on the development sets to evaluate the effectiveness of the proposed approaches for vector representations of the characters. Finally, the best performing model according to the feature experiment is applied to the test sets in the forms of single as well as ensemble and compared with ZPar.\n4The evaluation script is downloaded from: http://people.sutd.edu.sg/ yue zhang/doc/doc/joint files /evaluate.py"}, {"heading": "4.2.1 Feature Experiments", "text": "Table 3 shows the evaluation results of using concatenated n-grams up to different orders as the character representations. By introducing 2- grams, we can obtain vast improvements over solely using the conventional character embeddings, which indicates that not all the local information can be effectively captured by the BiRNN using context-free character representations. Utilising the concatenated n-grams ensures that the same character has different but yet closely related representations in different contexts, which is an effective way to encode contextual features.\nFrom the table, we see that notable improvements can be achieved further via employing 3- grams. 4-grams still help but only to CTB9 while adding 5-grams achieves almost no improvement on any of the datasets. The results imply that concatenating higher-order n-grams can be detrimental, especially on datasets in smaller sizes due to the fact that higher-order n-grams are more sparse in the training data and their vector representations cannot be trained well enough. Besides, adopting higher-order n-grams also substantially increases the numbers of weights and therefore both training and decoding become less efficient. Under the circumstances, we consider that 3-gram model is optimal for our task and it is employed in the following experiments for all the datasets.\nThe concatenated n-grams have a bigger size compared to the basic character representation. We conduct one additional experiment using a basic 1-gram character model with a larger character vector size of 300. The evaluation scores are similar to the basic character model with the size of 64, which shows that the improvements obtained by the n-gram model are not matched by enlarging the size of the vector representation.\nThe evaluation scores of the sub-character level features are reported in Table 4. The relevant features are added on top of the 3-gram model. Employing radicals and graphical features achieves similar improvements for segmentation while utilising radicals obtains better results for joint POS tagging on CTB5. However, radicals are not a very effective feature on CTB9, UD1 and UD2 whereas a notable enhancement is observed when employing graphical features on UD1. Using CNNs to extract graphical features is computationally much more expensive than simply adopting radicals via a lookup table, especially when GPU is not avail-\nable.\nFrom Table 5, we can learn that employing pretrained embeddings as initial vector representations for the characters achieves improvements in general, whereas the improvements are comparatively smaller if the the concatenated n-gram representations and the radicals are added. Additionally, the improvements obtained on UD Chinese are more significant than on CTBs, which indicates that the pre-trained character embeddings are more beneficial to the datasets in smaller sizes.\nIn general, the feature experiments indicate that the proposed Chinese character representations are all sensitive to dataset size. Using higher-order n-grams requires more data for training. On the other hand, the pre-trained embeddings are more vital if the dataset is small. In addition, the different representations are sensitive to tagging schemes as the evaluation results on UD1 and UD2 are quite diverse. Taking both robustness and efficiency into consideration, we select 3-grams along with radicals and pre-trained character embeddings as the best setting for final evaluation."}, {"heading": "4.2.2 Final Results", "text": "Table 6 shows the final scores on the test sets. The complete evaluation results in precision, re-\ncall and F1-scores are contained in Table 11 and Table 12 in Appendix. Our system is compared with ZPar. We retrained a ZPar model on CTB5 that reproduces the evaluation scores reported in Zhang and Clark (2010). We also modified the source code so that it is applicable to CTB9 and UD Chinese. In addition, we perform the mid-p McNemar\u2019s test (Fagerland et al., 2013) to examine the statistical significances.\nAs shown in Table 6, the single model is worse than the ensemble model but still outperforms ZPar on all the tested datasets. ZPar incorporates discrete local features at both character and word levels and employs structured perceptron for global optimisation, whereas we encode rich local information in the character representations and employ BiRNN to effectively extract global features and capture long term dependencies. The chain CRF layer is used for sentencelevel optimisation, which functions similarly to structured perceptron. As opposed to the taggers built with traditional machine learning algorithms, our model avoids heavy feature engineering and benefits from large plain texts via utilising pre-trained character embeddings. It is also very flexible to add sub-character level features as parts of the character representations. The model\nperforms very well despite being fully character based. Moreover, it has clear advantages when applied to smaller datasets like UD Chinese, while the prevalence is much smaller on CTB5.\nBoth our model and ZPar segment OOV words in UD Chinese with higher accuracies than the ones in CTBs despite that UD Chinese is notably smaller and the overall OOV rate is higher. Compared to CTB, the words in UD Chinese are more fine-grained and the average word length is shorter, which makes it easier for the tagger to correctly segment the OOV words as Zhang et al. (2016) show that the longer words are more difficult to be segmented correctly. For joint POS tagging for OOV words, the two systems both perform significantly better on CTB5 as it is only composed of news text.\nIn general, our model is more robust to OOV words than ZPar, except that ZPar yields better result for segmentation by a small margin on CTB9. ZPar also obtains higher accuracy for joint POS tagging than the single model on CTB9. The differences between ZPar and our model for both segmentation and POS tagging are more substantial on UD Chinese, which indicates that our model is relatively more advantageous for handling OOV words when the training sets are small, whereas ZPar is able to perform equally well when substantial amount of training data is available as they achieve similar results on the CTB sets.\nThe single model is further improved by ensemble-averaging four independently trained models. The improvements are not drastic but they are observed systematically across all the datasets. In general, ensemble decoding is beneficial to handling OOV words as well except that a small drop for segmentation on CTB5 is observed.\nTable 7 displays the evaluation of the ensemble model and ZPar on the decomposed test sets\nof CTB9 in different genres. Our model surpasses ZPar on all the genres in both segmentation and joint POS tagging. The differences are subtle on the genres in which the texts are normalised, such as News and Broadcast News. This, to a very large extent, explains why our model is only marginally better than ZPar on CTB5, whereas the experimental results reveal that our model is substantially better at processing non-standard text as it yields significantly higher scores on Conversations, Short Messages and Weblogs. The evaluation results of both our model and ZPar vary substantially across different genres as some genres are fundamentally more challenging to process.\nOur models are compared with the previous best-performing systems on CTB5 in Table 8. Our models are not optimised particularly with respect to CTB5 but still yield competitive results, especially for joint POS tagging. We are the first to report evaluation scores on CTB9 and UD Chinese."}, {"heading": "4.3 Tagging Speed", "text": "Our joint segmentation and POS tagger is very efficient with GPU devices and can be practically\nused for processing very large files. The memory demand of decoding is drastically milder compared to training, a large batch size therefore can be employed. The tagger takes constant time to build the sub-computational graphs and load the weights.\nWith bucket size of 10 and batch size of 500, Table 9 shows the tagging speed of the tagger using a single Tesla K80 GPU card and the pre-trained model on CTB5. The tagging speed of ZPar is also presented for comparison. GPU devices are not supported by ZPar and therefore the tagging speed is calculated using an Intel Core i7 CPU."}, {"heading": "5 Related Work", "text": "The fundamental BiRNN-CRF architecture is task-independent and has been applied to many sequence tagging problems on Chinese. Peng and Dredze (2016) adopt the model for Chinese segmentation and named entity recognition in the context of multi-task and multi-domain learning. Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition. Ma and Sun (2016) use a similar architecture but feed the Chinese characters pairwise as edge embeddings instead. Their model is applied respectively to chunking, segmentation and POS tagging.\nZheng et al. (2013) model joint Chinese segmentation and POS tagging via predicting the combinatory segmentation and POS tags. They\nemploy the adaptation of the feed forward neural network introduced in Collobert et al. (2011) that only extracts local features in a context window. A perceptron-style training algorithm is employed for sentence level optimisation, which is the same as the training algorithm of the BiRNNCRF model. Their proposed model is not evaluated on CTB5 and therefore difficult to be compared with our system. Kong et al. (2015) apply segmental recurrent neural networks to joint segmentation and POS tagging but the evaluation results are substantially below the state-of-the-art on CTB5.\nBojanowski et al. (2016) retrieve word embeddings via representing words as a bag of character n-grams for morphologically rich languages. A similar character n-gram model is proposed by Wieting et al. (2016). Sun et al. (2014) attempt to encode radical information into the conventional character embeddings. The radicalenhanced embeddings are employed and evaluated for Chinese segmentation. The results show that radical-enhanced embeddings outperform both skip-ngram and continues bag-of-word (Mikolov et al., 2013) in word2vec."}, {"heading": "6 Conclusion", "text": "We adapt and apply the BiRNN-CRFmodel for sequence tagging in NLP to joint Chinese segmentation and POS tagging via predicting the combinatory tags of word boundaries and POS tags. Concatenated n-grams as well as sub-character features are employed along with the conventional pre-trained character embeddings as the vector representations for Chinese characters. The feature experiments indicate that concatenated ngrams contribute substantially. However, both radicals and graphical features as sub-character level information are less effective. How to incorporate the sub-character level information more effectively will be further explored in the future.\nThe proposed model is extensively evaluated on CTB5, CTB9 and UD Chinese. Despite the fact that different character representation approaches are sensitive to data size and tagging schemes, we use one set of hyper-parameters and universal feature settings so that the model is robust across datasets. The experimental results on the test sets show that our model outperforms ZPar which is built on structured perceptron on all the datasets. We obtain state-of-the-art performances on CTB5.\nThe results on UD Chinese and CTB9 also reveal that our model has great advantages in processing non-standard text, such as weblogs, forum text and short messages. Moreover, the implemented tagger is very efficient with GPU devices and therefore can be applied to tagging very large files."}, {"heading": "Acknowledgments", "text": "We acknowledge the computational resources provided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu). This work is supported by the Chinese Scholarship Council (CSC) (No. 201407930015)."}], "references": [{"title": "TensorFlow: A system for largescale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606 .", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(August):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Characterbased LSTM-CRF with radical-level features for Chinese named entity recognition", "author": ["Chuanhai Dong", "Jiajun Zhang", "Chengqing Zong", "Masanori Hattori", "Hui Di."], "venue": "International Conference on Computer Processing of Oriental", "citeRegEx": "Dong et al\\.,? 2016", "shortCiteRegEx": "Dong et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of The 31st International Conference on Machine Learning", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Joint Chinese word segmen", "author": ["toshi Isahara"], "venue": null, "citeRegEx": "Isahara.,? \\Q2009\\E", "shortCiteRegEx": "Isahara.", "year": 2009}, {"title": "Finding function in form", "author": ["Tiago Luis"], "venue": null, "citeRegEx": "Luis.,? \\Q2015\\E", "shortCiteRegEx": "Luis.", "year": 2015}, {"title": "A new recurrent neural CRF for learning non-linear edge features", "author": ["Shuming Ma", "Xu Sun."], "venue": "arXiv preprint arXiv:1611.04233 .", "citeRegEx": "Ma and Sun.,? 2016", "shortCiteRegEx": "Ma and Sun.", "year": 2016}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, page 10641074.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing", "author": ["Hwee Tou Ng", "Jin Kiat Low."], "venue": "Barcelona, Spain, pages 277\u2013", "citeRegEx": "Ng and Low.,? 2004", "shortCiteRegEx": "Ng and Low.", "year": 2004}, {"title": "Universal dependencies v1: A multilingual", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "Multi-task multi-domain representation learning for sequence tagging", "author": ["Nanyun Peng", "Mark Dredze."], "venue": "arXiv preprint arXiv:1608.02689 .", "citeRegEx": "Peng and Dredze.,? 2016", "shortCiteRegEx": "Peng and Dredze.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Chinese morphological analysis with character-level POS tagging", "author": ["Mo Shen", "Hongxiao Liu", "Daisuke Kawahara", "Sadao Kurohashi."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A stacked sub-wordmodel for joint Chinese word segmentation and part-of-speech tagging", "author": ["Weiwei Sun."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Associ-", "citeRegEx": "Sun.,? 2011", "shortCiteRegEx": "Sun.", "year": 2011}, {"title": "Radical-enhanced Chinese character embedding", "author": ["Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "International Conference", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data", "author": ["Yiou Wang", "Jun\u2019ichi Kazama", "Yoshimasa Tsuruoka", "Wenliang Chen", "Yujie Zhang", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "CHARAGRAM: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1607.02789 .", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "The Penn Chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer."], "venue": "Natural language engineering 11(02):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Joint word segmentation and POS tagging using a single perceptron", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Columbus, Ohio, pages 888\u2013896.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "A fast decoder for joint word segmentation and POS-tagging using a single discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Zhang and Clark.,? 2010", "shortCiteRegEx": "Zhang and Clark.", "year": 2010}, {"title": "Deep learning for Chinese word segmentation and POS tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, USA, pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008).", "startOffset": 93, "endOffset": 134}, {"referenceID": 25, "context": "Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008).", "startOffset": 93, "endOffset": 134}, {"referenceID": 13, "context": "Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a joint model which predicts the combinatory labels of segmentation boundaries and POS tags at the character level.", "startOffset": 94, "endOffset": 364}, {"referenceID": 11, "context": ", 2001) as the output interface for sentence-level optimisation (BiRNN-CRF) achieves state-of-the-art accuracies on various sequence tagging tasks (Huang et al., 2015; Ma and Hovy, 2016) and outperforms the traditional linear statistical models.", "startOffset": 147, "endOffset": 186}, {"referenceID": 2, "context": "RNNs with gated recurrent cells, such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014) are capable of capturing long dependencies and retrieving rich global information.", "startOffset": 138, "endOffset": 156}, {"referenceID": 26, "context": "(Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "For the recurrent layer, we employ GRU as the basic recurrent unit as it has similar functionalities but fewer parameters compared to LSTM (Chung et al., 2014).", "startOffset": 139, "endOffset": 159}, {"referenceID": 18, "context": "Dropout (Srivastava et al., 2014) is applied to the outputs of the bidirectional recurrent layers.", "startOffset": 8, "endOffset": 33}, {"referenceID": 23, "context": "For instance, only the functional word \u7684 can be tagged as DEG in Chinese Treebank (Xue et al., 2005).", "startOffset": 82, "endOffset": 100}, {"referenceID": 16, "context": "We employ GloVe (Pennington et al., 2014) to train our character embeddings on Wikipedia and the freely available Sogou News Corpora (SogouCS).", "startOffset": 16, "endOffset": 41}, {"referenceID": 0, "context": "0 library (Abadi et al., 2016).", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "Adagrad (Duchi et al., 2011) with mini-batches is employed for optimisation with the initial learning rate \u03b70 = 0.", "startOffset": 8, "endOffset": 28}, {"referenceID": 23, "context": "We employ three different datasets for our experiments, namely Chinese Treebank (Xue et al., 2005) 5.", "startOffset": 80, "endOffset": 98}, {"referenceID": 14, "context": "0 (CTB9) along with the Chinese section in Universal Dependencies (UD Chinese) (Nivre et al., 2016) of version 1.", "startOffset": 79, "endOffset": 99}, {"referenceID": 21, "context": "We split CTB9 by referring to the partition of CTB7 in Wang et al. (2011). We extend the training, development and test sets from CTB5 by adding 80% of the new data in CTB9 to training and 10% each to development and test.", "startOffset": 55, "endOffset": 74}, {"referenceID": 25, "context": "We retrained a ZPar model on CTB5 that reproduces the evaluation scores reported in Zhang and Clark (2010). We also modified the source code so that it is applicable to CTB9 and UD Chinese.", "startOffset": 84, "endOffset": 107}, {"referenceID": 24, "context": "Compared to CTB, the words in UD Chinese are more fine-grained and the average word length is shorter, which makes it easier for the tagger to correctly segment the OOV words as Zhang et al. (2016) show that the longer words are more difficult to be segmented correctly.", "startOffset": 178, "endOffset": 198}, {"referenceID": 17, "context": "18 Shen et al. (2014) 98.", "startOffset": 3, "endOffset": 22}, {"referenceID": 12, "context": "Peng and Dredze (2016) adopt the model for Chinese segmentation and named entity recognition in the context of multi-task and multi-domain learning.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition. Ma and Sun (2016) use a similar architecture but feed the Chinese characters pairwise as edge embeddings instead.", "startOffset": 0, "endOffset": 156}, {"referenceID": 4, "context": "Dong et al. (2016) employ a character level BiLSTM-CRF model that utilises radicallevel information for Chinese named entity recognition. Ma and Sun (2016) use a similar architecture but feed the Chinese characters pairwise as edge embeddings instead. Their model is applied respectively to chunking, segmentation and POS tagging. Zheng et al. (2013) model joint Chinese segmentation and POS tagging via predicting the combinatory segmentation and POS tags.", "startOffset": 0, "endOffset": 351}, {"referenceID": 4, "context": "They employ the adaptation of the feed forward neural network introduced in Collobert et al. (2011) that only extracts local features in a context window.", "startOffset": 76, "endOffset": 100}, {"referenceID": 4, "context": "They employ the adaptation of the feed forward neural network introduced in Collobert et al. (2011) that only extracts local features in a context window. A perceptron-style training algorithm is employed for sentence level optimisation, which is the same as the training algorithm of the BiRNNCRF model. Their proposed model is not evaluated on CTB5 and therefore difficult to be compared with our system. Kong et al. (2015) apply segmental recurrent neural networks to joint segmentation and POS tagging but the evaluation results are substantially below the state-of-the-art on CTB5.", "startOffset": 76, "endOffset": 426}, {"referenceID": 12, "context": "The results show that radical-enhanced embeddings outperform both skip-ngram and continues bag-of-word (Mikolov et al., 2013) in word2vec.", "startOffset": 103, "endOffset": 125}], "year": 2017, "abstractText": "We present a character-based model for joint segmentation and POS tagging for Chinese. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and sub-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our model is accurate and robust across datasets in different sizes, genres and annotation schemes. We obtain stateof-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and POS tagging.", "creator": "LaTeX with hyperref package"}}}