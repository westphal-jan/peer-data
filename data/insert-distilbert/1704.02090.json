{"id": "1704.02090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Conceptualization Topic Modeling", "abstract": "recently, topic modeling has widely been widely used to discover the only abstract topics in text corpora. most of the existing topic models are currently based on the general assumption of three - layer hierarchical bayesian structure, i. e. each document is modeled as solely a probability distribution over topics, and each topic name is a probability distribution somewhere over more words. however, the assumption is not optimal. intuitively, it's more reasonable to assume thus that each topic is a probability distribution over concepts, and then each concept is a probability distribution over concept words, i. e. adding a latent concept layer between topic layer and word layer in traditional orthogonal three - matter layer assumption. in this paper, we verify the well proposed assumption by incorporating the initial new assumption in two representative topic models, and obtain us two novel topic models. extensive experiments were conducted among the proposed models preliminary and corresponding baselines, and the results show that the proposed models considerably significantly outperform the baselines in terms of case survival study and perplexity, which means the new consensus assumption is more reasonable than traditional simpler one.", "histories": [["v1", "Fri, 7 Apr 2017 05:12:38 GMT  (1481kb,D)", "http://arxiv.org/abs/1704.02090v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yi-kun tang", "xian-ling mao", "heyan huang", "guihua wen"], "accepted": false, "id": "1704.02090"}, "pdf": {"name": "1704.02090.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yi-Kun Tang", "Xian-Ling Mao", "Heyan Huang", "Guihua Wen"], "emails": ["hhy63}@bit.edu.cn", "crghwen@scut.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In recent years, topic modeling is becoming more and more popular in identifying latent semantic components in text corpora. Lots of topic models have been proposed. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al., 2009; Mao et al., 2012; Magnusson et al., 2016].\nThe basic assumption of most existing topic models is that each document is modeled as a probability distribution over topics, and each topic is directly a probability distribution over words, i.e. three-layer hierarchical Bayesian structure, shown in Figure 1 (a). \u2217Submission to IJCAI 2017 \u2020 Corresponding author.\nHowever, this assumption is not optimal, because it does not consider the importance of the concepts in topics. Concepts are very important in natural language and textual semantic understanding. Concepts can also help people better understand knowledge, as psychologist Gregory Murphy wrote: \u201dConcepts are the glue that holds our mental world together\u201d [Murphy, 2004].\nIntuitively, it\u2019s more reasonable that if we add a latent concept layer between topic layer and word layer in traditional three-layer assumption, i.e. a four-layer hierarchical Bayesian structure, shown in Figure 1 (b). In this novel assumption, each document is considered as a probability distribution over topics, each topic is a probability distribution over concepts, and each concept is a probability distribution over words. The assumption is similar to the writing process. For example, if we want to write a article about the topic \u201cmilitary\u201d, we then focus on the concepts related to the topic, such as army, navy and air force. Finally, we select related words from these concepts, maybe the word tank from the concept army, the word torpedo from the concept navy, and the word fighter from the concept air force.\nAs we known, Latent Dirichlet Allocation (LDA) [Blei et al., 2003b] is the beginning of topic modeling, and is the most important component in all kinds of topic models. If the novel assumption performs better than the traditional one in LDA, it\u2019s reasonable to infer that the novel assumption is more suitable for topic modeling than the traditional one. Thus, in this paper, we first propose a novel topic model, called Conceptualization Latent Dirichlet Allocation (CLDA), which applies the novel four-layer assumption in LDA, to verify our as-\nar X\niv :1\n70 4.\n02 09\n0v 1\n[ cs\n.C L\n] 7\nA pr\n2 01\n7\nsumption. Furthermore, we also apply the novel assumption in a supervised topic model, Labeled LDA (LLDA) [Ramage et al., 2009], to proof the novel assumption is more effective. The distribution of each concept over words in our models can be obtained from Probase knowledge base [Wu et al., 2012], which is a universal probabilistic taxonomy concept knowledge base.\nThe rest of the paper is organized as follows. In Section 2, we review the related work. In Section 3, two novel topic models, CLDA and CLLDA, are proposed by using new fourlayer assumption. Extensive experiments on two real datasets are introduced in Section 4. Finally, we conclude the paper in Section 5."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Topic Modeling", "text": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts.\nUnsupervised non-hierarchical topic models are widely studied, such as LDA [Blei et al., 2003b], Probase-LDA [Yao et al., 2015] , TCC [Jayabharathy et al., 2014] and COT [Yao et al., 2016] etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA [Hofmann, 1999], except that in LDA the topic distribution is assumed to have a Dirichlet prior.\nHowever, the above models cannot capture the relation between super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) [Blei et al., 2004], Hierarchical Dirichlet processes (HDP) [Teh et al., 2006], Hierarchical PAM (HPAM) [Mimno et al., 2007], PIE [Joshi et al., 2016] and Guided HTM [Shin and MOON, 2016] etc. The relations are usually in the form of a hierarchy, such as the tree or Directed Acyclic Graph (DAG).\nAlthough unsupervised topic models are sufficiently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al., 2011], Partially LDA (PLDA) [Ramage et al., 2011], NTM [Cao et al., 2015] and DOLDA [Magnusson et al., 2016] etc.\nNone of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. Lots of models, such as hLLDA [Petinot et al., 2011], HSLDA [Perotte et al., 2011], SSHLLDA [Mao et al., 2012], SHDP [Zhang et al., 2013] and EHLLDA [Mao et al., 2015], have been proposed to solve the problem.\nAll of these topic models are mainly based on the assumption of three-layer hierarchical Bayesian structure. However, the assumption is not optimal. Intuitively, it\u2019s more reasonable to add a latent concept layer between topic layer and\nword layer in traditional three-layer assumption. In this paper, we will verify the proposed assumption by incorporating the new assumption in two representative topic models."}, {"heading": "2.2 Concept Knowledge Base", "text": "It is easy for mankind to acquire the meaning of an article and extract the topics of the article, because there is a certain background conceptualized knowledge base in a brain. For example, when seeing a sentence: \u201dMicrosoft announced a project named, Microsoft Azure Information Protection.\u201d, a man will never mistake Microsoft as a person or other things, because we have known that Microsoft is a concept about software company.\nHowever, machines cannot conceptualize what they read, which is a great challenge for machines to understand natural language. Concept knowledge base is a kind of knowledge base that uses taxonomies and ontologies to obtain concepts and extract the relationships between instances and concepts. Therefore, concept knowledge base is a kind of tool to make machines understand nature language.\nThere are many existing concept knowledge bases, such as Probase [Wang et al., 2015; Wu et al., 2012], Freebase and WordNet etc. Among them, Probase is a state-of-the-art one, which contains above 5.4 million concepts that is greater than other concept knowledge bases. The main advantage of Probase is that it is the first to measure the correlation between instances and concepts with probabilities, while other concept knowledge bases use a boolean variable to represent relationships between instances and concepts.\nTherefore, in this paper, we use Probase API [Wang et al., 2015] to get the probability distribution of each concept over words."}, {"heading": "3 Conceptualization Topic Modeling", "text": "In this section, we will demonstrate that how to incorporate the four-layer assumption in unsupervised and supervised topic models, to verify the effectiveness of the novel assumption. For unsupervised topic modeling, we choose LDA as the manipulating object because it is the basic component of most existing topic models. For supervised topic modeling, we choose Labeled LDA [Ramage et al., 2009] because it is one of the most representative supervised models."}, {"heading": "3.1 Conceptualization LDA", "text": "To incorporate the four-layer assumption in LDA, we propose a novel topic model, called Conceptualization LDA (CLDA). It models each document as a mixture of underlying topics. Different from existing topic models, CLDA assumes that each topic is a distribution over concepts rather than directly over words, and regards concepts as distributions over words.\nIn addition, as for neologisms, which do not in the dictionary of the concept knowledge base, they will be regarded as new concepts. In other words, we define these neologisms as atomic concepts. In CLDA, the distribution of a concept over words is acquired from the concept knowledge base, Probase. The graphical model of CLDA is shown in Figure 2.\nIn CLDA, each document consists of a group of words represented as w(d) = (w1, ..., wNd). \u03b1 is the parameter of the Dirichlet distribution of the topic prior, and \u03b8(d) is the parameter of the multinomial distribution of the dth document. \u03b2 is the parameter of the Dirichlet distribution of the concept prior, and \u03c6k is the parameter of the multinomial distribution of the kth topic. \u03bb is the concept distribution over words gets from Probase. m is the number of words that do not belong to any concept in the concept knowledge base, and R is the size of concept set. zd,i is the latent topic for the concept or atom concept of the ith word in the dth document. cd,i is the concept of the ith word in the dth document.\nThe generative process of our CLDA is summarized in Algorithm 1. The generative process can be divided into three parts. Firstly, draw the concept and atom concept distribution from Dirichlet distribution for each topic in the datasets (line 1 \u223c 2). Secondly, draw the topic distribution for each document from Dirichlet distribution (line 3 \u223c 4). Finally, to generate the word wd,i, we first select a latent topic zi (line 5 \u223c 6), and then generate a variable \u03be from Bernoulli distribution, where 0 indicates the word does not belong to any concept, and 1 indicates the word wd,i belongs to some concepts in the given concept knowledge base. If \u03be equals to 0, then generate a word fromMult(\u00b7|\u03c6zi); otherwise, generate a concept from Mult(\u00b7|\u03c6zi), and then select a word from the concept, which conditionally is related to the concept distribution from the knowledge base (line 7 \u223c 12).\nLearning and Inference\nIn this section, we use collapsed Gibbs sampling to estimate parameters.\nSpecifically, if the word wd,i belongs to some concepts in the given concept knowledge base, the sampling probability for a topic and a concept in position i in document d can be expressed as follows:\nP (zd,i = k, cd,i = j|w, e\u2212(d,i), z\u2212(d,i);\u03b1, \u03b2)\n\u221d \u03b2k,cd,i + n\n(cd,i)\n\u2212(d,i),k\u2211E x=1 \u03b2k,x + n (\u00b7) \u2212(d,i),k \u00b7 \u03b1k + n (d) \u2212(d,i),k\u2211K t=1 \u03b1t + n (d) \u2212(d,i),\u00b7 \u00b7 P (wd,i|cd,i)\n(1) And if the word wd,i does not belong to any concept in the given concept knowledge base, the sampling probability for a\nAlgorithm 1 Generative process for CLDA. 1: For each topic k \u2208 {1, ...,K}: 2: Generate \u03c6k = (\u03c6k,1, ..., \u03c6k,C , \u03c6k,C+1, ..., \u03c6k,C+m)T \u223c Dir(\u00b7|\u03b2)\n3: For each document d \u2208 {1, ..., D}: 4: Generate \u03b8(d) = (\u03b81, ..., \u03b8k)T \u223c Dir(\u00b7|\u03b1) 5: For each i in {1,...,Nd}: 6: Generate zd,i \u2208 {1, ...,K} \u223cMult(\u00b7|\u03b8(d)) 7: Generate \u03be \u223c Bernoulli, where 0 indicates the word wd,i is an atom concept, and 1 indicates the word wd,i belongs to some concepts in the given concept knowledge base. 8: If \u03be = 0: 9: Generate wd,i \u2208 {1, ..., V } \u223cMult(\u00b7|\u03c6zi) 10: Else: 11: Generate cd,i \u2208 {1, ..., R} \u223cMult(\u00b7|\u03c6zi) 12: Select a word wd,i from \u03bb, a probability distribu-\ntion gets from Probase.\ntopic in position i in document d can be expressed as follows:\nP (z(d,i) = k|w, e\u2212(d,i), z\u2212(d,i);\u03b1, \u03b2)\n\u221d \u03b2k,ed,i + n\n(ed,i)\n\u2212(d,i),k\u2211E x=1 \u03b2k,x + n (\u00b7) \u2212(d,i),k \u00b7 \u03b1k + n (d) \u2212(d,i),k\u2211K t=1 \u03b1t + n (d) \u2212(d,i),\u00b7\n(2)\nwhere e is the vector of concepts and atomic concepts related to the words. e denotes a concept or an atomic concept. E is the number of concepts and atomic concepts. n(\u00b7)\u2212(d,i),k is the count of concepts and atomic concepts in e in topic k without zd,i. n (d) \u2212(d,i),k is the number of tokens in e assigned to topic k in document d without zd,i, and n (di) \u2212(d,i),\u00b7 indicates a summation over that dimension. In Eq. (1), n(ci)\u2212i,k is the count of concept ci in topic k, that does not include the current assignment z(d,i). And the conditional probability P (wd,i|cd,i) describes the probability of word wd,i in concept cd,i, which can be obtained from Probase. In Eq. (2), n(ei)\u2212(d,i),k is the count of atom concept ed,i, non-concept word, in topic k, that does not include the current assignment zd,i.\nFinally, the parameters can be estimated as follows:\n\u03c6\u0302e,k = \u03b2k,e + n (e) \u00b7,k\u2211E\nx=1 \u03b2k,x + n (\u00b7) \u00b7,k\n(3)\n\u03b8\u0302 (d) k = \u03b1k + n (d) \u00b7,k\u2211K\nt=1 \u03b1t + n (d) \u00b7,\u00b7\n(4)\nwhere e denotes a concept or an atomic concept. The two equations for parameter estimation are important. We can use the topic-specific distribution \u03c6 to obtain topical abstracts for topics; meanwhile the topic distribution for each document \u03b8 can be used to discover the most relevant topics for a document and find documents with similar topics."}, {"heading": "3.2 Conceptualization Labeled LDA", "text": "The proposed four-layer Bayesian assumption can be used in most of existing topic models, and we have demonstrated that the assumption can be used in unsupervised topic model, i.e. LDA. In this section, we will further demonstrate the use of the assumption in supervised topic modeling. Labeled Latent Dirichlet Allocation (Labeled LDA) [Ramage et al., 2009] which is a classical supervised topic model, will be extended by incorporating conceptualization assumption. The novel model is called Conceptualization Labeled Latent Dirichlet Allocation (CLLDA).\nLabeled LDA is very similar to LDA. Different with LDA, Labeled LDA assumes that the topics of each document are restricted to its labels. The topic distribution of each document in Labeled LDA is generated from a Dirichlet distribution, whose dimensionality of the prior parameter is the same as the number of labels of each document, rather than the number of the total topics of the datasets in LDA. Thus, CLLDA is also similar to CLDA.\nSpecifically, in order to restrict the latent topics to the label set of each document in CLLDA, we define an indicator function I(d)(k) as follows:\nI(d)(k) =  1 if the kth topic is in the label set of\nthe dth document. 0 otherwise.\n(5)\nIf the wordwd,i belongs to some concepts in the given concept knowledge base, the sampling probability for a topic and a concept in position i in document d can be expressed as follows:\nP (zd,i = k, cd,i = j|w, e\u2212(d,i), z\u2212(d,i);\u03b1, \u03b2) \u221d I(d)(k)\n\u00b7 \u03b2k,cd,i + n\n(cd,i)\n\u2212(d,i),k\u2211E x=1 \u03b2k,x + n (\u00b7) \u2212(d,i),k \u00b7 \u03b1k + n (d) \u2212(d,i),k\u2211K t=1 \u03b1t + n (d) \u2212(d,i),\u00b7 \u00b7 P (wd,i|cd,i)\n(6) And if the word wd,i does not belong to any concept in the given concept knowledge base, the sampling probability for a topic in position i in document d can be expressed as follows:\nP (z(d,i) = k|w, e\u2212(d,i), z\u2212(d,i);\u03b1, \u03b2) \u221d I(d)(k)\n\u00b7 \u03b2k,ed,i + n\n(ed,i)\n\u2212(d,i),k\u2211E x=1 \u03b2k,x + n (\u00b7) \u2212(d,i),k \u00b7 \u03b1k + n (d) \u2212(d,i),k\u2211K t=1 \u03b1t + n (d) \u2212(d,i),\u00b7 (7)\nwhere I(d)(k) is the indicator function, and other notations have the same meaning as that in CLDA stated above.\nFinally, the parameter can be estimated as follows:\n\u03c6\u0302e,k = \u03b2k,e + n (e) \u00b7,k\u2211E\nx=1 \u03b2k,x + n (\u00b7) \u00b7,k\n(8)\n\u03b8\u0302 (d) k = \u03b1k + n (d) \u00b7,k\u2211K\nt=1 \u03b1t + n (d) \u00b7,\u00b7\n(9)\nthe notations have the same meaning as that in CLDA stated above."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Experiment Setting", "text": "We conducted the experiments on two real datasets. One of them, called Conf, contains 2,317 full papers of four conferences ( CIKM, SIGIR, SIGKDD and WWW ) of three years (2011 \u223c 2013). And the other dataset named AP is a public dataset, which contains more than 106K full Associated Press news articles published in 1989. Both of the raw datasets contain more than 2 million concepts according to Probase, which is much larger than the size of vocabulary. It leads to the imbalance between concepts size and vocabulary size. Moreover, lots of concepts in Probase Concept Graph are similar to each other associated with the same word. For example, the word microsoft associates with concept company, concept software company and concept technology company, which are semantically similar. In order to address this issue, we use the concept clustering results provided by Probase, to reduce the number of concepts. Totally, it contains 4,819 concept clusters. In the above example, all concepts about company can be represented by a concept cluster company.\nThe statistics of the two datasets are summarized in Table 1. And we conduct all the experiments on a server with an Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz and 125GB memory. In the rest, we will compare the proposed models with corresponding baselines in terms of case study and perplexity. For all models, we set the number of iterations in each collapsed Gibbs sampler as 1000, and set the same initial hyperparameters, where \u03b1 and \u03b2 both equal to 0.01."}, {"heading": "4.2 Experiments for CLDA", "text": "In the experiments, we removed the standard stop words for both datasets, and then we further removed words that occurred less than ten times. We trained the two topic models and set the number of topics as 100.\nCase Studies Table 2 and Table 3 show top ten words and concepts associated with 5 topics learned on Conf and AP respectively. The topics learned by CLDA were matched to a LDA topic with smallest Kullback-Leibler divergence. It is noted that the bold phrase in the third column of Table 2 and Table 3 is the clustering concepts where pattern \u201cA, ..., B; C\u201d means \u201cA, ..., B\u201d is similar to \u201cA, ..., C\u201d, and the un-bold word is the atomic concepts.\nFrom the two tables, we can see that a topic can be reflected by concepts, and a topic can be represented by a distribution of concepts. For example, according to the top words for topic 4 in Table 2 learned by LDA, its topic may be trade. In the same topic, there are many concepts learned by CLDA correlating with that topic, such as handtools;hand tool, top brand name;brand, provide and specify. Meanwhile, many\nwords learned by CLDA in the same topic are related to these concepts, such as t-test, waste, bubble, taste and recipe.\nAnother example, as for the top ten words for topic 2 in Table 3 learned by LDA, the topic may be politics. In the same topic, concepts, such as committee, senate, facility factor;feasibility factor, legisl and republican learned by CLDA are related to topic politics, and words learned by CLDA in the same topic are related to these concepts.\nTherefore, our proposed CLDA performs better than LDA, and thus our conceptualization method for topic modeling sounds good.\nPerplexity In this experiment, we trained both CLDA and LDA for ten times with different number of topics varying from 10 to 100 in turn. We computed the perplexity of the proposed CLDA and LDA, which can quantificationally measure the quality\nof different models. A lower perplexity score indicates better generalization performance. The perplexity can be computed\nTable 4: Top ten words and concepts associated with five topics learned on Conf.\nXXXXXXLabel Term Words in LLDA Concepts in CLLDA Words in CLLDA\nsocial networks\nsocial, network, number, model, graph, set, inform,\nresult, show, problem\nsocioeconomic variable, limitation , exercise program;class, operation, asset, activity,\nperturbation, product, basic contact information;contact information, anomaly\nnetwork, social, user, number, result, set, inform, node,\ntopic, figure\nwikipedia wikipedia, article, inform,\nfeature, test, set, word, page, entity, data\nactivity, poem, article, exercise program;class, limitation, answer, metric, construct,\nrequirement, disadvantage\nwikipedia, article, feature, inform, entity, evalu, page,\ncategory, figure, table query recom-\nmender systems\nquery, term, suggest, recommend, node, compute,\nmodel, set, result, graph\nquery, limitation, artifact, writing system;script, suggest, activity, reinforcers;essential,\nrequirement, famous name, high wear area\nquery, suggest, recommend, model, user, generate, set, list,\nnode, qfg\nsocial media social, user, media, inform, data, number, work, time,\nfigure, twitter\nlimitation, exercise program;class, activity, asset, tax implication, operation, construct,\ncompany, perturbation, computer function;complex function\nsocial, user, media, inform, topic, number, result,\nnetwork, set, work\ndata mining data, mine, set, inform, result,\nsystem, work, number, provide, perform\nactivity, limitation, requirement, skill tab, asset, metric, mega-projects, operation, construct,\nreinforcers;essential\ndata, mine, result, set, user, inform, learn, model, provide,\nnumber\nas follows:\nperplexity = exp{\u2212\u03a3 M d=1 log p(wd)\n\u03a3Md=1Nd } (10)\nFigure 3 shows the perplexity of the two models on the two datasets for different number of topics. As we can see in Figure 3, the perplexity curves decrease as the number of topics increases on both of the two datasets. The perplexity value of CLDA is much smaller than that of LDA, which indicates that CLDA performs significantly better than LDA."}, {"heading": "4.3 Experiments for CLLDA", "text": "In this experiment, we train CLLDA and LLDA over Conf dataset. The keywords of each paper will be used as labels of the corresponding paper, and the number of labels for the whole dataset is 4760.\nCase Study Table 4 shows top ten words and concepts from five topics learned on Conf, where we can easily acquire the concepts under topics. From Table 4, we can learn that concepts is very important in understanding a document. For example, in topic social media, concepts such as limitation, activity and computer function;complex function, are different aspects of the topic social media, and the words like social, user, media are related to these concepts. Therefore, from the results we know that the proposed model performs better than the baseline, which means our conceptualization method for topic modeling sounds good, and our assumption is more reasonable.\nPerplexity To compute perplexity for the two models with different number of topics, we segment documents in Conf into ten groups, and the first nine groups all contain 200 documents, while the last group contains the rest of the documents. We train CLLDA and LLDA for ten times. The first time we use the first group, the second we use the first two groups, and so\non. The comparison of the two models\u2019 perplexity is shown in Figure 4. Since the data for training with different number of topics is different, the monotonic trend of the same curve is not comparable. Here, we only compare the perplexity values of the two curves under the same number of topics. It is apparent that CLLDA performs much better than the original LLDA."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a novel assumption of four-layer hierarchical Bayesian structure for topic modeling, which adds a latent concept layer between topic layer and word layer in the traditional assumption. To verify the effectiveness of the novel assumption, we apply the assumption in two representative topic models (LDA and LLDA). Extensive experiments have been conducted on two real datasets. The experimental results show that the proposed assumption performs better than the traditional assumption.\nIn the future, we will further verify the novel four-layer assumption in more topic models over more datasets. Also,\nwe will explore the use of the new assumption in multimedia data, such as images and videos."}], "references": [{"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "Proceeding of the Neural Information Processing Systems(nips)", "citeRegEx": "Blei and McAuliffe. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["David M Blei", "Michael I Jordan", "Thomas L Griffiths", "Joshua B Tenenbaum"], "venue": "pages 17\u201324,", "citeRegEx": "Blei et al.. 2003a", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research, 3:993\u20131022", "citeRegEx": "Blei et al.. 2003b", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum"], "venue": "Advances in neural information processing systems, 16:106", "citeRegEx": "Blei et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In AAAI", "author": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji. A novel neural topic model", "its supervised extension"], "venue": "pages 2210\u20132216,", "citeRegEx": "Cao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, 41(6):391\u2013407", "citeRegEx": "Deerwester et al.. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proc. of Uncertainty in Artificial Intelligence, UAI\u201999, page 21. Citeseer", "citeRegEx": "Hofmann. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Correlated concept based topic updation model for dynamic corpora", "author": ["J Jayabharathy", "S Kanmani", "N Sivaranjani"], "venue": "International Journal of Computer Applications, 89(10):1\u20137", "citeRegEx": "Jayabharathy et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Political issue extraction model: A novel hierarchical topic model that uses tweets by political and non-political authors", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark Carman"], "venue": "Proceedings of NAACL-HLT, pages 82\u201390,", "citeRegEx": "Joshi et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dolda-a regularized supervised topic model for high-dimensional multi-class regression", "author": ["M\u00e5ns Magnusson", "Leif Jonsson", "Mattias Villani"], "venue": "arXiv preprint arXiv:1602.00260,", "citeRegEx": "Magnusson et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sshlda: a semi-supervised hierarchical topic model", "author": ["Xianling Mao", "Zhaoyan Ming", "Tatseng Chua", "Si Li", "Hongfei Yan", "Xiaoming Li"], "venue": "pages 800\u2013809,", "citeRegEx": "Mao et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ehllda: A supervised hierarchical topic model", "author": ["Xian-Ling Mao", "Yixuan Xiao", "Qiang Zhou", "Jun Wang", "Heyan Huang"], "venue": "Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, pages 215\u2013226. Springer,", "citeRegEx": "Mao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Jon D Mcauliffe", "David M Blei. Supervised topic models"], "venue": "pages 121\u2013128,", "citeRegEx": "Mcauliffe and Blei. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Mixtures of hierarchical topics with pachinko allocation", "author": ["David Mimno", "Wei Li", "Andrew Mccallum"], "venue": "pages 633\u2013640,", "citeRegEx": "Mimno et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Journal of Child Language", "author": ["Gregory L Murphy. The big book of concepts"], "venue": "31(1):247\u2013253,", "citeRegEx": "Murphy. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchically supervised latent dirichlet allocation", "author": ["A. Perotte", "N. Bartlett", "N. Elhadad", "F. Wood"], "venue": "Neural Information Processing Systems (to appear)", "citeRegEx": "Perotte et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "author": ["Yves Petinot", "Kathleen McKeown", "Kapil Thadani. A hierarchical model of web summaries"], "venue": "pages 670\u2013675. ACL,", "citeRegEx": "Petinot et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["Ramage et al", "2009] D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Partially labeled topic models for interpretable text mining", "author": ["D. Ramage", "C.D. Manning", "S. Dumais"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457\u2013465. ACM", "citeRegEx": "Ramage et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical topic models for multi-label document classification", "author": ["T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Arxiv preprint arXiv:1107.2462", "citeRegEx": "Rubin et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Guided htm: Hierarchical topic model with dirichlet forest priors", "author": ["Su-Jin Shin", "IL CHUL MOON"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Shin and MOON. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Journal of the American Statistical Association", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei. Hierarchical dirichlet processes"], "venue": "101(476):1566\u20131581,", "citeRegEx": "Teh et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "An inference approach to basic level of categorization", "author": ["Zhongyuan Wang", "Haixun Wang", "Jirong Wen", "Yanghua Xiao"], "venue": "pages 653\u2013662,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Probase: a probabilistic taxonomy for text understanding", "author": ["Wentao Wu", "Hongsong Li", "Haixun Wang", "Kenny Q Zhu"], "venue": "pages 481\u2013492,", "citeRegEx": "Wu et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating probabilistic knowledge into topic models", "author": ["Liang Yao", "Yin Zhang", "Baogang Wei", "Hongze Qian", "Yibing Wang"], "venue": "pages 586\u2013597,", "citeRegEx": "Yao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Concept over time: the combination of probabilistic topic model with wikipedia knowledge", "author": ["Liang Yao", "Yin Zhang", "Baogang Wei", "Lei Li", "Fei Wu", "Peng Zhang", "Yali Bian"], "venue": "Expert Systems With Applications, 60:27\u201338,", "citeRegEx": "Yao et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of the IEEE International Conference on Computer Vision Workshops", "author": ["Cheng Zhang", "Carl Ek", "Xavi Gratal", "Florian Pokorny", "Hedvig Kjellstrom. Supervised hierarchical dirichlet processes with variational inference"], "venue": "pages 254\u2013261,", "citeRegEx": "Zhang et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 6, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 2, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 25, "context": "The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models [Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003b; Yao et al., 2016], Unsupervised hierarchical topic models [Blei et al.", "startOffset": 106, "endOffset": 184}, {"referenceID": 1, "context": ", 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al.", "startOffset": 48, "endOffset": 106}, {"referenceID": 21, "context": ", 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al.", "startOffset": 48, "endOffset": 106}, {"referenceID": 8, "context": ", 2016], Unsupervised hierarchical topic models [Blei et al., 2003a; Teh et al., 2006; Joshi et al., 2016], and their corresponding supervised counterparts [Ramage et al.", "startOffset": 48, "endOffset": 106}, {"referenceID": 10, "context": ", 2016], and their corresponding supervised counterparts [Ramage et al., 2009; Mao et al., 2012; Magnusson et al., 2016].", "startOffset": 57, "endOffset": 120}, {"referenceID": 9, "context": ", 2016], and their corresponding supervised counterparts [Ramage et al., 2009; Mao et al., 2012; Magnusson et al., 2016].", "startOffset": 57, "endOffset": 120}, {"referenceID": 14, "context": "Concepts can also help people better understand knowledge, as psychologist Gregory Murphy wrote: \u201dConcepts are the glue that holds our mental world together\u201d [Murphy, 2004].", "startOffset": 158, "endOffset": 172}, {"referenceID": 2, "context": "As we known, Latent Dirichlet Allocation (LDA) [Blei et al., 2003b] is the beginning of topic modeling, and is the most important component in all kinds of topic models.", "startOffset": 47, "endOffset": 67}, {"referenceID": 23, "context": "The distribution of each concept over words in our models can be obtained from Probase knowledge base [Wu et al., 2012], which is a universal probabilistic taxonomy concept knowledge base.", "startOffset": 102, "endOffset": 119}, {"referenceID": 2, "context": "Unsupervised non-hierarchical topic models are widely studied, such as LDA [Blei et al., 2003b], Probase-LDA [Yao et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 24, "context": ", 2003b], Probase-LDA [Yao et al., 2015] , TCC [Jayabharathy et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 7, "context": ", 2015] , TCC [Jayabharathy et al., 2014] and COT [Yao et al.", "startOffset": 14, "endOffset": 41}, {"referenceID": 25, "context": ", 2014] and COT [Yao et al., 2016] etc.", "startOffset": 16, "endOffset": 34}, {"referenceID": 6, "context": "LDA is similar to pLSA [Hofmann, 1999], except that in LDA the topic distribution is assumed to have a Dirichlet prior.", "startOffset": 23, "endOffset": 38}, {"referenceID": 3, "context": "To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) [Blei et al., 2004], Hierarchical Dirichlet processes (HDP) [Teh et al.", "startOffset": 112, "endOffset": 131}, {"referenceID": 21, "context": ", 2004], Hierarchical Dirichlet processes (HDP) [Teh et al., 2006], Hierarchical PAM (HPAM) [Mimno et al.", "startOffset": 48, "endOffset": 66}, {"referenceID": 13, "context": ", 2006], Hierarchical PAM (HPAM) [Mimno et al., 2007], PIE [Joshi et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 8, "context": ", 2007], PIE [Joshi et al., 2016] and Guided HTM [Shin and MOON, 2016] etc.", "startOffset": 13, "endOffset": 33}, {"referenceID": 20, "context": ", 2016] and Guided HTM [Shin and MOON, 2016] etc.", "startOffset": 23, "endOffset": 44}, {"referenceID": 0, "context": "Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al.", "startOffset": 117, "endOffset": 169}, {"referenceID": 12, "context": "Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al.", "startOffset": 117, "endOffset": 169}, {"referenceID": 19, "context": "Several modifications of LDA to incorporate supervision have been proposed in the literature, such as Supervised LDA [Blei and McAuliffe, 2007; Mcauliffe and Blei, 2008], Prior-LDA [Rubin et al., 2011], Partially LDA (PLDA) [Ramage et al.", "startOffset": 181, "endOffset": 201}, {"referenceID": 18, "context": ", 2011], Partially LDA (PLDA) [Ramage et al., 2011], NTM [Cao et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": ", 2011], NTM [Cao et al., 2015] and DOLDA [Magnusson et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 9, "context": ", 2015] and DOLDA [Magnusson et al., 2016] etc.", "startOffset": 18, "endOffset": 42}, {"referenceID": 16, "context": "Lots of models, such as hLLDA [Petinot et al., 2011], HSLDA [Perotte et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 15, "context": ", 2011], HSLDA [Perotte et al., 2011], SSHLLDA [Mao et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 10, "context": ", 2011], SSHLLDA [Mao et al., 2012], SHDP [Zhang et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 26, "context": ", 2012], SHDP [Zhang et al., 2013] and EHLLDA [Mao et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 11, "context": ", 2013] and EHLLDA [Mao et al., 2015], have been proposed to solve the problem.", "startOffset": 19, "endOffset": 37}, {"referenceID": 22, "context": "There are many existing concept knowledge bases, such as Probase [Wang et al., 2015; Wu et al., 2012], Freebase and WordNet etc.", "startOffset": 65, "endOffset": 101}, {"referenceID": 23, "context": "There are many existing concept knowledge bases, such as Probase [Wang et al., 2015; Wu et al., 2012], Freebase and WordNet etc.", "startOffset": 65, "endOffset": 101}, {"referenceID": 22, "context": "Therefore, in this paper, we use Probase API [Wang et al., 2015] to get the probability distribution of each concept over words.", "startOffset": 45, "endOffset": 64}], "year": 2017, "abstractText": "Recently, topic modeling has been widely used to discover the abstract topics in text corpora. Most of the existing topic models are based on the assumption of three-layer hierarchical Bayesian structure, i.e. each document is modeled as a probability distribution over topics, and each topic is a probability distribution over words. However, the assumption is not optimal. Intuitively, it\u2019s more reasonable to assume that each topic is a probability distribution over concepts, and then each concept is a probability distribution over words, i.e. adding a latent concept layer between topic layer and word layer in traditional three-layer assumption. In this paper, we verify the proposed assumption by incorporating the new assumption in two representative topic models, and obtain two novel topic models. Extensive experiments were conducted among the proposed models and corresponding baselines, and the results show that the proposed models significantly outperform the baselines in terms of case study and perplexity, which means the new assumption is more reasonable than traditional one.", "creator": "LaTeX with hyperref package"}}}