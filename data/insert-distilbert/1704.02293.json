{"id": "1704.02293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Comparison of Global Algorithms in Word Sense Disambiguation", "abstract": "this article compares four probabilistic algorithms ( global algorithms ) for word sense disambiguation ( wsd ) in terms measure of the number of scorer calls ( generally local algo - rithm ) and the f1 score as determined by a gold - standard scorer. fundamentally two algorithms come from the state of the art, a simulated collision annealing adaptive algorithm ( db saa ) and a genetic algorithm ( ga ) as well as two algorithms that we first adapt from wsd that are state of the two art probabilistic search algorithms, namely a cuckoo search rate algorithm ( csa ) and a bat binary search algorithm ( kb bs ). as wsd requires to evaluate exponentially many word type sense combinations ( though with arbitrary branching factors of up to 6 rounds or more ), probabilistic algorithms allow to find approximate solution in a tractable time by sampling the search space. we do find that csa, ga pd and sa all eventually converge mainly to similar rally results ( 0. 98 f1 score ), but csa usually gets there faster ( in much fewer scorer calls ) and reaches up halfway to 0. 95 f1 before sa in fewer scorer calls. if in ba a strict convergence criterion prevents even it from reaching above 0. 89 f1.", "histories": [["v1", "Fri, 7 Apr 2017 17:04:51 GMT  (1725kb,D)", "http://arxiv.org/abs/1704.02293v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lo\\\"ic vial", "andon tchechmedjiev", "didier schwab"], "accepted": false, "id": "1704.02293"}, "pdf": {"name": "1704.02293.pdf", "metadata": {"source": "CRF", "title": "Comparison of Global Algorithms in Word Sense Disambiguation", "authors": ["Lo\u0131\u0308c Vial", "Andon Tchechmedjiev", "Didier Schwab"], "emails": ["Didier.Schwab}@imag.fr"], "sections": [{"heading": null, "text": "This article compares four probabilistic algorithms (global algorithms) for Word Sense Disambiguation (WSD) in terms of the number of scorer calls (local algorithm) and the F1 score as determined by a gold-standard scorer. Two algorithms come from the state of the art, a Simulated Annealing Algorithm (SAA) and a Genetic Algorithm (GA) as well as two algorithms that we first adapt from WSD that are state of the art probabilistic search algorithms, namely a Cuckoo search algorithm (CSA) and a Bat Search algorithm (BS). As WSD requires to evaluate exponentially many word sense combinations (with branching factors of up to 6 or more), probabilistic algorithms allow to find approximate solution in a tractable time by sampling the search space. We find that CSA, GA and SA all eventually converge to similar results (0.98 F1 score), but CSA gets there faster (in fewer scorer calls) and reaches up to 0.95 F1 before SA in fewer scorer calls. In BA a strict convergence criterion prevents it from reaching above 0.89 F1.\nKeywords: Word Sense Disambiguation, local algorithms, global algorithms, Stochastic optimization algorithms, comparison of global algorithms, Simulated Annealing Algorithm, Genetic Algorithm, Bat Algorithm, Cuckoo Search Algorithm"}, {"heading": "1. Introduction", "text": "Word Sense Disambiguation (WSD) is a complex task that consists in finding the best sense of each word of a text with relation to the surrounding words (context). It is a fundamental problem in Natural Language Processing, which\nPreprint submitted to Swarm and Evolutionary Computation April 10, 2017\nar X\niv :1\n70 4.\n02 29\n3v 1\n[ cs\n.C L\n] 7\ncan help in performing tasks like the automatic extraction of multilingual information or machine translation. A lot of research has already been carried out on WSD. Consequently, there are many approaches, including fully supervised methods (make use of sense-annotated corpora to train supervised classifiers) and similarity-based methods (rely on dictionaries, thesauri and more generally knowledge sources). Fully supervised methods require large hand-annotated corpora, a somewhat rare and expensive resource that must be crafted specifically for a particular sense inventory, language even domain. Moreover, a supervised classifier requires at least 1000 examples/word to achieve a stable performance, while there are at most 200/word in the largest sense-annotated corpora [7]. Due to this bottleneck, similarity-based methods perform on par with supervised algorithms on standard evaluation datasets. While there are few prospects for improvement with supervised system, similarity-based systems offer many avenues for improvement even for English (the best resourced language) [12].\nIn this article, we focus on similarity-based methods, which rely on the notion of global and local algorithms. Local algorithms compute a similarity measure between two senses of two words of the text. Global algorithms make use of the local similarities to find the most suitable sense for each word of the text. The execution time is exponential in the size of the input, thus requiring the use of a window of limited width around each word when evaluating sense combination. The problem can become intractable even for short sentences: a linguistically motivated context, such as a paragraph for instance would be intractable. Probabilistic approaches attempt to alleviate the intractability by exploring only a sampling of the search space in order to find an approximate solution as close to the optimal solution as possible. Probabilistic sampling approaches have been successfully used for decades for complex optimisation where the geometry (and the precise formula) of the search function are unknown (non differentiable). Two classical algorithms that belong to this category are Simulated Annealing and Genetic Algorithms and have been successfully used for WSD in past years. However, since the appearance of SA and GA there have been many new and more efficient probabilistic optimisation algorithms, in particular two of them that are at the state of the art of the field (in classical computational model, thus excluding quantum versions). However, Bat Optimisation and Cuckoo Optimisation have never been applied to Word Sense Disambiguation, a combinatorial problem more difficult in magnitudes (number of parameters and branching factor of the search space) compared to classical NP-complete combinatorial problem (e.g. the travelling salesman algorithm).\nAfter a presentation of similarity-based methods for Word Sense Disambigua-\ntion, we present existing global algorithms and in particular the Simulated Annealing Algorithm (SAA) and the Genetic Algorithm. We then introduce our adaptation of Bat Search and Cuckoo Optimisation for WSD."}, {"heading": "2. Similarity-based Word Sense Disambiguation", "text": "Similarity-based word sense disambiguation rests on the notion of local algorithm and global algorithm introduced by Schwab et al. [10].\nA local algorithm gives a similarity measure between two pairs of (word/sense). For example, considering the sentence \u201dYour algorithm contains many bugs\u201d, if we compute the similarity measure between the pair (algorithm/piece of code) and (bugs/insect) using a local algorithm, we expect to have a smaller value than if we compute the similarity measure between the pair (algorithm/piece of code) and (bugs/fault in a system).\nThen, a global algorithm will propagate these local similarity measures to an upper level, in order to achieve the actual disambiguation of a complete document. In general, global algorithms are heuristic. Indeed, in a large document, we cannot compute the local similarity measure algorithm for every possible (word/sense) pair. For example, if we consider a 500 word text with 5 senses for each word, there are 5005 possibilities. Even if the local algorithm could run in 1 nanosecond (which it clearly doesn\u2019t), it would still take approximately one year to evaluate all combinations\nGlobal algorithms represent a particular combination of sense assignments as a configuration and make use of a scoring function (a.k.a. scorer or fitness function) to evaluate the fitness of the configuration.\nA configuration is the set of all (word/sense) associations, for every word in a document, we say that a configuration annotates each word of the text with a sense. That is, for example, if a document contains 100 words that we need to annotate with senses, a possible configuration for this document is a vector of dimension 100, where for every index i \u2208 [0, 100[ (the position of the word in the document) is assigned an integer j in the range of the possible senses indexes for this word.\nA configuration scorer, scorer, or fitness function, is a measurement of the quality of a configuration. Generally, it can be as simple as the sum of all similarity measure for every (word/sense) pair in a configuration, using a particular local algorithm.\nGiven these definitions, and knowing that we cannot compute the fitness score of every single combination in the search space, global algorithms use heuristics to\nmaximise the exploration of the configuration space while minimizing the number of evaluations of the fitness function.\nThis paper will focus on the compared performance of four different global algorithms: a Simulated Annealing Algorithm (SAA), a Genetic Algorithm (GA), a Bat Algorithm (BA) and a Cuckoo Search Algorithm (CSA)."}, {"heading": "3. Existing Probabilistic Global Algorithms for WSD", "text": "Many experiments that compare heuristic global algorithms for disambiguation have been carried out. In particular, we can take note of the work of Schwab et al. [9, 10], where a SAA and a GA (neighbourhood/local search approaches) were implemented for the WSD problem, with the objective of comparing them to an Ant Colony Algorithm (a constructive approach). We will reuse the existing implementations of SAA and GA in order to compare them to the two new algorithms that we have adapted for WSD. SAA and GA are briefly described in the following. All data and programs are accessible through the companion page of this article1."}, {"heading": "3.1. The Simulated Annealing Algorithm", "text": "Simulated annealing for Word Sense Disambiguation has been introduced by Cowie et al. [1]. This approach is inspired by the annealing process in metallurgy, where the sturdiness of the metal is increased by controlling the rate of cooling applied to it [3]. In context of WSD, the \u201cmetal\u201d is simply the configuration of all (word/sense) pairs in the text.\nThe algorithm works in cycles. Each cycle is composed of iterations. At each iteration a single random change is made in the configuration: the sense assigned to a random word in the text is changed to another sense randomly (uniform distribution) and the score of the modified configuration is computed. If the new configuration has a higher score than the configuration before the modification, it replaces the previous configuration. Otherwise, we compute a probability of accepting the new configuration anyway despite the fact it has a lower score. The acceptance probability is classically computed following a Boltzmann distribution. Simulated annealing is a particular instance of the Metropolis-Hastings algorithm [6]. The strategy of accepting lower scores is a mechanism that allows escaping from local minima that typically abound in complex highly dimensional search spaces.\n1http://getalp.imag.fr/static/wsd/Vial-et-al-global-howto/"}, {"heading": "3.2. The Genetic Algorithm", "text": "Genetic algorithms are evolutionary algorithm based on the principle of the natural selection of species, and more particularly \u201cSurvival of the Fittest\u201d. The idea of GA is to consider search space configurations as individuals in a population.\nGA starts with an initial population, where a certain number of the fittest individuals (as evaluated by the fitness function) will be crossed with each other (crossover operation), that is parts of the configurations will be swapped between two individuals.\nThen, a certain number of random changes (mutation operator]) are applied to the population (like for SAA but repeated). Finally a subset of the fittest the new population are selected and kept (elitism). Variants of GA copy the best individual in proportion to their fitness in the new population. A convergence criterion is used to determine when the population converges and when there are no more beneficial evolutions taking place.\nGA was first adapted to WSD by Gelbukh et al.[2] where the population is a set of configurations, and the fitness function is the configuration scorer."}, {"heading": "4. Two New Global Algorithms for WSD", "text": "Among of non-quantum probabilistic search algorithms, Bat Search and Cuckoo Search offer the best overall performance (speed with relation to approximation quality), both invented by Xin-She Yang. As these algorithms have never been adapted to WSD and given that in many applications they are better than other similar algorithm, we have decided to propose an adaptation to solve the WSD problem. We will describe them, their advantages, and our implementation as global algorithms for WSD in the following."}, {"heading": "4.1. The Bat Algorithm", "text": "The Bat Algorithm was originally introduced by Yang [13] and is inspired by they way bats move through echolocation in order to locate and capture pray. These bats emit a very loud ultrasound pulse (inaudible to humans) and listen for the echo that bounces back on the boundaries of the environment and of the objects therein in order to pinpoint their position and the positions of their prey. A sound pulse is characterised by its frequency, rate of emission and loudness. There are different sound pulses depending on the situation and context. When the bat is searching for prey, the loudness is at its highest and the rate of emission is very low (rough sketch of the environment). Once a prey is located, the bat\nlowers the loudness and increase the rate of emission in order to track the prey with precision.\nThe particularity of this algorithm is its ability to converge to a promising region of the search space rapidly while concurrently exploring alternative areas."}, {"heading": "4.1.1. Description of the Algorithm", "text": "The behaviour is highly configurable through many parameters. The minimum and maximum frequency fmin, fmax of the bats sound pulse define the scale of the movement speed of the bats. Large values are more suitable for large space.\nThe minimum and maximum \u03bbmin, \u03bbmax loudness of the sound pulses have an impact on the width of the local search.\n\u03b1 and \u03b3 are threshold parameters to influence when the bat decides to converge to a local solution when it is close enough to its \u201dprey\u201d (When its score is better than the current best score).\nLet the current iteration be i. Let the set of all bats be B and a bat b \u2208 B. Let bcurrent \u2208 B be the current bat in the iteration and let bbest \u2208 B be the bat with the best fitness.\nLet score : B 7\u2192 R be a function that evaluates the fitness of a bat. Let the frequency be denoted by f . Let r(min,max) : R2 7\u2192 [min,max] \u2208 R be a function that yields a random integer drawn from the uniform distribution with a value between min and max.\nLet the velocity be noted \u03bd and let p : B 7\u2192 R be a function that gives the relative position of a bat b expressed by a real number. Let the loudness be \u03bb and the average loudness across B be \u03bb\u0304. Let the pulse rate be noted \u03c1.\nAlgorithm 1 summarizes the Bat Search algorithm in pseudo code."}, {"heading": "4.2. The Cuckoo Search Algorithm", "text": ""}, {"heading": "4.2.1. General idea", "text": "The Cuckoo Search Algorithm is another creation of Yang [14]. The idea comes from the aggressive reproduction strategy of some cuckoo birds: they lay their eggs in the nests of other host birds (of other species). If the host bird later discovers that an foreign egg was laid its nest, it will either discard it, or just abandon its nest to build another elsewhere."}, {"heading": "4.2.2. Description of the Algorithm", "text": "Cuckoo search is described in pseudo-code in Algorithm 2.\nAlgorithm 1: Bat Algorithm pseudocode Create a population of bats; For each bat is assigned a random position, a null velocity, a random pulse frequency, a random pulse loudness and a random pulse rate;\nwhile stop condition is not met do Rank the bats and find the best one according to the objective function; foreach bat do\nUpdate the pulse frequency, the position and the velocity of the bat:\nf = r(fmin, fmax)\n\u03bd = \u03bd + (p(bbest)\u2212 p(bcurrent)) \u00b7 f\np(bcurrent) = p(bcurrent) + \u03bd\nif r(\u22121, 1) > \u03c1 then Fly randomly around the best bat\np(bcurrent) = p(bbest) + r(\u22121, 1) \u00b7 \u03bb\u0304\nend if r(\u22121, 1) < \u03c1 and score(b) > score(bbest) then\nAccept the new solution; Increase the bat pulse rate and reduce the bat pulse loudness\n\u03bb = \u03bb \u00b7 \u03b1\n\u03c1 = \u03c1 \u00b7 (1\u2212 e\u2212\u03b3\u2217i)\nend end\nend Return the best bat;"}, {"heading": "4.2.3. The Importance of Le\u0301vy Flights", "text": "A fundamental aspect of Cuckoo search is the use of the Le\u0301vy flights to determine the movements of the cuckoos (sample the search space). A Le\u0301vy flight is a random walk following a Le\u0301vy distribution. The Le\u0301vy distribution is an interesting probability distribution. The Le\u0301vy distribution yields values most of the time\nAlgorithm 2: Cuckoo Search Algorithm pseudocode Generate a population of n cuckoos; while convergence criterion not met do\nDraw a cuckoo i among n randomly (uniform); Move the cuckoo pseudo-randomly following a Le\u0301vy flight; Choose another cuckoo j among n randomly (uniform); Replace j by i if score(i) > score(j); Sort the cuckoos by their score; Replace the cuckoos with the worst scores by new randomly generated cuckoos;\nend Return the cuckoo with the best score;\nin proximity to zero (when not shifted) but sometimes a significant larger value. The c parameter is the Le\u0301vy scale, the only parameter of the distribution. There is an optional shift called Le\u0301vy location, that centres the distribution around 0. The Le\u0301vy flight is important because it allows the algorithm to intensively explore local regions of the search space and to sometimes jump in far regions that allows to escape local minima."}, {"heading": "4.3. Adaptation to WSD", "text": "In order to tailor implementations of those algorithms as global algorithms for WSD, we need to make a number of assumptions and modifications to the original algorithms.\nThe original algorithms specify the movement of bats or cuckoos with a vector with real coordinated (characterize by a direction and a norm). Given that WSD is a discrete problem, the definitions of what a movement means must be adapted as a vector with real coordinates does not exist. Moreover all senses are semantically distinct and independent and there isn\u2019t any meaningful relative distance between them, thus moving from sense #1 to sense #2 is no less significant than moving from sense #1 to sense #4. For example, in the WordNet 3.1 lexical database, the sense #1 of the noun \u201dmouse\u201d corresponds to the animal, whereas the sense #2, #3 and #4 correspond respectively to \u201da black eye\u201d, \u201da person who is timid\u201d and \u201da computer device\u201d. It is clear that the sense #2 is not closer to the sense #1 than the sense #3 or #4. The position of the bats in the BA and of the cuckoos in the CSA in the search space is represented by the configuration that assigns a sense\nto each word of the text (word/sense) for a given text (that is, a configuration is a particular solution to our problem). We adapt the original definition by using the velocity of bats and the Le\u0301vy flight for cuckoos to change the current position of the agents by making random changes to their correspond configuration just like for GA and SA.\nFor example, a flight of distance 5 will have the effect of performing 5 random changes in the configuration of a cuckoo. The same is true for the velocity of bats.\nThe algorithms were implemented in Java2 and a detailed code is provided in Appendix B."}, {"heading": "5. Comparing Global Algorithms", "text": ""}, {"heading": "5.1. Method", "text": "We compared the four algorithms using the evaluation corpus from the coarsegrained English all-words WSD task from SemEval-2007 campaign [8]. The corpus is composed of 5 sense annotated texts from different domains and sources:\n\u2022 The first, second and third texts come from the Wall Street Journal and are respectively 368, 379 and 500 words long.\n\u2022 The fourth text is the entirety of the Wikipedia entry for Computer Programming and is 677 words long.\n\u2022 The fifth text is an excerpt from the novel \u201dKnights of the Art\u201d by Amy Steedman and is 345 words long.\nThe campaign contain a reference annotation that we use to calculate how good a particular configuration is in terms of the F1 score. The two criteria we use for our evaluation is the F1 score of the results with relation to the number of invocation of the scoring function as we want to obtain the best solutions in the least amount of time."}, {"heading": "5.1.1. The Choice of the Configuration Scorer", "text": "As we explained in the introduction, a global algorithm is a method to explore effectively the space of possible configurations, without trying every single possibility of pair (word/sense) in a document.\n2http://getalp.imag.fr/static/wsd/Vial-et-al-global-howto/\nTo do so, a global algorithm needs a configuration scorer which helps by evaluating the quality of a configuration by giving a score value for a configuration, in order to see if a configuration is better or worse than another one in terms of the objective.\nA classical objective function is to use a semantic similarity measure and to sum all pairwise similarities between selected senses in a configuration. For example, in previous implementation of SAA and GA for WSD, the Lesk similarity measure that calculates the overlap between the definitions of senses [4] was used. Given that we are merely interested in the global algorithms, in this article we make the choice to use the F1 score from the gold standard scorer as the objective function for the optimisation, in other words, the local algorithm is an oracle from the point of view of the local algorithm so as to reduce the problem from AI-complete to an NP-complete optimisation problem and to be able to evaluate only the influence of the global algorithm."}, {"heading": "5.1.2. Parameters Tuning", "text": "Both BA and CSA have numerous parameters that must be tuned for each specific problem and have a significant influence on the outcome. In our implementation of the algorithms, we have the following parameters:\n\u2022 The BA has 7 parameters: the number of bats, the minimum frequency of their sound pulses, the maximum frequency of their sound pulses, the minimum loudness of their sound pulses, the maximum loudness of their sound pulses, the smoothing coefficients \u03b1 and \u03b3.\n\u2022 The CSA has 4 parameters: the number of cuckoos, the number cuckoos destroyed per iteration, the location parameter of the Le\u0301vy distribution and the scale parameter of the Le\u0301vy distribution.\n\u2022 The GA has 3 parameters: the size of the population, the crossover rate and the mutation rate.\n\u2022 The SAA has 2 parameters: the cooling rate, number of iterations, the initial acceptance probability (the initial temperature is estimated to match the initial acceptance probability). We fixed the initial acceptance probability to 0.8.\nIn order to find the optimal parameters for each algorithm without manually testing every combination, we implemented a Cuckoo Search Algorithm that automatically finds the optimal parameters.\nWe run the algorithms many times (e.g. 100\u20131000) in order to obtain a representative sample of the distribution of possible answers, and use the MannWhitney-U statistical test [5] in addition to the score similarly to the modified Simulated Annealing algorithm used for parameter estimation by Tchechmedjiev et al. [11].\nWe perform the parameter estimation on the first two of each document of the SemEval 2007 task 7 corpus, or a total of 94 words to annotate. In order to avoid any bias in the results, we remove the sentences used for parameter estimation from the evaluation corpus. Moreover, we perform the estimation several times with different limits on the number of scorer calls rather than a traditional convergence criterion: 200, 800, 2000, 4000, so that we can have an idea of the optimal parameters and the resulting F1 scores we could obtain with varying computational constraints."}, {"heading": "5.1.3. Evaluation", "text": "After finding the optimal parameters for each algorithm, we evaluate their efficiency by comparing the best configuration score (F1 score) they can obtain in function of the number of calls to the scorer (200, 800, 2000 and 4000). For each algorithm and each scorer call threshold we run the algorithm 100 times and plot the average F1 score across the whole corpus and the 100 runs compared to the average number of evaluations of the scoring function."}, {"heading": "5.2. Results and Discussion", "text": "The results of optimal parameters estimation are presented in Table 1 for the Bat Algorithm, in Table 2 for Cuckoo Search, in Table 3 for the Genetic Algorithm and in Table 4 with several thresholds for the number of calls to the scorer as a convergence criterion.\n# calls to scorer # cuckoos # cuckoos destroyed Le\u0301vy location Le\u0301vy scale"}, {"heading": "200 100 0.02 0.01", "text": ""}, {"heading": "800 100 0.01 0.01", "text": "Figure 1 presents the average overall results for all for algorithms for 200 (Figure 1a), 800(Figure 1b), 2000 (Figure 1c) and 4000 (Figure 1d) scorer calls. Given that with 4000 calls, the appearance of the charts suggest that convergence has not occurred. Thus, we make two additional executions with a limit of 8000 (Figure 1e) and 16000 (Figure 1f) scorer calls, with the same parameters as the run with the limit at 4000 scorer calls.\nWe see that across the 6 experiments, the progression of the average F1 score compared to the average number of scorer calls remains the same, thus we will make a sweeping analysis across all size experiments. Lower limits to the number\nof scorer calls can be seen as a different \u201dzoom levels\u201d at the beginning of the execution.\nBelow 20 calls, SA cannot yet obtain a solution, while CSA obtains the best F1 (0.6) followed by BS (0.59) followed by GA (0.55). After 100 scorer calls, the order stabilizes and we have CSA first (0.7), followed by BS (0.68) followed by GA (0.65) followed by SA (0.6). Until 500 calls, all algorithms except SA follow a slow increase (from 0.70 to 0.79 for CSA, from 0.68 to 0.71 for BA, from 0.65 to 0.67 for GA) while SA stagnates at 0.60. After 500 the algorithm continue the trend, however SA catches up rapidly and starts to overtake GA at 600 scorer calls (0.68 F1) and BS at 1400 scorer calls (0.8 F1 and stays above them). GA overtakes bat search after 3400 scorer calls and SA seems to overtake CSA after 4000 scorer evaluations.\nIn order to better see if SA continues to rise after overtaking CSA, we perform two additional experiments with limits of 8000 and then 16000 scorer calls. In the long run The bat algorithm converges slightly below 0.90 F1 while, CSA, GA and SA all converge (0.98 F1) and continue rising, likely reaching a F1 of 1 eventually.\nWhile all algorithms save for BS all converge to the same solutions, for lower counts of scorer calls, CSA gets better results with fewer scorer evaluations. This will be particularly advantageous for scorer functions that are costly to compute and where simulated annealing or GA will be prohibitive. Indeed, for these experiments we used a perfect scoring function that is very fast to calculate. However, typical heuristic scoring functions used when there is no gold-standard data available, such as Lesk are much more costly and may not allow to reach enough scorer calls to reach the point where SA overtakes CSA is a tractable time.\nAs for the lacklustre performance of BS, after a point (depending on the loudness parameter) BS stops accepting the configurations even if they are better (the target is acquired by the bat), which likely explains the early convergence of the algorithm."}, {"heading": "6. Conclusion", "text": "In this paper, we adapt two state of the art probabilistic search algorithms as global algorithms the Word Sense Disambiguation problem : a Bat Algorithm and a Cuckoo Search Algorithm. We further adapt Cuckoo search to perform parameter estimation for probabilistic global WSD algorithms. We compared the two algorithms to two existing implementation of classical probabilistic optimisation algorithms, a Simulated Annealing Algorithm and a Genetic Algorithm. We make a comparative evaluation using the F1 score computed from the gold standard of\nthe Semeval 2007 Task 7 WSD task against the number of calls to the scoring function. We used an oracle objective function in order to only evaluate the influence of the global algorithm on the results, rather than letting the heuristic scoring function have an influence as well. We find that CSA, SA and GA all converge at around 16000 scorer calls and that SA slightly overtakes CSA after 4000 calls, while CSA gets there faster (in much fewer scorer calls). While the scorer used here was perfect and fast to compute (comparison to the gold-standard), actual heuristic scorer are much slower and may not allow to tractably reach the point where SA overtakes CSA. Thus, we conclude that CSA is a much better sampler of the WSD search space and that it should be preferred to SA and GA. We also conclude that because BS stops accepting solutions after a while (inherent to the algorithm, not an explicit convergence criterion) the convergence is too fast.\nIn this work we only considered a perfect scorer, however we currently work on evaluating and comparing with an actual heuristic scoring function and on studying the correlation between the oracle objective function and the heuristic scoring function. This study will then allow to optimize and adapt the heuristic scoring function to be more like the oracle scoring function on the same subset of the data we used to evaluate the parameters for the global algorithms."}, {"heading": "Appendix A.1. Number of call to the scorer : 200", "text": ""}, {"heading": "Appendix A. Detailed results of the parameter estimation", "text": ""}, {"heading": "Appendix A.2. Number of call to the scorer : 800", "text": ""}, {"heading": "Appendix A.3. Number of call to the scorer : 2000", "text": ""}, {"heading": "Appendix B. Implementation details", "text": ""}, {"heading": "Appendix B.1. The Bat Algorithm", "text": "C o n f i g u r a t i o n d i s a m b i g u a t e ( Document document ) {\nf o r ( i n t i = 0 ; i < batsNumber ; ++ i ) { b a t s [ i ] = new Bat ( ) ; } u p d a t e B e s t B a t ( ) ;\nwhi le ( ! s t o p C o n d i t i o n . s t o p ( ) && n b B a t s F i n i s h e d < batsNumber ) {\nf o r ( Bat c u r r e n t B a t : b a t s ) {\nC o n f i g u r a t i o n p r e v i o u s P o s i t i o n = c u r r e n t B a t . p o s i t i o n . c l o n e ( ) ; i n t p r e v i o u s V e l o c i t y = c u r r e n t B a t . v e l o c i t y ; double p r e v i o u s S c o r e = c u r r e n t B a t . s c o r e ;\ni f ( c u r r e n t B a t . r a t e < randomDoubleInRange ( minRate , maxRate ) ) {\nc u r r e n t B a t . p o s i t i o n = b e s t B a t . p o s i t i o n . c l o n e ( ) ; c u r r e n t B a t . p o s i t i o n . makeRandomChanges ( ( i n t ) ge tAve rageLoudnes s ( ) ) ;\n} e l s e {\nc u r r e n t B a t . f r e q u e n c y = randomDoubleInRange ( minFrequency , maxFrequency ) ; f o r ( i n t i = 0 ; i < d imens ion ; i ++) {\ni f ( c u r r e n t B a t . p o s i t i o n . g e t A s s i g n m e n t ( i ) != b e s t B a t . p o s i t i o n . g e t A s s i g n m e n t ( i ) ) { c u r r e n t B a t . v e l o c i t y ++; }\n} c u r r e n t B a t . v e l o c i t y \u2217= c u r r e n t B a t . f r e q u e n c y ; c u r r e n t B a t . p o s i t i o n . makeRandomChanges ( c u r r e n t B a t . v e l o c i t y ) ;\n}\ni f ( c u r r e n t B a t . l o u d n e s s >= randomDoubleInRange ( minLoudness , maxLoudness ) && c u r r e n t B a t . s c o r e > b e s t B a t . s c o r e ) {\nc u r r e n t B a t . l o u d n e s s \u2217= a l p h a ; i f ( c u r r e n t B a t . l o u d n e s s < minLoudness ) n b B a t s F i n i s h e d ++; c u r r e n t B a t . r a t e = c u r r e n t B a t . i n i t i a l R a t e \u2217 (1 \u2212 Math . exp(\u2212gamma \u2217 c u r r e n t I t e r a t i o n ) ) ; b e s t B a t = c u r r e n t B a t ;\n} e l s e {\nc u r r e n t B a t . p o s i t i o n = p r e v i o u s P o s i t i o n ; c u r r e n t B a t . v e l o c i t y = p r e v i o u s V e l o c i t y ; c u r r e n t B a t . s c o r e = p r e v i o u s S c o r e ;\n} }\n} re turn b e s t B a t . p o s i t i o n ;\n}"}, {"heading": "Appendix B.2. The Cuckoo Search Algorithm", "text": "C o n f i g u r a t i o n run ( Document document ) {\nf o r ( i n t i = 0 ; i < nes tsNumber ; i ++) { n e s t s [ i ] = new Nest ( ) ; }\nwhi le ( ! s t o p C o n d i t i o n . s t o p ( ) ) {\ni n t i = random . n e x t I n t ( n e s t s . l e n g t h ) ; Nes t new i = n e s t s [ i ] . c l o n e ( ) ; new i . randomFly ( ) ;\ni n t j = random . n e x t I n t ( n e s t s . l e n g t h ) ; whi le ( j == i ) j = random . n e x t I n t ( n e s t s . l e n g t h ) ;\ni f ( new i . g e t S c o r e ( ) > n e s t s [ j ] . g e t S c o r e ( ) ) { n e s t s [ j ] = new i ; }\ns o r t N e s t s ( ) ; a b a n d o n W o r t h l e s s N e s t s ( ) ;\n} s o r t N e s t s ( ) ; re turn n e s t s [ nes t sNumber \u2212 1 ] . c o n f i g u r a t i o n ;\n}\nvoid a b a n d o n W o r t h l e s s N e s t s ( ) {\nf o r ( i n t i = 0 ; i < des t royedNes t sNumber ; i ++) { n e s t s [ i ] = new Nest ( ) ; }\n}\nvoid randomFly ( ) {\ndouble d i s t a n c e = l e v y D i s t r i b u t i o n . sample ( ) ; c o n f i g u r a t i o n . makeRandomChanges ( ( i n t ) d i s t a n c e ) ; needRecomputeScore = t rue ;\n}\np u b l i c double g e t S c o r e ( ) {\ni f ( needRecomputeScore ) {\ns c o r e = c o n f i g u r a t i o n S c o r e r . computeScore ( cur ren tDocument , c o n f i g u r a t i o n ) ; needRecomputeScore = f a l s e ;\n} re turn s c o r e ;\n}"}], "references": [{"title": "Lexical disambiguation using simulated annealing", "author": ["J. Cowie", "J. Guthrie", "L. Guthrie"], "venue": "aou\u0302t", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Evolutionary approach to natural language wsd through global coherence optimization", "author": ["A. Gelbukh", "G. Sidorov", "S.Y. Han"], "venue": "WSEAS Transactions on Communications", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Simulated annealing: theory and applications", "author": ["P. Laarhoven", "E. Aarts"], "venue": "Mathematics and its applications. D. Reidel. URL http://books.google.com/books?id=-IgUab6Dp_IC", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream", "author": ["M. Lesk"], "venue": "cone. SIGDOC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1986}, {"title": "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other", "author": ["H.B. Mann", "D.R. Whitney"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1947}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The Journal of Chemical Physics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1953}, {"title": "A semantic concordance", "author": ["G.A. Miller", "C. Leacock", "R. Tengi", "R.T. Bunker"], "venue": "Proceedings of the Workshop on Human Language Technology. HLT \u201993. Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "D\u00e9sambigu\u0131\u0308sation lexicale de textes : efficacit\u00e9 qualitative et temporelle d\u2019un algorithme \u00e0 colonies de fourmis", "author": ["D. Schwab", "J. Goulian", "A. Tchechmedjiev"], "venue": "TAL", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Worst-case complexity and empirical evaluation of artificial intelligence methods for unsupervised word sense disambiguation", "author": ["D. Schwab", "J. Goulian", "A. Tchechmedjiev"], "venue": "Int. J. Web Engineering and Technology", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Parameter estimation under uncertainty with simulated annealing applied to an ant colony based probabilistic wsd algorithm", "author": ["A. Tchechmedjiev", "D. Schwab", "J. Goulian", "G. S\u00e9rasset"], "venue": "Proceedings of the 1st International Workshop on Optimization Techniques for Human Language Technology", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Using sense-annotated corpora for lexical expansion to improve knowledge-based relatedness measures. In: LREC 2016 (10th Edition of its Language Resources and Evaluation Conference)", "author": ["L. Vial", "A. Tchechmedjiev", "D. Schwab"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "A new metaheuristic bat-inspired algorithm", "author": ["Yang", "X.-S"], "venue": "Nature Inspired Cooperative Strategies for Optimization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Moreover, a supervised classifier requires at least 1000 examples/word to achieve a stable performance, while there are at most 200/word in the largest sense-annotated corpora [7].", "startOffset": 176, "endOffset": 179}, {"referenceID": 10, "context": "While there are few prospects for improvement with supervised system, similarity-based systems offer many avenues for improvement even for English (the best resourced language) [12].", "startOffset": 177, "endOffset": 181}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9, 10], where a SAA and a GA (neighbourhood/local search approaches) were implemented for the WSD problem, with the objective of comparing them to an Ant Colony Algorithm (a constructive approach).", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[9, 10], where a SAA and a GA (neighbourhood/local search approaches) were implemented for the WSD problem, with the objective of comparing them to an Ant Colony Algorithm (a constructive approach).", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This approach is inspired by the annealing process in metallurgy, where the sturdiness of the metal is increased by controlling the rate of cooling applied to it [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "Simulated annealing is a particular instance of the Metropolis-Hastings algorithm [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "[2] where the population is a set of configurations, and the fitness function is the configuration scorer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "The Bat Algorithm The Bat Algorithm was originally introduced by Yang [13] and is inspired by they way bats move through echolocation in order to locate and capture pray.", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "For example, in previous implementation of SAA and GA for WSD, the Lesk similarity measure that calculates the overlap between the definitions of senses [4] was used.", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "100\u20131000) in order to obtain a representative sample of the distribution of possible answers, and use the MannWhitney-U statistical test [5] in addition to the score similarly to the modified Simulated Annealing algorithm used for parameter estimation by Tchechmedjiev et al.", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "This article compares four probabilistic algorithms (global algorithms) for Word Sense Disambiguation (WSD) in terms of the number of scorer calls (local algorithm) and the F1 score as determined by a gold-standard scorer. Two algorithms come from the state of the art, a Simulated Annealing Algorithm (SAA) and a Genetic Algorithm (GA) as well as two algorithms that we first adapt from WSD that are state of the art probabilistic search algorithms, namely a Cuckoo search algorithm (CSA) and a Bat Search algorithm (BS). As WSD requires to evaluate exponentially many word sense combinations (with branching factors of up to 6 or more), probabilistic algorithms allow to find approximate solution in a tractable time by sampling the search space. We find that CSA, GA and SA all eventually converge to similar results (0.98 F1 score), but CSA gets there faster (in fewer scorer calls) and reaches up to 0.95 F1 before SA in fewer scorer calls. In BA a strict convergence criterion prevents it from reaching above 0.89 F1.", "creator": "LaTeX with hyperref package"}}}