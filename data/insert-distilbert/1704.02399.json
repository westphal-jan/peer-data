{"id": "1704.02399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Stein Variational Policy Gradient", "abstract": "policy gradient methods have been successfully applied to many complex reinforcement learning problems. however, policy configuration gradient methods suffer from high variance, slow gibbs convergence, and inefficient exploration. in this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this feedback framework can be reduced to a bayesian inference problem. we then propose writing a novel stein variational policy gradient method ( svpg ) which thereby combines existing policy gradient methods and a repulsive functional to generate a set of diverse options but well - behaved policies. svpg is robust to initialization and can for easily just be implemented in a quasi parallel manner. on continuous control problems, we find that implementing concurrent svpg on top of reinforce and advantage actor - critic algorithms improves both average return and data efficiency.", "histories": [["v1", "Fri, 7 Apr 2017 23:24:07 GMT  (4246kb,D)", "http://arxiv.org/abs/1704.02399v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang liu", "prajit ramachandran", "qiang liu", "jian peng"], "accepted": false, "id": "1704.02399"}, "pdf": {"name": "1704.02399.pdf", "metadata": {"source": "CRF", "title": "Stein Variational Policy Gradient", "authors": ["Yang Liu", "Prajit Ramachandran", "Qiang Liu"], "emails": ["liu301@illinois.edu", "prajitram@gmail.com", "qiang.liu@dartmouth.edu", "jianpeng@illinois.edu"], "sections": [{"heading": null, "text": "Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but wellbehaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency."}, {"heading": "1 Introduction", "text": "Recent advances in policy gradient methods and deep learning have demonstrated their applicability for complex reinforcement learning problems. Instead of deriving a policy from a value function, policy gradient methods directly optimize a parametrized policy with gradients approximated from rollout trajectories. Deep neural networks trained with policy gradient methods have demonstrated impressive performance on continuous control, vision-based navigation and Atari games (Schulman et al., 2015b, Kakade, 2002, Schulman et al., 2015a, Mnih et al., 2016). However, these algorithms are not yet applicable for hard real-world tasks, partially due to high variance, slow convergence, and insufficient exploration. Because of the non-convexity of neural-network policies, the performance of trained agents is often very sensitive to their initializations.\nIn this work, we introduce the Stein variational policy gradient (SVPG) method, a new policy optimization method that leverages a recent Stein variational gradient descent method (Liu and Wang, 2016) to allow simultaneous exploitation and exploration of multiple policies. Unlike traditional policy optimization which attempts to learn a single policy, we model a distribution of policy parameters, where samples from this distribution will represent strong policies. We first introduce a framework that optimizes this distribution of policy parameters with (relative) entropy regularization. The (relative) entropy term explicitly encourages exploration in the parameter space while also optimizing the expected utility of polices drawn from this distribution. We show that this framework can be reduced to a Bayesian inference problem in which we generate samples of policy parameters from a posterior. We then use Stein variational gradient descent (SVGD) to optimize this distribution. SVGD leverages efficient deterministic dynamics to transport a set of particles to approximate given target posterior distributions. It combines the advantages of MCMC, which does not confine the approximation within a parametric family, and variational inference, which converges fast due to deterministic updates that utilize the gradient. Specifically, in SVPG a policy gradient estimator is applied to improve policy parameter particles while a repulsive functional is used to diversify these particles to enable parameter exploration.\nIn our experiments, we implementing SVPG on top of existing policy gradient methods, including REINFORCE (Williams, 1992) and advantageous actor critic, improves average return on continuous control problems. The SVPG versions enjoy better data efficiency through explicit parameter exploration. SVPG is also more robust to different initializations. Because of its simplicity and generality, we believe that SVPG provides a generic tool that can be broadly applied to boost the performance of existing policy optimization methods.\nar X\niv :1\n70 4.\n02 39\n9v 1\n[ cs\n.L G\n] 7\nA pr\n2 01"}, {"heading": "2 Preliminaries", "text": "We introduce the background of reinforcement learning and discuss different policy gradient estimation methods."}, {"heading": "2.1 Reinforcement Learning", "text": "Reinforcement learning considers agents which are able to take a sequence of actions in an environment and experience at most one scalar reward per action. The task of the agents is to learn a policy that maximizes cumulative rewards.\nFormally, consider an agent operating over time t \u2208 {1, . . . , T}. At time t the agent is in an environment state st and produces an action at \u2208 A. The agent will then observe a new state st+1 and receive a scalar reward rt \u2208 R. The set of possible actions A can be discrete or continuous. The goal of reinforcement learning is to find a policy \u03c0(at|st) for choosing an action in state st to maximize a utility function (or expected return)\nJ(\u03c0) = Es0,a0,...[ \u221e\u2211 t=0 \u03b3tr(st, at)],\nwhere 0 \u2264 \u03b3 \u2264 1 is a discount factor; at \u223c \u03c0(at|st) is drawn from the policy; st+1 \u223c P (st+1|st, at) is generated by the environmental dynamics. In this work, we consider policy \u03c0(at|st) to be a stochastic policy which defines the distribution over actions given the current state st. In the model free setting, we assume that the environmental dynamics P (st+1|st, at) is unknown. The state value function\nV \u03c0(st) = Eat,st+1,...[ \u221e\u2211 i=0 \u03b3ir(st+i, at+i)]\nis the expected return by policy \u03c0 from state st. The stateaction function\nQ\u03c0(st, at) = Est+1,at+1,...[ \u221e\u2211 i=0 \u03b3ir(st+i, at+i)]\nis the expected return by policy \u03c0 after taking action at at state st."}, {"heading": "2.2 Policy Gradient Estimation", "text": "In policy-based reinforcement learning approaches, policy \u03c0(a|s; \u03b8) is parameterized by \u03b8 and is iteratively improved by updating \u03b8 to optimize the utility function J(\u03b8); here by an abuse of notation, we denote J(\u03b8) = J(\u03c0(a|s; \u03b8). There are two main approaches to estimate policy gradient from rollout samples.\nFinite difference methods. It is possible to estimate the gradient \u2207\u03b8J(\u03b8) using finite difference methods. Instead\nof computing the finite difference for each individual parameter, rollout efficient methods such as SPSA (Spall, 1998), PEPG (Sehnke et al., 2010) and evolutionary strategy approximations (Hansen, 2006, Mannor et al., 2003) have been proposed. The key idea of such methods is to use a small number of random perturbations to approximate the gradient values of all parameters simultaneously. In particular, the following stochastic finite difference approximation has recently been shown to be highly effective over several complex domains (Salimans et al., 2017):\n\u2207\u03b8J(\u03b8) \u2248 1\nm m\u2211 i=1 J(\u03b8 + h\u03bei)\u03bei h , (1)\nwhere \u03bei are drawn from standard GaussianN (0, I). When h is very small, this estimate provides an accurate approximation of\u2207\u03b8J(\u03b8).\nLikelihood ratio methods. Policy gradient algorithms, such as the well-known REINFORCE (Williams, 1992), estimate the gradient \u2207\u03b8J(\u03b8) from rollout samples generated by \u03c0(a|s; \u03b8) using the likelihood ratio trick. Specifically, REINFORCE uses the following approximator of the policy gradient\n\u2207\u03b8J(\u03b8) \u2248 \u221e\u2211 t=0 \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt (2)\nbased on a single rollout trajectory, where Rt =\u2211\u221e i=0 \u03b3\nir(st+i, at+i) is the accumulated return from time step t. It was shown that this gradient is an unbiased estimation of \u2207\u03b8J(\u03b8). With a careful choice of baseline function b(st), another estimate of policy gradient\n\u2207\u03b8J(\u03b8) \u2248 \u221e\u2211 t=0 \u2207\u03b8 log \u03c0(at|st; \u03b8)(Rt \u2212 b(st)) (3)\nwas shown to be unbiased but with smaller variance. It is common to use the value function V \u03c0(st) as the baseline. Rt \u2212 V \u03c0(st) is an estimate of advantage function A\u03c0(st, at) = Q \u03c0(st, at)\u2212 V \u03c0(st).\n\u2207\u03b8J(\u03b8) \u2248 \u221e\u2211 t=0 \u2207\u03b8 log \u03c0(at|st; \u03b8)(Rt \u2212 V \u03c0(st)) (4)\nThis method is also known as advantage actor critic (A2C) (Schulman et al., 2015b, Mnih et al., 2016). In practice, multiple rollouts are sometimes used to estimate these policy gradient in a batch."}, {"heading": "3 Stein Variational Policy Gradient", "text": "This section introduces our main framework. We start with introducing a maximum entropy policy optimization framework for learning upper level policies in Section 3.1, and then develop our Stein variational policy gradient in Section 3.2."}, {"heading": "3.1 Maximum Entropy Policy Optimization", "text": "Instead of finding a single policy \u03b8, here we consider the policy parameter \u03b8 as a random variable and seek a distribution q(\u03b8) to optimize the expected return. We also introduce a default parameter distribution q0 to incorporate prior domain knowledge of parameters or provide the regularization of \u03b8. We formulate the optimization of q as the following regularized expected utility problem:\nmax q\n{ Eq(\u03b8)[J(\u03b8)]\u2212 \u03b1D(q\u2016q0) } , (5)\nwhere q maximizes the expected utility, regularized by a relative entropy or Kullback-Leibler (KL) divergence D(q\u2016q0) with a \u201cprior distribution\u201d q0,\nD(q\u2016q0) = Eq(\u03b8)[log q(\u03b8)\u2212 log q0(\u03b8)].\nIf we use an uninformative (and improper) prior q0(\u03b8) = const, the second KL term is simplified to the entropy H(q) = Eq(\u03b8)[\u2212 log q(\u03b8)] of q. The optimization\nmax q\n{ Eq(\u03b8)[J(\u03b8)] + \u03b1H(q) } , (6)\nthen explicitly encourages exploration in the \u03b8 parameter space according to the principle of maximum entropy.\nBy taking the derivative of the objective function in (5) and setting it to zero, we can show that the optimal distribution of policy parameter \u03b8 of the above optimization is\nq(\u03b8) \u221d exp ( 1\n\u03b1 J(\u03b8)\n) q0(\u03b8). (7)\nThis formulation is equivalent to a Bayesian formulation of parameter \u03b8, where q(\u03b8) can be seen as the \u201cposterior\u201d distribution; exp(J(\u03b8)/\u03b1) is the \u201clikelihood\u201d function; q0(\u03b8) is the prior distribution. The coefficient \u03b1 can be viewed as a temperature parameter that controls the strength of exploration in the parameter space. When\u03b1\u2192 0, samples drawn from q(\u03b8) will be around global optima (or optimum) of the expected return J(\u03b8).\nA similar idea of entropy regularization has been investigated in the context of bounded relational decision-makers to trade off utility and information costs (Wolpert, 2006, Ortega and Braun, 2011). Learning upper level policies are also discussed in (Salimans et al., 2017), in which the q(\u03b8) distribution is often assumed to be follow certain parametric form, such as a Gaussian distribution, whose parameters are optimized to maximize the expected reward (their method does not use entropy regularization and hence is non-Bayesian). Our method does not require a parametric assumption on q(\u03b8). In many other works, entropy regularization has been proposed directly on policy \u03c0. For example, TRPO (Schulman et al., 2015a) enforces a relative entropy constraint to make sure the current policy stays close\nto a previous policy. Recently (Haarnoja et al., 2017) proposed to leverage the maximum entropy principle on policies and derived a deep energy-based policy optimization method."}, {"heading": "3.2 Stein Variational Gradient Descent", "text": "Traditional Bayesian inference approaches that are used to draw samples from the posterior distribution q(\u03b8), such as MCMC, may suffer from slow convergence or oscillations due to the high-variance and stochastic nature of estimating J(\u03b8). Accurate estimation of J(\u03b8) for a single policy often requires a large number of rollout samples. Instead of utilizing J(\u03b8), we hope to use the gradient information \u2207\u03b8J(\u03b8) which provides a noisy direction in which to update the policy. For this purpose, here we use the Stein variational gradient descent (SVGD) for Bayesian inference (Liu and Wang, 2016). SVGD is a nonparametric variational inference algorithm that leverages efficient deterministic dynamics to transport a set of particles {\u03b8i}ni=1 to approximate given target posterior distributions q(\u03b8). SVGD combines the benefits of typical MCMC and variational inference in that it does not confine the approximation within parametric families, unlike traditional variational inference methods, and converges faster than MCMC due to the deterministic updates that efficiently leverage gradient information. Thus SVGD has a number of advantages that are critical for policy exploration and optimization.\nBriefly, SVGD iteratively updates the \u201cparticles\u201d {\u03b8i} via\n\u03b8i \u2190 \u03b8i + \u03c6\u2217(\u03b8i),\nwhere is a step size and \u03c6(\u00b7) is a function in the unit ball of a reproducing kernel Hilbert space (RKHS) H = H0 \u00d7 \u00b7 \u00b7 \u00b7H0 of d \u00d7 1 vector-valued functions, chosen to maximumly decrease the KL divergence between the particles and the target distribution in that sense that\n\u03c6\u2217 \u2190 max \u03c6\u2208H { \u2212 d d D(\u03c1[ \u03c6]\u2016q), s.t. ||\u03c6||H \u2264 1},\nwhere \u03c1[ \u03c6] denotes the distribution of \u03b8\u2032 = \u03b8 + \u03c6(\u03b8), and the distribution of \u03b8 is \u03c1. Liu and Wang (2016) showed that this functional optimization yields a closed form solution,\n\u03c6\u2217(\u03b8) = E\u03d1\u223c\u03c1[\u2207 log q(\u03d1)k(\u03d1, \u03b8) +\u2207\u03d1k(\u03d1, \u03b8)],\nwhere k(x, x\u2032) is the positive definite kernel associated with the RKHS H0. By replacing the expectation E\u03d1\u223c\u03c1 with an empirical averaging on the current particles {\u03b8i}, we can derive the following Stein variational gradient\n\u03c6\u0302(\u03b8i) = 1\nn n\u2211 j=1 [\u2207\u03b8j log q(\u03b8j)k(\u03b8j , \u03b8i) +\u2207\u03b8jk(\u03b8j , \u03b8i)].\n(8)\nIn this update, \u03c6\u0302 includes two important terms that play different roles.\nAlgorithm 1 Stein Variational Policy Gradient Input: Learning rate , kernel k(x, x\u2032), temperature, initial policy particles {\u03b8i}. for iteration t = 0, 1, .., T do\nfor particle i = 0, 1, .., n do Compute \u2207\u03b8iJ(\u03b8i) e.g., by (1)-(4). end for for particle i = 0, 1, .., n do\n\u2206\u03b8i \u2190 1\nn n\u2211 j=1 [\u2207\u03b8j ( 1 \u03b1 J(\u03b8j) + log q0(\u03b8j) ) k(\u03b8j , \u03b8i)\n+\u2207\u03b8jk(\u03b8j , \u03b8i)] \u03b8i \u2190 \u03b8i + \u2206\u03b8i\nend for end for\nThe first term involves the gradient \u2207\u03b8 log q(\u03b8) which drives the policy particles \u03b8i towards the high probability regions of q with information sharing across similar particles. The second term \u2207\u03b8jk(\u03b8j , \u03b8i) pushes the particles away from each other, diversifying the policies. Note that without the second term, all the particles would collapse to the local modes of log q and the algorithm would reduce to the typical gradient ascent for maximum a posteriori (MAP). In particular, if we just use one particle (n = 1) and choose the kernel with \u2207\u03b8k(\u03b8, \u03b8) = 0, then SVGD is reduced to a single chain of gradient ascent for MAP.\nApplying SVGD to the posterior in (7), we introduce the Stein Variational Policy Gradient method (SVPG) that estimates the policy gradient \u2207\u03b8J(\u03b8) using existing methods, such as those we introduced in (1) -(4).\n\u03c6\u0302(\u03b8i) = 1\nn n\u2211 j=1 [\u2207\u03b8j ( 1 \u03b1 J(\u03b8j) + log q0(\u03b8j) ) k(\u03b8j , \u03b8i)\n+\u2207\u03b8jk(\u03b8j , \u03b8i)] (9)\nNote that here the magnitude of \u03b1 adjusts the relative importance between the policy gradient and the prior term \u2207\u03b8j ( 1 \u03b1J(\u03b8j) + log q0(\u03b8j) ) k(\u03b8j , \u03b8i) and the repulsive term\u2207\u03b8jk(\u03b8j , \u03b8i). A suitable \u03b1 provides a good tradeoff between exploitation and exploration. If \u03b1 is too large, the Stein gradient would only drive the particles to be consistent with the prior q0. As \u03b1 \u2192 0, this algorithm is reduced to running n copies of independent policy gradient algorithms, if {\u03b8i} are initialized very differently. In practice, we found that with a reasonable \u03b1, SVPG consistently outperforms the original versions of several policy gradient methods on continuous control tasks. A careful annealing scheme of \u03b1 allows efficient exploration in the beginning of training and later focuses on exploitation towards the end of training."}, {"heading": "4 Related Work", "text": "Policy gradient techniques have recently shown strong results for deep reinforcement learning. Trust region policy optimization (Schulman et al., 2015a) optimizes its policy by establishing a trust region centered at its current policy. Asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) runs multiple agents asynchronously in parallel to train an actor-critic architecture. Numerous works have built on top of A3C (Jaderberg et al., 2016, Mirowski et al., 2016) to solve difficult environments like navigating a 3D maze (Beattie et al., 2016). Recent work has also explored sample efficient policy gradients (Gu et al., 2016, Wang et al., 2016). Notably, all such methods and improvements can be applied to SVPG without any modification.\nThere has been a wealth of prior work to encourage exploration in reinforcement learning. For example, entropy penalties (Williams and Peng, 1991, Mnih et al., 2016) are used to learn a smoother policy by penalizing the model when the distribution over actions is too sharp. Recent work (Nachum et al., 2016) has explicitly attempted to explore \u2019under-appreciated\u2019 rewards through importance sampling of trajectories over the current policy.\nIntrinsic motivation techniques have recently gained popularity in the deep reinforcement learning setting. These techniques (Singh et al., 2004, Storck et al., 1995) attempt to train the agent with rewards beyond those given from the environment. For example, the notion of curiosity can be used to encourage exploration by giving the agent additional rewards for visiting novel states. Novelty can be measured though Bayesian methods (Mohamed and Rezende, 2015, Houthooft et al., 2016), where information gain is used as a proxy for curiosity. Count-based methods (Bellemare et al., 2016, Ostrovski et al., 2017) produce a reward inversely proportional to the number of times a state has been visited, using a density model to approximate visits in large state spaces. SVPG is compatible with these reward shaping (Ng et al., 1999) procedures since it does not impose any constraints on the parameters and gradients.\nPrior work has also explored the use of different agents to collect experience in parallel. Nair et al. (2015) developed a system called GORILA that deployed multiple actors in parallel to collect experience. Agents differed slightly in parameters due to the asynchronous nature of their training procedure. Mnih et al. (2016) extended this idea by sampling different exploration hyperparameters for the - greedy policy of different agents. Agents with different sets of parameters was also explored by Osband et al. (2016) to encourage deep exploration. In practice, all their agents were trained with the same gradient update. In contrast, SVPG uses different updates for each agent and explicitly encourages diversity through the kernel gradient term.\nTable 1: Best test return and the number of episodes required to reach within 5% of the maximum return.\nA2C Joint episodes Independent episodes SVPG episodes Cartpole Swing Up 308.71 189 419.62 474 436.84 171 Double Pendulum -938.73 46 -256.64 638 -244.85 199 REINFORCE Joint episodes Independent episodes SVPG episodes Cartpole Swing Up 232.96 253 391.46 594 426.69 238 Double Pendulum -892.31 446 -797.45 443 -319.66 327\nFigure 2: State visitation density by REINFORCE-SVPG/Independent algorithms. The he state visitation landscapes of the best four policies learned by SVPG (first row) and Independent agents (second row). The value in the parenthesis is the average return of a policy. All states are projected from a 4D space into a 2D space by t-SNE (Maaten and Hinton, 2008)."}, {"heading": "5 Experiments", "text": "In this section, we design experiments to 1) compare the SVPG versions of policy gradient algorithms with their original versions in terms of convergence and data efficiency, 2) analyze the characteristics of policies simultaneously learned by SVPG, 3) investigate the trade-off between exploration and exploitation in SVPG.\nFor comparisons, we implement the SVPG and the original versions of two popular policy gradient algorithms: REINFORCE (Eq (2)) and the advantage actor critic (A2C) (Eq (4)). In the A2C implementations, we use the generalized advantage estimator (GAE) (Schulman et al., 2015b) for critic, which is implemented in the rllab toolkit (Schulman et al., 2015b, Duan et al., 2016). Only the policy parameters are updated by SVPG, while the critics are updated with the their normal TD-error gradient. After training, the agent that gives the best average return is selected.\nWe use the flat improper prior log q0(\u03b8) = 1 in all our experiments. For SVGD, we follow Liu and Wang (2016) to use the Gaussian RBF kernel k(\u03d1, \u03b8) = exp(\u2212||\u03b8\u2212\u03d1||22/h) with the bandwidth h chosen to bemed2/ log(n+1) where med denotes the median of pairwise distances between the particles {\u03b8i}. This simple heuristic allows the bandwidth to adaptively change as the particles move, ensuring that there is always a significant number of particles that interact with each other.\nSince SVPG is a multi-agent method that simultaneously trains a number of policies, we design two implementations of original REINFORCE and A2C for fairness when comparing data efficiency:\n\u2022 REINFORCE/A2C-Joint: We train a single agent with the same amount of data as used by multiple agents in SVPG. Assume that we use m transition samples for each of the n agents by SVPG during each iteration, we use in total nm samples per iteration to train a single agent here. In this way, we can also view this joint version as a parallel policy gradient implementation with n threads, each accumulating the gradient information in parallel. This joint variation enjoys better gradient estimation, but SVPG\u2019s multiple agents will provide more exploration. This experiment tries to determine whether the repulsive functional in SVPG will encourage exploration that lead to better policies.\n\u2022 REINFORCE/A2C-Independent: We train multiple agents independently with no communication among them. So for each of the n agents, we use m samples to calculate the gradient. After training, the agent that gives best average return is selected. This experiment tries to determine the importance of information sharing between different agents through gradients."}, {"heading": "5.1 Experimental Setting", "text": "All the experiments are constructed with the OpenAI rllab development toolkit (Duan et al., 2016). Specifically, we benchmark on four classical continuous control tasks: Cartpole Swing-Up, Double Pendulumn, Cartpole, and MountainCar. The maximal length of trajectories is set to 500. Details of these environments can be found in (Duan et al., 2016) and on the GitHub website1.\nHere we describe the default setting of the policy gradient algorithms we test on these control tasks. We used a Gaussian policy with a diagonal covariance for all these tasks. The mean is parameterized by a neural networks with three hidden layers (100-50-25 hidden units) and tanh as the activation function. The log standard deviation is parameterized by a vector, as suggested in (Duan et al., 2016, Schulman et al., 2015a). The SVPG and Independent algorithms used m = 10000 samples for policy gradient estimation for each of the n = 16 agents. We found that the choice of m is quite robust as long as it is not too small. For the Joint version, the number of samples used in each iteration is nm = 160, 000. For A2C, we set \u03bb = 1 for the generalized advantage estimation as suggested in (Duan et al., 2016). In the SVPG algorithms, we set \u03b1 = 10, which seems to find a good trade-off between exploration and exploitation in these tasks. We also investigate this hyperparameter in a later experiment. We used ADAM (Kingma and Ba, 2014) to adjust the step-size of gradient descent in all algorithms. For the two easy tasks, Mountain Car and Cartpole, all agents are trained for 50 iterations with 5 reruns. For the two complex tasks, Cartpole Swing-Up and Double Pendulum, we train all agents up to 1000 iterations with 5 re-runs with different random seeds."}, {"heading": "5.2 Convergence and Data Efficiency", "text": "The learning curves of algorithms are shown in Figure 1. On the two easy tasks, Mountain Car and Cartpole, almost all algorithms can successfully solve them in around 20 iterations (except of REINFORCE-Joint on Mountain Car). On the two more challenging tasks, the differences between algorithms are substantial. On Cartpole SwingUp, REINFORCE-SVPG and A2C-SVPG converges substantially faster than the corresponding baselines. A2CIndependent is able to achieve the desirable performance after more than 800 iterations, while all other baselines cannot converge after 1,000 iterations. On Double Pendulum, We also performed the same training with a different batch size m = 5000 and found similar results. The variances of the SVPG algorithms are close to those of the Independent versions, but much smaller than the Joint versions, probably due to the robustness of multiple agents. These results demonstrate that SVPG is able to improve the training convergence of policy gradient algorithms.\n1https://github.com/openai/rllab"}, {"heading": "5.3 SVPG learns strong, diverse policies", "text": "We further wanted to investigate the quality and diversity of the policies learned by SVPG.\nFirst, we computed the best test returns of the policies learned by all algorithms and counted how many episodes each policy needed to achieve the 95% of its best return. Results are shown in Table 1. SVPG versions clearly outperforms policies learned by Joint and Independent versions, in terms of both best test returns and the episodes required to achieve 95% of the return.\nSecond, we compare the quality of all policies learned by SVPG and Independent versions. It is worth noting that in Independent versions, parameter exploration is only done through random initialization, while in SVPG, parameter exploration is explicitly done through repulsion. Thus it is possible that SVPG can better explore the parameter landscape and find multiple good local optima or solutions. We compute average returns of all policies with 50,000 test transitions. The results are then averaged across 5 re-runs with different training random seeds. The sorted policies are shown in Figure 3. SVPG has found many good policies, while only the very top policies learned by Independent algorithms can achieve satisfactory performance.\nThird, we visualize the diversity of the policies learned by SVPG algorithms. Since SVPG learns policies simultaneously, it is possible that all the top policies are very similar. It is worth noting that good policies cannot be starkly different from each other. To illustrate the diversity of the learned policies in SVPG approach, we generated the statevisitation landscape of individual policies. We chose the best four policies learned by REINFORCE-SVPG and the best four by REINFORCE-Independent and randomly generate 100 test trajectories for each policy. Then all states are projected into 2D space with the t-SNE algorithm (Maaten and Hinton, 2008, Van Der Maaten, 2014, Ulyanov, 2016) for visualization. Then we plotted the density contours of state-visitation frequencies for each policy in Figure 2. Note that the 2D projection may introduce artifacts in the visualization.\nNotwithstanding artifacts, it seems that the state visitation landscapes of the policies learned by SVPG are different from each other. Each policy covers certain regions in the projected state space with high visitation frequencies. Three policies learned by the Independent algorithm (#2- #4) are quite similar to each other, but it average return of these policies are far from the best average return. It is worth noting that the best policy by the Independent algorithm looks similar to SVPG policy #3, and it is possible that they are close to the same local optimum.\nThese observations indicate that SVPG can learn robust and diverse policies thanks to the balance between information sharing and repulsive forces."}, {"heading": "5.4 Exploration and Exploitation", "text": "The above results demonstrate that SVPG can better explore in the parameter space than the original policy gradient methods. In SVPG, the temperature hyperparameter \u03b1 controls the trade-off between exploitation, which is driven by policy gradient, and exploration, which is driven by the repulsion between parameters of different policies. Lower temperatures make the algorithm focus on exploitation, as the first term with the policy gradient becomes much more important than the second repulsive term. Higher temperature will drive the policies to be more diverse. That is,\nn\u2211 j=1 [\u2207\u03b8j 1 \u03b1 J(\u03b8j)k(\u03b8j , \u03b8i)\ufe38 \ufe37\ufe37 \ufe38\nexploitation\n+ \u2207\u03b8jk(\u03b8j , \u03b8i)\ufe38 \ufe37\ufe37 \ufe38 exploration ].\nFigure 4 shows an ablation over different values of the temperature hyperparameter \u03b1 with batch size m = 5, 000.\nWhen the temperature is very high (e.g. \u03b1 = 100), exploration dominates exploitation. Policies are repelled too much from each other, which prevents them from converging to a good set of policies. When the temperature is too low (\u03b1 \u2192 0), exploitation dominates exploration. The\nalgorithm is then similar to training a set of independent policies. As training proceeding, the Euclidean distances between parameters of policies become very large. As a result, both information sharing through kernel and interagent exploration are no longer effective. An intermediate \u03b1 = 10 balances between exploration and exploitation, and outperforms other temperatures for both actor-critic and REINFORCE. We believe that a good annealing scheme of \u03b1 enables efficient exploration in the beginning of policy training and while focusing on exploitation in the later stage for policy tuning."}, {"heading": "6 Conclusion", "text": "In this work, we first introduced a maximum entropy policy optimization framework. By modeling a distribution of policy parameters with the (relative) entropy regularization, this framework explicitly encourages exploration in the parameter space while also optimizing the expected utility of polices generated from this distribution. We showed that this framework can be reduced to a Bayesian inference problem and then propose the Stein variational policy gradient to allows simultaneous exploitation and exploration of multiple policies. In our experiments, we evaluated the SVPG versions of REINFORCE and A2C on several continuous control tasks and found that they greatly improved the performance of the original policy gradient methods. We also performed extensive analysis of SVPG to demonstrate its robustness and the ability to generate diverse policies. Due to its simplicity, we expect that SVPG provides a generic approach to improve existing policy gradient methods. The parallel nature of SVPG also makes it attractive to implement in distributed systems.\nThere are a lot of potential future directions based on this work. The simplest extension is to measure the empirical performance of SVPG in other RL domains. Furthermore, the choices of kernel in SVPG may be critical for high dimensional RL problems, and future work should explore the impact of the choice of kernel. For example, it is possible to design a layer-wise kernel for neural network policies. Another direction is to further study the trade-off between exploration and exploitation in SVPG. Currently, we simply choose a fixed \u03b1 to control the ratio between the policy gradient and the repulsive terms. Smarter annealing schemes of \u03b1 may lead to a better and adaptive trade-off. Finally, it would be very interesting to extend SVPG for other policy optimization methods such as natural policy gradient and trust region policy optimization."}], "references": [{"title": "Unifying countbased exploration and intrinsic motivation", "author": ["M. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "Advances in Neural Information Processing Systems, pages 1471\u2013 1479.", "citeRegEx": "Bellemare et al\\.,? 2016", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "Proceedings of the 33rd International Conference on Machine Learning (ICML).", "citeRegEx": "Duan et al\\.,? 2016", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["S. Gu", "T. Lillicrap", "Z. Ghahramani", "R.E. Turner", "S. Levine"], "venue": "arXiv preprint arXiv:1611.02247.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Reinforcement learning with deep energy-based policies", "author": ["T. Haarnoja", "H. Tang", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1702.08165.", "citeRegEx": "Haarnoja et al\\.,? 2017", "shortCiteRegEx": "Haarnoja et al\\.", "year": 2017}, {"title": "The cma evolution strategy: a comparing review", "author": ["N. Hansen"], "venue": "Towards a new evolutionary computation, pages 75\u2013102. Springer.", "citeRegEx": "Hansen,? 2006", "shortCiteRegEx": "Hansen", "year": 2006}, {"title": "Vime: Variational information maximizing exploration", "author": ["R. Houthooft", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1109\u20131117.", "citeRegEx": "Houthooft et al\\.,? 2016", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397.", "citeRegEx": "Jaderberg et al\\.,? 2016", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "Advances in neural information processing systems, 2:1531\u20131538.", "citeRegEx": "Kakade,? 2002", "shortCiteRegEx": "Kakade", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Q. Liu", "D. Wang"], "venue": "Advances In Neural Information Processing Systems, pages 2370\u20132378.", "citeRegEx": "Liu and Wang,? 2016", "shortCiteRegEx": "Liu and Wang", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "L. v. d.", "G. Hinton"], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten et al\\.,? 2008", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "The cross entropy method for fast policy search", "author": ["S. Mannor", "R.Y. Rubinstein", "Y. Gat"], "venue": "ICML, pages 512\u2013519.", "citeRegEx": "Mannor et al\\.,? 2003", "shortCiteRegEx": "Mannor et al\\.", "year": 2003}, {"title": "Learning to navigate in complex environments", "author": ["P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.03673", "citeRegEx": "Mirowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["S. Mohamed", "D.J. Rezende"], "venue": "Advances in neural information processing systems, pages 2125\u20132133.", "citeRegEx": "Mohamed and Rezende,? 2015", "shortCiteRegEx": "Mohamed and Rezende", "year": 2015}, {"title": "Improving policy gradient by exploring under-appreciated rewards", "author": ["O. Nachum", "M. Norouzi", "D. Schuurmans"], "venue": "arXiv preprint arXiv:1611.09321.", "citeRegEx": "Nachum et al\\.,? 2016", "shortCiteRegEx": "Nachum et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S Petersen"], "venue": null, "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "ICML, volume 99, pages 278\u2013287.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Information, utility and bounded rationality", "author": ["D.A. Ortega", "P.A. Braun"], "venue": "International Conference on Artificial General Intelligence, pages 269\u2013274. Springer.", "citeRegEx": "Ortega and Braun,? 2011", "shortCiteRegEx": "Ortega and Braun", "year": 2011}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "Advances In Neural Information Processing Systems, pages 4026\u20134034.", "citeRegEx": "Osband et al\\.,? 2016", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Count-based exploration with neural density models", "author": ["G. Ostrovski", "M.G. Bellemare", "Oord", "A. v. d.", "R. Munos"], "venue": "arXiv preprint arXiv:1703.01310.", "citeRegEx": "Ostrovski et al\\.,? 2017", "shortCiteRegEx": "Ostrovski et al\\.", "year": 2017}, {"title": "Evolution strategies as a scalable alternative to reinforcement learning", "author": ["T. Salimans", "J. Ho", "X. Chen", "I. Sutskever"], "venue": "arXiv preprint arXiv:1703.03864.", "citeRegEx": "Salimans et al\\.,? 2017", "shortCiteRegEx": "Salimans et al\\.", "year": 2017}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML, pages 1889\u20131897.", "citeRegEx": "Schulman et al\\.,? 2015a", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438.", "citeRegEx": "Schulman et al\\.,? 2015b", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Parameterexploring policy gradients", "author": ["F. Sehnke", "C. Osendorfer", "T. R\u00fcckstie\u00df", "A. Graves", "J. Peters", "J. Schmidhuber"], "venue": "Neural Networks, 23(4):551\u2013 559.", "citeRegEx": "Sehnke et al\\.,? 2010", "shortCiteRegEx": "Sehnke et al\\.", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning", "author": ["S.P. Singh", "A.G. Barto", "N. Chentanez"], "venue": "NIPS, volume 17, pages 1281\u20131288.", "citeRegEx": "Singh et al\\.,? 2004", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "An overview of the simultaneous perturbation method for efficient optimization", "author": ["J.C. Spall"], "venue": "Johns Hopkins apl technical digest, 19(4):482\u2013492.", "citeRegEx": "Spall,? 1998", "shortCiteRegEx": "Spall", "year": 1998}, {"title": "Reinforcement driven information acquisition in nondeterministic environments", "author": ["J. Storck", "S. Hochreiter", "J. Schmidhuber"], "venue": "Proceedings of the international conference on artificial neural networks, Paris, volume 2, pages 159\u2013164. Citeseer.", "citeRegEx": "Storck et al\\.,? 1995", "shortCiteRegEx": "Storck et al\\.", "year": 1995}, {"title": "Muticore-tsne", "author": ["D. Ulyanov"], "venue": "https://github. com/DmitryUlyanov/Muticore-TSNE.", "citeRegEx": "Ulyanov,? 2016", "shortCiteRegEx": "Ulyanov", "year": 2016}, {"title": "Accelerating t-sne using treebased algorithms", "author": ["L. Van Der Maaten"], "venue": "Journal of machine learning research, 15(1):3221\u20133245.", "citeRegEx": "Maaten,? 2014", "shortCiteRegEx": "Maaten", "year": 2014}, {"title": "Sample efficient actor-critic with experience", "author": ["Z. Wang", "V. Bapst", "N. Heess", "V. Mnih", "R. Munos", "K. Kavukcuoglu", "N. de Freitas"], "venue": "replay. arXiv preprint arXiv:1611.01224", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["R.J. Williams", "J. Peng"], "venue": "Connection Science, 3(3):241\u2013268.", "citeRegEx": "Williams and Peng,? 1991", "shortCiteRegEx": "Williams and Peng", "year": 1991}, {"title": "Information theorythe bridge connecting bounded rational game theory and statistical physics", "author": ["D.H. Wolpert"], "venue": "Complex Engineered Systems, pages 262\u2013 290. Springer.", "citeRegEx": "Wolpert,? 2006", "shortCiteRegEx": "Wolpert", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "In this work, we introduce the Stein variational policy gradient (SVPG) method, a new policy optimization method that leverages a recent Stein variational gradient descent method (Liu and Wang, 2016) to allow simultaneous exploitation and exploration of multiple policies.", "startOffset": 179, "endOffset": 199}, {"referenceID": 31, "context": "In our experiments, we implementing SVPG on top of existing policy gradient methods, including REINFORCE (Williams, 1992) and advantageous actor critic, improves", "startOffset": 105, "endOffset": 121}, {"referenceID": 26, "context": "Instead of computing the finite difference for each individual parameter, rollout efficient methods such as SPSA (Spall, 1998), PEPG (Sehnke et al.", "startOffset": 113, "endOffset": 126}, {"referenceID": 24, "context": "Instead of computing the finite difference for each individual parameter, rollout efficient methods such as SPSA (Spall, 1998), PEPG (Sehnke et al., 2010) and evolutionary strategy approximations (Hansen, 2006, Mannor et al.", "startOffset": 133, "endOffset": 154}, {"referenceID": 21, "context": "In particular, the following stochastic finite difference approximation has recently been shown to be highly effective over several complex domains (Salimans et al., 2017):", "startOffset": 148, "endOffset": 171}, {"referenceID": 31, "context": "Policy gradient algorithms, such as the well-known REINFORCE (Williams, 1992), estimate the gradient \u2207\u03b8J(\u03b8) from rollout samples generated by \u03c0(a|s; \u03b8) using the likelihood ratio trick.", "startOffset": 61, "endOffset": 77}, {"referenceID": 21, "context": "Learning upper level policies are also discussed in (Salimans et al., 2017), in which the q(\u03b8) distribution is often assumed to be follow certain parametric form, such as a Gaussian distribution, whose parameters are optimized to maximize the expected reward (their method does not use entropy regularization and hence is non-Bayesian).", "startOffset": 52, "endOffset": 75}, {"referenceID": 22, "context": "For example, TRPO (Schulman et al., 2015a) enforces a relative entropy constraint to make sure the current policy stays close to a previous policy.", "startOffset": 18, "endOffset": 42}, {"referenceID": 3, "context": "Recently (Haarnoja et al., 2017) proposed to leverage the maximum entropy principle on policies and derived a deep energy-based policy optimization method.", "startOffset": 9, "endOffset": 32}, {"referenceID": 9, "context": "For this purpose, here we use the Stein variational gradient descent (SVGD) for Bayesian inference (Liu and Wang, 2016).", "startOffset": 99, "endOffset": 119}, {"referenceID": 9, "context": "Liu and Wang (2016) showed that this functional optimization yields a closed form solution,", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "Trust region policy optimization (Schulman et al., 2015a) optimizes its policy by establishing a trust region centered at its current policy.", "startOffset": 33, "endOffset": 57}, {"referenceID": 13, "context": "Asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) runs multiple agents asynchronously in parallel to train an actor-critic architecture.", "startOffset": 42, "endOffset": 61}, {"referenceID": 15, "context": "Recent work (Nachum et al., 2016) has explicitly attempted to explore \u2019under-appreciated\u2019 rewards through importance sampling of trajectories over the current policy.", "startOffset": 12, "endOffset": 33}, {"referenceID": 17, "context": "SVPG is compatible with these reward shaping (Ng et al., 1999) procedures since it does not impose any constraints on the parameters and gradients.", "startOffset": 45, "endOffset": 62}, {"referenceID": 15, "context": "Nair et al. (2015) developed a system called GORILA that deployed multiple actors in parallel to collect experience.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Mnih et al. (2016) extended this idea by sampling different exploration hyperparameters for the greedy policy of different agents.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "of parameters was also explored by Osband et al. (2016) to encourage deep exploration.", "startOffset": 35, "endOffset": 56}, {"referenceID": 23, "context": "In the A2C implementations, we use the generalized advantage estimator (GAE) (Schulman et al., 2015b) for critic, which is implemented in the rllab toolkit (Schulman et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 9, "context": "For SVGD, we follow Liu and Wang (2016) to use the Gaussian RBF kernel k(\u03b8, \u03b8) = exp(\u2212||\u03b8\u2212\u03b8||2/h) with the bandwidth h chosen to bemed/ log(n+1) where med denotes the median of pairwise distances between the particles {\u03b8i}.", "startOffset": 20, "endOffset": 40}, {"referenceID": 1, "context": "All the experiments are constructed with the OpenAI rllab development toolkit (Duan et al., 2016).", "startOffset": 78, "endOffset": 97}, {"referenceID": 1, "context": "Details of these environments can be found in (Duan et al., 2016) and on the GitHub website1.", "startOffset": 46, "endOffset": 65}, {"referenceID": 1, "context": "For A2C, we set \u03bb = 1 for the generalized advantage estimation as suggested in (Duan et al., 2016).", "startOffset": 79, "endOffset": 98}, {"referenceID": 8, "context": "We used ADAM (Kingma and Ba, 2014) to adjust the step-size of gradient descent in all algorithms.", "startOffset": 13, "endOffset": 34}], "year": 2017, "abstractText": "Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but wellbehaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency.", "creator": "LaTeX with hyperref package"}}}