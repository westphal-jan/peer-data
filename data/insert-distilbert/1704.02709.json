{"id": "1704.02709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments", "abstract": "we introduce an approach to implicit semantic role labeling ( isrl ) based on a remarkably recurrent underlying neural semantic frame model set that learns probability distributions over sequences of explicit semantic frame arguments. on utilizing the nombank isrl test set, the approach results in better state - of - the - art performance though with much previously less reliance on manually constructed language resources.", "histories": [["v1", "Mon, 10 Apr 2017 04:48:53 GMT  (185kb,D)", "http://arxiv.org/abs/1704.02709v1", null], ["v2", "Thu, 5 Oct 2017 10:32:17 GMT  (184kb,D)", "http://arxiv.org/abs/1704.02709v2", "IJCNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["quynh ngoc thi do", "steven bethard", "marie-francine moens"], "accepted": false, "id": "1704.02709"}, "pdf": {"name": "1704.02709.pdf", "metadata": {"source": "CRF", "title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments", "authors": ["Quynh Ngoc Thi Do", "Steven Bethard", "Marie-Francine Moens"], "emails": ["quynhngocthi.do@cs.kuleuven.be", "bethard@cis.uab.edu", "sien.moens@cs.kuleuven.be"], "sections": [{"heading": "1 Introduction", "text": "Semantic role labeling (SRL) has traditionally focused on semantic frames consisting of verbal or nominal predicates and explicit arguments that occur within the clause or sentence that contains the predicate. However, many predicates, especially nominal ones, may bear arguments in discourse (Ruppenhofer et al., 2010; Gerber et al., 2009). These arguments, called implicit arguments, are resolved by another semantic task, Implicit Semantic Role Labeling (iSRL). Let us borrow a NomBank (Meyers et al., 2004) annotation example from Gerber and Chai (2012):\nA SEC proposal to ease [A1 reporting] [PRED requirements] [A2 for company executives]. . .\nThe NomBank role set for requirement includes A0, the requirer, A1, the thing required, and A2, the thing being required. In the example above, NomBank does not annotate an A0. However, a reasonable interpretation of the sentence is that the SEC plays the requirer role. Such an implicit argument is known as an implicit semantic role, and identifying such roles is the focus of this paper.\nAs an emerging task, implicit semantic role labeling (iSRL) faces a lack of resources. First, manual implicit role annotations for use as training data are seriously limited: the one existing NomBank-based dataset covers just ten predicates (Gerber and Chai,\n2010). Second, most existing iSRL systems depend on other systems (named entity taggers, explicit semantic role labelers, etc.), and as a result need not only iSRL annotations, but annotations for all of their sub-systems as well.\nSeveral approaches have attempted to address this lack of resources. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories. Schenk and Chiarcos (2016) aimed at a simpler strategy that would not rely on manual iSRL annotations, requiring only an existing corpus of explicit SRL annotations: induce prototypical roles from large amounts of explicit SRL annotations and distributed word representations. However, the model performance was almost 10 points lower than the state-of-the-art Laparra and Rigau (2013).\nWe propose an iSRL approach that works in the low-resource setting of Schenk and Chiarcos (2016), but improves state-of-the-art performance by predicting selectional preferences learned through a recurrent neural semantic frame model that learns probability distributions over sequences of explicit semantic frame arguments."}, {"heading": "2 Neural Semantic Frame Model", "text": "Our goal is to use unlabeled data to acquire selectional preferences that characterize how likely a phrase is to be an argument of a semantic frame. We rely on the fact that current explicit SRL sys-\nar X\niv :1\n70 4.\n02 70\n9v 1\n[ cs\n.C L\n] 1\n0 A\npr 2\n01 7\ntems achieve high performance on verbal predicates, and run a state-of-the-art explicit SRL system on unlabeled data. We then construct a predictive recurrent neural semantic frame model (PRNSFM) from these predicted frames and roles.\nOur PRNSFM views semantic frames as a sequence: a predicate, followed by the arguments in the order they appear in the text, and terminated by a special EOS symbol. The model\u2019s task is to take a predicate and zero or more arguments, and predict the next argument in the sequence, or EOS if no more arguments will follow. We draw predicates from PropBank verbal semantic frames, and represent arguments with their nominal/pronominal heads. For example, Michael Phelps swam at the Olympics is represented as [swam:PRED, Phelps:A0, Olympics:AMLOC, EOS], where the predicate is labeled PRED and the arguments Phelps and Olympics are labeled A0 and AM-LOC, respectively.\nFormally, for each tth argument of a semantic frame f , we denote its word (e.g., Phelps) as wf,t, its semantic label (e.g., A0) as lf,t, its preceding argument words as wf,<t, and its preceding argument semantic labels as lf,<t, where w \u2208 V, the word vocabulary, and l \u2208 L \u222a [PRED], the set of semantic labels. We denote predicates in the same way as arguments: wf,0 and lf,0. Our model then aims to estimate the conditional probability of the occurrence of wf,t as semantic role lf,t given the preceding argument words and their labels:\nP (wf,t:lf,t|wf,<t:lf,<t)\nWe use a recurrent neural network to learn this probability distribution over sequences of semantic frame arguments. For a semantic frame f with N arguments, at each time step 0 \u2264 t \u2264 N , given the input wf,t:lf,t, the model computes the distribution P (wf,t+1:lf,t+1|wf,<t+1:lf,<t+1) and predicts the next most likely argument (or EOS). During training, model parameters are optimized by minimizing prediction errors over all time steps.\nWe consider two versions of this model that differ in input (Vin) and output (Vout) vocabularies."}, {"heading": "2.1 Model 1: 1-1 LSTM Model", "text": "Model 1 treats the word and semantic label as a single unit in both input and output layers. For example, Phelps:A0 would be a single value. Our model consists of three layers (see Figure 1(a)):\nEmbedding Layer is a matrix of size |Vin| \u00d7 d\nthat maps each unit of input into an d-dimensional vector. The matrix is initialized randomly and updated during network training.\nLong short-term memory (LSTM) Layer consists of m LSTM units which take as input the output of the embedding layer, xt, and produce an output ht by updating at every time step 0 \u2264 t \u2264 T :\nit = sigmoid(Wixt + Uiht\u22121 + bi)\nC\u0302t = tanh(Wcxt + Ucht\u22121 + bc)\nft = sigmoid(Wfxt + Ufht\u22121 + bf )\nCt = it \u2217 C\u0302t + ft \u2217 Ct\u22121 ot = sigmoid(Woxt + Uoht\u22121 + bo) ht = ot \u2217 tanh(Ct)\nwhere Wi,Wc,Wf ,Wo are weight matrices of size d\u00d7m; Ui, Uc, Uf , Uo are weight matrices of size m\u00d7m; bi, bc, bf , bo are bias vectors of sizem; and \u2217 is element-wise multiplication.\nSoftmax Layer computes the probability distribution of the next argument given the preceding arguments at time step t:\nP (wf,t+1:lf,t+1|wf,<t+1:lf,<t+1) = softmax(htW + b) (1)\nwere W is a weight matrix of size m\u00d7|Vout|, and b is a bias vector of size |Vout|. The predicted next argument is:\nargmax wf,t+1:lf,t+1\nP (wf,t+1:lf,t+1|wf,<t+1:lf,<t+1)\nThe network is trained using the negative log likelihood loss function."}, {"heading": "2.2 Model 2: 2-1 LSTM Model", "text": "Model 2 considers the word and the semantic label as two different units in the input layer. For example, Phelps:A0 is considered as two separate values Phelps and A0. As shown in Figure 1(b), we use two different embedding layers, one for word values and one for semantic labels, and the two embedding vectors are concatenated before being passed to the LSTM layer. The LSTM and softmax layers are then the same as in Model 1. The\nembedding layer for word values can be initialized with pre-trained word embeddings, e.g., (Mikolov et al., 2013; Pennington et al., 2014); the embedding layer for labels is initialized randomly."}, {"heading": "2.3 Selectional Preferences", "text": "To assist an iSRL model, we extract from the PRNSFM selectional preferences that indicate how likely a word is to be an argument of a semantic frame evoked by a predicate.\nGiven a predicate p, arguments of the semantic frame f evoked by p can be predicted sequentially by the trained predictive model. We define the selectional preference as P (w:l|p), the probability of a wordw being the l argument of frame f . Our goal is to approximate P (w:l|p). For each sequence q in the set of all possible argument sequences predicted by the model before t (St), we select a threshold k and generate argmaxk,s, the top k word-label pairs that have the highest probability of being the next argument. Formally:\nS0 = {[p:PRED]} St+1 = {[q, wt+1:lt+1] :\nq \u2208 St, wt+1:lt+1 \u2208 argmaxk,q}\nwith wf,0:lf :0 = p:PRED. For each sequence q = [p:PRED, w1:l1, w2:l2, . . . , wt:lt] \u2208 St, with t \u2265 1, we define P (p:PRED) = 1 and compute the probability of the sequence as:\nP (q) = P (wt:lt|wt\u22121:lt\u22121, . . . , p : PRED) \u00d7 P (wt\u22121:lt\u22121|wt\u22122:lt\u22122, . . . , p:PRED) \u00d7 . . . \u00d7 P (w1:l1|p:PRED) (2)\nOur goal, P (w:l|p) is approximated as the sum of probabilities that w:l is predicted by our model at all time steps t up to a threshold T :\nP (w:l|p) \u2248 \u2211\n0\u2264t\u2264T P (w:l|wf,<t+1:lf,<t+1)\n\u2248 \u2211\n0\u2264t\u2264T \u2211 q\u2208St P (w:l|q)\u00d7 P (q)\nwhere P (w:l|q) and P (q) are computed by Equation 1 and Equation 2, respectively. An example of the calculation of P (q) is shown in Figure 2."}, {"heading": "3 Implicit Semantic Role Labeling", "text": "Using only an explicit SRL system, we extract selectional preferences from our PRNSFM as following: For each triple of a nominal predicate np, a word candidate w, and a label l, the selectional preference score of w to be the implicit argument\np:PRED\nw 1 :l 1\nw w:lw:l\nP 1\nP 2\nP w\nP 11 P 12\nP w1\nP 21 P\n22\nP w2\nP w11 P\nw12\nw 2 :l 2 w:l\nw 11 :l 11 w 12 :l 12\nw 21 :l 21 w 22 :l 22\nw:lw:lw:lw:l\nP w21 P w22\nTime 0: S 0 = [[p:PRED]]\nTime 1: S 1 = [[p:PRED,w 1 :l 1 ], [p:PRED,w 2 :l 2 ]]\nTime 2: S 2 = [[p:PRED,w 1 :l 1 ,w 11 :l 11 ], [[p:PRED,w 1 :l 1 ,w 12 :l 12 ],\nrole l of np is approximated as:\nP (w:l|np) = maxp\u2208V (np)P (w:l|p)\nwhere P (w:l|p) is the selectional preference score produced by our PRNSFM, and V (np) is set of verbal forms of np. For example, for the noun funds, V (funds) = {funds, fund, funding, funded}.\nWe apply selectional preferences to iSRL in a similar way to (Laparra and Rigau, 2013). For each nominal predicate np and implicit label l, we consider a \u201ccontext window\u201d of the previous two sentences and the current sentence. Each sentence in the context window is annotated with the explicit SRL system. If there are any other instances of np or V (np) in the text that have an explicit argument of type l, deterministically predict the closest such argument as the implicit l argument of np. Otherwise, run the PRNSFM over each word in the context window, and select the word with the highest selectional preference score above a threshold s. We optimized this threshold on the development data, resulting in s = 0.0003.\nAs in Laparra and Rigau (2013), the selectional preferences can be updated by using sentence recency factor to emphasize recent candidates. The selectional preference score p is updated as p\u2032 = p \u2212 z + z \u00d7 \u03b1d where d is the sentence distance, and \u03b1 and z are parameters. We set z = 0.00005 based on the development set and set \u03b1 = 0.5 as in (Laparra and Rigau, 2013)."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Building PRNSFM", "text": "Semantic Role Labeling We use the full pipeline from MATE1 (Bjo\u0308rkelund et al., 2010) as the explicit SRL system. The system is retrained on the CoNLL 2009 training portion. Unannotated Data The unannotated data includes Wikipedia2, Reuters3, and Brown4. Dataset for PRNSFM The first 15 milion short and medium (less than 100 words) sentences from the unannotated data were annotated automatically by the SRL system. The annotated results were then used together with the gold standard CoNLL 2009 SRL training data to train the PRNSFM. To evaluate how well the system acquires knowledge from unlabeled data, we also train PRNSFM on only the gold standard CoNLL 2009 training data. Neural network training and inference Parameters were selected using an evaluation over the CoNLL 2009 development set. We set the dimensions of word and label embeddings in the PRNSFM as 50 and 16 respectively. The hidden sizes of LSTM layers are the same as their input sizes. Word embedding layers are initialized by Skip-gram embeddings learned by training the word2vec tool (Mikolov et al., 2013) on unannotated data. Our models were trained for 120 epochs using the AdaDelta optimization algorithm (Zeiler, 2012). For fast selectional preference computing, we set k = 1 and T = 3."}, {"heading": "4.2 Evaluation", "text": "We follow the evaluation setting in Gerber and Chai (2010); Laparra and Rigau (2013); Schenk and Chiarcos (2016)5: the method is evaluated on the evaluation portion of the nominal iSRL data by Dice coefficient metrics."}, {"heading": "4.3 Results", "text": "Table 1 shows the prior state-of-the-art and the performance of our PRNSFM-based models. We include a baseline system (Skip-gram) that is trained on the same unlabeled and labeled data as the PRNSFM, but treats the predicates and arguments\n1https://code.google.com/archive/p/mate-tools/ 2http://corpus.byu.edu/wiki/ 3http://about.reuters.com/researchandstandards/corpus/ 4https://catalog.ldc.upenn.edu/ldc99t42 5 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained.\nOur Model 2 outperforms all other models in terms of precision and F1 scores6. This is notable since the first two models require many more language resources than just an explicit SRL system: Gerber and Chai (2010) use WordNet and manually annotated iSRL data, while Laparra and Rigau (2013) use WordNet, named entity annotations, and manual semantic category mappings. Schenk and Chiarcos (2016), like our approach, uses only an explicit SRL system, but both of our models strongly outperform their result.\nTable 1 also shows that training on large unlabeled data results in a marked improvement compared to training on only the CoNLL 2009 labeled data, evidence that the models have acquired linguistic knowledge from the unlabeled data. Moreover, the better performance of our models over the standard Skip-gram proves the effectiveness of modeling semantic frames as sequential data."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented the idea of predictive recurrent neural semantic frame models for learning probability distributions over semantic argument sequences. The predicted selectional preferences are applied to an iSRL task. Based on our evaluations on the NomBank iSRL dataset, our best model improves state-of-the-art performance while reducing the amount of language resources needed. In future work, we plan to evaluate how our predictive models can be applied to other tasks.\n6Calculating statistical significance is challenging, because item-level predictions from the other systems are not publicly available. As an overly conservative estimate, we take a t-test over the 10 predicate-level F1 scores (available in published papers and only omitted in this draft due to space constraints). Comparing against Model 2, this yields p=0.28 for Gerber and Chai (2010), p=0.46 for Laparra and Rigau (2013), and most importantly p=0.058 for Schenk and Chiarcos (2016)."}], "references": [{"title": "Efficient estimation of word", "author": ["frey Dean"], "venue": null, "citeRegEx": "Dean.,? \\Q2013\\E", "shortCiteRegEx": "Dean.", "year": 2013}, {"title": "Inducing Implicit Arguments from Comparable Texts: A Framework and its Applications", "author": ["Michael Roth", "Anette Frank."], "venue": "Computational Linguistics 41:625\u2013664.", "citeRegEx": "Roth and Frank.,? 2015", "shortCiteRegEx": "Roth and Frank.", "year": 2015}, {"title": "Semeval-2010 task 10: Linking events and their participants in discourse", "author": ["Josef Ruppenhofer", "Caroline Sporleder", "Roser Morante", "Collin Baker", "Martha Palmer."], "venue": "Proceedings of the 5th International Workshop on Semantic Evalua-", "citeRegEx": "Ruppenhofer et al\\.,? 2010", "shortCiteRegEx": "Ruppenhofer et al\\.", "year": 2010}, {"title": "Unsupervised learning of prototypical fillers for implicit semantic role labeling", "author": ["Niko Schenk", "Christian Chiarcos."], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Schenk and Chiarcos.,? 2016", "shortCiteRegEx": "Schenk and Chiarcos.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "However, many predicates, especially nominal ones, may bear arguments in discourse (Ruppenhofer et al., 2010; Gerber et al., 2009).", "startOffset": 83, "endOffset": 130}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories.", "startOffset": 0, "endOffset": 187}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories.", "startOffset": 0, "endOffset": 213}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories. Schenk and Chiarcos (2016) aimed at a simpler strategy that would not rely on manual iSRL annotations, requiring only an existing corpus of explicit SRL annotations: induce prototypical roles from large amounts of explicit SRL annotations and distributed word representations.", "startOffset": 0, "endOffset": 628}, {"referenceID": 1, "context": "Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art Laparra and Rigau (2012). Laparra and Rigau (2013) proposed an approach that did not require any manual iSRL annotations, based on exploiting argument coherence over different instances of a predicate, but this method required a large set of manuallyconstructed resources: explicit SRL annotations, WordNet super-senses, named entity annotations, and a manual mapping from SuperSenseTagger semantic classes to general semantic categories. Schenk and Chiarcos (2016) aimed at a simpler strategy that would not rely on manual iSRL annotations, requiring only an existing corpus of explicit SRL annotations: induce prototypical roles from large amounts of explicit SRL annotations and distributed word representations. However, the model performance was almost 10 points lower than the state-of-the-art Laparra and Rigau (2013).", "startOffset": 0, "endOffset": 987}, {"referenceID": 3, "context": "We propose an iSRL approach that works in the low-resource setting of Schenk and Chiarcos (2016), but improves state-of-the-art performance by predicting selectional preferences learned through a recurrent neural semantic frame model that learns probability distributions over sequences of explicit semantic frame arguments.", "startOffset": 70, "endOffset": 97}, {"referenceID": 4, "context": "Our models were trained for 120 epochs using the AdaDelta optimization algorithm (Zeiler, 2012).", "startOffset": 81, "endOffset": 95}, {"referenceID": 3, "context": "We follow the evaluation setting in Gerber and Chai (2010); Laparra and Rigau (2013); Schenk and Chiarcos (2016)5: the method is evaluated on the evaluation portion of the nominal iSRL data by Dice coefficient metrics.", "startOffset": 86, "endOffset": 113}, {"referenceID": 3, "context": "edu/ldc99t42 5 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained.", "startOffset": 25, "endOffset": 52}, {"referenceID": 3, "context": "edu/ldc99t42 5 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained.", "startOffset": 25, "endOffset": 124}, {"referenceID": 3, "context": "8 Schenk and Chiarcos (2016) 33.", "startOffset": 2, "endOffset": 29}, {"referenceID": 3, "context": "Schenk and Chiarcos (2016), like our approach, uses only an explicit SRL system, but both of our models strongly outperform their result.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "058 for Schenk and Chiarcos (2016).", "startOffset": 8, "endOffset": 35}], "year": 2017, "abstractText": "We introduce an approach to implicit semantic role labeling (iSRL) based on a recurrent neural semantic frame model that learns probability distributions over sequences of explicit semantic frame arguments. On the NomBank iSRL test set, the approach results in better state-of-the-art performance with much less reliance on manually constructed language resources.", "creator": "LaTeX with hyperref package"}}}