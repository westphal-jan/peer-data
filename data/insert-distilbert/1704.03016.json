{"id": "1704.03016", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Automatic semantic role labeling on non-revised syntactic trees of journalistic texts", "abstract": "semantic role labeling ( srl ) is a natural language processing task that enables the pattern detection of events described in sentences included and the participants of these events. for brazilian portuguese ( bp ), there are two studies recently concluded that perform srl in journalistic texts. [ 1 ] obtained f1 - measure scores of 79. 6, using the propbank. da br corpus, which has syntactic trees manually revised, [ 8 ], without using a treebank for training, obtained f1 - measure scores of 68. 0 for the same corpus. however, the use of specially manually revised syntactic trees for this task does not represent a real scenario of application. the goal however of this paper is to evaluate the performance measurement of srl on revised and non - revised syntactic trees using a larger and balanced corpus of bp journalistic texts. first, we have shown that [ 1 ]'s system also performs better than [ 8 ]'s system on the otherwise larger corpus. similarly second, the srl programming system trained on non - revised syntactic trees performs better over non - revised trees than a system trained on gold - standard corpus data.", "histories": [["v1", "Mon, 10 Apr 2017 19:02:18 GMT  (31kb)", "http://arxiv.org/abs/1704.03016v1", "PROPOR International Conference on the Computational Processing of Portuguese, 2016, 8 pages"]], "COMMENTS": "PROPOR International Conference on the Computational Processing of Portuguese, 2016, 8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nathan siegle hartmann", "magali sanches duran", "sandra maria alu\\'isio"], "accepted": false, "id": "1704.03016"}, "pdf": {"name": "1704.03016.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n03 01\n6v 1\n[ cs\n.C L\n] 1\n0 A\npr 2\n01 7\nthe detection of events described in sentences and the participants of these events. For Brazilian\nPortuguese (BP), there are two studies recently concluded that perform SRL in journalistic texts.\n[1] obtained F1-measure scores of 79.6, using the PropBank.Br corpus, which has syntactic trees\nmanually revised; [8], without using a treebank for training, obtained F1-measure scores of 68.0\nfor the same corpus. However, the use of manually revised syntactic trees for this task does not\nrepresent a real scenario of application. The goal of this paper is to evaluate the performance of SRL\non revised and non-revised syntactic trees using a larger and balanced corpus of BP journalistic\ntexts. First, we have shown that [1]\u2019s system also performs better than [8]\u2019s system on the larger\ncorpus. Second, the SRL system trained on non-revised syntactic trees performs better over non-\nrevised trees than a system trained on gold-standard data.\nKeywords: Semantic role labeling. Non-revised syntactic trees. Brazilian Portuguese."}, {"heading": "1 Introduction", "text": "Semantic Role Labelling (SRL) is a Natural Language Processing (NLP) task responsible for detecting events described in sentences and the participants of these events [11]. The events are held by predicators, such as verbs and eventive names (some nouns, adjectives and adverbs) and the participants are called arguments. This work focuses on verbs.\nTo automatically annotate a text with semantic roles, most current SRL systems employ Machine Learning (ML) techniques. When using ML, the SRL task is generally performed on syntatic trees due to the extensive set of features that have been identified in the syntactic structure of a sentence, such as those presented by [11].\nHowever, as the syntactic trees of a sentence are generated by automatic parsers and these tools are subject to errors, problematic trees are often manually revised by linguists. For Brazilian Portuguese SRL, the best performance was obtained by [1], with F1-measure scores of 79.6 when annotating revised syntactic trees, without reported outcomes for non-revised trees. However, the use of corrected trees does not represent a real scenario of application. In this sense, Fonseca\u2019s work [8], following an approach that does not use syntactic features, obtained F1-measure scores of 68.0 in Portuguese sentences. It is known that SRL systems using syntactic features perform better than those that do not use them. However, there are no SRL results for Portuguese on non-revised syntactic trees.\nThis work evaluates the use of revised and non-revised syntactic trees for manual and automatic SRL tasks in Brazilian Portuguese. We demonstrate that human annotation errors are directly related to annotation errors made by machines. We attribute these errors to problematic syntactic trees generated by the parser. We also show that, for a good performance in automatic SRL on non-revised syntactic trees, it is necessary to train the SRL system with the same type of data and/or invest in improving the parser used to preprocess the corpus.\nIn Section 2, we show the corpora collected and compiled in this work. In Section 3, we present the methodology for manual annotation of the corpus whose non-revised syntactic trees are annotated with semantic roles. In Section 4, we present the state-of-art SRL system for Portuguese and a system whose methodology does not rely on syntactic errors. In Section 5, we show conducted experiments and the obtained results. At last, Section 6 presents this work\u2019s conclusions."}, {"heading": "2 Selection of the Corpora", "text": "To evaluate the SRL task on syntactic trees with errors, we annotated semantic roles in a new corpus compiled for this work, whose syntactic trees had not been revised, and also used a corpus annotated with semantic roles, whose syntactic trees had been manually revised, so that we could compare the results. The syntactic trees of both corpora were generated by the PALAVRAS parser [2]. The corpora used in this work are PropBank.Br version 1.11 [6], referred to as PB-Br.v1 and a selection of the PLN-Br, corpus of texts from Folha de Sa\u0303o Paulo [3], referred to as PB-Br.v22.\nIn the PB-Br.v1 corpus, we observed that many verbs have only one annotation instance and this data sparsity is undesirable for machine learning purposes. When it comes to learning annotation of semantic roles, we have to consider three aspects: (i) which verbs are represented in the corpus; (ii) which meanings of verbs are represented in the corpus; and (iii) which syntactic alternations are represented in the corpus. Alternation is changing the order of constituents (syntactic, semantic, or both at once). For example, the passive voice is a syntactic alternation marking a semantic alternation (the patient takes the place of the agent in the syntactic subject position). Normally, the number of meanings for a single verb is associated with the amount of syntactic alternations that it admits, but there may be verbs that admit a large number of alternations for the same meaning. Our goal in compiling the PB-Br.v2 was to get as much representation as possible in the three items above with the lowest number of annotation instances, since manual annotation is costly. The sentences were divided into classes according to linguistic criteria and considering the ML, as we chose to start the annotation process with the most frequent verbs. Using corpus statistics, we found that verbs with a frequency higher than 1000 represent 90% of verb occurrences in the corpus. We assume that they are good material for training a classifier of semantic roles, because if the classifier learns to rank well 90% of the corpus it should have good accuracy. In addition, the highly frequent verbs, excluding auxiliary, copula verbs and verbs that require clausal complements, are probably the most polysemous in the language. More information about the selection of sentences for the PB-Br.v2 can be found in [7]3.\nThe PB-Br.v1 corpus contains 5,931 annotated instances of 3,348 sentences and the PB-Br.v2 contains 7,661 annotated instances of 7,442 sentences. We generate instances of a sentence for each verb contained in it. Furthermore, we only selected instances whose syntactic trees generated by the parser are related, i.e. all elements of the syntactic tree are connected. Section 3 presents the results of the manual annotation of semantic roles on the selection made for the PB-Br.v2."}, {"heading": "3 Manual Annotation of the PB-Br.v2", "text": "The manual annotation of semantic roles was performed on 7,661 selected instances of the PB-Br.v2, following the annotation of the PropBank project [10], but based on annotation guidelines4 customized to the Portuguese language enriched with wrong syntactic trees annotation process. The Tiger XML output of the PALAVRAS syntactic parser was used, in the same format of the PB-Br.v1 corpus. Furthermore, Tiger XML syntactic trees can be processed with the SALTO tool [4] that was used in annotating the corpus in question.\nA group of seven annotators and one adjudicator participated in the annotation process. The annotators were trained and received a copy of the annotation guidelines. In addition, a repository of verbs and their meanings, called Verbo-Brasil, was available during the annotation5. The task consisted of annotating the meaning of the verb \u2013 to identify the set of expected semantic roles \u2013 and assigning semantic role labels chosen from a set of six numbered arguments (ArgN) and 12 modifiers (ArgM). The description of each role is detailed in the annotation manual. The annotation scheme followed the double-blind standard, in which two annotators annotate the same portion of instances and, afterwards, the adjudicator solves disagreements. We distribute the annotation of instances by blocks, gathering instances of the same verb in the same task. Separately, the sets of copula verbs and verbs that require clausal complements were distributed. The same annotation instance was never assigned to more than two annotators.\n1 Available at http://nilc.icmc.usp.br/portlex/images/arquivos/propbank-br/PropBankBr_v1.1.xml.zip. 2 Available at http://nilc.icmc.usp.br/semanticnlp/propbankbr/pbbr-v2.html . 3 Available at http://nilc.icmc.usp.br/semanticnlp/propbankbr/relat.html. 4 Available at http://nilc.icmc.usp.br/semanticnlp/propbankbr/manual.html. 5 Available at http://143.107.183.175:12680/verbobrasil/.\nAfter concluding the annotation, we calculated the Kappa statistics [5]. As the SRL task traditionally consists of two steps: identification of arguments and classification of semantic roles of each argument, we calculated the individual Kappa for each step: 0,96 and 0,90 for copula verbs; 0,96 and 0,95 for clausal complement verbs; and 0,79 and 0,75 for verbs with no syntactic pattern. We also calculated the Kappa statistics of the identification of verb meaning: 1,0 for copula verbs, 1.0 for clausal complement verbs and 0,92 for verbs with no syntactic pattern.\nThe set of verbs that require clausal complements obtained important Kappa results, considered almost perfect by the Kappa scale of Landis and Koch [9]. As the annotation of this set is very predictable and has little syntactic diversity, the identification and classification obtained almost maximum values. The annotation of copula verbs also obtained almost perfect results. In this scenario, however, there is a greater syntactic diversity, which led to a drop in the agreement of argument classification compared to the set of verbs that require clausal complements. This means that the annotators agreed in identifying that a particular syntactic node is the verb argument, but disagreed in selecting its semantic role. Finally, the set of plain verbs that present several alternations obtained a substantial Kappa that, in the used Kappa scale, is lower than that of the other annotated verb sets. For this set, we found that there was disagreement in selecting the meaning of verbs, which triggered disagreements about the detection of arguments and the selection of semantic roles. Additionally, this is the set of verbs whose annotation is far from being predictable as they present a lot of syntactic alternation, i.e., syntactic constituents may appear in different orders, which affects the distribution of roles. After analyzing human annotation, we separated the portion in which there was disagreement between the annotators and sent it to the adjudicator, who solved the disagreements and generated a final version of the annotation. Table 1 shows the number of instances in which there was full agreement between annotators and those that had to be revised. We noted there was disagreement in over 50% of the instances. This means that, despite the high Kappa values, most instances had to be revised. However, this does not imply a high rate of disagreement \u2013 as suggested by the Kappa values. For example, an instance is sent for adjudication even if annotators have agreed on 9 out of 10 annotated arguments and disagreed on just one.\nIt is important to note that many syntactic trees contain errors generated by the parser. As the annotation guidelines do not cover how to deal with all possible errors generated by the parser, the free interpretation of how to annotate these inconsistent trees may have contributed to the number of disagreements obtained."}, {"heading": "4 SRL Systems", "text": "This paper analyzes the performance of two newly developed systems to annotate semantic roles of texts written in Brazilian Portuguese. Fonseca [8] developed a system for annotation of semantic roles for Brazilian Portuguese, avoiding dependence on external NLP tools, such as syntactic parser. Its results on revised syntactic trees of the PB-Br.v1 corpus were a F1-measure of 68.0. The work of Alva-Manchego [1], which uses the PB-Br.v1 corpus for training, employed a supervised approach for automatic annotation of semantic roles on syntactic trees of Brazilian Portuguese sentences. The system performance on revised syntactic trees of the PB-Br.v1 corpus was a F1-measure of 79.6. The main difference between the systems is the use of syntactic features (the former does not use them). In addition, none of the systems evaluated their performance on non-revised syntactic trees. The contrast on the SRL performance on revised and non-revised syntactic trees has never been done for Portuguese. Yet, few studies relate the causes of errors in human annotation with the errors generated by the machine in the SRL task, and this is one of the contributions of our work."}, {"heading": "5 Experiments", "text": "In this section, we present the experiments developed to contrast the performance of SRL systems trained on syntactic trees revised by humans (treebanks or gold-standard data) and non-revised syntactic trees. We used Alva-Manchego\u2019s system [1] in most experiments because it is the state-of-art system in SRL for Brazilian Portuguese. We also used Fonseca\u2019s system [8] in a final experiment to contrast its performance with the one of Alva-Manchego\u2019s system. Fonseca\u2019s system does not use syntactic trees and, therefore, it does not suffer from the syntactic errors generated by the parser. In the scenario of the PB-Br.v2, it is interesting to note whether the errors generated by the parser are significant to cause the AlvaManchego\u2019s system to have a SRL performance lower than Fonseca\u2019s.\nThe experiments reported below evaluate Alva-Manchego\u2019s system on different sets of the PB-Br.v2: (i) the Agreement set, composed of instances where the annotators fully agreed, therefore, not sent for adjudication; (ii) the Adjudication set, composed of instances that were sent for adjudication; and (iii) the Full set, comprising Agreement and Adjudication.\nFirst, we conducted a 10-fold cross-validation for the set where there was agreement between PBBr.v2 annotators. The following results are categorized by \u201ccorr.\u201d indicating correct labeling, \u201cexcess\u201d indicating a mislabeled annotation, \u201cmissed\u201d indicating an argument that was not selected as a candidate to annotation, \u201cprec.\u201d as precision, \u201crec.\u201d as recall and \u201cF1\u201d as F-measure. The results in Table 2 show a reasonable indication of the SRL quality, since the Agreement set has approximately 50% of the amount of PB-Br.v1 instances used by Alva-Manchego. To contrast it with the Agreement set, we also checked the quality of the SRL on the Adjudication set. The results in Table 3 show that the Adjudication set, in spite of being approximately 80% greater than the PB-Br.v1 and approximately 40% greater than the Agreement set, does not allow an easy (or simple) automatic annotation of semantic roles. We can interpret that if humans find it hard to annotate, the machine will have the same problem \u2013 which justifies the difference of F1 scores of 6.98 on the performance of the Agreement (F1 = 72.71) and Adjudication (F1 = 65.73) sets. We can also speculate that the parsing errors, which caused different annotation interpretations to the human eye, hindered the automatic learning of the task because it increases data sparsity.\nIn the 10-fold cross-validation experiment on the Full set of the PB-Br.v2, we obtained F1 of 69.12 (Table 4), which is between the 65.73 (Adjudication set) and 72, 71 (Agreement set) values. We can speculate that despite the Adjudication set has more instances and results lower than the Agreement set, the latter supplies the system with syntactic trees without errors or trees that have frequent errors for which a standard treatment is predicted in the guidelines, and therefore can be identified by ML. It is also interesting to mention that the F1 difference between the system trained on the PB-Br.v1 gold-standard data and the system trained on the non-revised syntactic trees of the PB-Br.v2 is 10.48 F1 scores. The difference of around 10.0 F1 scores between a system trained on revised trees and on non-revised trees has already been investigated by [10] and [12] for the English language.\nThe contrast between the F1 values for the PB-Br.v1 and the PB-Br.v2 is noteworthy. Although the selection made on the PB-Br.v2 respects a distribution that should favor the SRL and this corpus is approximately 24% larger than the PB-Br.v1, the results show a system with 10.48 F1 scores lower\nthan the result obtained from the system trained by Alva-Manchego on the PB-Br.v1 corpus (F1 69.12 against 79.6). The evidence that the use of automatic parser without human revision adversely affects the performance of SRL systems become more evident when we contrast the results in Table 6 with those of Table 7. The question, however, is whether this drop on performance actually represents a system with worse overall performance, or if the annotation in the automatic parser scenario is difficult to the point that the F1 value of the trained system in PB-Br.v2 (Table 4) is considered good. To this end, we conducted two experiments: one annotating the PB-Br.v1 corpus with the system trained on the Full set of the PB-Br.v2 and another in the opposite direction \u2013 annotating the Full set of the PB-Br.v2 with the system trained on the PB-Br.v1 corpus.\nWe can see in Table 6 that the SRL in revised syntactic trees, using Alva-Manchego\u2019s system trained on non-revised syntactic trees, is feasible, even though its quality is not comparable to that of a system trained on revised trees (F1 72.62 versus F1 79.6). We also noted that the value obtained of 72.62 is greater than the 10-fold cross-validation value of 69.12 of the system trained on the PB-Br.v2. Thus, we realized that in addition to a system trained on non-revised syntactic trees being able to annotate syntactic trees containing errors, it could annotate perfect trees with a performance superior than what it annotates in its own scenario. This means that training a SRL system on non-correct syntactic trees makes the system capture syntactic patterns of correct trees and learn to deal with the noise contained in not syntactically correct trees.\nDoing the opposite direction of annotation, we can see in Table 7 that the quality of the SRL of the PB-Br.v2 problematic trees using the system trained on PB-Br.v1 is 14.36 F1 scores lower than the result of the 10-fold cross-validation for the Full set of PB-Br.v2 (F1 of 69.12 against 54.76). This result shows that a system trained on perfect syntactic trees faces greater difficulty when annotating faulty syntactic trees. As the PB-Br.v1 gold standard corpus does not contain noisy examples, a minimum deviation from the correct pattern of syntactic tree is enough to cause the system to make annotation errors.\nTo strengthen the analysis of results, we annotated the PB-Br.v2 Agreement and Adjudication sets with the system trained on the PB-Br.v1. Table 8 shows the result of 58.96 F1 in the annotation of the Agreement set of the PB-Br.v2 by the system trained in PB-Br.v1 and Table 9 presents F1 of 52.66 for annotation of the Adjudication set. The difference in the performance of annotation of the sets was 6.3 F1 scores, while the difference of the 10-fold cross-validation performed on these sets was 6.98 F1 scores. As the difference in the performance of annotation of the sets is close, we understand that, indeed, there is a syntactic complexity threshold that distinguishes the Agreement and the Adjudication sets of the PB-Br.v2. Table 10 and Table 11 show confusion matrices for the results presented on Table 6 and Table 7. These matrices show that our SRL system trained on noisy data performs better when annotating revised trees (Table 9) than the system trained on revised trees when annotating noisy data (Table 10). For example, there are 32% of AM-TMP syntactic nodes not identified by the SRL trained on PB-Br.v.1 and tested on noisy data (PB-Br.v2) whereas there are only 6% of AM-TMP not identified by the SRL trained on PB-Br.v2 and tested on the treebank (PB.Br-v1).\nFinally, we conducted a 10-fold cross-validation experiment with Fonseca\u2019s system and the PB-Br.v2 full. The F1 value of 53,58 for the system that does not use syntactic features presented in Table 5 shows that even with the use of syntactic trees with errors, the SRL approach for syntactic trees is better."}, {"heading": "6 Conclusions", "text": "The obtained results show that a SRL system responsive to syntactic errors, trained on noisy data (nonrevised syntactic trees), performs a better SRL on noisy data than when it is trained on revised trees (treebank). We also noted that the SRL system obtained better results when tested on the set in which there was full agreement between annotators and lower results when tested on the set in which there was disagreement between annotators. We did not include in PB-Br.v2 any annotation mark to distinguish well-formed parse trees from those containing parsing errors. For this reason, it was not possible to verify whether the existence of parsing errors are correlated to the drop in inter-annotator agreement rates. During the adjudication process, however, we realized that this correlation probably exists and it would be worthwhile to explore this hypothesis in future work. Based on confusion matrices\u2019 results, however, we speculate that the learning from non-revised trees allows the system to better identify the SRL candidates. We also noted that Fonseca\u2019s system [8] performs a SRL inferior to Alva-Manchego\u2019s [1], even in an unfavorable scenario for the latter system. We believe that, for Brazilian Portuguese, the use of syntactic trees for the SRL task is still the most promising means and, therefore, efforts to the improvement of syntactic parsers are welcome. We also understand that in a real scenario of application (on-the-fly processing), the data is passed directly from the syntactic parser to the SRL system without human intervention to correct the trees. Therefore, we can say that in a real scenario of application, the training of a SRL system on non-revised syntactic trees corpus, such as the PB-Br.v2, provides better annotation of journalistic texts than the system trained on revised syntactic trees corpus (PB-Br.v1)."}, {"heading": "Acknowledgments", "text": "Part of the research developed for this work was sponsored by Samsung Eletro\u0302nica da Amazo\u0302nia Ltda. under the terms of Brazilian federal law number 8.248/91. Part of the results presented in this paper were obtained through research activity in the project titled \u201cSemantic Processing of Brazilian Portuguese Texts\u201d, sponsored by Samsung Eletro\u0302nica da Amazo\u0302nia Ltda. under the terms of Brazilian federal law number 8.248/91."}], "references": [{"title": "Semantic role labeling for brazilian portuguese: A benchmark", "author": ["F.E. Alva-Manchego", "J.L.G. Rosa"], "venue": "Advances in Artificial Intelligence \u2013 IBERAMIA 2012, Lecture Notes in Computer Science, vol. 7637, pp. 481\u2013490. Springer Berlin Heidelberg", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "The Parsing System \u201cPalavras\u201d: Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework", "author": ["E. Bick"], "venue": "Aarhus University Press Aarhus", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Anota\u00e7\u00e3o Ling\u01d8\u0131stica em XML do Corpus PLN-BR", "author": ["M. Bruckschen", "F. Muniz", "J. Souza", "J. Fuchs", "K. Infante", "M. Muniz", "P. Gon\u00e7alves", "R. Vieira", "S. Al\u00fa\u0131sio"], "venue": "NILC\u2013TR\u201309\u201308. Tech. rep., University of S\u00e3o Paulo, Brazil", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Salto \u2013 a versatile multi-level annotation tool", "author": ["A. Burchardt", "K. Erk", "A. Frank", "A. Kowalski", "S. Pado"], "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006). pp. 517\u2013520", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Assessing agreement on classification tasks: The kappa statistic", "author": ["J. Carletta"], "venue": "Computacional Linguistics 22(2), 249\u2013254", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Propbank-br: a brazilian treebank annotated with semantic role labels", "author": ["M.S. Duran", "S.M. Al\u00fa\u0131sio"], "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC-2012). pp. 1862\u20131867", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Sele\u00e7\u00e3o de senten\u00e7as do c\u00f3rpus PLN-Br para compor o c\u00f3rpus de anota\u00e7\u00e3o de pap\u00e9is sem\u00e2nticos Propbank-Br.v2", "author": ["M.S. Duran", "L. Sep\u00falveda-Torres", "M.C. Viviani", "N.S. Hartmann", "S.M. Al\u00fa\u0131sio"], "venue": "NILC-TR-14-07. Tech. rep.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A two-step convolutional neural network approach for semantic role labeling", "author": ["E.R. Fonseca", "J.L.G. Rosa"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on Neural Networks. pp. 1\u20137", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "The measurement of observer agreement for categorical data", "author": ["J.R. Landis", "G.G. Koch"], "venue": "Biometrics 33(1), pp. 159\u2013174", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1977}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["M. Palmer", "D. Gildea", "P. Kingsbury"], "venue": "Computational Linguistics 31(1), 71\u2013106", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Semantic role Labeling, Synthesis Lectures on Human Language Technologies, vol", "author": ["M. Palmer", "D. Gildea", "N. Xue"], "venue": "3. Morgan & Claypool Publishers", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A global joint model for semantic role labeling", "author": ["K. Toutanova", "A. Haghighi", "C.D. Manning"], "venue": "Computational Linguistics 34(2), 161\u2013191", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "[1] obtained F1-measure scores of 79.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "manually revised; [8], without using a treebank for training, obtained F1-measure scores of 68.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "First, we have shown that [1]\u2019s system also performs better than [8]\u2019s system on the larger", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "First, we have shown that [1]\u2019s system also performs better than [8]\u2019s system on the larger", "startOffset": 65, "endOffset": 68}, {"referenceID": 10, "context": "Semantic Role Labelling (SRL) is a Natural Language Processing (NLP) task responsible for detecting events described in sentences and the participants of these events [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "When using ML, the SRL task is generally performed on syntatic trees due to the extensive set of features that have been identified in the syntactic structure of a sentence, such as those presented by [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 0, "context": "For Brazilian Portuguese SRL, the best performance was obtained by [1], with F1-measure scores of 79.", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "In this sense, Fonseca\u2019s work [8], following an approach that does not use syntactic features, obtained F1-measure scores of 68.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "The syntactic trees of both corpora were generated by the PALAVRAS parser [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "1 [6], referred to as PB-Br.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "v1 and a selection of the PLN-Br, corpus of texts from Folha de S\u00e3o Paulo [3], referred to as PB-Br.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "v2 can be found in [7].", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "v2, following the annotation of the PropBank project [10], but based on annotation guidelines customized to the Portuguese language enriched with wrong syntactic trees annotation process.", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "Furthermore, Tiger XML syntactic trees can be processed with the SALTO tool [4] that was used in annotating the corpus in question.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "After concluding the annotation, we calculated the Kappa statistics [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "The set of verbs that require clausal complements obtained important Kappa results, considered almost perfect by the Kappa scale of Landis and Koch [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Fonseca [8] developed a system for annotation of semantic roles for Brazilian Portuguese, avoiding dependence on external NLP tools, such as syntactic parser.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "The work of Alva-Manchego [1], which uses the PB-Br.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "We used Alva-Manchego\u2019s system [1] in most experiments because it is the state-of-art system in SRL for Brazilian Portuguese.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "We also used Fonseca\u2019s system [8] in a final experiment to contrast its performance with the one of Alva-Manchego\u2019s system.", "startOffset": 30, "endOffset": 33}, {"referenceID": 9, "context": "0 F1 scores between a system trained on revised trees and on non-revised trees has already been investigated by [10] and [12] for the English language.", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "0 F1 scores between a system trained on revised trees and on non-revised trees has already been investigated by [10] and [12] for the English language.", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "We also noted that Fonseca\u2019s system [8] performs a SRL inferior to Alva-Manchego\u2019s [1], even in an unfavorable scenario for the latter system.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "We also noted that Fonseca\u2019s system [8] performs a SRL inferior to Alva-Manchego\u2019s [1], even in an unfavorable scenario for the latter system.", "startOffset": 83, "endOffset": 86}], "year": 2017, "abstractText": "Semantic Role Labeling (SRL) is a Natural Language Processing task that enables the detection of events described in sentences and the participants of these events. For Brazilian Portuguese (BP), there are two studies recently concluded that perform SRL in journalistic texts. [1] obtained F1-measure scores of 79.6, using the PropBank.Br corpus, which has syntactic trees manually revised; [8], without using a treebank for training, obtained F1-measure scores of 68.0 for the same corpus. However, the use of manually revised syntactic trees for this task does not represent a real scenario of application. The goal of this paper is to evaluate the performance of SRL on revised and non-revised syntactic trees using a larger and balanced corpus of BP journalistic texts. First, we have shown that [1]\u2019s system also performs better than [8]\u2019s system on the larger corpus. Second, the SRL system trained on non-revised syntactic trees performs better over nonrevised trees than a system trained on gold-standard data.", "creator": "LaTeX with hyperref package"}}}