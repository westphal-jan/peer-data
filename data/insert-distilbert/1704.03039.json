{"id": "1704.03039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Semantically Consistent Regularization for Zero-Shot Recognition", "abstract": "the role of semantics in zero - shot learning is considered. the effectiveness of previous approaches is analyzed according to the form of supervision provided. while some learn semantics independently, others only supervise the semantic subspace explained by training classes. note thus, the former concept is able to constrain the whole space but lacks the ability to model semantic correlations. the latter addresses this issue but leaves part of the semantic space unsupervised. this complementarity is exploited in a new convolutional neural network ( cnn ) framework, which proposes the use of semantics as constraints for recognition. although a cnn trained for classification has absolutely no transfer ability, this can be encouraged by learning an immediately hidden existing semantic code layer together with a semantic code for classification. two forms of semantic constraints are then introduced. the first is a loss - detection based regularizer that introduces a generalization constraint on each semantic predictor. the second criterion is a codeword regularizer process that favors semantic - to - class mappings consistent with prior semantic knowledge while allowing these to be learned from data. significant improvements over computing the state - of - the - art are achieved on several datasets.", "histories": [["v1", "Mon, 10 Apr 2017 19:59:33 GMT  (1689kb,D)", "http://arxiv.org/abs/1704.03039v1", "Accepted to CVPR 2017"]], "COMMENTS": "Accepted to CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["pedro morgado", "nuno vasconcelos"], "accepted": false, "id": "1704.03039"}, "pdf": {"name": "1704.03039.pdf", "metadata": {"source": "CRF", "title": "Semantically Consistent Regularization for Zero-Shot Recognition", "authors": ["Pedro Morgado", "Nuno Vasconcelos"], "emails": ["pmaravil@ucsd.edu", "nuno@ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "Significant advances in object recognition have been recently achieved with the introduction of deep convolutional neural networks (CNNs). The main limitation of this approach is the effort required to 1) collect and annotate millions of images necessary to train these models, and 2) the complexity of training a CNN from scratch. In fact, most recent computer vision papers use or adapt a small set of popular models, such as AlexNet [28], GoogLeNet [54], and VGG [51], learned from the Imagenet dataset [13]. Hence, there is an interest in techniques for transfer learning, where\nThis work was funded by graduate fellowship SFRH/BD/109135/2015 from the Portuguese Ministry of Sciences and Education and NRI Grants IIS-1208522 and IIS-1637941 from the National Science Foundation.\na model learned on a dataset is used to recognize object classes that are not represented in it. Ideally, transfer learning methods would replicate the human ability to recognize objects from a few example images or even from a description in terms of concepts in some semantic vocabulary.\nThis has motivated the introduction of semantic representations for object recognition [34, 44, 45, 55, 56], which rely on a predefined vocabulary of visual concepts to define a semantic space S and a set of classifiers to map each image into that space. The scores of these classifiers can then be used as semantic features for object classification. Furthermore, because simple rules of thumb can be designed, a priori, to describe new object classes in terms of these semantics, the image mapping into S can be exploited to recognize previously unseen objects. This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].\nThe fundamental difficulty of ZSL is that training cannot be guided by the end goal of the classifier. While the recognizer is learned from a set of training classes, it must provide accurate predictions for image classification into a non-overlapping set of unseen or zero-shot (ZS) classes. Historically, early efforts were devoted to the identification of good semantics for ZSL. This motivated the collection of datasets containing images annotated with respect to semantics such as visual attributes [14,31]. Subsequent works addressed the design of the semantic space S, using one of two strategies previously proposed in the semantic representation literature. The first, recognition using independent semantics (RIS), consists of learning an independent classifier per semantic [34, 55, 56]. Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58]. Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.\nThis motivated a shift to the second strategy, which ties the design of S to the goal of recognition, by learning a single multi-class classifier that optimally discriminates between all training classes [44, 45]. The difficulty of extending this approach to ZSL is that the semantics of interest\nar X\niv :1\n70 4.\n03 03\n9v 1\n[ cs\n.C V\n] 1\n0 A\npr 2\n01 7\nare not the classes themselves. [2] proposed an effective solution to this problem by noting that there is a fixed linear transformation, or embedding, between the semantics of interest and the class labels, which can be specified by hand, even for ZS classes. This was accomplished using a label embedding function \u03c6, to map each class y into a vector \u03c6(y) in the space of attributes. Recently, various works have proposed variations on this approach [1,4,35,43,47,49]. We refer to this class of methods as recognition using semantic embeddings (RULE). By learning all semantics simultaneously, RULE is able to leverage dependencies between concepts, thus addressing the main limitation of RIS.\nIn this work, we investigate the advantages and disadvantages of the two approaches for implementations based on deep learning and CNNs. We show that, in this context, the two methods reduce to a set of constraints on the CNN architecture: RIS learns a bank of independent CNNs, and RULE uses a single CNN with fixed weights in the final layer. It follows that the performance of the two approaches is constrained by the form in which supervision is provided on the space A of image attributes. While RIS provides supervision along each dimension independently, RULE does so along the subspace spanned by the label embedding directions \u03c6(y). Because the number of attributes is usually larger than classes, this exposes the strengths and weaknesses of the two approaches. On one hand, RIS supervises all attributes but cannot model their dependencies. On the other, RULE models dependencies but leaves a large number of dimensions of A unconstrained.\nTo exploit this complementarity, we propose a new framework denoted Semantically COnsistent REgularization (SCoRe) that leverages the advantages of both RIS and RULE. This is achieved by recognizing that the two methods exploit semantics as constraints for recognition. While RIS enforces first-order constraints (single semantics), RULE focuses second-order (linear combinations). However, both are suboptimal for ZSL. RIS ignores the recognition of training classes, sacrificing the modeling of semantic dependencies, and RULE ignores a large subspace of A and fixes network weights. SCoRe addresses these problems by exploiting the view of a CNN as an optimal classifier with respect to a multidimensional classification code, implemented at the top CNN layer. It interprets this code as a mapping between semantics (layer before last) and classes (last layer). It then enforces both first and secondorder regularization constraints through a combination of 1) an RIS like loss-based regularizer that constraints semantic predictions, and 2) a codeword regularizer that favors classification codes consistent with RULE embeddings."}, {"heading": "2. Previous Work", "text": "Semantics Semantics are visual descriptions that convey meaning about an image x \u2208 X , and may include any\nmeasurable visual property: discrete or continuous, numerical or categorical. Given a semantic vocabulary V = {v1, . . . , vQ}, a semantic feature space S is defined as the Cartesian product of the vector spaces Sk associated with each semantic vk, S = S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SQ. A classifier is denoted semantic if it operates on S. As an example, for animal recognition, a semantic vocabulary containing visual attributes, e.g. V \u2208 {furry, has legs, is brown, etc.}, is usually defined along with their corresponding vector spaces. In this case, since all semantics are binary, Sk = R where large positive values indicate the attribute presence, and large negative values, its absence.\nEarly approaches to semantic recognition [45] used the set of image classes to be recognized as the semantic vocabulary. The rationale is to create a feature space with a high-level abstraction, where operations such as image search [44] or classification [34, 45] can be performed more robustly. More recently, there has been substantial interest in semantic feature spaces for transfer learning, which use an auxiliary semantic vocabulary, defined by mid-level visual concepts. Three main categories of concepts have been explored, including visual attributes, hierarchies and word vector representations. Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60]. Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].\nZero-shot learning Most current solutions to ZSL fall under two main categories: RIS and RULE. Early approaches adopted the RIS strategy. One of the most popular among these is the direct attribute prediction (DAP) method [31], which learns attributes independently using SVMs and infers ZS predictions by a maximum a posteriori rule that assumes attribute independence. Several enhancements have been proposed to account for attribute correlations a posteriori, e.g. by using CRFs to model attribute/class correlations [10], directed Bayesian networks to merge attribute predictions into class scores [58], or random forests learned so as to mitigate the effect of unreliable attributes [25]. More recently, [37] proposed a multiplicative framework that enables class-specific attribute classifiers, and [5] learns independent attributes which were previously discovered from Word2Vec representations.\nRULE is an alternative strategy that exploits the one-toone relationship between semantics and object classes. The central idea is to define an embedding \u03c6(\u00b7) that maps each class y into a Q-dimensional vector of attribute states \u03c6(y) that identifies it. A bilinear compatibility function\nh(x, y;T) = \u03c6(y)TTT \u03b8(x) (1)\nof parameters T \u2208 Rd\u00d7Q is then defined between the feature vector \u03b8(x) \u2208 Rd of image x and the encoding of its\nclass y. In the first implementation of RULE for ZSL [2], T is learned by a variant of the structured SVM. Several variants have been proposed, such as the addition of different regularization terms [43,49], the use of least-squares losses for faster training [49], or improved semantic representations of objects learned from multiple text sources [1, 47]."}, {"heading": "3. Semantics and deep learning", "text": "We now discuss the CNN implementation of RIS and RULE. For simplicity, we assume attribute semantics. Sections 5 and 6 extend the treatment to other concepts. For quick consultation, Table 1 summarizes important notation used in the rest of the paper."}, {"heading": "3.1. Deep-RIS", "text": "Under the independence assumption that underlies RIS, the CNN implementation reduces to learning Q independent attribute predictors. Inspired by the success of multitask learning, it is advantageous to share CNN parameters across attributes, and rely on a common feature extractor \u03b8(x; \u0398) of parameters \u0398, which can be implemented with one of the popular CNNs in the literature. Thus, each attribute predictor ak of Deep-RIS takes the form\nak(x; tk,\u0398) = \u03c3 ( tTk \u03b8(x; \u0398) ) (2)\nwhere \u03c3(\u00b7) is the sigmoid function and tk a parameter vector. Given a training set D = {(x(i), s(i))Ni=1}, where s(i) = (s\n(i) 1 , . . . , s (i) Q ) are attribute labels, tk and \u0398 are\nlearned by minimizing the risk R[a1, . . . , aQ,D] = \u2211 i \u2211 k Lb(ak(x (i); tk,\u0398), s (i) k ) (3)\nwhere Lb is a binary loss function, typically the crossentropy loss Lb(v, y) = \u2212y log(v)\u2212 (1\u2212 y) log(1\u2212 v)."}, {"heading": "3.2. Deep-RULE", "text": "The implementation of RULE follows immediately from the bilinear form of (1). Note that \u03c6(y) is a fixed mapping from the space of attributes to the space of class labels. For example, if there are Q binary attributes and C class labels, \u03c6(y) is a Q dimensional vector that encodes the presence/absence of the Q attributes in class y\n\u03c6k(y) = { 1 if class y contains attribute k, \u22121 if class y lacks attribute k. (4)\nWe denote \u03c6(y) the semantic code of class y. To implement (1) in a CNN, it suffices to use one of the popular models to compute \u03b8(x; \u0398), add a fully-connected layer of Q units and parameters T, so that a(x) = TT \u03b8(x; \u0398) is a vector of attribute scores, and define the CNN class outputs\nh(x;T,\u0398) = \u03a6Ta(x) = \u03a6TTT \u03b8(x; \u0398), (5)\nwhere \u03a6 = [\u03c6(1), . . . , \u03c6(C)] \u2208 RQ\u00d7C . Given a training set D = {(x(i), y(i))Ni=1}, where y(i) is the class label of image x(i), T and \u0398 are learned by minimizing\nR[h,D] = \u2211 i L ( h(x(i);T,\u0398), y(i) ) (6)\nwhere L is some classification loss, typically the crossentropy L(v, y) = \u2212 log(\u03c1y(v)) of softmax outputs \u03c1(v)."}, {"heading": "3.3. Relationships", "text": "Both Deep-RIS and Deep-RULE have advantages and disadvantages, which can be observed by comparing the risks of (3) and (6). Since the attributes ak(x) are the quantities of interest for ZSL, it is useful to understand how the two methods provide supervision to the space A of attributes. From (3), Deep-RIS provides supervision to the individual attributes ak(x). Since ak(\u00b7) = 1Tk a(\u00b7), where 1k is the kth vector in the canonical basis (1 in the kth position and 0 elsewhere), the supervision is along the canonical directions of A. On the other hand, (5)-(6) only depend on the projections \u03c6(y)Ta(x) of a(\u00b7) along the vector encodings \u03c6(\u00b7) of all training classes. Hence, RULE only provides supervision to the the column space C(\u03a6) of \u03a6.\nIn practice, we are often on the regime of Figure 1, where the number of attributes Q is larger than the number of training classes C. It follows that C(\u03a6) can be fairly low dimensional (dimension C) and the left null space N (\u03a6T ) fairly high dimensional (dimension Q \u2212 C). Hence, while RIS constraints all attributes, RULE leaves Q\u2212 C attribute dimensions unconstrained. In this case, ZS classes with semantic codes \u03c6ZS misaligned with C(\u03a6) cannot be expected to be accurately predicted. In the limit, RULE is completely inadequate to discriminate ZS classes when \u03c6ZS is perpendicular to C(\u03a6), such as \u03c6ZS(1) in Figure 1. This suggests the superiority of RIS over RULE. However, because RIS supervises attributes independently, it has no ability to learn attribute dependencies, e.g. that the attributes \u201chas wings\u201d and \u201clives in the water\u201d have a strong negative correlation. These dependencies can be thought of as constraints that reduce the effective dimensionality of the attribute space. They imply that the attribute vectors a(x) of natural images do not spanA, but only an effective attribute subspaceA\u2032 of dimension Q\u2032 < Q. By learning only on C(\u03a6) \u2282 A\u2032, DeepRULE provides supervision explicitly in this space. This suggests that Deep-RULE should outperform Deep-RIS.\nOverall, the relative performance of the two approaches depends on the overlap between the subspaces of A\u2032 covered by the training and ZS classes, denoted A\u2032T and A\u2032ZS respectively. If A\u2032T contains all the directions \u03c6ZS that defineA\u2032ZS , Deep-RULE will outperform Deep-RIS. If the ZS classes are defined by directions \u03c6ZS not contained in A\u2032T , Deep-RIS will likely outperform Deep-RULE."}, {"heading": "4. Semantically consistent regularization", "text": "In this section, we introduce the Semantically COnsistent REgularizer (SCoRe) architecture."}, {"heading": "4.1. Attributes as regularization constraints", "text": "In the previous section, we saw that the relative performance of Deep-RIS and Deep-RULE depends on the alignment between the subspaces of A\u2032 that define the training and ZS classes, A\u2032T and A\u2032ZS . In an ideal scenario, A\u2032T = A\u2032 and so \u03c6ZS(c) \u2208 A\u2032T for any ZS class c. However, this is unlikely to happen for datasets of tractable size, and the subsets A\u2032T and A\u2032ZS are most likely not aligned.\nUnder this scenario, Deep-RIS and Deep-RULE compliment each other. While Deep-RIS enforces first-order constraints on the statistics of single attributes, Deep-RULE enforces second-order constraints, by constraining the statistics of linear attribute combinations. If the two strategies are combined, Deep-RULE can explain attribute dependencies that appear on both training and ZS classes, leaving to Deep-RIS the task of constraining the attribute distribution on the remainder of the space. It is, thus, natural to combine the two strategies. We accomplish this by mapping them into regularization constraints."}, {"heading": "4.2. Recognition and regularization", "text": "An object recognizer maps an image x into a class\ny\u2217 = arg max c\u2208{1,...,C} hc(x), (7)\nwhere h(x) = (h1(x), . . . , hC(x)) is a vector of confidence scores for the assignment of x to each class, and y\u2217 the class prediction. The score function h(\u00b7) is usually learned by minimizing an empirical riskRE [h], under a complexity constraint \u2126[h] to improve generalization, i.e.\nh\u2217 = arg min h RE [h] + \u03bb\u2126[h] (8)\nwhere \u03bb \u2265 0 is a Lagrange multiplier, and \u2126[\u00b7] a regularizer that favors score functions of low complexity. Common usages of \u2126[\u00b7] include shrinkage [22], sparse representations [11] or weight decay [29]. Since all these approaches simply favor solutions of low complexity, they are a form of task-insensitive regularization. For ZSL, this type of regularization has indeed been used to control the variance of 1) semantic scores or 2) backward projections of object embeddings into the feature space [49], as well as to suppress noisy semantics [43].\nIn this work, rather than a generic penalty on the complexity of h(.), we propose a task-sensitive form of regularization, which favors score functions h(\u00b7) with the added functionality of attribute prediction. This regularization is implemented with two complimentary mechanisms, introduced in the next two sections."}, {"heading": "4.3. Codeword regularization", "text": "The first mechanism exploits the fact that the score functions of (7) are always of the form\nhc(x) = \u3008wc, f(x)\u3009 , (9)\nwhere \u3008\u00b7, \u00b7\u3009 denotes an inner product, f(\u00b7) a predictor, and {w1, . . . ,wC} a set of C class codewords. We denote wc as the classification code of class c. For example, in binary classification, algorithms such as boosting [15] or SVM [12] simply choose 1/ \u2212 1 as the codewords of the positive/negative class.Similarly, for C-ary classification, neural networks [33] or multi-class SVMs [59] rely on one-hot encodings that lead to the typical decision rule y\u2217 = arg maxj\u2208{1,...,C} fj(x). There is, however, no reason to be limited by these classical sets.\nBy comparing the score functions of (5) and (9), DeepRULE can be interpreted as learning the optimal predictor a(x) for a classification code given by (4), i.e. wc = \u03c6(c). Hence, Deep-RULE can be seen as a form of very strict CNN regularization, where the final fully-connected layer is set to these semantic codes. In general, fixing network weights is undesirable, as better results are usually obtained by learning them from data. We avoid this by using the semantic codes \u03c6(c) as loose regularization constraints, under the framework of (8). Similarly to Deep-RULE, we learn the predictor f using cross-entropy as the empirical risk RE , and score functions of the form\nh(x;W,T,\u0398) = WT f(x) = WTTT \u03b8(x; \u0398) (10)\nwhere the columns of W contain the weight vectors wc of the last CNN layer. This is complemented by a codeword regularizer\n\u2126[W] = 12 \u2211C c=1 ||wc \u2212 \u03c6(c)||2 (11)\nthat favors classification codes wc similar to the semantic codes \u03c6(c). Note that, up to terms that do not depend\non wc, this can be written as \u2126[W] \u223c \u2211C c=1 1 2 ||wc||\n2 \u2212\u2211C c=1 w T c \u03c6(c). In the Lagrangian of (8), the first summation becomes the \u201cweight decay\u201d regularizer already implemented by most CNN learning packages. Thus, effectively,\n\u2126[W] = \u2212 \u2211C\nc=1 w T c \u03c6(c). (12)\nIn sum, the use of codeword regularization forces the CNN to model attribute dependencies by aligning the learned classification codes wc with semantic codes \u03c6(c)."}, {"heading": "4.4. Loss-based regularization", "text": "The second mechanism, denoted loss-based regularization, aims to constraint attributes beyond A\u2032T , and provides explicit regularization to attribute predictions. It is implemented by introducing an auxiliary risk RA[f ] in the optimization, i.e. replacing RE [h] in (8) by RE [h] + \u03bbRA[f ] where RA[f ] is the sum of attribute prediction risks of (3). This drives the score function to produce accurate attribute predictions, in addition to classification."}, {"heading": "4.5. SCoRe", "text": "Given a training set of images x(i), attribute labels (s\n(i) 1 , . . . , s (i) Q ), and class labels y (i), the regularizers of the previous sections are combined into the SCoRe objective\nminimize \u0398,T,W\n\u2211 i L ( h(x(i);W,T,\u0398), y(i) ) +\u03bb \u2211\ni \u2211 k Lb ( fk(x (i); tk,\u0398), s (i) k ) +\u03b2\u2126[W], (13)\nwhere h(\u00b7) is given by (10), fk(x; tk,\u0398) = tTk \u03b8(x; \u0398) is the kth semantic predictor, \u2126[W] the codeword regularizer of (11), and \u03bb and \u03b2 Lagrange multipliers that control the tightness of the regularization constraints.\nDepending on the value of these multipliers, SCoRe can learn a standard CNN, Deep-RIS, or Deep-RULE. When \u03bb = \u03b2 = 0, all the regularization constraints are disregarded and the classifier is a standard recognizer for the training classes. Increasing \u03bb and \u03b2 improves its transfer ability. On one hand, regardless of \u03b2, increasing \u03bb makes SCoRe more like Deep-RIS. In the limit of \u03bb \u2192 \u221e, the first summation plays no role in the optimization, \u2126 is trivially minimized by wc = \u03c6(c), and (13) is reduced to the Deep-RIS optimization problem of (3). On the other hand, maintaining \u03bb = 0 while increasing \u03b2 makes SCoRe similar Deep-RULE. For large values of \u03b2, the learning algorithm emphasizes the similarity between classification and semantic codes, trading off classification performance for semantic alignment. Finally, when both \u03bb and \u03b2 are nonzero, SCoRe learns the classifier that best satisfies the corresponding trade-off between the three goals: recognition, attribute predictions, and alignment with the semantic code."}, {"heading": "5. Semantics", "text": "In this section, we discuss the encoding of different semantics under the SCoRe framework."}, {"heading": "5.1. Attributes", "text": "So far, we assumed that semantics are binary attributes. Each attribute is mapped into an entry of the semantic code according to (4), which is used to represent each class, i.e.\n\u03c6(y) = concat(\u03c61(y), . . . , \u03c6Q(y)). (14)\nTo support different degrees of certainty on class/attribute associations, continuous attributes are also easily implemented by making \u03c6k(y) \u2208 [\u22121, 1]."}, {"heading": "5.2. Beyond binary semantics", "text": "SCoRe can be easily extended to semantics with more than two states. Consider a semantic k with Sk states. In this case, each state is itself represented by a codeword, i.e.\n\u03c6k(y) \u2208 \u03a8(k) = {\u03c8(k)1 , . . . , \u03c8 (k) Sk }, (15)\nwhere \u03c8(k)i are semantic state codewords. Then, the semantic code \u03c6(y) of class y is built by concatenating \u03c6k(y) for all k, as in (14). Similarly to the binary case, a predictor f(x) learned under this codeword set will attempt to approximate \u03c6k(y) for images x of class y. The state of the kth semantic can thus be recovered from f with s\u2217k = arg maxi=1,...,Sk\u3008\u03c8 (k) i , fk(x)\u3009 where \u03c8 (k) i is the codeword of state i of the kth semantic, and fk(\u00b7) the corresponding subspace of f(\u00b7). Many semantic state codewords can be defined. We now provide some examples.\nTaxonomies In this work, we consider taxonomic encodings that emphasize node specific decisions, by interpreting each node as a semantic concept. As illustrated in Figure 2, a semantic state codeword set \u03a8(k) is defined per node k. Its state codewords identify all possible children nodes plus a reject option. For example, the codeword set \u03a8(2) of node 2 contains codewords \u03c8(2)dolphin and \u03c8 (2) whale, plus the reject codeword \u03c8(2)other. Under this taxonomic encoding, the semantic code \u03c6(y) identifies the relevance of each node to\n\ud835\udc7e\ud835\udc44\n\ud835\udc7e1\n\ud835\udc7e2\n\ud835\udc7e3\nSemantic Scores\ud835\udc53 \ud835\udc99\n\u22ee \u22ee \u22ee\n\ud835\udc601 \ud835\udc50\n\ud835\udc602 \ud835\udc50\n\ud835\udc603 \ud835\udc50\n\ud835\udc60\ud835\udc44 \ud835\udc50\nObject scores\n+\n\ud835\udc50 Class 1 Class 2\nClass \ud835\udc50\nClass \ud835\udc36\n\u22ee\n\u22ee\nSemantic Supervision\n\ud835\udc94 \ud835\udc56 = \ud835\udc601 (\ud835\udc56) , \ud835\udc602 (\ud835\udc56) , \u2026 , \ud835\udc60\ud835\udc44 (\ud835\udc56)\nClass Supervision\n\ud835\udc66 \ud835\udc56\nStandard CNN\n\ud835\udc7b\nFigure 3. Deep-SCoRe. Feature extraction based on common CNN architectures. Classification is performed by first computing semantic scores through codewords Wk, and then combining them into class scores using known class/semantics relations sck.\nthe class y. An internal node that is an ancestor of y contributes with the codeword corresponding to the branch selection (needed to reach the class) at that node. A node that is not an ancestor contributes with the reject codeword. For example, in Figure 2, the class \u201cbear\u201d receives the code \u03c6(bear) = concat ( \u03c8\n(1) Ter, \u03c8 (2) Other, \u03c8 (3) Bear, \u03c8 (4) Other\n) .\nIt remains to define the codeword sets V(k). These could be used to reflect further semantic information. In the tree of Figure 2, V(1) could encode a set of attributes that distinguish aquatic, terrestrial, and aerial animals, such as \u201chas fins,\u201d \u201chas legs\u201d or \u201chas wings\u201d. In this work, since no semantic information is available beyond the taxonomy itself, we rely on the maximally separated codeword sets of [50]. Under this procedure, aQ-ways decision is mapped into the set of codewords defined as the vertices of aQ-sided regular polygon in Q\u2212 1 dimensions centered at the origin.\nWord2Vec Word2Vec is a procedure to generate word embeddings. A word w is mapped into a high-dimensional vector \u03be(w) \u2208 \u03c7 by a neural network trained from large text corpora to reconstruct linguistic contexts of words. For semantic annotation, this mapping is used as the semantic code, i.e. each class y is encoded by the vector \u03c6(y) = \u03be(y).\nIn this work, we use the skip-gram architecture proposed by Mikolov et al. [39]. Its embeddings are determined by two parameters: size of the encoding layer and the window size that defines a context for each word. Rather than relying on a single model, we learn Q Word2Vec embeddings \u03bek(y), k \u2208 {1, . . . , Q}, using Q different combinations of the two parameters. This createsQ codeword sets V(k). The semantic code then represents class c by a string of the resulting vectors \u03c6k(y) = \u03bek(y), using (14)."}, {"heading": "6. Deep-SCoRe", "text": "Deep-SCoRe implements (10) using a CNN to compute \u03b8(x; \u0398). Parameters \u0398, W and T are learned from (13), using a semantic code that combines various semantic state\ncodeword sets V(k). These can be relative to attributes, taxonomy nodes, Word2Vec mappings, or any other semantic encoding. From (9), class scores decompose into\nhc(x) = \u2211 k h (k) c (x) = \u2211 k\u3008w (k) sck , fk(x)\u3009 (16)\nwhere sck is the state of the k th semantic under class c, w(k)sck the corresponding codeword, and fk(\u00b7) the corresponding subspace of f(\u00b7). Semantic predictions are obtained by computing the dot-products\nu (k) i (x) = \u3008w (k) i , fk(x)\u3009 (17)\nfor all states i of semantic k and choosing the state\ns\u2217k = arg max i u (k) i (x). (18)\nWhile (16) and (17) could be computed separately, the structure of (16) allows shared computation. This can be accomplished by adding two layers to the semantic predictor f(x), which we denote semantic encoding (SE) layers.\nAs shown in Figure 3, a CNN is used to compute the predictor f(x) = (f1, . . . , fQ) (x). Similarly to Deep-RIS and Deep-RULE, this is implemented through a linear transformation T of a feature vector \u03b8(x) computed with one of the popular CNN models. The first SE layer then consists of Q parallel fully-connected layers that compute the semantic scores u(k)i (x) for each of the Q semantics. The weights of each branch k contain the classification codewords w(k)i and are learned under the codeword regularizer of (11). The second SE layer then selects, for each class c, a single output from each branch k corresponding to the state sck of the k th semantic of class c. These outputs are added to obtain the class recognition score hc(x). This is easily implemented by a fully connected layer of predetermined sparse weights of 0s and 1s that remain fixed throughout training.\nLearning: Consider a training set of three-tuples: (a) the image x(i); (b) the vector of semantic states s(i); and (c)\nthe class label y(i). As shown in Figure 3, the state vectors s(i) are used as supervisory signals for the first SE layer and the labels y(i) as supervisory signals for the second. These supervisory signals and the semantic codes \u03c6(y) are used to compute the Lagrangian risk of (13), and all parameters are optimized by back-propagation using Caffe toolbox [26].\nDeep-SCoRe models were trained by fine tuning pretrained CNNs using stochastic gradient descent (SGD) with momentum of 0.9 and weight decay of 0.0005. The learning rate was chosen empirically for each experiment."}, {"heading": "7. Experiments", "text": "In this section, we discuss several experiments carried out to evaluate the ZSL performance of DeepSCoRe. Source code is available at https://github.com/ pedro-morgado/score-zeroshot."}, {"heading": "7.1. Experimental setup", "text": "Datasets: Three datasets were considered: Animals with Attributes [31] (AwA), Caltech-UCSD Birds 200-2011 [57] (CUB), and a subset of the Imaging FlowCytobot [52] (IFCB) dataset. Table 2 summarizes their statistics. On AwA and CUB, the partition into source and target classes for ZSL is as specified by [31] and [2], respectively. On IFCB, which is now first used for ZSL, classes were partitioned randomly. A separate set of validation classes (10/50/6 for the AwA/CUB/IFCB datasets, respectively) was also drawn randomly to tune SCoRe parameters.\nImage representation: Images were resized to 256\u00d7256 pixels, with the exception of IFCB, where aspect ratios differ widely and resizing introduces considerable distortion. Instead, each image was first resized along the longest axis and the shortest axis then padded with the average pixel value, to preserve the aspect ratio. Typical data augmentation techniques were used for training (random cropping and mirroring), and the center crop was used for testing. Three CNN architectures were used to implement \u03b8(x): AlexNet [28] (layer fc7), GoogLeNet [54] (layer pool5) and VGG19 [51] (layer fc7).\nSemantics: Three sources of semantics were evaluated. Visual attributes: Continuous attributes have been shown to be superior to their binary counterparts and were used on AwA and CUB. On IFCB, where no attributes were defined previously, a list of 35 visual attributes was assembled and annotated by an expert with binary labels, using several sources from the oceanographic community [7, 38].\nTaxonomies were created by pruning the WordNet tree [40] for the training and ZS classes, and eliminating dummy nodes containing a single child. In the rare situations where WordNet was not fine-grained enough to distinguish between a set of classes, the taxonomy was expanded by simply assigning each object into its own leaf.\nWord2Vec models were trained on a Wikipedia archive, dated June 1st, 2016. Three different window sizes (3, 5 and 10) and vector dimensions (50, 100 and 500) were used, leading to a total of 9 Word2Vec codeword sets."}, {"heading": "7.2. Results", "text": "Gains of regularization: We started by evaluating codeword and loss-based regularization. The importance of the two regularizers was assessed separately on all datasets using visual attributes and GoogLeNet. In both cases, we measured the gains over Deep-RULE, in which classification codewords are set to wc = \u03c6(c) and \u03bb = 0. The gains of loss-based regularization were evaluated by increasing \u03bb while keeping \u03b2 = 0. Under this setting, the classifier converges to Deep-RIS in the limit of \u03bb \u2192 \u221e. Conversely, the gains of codeword regularization were measured by increasing \u03b2 while keeping \u03bb = 0. In this case, the classifier converges to an unrestricted object recognizer when \u03b2 = 0 and to Deep-RULE when \u03b2 \u2192\u221e. Figure 4 presents the absolute improvement in ZS mean class accuracy (ZS-MCA) over Deep-RULE, as a function of the Lagrange multipliers.\nBoth regularizers produced gains over Deep-RULE with absolute gains as high as 3 ZS-MCA points. This demonstrates the importance of learning the classification codewords, rather than fixing them. Note that, for codeword regularization, best results were obtained for intermediate values of \u03b2, which encourage consistency between the se-\nmantic and classification codes, but leave enough flexibility to learn a classification code superior to its semantic counterpart. In all cases, the MCA of SCoRe was much superior to that of RIS, confirming the importance of modeling attribute dependencies through the first term of (13). Finally, SCoRe performance was also superior to that of the unrestricted CNN. This demonstrates the benefits of regularization. Interestingly, this was not the case of RIS, which always underperformed the unrestricted CNN, or RULE that only achieved on par results in CUB and IFCB1.\nIn Section 3.3, we hypothesized that loss-based regularization becomes more important as the alignment between the subspaces of A\u2032 spanned by training and ZS classes decreases. To test this hypothesis, we measured this alignment by computing the average orthogonal distance between the semantic codeword \u03c6(c) of each ZS class and the subspace spanned by the codewords of training classes. The average distances were 0.1244 for CUB, 0.3063 for AwA, and 0.4181 for IFCB, indicating that the transfer is easiest for CUB and hardest for IFC. This is consistent with the plots of Figure 4, which show largest gains of loss-based regularization on IFCB followed by AwA and then CUB.\nComparisons to state-of-the-art methods: A comparison to the literature is not trivial since methods differ in 1) CNN implementation, 2) train/ZS class partitioning, and 3) semantic space representation. To mitigate these differences, we focused on attribute semantics which have most available results. Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison. Deep-SCoRe hyper-parameters \u03bb and \u03b2 were tuned on a subset of the training classes.\nTable 3 compares our ZS-MCA to previous approaches\n1The unrestricted CNN is initialized with semantic codes. If random initialization was used, ZSL would not be possible.\nusing three CNN architectures: AlexNet, GoogLeNet and VGG19. Although results vary drastically with CNN, it is clear that Deep-SCoRe outperforms all previous approaches on all datasets, achieving impressive gains over the stateof-the-art for every architecture: 4.8%, 4.1% and 6.5% on AwA and 7.9%, 3.7% and 10.5% on CUB with AlexNet, GoogLeNet and VGG19, respectively.\nMultiple semantics: We finally studied the performance of Deep-SCoRe with attributes, taxonomies, and Word2Vec embeddings. Figure 5 compares Deep-SCoRe and its variants to popular RIS and RULE approaches in the literature: DAP [31] (RIS), SJE [4] and ES-ZSL [49] (RULE). All approaches were implemented with the semantic codes of Section 5. The best results, which were all obtained with Deep-SCoRe, are also shown. Figure 5 supports two main conclusions. First, as shown in [3, 8, 60], attributes enable by far the most effective transfer. This is not surprising since attributes tend to be discriminant properties of the various object classes. Taxonomies or Word2Vec are most informative of grouping or contextual information. Second, while all approaches rely on regularization, the nature of this regularization matters. The task-sensitive regularization of Deep-SCoRe always outperformed the taskinsensitive regularization of ES-ZSL, and the combination of loss-based and codeword regularization (Deep-SCoRe) always outperformed a fixed semantic code (Deep-RULE and SJE) or loss-based regularization (Deep-RIS and DAP)."}, {"heading": "8. Conclusion", "text": "In this work, we analyzed the type of supervision provided by previous approaches. The complementarity found between class and semantic supervision lead to the introduction of a new ZSL procedure, denoted SCoRe, where a CNN is learned together with a semantic codeword set and two forms of semantic constraints: loss-based and codeword regularization. State-of-the-art zero-shot performance was achieved in various datasets."}], "references": [{"title": "Multicue zero-shot learning with strong supervision", "author": ["Z. Akata", "M. Malinowski", "M. Fritz", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Labelembedding for attribute-based classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Labelembedding for image classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "Pattern Analysis and Machine Intelligence (TPAMI), IEEE Trans. on, 38(7):1425\u2013 1438", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation of output embeddings for fine-grained image classification", "author": ["Z. Akata", "S. Reed", "D. Walter", "H. Lee", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "How to transfer? Zeroshot object recognition via hierarchical transfer of semantic attributes", "author": ["Z. Al-Halah", "R. Stiefelhagen"], "venue": "Applications of Computer Vision, IEEE Winter Conf. on", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning", "author": ["Z. Al-Halah", "M. Tapaswi", "R. Stiefelhagen"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Phytopedia - the phytoplankton encyclpaedia project", "author": ["D. Cassis"], "venue": "Available at: http://www.eos.ubc.ca/research/ phytoplankton/", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Synthesized classifiers for zero-shot learning", "author": ["S. Changpinyo", "W.-L. Chao", "B. Gong", "F. Sha"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Inferring analogous attributes", "author": ["C.-Y. Chen", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing clothing by semantic attributes", "author": ["H. Chen", "A. Gallagher", "B. Girod"], "venue": "Computer Vision (ECCV), European Conf. on", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse representation", "author": ["H. Cheng"], "venue": "modeling and learning in visual recognition. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational Learning Theory, European Conf. on. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "et al", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive multi-view zero-shot recognition and annotation", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "Computer Vision (ECCV), European Conf. on", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Transductive multi-view zero-shot learning", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "Pattern Analysis and Machine Intelligence (TPAMI), IEEE Trans. on, 37(11):2332\u2013 2345", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised vocabulary-informed learning", "author": ["Y. Fu", "L. Sigal"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-shot object recognition by semantic manifold distance", "author": ["Z. Fu", "T. Xiang", "E. Kodirov", "S. Gong"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning attributes equals multi-source domain generalization", "author": ["C. Gan", "T. Yang", "B. Gong"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving Efficiency by Shrinkage: The James\u2013 Stein and Ridge Regression Estimators", "author": ["M. Gruber"], "venue": "volume 156. CRC Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning hypergraph-regularized attribute predictors", "author": ["S. Huang", "M. Elhoseiny", "A. Elgammal", "D. Yang"], "venue": "arXiv", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Sharing features between objects and their attributes", "author": ["S.J. Hwang", "F. Sha", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on, pages 1761\u2013 1768", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-shot recognition with unreliable attributes", "author": ["D. Jayaraman", "K. Grauman"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv:1408.5093", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised domain adaptation for zero-shot learning", "author": ["E. Kodirov", "T. Xiang", "Z. Fu", "S. Gong"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A simple weight decay can improve generalization", "author": ["A. Krogh", "J.A. Hertz"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1991}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "Computer Vision (ICCV), IEEE International Conf. on", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Attributebased classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "Pattern Analysis and Machine Intelligence (TPAMI), IEEE Trans. on, 36(3)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series. The handbook of brain theory and neural networks, 3361(10)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1995}, {"title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification", "author": ["L.-J. Li", "H. Su", "L. Fei-Fei", "E.P. Xing"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Max-margin zero-shot learning for multiclass classification", "author": ["X. Li", "Y. Guo"], "venue": "Artificial Intelligence and Statistics (ICAIS), International Conf. on", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised zero-shot classification with label representation learning", "author": ["X. Li", "Y. Guo", "D. Schuurmans"], "venue": "Computer Vision (ICCV), IEEE International Conf. on", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified multiplicative framework for attribute learning", "author": ["K. Liang", "H. Chang", "S. Shan", "X. Chen"], "venue": "Computer Vision (ICCV), IEEE International Conf. on", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "World Register of Marine Species (WoRMS)", "author": ["J. Mees", "G. Boxshall", "M. Costello"], "venue": "Available at: http://www. marinespecies.org", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM, 38(11):39\u201341", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1995}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv:1312.5650", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Computer Vision (ICCV), IEEE International Conf. on", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "and A", "author": ["R. Qiao", "L. Liu", "C. Shen"], "venue": "v. d. Hengel. Less is more: zero-shot learning from online textual documents with noise suppression. In Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Bridging the gap: Query by semantic example", "author": ["N. Rasiwasia", "P.J. Moreno", "N. Vasconcelos"], "venue": "Multimedia, IEEE Trans. on, 9(5):923\u2013938", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Holistic context models for visual recognition", "author": ["N. Rasiwasia", "N. Vasconcelos"], "venue": "Pattern Analysis and Machine Intelligence (TPAMI), IEEE Trans. on, 34(5):902\u2013917", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D. Forsyth"], "venue": "Computer Vision (ECCV), European Conf. on", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning deep representations of fine-grained visual descriptions", "author": ["S. Reed", "Z. Akata", "B. Schiele", "H. Lee"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2011}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["B. Romera-Paredes", "P. Torr"], "venue": "Machine Learning (ICCV), International Conf. on, pages 2152\u20132161", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiclass boosting: Theory and algorithms", "author": ["M.J. Saberian", "N. Vasconcelos"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Automated taxonomic classification of phytoplankton sampled with imaging-in-flow cytometry", "author": ["H.M. Sosik", "R.J. Olson"], "venue": "Limnology and Oceanography: Methods, 5(6):204\u2013 216", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving object classification using semantic attributes", "author": ["Y. Su", "M. Allan", "F. Jurie"], "venue": "British Machine Vision Conference (BMVC)", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "Computer Vision (ECCV), European Conf. on", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic modeling of natural scenes for content-based image retrieval", "author": ["J. Vogel", "B. Schiele"], "venue": "Computer Vision, International Journal of, 72(2):133\u2013157", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified probabilistic approach modeling relationships between attributes and objects", "author": ["X. Wang", "Q. Ji"], "venue": "Computer Vision (ICCV), IEEE International Conf. on", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": "Technical report, Citeseer", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "Latent embeddings for zero-shot classification", "author": ["Y. Xian", "Z. Akata", "G. Sharma", "Q. Nguyen", "M. Hein", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-shot learning via semantic similarity embedding", "author": ["Z. Zhang", "V. Saligrama"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "In fact, most recent computer vision papers use or adapt a small set of popular models, such as AlexNet [28], GoogLeNet [54], and VGG [51], learned from the Imagenet dataset [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 53, "context": "In fact, most recent computer vision papers use or adapt a small set of popular models, such as AlexNet [28], GoogLeNet [54], and VGG [51], learned from the Imagenet dataset [13].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "In fact, most recent computer vision papers use or adapt a small set of popular models, such as AlexNet [28], GoogLeNet [54], and VGG [51], learned from the Imagenet dataset [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "In fact, most recent computer vision papers use or adapt a small set of popular models, such as AlexNet [28], GoogLeNet [54], and VGG [51], learned from the Imagenet dataset [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 33, "context": "This has motivated the introduction of semantic representations for object recognition [34, 44, 45, 55, 56], which rely on a predefined vocabulary of visual concepts to define a semantic space S and a set of classifiers to map each image into that space.", "startOffset": 87, "endOffset": 107}, {"referenceID": 43, "context": "This has motivated the introduction of semantic representations for object recognition [34, 44, 45, 55, 56], which rely on a predefined vocabulary of visual concepts to define a semantic space S and a set of classifiers to map each image into that space.", "startOffset": 87, "endOffset": 107}, {"referenceID": 44, "context": "This has motivated the introduction of semantic representations for object recognition [34, 44, 45, 55, 56], which rely on a predefined vocabulary of visual concepts to define a semantic space S and a set of classifiers to map each image into that space.", "startOffset": 87, "endOffset": 107}, {"referenceID": 54, "context": "This has motivated the introduction of semantic representations for object recognition [34, 44, 45, 55, 56], which rely on a predefined vocabulary of visual concepts to define a semantic space S and a set of classifiers to map each image into that space.", "startOffset": 87, "endOffset": 107}, {"referenceID": 55, "context": "This has motivated the introduction of semantic representations for object recognition [34, 44, 45, 55, 56], which rely on a predefined vocabulary of visual concepts to define a semantic space S and a set of classifiers to map each image into that space.", "startOffset": 87, "endOffset": 107}, {"referenceID": 1, "context": "This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].", "startOffset": 41, "endOffset": 63}, {"referenceID": 3, "context": "This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].", "startOffset": 41, "endOffset": 63}, {"referenceID": 13, "context": "This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].", "startOffset": 41, "endOffset": 63}, {"referenceID": 30, "context": "This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].", "startOffset": 41, "endOffset": 63}, {"referenceID": 47, "context": "This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].", "startOffset": 41, "endOffset": 63}, {"referenceID": 48, "context": "This is known as zeroshot learning (ZSL) [2, 4, 14, 31, 48, 49].", "startOffset": 41, "endOffset": 63}, {"referenceID": 13, "context": "This motivated the collection of datasets containing images annotated with respect to semantics such as visual attributes [14,31].", "startOffset": 122, "endOffset": 129}, {"referenceID": 30, "context": "This motivated the collection of datasets containing images annotated with respect to semantics such as visual attributes [14,31].", "startOffset": 122, "endOffset": 129}, {"referenceID": 33, "context": "The first, recognition using independent semantics (RIS), consists of learning an independent classifier per semantic [34, 55, 56].", "startOffset": 118, "endOffset": 130}, {"referenceID": 54, "context": "The first, recognition using independent semantics (RIS), consists of learning an independent classifier per semantic [34, 55, 56].", "startOffset": 118, "endOffset": 130}, {"referenceID": 55, "context": "The first, recognition using independent semantics (RIS), consists of learning an independent classifier per semantic [34, 55, 56].", "startOffset": 118, "endOffset": 130}, {"referenceID": 13, "context": "Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58].", "startOffset": 89, "endOffset": 108}, {"referenceID": 30, "context": "Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58].", "startOffset": 89, "endOffset": 108}, {"referenceID": 41, "context": "Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58].", "startOffset": 89, "endOffset": 108}, {"referenceID": 47, "context": "Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58].", "startOffset": 89, "endOffset": 108}, {"referenceID": 52, "context": "Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58].", "startOffset": 89, "endOffset": 108}, {"referenceID": 57, "context": "Due to its simplicity, RIS became widely popular in the attribute recognition literature [14,31,42,48,53,58].", "startOffset": 89, "endOffset": 108}, {"referenceID": 8, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 60, "endOffset": 79}, {"referenceID": 13, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 60, "endOffset": 79}, {"referenceID": 29, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 60, "endOffset": 79}, {"referenceID": 41, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 60, "endOffset": 79}, {"referenceID": 45, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 60, "endOffset": 79}, {"referenceID": 24, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 107, "endOffset": 117}, {"referenceID": 30, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 107, "endOffset": 117}, {"referenceID": 57, "context": "Notwithstanding efforts in discriminant attribute discovery [9, 14, 30, 42, 46] or modeling of uncertainty [25,31,58], learning semantics independently proved too weak to guarantee reliable ZS predictions.", "startOffset": 107, "endOffset": 117}, {"referenceID": 43, "context": "This motivated a shift to the second strategy, which ties the design of S to the goal of recognition, by learning a single multi-class classifier that optimally discriminates between all training classes [44, 45].", "startOffset": 204, "endOffset": 212}, {"referenceID": 44, "context": "This motivated a shift to the second strategy, which ties the design of S to the goal of recognition, by learning a single multi-class classifier that optimally discriminates between all training classes [44, 45].", "startOffset": 204, "endOffset": 212}, {"referenceID": 1, "context": "[2] proposed an effective solution to this problem by noting that there is a fixed linear transformation, or embedding, between the semantics of interest and the class labels, which can be specified by hand, even for ZS classes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Recently, various works have proposed variations on this approach [1,4,35,43,47,49].", "startOffset": 66, "endOffset": 83}, {"referenceID": 3, "context": "Recently, various works have proposed variations on this approach [1,4,35,43,47,49].", "startOffset": 66, "endOffset": 83}, {"referenceID": 34, "context": "Recently, various works have proposed variations on this approach [1,4,35,43,47,49].", "startOffset": 66, "endOffset": 83}, {"referenceID": 42, "context": "Recently, various works have proposed variations on this approach [1,4,35,43,47,49].", "startOffset": 66, "endOffset": 83}, {"referenceID": 46, "context": "Recently, various works have proposed variations on this approach [1,4,35,43,47,49].", "startOffset": 66, "endOffset": 83}, {"referenceID": 48, "context": "Recently, various works have proposed variations on this approach [1,4,35,43,47,49].", "startOffset": 66, "endOffset": 83}, {"referenceID": 44, "context": "Early approaches to semantic recognition [45] used the set of image classes to be recognized as the semantic vocabulary.", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "The rationale is to create a feature space with a high-level abstraction, where operations such as image search [44] or classification [34, 45] can be performed more robustly.", "startOffset": 112, "endOffset": 116}, {"referenceID": 33, "context": "The rationale is to create a feature space with a high-level abstraction, where operations such as image search [44] or classification [34, 45] can be performed more robustly.", "startOffset": 135, "endOffset": 143}, {"referenceID": 44, "context": "The rationale is to create a feature space with a high-level abstraction, where operations such as image search [44] or classification [34, 45] can be performed more robustly.", "startOffset": 135, "endOffset": 143}, {"referenceID": 13, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 30, "endOffset": 38}, {"referenceID": 30, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 30, "endOffset": 38}, {"referenceID": 1, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 7, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 20, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 22, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 23, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 24, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 26, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 47, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 52, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 57, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 59, "context": "Attributes were introduced in [14, 31] and quickly adopted in many other works [2,8,21,23\u201325,27,48,53,58,60].", "startOffset": 79, "endOffset": 108}, {"referenceID": 1, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 79, "endOffset": 90}, {"referenceID": 3, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 79, "endOffset": 90}, {"referenceID": 47, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 79, "endOffset": 90}, {"referenceID": 59, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 79, "endOffset": 90}, {"referenceID": 3, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 7, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 15, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 17, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 19, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 40, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 42, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 46, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 59, "context": "Semantic concepts extracted from hierarchies/taxonomies were later explored in [2,4,48,60], and vector representations for words/entities in [4, 8, 16, 18, 20, 41, 43, 47, 60].", "startOffset": 141, "endOffset": 175}, {"referenceID": 30, "context": "One of the most popular among these is the direct attribute prediction (DAP) method [31], which learns attributes independently using SVMs and infers ZS predictions by a maximum a posteriori rule that assumes attribute independence.", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "by using CRFs to model attribute/class correlations [10], directed Bayesian networks to merge attribute predictions into class scores [58], or random forests learned so as to mitigate the effect of unreliable attributes [25].", "startOffset": 52, "endOffset": 56}, {"referenceID": 57, "context": "by using CRFs to model attribute/class correlations [10], directed Bayesian networks to merge attribute predictions into class scores [58], or random forests learned so as to mitigate the effect of unreliable attributes [25].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "by using CRFs to model attribute/class correlations [10], directed Bayesian networks to merge attribute predictions into class scores [58], or random forests learned so as to mitigate the effect of unreliable attributes [25].", "startOffset": 220, "endOffset": 224}, {"referenceID": 36, "context": "More recently, [37] proposed a multiplicative framework that enables class-specific attribute classifiers, and [5] learns independent attributes which were previously discovered from Word2Vec representations.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "More recently, [37] proposed a multiplicative framework that enables class-specific attribute classifiers, and [5] learns independent attributes which were previously discovered from Word2Vec representations.", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "In the first implementation of RULE for ZSL [2], T is learned by a variant of the structured SVM.", "startOffset": 44, "endOffset": 47}, {"referenceID": 42, "context": "Several variants have been proposed, such as the addition of different regularization terms [43,49], the use of least-squares losses for faster training [49], or improved semantic representations of objects learned from multiple text sources [1, 47].", "startOffset": 92, "endOffset": 99}, {"referenceID": 48, "context": "Several variants have been proposed, such as the addition of different regularization terms [43,49], the use of least-squares losses for faster training [49], or improved semantic representations of objects learned from multiple text sources [1, 47].", "startOffset": 92, "endOffset": 99}, {"referenceID": 48, "context": "Several variants have been proposed, such as the addition of different regularization terms [43,49], the use of least-squares losses for faster training [49], or improved semantic representations of objects learned from multiple text sources [1, 47].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "Several variants have been proposed, such as the addition of different regularization terms [43,49], the use of least-squares losses for faster training [49], or improved semantic representations of objects learned from multiple text sources [1, 47].", "startOffset": 242, "endOffset": 249}, {"referenceID": 46, "context": "Several variants have been proposed, such as the addition of different regularization terms [43,49], the use of least-squares losses for faster training [49], or improved semantic representations of objects learned from multiple text sources [1, 47].", "startOffset": 242, "endOffset": 249}, {"referenceID": 21, "context": "Common usages of \u03a9[\u00b7] include shrinkage [22], sparse representations [11] or weight decay [29].", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "Common usages of \u03a9[\u00b7] include shrinkage [22], sparse representations [11] or weight decay [29].", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "Common usages of \u03a9[\u00b7] include shrinkage [22], sparse representations [11] or weight decay [29].", "startOffset": 90, "endOffset": 94}, {"referenceID": 48, "context": "For ZSL, this type of regularization has indeed been used to control the variance of 1) semantic scores or 2) backward projections of object embeddings into the feature space [49], as well as to suppress noisy semantics [43].", "startOffset": 175, "endOffset": 179}, {"referenceID": 42, "context": "For ZSL, this type of regularization has indeed been used to control the variance of 1) semantic scores or 2) backward projections of object embeddings into the feature space [49], as well as to suppress noisy semantics [43].", "startOffset": 220, "endOffset": 224}, {"referenceID": 14, "context": "For example, in binary classification, algorithms such as boosting [15] or SVM [12] simply choose 1/ \u2212 1 as the codewords of the positive/negative class.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "For example, in binary classification, algorithms such as boosting [15] or SVM [12] simply choose 1/ \u2212 1 as the codewords of the positive/negative class.", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "Similarly, for C-ary classification, neural networks [33] or multi-class SVMs [59] rely on one-hot encodings that lead to the typical decision rule y\u2217 = arg maxj\u2208{1,.", "startOffset": 53, "endOffset": 57}, {"referenceID": 58, "context": "Similarly, for C-ary classification, neural networks [33] or multi-class SVMs [59] rely on one-hot encodings that lead to the typical decision rule y\u2217 = arg maxj\u2208{1,.", "startOffset": 78, "endOffset": 82}, {"referenceID": 49, "context": "In this work, since no semantic information is available beyond the taxonomy itself, we rely on the maximally separated codeword sets of [50].", "startOffset": 137, "endOffset": 141}, {"referenceID": 38, "context": "[39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "These supervisory signals and the semantic codes \u03c6(y) are used to compute the Lagrangian risk of (13), and all parameters are optimized by back-propagation using Caffe toolbox [26].", "startOffset": 176, "endOffset": 180}, {"referenceID": 30, "context": "Datasets: Three datasets were considered: Animals with Attributes [31] (AwA), Caltech-UCSD Birds 200-2011 [57] (CUB), and a subset of the Imaging FlowCytobot [52] (IFCB) dataset.", "startOffset": 66, "endOffset": 70}, {"referenceID": 56, "context": "Datasets: Three datasets were considered: Animals with Attributes [31] (AwA), Caltech-UCSD Birds 200-2011 [57] (CUB), and a subset of the Imaging FlowCytobot [52] (IFCB) dataset.", "startOffset": 106, "endOffset": 110}, {"referenceID": 51, "context": "Datasets: Three datasets were considered: Animals with Attributes [31] (AwA), Caltech-UCSD Birds 200-2011 [57] (CUB), and a subset of the Imaging FlowCytobot [52] (IFCB) dataset.", "startOffset": 158, "endOffset": 162}, {"referenceID": 30, "context": "On AwA and CUB, the partition into source and target classes for ZSL is as specified by [31] and [2], respectively.", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "On AwA and CUB, the partition into source and target classes for ZSL is as specified by [31] and [2], respectively.", "startOffset": 97, "endOffset": 100}, {"referenceID": 27, "context": "Three CNN architectures were used to implement \u03b8(x): AlexNet [28] (layer fc7), GoogLeNet [54] (layer pool5) and VGG19 [51] (layer fc7).", "startOffset": 61, "endOffset": 65}, {"referenceID": 53, "context": "Three CNN architectures were used to implement \u03b8(x): AlexNet [28] (layer fc7), GoogLeNet [54] (layer pool5) and VGG19 [51] (layer fc7).", "startOffset": 89, "endOffset": 93}, {"referenceID": 50, "context": "Three CNN architectures were used to implement \u03b8(x): AlexNet [28] (layer fc7), GoogLeNet [54] (layer pool5) and VGG19 [51] (layer fc7).", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "On IFCB, where no attributes were defined previously, a list of 35 visual attributes was assembled and annotated by an expert with binary labels, using several sources from the oceanographic community [7, 38].", "startOffset": 201, "endOffset": 208}, {"referenceID": 37, "context": "On IFCB, where no attributes were defined previously, a list of 35 visual attributes was assembled and annotated by an expert with binary labels, using several sources from the oceanographic community [7, 38].", "startOffset": 201, "endOffset": 208}, {"referenceID": 39, "context": "Taxonomies were created by pruning the WordNet tree [40] for the training and ZS classes, and eliminating dummy nodes containing a single child.", "startOffset": 52, "endOffset": 56}, {"referenceID": 39, "context": "AwA 30,475 40/10 85 WordNet [40]", "startOffset": 28, "endOffset": 32}, {"referenceID": 39, "context": "CUB 11,788 150/50 312 WordNet [40]", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "A - AlexNet [28]; G - GoogLeNet [54]; V - VGG19 [51].", "startOffset": 12, "endOffset": 16}, {"referenceID": 53, "context": "A - AlexNet [28]; G - GoogLeNet [54]; V - VGG19 [51].", "startOffset": 32, "endOffset": 36}, {"referenceID": 50, "context": "A - AlexNet [28]; G - GoogLeNet [54]; V - VGG19 [51].", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "DAP [32] 45.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "6\u2021 SJE [4] 61.", "startOffset": 7, "endOffset": 10}, {"referenceID": 48, "context": "1 ES-ZSL\u00a7 [49] 53.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "[23] 45.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] 48.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] - 72.", "startOffset": 0, "endOffset": 3}, {"referenceID": 59, "context": "[60] - 72.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[61] - - 76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] - - 73.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 39, "endOffset": 54}, {"referenceID": 18, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 39, "endOffset": 54}, {"referenceID": 42, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 39, "endOffset": 54}, {"referenceID": 46, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 39, "endOffset": 54}, {"referenceID": 16, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 113, "endOffset": 126}, {"referenceID": 17, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 113, "endOffset": 126}, {"referenceID": 26, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 113, "endOffset": 126}, {"referenceID": 35, "context": "Methods that use alternative semantics [1, 19, 43, 47] or that use unlabeled images from ZS classes for training [17,18,27,36] were disregarded for this comparison.", "startOffset": 113, "endOffset": 126}, {"referenceID": 4, "context": "DAP results reported in [5].", "startOffset": 24, "endOffset": 27}, {"referenceID": 30, "context": "Figure 5 compares Deep-SCoRe and its variants to popular RIS and RULE approaches in the literature: DAP [31] (RIS), SJE [4] and ES-ZSL [49] (RULE).", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "Figure 5 compares Deep-SCoRe and its variants to popular RIS and RULE approaches in the literature: DAP [31] (RIS), SJE [4] and ES-ZSL [49] (RULE).", "startOffset": 120, "endOffset": 123}, {"referenceID": 48, "context": "Figure 5 compares Deep-SCoRe and its variants to popular RIS and RULE approaches in the literature: DAP [31] (RIS), SJE [4] and ES-ZSL [49] (RULE).", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "First, as shown in [3, 8, 60], attributes enable by far the most effective transfer.", "startOffset": 19, "endOffset": 29}, {"referenceID": 7, "context": "First, as shown in [3, 8, 60], attributes enable by far the most effective transfer.", "startOffset": 19, "endOffset": 29}, {"referenceID": 59, "context": "First, as shown in [3, 8, 60], attributes enable by far the most effective transfer.", "startOffset": 19, "endOffset": 29}], "year": 2017, "abstractText": "The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.", "creator": "LaTeX with hyperref package"}}}