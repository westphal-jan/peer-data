{"id": "1704.03169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Later-stage Minimum Bayes-Risk Decoding for Neural Machine Translation", "abstract": "for extended periods of time, sequence algorithm generation models rely on beam search algorithm to generate output sequence. inevitably however, the correctness of beam search degrades when the a model is over - confident reading about a suboptimal prediction. in defining this paper, we propose to perform minimum bayes - risk ( mbr ) decoding for some extra steps at a later stage. in order to speed up mbr frame decoding, we compute the bayes risks on gpu buses in automated batch screening mode. similarly in our baseline experiments, we found that mbr reranking works with a large beam component size. later - preliminary stage mbr decoding is shown to outperform simple and mbr reranking in machine driven translation tasks.", "histories": [["v1", "Tue, 11 Apr 2017 06:48:45 GMT  (570kb,D)", "https://arxiv.org/abs/1704.03169v1", null], ["v2", "Thu, 8 Jun 2017 08:02:00 GMT  (930kb,D)", "http://arxiv.org/abs/1704.03169v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raphael shu", "hideki nakayama"], "accepted": false, "id": "1704.03169"}, "pdf": {"name": "1704.03169.pdf", "metadata": {"source": "CRF", "title": "Later-Stage Minimum Bayes-Risk Decoding for Neural Machine Translation", "authors": ["Raphael Shu", "Hideki Nakayama"], "emails": ["shu@nlab.ci.i.u-tokyo.ac.jp,", "nakayama@ci.i.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Recently, neural-based sequence generation models have achieved state-of-the-art performance in machine translation (Wu et al., 2016). Neural machine translation (NMT) models normally utilize recurrent neural networks (RNNs) as decoders to generate output tokens sequentially. In the decoding phase, it is a common practice to use the beam search algorithm to find a candidate translation, which approximately maximizes the posterior probability.\nAlthough beam search can find much better candidates compared to greedy decoding, sometimes it still produces inappropriate outputs. Szegedy et al. (2016) has shown that a neural network can become too confident in a suboptimal prediction. In beam search, assigning high probability to a suboptimal prediction in one step may lead to a\nchain reaction that generates unnatural output sequences.\nTo improve beam search, various approaches have been explored recently either by enhancing the scoring method (Li et al., 2016) or using reinforcement learning (Li et al., 2017; Gu et al., 2017). In this work, we try to apply Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004) to guide the decoding algorithm of NMT to find a better candidate. This approach exploits the similarity between candidate translations instead of predicting the quality of each single candidate.\nIn preliminary experiments, we found that simply reranking the results of NMT by their Bayes risks only works with a large beam size. However, we are facing two major difficulties when integrating the MBR reranking into beam search.\nFirstly, we found that performing MBR reranking simultaneously with beam search does not work. MBR reranking shrinks the diversity of the candidate space, thus traps the decoding algorithm in a suboptimal search space. Moreover, as the probability of a partial translation cannot represent final confidence, the values of Bayes risks are inaccurate in the beginning.\nSecondly, as MBR reranking requires computingN\u00d7N discrepancy values to obtains the Bayes risks for N candidates, CPU-based MBR reranking is excessively time-consuming.\nIn this paper, we propose to perform MBR decoding at a later stage to search for a \u201crefined\u201d candidate with low Bayes risk. To speed up MBR decoding, we designed two approaches to compute the Bayes risks on GPU, which are shown to be much faster than a standard implementation. The main contributions of this paper can be summarized as two folds:\n1. We found that MBR reranking only works with a large beam size. Conversely, per-\nar X\niv :1\n70 4.\n03 16\n9v 2\n[ cs\n.C L\n] 8\nJ un\n2 01\n7\nforming MBR decoding at a later-stage is proved to be effective regardless of the choice of beam size, and outperform simple MBR reranking.\n2. We found that the computation of Bayes risks can be much faster by computing the discrepancy matrix on GPU in batch mode."}, {"heading": "2 Related Work", "text": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonza\u0301lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT. A drawback of this approach is that it requires a SMT system to be available and decode simultaneously with the NMT model.\nRecently, some studies are proposed to enhance beam search with reinforcement learning. Li et al. (2017) utilizes a simplified version of the actorcritic model to decode for arbitrary decoding objectives. The scoring function is modified to be an interpolation of the log probability and the output of the value function (or Q function). Gu et al. (2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al., 2014). These approaches predict the quality of each single candidate, whereas MBR reranking considers the relation between multiple candidates. Therefore, they can be combined with our model to further improve the quality of output sequences."}, {"heading": "3 Minimum Bayes-Risk Decoding", "text": "MBR decoding is a technique to find a candidate with the least expected loss (Bickel and Doksum, 1977). Following previous work in SMT (Kumar and Byrne, 2004), given an evidence space E, the Bayes risk of a candidate y is computed by:\nR(y) = \u2211 y\u2032\u2208E \u2206(y, y\u2032)p(y\u2032|x) . (1)\nThe term \u2206(y, y\u2032) gives the discrepancy between two candidates, which is normally computed by using 1\u2212BLEU(y, y\u2032) in machine translation. In this paper, we use smoothed BLEU (Lin\nAlgorithm 1 Later-stage MBR decoding 1: Inputs:\nB = beam size, T = time budget 2: Initialize:\nH \u2190 discarded hypotheses E \u2190 B finished candidates\n3: for t\u2190 1 to T do 4: sort H by Eq. 2 in descending order 5: S \u2190 pop B hyps. from H with highest\nscores 6: S\u2032 \u2190 decode S to get B \u00d7B new hyps. 7: push finished hyps. in S\u2032 to E 8: push unfinished hyps. in S\u2032 back to H 9: perform MBR reranking for E\n10: output E[1]\nand Och, 2004), and the probability p(y\u2032|x) is calculated by a softmax over the average log probabilities of all candidates inE given by a NMT model. Intuitively, a candidate gets low Bayes risk if it is similar to the candidates in the evidence space."}, {"heading": "4 Later-stage MBR Decoding", "text": "In this section, we propose a simple decoding strategy, which searches for low-risk hypotheses after finishing beam search. The basic idea is to utilize the results of beam search as an evidence space to guide the later-stage MBR decoding.\nAs the hypotheses1 that were discarded by beam search provide good starting points for finding low-risk hypotheses, we begin the later-stage decoding from a selection of discarded hypotheses rather than from scratch. To do this, in each step of beam search, we save B discarded hypotheses that are outside the \u201cbeam\u201d to a hypothesis list H , where B is the number of beam size. After beam search, we select top B finished hypotheses2, and save them to an evidence space E.\nAfter we collected the discarded hypotheses in H and the evidences in E from beam search, we perform the later-stage MBR decoding for an extra T steps to recover low-risk hypotheses. In our experiments, T is fixed to be the number of input words.\nIn each extra step, we sort3 the hypothesis list\n1Each hypothesis is a tuple of the average log probability, candidate tokens, and the lastly computed hidden state of the decoder LSTM.\n2A hypothesis is finished when a \u201cEOS\u201d token is reached. 3As computing S(y) for all hypotheses in H is timeconsuming, in practice, we only rerank top 3\u00d7B hypotheses\nH with a score function, which will be described later in Section 4.1. Then we pick B hypotheses with the highest scores from H , and generate the next words for them, resulting in B \u00d7 B new hypotheses. If a hypothesis is finished, we add it to the evidence space E. Otherwise, it is put back to the hypothesis list H .\nFinally, after performing the later-stage MBR decoding for T steps, we select the candidate translation with the lowest risk in E as the output. The complete algorithm is summarized in Alg. 1."}, {"heading": "4.1 Score Function for Hypothesis Selection", "text": "In the later-stage MRB decoding, we desire to guide the algorithm to find a hypothesis with low Bayes risk under the evidence spaceE. To achieve this, we select the hypotheses in H to decode according to a score function computed for each hypothesis y in each extra step, which is computed as follows:\nS(y) = 1\n|y| log p(y|x)\u2212 \u03b1R(y)\u2212 \u03b2L(y). (2)\nwhere the first part of the equation is the average log probability. The risk term R(y) is computed by Eq. 1.\nHowever, reranking with R(y) alone will overpenalize short hypotheses as they are certainly dissimilar to the finished hypotheses inE. Therefore, we add a length penalty term L(y) to encourage the selection of short hypotheses, which is proportional to the number of remaining steps:\nL(y) = (T \u2212 t) \u00b7 |y|. (3)\nIn the last step of MBR decoding when t = T , as L(y) will be zero, the hypotheses are selected only by their confidence scores and risks."}, {"heading": "4.2 Fast Computation of Bayes Risk", "text": "Unfortunately, computing the Bayes risk for N candidates requires evaluating the discrepancy function \u2206(y, y\u2032) by N \u00d7 N times. The computation is excessively time-consuming with a CPU-based implementation, whose bottleneck is to compute the following discrepancy matrix:\u2206(y 1, y1) . . . \u2206(y1, yN ) ... . . . ...\n\u2206(yN , y1) . . . \u2206(yN , yN )  (4) with the highest average log probabilities.\nTo tackle this problem, we designed two approaches to compute the discrepancy matrix efficiently on GPU: (1) compute BLEU values in batches with a sophisticated GPU-based implementation (2) approximate the discrepancy values with a neural net. The advantage of the approximation approach is that the implementation is independent of the chosen criterion of discrepancy. In practice, we found that reranking with approximated discrepancies performs as good as a standard reranker.\nFor the GPU-based BLEU computation, the trick is to construct a count vector that contains the counts of all unique n-grams to compute matches. The implementational details are described in the supplementary material.\nThe neural-based approach approximates true discrepancy values with a simple LSTM, which can be computed in batches naturally:\nht = LSTM\u03b8(E(yt),ht\u22121), (5) h\u2032l = LSTM\u03b8(E(y \u2032 l),h \u2032 l\u22121), (6)\n\u2206(y1:t, y \u2032 1:l) = h > t h \u2032 l + v > \u03b8 (ht + h \u2032 l) + b\u03b8 . (7)\nwhere E(\u00b7) is the one-hot embedding a token."}, {"heading": "4.3 Dynamically Adjusting Weights", "text": "In this section, we turn our attention to the scoring function in Eq. 2. Similar to Li et al. (2016), we apply the REINFORCE algorithm (Williams, 1992) to learn the optimal weights (\u03b1 and \u03b2) for each input. The difference is that we do not discretize the weights to make a finite action space. Instead, we directly apply REINFORCE algorithm to learn a Gaussian policy with continuous actions. The merit of this approach is that we do not need to find the effective range of each weight value beforehand.\nWe use the last state of the backward encoder LSTM in the NMT model to represent an input sequence, which is denoted by s. The stochastic policy is defined as:\n\u00b5 = f(s;\u03b8\u00b5) (8)\n\u03c3 = softplus(f(s;\u03b8\u03c3)) (9)\n\u03c0(a|s;\u03b8) = |A|\u220f i=1\n1 \u03c3i \u221a 2\u03c0 exp\n( \u2212 (ai \u2212 \u00b5i) 2\n2\u03c32i ) (10)\nwhere |A| is the number of actions, which equals 2 in our case. The function approximators f(\u00b7)\nare implemented with simple two-layer neural networks. In the training time, we sample actions from the distribution defined by the policy \u03c0(a|s;\u03b8), whereas in the test time, the actions are computed deterministically with f(s;\u03b8\u00b5)."}, {"heading": "5 Experiments", "text": "We evaluate our proposed decoding strategy on English-Japanese translation task, using ASPEC parallel corpus (Nakazawa et al., 2016). The corpus contains 3M training pairs and 1812 sentence pairs in the test set. We tokenize the sentences with \u201ctokenizer.perl\u201d for English side, and Kytea (Neubig et al., 2011) for Japanese side. The sizes of vocabulary are cropped to 80k and 40k respectively. In our experiments, we trained a NMT model with a standard architecture (Bahdanau et al., 2014), which has 1000 units for both the embeddings and LSTMs."}, {"heading": "5.1 Fast Bayes-Risk Computation", "text": "In this section, we compare the speed between the GPU-based Bayer-risk rerankers and a standard reranker. A comparison of reranking speed is shown in Fig. 1.\nFor each input sentence, we rerank a list of N candidate translations. The average reranking time per sentence is reported. The results show that both GPU-based approaches are much faster than a standard CPU-based reranker, thus capable of being integrated into beam search. Remarkably, the growing pattern of the average reranking time for GPU-based approaches is linear rather than exponential.\nIn our experiment, reranking candidates with approximated discrepancy values is found to be as good as a normal reranker, shown in the middle of\nTable 1. The training details are provided in supplementary material."}, {"heading": "5.2 Later-stage MBR decoding", "text": "In Table 1, we compare different decoding strategies in various settings of beam sizes. The BLEU scores are reported following a standard postprocessing procedure4.\nWe found that increasing the beam size to a large number does not consequently contribute to the evaluation scores. On the contrary, MBR reranking improves the scores when a large beam size is used, but less effective with a small beam size. Later-stage MBR decoding is shown to outperform the simple MBR reranking in all settings of beam size.\nAdditionally, we also found that the number of candidates in the evidence space largely affects the effectiveness of MBR reranking. In our experiments, the number of evidences is fixed to the same number of beam size. Using more evidences degrades the quality of selected candidates."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we propose a simple decoding strategy to search a hypotheses with lowest Bayes risk at a later stage, which outperforms simple MBR reranking. We compute the Bayes risk on GPU to speed up step-wise MBR decoding.\nInterestingly, we found the simple MBR reranking is especially effective with a large beam size. Without MBR reranking, further increasing the beam size does not result in significant gain.\nFor future work, we intend to construct a better evidence space with an alternative neural network, in order to benefit the later-stage MBR decoding phase.\n4We produce the BLEU scores with Kytea tokenizer. The post-processing script can be found in http://lotus. kuee.kyoto-u.ac.jp/WAT/ ."}, {"heading": "A Supplemental Materials", "text": "A.1 A Note on the Standard Implementation for Computing Bayes Risks\nAs in the equation for computing R(y), all the terms in the summation are positive numbers, we stop computing the risk of a candidate if the sum is already higher than the lowest risk value of computed candidates. Even with this early stopping technique, the standard MBR reranker still runs very slow as shown in Fig. 1 of the paper.\nA.2 GPU-based BLEU Computation For the particular discrepancy function based on BLEU, we found that a matrix of N \u00d7 N BLEU values can be calculated efficiently on GPU. The trick is to build a M -dimensional count vector ci for each candidate yi, which contains the count of all M uniq n-grams in the candidate space. Another vector g is used to indicate the rank of each n-gram.\nFor example, let a set of n-grams be {\u201ca\u201d, \u201cin\u201d, \u201cpark\u201d, \u201cball\u201d, \u201cin a\u201d, \u201ca park\u201d}. Then for the sentence \u201ca park in a park\u201d, ci will be a 6-dimensional vector of {2, 1, 2, 0, 1, 2}, whereas g will be a vector of {1, 1, 1, 1, 2, 2}.\nThe BLEU score of a candidate pair can be computed with:\npi,j = 1\n4 4\u2211 n=1 log \u2211M k=1 I(gk = n) \u00b7min(cik, c j k)\n|yi|+ 1\u2212 n (11)\nBLEU(yi, yj) = exp(min(1\u2212 |y j | |yi| , 0) + pi,j)\n(12)\nIn this way, a N \u00d7N BLEU matrix can be obtained in one shot with an input of N \u00d7M count matrix. In practice, we use smoothed BLEU, which simply adds 1 to both top and bottom parts of the fraction in Eq. 11.\nA.3 Training details of Neural-based Bayes-risk Approximation\nTo learn a neural-based estimator that approximates the discrepancy matrix, we collect 100K candidate pairs by decoding the source sentences in the training data. For each input sentence, a new ID is assigned for each unique token in the candidate space to reduce the vocabulary size. For example, \u201cA cat eats\u201d and \u201cA dog eats\u201d will be converted to {0, 1, 3} and {0, 2, 3} respectively.\nThe model is trained with MSE loss function to predict 1 \u2212 BLEU(y, y\u2032) given a candidate pair. We use Adam optimizer (Kingma and Ba, 2014) and train the model for 50 epoch. The learning rate is fixed to 0.0001. In practice, we scale the discrepancy scores by 0.1. After 50 epochs, we obtain a MSE loss of 0.06.\nA.4 A Note on Dynamic Weight Adjusting\nThe model for predicting \u00b5 and \u03c3 are both simple two-layer neural networks with a shared hidden layer of 100 units, followed by a tanh nonlinearity.\nThe gradient for updating \u03b8 by gradient ascent is given by the REINFORCE rule with a baseline: \u2207\u03b8J(\u03b8) = Ea\u223c\u03c0 [ (Rs,a \u2212 bs)\u2207\u03b8 log \u03c0(a|s;\u03b8) ] .\n(13)\nwhere Rs,a is the reward for taking action a given input s, whereas bs is a baseline computed by another simple two-layer neural network. In practice, the computation of gradients used in the\nREINFORCE rule (Eq. 13) can be simplified as: \u2207\u03b8 log \u03c0(a|s;\u03b8) = \u2207\u03b8 \u2212 |A|\u2211 i=1 (ai \u2212 \u00b5i)2 2\u03c32i + log \u03c3i .\n(14)"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Mathematical statistics: basic ideas and selected topics", "author": ["PJ Bickel", "KA Doksum"], "venue": null, "citeRegEx": "Bickel and Doksum.,? \\Q1977\\E", "shortCiteRegEx": "Bickel and Doksum.", "year": 1977}, {"title": "Noisy parallel approximate decoding for conditional recurrent language model", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1605.03835 .", "citeRegEx": "Cho.,? 2016", "shortCiteRegEx": "Cho.", "year": 2016}, {"title": "Generalized minimum bayes risk system combination", "author": ["Kevin Duh", "Katsuhito Sudoh", "Xianchao Wu", "Hajime Tsukada", "Masaaki Nagata."], "venue": "IJCNLP. pages 1356\u20131360.", "citeRegEx": "Duh et al\\.,? 2011", "shortCiteRegEx": "Duh et al\\.", "year": 2011}, {"title": "Minimum bayes risk decoding for bleu", "author": ["Nicola Ehling", "Richard Zens", "Hermann Ney."], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics, pages 101\u2013104.", "citeRegEx": "Ehling et al\\.,? 2007", "shortCiteRegEx": "Ehling et al\\.", "year": 2007}, {"title": "Minimum bayes-risk system combination", "author": ["Jes\u00fas Gonz\u00e1lez-Rubio", "Alfons Juan", "Francisco Casacuberta."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.", "citeRegEx": "Gonz\u00e1lez.Rubio et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Rubio et al\\.", "year": 2011}, {"title": "Trainable greedy decoding for neural machine translation", "author": ["Jiatao Gu", "Kyunghyun Cho", "Victor OK Li."], "venue": "arXiv preprint arXiv:1702.02429 .", "citeRegEx": "Gu et al\\.,? 2017", "shortCiteRegEx": "Gu et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Minimum bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William J. Byrne."], "venue": "HLT-NAACL.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "A simple, fast diverse decoding algorithm for neural generation", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1611.08562 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning to decode for future success", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1701.06549 .", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "ACL. https://doi.org/10.3115/1218955.1219032.", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "Aspec: Asian scientific paper excerpt corpus", "author": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "Proceedings of the Ninth International Conference on Language Re-", "citeRegEx": "Nakazawa et al\\.,? 2016", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2016}, {"title": "Pointwise prediction for robust, adaptable japanese morphological analysis", "author": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori."], "venue": "ACL. pages 529\u2013533.", "citeRegEx": "Neubig et al\\.,? 2011", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin A. Riedmiller."], "venue": "ICML.", "citeRegEx": "Silver et al\\.,? 2014", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Neural machine translation by minimising the bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne."], "venue": "arXiv preprint arXiv:1612.03791 .", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2818\u20132826.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 16, "context": "Szegedy et al. (2016) has shown that a neural network can become too confident in a suboptimal prediction.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "To improve beam search, various approaches have been explored recently either by enhancing the scoring method (Li et al., 2016) or using reinforcement learning (Li et al.", "startOffset": 110, "endOffset": 127}, {"referenceID": 10, "context": ", 2016) or using reinforcement learning (Li et al., 2017; Gu et al., 2017).", "startOffset": 40, "endOffset": 74}, {"referenceID": 6, "context": ", 2016) or using reinforcement learning (Li et al., 2017; Gu et al., 2017).", "startOffset": 40, "endOffset": 74}, {"referenceID": 8, "context": "In this work, we try to apply Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004) to guide the decoding algorithm of NMT to find a better candidate.", "startOffset": 64, "endOffset": 87}, {"referenceID": 8, "context": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonz\u00e1lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al.", "startOffset": 38, "endOffset": 108}, {"referenceID": 5, "context": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonz\u00e1lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al.", "startOffset": 38, "endOffset": 108}, {"referenceID": 3, "context": "MBR decoding is widely applied in SMT (Kumar and Byrne, 2004; Gonz\u00e1lez-Rubio et al., 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al.", "startOffset": 38, "endOffset": 108}, {"referenceID": 4, "context": ", 2011), which is also found to improve the translation quality (Ehling et al., 2007).", "startOffset": 64, "endOffset": 85}, {"referenceID": 2, "context": "(2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al.", "startOffset": 73, "endOffset": 84}, {"referenceID": 14, "context": "(2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al., 2014).", "startOffset": 219, "endOffset": 240}, {"referenceID": 2, "context": ", 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT.", "startOffset": 8, "endOffset": 139}, {"referenceID": 2, "context": ", 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT. A drawback of this approach is that it requires a SMT system to be available and decode simultaneously with the NMT model. Recently, some studies are proposed to enhance beam search with reinforcement learning. Li et al. (2017) utilizes a simplified version of the actorcritic model to decode for arbitrary decoding objectives.", "startOffset": 8, "endOffset": 534}, {"referenceID": 2, "context": ", 2011; Duh et al., 2011), which is also found to improve the translation quality (Ehling et al., 2007). Recently, Stahlberg et al. (2016) utilized the translation lattice of SMT to guide NMT decoding with the MBR decision rule, which is shown to be better than simply rescoring the N-best results of SMT. A drawback of this approach is that it requires a SMT system to be available and decode simultaneously with the NMT model. Recently, some studies are proposed to enhance beam search with reinforcement learning. Li et al. (2017) utilizes a simplified version of the actorcritic model to decode for arbitrary decoding objectives. The scoring function is modified to be an interpolation of the log probability and the output of the value function (or Q function). Gu et al. (2017) extends the noisy, parallel approximate decoding (NPAD) algorithm (Cho, 2016) by adjusting the hidden states in recurrent networks with an agent, which is trained with the deterministic policy gradient algorithm (Silver et al.", "startOffset": 8, "endOffset": 784}, {"referenceID": 1, "context": "MBR decoding is a technique to find a candidate with the least expected loss (Bickel and Doksum, 1977).", "startOffset": 77, "endOffset": 102}, {"referenceID": 8, "context": "Following previous work in SMT (Kumar and Byrne, 2004), given an evidence space E, the Bayes risk of a candidate y is computed by:", "startOffset": 31, "endOffset": 54}, {"referenceID": 17, "context": "(2016), we apply the REINFORCE algorithm (Williams, 1992) to learn the optimal weights (\u03b1 and \u03b2) for each input.", "startOffset": 41, "endOffset": 57}, {"referenceID": 9, "context": "Similar to Li et al. (2016), we apply the REINFORCE algorithm (Williams, 1992) to learn the optimal weights (\u03b1 and \u03b2) for each input.", "startOffset": 11, "endOffset": 28}, {"referenceID": 12, "context": "We evaluate our proposed decoding strategy on English-Japanese translation task, using ASPEC parallel corpus (Nakazawa et al., 2016).", "startOffset": 109, "endOffset": 132}, {"referenceID": 13, "context": "perl\u201d for English side, and Kytea (Neubig et al., 2011) for Japanese side.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": "In our experiments, we trained a NMT model with a standard architecture (Bahdanau et al., 2014), which has 1000 units for both the embeddings and LSTMs.", "startOffset": 72, "endOffset": 95}], "year": 2017, "abstractText": "For extended periods of time, sequence generation models rely on beam search as the decoding algorithm. However, the performance of beam search degrades when the model is over-confident about a suboptimal prediction. In this work, we enhance beam search by performing minimum Bayes-risk (MBR) decoding for some extra steps at a later stage. In our experiments, we found that the conventional MBR reranking is only effective with a large beam size. In contrast, later-stage MBR decoding is shown to work regardless of the choice of beam size, and outperform simple MBR reranking. Additionally, we found that the computation of Bayes risks can be much faster by calculating the discrepancies on GPU in batch mode.", "creator": "LaTeX with hyperref package"}}}