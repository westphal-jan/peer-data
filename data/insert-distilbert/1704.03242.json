{"id": "1704.03242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Automatic Keyword Extraction for Text Summarization: A Survey", "abstract": "in more recent times, data is only growing rapidly in every domain such as news, social media, banking, education, etc. due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. text summarization is emerged as an important market research area in recent past. in this regard, review of existing previous work on text summarization process is useful for carrying you out further research. in this paper, recent literature on automatic keyword extraction and text summarization are presented explicitly since text summarization process is highly depend on keyword extraction. this literature includes the discussion about six different methodology used for keyword extraction and text summarization. it also discusses about different databases used for text summarization in several domains along. with evaluation matrices. finally, it discusses briefly issues about issues and research challenges faced by researchers along with future migration direction.", "histories": [["v1", "Tue, 11 Apr 2017 11:20:19 GMT  (1026kb)", "http://arxiv.org/abs/1704.03242v1", "12 pages, 4 figures"]], "COMMENTS": "12 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["santosh kumar bharti", "korra sathya babu"], "accepted": false, "id": "1704.03242"}, "pdf": {"name": "1704.03242.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["prof.ksb}@gmail.com,", "skjena@nitrkl.ac.in"], "sections": [{"heading": null, "text": "Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.\nKeywords: Abstractive summary, extractive summary, Keyword Extraction, Natural language processing, Text Summarization."}, {"heading": "1. INTRODUCTION", "text": "In the era of internet, plethora of online information are\nfreely available for readers in the form of e-Newspapers,\njournal articles, technical reports, transcription dialogues etc.\nThere are huge number of documents available in above\ndigital media and extracting only relevant information from all\nthese media is a tedious job for the individuals in stipulated\ntime. There is a need for an automated system that can extract\nonly relevant information from these data sources. To achieve\nthis, one need to mine the text from the documents. Text\nmining is the process of extracting large quantities of text to\nderive high-quality information. Text mining deploys some of\nthe techniques of natural language processing (NLP) such as\nparts-of-speech (POS) tagging, parsing, N-grams,\ntokenization, etc., to perform the text analysis. It includes\ntasks like automatic keyword extraction and text\nsummarization.\nAutomatic keyword extraction is the process of selecting\nwords and phrases from the text document that can at best\nproject the core sentiment of the document without any human\nintervention depending on the model [1]. The target of\nautomatic keyword extraction is the application of the power\nand speed of current computation abilities to the problem of\naccess and recovery, stressing upon information organization\nwithout the added costs of human annotators.\nSummarization is a process where the most salient features of\na text are extracted and compiled into a short abstract of the\noriginal document [2]. According to Mani and Maybury [3],\ntext summarization is the process of distilling the most\nimportant information from a text to produce an abridged\nversion for a particular task and user. Summaries are usually\naround 17\\% of the original text and yet contain everything\nthat could have been learned from reading the original article\n[4]. In the wake of big data analysis, summarization is an\nefficient and powerful technique to give a glimpse of the\nwhole data. The text summarization can be achieved in two\nways namely, abstractive summary and extractive summary.\nThe abstractive summary is a topic under tremendous\nresearch; however, no standard algorithm has been achieved\nyet. These summaries are derived from learning what was\nexpressed in the article and then converting it into a form\nexpressed by the computer. It resembles how a human would\nsummarize an article after reading it. Whereas, extractive\nsummary extract details from the original article itself and\npresent it to the reader.\nIn this paper, reviewed the recent literature on automatic keyword extraction and text summarization. The valuable keywords extraction is the primary phase of text summarization. Therefore, in this literature, we focused on both the techniques. In keyword extraction, the literature discussed about different methodologies used for keyword extraction process and what algorithms used under each methodology as shown in Figure 1. It also discussed about different domains in which keyword extraction algorithms applied. Similarly, in the process of text summarization, literature covers all the possible process of text summarization such as document types (single or multi), summary types (generic or query based), techniques (supervised or unsupervised), characteristics of summary (abstractive or extractive), etc. Further, it includes all the possible methodologies for text summarization as shown in Figure 3. This literature also discusses about different databases used for text summarization such as DUC, TAC, MEDLINE, etc. along with differnt evaluation matrices such as precision, recall, and ROUGE series. Finally, it discusses briefly about issues and research challenges faced by researchers followed by future direction of text summarization. The rest of this paper is organized as follows. Section 2 presents a review on automatic keyword extraction Section 3\npresents a review on text summarization process. A review on\nautomatic text summarization methologies are given in\nSection 4. The details about different databases are described\nin Section 5. Section 6 explains about evaluation matrices for\ntext summarization. Finally, the conclusion with future\ndirection is drawn in Section 7."}, {"heading": "2 AUTOMATIC KEYWORD EXTRACTION: A REVIEW", "text": "On the premise of past work done towards automatic keyword extraction from the text for its summarization, extraction systems can be classified into four classes, namely, simple statistical approach, linguistics approach, machine learning approach, and hybrid approaches [1] as soon in Figure 1."}, {"heading": "2.1 Simple Statistical Approach", "text": "These strategies are rough, simplistic and have a tendency to\nhave no training sets. They concentrate on statistics got from\nnon-linguistic features of the document, for example, the\nposition of a word inside the document, the term frequency,\nand inverse document frequency. These insights are later used\nto build up a list of keywords. Cohen [15], utilized n-gram\nstatistical data to discover the keyword inside the document\nautomatically. Other techniques in- side this class incorporate\nword frequency, term frequency (TF) [16] or term frequency-\ninverse document frequency (TF-IDF) [17], word co-\noccurrences [18], and PAT-tree [19]. The most essential of\nthem is term frequency. In these strategies, the frequency of\noccurrence is the main criteria that choose whether a word is a\nkeyword or not. It is extremely unrefined and tends to give\nvery unseemly results. An improvement of this strategy is the\nTF-IDF, which also takes the frequency of occurrence of a\nword as the model to choose a keyword or not. Similarly,\nword co-occurrence methods manage statistical information\nabout the number of times a word has happened and the\nnumber of times it has happened with another word. This\nstatistical information is then used to compute support and\nconfidence of the words. Apriori technique is then used to\ninfer the keywords."}, {"heading": "2.2 Linguistics Approach", "text": "This approach utilizes the linguistic features of the words for keyword detection and extraction in text documents. It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc. The resources used for lexical analysis are an electronic dictionary, tree tagger, WordNet, ngrams, POS pattern, etc. Similarly, noun phrase (NP), chunks (Parsing) are used as resources for syntactic analysis."}, {"heading": "2.3 Machine Learning Approach", "text": "Keyword extraction can also be seen as a learning problem.\nThis approach requires manually annotated training data and\ntraining models. Hidden Markov model [23], support vector\nmachine (SVM) [24], naive Bayes (NB) [25], bagging [21],\netc. are commonly used training models in these approaches.\nIn the second phase, the document whose keywords are to be\nextracted is given as inputs to the model, which then extracts\nthe keywords that best fit the model\u2019s training. One of the\nmost famous algorithms in this approach is the keyword\nextraction algorithm (KEA) [26]. In this approach, the article\nis first converted into a graph where each word is treated as a\nnode, and whenever two words appear in the same sentence,\nthe nodes are connected with an edge for each time they\nappear together. Then the number of edges connecting the\nvertices are converted into scores and are clustered\naccordingly. The cluster heads are treated as keywords.\nBayesian algorithms use the Bayes classifier to classify the\nword into two categories: keyword or not a keyword\ndepending on how it is trained. GenEx [27] is another tool in\nthis approach."}, {"heading": "2.4 Hybrid Approach", "text": "These approaches combine the above two methods or use\nheuristics, such as position, length, layout feature of the\nTable 1: Previous studies on automatic keyword extraction\nStudy Types of Approach Domain Types\nT1 T2 T3 T4 D1 D2 D3 D4 D5 D6 D7\nDennis et al.[29], 1967 \u221a \u221a Salton et al.[30], 1991 \u221a \u221a Cohen et al.[15], 1995 \u221a \u221a Chien et al.[19], 1997 \u221a \u221a Salton et al.[22], 1997 \u221a \u221a Ohsawa et al.[31], 1998 \u221a \u221a Hovy et al.[2], 1998 \u221a \u221a Fukumoto et al.[32], 1998 \u221a \u221a \u221a \u221a Mani et al.[3], 1999 \u221a Witten et al.[26], 1999 \u221a \u221a Frank et al.[25], 1999 \u221a \u221a \u221a Barzilay et al.[20], 1999 \u221a Turney et al.[27], 1999 \u221a \u221a Conroy et al.[23], 2001 \u221a \u221a Humphreys et al.[28], 2002 \u221a \u221a \u221a Hulth et al.[21], 2003 \u221a \u221a \u221a \u221a \u221a Ramos et al.[17], 2003 \u221a Matsuo et al.[18], 2004 \u221a Erkan et al.[4], 2004 \u221a Van et al.[6], 2004 \u221a \u221a Mihalcea et al.[33], 2004 \u221a \u221a Zhang et al.[24], 2006 \u221a \u221a Ercan et al.[8], 2007 \u221a \u221a Litvak et al.[9], 2008 \u221a \u221a Zhang et al.[1], 2008 \u221a \u221a Thomas et al.[5], 2016 \u221a \u221a \u221a \u221a\nwords, HTML tags around the words, etc. [28]. These\nalgorithms are designed to take the best features from above\nmentioned approaches.\nBased on the classification shown in Figure 1, we bring a\nconsolidated summary of previous studies on automatic\nkeyword extraction and is shown in Table 1. It discusses the\napproaches that are used for keyword extraction and various\ndomains of dataset in which experiments are performed as\nshown in Table 2."}, {"heading": "3. TEXT SUMMARIZATION PROCESS: A REVIEW", "text": "Based on the literature, text summarization process can be\ncharacterized into five types, namely, based on the number of\nthe document, based on summary usage, based on techniques,\nbased on characteristics of summary as text and based on\nlevels of linguistics process [1] as shown in Figure 2."}, {"heading": "3.1 Single Document Text Summarization", "text": "In single document text summarization, it takes a single\ndocument as an input to perform summarization and produce a\nsingle output document [5][34][35][36][37]. Thomas et al. [5]\ndesigned a system for automatic keyword extraction for text\nsummarization in single document e-Newspaper article.\nMarcu et al. [35] developed a discourse-based summarizer\nthat determines adequacy for summarizing texts for discourse-\nbased methods in the domain of single news articles."}, {"heading": "3.2 Multiple Document Text Summarization", "text": "In multiple documents text summarization, it takes numerous\ndocuments as an input to perform summarization and deliver a\nsingle output document [14][38][39][40][41][42][43][44].\nMirroshandel et al. [44] presents two different algorithms\ntowards temporal relation based keyword extraction and text\nsummarization in multi-document. The first algorithm was a\nweakly supervised machine learning approach for\nclassification of temporal relations between events and the\nsecond algorithm was expectation maximization (EM) based\nunsupervised learning approach for temporal relation\nextraction. Min et al. [40] used the information which is\ncommon to document sets belonging to a common category to\nimprove the quality of automatically extracted content in\nmulti-document summaries."}, {"heading": "3.3 Query-based Text Summarization", "text": "In this summarization technique, a particular portion is\nutilized to extract the essential keyword from input document\nto make the summary of corresponding document\n[11][37][48][49][50][51]. Fisher et al. [50] developed a query-\nbased summarization system that uses a log-linear model to\nclassify each word in a sentence. It exploits the property of\nsentence ranking methods in which they consider neural query\nranking and query-focused ranking. Dong et al. [51]\ndeveloped a query-based summarization that uses document\nranking, time-sensitive queries and ranks recency sensitive\nqueries as the features for text summarization."}, {"heading": "3.4 Extractive Text Summarization", "text": "In this procedure, summarizer discovers more critical\ninformation (either words or sentences) from input document\nto make the summary of the corresponding document\n[2][5][52][35][53][54][55][65][75][39][40]. In this process, it\nuses statistical and linguistic features of the sentences to\ndecide the most relevant sentences in the given input\ndocument. Thomas et al. [5] designed a hybrid model based\nextractive summarizer using machine learning and simple\nstatistical method for keyword extraction from e-Newspaper\narticle. Min et al. [40] used freely available, open-source\nextractive summarization system, called SWING to\nsummarize the text in multi-document. They used information\nwhich is common to document sets belonging to a common\ncategory as a feature and encapsulated the concept of\ncategory-specific importance (CSI). They showed that CSI is a\nvaluable metric to aid sentence selection in extractive\nFigure 2: Characterization of the text summarization process\nsummarization tasks. Marcu et al. [35] developed a discourse-\nbased extractive summarizer that uses the rhetorical parsing\nalgorithm to determine discourse structure of the text of given\ninput, determine partial ordering on the elementary and\nparenthetical units of the text. Erkan et al. [65] developed an\nextractive summarization environment. It consists of three\nsteps: feature extractor, the feature vector, and reranker.\nFeatures are Centroid, Position, Length Cutoff, SimWithFirst,\nLexPageRank, and QueryPhraseMatch. Alguliev et al. [39]\ndeveloped an unsupervised learning based extractive\nsummarizer that optimizes three properties: relevance,\nredundancy, and length. It split documents into sentences and\nselect salient sentences from the document. Aramaki et al.\n[75] destined a supervised learning based extractive text\nsummarizer that identifies the negative event and it also\ninvestigates what kind of information is helpful for negative\nevent identification. An SVM classifier is used to distinguish\nnegative events from other events."}, {"heading": "3.5 Abstractive Text Summarization", "text": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58]. It uses linguistic methods to examine and interpret the text and then to find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document. Brandow et al. [56] developed an abstractive summarization system that analyses the statistical corpus and extracts the signature words from the corpus. Then it assigns the weight for all the signature words. Based on the extracted signature words, they assign the weight to the sentences and select few top weighted sentences as the summary. Daume et al. [37] developed an abstractive summarization system that maps all the documents into database-like representation. Further, it classifies into four categories: a single person, single event, multiple event, and natural disaster. It generates a short headline using a set of predefined templates. It generates summaries by extracting sentences from the database."}, {"heading": "3.6 Supervised Learning Based Text Summarization", "text": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75]. Thomas et al. [5] designed a system for automatic keyword extraction for text summarization using hidden Markov model. The learning process was supervised, it used human annotated keyword set to train the model. Mirroshandel et al. [44] used a set of labeled dataset to train the system for the classification of temporal relations between events. Aramaki et al. [75]\ndestined a supervised learning based extractive text summarizer that identifies the negative event and also investigates what kind of information is helpful for negative event identification. An SVM classifier is used to distinguish negative events from other events."}, {"heading": "3.7 Unsupervised Learning Based Text Summarization", "text": "In this technique, there are no predefined guidelines available\nat the time of training [13][38][39][44][65]. Mirroshandel et\nal. [44] proposed a method for temporal relation extraction\nbased on the Expectation-Maximization (EM) algorithm.\nWithin EM, they used different techniques such as a greedy\nbest-first search and integer linear programming for temporal\ninconsistency removal. The EM-based approach was a fully\nunsupervised temporal relation based extraction for text\nsummarization. Alguliev et al. [39] developed an\nunsupervised learning based extractive summarizer that\noptimizes three properties: relevance, redundancy, and length.\nIt split documents into sentences and select salient sentences\nfrom the document."}, {"heading": "2. TEXT SUMMARIZATION APPROACH: A REVIEW", "text": "Based on the literature, text summarization approaches can be classified into five types, namely, statistical based, machine learning based, coherent based, graph based, algebraic based as shown in Figure 3."}, {"heading": "4.1 Statistical Based Approach", "text": "This approach is very simple and crude often used for keyword extraction from the documents. There is no predefined dataset required for this approach. To extract the keywords from documents it uses several statistical features of the document such as, term or word frequency (TF), Term Frequency-inverse document frequency (TF-IDF), position of keyword (POK), etc. as mentioned in Figures 1 and 3. These statistical features are used for text summarization [5][90][91][92]."}, {"heading": "4.2 Machine learning Based Approach", "text": "Machine learning is a feature dependent approach we one need annotated dataset to trained the models. There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc. used for text summarization."}, {"heading": "4.3 Coherent Based Approach", "text": "A coherent based approach basically deals with the cohesion relations among the words. Cohesion relations among elements in a text: reference, ellipsis, substitution, conjunction, and lexical cohesion [112]. Lexical chain (LC)\n[113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9]."}, {"heading": "4.4 Graph Based Approach", "text": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120]."}, {"heading": "4.5 Algebraic Approach", "text": "In this approach, one use algebraic theories namely, matrix,\ntranspose of matrix, Eigen vectors, etc. There are many\nalgorithms used for text summarization using algebraic\napproach such as Latent Semantic Analysis (LSA)\n[121][122][123], Meta Latent Semantic Analysis (MLSA)\n[124][125], Symmetric nonnegative matrix factorization\n(SNMF) [126], Sentence level semantic analysis (SLSS)\n[126], Non-Negative Matrix factorization (NMF) [127],\nSingular Value Decomposition (SVD) [128], Semi-Discrete\nDecomposition (SDD) [129]."}, {"heading": "3. DATABASES: A REVIEW", "text": "In the literature, we observed that, there are seven types of databases used for text summarization process namely, document understanding workshop (DUC), MEDLINE, Text Analysis Conference (TAC), Computational Linguistics Scientific Document Summarization Shared Task Corpus (CL-SciSumm), TIPSTER Text Summarization Evaluation Conference (SUMMAC), Topic Detection and Tracking (TDT). The DUC is the international conference for performance evaluation in the area of text summarization. This dataset is composed of 50 topics and 25 documents\nrelevant to each topic from the AQUAINT corpus for queryrelevant multi-document summarization [21]. MEDLINE/ PubMed dataset is a baseline repository that links 19 million articles to with http://dx.doi.org/ article identifiers and http://crossref.org/ with journal identifiers [130] and it contains abstracts from more than 3500 journals [131]. MEDLINE also provides keyword searches and returns abstracts that contain the keywords. The TAC is the international conference for performance evaluation in the area of subparts of Natural Language Processing such as text summarization. Usually, TAC- 2009, 2010 and 2011 conference dataset used for text summarization in past. The CL-SciSumm Shared Task is run off the CL-SciSumm corpus, and comprises three sub-tasks in automatic research paper summarization on a new corpus of research papers. A training corpus of twenty topics and a test corpus of ten topics were released. The topics comprised of ACL Computational Linguistics research papers, and their citing papers and three output summaries each. The three output summaries comprise: the traditional self-summary of the paper (the abstract), the community summary (the collection of citation sentences \u2018citances\u2019) and a human summary written by a trained annotator [132]. TIPSTER is a corpus of 183 documents from the Computation and Language (cmp-lg) collection has been marked up in xml and made available as a general resource to the information retrieval, extraction, and summarization communities."}, {"heading": "4. 6. PERFORMANCE EVALUATION MEASURE: A REVIEW", "text": "In the field of text summarization, performance evaluation measure can be classified into two categories, namely, intrinsic and extrinsic. The intrinsic evaluation judges the quality of the summary directly based on analysis in terms of some set of norms whereas, extrinsic evaluation judges the quality of the summary based on the how it affects the completion of some other task. The complete structure of classification about evaluation measure for text summarization is given in Figure 4. To compare the performances, one used the ROUGE evaluation software package, which compares various summary results from several summarization methods with summaries generated by humans. ROUGE has been applied by the Document Understanding Conference (DUC) for performance evaluation. ROUGE includes five automatic evaluation methods, ROUGEN, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU [24]. Each method estimates recall, precision, and f-measure between experts\u2019 reference summaries and candidate summaries of the proposed system. ROUGE-N uses the n-gram recall between a candidate summary and a set of reference summaries.\nBased on the characterization of text summarization process as shown in Figure 2, classifications of text summarization approaches as shown in Figure 3, databases as discussed in Section 3 and performance evaluation matrices as shown in Figure 4, we bring a consolidated summary of previous studies in text summarization and is shown in Table 4. It discusses the approaches that are used for text summarization; experiment performed using single or multiple documents, types of summary usage, characteristics of the summary and metrics used. The details of the parameters are given in Table 5."}, {"heading": "7. ISSUES AND CHALLENGES OCCURS IN TEXT SUMMARIZATION", "text": "In the area of text summarization, there are following research issues and challenges occurs during implementation."}, {"heading": "7.1 Research Issues", "text": " In the case of multi-document text summarization, several issues occurs frequently while evaluation of\nsummary such as redundancy, temporal dimension, co-reference or sentence ordering, etc. which makes very difficult to achieve quality summary. Some other issues occurs such as grammaticality, cohesion, coherence which is harmful for summary.\n The quality of summaries are varying from system to system or person to person. Some person feels some\nset of sentences are important for summary, at the same time other person feel the other set of sentences are important for required summary."}, {"heading": "7.2 Implementation Challenges", "text": " To get the quality summary, quality keywords are required for text summarization.\n There is no standard to identify quality keywords within or multiple documents. The extracted\nkeywords are varying for applying different approaches of keyword extraction.\n Multi-lingual text summarisation is another challenging task."}, {"heading": "8. CONCLUSION AND FUTURE DIRECTION", "text": "Text summarization is very helpful for users to extract only needed information in stipulated time. In this area, considerable amount of work has been done in the recent past. Due to lack of information and standardization lot of research overlap is a common phenomenon. Since 2012, exhaustive review paper is not published on automatic keyword extraction and text summarization especially in Indian context. Therefore, we thought that, the survey paper covering recent work in keyword extraction and text summarization may ignite the research community for filling some important research gaps. This paper contains the literature review of recent work in text summarization from the point of views of automatic keyword extraction, text databases, summarization process, summarization methodologies and evaluation matrices. Some important research issues in the area of text summarization are also highlighted in the paper.\nIn future, one can target following direction in the field of\nsummarization:\n Text summarization in low resourced languages especially in Indian language context such as Telugu,\nHindi, Tamil, Bengali, etc.\n This work can also be extended to multi-lingual text summarization.\n Multimedia summarization.\n Multi-lingual multimedia summarization."}], "references": [{"title": "Automatic keyword extraction from documents using conditional random fields,", "author": ["C. Zhang"], "venue": "Journal of Computational Information Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Automated text summarization and the summarist system,", "author": ["E. Hovy", "C.-Y. Lin"], "venue": "in: Proceedings of a workshop on held at Baltimore, ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Lexrank: graph-based lexical centrality as salience in text summarization,", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Automatic keyword extraction for text summarization in e-newspapers,", "author": ["J.R. Thomas", "S.K. Bharti", "K.S. Babu"], "venue": "Proceedings of the International Conference on Informatics and Analytics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Giarlo, \u201cA comparative analysis of keyword extraction techniques,", "author": ["J. M"], "venue": "citeseer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Using lexical chains for keyword extraction, Information", "author": ["G. Ercan", "I. Cicekli"], "venue": "Processing & Management,\u201d vol", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Graph-based keyword extraction for singledocument summarization,", "author": ["M. Litvak", "M. Last"], "venue": "in: Proceedings of the workshop on Multisource Multilingual Information Extraction and Summarization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Event-based extractive summarization,", "author": ["E. Filatova", "V. Hatzivassiloglou"], "venue": "in: Proceedings of ACL Workshop on Summarization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Classy query-based multi-document summarization,", "author": ["J.M. Conroy", "J.D. Schlesinger", "J.G. Stewart"], "venue": "in: Proceedings of the 2005 Document Understanding Workshop, Citeseer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Fastsum: fast and accurate query-based multi-document summarization,", "author": ["F. Schilder", "R. Kondadadi"], "venue": "in: Proceedings of the 46th annual meeting of the association for computational linguistics on human language technologies,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Fuzzy clustering for topic analysis and summarization of document collections,", "author": ["R. Witte", "S. Bergler"], "venue": "in: Advances in Artificial Intelligence, Springier,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Themecrowds: Multiresolution summaries of twitter usage,", "author": ["D. Archambault", "D. Greene", "P. Cunningham", "N. Hurley"], "venue": "in: Proceedings of the 3rd international workshop on Search and mining user-generated contents,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Highlights: Language- and domain-independent automatic indexing terms for abstracting,", "author": ["J.D. Cohen"], "venue": "JASIS, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "A statistical approach to mechanized encoding and searching of literary information,", "author": ["H.P. Luhn"], "venue": "IBM Journal of research and development,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1957}, {"title": "Using tf-idf to determine word relevance in document queries,", "author": ["J. Ramos"], "venue": "in: Proceedings of the first instructional conference on machine learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Keyword extraction from a single document using word co-occurrence statistical information,", "author": ["Y. Matsuo", "M. Ishizuka"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Pat-tree-based keyword extraction for Chinese information retrieval,", "author": ["L.-F. Chien"], "venue": "in: ACM SIGIR Forum,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Using lexical chains for text summarization,", "author": ["R. Barzilay", "M. Elhadad"], "venue": "Advances in automatic text summarization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Improved automatic keyword extraction given more linguistic knowledge,", "author": ["A. Hulth"], "venue": "in: Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Automatic text structuring and summarization,", "author": ["G. Salton", "A. Singhal", "M. Mitra", "C. Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Text summarization via hidden Markov models,", "author": ["J.M. Conroy", "D.P. O'leary"], "venue": "in: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Keyword extraction using support vector machine,", "author": ["K. Zhang", "H. Xu", "J. Tang", "J. Li"], "venue": "in: Advances in Web-Age Information Management, Springier,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Domain-specific key-phrase extraction,", "author": ["E. Frank", "G.W. Paynter", "I.H. Witten", "C. Gutwin", "C.G. Nevill-Manning"], "venue": "In 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Kea: Practical automatic key-phrase extraction,", "author": ["I.H. Witten", "G.W. Paynter", "E. Frank", "C. Gutwin", "C.G. Nevill-Manning"], "venue": "in: Proceedings of the fourth ACM conference on Digital libraries,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Learning to extract key-phrases from text,", "author": ["P. Turney"], "venue": "arXiv preprint cs/0212013", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "The design and testing of a fully automatic indexing searching system for documents consisting of expository text,", "author": ["S.F. Dennis"], "venue": "Information Retrieval: a Critical Review, Washington DC: Thompson Book Company,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1967}, {"title": "Automatic text structuring and retrieval experiments in automatic encyclopedia searching", "author": ["G. Salton", "C. Buckley"], "venue": "in: Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1991}, {"title": "Keygraph: Automatic indexing by co-occurrence graph based on building construction metaphor,", "author": ["Y. Ohsawa", "N.E. Benson", "M. Yachida"], "venue": "in: Research and Technology Advances in Digital Libraries,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Keyword extraction of radio news using term weighting with an encyclopedia and newspaper articles,", "author": ["F. Fukumoto", "Y. Sekiguchi", "Y. Suzuki"], "venue": "in: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Graph-based ranking algorithms for sentence extraction, applied to text summarization,", "author": ["R. Mihalcea"], "venue": "In Proceedings of the Association for Computational Linguistics on Interactive poster and demonstration sessions. ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Automatic abstracting research at chemical abstracts service, Journal of Chemical Information and Computer Sciences", "author": ["J.J. Pollock", "A. Zamora"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1975}, {"title": "Discourse trees are good indicators of importance in text, Advances in automatic text summarization", "author": ["D. Marcu"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Generating single and multi-document summaries with gistexter", "author": ["S.M. Harabagiu", "F. Lacatusu"], "venue": "in: Document Understanding Conferences,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Robust generic and querybased summarization,", "author": ["H. Saggion", "K. Bontcheva", "H. Cunningham"], "venue": "in: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Exploiting category-specific information for multi-document summarization,", "author": ["Z.L. Min", "Y.K. Chew", "L. Tan"], "venue": "Proceedings of COLING,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Framework for abstractive summarization using text-to-text generation,", "author": ["P.-E. Genest", "G. Lapalme"], "venue": "in: Proceedings of the Workshop on Monolingual Text-To-Text Generation,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Mashechkin, \u201cUsing NMF-based text summarization to improve supervised and unsupervised classification,", "author": ["D. Tsarev", "I.M. Petrovskiy"], "venue": "in: Hybrid Intelligent Systems (HIS),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Uwn: A large multilingual lexical knowledge base,", "author": ["G. de Melo", "G. Weikum"], "venue": "in: Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Webinessence: A personalized web based multi-document summarization and recommendation system,", "author": ["D.R. Radev", "W. Fan", "Z. Zhang"], "venue": "Ann Arbor, vol", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2001}, {"title": "Multi-document summarization using generic relation extraction,", "author": ["B. Hachey"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Cats a topic-oriented multidocument summarization system at duc 2005,", "author": ["A. Farzindar", "F. Rozon", "G. Lapalme"], "venue": "in: Proc. of the Document Understanding Workshop,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2005}, {"title": "Towards recency ranking in web search,", "author": ["A. Dong", "Y. Chang", "Z. Zheng", "G. Mishne", "J. Bai", "R. Zhang", "K. Buchner", "C. Liao", "F. Diaz"], "venue": "in: Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Generating natural language summaries from multiple on-line sources,", "author": ["D.R. Radev", "K.R. McKeown"], "venue": "Computational Linguistics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1998}, {"title": "A multilingual news summarizer,", "author": ["H.-H. Chen", "C.-J. Lin"], "venue": "in: Proceedings of the 18th conference on Computational linguistics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2000}, {"title": "Experiments in single and multi-document summarization using mead,", "author": ["D.R. Radev", "S. Blair-Goldensohn", "Z. Zhang"], "venue": "Ann Arbor, vol", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2001}, {"title": "Newsinessence: A system for domain-independent, real-time news clustering and multi-document summarization,", "author": ["D.R. Radev", "S. Blair-Goldensohn", "Z. Zhang", "R.S. Raghavan"], "venue": "in: Proceedings of the first international conference on Human language technology research,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2001}, {"title": "Automatic condensation of electronic publications by sentence selection,", "author": ["R. Brandow", "K. Mitze", "L.F. Rau"], "venue": "Information Processing & Management, vol", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1995}, {"title": "Information fusion in the context of multi-document summarization,", "author": ["R. Barzilay", "K.R. McKeown", "M. Elhadad"], "venue": "in: Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1999}, {"title": "Automated multi-document summarization in neats,", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "in: Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2002}, {"title": "Sigelman, \u201cTracking and summarizing news on a daily basis with columbia's newsblaster,", "author": ["K.R. McKeown", "R. Barzilay", "D. Evans", "V. Hatzivassiloglou", "J.L. Klavans", "A. Nenkova", "C. Sable", "S.B. Schi man"], "venue": "in: Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2002}, {"title": "Generating indicative-informative summaries with sumum,", "author": ["H. Saggion", "G. Lapalme"], "venue": "Computational linguistics,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2002}, {"title": "The university of Lethbridge text summarizer at duc 2003,", "author": ["Y. Chali", "M. Kolla", "N. Singh", "Z. Zhang"], "venue": "in: the Proceedings of the HLT/NAACL workshop on Automatic Summarization/Document Understanding Conference,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2003}, {"title": "Picking phrases, picking sentences,", "author": ["T. Copeck", "S. Szpakowicz"], "venue": "in: the Proceedings of the HLT/NAACL workshop on Automatic Summarization/ Document Understanding Conference,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2003}, {"title": "The university of Michigan at duc 2004,", "author": ["G. Erkan", "D.R. Radev"], "venue": "in: Proceedings of the Document Understanding Conferences Boston, MA,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Crl/nyu summarization system at duc-2004,", "author": ["C. Nobata", "S. Sekine"], "venue": "in: Proceedings of DUC,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2004}, {"title": "Erss 2005: Co-reference-based summarization reloaded,", "author": ["R. Witte", "R. Krestel", "S. Bergler"], "venue": "in: DUC 2005 Document Understanding Workshop,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Context-based multi-document summarization using fuzzy co-reference cluster graphs,", "author": ["R.Witte", "R. Krestel", "S. Bergler"], "venue": "in: Proc. of Document Understanding Workshop at HLT-NAACL,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "Msbga: A multidocument summarization system based on genetic algorithm,", "author": ["Y.-X. He", "D.-X. Liu", "D.-H. Ji", "H. Yang", "C. Teng"], "venue": "in: 2006 International Conference on Machine Learning and Cybernetics,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2006}, {"title": "Qcs: A system for querying, clustering and summarizing documents,", "author": ["D.M. Dunlavy", "D.P.O. Leary", "J.M. Conroy", "J.D. Schlesinger"], "venue": "Information Processing & Management, vol", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2007}, {"title": "Enhancing single document summarization by combining ranknet and third-party sources,", "author": ["K.M. Svore", "L. Vanderwende", "C.J. Burges"], "venue": "in: EMNLP-CoNLL,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2007}, {"title": "Personalized pagerank based multidocument summarization,", "author": ["Y. Liu", "X. Wang", "J. Zhang", "H. Xu"], "venue": "in: International Workshop on Semantic Computing and Systems,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2008}, {"title": "Adasum: an adaptive model for summarization,", "author": ["J. Zhang", "X. Cheng", "G. Wu", "H. Xu"], "venue": "in: Proceedings of the 17th ACM conference on Information and knowledge management,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2008}, {"title": "Text2table: Medical text summarization system based on named entity recognition and modality identification,", "author": ["E. Aramaki", "Y. Miura", "M. Tonoike", "T. Ohkuma", "H. Mashuichi", "K. Ohe"], "venue": "in: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2009}, {"title": "Tiara: a visual exploratory text analytic system,", "author": ["F. Wei", "S. Liu", "Y. Song", "S. Pan", "M.X. Zhou", "W. Qian", "L. Shi", "L. Tan", "Q. Zhang"], "venue": "in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}, {"title": "An algorithm for pronominal anaphora resolution,", "author": ["S. Lappin", "H.J. Leass"], "venue": "Computational linguistics,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1994}, {"title": "Sarcastic sentiment detection in tweets streamed in real time: a big data approach,", "author": ["S. Bharti", "B. Vachha", "R. Pradhan", "K. Babu", "S. Jena"], "venue": "Digital Communications and Networks,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "On helmholtz's principle for documents processing,", "author": ["A. Balinsky", "H.Y. Balinsky", "S.J. Simske"], "venue": "in: Proceedings of the 10th ACM symposium on Document engineering,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2010}, {"title": "Automatic multi document summarization approaches,", "author": ["Y.J. Kumar", "N. Salim"], "venue": "In Journal of Computer Science,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2012}, {"title": "A survey on automatic text summarization. Literature Survey for the Language and Statistics II course", "author": ["D. Das", "A.F. Martins"], "venue": null, "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2007}, {"title": "A survey on automatic text summarization,", "author": ["C.S. Saranyamol", "L. Sindhu"], "venue": "Int. J. Computer. Sci. Inf. Technology,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2014}, {"title": "Automated Knowledge Provider System with Natural Language Query Processing,", "author": ["P. Mukherjee", "B. Chakraborty"], "venue": "IETE Technical Review, vol. 33(5),", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2016}, {"title": "Automatic multi-document summarization based on clustering and nonnegative matrix factorization,", "author": ["S. Park", "B. Cha", "D.U. An"], "venue": "IETE Technical Review,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2010}, {"title": "A literature survey on information extraction and text summarization,", "author": ["Zechner"], "venue": "Computational Linguistics Program,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 1997}, {"title": "Text summarisation in progress: a literature review,", "author": ["E. Lloret", "M. Palomar"], "venue": "Artificial Intelligence Review,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2012}, {"title": "The automatic creation of literature abstract,", "author": ["H.P. Luhn"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1958}, {"title": "Statistical approach based keyword extraction aid dimensionality reduction,", "author": ["M.R. Murthy", "J.V.R. Reddy", "P.P. Reddy", "S.C. Satapathy"], "venue": "In Proceedings of the International Conference on Information Systems Design and Intelligent Applications. Springer,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2011}, {"title": "Naive (Bayes) at forty: The independence assumption in information retrieval,", "author": ["D.D. Lewis"], "venue": "In Proceedings of tenth European Conference on Machine Learning,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1998}, {"title": "Tackling the poor assumptions of nave Bayes classifiers,", "author": ["J.D.M. Rennie", "L. Shih", "J. Teevan", "D.R. Karger"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2003}, {"title": "Increasing the accuracy of discriminative of multinomial Bayesian classifier in text classification,", "author": ["T. Mouratis", "S. Kotsiantis"], "venue": "In Proceedings of fourth International Conference on Computer Sciences and Convergence Information Technology,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2009}, {"title": "Training a selection function for extraction,", "author": ["C.Y. Lin"], "venue": "In Proceedings of the eighth International Conference on Information and Knowledge Management,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1999}, {"title": "Statistics-based summarization- step one: Sentence compression,", "author": ["K. Knight", "D. Marcu"], "venue": "In Proceeding of the Seventeenth National Conference of the American Association for Artificial Intelligence,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2000}, {"title": "An introduction to hidden Markov model,", "author": ["L. Rabiner", "B. Juang"], "venue": "Acoustics Speech and Signal Processing Magazine, vol", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2003}, {"title": "Script recognition using hidden Markov models,", "author": ["R. Nag", "K.H. Wong", "F. Fallside"], "venue": "In Proceedings of International Conference on Acoustics Speech and Signal Processing,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 1986}, {"title": "Research of information extraction algorithm based on hidden Markov model,", "author": ["C. Zhou", "S. Li"], "venue": "In Proceedings of second International Conference on Information Science and Engineering,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2010}, {"title": "Using maximum entropy for sentence extraction,", "author": ["M. Osborne"], "venue": "In Proceedings of the AssociaCL-02 Workshop on Automatic Summarization,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2002}, {"title": "Thumbs up? Sentiment classification using machine learning technique,", "author": ["B. Pang", "L. Lee", "S.Vaithyanathan"], "venue": "In Proceedings of the Association for Computational Linguistics conference on Empirical methods in Natural Language Processing,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2002}, {"title": "A maximum entropy approach information extraction from semi-structured and free text,", "author": ["H.L. Chieu", "H.T. Ng"], "venue": "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, American Association for Artificial Intelligence,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2002}, {"title": "A comparison of algorithms for maximum entropy parameter estimation,", "author": ["R. Malouf"], "venue": "In Proceedings of sixth conference on Natural Language Learning,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2002}, {"title": "A hidden state maximum entropy model forward confidence estimation,", "author": ["P. Yu", "J. Xu", "G.L. Zhang", "Y.C. Chang", "F. Seide"], "venue": "In International Conference on Acoustic, Speech and Signal Processing,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2007}, {"title": "Automatic text categorization using neural networks,", "author": ["M.E. Ruiz", "P. Srinivasan"], "venue": "In Proceedings of the Eighth American Society for Information Science/", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 1997}, {"title": "Ntc (neural network categorizer) neural network for text categorization,", "author": ["T. Jo"], "venue": "International Journal of Information Studies,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2010}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "mach. learn. Machine Learning,", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2002}, {"title": "Ntt\u2019s text summarization system for duc-2002,", "author": ["T. Hirao", "Y. Sasaki", "H. Isozaki", "E. Maeda"], "venue": "In Proceedings of the Document Understanding Conference,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2002}, {"title": "Sentence extraction with support vector machine ensemble,", "author": ["L.N. Minh", "A. Shimazu", "H.P. Xuan", "B.H. Tu", "S. Horiguchi"], "venue": "In Proceedings of the First World Congress of the International Federation for Systems Research,", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2005}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text,", "author": ["J. Morris", "G. Hirst"], "venue": "Journal of Computational Linguistics,", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 1999}, {"title": "WordNet::similarity: Measuring the relatedness of concepts,", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "In Proceedings in Human Language Technology Conference,", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2004}, {"title": "Efficient text summarization using lexical chains,", "author": ["H.G. Silber", "K.F. McCoy"], "venue": "In Proceedings of Fifth International Conference on Intelligent User Interfaces,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2000}, {"title": "Rhetorical structure theory: Towards a function theory of text organization,", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Text - Interdisciplinary Journal for the Study of Discourse,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 1988}, {"title": "Evaluation of automatic summarization methods based on rhetorical structure theory,", "author": ["V.R. Uzeda", "T. Pard", "M. Nunes"], "venue": "In Eighth International Conference on Intelligent Systems Design and Applications,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2008}, {"title": "Automatic text summarization based on rhetorical structure theory,", "author": ["L. Chengcheng"], "venue": "In International Conference on Computer Application and System Modeling,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2010}, {"title": "The anatomy of a large-scale hyper textual web search engine,", "author": ["S. Brin", "L. Page"], "venue": "In Proceedings of the Seventh International World Wide Web Conference, Computer Networks and ISDN Systems,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 1988}, {"title": "Anatomy of a large-scale social search engine,", "author": ["D. Horowitz", "S.D. Kamvar"], "venue": "In Nineteenth International Conference on World Wide Web,", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2010}, {"title": "Information search and retrieval in microblogs,", "author": ["M. Efron"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2011}, {"title": "Introduction to latent semantic analysis,", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Journal of Discourse Processes, vol", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 1998}, {"title": "Latent semantic indexing: An overview,", "author": ["B. Rosario"], "venue": "Technical Report of INFOSYS 240, University of California,", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2000}, {"title": "Unsupervised learning by probabilistic latent semantic analysis,", "author": ["T. Hofmann"], "venue": "Journal of Machine Learning,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2001}, {"title": "Meta latent semantic analysis", "author": ["M. Simina", "C. Barbu"], "venue": "In IEEE International conference on Systems, Man and Cybernetics,", "citeRegEx": "124", "shortCiteRegEx": "124", "year": 2004}, {"title": "Adaptive Bayesian latent semantic analysis", "author": ["J.T. Chien"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 2008}, {"title": "Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization,", "author": ["D. Wang", "T. Li", "S. Zhu", "C. Ding"], "venue": "In Proceedings of the 31 Annual International ACM Special Interest Group on Information Retrieval Conference on Research and Development in Information Retrieval,", "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2008}, {"title": "Automatic generic document summarization based on non-negative matrix factorization", "author": ["J.H. Lee", "S. Park", "C.M. Ahn", "D. Kim"], "venue": "Journal on Information Processing and Management,", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 2009}, {"title": "Using semi-discrete decomposition for topic identification,", "author": ["V. Snasel", "P. Moravec", "J. Pokorny"], "venue": "In Proceedings of the Eighth International Conference on Intelligent Systems Design and Applications,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 2008}, {"title": "Summarization from medical documents: a survey,", "author": ["S. Afantenos", "V. Karkaletsis", "P. Stamatopoulos"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 2005}, {"title": "Technology Rourkela, India. His research interest includes opinion mining and sarcasm sentiment detection resume. Email-id: sbharti1984@gmail.com Korra Sathya Babu is working as an Assistant Professor in the Dept. of CSE, National Institute of Technology Rourkela India. His research interest includes Data engineering, Data privacy, Opinion mining and Sarcasm sentiment detection. Email-id: prof.ksb@gmail.com Sanjay Kumar Jena is working as a Professor in the Dept", "author": [], "venue": null, "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "intervention depending on the model [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Summarization is a process where the most salient features of a text are extracted and compiled into a short abstract of the original document [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "Summaries are usually around 17\\% of the original text and yet contain everything that could have been learned from reading the original article [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "On the premise of past work done towards automatic keyword extraction from the text for its summarization, extraction systems can be classified into four classes, namely, simple statistical approach, linguistics approach, machine learning approach, and hybrid approaches [1] as soon in Figure 1.", "startOffset": 271, "endOffset": 274}, {"referenceID": 12, "context": "Cohen [15], utilized n-gram statistical data to discover the keyword inside the document automatically.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Other techniques in- side this class incorporate word frequency, term frequency (TF) [16] or term frequency-", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "inverse document frequency (TF-IDF) [17], word cooccurrences [18], and PAT-tree [19].", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "inverse document frequency (TF-IDF) [17], word cooccurrences [18], and PAT-tree [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "inverse document frequency (TF-IDF) [17], word cooccurrences [18], and PAT-tree [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc.", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "It incorporates the lexical analysis [20], syntactic analysis [21], discourse analysis [22], etc.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "Hidden Markov model [23], support vector machine (SVM) [24], naive Bayes (NB) [25], bagging [21], etc.", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "One of the most famous algorithms in this approach is the keyword extraction algorithm (KEA) [26].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "GenEx [27] is another tool in this approach.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "[29], 1967 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30], 1991 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15], 1995 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19], 1997 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22], 1997 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31], 1998 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], 1998 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[32], 1998 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26], 1999 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25], 1999 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20], 1999 \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27], 1999 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23], 2001 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21], 2003 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17], 2003 \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18], 2004 \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[4], 2004 \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[33], 2004 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24], 2006 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[8], 2007 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9], 2008 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], 2008 \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5], 2016 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Based on the literature, text summarization process can be characterized into five types, namely, based on the number of the document, based on summary usage, based on techniques, based on characteristics of summary as text and based on levels of linguistics process [1] as shown in Figure 2.", "startOffset": 267, "endOffset": 270}, {"referenceID": 3, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 140, "endOffset": 143}, {"referenceID": 30, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "In single document text summarization, it takes a single document as an input to perform summarization and produce a single output document [5][34][35][36][37].", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "[5] designed a system for automatic keyword extraction for text summarization in single document e-Newspaper article.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[35] developed a discourse-based summarizer that determines adequacy for summarizing texts for discoursebased methods in the domain of single news articles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "single output document [14][38][39][40][41][42][43][44].", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "[40] used the information which is common to document sets belonging to a common category to improve the quality of automatically extracted content in multi-document summaries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 165, "endOffset": 169}, {"referenceID": 33, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 169, "endOffset": 173}, {"referenceID": 40, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 173, "endOffset": 177}, {"referenceID": 41, "context": "In this summarization technique, a particular portion is utilized to extract the essential keyword from input document to make the summary of corresponding document [11][37][48][49][50][51].", "startOffset": 185, "endOffset": 189}, {"referenceID": 41, "context": "[51] developed a query-based summarization that uses document ranking, time-sensitive queries and ranks recency sensitive queries as the features for text summarization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 167, "endOffset": 170}, {"referenceID": 42, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 170, "endOffset": 174}, {"referenceID": 31, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 174, "endOffset": 178}, {"referenceID": 43, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 178, "endOffset": 182}, {"referenceID": 44, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 182, "endOffset": 186}, {"referenceID": 45, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 186, "endOffset": 190}, {"referenceID": 53, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 190, "endOffset": 194}, {"referenceID": 62, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 194, "endOffset": 198}, {"referenceID": 34, "context": "In this procedure, summarizer discovers more critical information (either words or sentences) from input document to make the summary of the corresponding document [2][5][52][35][53][54][55][65][75][39][40].", "startOffset": 202, "endOffset": 206}, {"referenceID": 3, "context": "[5] designed a hybrid model based extractive summarizer using machine learning and simple statistical method for keyword extraction from e-Newspaper article.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[40] used freely available, open-source extractive summarization system, called SWING to summarize the text in multi-document.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35] developed a discoursebased extractive summarizer that uses the rhetorical parsing algorithm to determine discourse structure of the text of given input, determine partial ordering on the elementary and parenthetical units of the text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[65] developed an extractive summarization environment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[75] destined a supervised learning based extractive text summarizer that identifies the negative event and it also investigates what kind of information is helpful for negative event identification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 144, "endOffset": 148}, {"referenceID": 42, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 148, "endOffset": 152}, {"referenceID": 46, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 152, "endOffset": 156}, {"referenceID": 47, "context": "In this procedure, a machine needs to comprehend the idea of all the input documents and then deliver summary with its particular sentences [34][37][52][56][57][58].", "startOffset": 156, "endOffset": 160}, {"referenceID": 46, "context": "[56] developed an abstractive summarization system that analyses the statistical corpus and extracts the signature words from the corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37] developed an abstractive summarization system that maps all the documents into database-like representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 78, "endOffset": 82}, {"referenceID": 60, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 90, "endOffset": 94}, {"referenceID": 62, "context": "This type of learning techniques used labeled dataset for training [5][12][38][40][44][50][73][75].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "[5] designed a system for automatic keyword extraction for text summarization using hidden Markov model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 62, "context": "[75] destined a supervised learning based extractive text summarizer that identifies the negative event and also investigates what kind of information is helpful for negative event identification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In this technique, there are no predefined guidelines available at the time of training [13][38][39][44][65].", "startOffset": 88, "endOffset": 92}, {"referenceID": 53, "context": "In this technique, there are no predefined guidelines available at the time of training [13][38][39][44][65].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "These statistical features are used for text summarization [5][90][91][92].", "startOffset": 59, "endOffset": 62}, {"referenceID": 74, "context": "These statistical features are used for text summarization [5][90][91][92].", "startOffset": 66, "endOffset": 70}, {"referenceID": 75, "context": "These statistical features are used for text summarization [5][90][91][92].", "startOffset": 70, "endOffset": 74}, {"referenceID": 76, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 77, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 78, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 86, "endOffset": 90}, {"referenceID": 79, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 113, "endOffset": 117}, {"referenceID": 80, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 81, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 149, "endOffset": 153}, {"referenceID": 82, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 153, "endOffset": 157}, {"referenceID": 83, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 157, "endOffset": 162}, {"referenceID": 84, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 185, "endOffset": 190}, {"referenceID": 85, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 190, "endOffset": 195}, {"referenceID": 86, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 195, "endOffset": 200}, {"referenceID": 87, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 200, "endOffset": 205}, {"referenceID": 88, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 205, "endOffset": 210}, {"referenceID": 89, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 232, "endOffset": 237}, {"referenceID": 90, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 237, "endOffset": 242}, {"referenceID": 91, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 278, "endOffset": 283}, {"referenceID": 92, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 283, "endOffset": 288}, {"referenceID": 93, "context": "There are several popular machine learning approaches namely, Nave Bayes (NB) [93][94][95], decision trees (DTs) [96][97], Hidden Markov Model (HMM) [98][99][100], Maximum Entropy (ME) [101][102][103][104][105], Neural Network (NN) [106][107][108], Support Vector Machine (SVM) [109][110][111] etc.", "startOffset": 288, "endOffset": 293}, {"referenceID": 94, "context": "Cohesion relations among elements in a text: reference, ellipsis, substitution, conjunction, and lexical cohesion [112].", "startOffset": 114, "endOffset": 119}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 19, "endOffset": 24}, {"referenceID": 5, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 39, "endOffset": 42}, {"referenceID": 96, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 42, "endOffset": 47}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 85, "endOffset": 90}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 136, "endOffset": 141}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 185, "endOffset": 190}, {"referenceID": 95, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 242, "endOffset": 247}, {"referenceID": 97, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 283, "endOffset": 288}, {"referenceID": 98, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 288, "endOffset": 293}, {"referenceID": 6, "context": "Lexical chain (LC) [113], WordNet (WN) [8][114], lexical chain score of a word (LCS) [113], direct lexical chain score of a word (DLCS) [113], lexical chain span score of a word (LCSS) [113], direct lexical chain span score of a word (DLCSS) [113], Rhetorical Structure Theory (RST) [115][116][9].", "startOffset": 293, "endOffset": 296}, {"referenceID": 99, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 121, "endOffset": 126}, {"referenceID": 100, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 126, "endOffset": 131}, {"referenceID": 99, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 160, "endOffset": 165}, {"referenceID": 29, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 165, "endOffset": 169}, {"referenceID": 101, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 169, "endOffset": 174}, {"referenceID": 102, "context": "There are two popular graph-based approaches used for text summarization namely, Hyperlinked Induced Topic Search (HITS) [117][118] and Google\u2019s PageRank (GPR) [117][33][119][120].", "startOffset": 174, "endOffset": 179}, {"referenceID": 103, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 118, "endOffset": 123}, {"referenceID": 104, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 123, "endOffset": 128}, {"referenceID": 105, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 128, "endOffset": 133}, {"referenceID": 106, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 172, "endOffset": 177}, {"referenceID": 107, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 177, "endOffset": 182}, {"referenceID": 108, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 234, "endOffset": 239}, {"referenceID": 108, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 281, "endOffset": 286}, {"referenceID": 109, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 328, "endOffset": 333}, {"referenceID": 110, "context": "There are many algorithms used for text summarization using algebraic approach such as Latent Semantic Analysis (LSA) [121][122][123], Meta Latent Semantic Analysis (MLSA) [124][125], Symmetric nonnegative matrix factorization (SNMF) [126], Sentence level semantic analysis (SLSS) [126], Non-Negative Matrix factorization (NMF) [127], Singular Value Decomposition (SVD) [128], Semi-Discrete Decomposition (SDD) [129].", "startOffset": 411, "endOffset": 416}, {"referenceID": 18, "context": "relevant to each topic from the AQUAINT corpus for queryrelevant multi-document summarization [21].", "startOffset": 94, "endOffset": 98}, {"referenceID": 111, "context": "org/ with journal identifiers [130] and it contains abstracts from more than 3500 journals [131].", "startOffset": 91, "endOffset": 96}, {"referenceID": 112, "context": "The three output summaries comprise: the traditional self-summary of the paper (the abstract), the community summary (the collection of citation sentences \u2018citances\u2019) and a human summary written by a trained annotator [132].", "startOffset": 218, "endOffset": 223}, {"referenceID": 21, "context": "ROUGE includes five automatic evaluation methods, ROUGEN, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "[34], 1975 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[56], 1995 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], 1998 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "[52], 1998 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35], 1999 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[57], 1999 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[53], 2000 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[54], 2001a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[55], 2001b \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[46], 2001c \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[59], 2002 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[60], 2002 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[36], 2002 \u221a \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[61], 2002 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37], 2003 \u221a \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[62], 2003 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[63], 2003 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[65], 2004 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[10], 2004 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[66], 2004 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11], 2005 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[48], 2005 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[67], 2005 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[68], 2006 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[69], 2006 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13], 2007 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[70], 2007 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[72], 2007 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12], 2008 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[73], 2008 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[74], 2008 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[75], 2009 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[47], 2009 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[76], 2010 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[51], 2010 \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 71, "context": "[87], 2010 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14], 2011 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[41], 2011 \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[42], 2011 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[40], 2012 \u221a \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[43], 2012 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5], 2012 \u221a \u221a \u221a \u221a", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In recent times, data is growing rapidly in every domain such as news, social media, banking, education, etc. Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.", "creator": "Microsoft\u00ae Word 2013"}}}