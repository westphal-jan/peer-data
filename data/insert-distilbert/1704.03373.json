{"id": "1704.03373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Quality Aware Network for Set to Set Recognition", "abstract": "this paper targets on the problem of set to set recognition, which constantly learns the communication metric between two image sets. images in each set belong to the same identity. since images in a set can be complementary, they hopefully lead to higher accuracy in practical engineering applications. however, the quality of each sample cannot be guaranteed, defects and samples with poor quality will hurt the metric. in this paper, the quality aware network ( qan ) is proposed to confront this problem, where the quality of each sample can be automatically learned although such genetic information is not only explicitly provided in the training stage. the network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. features and quality scores of all samples in a set alone are then aggregated to generate the final feature embedding. we show that the two branches can be trained in an end - to - end manner given only the set - level identity annotation. analysis on gradient spread of this mechanism indicates that the quality learned by the whole network is beneficial to set - to - set recognition and simplifies the robust distribution that the network needs to fit. experiments on both face verification and person - re - identification show advantages of the proposed qan. the source code and network structure can be downloaded at", "histories": [["v1", "Tue, 11 Apr 2017 15:47:41 GMT  (903kb,D)", "http://arxiv.org/abs/1704.03373v1", "Accepted at CVPR 2017"]], "COMMENTS": "Accepted at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yu liu", "junjie yan", "wanli ouyang"], "accepted": false, "id": "1704.03373"}, "pdf": {"name": "1704.03373.pdf", "metadata": {"source": "CRF", "title": "Quality Aware Network for Set to Set Recognition", "authors": ["Yu Liu", "Junjie Yan", "Wanli Ouyang"], "emails": ["liuyuisanai@gmail.com", "yanjunjie@sensetime.com", "wanli.ouyang@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance. Both the two tasks need to measure the distance between two face or person images. Such tasks can be naturally formalized as a metric learning problem, where the distance of images from the same identity should be smaller than that from different\n1 https://github.com/sciencefans/Quality-Aware-Network Note that we are developing P-QAN (a fine-grained version of QAN, see Sec.5) in this repository. So the performance may be higher than that we report in this paper.\nidentities. Built on large scale training data, convolutional neural networks and carefully designed optimization criterion, current methods can achieve promising performance on standard benchmarks, but may still fail due to appearance variations caused by large pose or illumination.\nIn practical applications, instead of one single image, a set of images for each identity can always be collected. For example, the image set of one identity can be sampled from the trajectory of the face or person in videos. Images in a set can be complementary to each other, so that they provide more information than a single image, such as images from different poses. The direct way to aggregate identity infor-\n1\nar X\niv :1\n70 4.\n03 37\n3v 1\n[ cs\n.C V\n] 1\n1 A\npr 2\n01 7\nmation from all images in a set can be simply max/average pooling appearance features of all images. However, one problem in this pooling is that some images in the set may be not suitable for recognition. As shown in Figure 1, both sets from left-top and left-bottom hold noisy images caused by shake or blur. If the noisy images are treated equally and max/average pooling is used to aggregate all images\u2019 features, the noisy images will mislead the final representation.\nIn this paper, in order to be robust to images with poor quality as described above and simultaneously use the rich information provided by the other images, our basic idea is that each image can have a quality score in aggregation. For that, we propose a quality aware network (QAN), which has two branches and then aggregated together. The first branch named feature generation part extracts the feature embedding for each image, and the other branch named quality generation part predicts quality score for each image. Features of images in the whole set are then aggregated by the final set pooling unit according to their quality.\nA good property of our approach is that we do not supervise the model by any explicit annotations of the quality. The network can automatically assign low quality scores to images with poor quality in order to keep the final feature embedding useful in set-to-set recognition. To implement that, an elaborate model is designed in which embedding branch and score generation branch can be jointly trained through optimization of the final embedding. Specially in this paper, we use the joint triplet and softmax loss on top of image sets. The designed gradient of image set pooling unit ensures the correctness of this automatic process.\nExperiments indicate that the predicted quality score is correlated with the quality annotated by human, and the predicted quality score performs better than human in recognition. In this paper, we show the applications of the proposed method on both person re-identification and face verification. For person re-identification task, the proposed quality aware network improves top-1 matching rates over the baseline by 14.6% on iLIDS-VID and 9.0% on PRID2011. For face verification, the proposed method reduces 15.6% and 29.32% miss ratio when the false positive rate is 0.001 on YouTube Face and IJB-A benchmarks.\nThe main contributions of the paper are summarized as follows.\n\u2022 The proposed quality aware network automatically generates quality scores for each image in a set and leads to better representation for set-to-set recognition.\n\u2022 We design an end-to-end training strategy and demonstrate that the quality generation part and feature generation part benefit from each other during back propagation.\n\u2022 Quality learnt by QAN is better than quality estimated\nby human and we achieves new state-of-the-art performance on four benchmarks for person re-identification and face verification."}, {"heading": "2. Related work", "text": "Our work is build upon recent advances in deep learning based person re-identification and unconstrained face recognition. In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance. To learn face representations in unconstrained face recognition, Huang et al. [11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30]. Furthermore, [26, 29] use deeper convolutional network and achieved accuracy that even surpasses human performance. The accuracy achieved by deep learning on image-based face verification benchmark LFW [12] has been promoted to 99.78%. Although deep neural network has achieved such great performance on these two problems, in present world, unconstrained set-to-set recognition is more challenging and useful.\nLooking backward, there are two different approaches handling set-to-set recognition. The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13]. Under these settings, samples in a set distribute in a Hilbert space or Grassmann mainfold so that this issue can be formulated as a metric learning problem [23, 39].\nSome other works degrade set-to-set recognition to point-to-point recognition through aggregating images in a set to a single representation in hyperspace. The most famous approach in this kind is the Bag of features [17], which uses histogram to represent the whole set for feature aggregation. Another classical work is vector of locally aggregated descriptors (VLAD) [14], which aggregates all local descriptors from all samples. Temporal max/average pooling is used in [36] to integrate all frames\u2019 features generated by recurrent convolutional network. This method uses the 1st order statistics to aggregate the set. The 2nd order statistics is used in [32, 43] in assuming that samples follow Gaussian distribution. In [8], original faces in a set are classified into 20 bins based on their pose and quality. Then faces in each bin are pooled to generate features and finally feature vectors in all bins are merged to be the final representation. [38] uses attention mechanism to summarize several sample points to a single aggregated point.\nThe proposed QAN belongs to the second approach. It discards the dross and selects the essential information in all images. Different from recent works which learn aggregation based on fixed feature [38] or image [8], the QAN learns feature representation and aggregation simultaneously. [7] proposed a similar quality aware module named \u201cmemorability based frame selection\u201d which takes \u201cvisual entropy\u201d to be the score of a frame. But the score of a frame\nis defined by human and independent with feature generation unit. In QAN, score is automatically learned and quality generation unit is joint trained with feature generation unit. Due to mutual benefit between the two parts during training, performance is improved significantly by jointly optimizing images aggregation parameter and images\u2019 feature generator."}, {"heading": "3. Quality aware network (QAN)", "text": "In our work we focus on improving image set embedding model, which maps an image set S = {I1, I2, \u00b7 \u00b7 \u00b7 , IN} to an representation with fixed dimension so that image sets with different number of images are comparable with each other. Let Ra(S) and RIi denote representation of S and Ii. Ra(S) is determined by all elements in S, therefore it can be denoted as\nRa(S) = F(RI1 , RI2 , \u00b7 \u00b7 \u00b7 , RIN ). (1)\nTheRIi is produced by a feature extraction process, containing traditional hand-craft feature extractors or convolutional neural network. F(\u00b7) is an aggregative function, which maps a variable-length input set to a representation of fixed dimension. The challenge is to find an optimized F(\u00b7), which aggregate features from the whole image set to obtain the most discriminative representation. Based on notion that images with higher quality are easier for recognition while images with lower quality containing occlusion and large pose have less effect on set representation, we denote F(\u00b7) as\nF(RI1 , RI2 , \u00b7 \u00b7 \u00b7 , RIN ) = \u2211N\ni=1 \u00b5iRIi\u2211N i=1 \u00b5i\n(2)\n\u00b5i = Q(Ii), (3)\nwhere Q(Ii) predicts a quality score \u00b5i for image Ii. So the representation of a set is a fusion of each images\u2019 features, weighted by their quality scores."}, {"heading": "3.1. QAN for image set embedding", "text": "In this paper, feature generation and aggregation module is implemented through an end-to-end convolutional neural network named QAN as shown in Fig. 2. Two branches are splited from the middle of it. In the first branch, quality generation part followed by a set pooling unit composes the aggregation module. And in the second branch, feature generation part generates images\u2019 representation. Now we introduce how an image set flows through QAN. At the beginning of the process, all images are sent into a fully convolutional network to generate middle representations. After that, QAN is divided into two branches. The first one (upper) named quality generation part is a tiny convolution neural network (see Sec. 3.4 for details) which is employed to predict quality score \u00b5. The second one (lower), called feature generation part, generates image representations RI for all images. \u00b5 and RI are aggregated at set pooling unit F , and then pass through a fully connected layer to get the final representation Ra(S). To sum up, this structure generates quality scores for images, uses these quality scores to weight images\u2019 representations and sums them up to produce the final set\u2019s representation."}, {"heading": "3.2. Training QAN without quality supervision", "text": "We train the QAN in an end-to-end manner. The data flow is shown in Fig. 2. QAN is supposed to generate discriminative representations for images and sets belonging to different identities. For image level training, a fully connection layer is established after feature generation part, which is supervised by Softmax loss Lclass. For set level training, a set\u2019s representation Ra(S) is supervised by Lveri which is formulated as:\nLveri = \u2016Ra(Sa)\u2212Ra(Sp)\u20162\u2212 \u2016Ra(Sa)\u2212Ra(Sn)\u20162 + \u03b4 (4)\nThe loss function above is referred as Triplet Loss in previous works [26]. We define Sa as anchor set, Sp as positive set, and Sn as negative set. This function minimizes variances of intra-class samples while Softmax loss cannot\nguarantee that because softmax-loss directly optimizes the probability of each class, but not the discrimination of representation.\nKeeping this in mind, we consider the set pooling operation F . The gradients back propagated through set pooling unit can be formulated as follows,\n\u2202F \u2202RIi = \u2202Ra(S) \u2202RIi = \u00b5i (5)\n\u2202F \u2202\u00b5i = \u2202Ra(S) \u2202\u00b5i = RIi \u2212Ra(S) (6)\nSo we can formulate propagation process of the final loss as\n\u2202Lveri \u2202RIi = \u2202Ra(S) \u2202RIi \u00b7 \u2202Lveri \u2202Ra(S) = \u2202Lveri \u2202Ra(S) \u00b7 \u00b5i (7)\n\u2202Lveri \u2202\u00b5i = \u2202Ra(S) \u2202\u00b5i \u00b7 ( \u2202Lveri \u2202Ra(S) )T\n= D\u2211 j=1 ( \u2202Lveri \u2202Ra(S)j \u00b7 (xij \u2212Ra(S)j)) (8)\nWhere D is the dimension of images\u2019 representation. We discuss how a quality score \u00b5 is automatically learned by this back propagation process."}, {"heading": "3.3. Mechanism for learning quality score", "text": "Automatic gradient of \u00b5. After back-propagation through set pooling unit, gradient of \u00b5i with regard to Lveri\ncan be calculated according to the Eq. 8, which is the dot product of gradient from Ra(S) and RIi . So if angle of \u2207Ra(S) and RIi belongs to (\u221290\u25e6, 90\u25e6), \u00b5i\u2019s gradient will be positive. For example, as shown in Fig. 3, the angle of \u2207Ra(Sneg) and xni\u2212Ra(Sneg) is less than 90\u25e6, so the x\u2032nis quality score \u00b5ni will become larger after this back propagation process. In contrast, the relative direction of xai is in the opposite side of the gradient of Ra(Sanchor), making it obviously a hard sample, so its quality score \u00b5ai will tend to be smaller. Obviously, samples in the \u201ccorrect\u201d directions along with set gradient always score higher in quality, while those in the \u201cwrong\u201d directions gain lower weight. For example in Fig. 3, green samples in the upper area and red samples in the lower area keep improving their quality consistently while in the middle area, sample\u2019s quality reduces. To this end, \u00b5i represents whether i \u2212 th image is a good sample or a hard sample. This conclusion will be further demonstrated by experiments. \u00b5 regulates the attention of RIi . The gradient of RIi is shown in Eq. 7 with a factor \u00b5i, together with the gradient propagated from Softmax loss. Since most of hard samples with lower \u00b5i are always poor images or even full of background noises, the factor \u00b5i in gradient of RIi weaken their harmful effect on the whole model. That is, their impact on parameters in feature generation part is negligible during back propagation. This mechanism helps feature generation part to focus on good samples and neglect ones, which benefits set-to-set recognition."}, {"heading": "3.4. Details of quality generation part", "text": "In quality aware network (QAN), quality generation part is a convolution neural network. We design different score generation parts start at different feature maps. We use QAN split at Pool4 as an instance. As shown in Fig. 4, the output spatial of Pool4 layer is 512\u00d7 14\u00d7 14. In order to generate a 1 \u00d7 1 quality score, the convolution part contains a 2-stride pooling layer and a final pooling layer with kernel size 7 \u00d7 7. A fully connected layer is followed by the final pooling layer to generate the original quality score. After that, the origin scores of all images in a set are sent to\nsigmoid layer and group L1-normalization layer to generate the final scores \u00b5. For QAN split at Pool3, we will add a block containing three 1-stride convolution layer and a 2- stride pooling layer at the beginning of quality generation unit."}, {"heading": "4. Experiments", "text": "In this section, we first explore the meaning of the quality score learned by QAN. Then QAN\u2019s sensitivity to level of feature is analysed. Based on above knowledge, we evaluate QAN on two human re-identification benchmarks and two unconstrained face verification benchmarks. Finally, we analyse the concept learned by QAN and compare it with score labelled by human."}, {"heading": "4.1. What is learned in QAN?", "text": "Qualitative analysis We visualize images with their \u00b5 generated by QAN to explore the meaning of \u00b5. Instances of same person with different qualities are shown in the first two rows in Fig. 5. All images are selected from test set. The two images in the same column belong to a same person. The upper images are random selected from images with quality scores higher than 0.8 and the lower images are selected from images with quality scores lower than the corresponding higher one. It is easy to find that images with de-\nformity, superposition, blur or extreme light condition tend to obtain lower quality scores than normal images.\nThe last two rows in Fig. 5 give some examples of other images random selected from test set. They are sorted by their quality scores from left to right. We can observe that instances with quality scores larger than 0.70 are easy to recognize by human while the others are hard. Especially many of hard images include two or more bodies in the center and we can hardly discriminate which one is the right target.\nQuantitative analysis In order to measure the relationship between the quality labelled by human and \u00b5 predicted by QAN, 1000 images in YouTube Face are selected randomly and the quality of them are rated subjectively by 6 volunteers, where each volunteer estimates a quality score for each image, ranging from 0 to 1. All the ratings of each volunteer are aligned by logistic regression. Then the 6 aligned scores of each image are averaged and finally normalized to [0, 1] to get the final quality score from human.\nWe divide the images into ten partitions based on human\u2019s score as shown in Fig. 6. In which we show the corresponding quality statistics generated by QAN. It is obvious that the scores given by the QAN are strongly correlated with human-defined quality. We further analyse the 499,500 image pairs from these 1000 images and ask hu-\nman and QAN to select the better one in each pair. Result shows that the decision made by QAN has 78.1% in common with human decision."}, {"heading": "4.2. Person re-identification", "text": "Datasets. For person re-identification, we collect 134,942 frames with 16,133 people and 212,726 bounding boxes as the training data. Experiments are conducted on PRID2011 [9] and iLiDS-VID [33] datasets. PRID2011 contains frames in two views captured at different positions of a street. CameraA has 385 identities while CameraB has 749 identities, and the two videos have a overlap of 200 people. Each person has 5 to 675 images, and the average number is 100. iLIDS-VID dataset has 300 people, and each person has two sets also captured from different positions. Each person has 23 to 192 images.\nEvaluation procedure. The results are reported in terms of Cumulative Matching Characteristics (CMC) table, each column in which represents matching rate in a certain topN matching. Two settings are used for comprehensive evaluation. In the first setting, we follow the state-of-the-art method described in [40] and [34]. The sets whose frame number is larger than 21 are used in PRID2011, and all the sets in iLIDS-VID are used. Each dataset is divided into two parts for fine-tuning and testing, respectively. For the testing set, sets form CameraA are taken as probe set while sets from CameraB are taken as the gallery. The final number is reported as the average of \u201c10-fold cross validation\u201d. In the second setting, we conduct cross-dataset testing. Different from the first setting, we ignore the finetuning process and use all data to test our model. That is, in PRID2011, the first 200 people from CameraA serve as probes, and all sets from CameraB are used as the gallery set. In iLIDS-VID, CameraA are used as the probe set, and Camera B serve as gallery set.\nBaseline. We implement two baseline approaches. In the first baseline, we use average pooling to aggregate all images\u2019 representations. In the second baseline, a minimal\ncosine distance between two closures is used to be their similarity."}, {"heading": "4.2.1 Evaluation on common setting", "text": "Results of evaluation obeying \u201c10-fold cross validation\u201d on PRID2011 and iLIDS-VID are shown in Table 1 and Table 2. Benefiting from the large scale training dataset, our CNN+AvePool and CNN+Min(cos) baselines are close to or even better than the state-of-the-art. Notice that most of the leading methods listed in table consider both appearance and spatio-temporal information while our method only considers appearance information. On PRID2011 dataset, QAN increase top-1 matching rate by 11.1% and 29.4% compared with CNN+AvePool and CNN+Min(cos). On iLIDS-VID dataset, inherent noise is much more than that in PRID2011, which significantly influence the accuracy of CNN+Min(cos) since operator \u201cMin(cos)\u201d is more sensitive than \u201cAvePool\u201d to noisy samples . However, QAN achieves more gain on this noisy dataset. It increase top-1 matching rate by 12.21% and 37.9%.\nBased on these two experiments, QAN significantly outperforms two baselines on both datasets. It also performs better than many state-of-the-art approaches and pushes top-1 matching rate 20.3% higher than previous best CNN+RNN [36] on PRID2011 and 10% on iLIDSVID. The performance gain is more significant on noisy iLIDS-VID dataset, which meets the expectation and proves QAN\u2019s ability to deal with images of poor quality."}, {"heading": "4.2.2 Dataset cross evaluation", "text": "To prevent our model from over-fitting the quality distribution of test set, we conduct dataset cross evaluation. We\nextract set representation of iLIDS-VID and PRID2011 directly using trained QAN without fine-tuning. The QAN representation is then evaluated for CMC scores. Table 3 and 4 shows the results of QAN and the two baselines. It can be found that the QAN is robust even in cross-dataset setting. It improves top-1 matching by 15.6% and 8.2% compared to the baselines. This result shows that the quality distribution learned from different datasets by QAN is able to generalize to other datasets."}, {"heading": "4.3. Unconstrained face verification", "text": "Datasets. For face verification, we train our base model on extended version of VGG Face dataset [24], in which we extend the identity number from 2.6K to 90K and im-\nage number from 2.6M to 5M. The model is evaluated on YouTube Face Database [35] and IARPA Janus Benchmark A (IJB-A) dataset. YouTube Face contains 3425 videos of 1595 identities. It is challenging in that most faces are blurred or has low resolution. IJB-A dataset contains 2042 videos of 500 people. Faces in IJB-A have large pose variance.\nEvaluation procedure. We follow the 1:1 protocol in both two benchmarks and evaluate results using receiver operating characteristic (ROC) curves. Area under curve (AUC) and accuracy are two important indicators of the ROC. The datasets are evaluated using 10-fold crossvalidation.\nTraining details. All faces in training and testing sets are detected and aligned by a multi-task region proposal network as described in [3]. Then we crop the face regions and resize them to 256\u00d7 224. After that, a convolutional neural networks with 256 \u00d7 224 inputs are used for face verification. It begins with a 2-stride convolution layer, followed by 4 basic blocks, while each block has three 1-stride convolution layers and one 2-stride pooling layers. After that, a fully connected layer is used to get the final feature. Quality generation branch is built on top of the third pooling layer, where the spatial size of middle representation response is 256 \u00d7 16 \u00d7 14. We pre-train the network supervised by classification signal and then train the whole QAN."}, {"heading": "4.3.1 Results on YouTube Face and IJB-A benchmark", "text": "10 \u22123\n10 \u22122\n10 \u22121\n10 0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFalse Positive Rate\nT ru\ne Po\nsi tiv\ne R\nat e\nQAN_pool2 Baseline(AvePool) Baseline(MinCos) DeepFace EigenPEP DDML(combine)\nFigure 7. Average ROC curves of different methods on YouTube Face Dataset 10\u22123 10\u22122 10\u22121 100\n0.8\n0.85\n0.9\n0.95\n1\nFalse Positive Rate\nT ru\ne Po\nsi tiv\ne R\nat e\nBaseline(MinCos) Baseline(AvePool) QAN@FC&Fix QAN@FC QAN@Input QAN@Pool1 QAN@Pool2 QAN@Pool3 QAN@Pool4\nFigure 8. ROC results for score generation part learned by different level of feature.\nOn YouTube Face dataset, it can be observed in Fig. 7 and Table 5 that the accuracy and AUC of our baselines are similar with the state-of-the-art methods such as FaceNet and NAN. Based on this baseline, QAN further reduces 15.6% error ratio. Under ROC evaluation metric, QAN surpasses NAN by 8% and DeepFace by 80% at 0.001 FPR (false positive rate), which ensembles 25 models.\nOn IJB-A dataset, QAN significantly outperforms the state-of-the-art algorithm NAN by 10.81% at 0.001 FPR, 4.5% at 0.01 FPR and 2.12% at FPR=0.1, as shown in Table 6. Compared with average pooling baseline, QAN reduces false negative rate at above three FPRs by 29.32%, 6.45% and 7.91%.\nOur experiments on the two tasks show that QAN is robust for set-to-set recognition. Especially on the point of low FPR, QAN can recall more matched samples with less errors."}, {"heading": "4.4. Quality by QAN VS. quality by human", "text": "There is no explicit supervision signals for the cascade score generation unit in training. So another problem arises: is it better to use human-defined scores instead of letting the network learn itself? In YouTube Face experiment, we replace the quality score Q(I) with volunteer-rated score and get the following result in Fig. 9, which is better than the two baselines but inferior to the result of original QAN. It shows that Q is similar with human thoughts, but more suitable for recognition. Quality score by human can also enhance the accuracy but is still worse than QAN\u2019s."}, {"heading": "4.5. Diagnosis experiments", "text": "Level of middle representation may affect the performance of QAN. We use YouTube Face to analyse this factor by comparing different configurations.\nIn the first configuration, the weight generation part is connected to the image. In the second to fifth configurations, weight generation part is set after four pooling layers in each block, respectively. In the sixth configuration, we\nconnect weight generation part to a fully connected layer. For the final configuration, we fix all parameters before the final fully connection layer in the sixth configuration and only update parameters in weight generation part, which is taken as the seventh structure. To minimize the influence by parameters\u2019 number, the total size of different models is restricted to the same by changing the channel number.\nResults are shown in Fig. 8. It can be found that the performance of QAN improves at the beginning and reaches the top accuracy at Pool3. The end-to-end training version of feature generation part with quality generation part performs better than that of fixed. So we can make the conclusion that 1) the middle level feature is better for QAN to learn and 2) significant improvement can be achieved by jointly training feature generation part and quality generation part."}, {"heading": "5. Conclusion and future work", "text": "In this paper we propose a Quality Aware Network (QAN) for set-to-set recognition. It automatically learns the concept of quality for each sample in a set without supervised signal and aggregates the most discriminative samples to generate set representation. We theoretically and experimentally demonstrate that the quality predicted by network is beneficial to set representation and better than human labelled.\nQAN can be seen as an attention model that pay attention to high quality elements in a image set. However, an image with poor quality may still has some discriminative regions. Considering this, our future work will explore a fine-grained quality aware network that pay attention to high quality regions instead of high quality images in a image set."}], "references": [{"title": "Approximate nearest subspace search", "author": ["Ronen Basri", "Tal Hassner", "Lihi Zelnik-Manor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Face recognition based on image sets", "author": ["Hakan Cevikalp", "Bill Triggs"], "venue": "In CVPR\u201910,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Supervised transformer network for efficient face detection", "author": ["Dong Chen", "Gang Hua", "Fang Wen", "Jian Sun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "An end-to-end system for unconstrained face verification with deep convolutional neural networks", "author": ["Jun-Cheng Chen", "Rajeev Ranjan", "Amit Kumar", "Ching-Hui Chen", "Vishal Patel", "Rama Chellappa"], "venue": "In ICCV Workshops,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Person re-identification by symmetrydriven accumulation of local features", "author": ["Michela Farenzena", "Loris Bazzani", "Alessandro Perina", "Vittorio Murino", "Marco Cristani"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Mdlface: Memorability augmented deep learning for video face recognition", "author": ["Gaurav Goswami", "Romil Bhardwaj", "Richa Singh", "Mayank Vatsa"], "venue": "In Biometrics (IJCB),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Pooling faces: template based face recognition with pooled face images", "author": ["Tal Hassner", "Iacopo Masi", "Jungyeon Kim", "Jongmoo Choi", "Shai Harel", "Prem Natarajan", "Gerard Medioni"], "venue": "In CVPR\u201916 Workshops,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Person re-identification by descriptive and discriminative classification", "author": ["Martin Hirzer", "Csaba Beleznai", "Peter M. Roth", "Horst Bischof"], "venue": "In Proc. Scandinavian Conference on Image Analysis (SCIA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Sparse approximated nearest points for image set classification", "author": ["Yiqun Hu", "Ajmal S Mian", "Robyn Owens"], "venue": "In CVPR\u201911,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Learning hierarchical representations for face verification with convolutional deep belief networks", "author": ["Gary B. Huang"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Gary B Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned- Miller"], "venue": "Technical report, Technical Report 07-49,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Projection metric learning on grassmann manifold with application to video based face recognition", "author": ["Zhiwu Huang", "Ruiping Wang", "Shiguang Shan", "Xilin Chen"], "venue": "In CVPR\u201915,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid", "Patrick P\u00e9rez"], "venue": "In CVPR\u201910,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Open source biometric recognition", "author": ["Joshua C Klontz", "Brendan F Klare", "Scott Klum", "Anubhav K Jain", "Mark J Burge"], "venue": "In Biometrics: Theory, Applications and Systems (BTAS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Large scale metric learning from equivalence constraints", "author": ["Martin Koestinger", "Martin Hirzer", "Paul Wohlhart", "Peter M Roth", "Horst Bischof"], "venue": "In CVPR\u201912,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["Svetlana Lazebnik", "Cordelia Schmid", "Jean Ponce"], "venue": "In CVPR\u201906,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Eigen-pep for video face recognition", "author": ["Haoxiang Li", "Gang Hua", "Xiaohui Shen", "Zhe Lin", "Jonathan Brandt"], "venue": "ACCV", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Top rank optimization in linear time", "author": ["Nan Li", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Deepreid: Deep filter pairing neural network for person re-identification", "author": ["Wei Li", "Rui Zhao", "Tong Xiao", "Xiaogang Wang"], "venue": "In ICCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Zhen Li", "Shiyu Chang", "Feng Liang", "Thomas Huang", "Liangliang Cao", "John Smith"], "venue": "In CVPR\u201913,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Multi-manifold deep metric learning for image set classification", "author": ["Jiwen Lu", "Gang Wang", "Weihong Deng", "Pierre Moulin", "Jie Zhou"], "venue": "In CVPR\u201915,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Local fisher discriminant analysis for pedestrian reidentification", "author": ["Sateesh Pedagadi", "James Orwell", "Sergio Velastin", "Boghos Boghossian"], "venue": "In ICCV\u201913,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Yi Sun", "Yuheng Chen", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In NIPS, pages 1988\u20131996,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Deeply learned face representations are sparse, selective, and robust", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In ICCV, pages 1701\u20131708,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Face search at scale: 80 million gallery", "author": ["Dayong Wang", "Charles Otto", "Anil K Jain"], "venue": "arXiv preprint arXiv:1507.07242,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Covariance discriminative learning: A natural and efficient approach to image set classification", "author": ["Ruiping Wang", "Huimin Guo", "Larry S Davis", "Qionghai Dai"], "venue": "In CVPR\u201912,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Person re-identification by video ranking", "author": ["Taiqing Wang", "Shaogang Gong", "Xiatian Zhu", "Shengjin Wang"], "venue": "ECCV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Person re-identification by discriminative selection in video", "author": ["Taiqing Wang", "Shaogang Gong", "Xiatian Zhu", "Shengjin Wang"], "venue": "ranking. 2016", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Face recognition in unconstrained videos with matched background similarity", "author": ["Lior Wolf", "Tal Hassner", "Itay Maoz"], "venue": "In CVPR\u201911,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Deep recurrent convolutional networks for video-based person re-identification: An end-to-end approach", "author": ["Lin Wu", "Chunhua Shen", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1606.01609,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Learning deep feature representations with domain guided dropout for person re-identification", "author": ["Tong Xiao", "Hongsheng Li", "Wanli Ouyang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1604.07528,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Neural aggregation network for video face recognition", "author": ["Jiaolong Yang", "Peiran Ren", "Dong Chen", "Fang Wen", "Hongdong Li", "Gang Hua"], "venue": "arXiv preprint arXiv:1603.05474,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Face recognition based on regularized nearest points between image sets", "author": ["Meng Yang", "Pengfei Zhu", "Luc Van Gool", "Lei Zhang"], "venue": "In Automatic Face and Gesture Recognition (FG),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Top-push video-based person re-identification", "author": ["Jinjie You", "Ancong Wu", "Xiang Li", "Wei-Shi Zheng"], "venue": "arXiv preprint arXiv:1604.08683,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Mars: A video benchmark for large-scale person re-identification", "author": ["Liang Zheng", "Zhi Bie", "Yifan Sun", "Jingdong Wang", "Chi Su", "Shengjin Wang", "Qi Tian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Person reidentification by probabilistic relative distance comparison", "author": ["Wei-Shi Zheng", "Shaogang Gong", "Tao Xiang"], "venue": "In CVPR,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "From point to set: Extend the learning of distance metrics", "author": ["Pengfei Zhu", "Lei Zhang", "Wangmeng Zuo", "David Zhang"], "venue": "In ICCV\u201913,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 24, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 25, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 27, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 67, "endOffset": 78}, {"referenceID": 18, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 67, "endOffset": 78}, {"referenceID": 39, "context": "Face verification [12, 26, 27, 28, 30] and person reidentification [5,6,20,42] have been well studied and widely used in computer vision applications such as financial identity authentication and video surveillance.", "startOffset": 67, "endOffset": 78}, {"referenceID": 18, "context": "In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance.", "startOffset": 29, "endOffset": 41}, {"referenceID": 34, "context": "In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance.", "startOffset": 29, "endOffset": 41}, {"referenceID": 38, "context": "In person re-identification, [20, 37, 41] use features generated by deep convolutional network and obtain state-of-the-art performance.", "startOffset": 29, "endOffset": 41}, {"referenceID": 9, "context": "[11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30].", "startOffset": 104, "endOffset": 112}, {"referenceID": 27, "context": "[11] uses convolutional Restricted Boltzmann Machine while deep convolutional neural network is used in [28, 30].", "startOffset": 104, "endOffset": 112}, {"referenceID": 23, "context": "Furthermore, [26, 29] use deeper convolutional network and achieved accuracy that even surpasses human performance.", "startOffset": 13, "endOffset": 21}, {"referenceID": 26, "context": "Furthermore, [26, 29] use deeper convolutional network and achieved accuracy that even surpasses human performance.", "startOffset": 13, "endOffset": 21}, {"referenceID": 10, "context": "The accuracy achieved by deep learning on image-based face verification benchmark LFW [12] has been promoted to 99.", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 86, "endOffset": 92}, {"referenceID": 11, "context": "The first approach takes image set as a convex hull [2], affine hull [10] or subspace [1,13].", "startOffset": 86, "endOffset": 92}, {"referenceID": 20, "context": "Under these settings, samples in a set distribute in a Hilbert space or Grassmann mainfold so that this issue can be formulated as a metric learning problem [23, 39].", "startOffset": 157, "endOffset": 165}, {"referenceID": 36, "context": "Under these settings, samples in a set distribute in a Hilbert space or Grassmann mainfold so that this issue can be formulated as a metric learning problem [23, 39].", "startOffset": 157, "endOffset": 165}, {"referenceID": 15, "context": "The most famous approach in this kind is the Bag of features [17], which uses histogram to represent the whole set for feature aggregation.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Another classical work is vector of locally aggregated descriptors (VLAD) [14], which aggregates all local descriptors from all samples.", "startOffset": 74, "endOffset": 78}, {"referenceID": 33, "context": "Temporal max/average pooling is used in [36] to integrate all frames\u2019 features generated by recurrent convolutional network.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "The 2nd order statistics is used in [32, 43] in assuming that samples follow Gaussian distribution.", "startOffset": 36, "endOffset": 44}, {"referenceID": 40, "context": "The 2nd order statistics is used in [32, 43] in assuming that samples follow Gaussian distribution.", "startOffset": 36, "endOffset": 44}, {"referenceID": 6, "context": "In [8], original faces in a set are classified into 20 bins based on their pose and quality.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "[38] uses attention mechanism to summarize several sample points to a single aggregated point.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Different from recent works which learn aggregation based on fixed feature [38] or image [8], the QAN learns feature representation and aggregation simultaneously.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Different from recent works which learn aggregation based on fixed feature [38] or image [8], the QAN learns feature representation and aggregation simultaneously.", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "[7] proposed a similar quality aware module named \u201cmemorability based frame selection\u201d which takes \u201cvisual entropy\u201d to be the score of a frame.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Lveri = \u2016Ra(Sa)\u2212Ra(Sp)\u2016\u2212 \u2016Ra(Sa)\u2212Ra(Sn)\u2016 + \u03b4 (4) The loss function above is referred as Triplet Loss in previous works [26].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "Then the 6 aligned scores of each image are averaged and finally normalized to [0, 1] to get the final quality score from human.", "startOffset": 79, "endOffset": 85}, {"referenceID": 7, "context": "Experiments are conducted on PRID2011 [9] and iLiDS-VID [33] datasets.", "startOffset": 38, "endOffset": 41}, {"referenceID": 30, "context": "Experiments are conducted on PRID2011 [9] and iLiDS-VID [33] datasets.", "startOffset": 56, "endOffset": 60}, {"referenceID": 37, "context": "In the first setting, we follow the state-of-the-art method described in [40] and [34].", "startOffset": 73, "endOffset": 77}, {"referenceID": 31, "context": "In the first setting, we follow the state-of-the-art method described in [40] and [34].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "8 CNN+RNN [36] 70 90 95 97 STFV3D [22] 42.", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "6 TDL [40] 56.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "6 eSDC [34] 48.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "4 DVR [34] 40.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "2 LFDA [25] 43.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "9 KISSME [16] 34.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "0 LADF [21] 47.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "1 TopRank [19] 31.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "3% higher than previous best CNN+RNN [36] on PRID2011 and 10% on iLIDSVID.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "9 CNN+RNN [36] 58 84 91 96 STFV3D [22] 37.", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "9 TDL [40] 56.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "3 eSDC [34] 41.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "1 DVR [34] 39.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "0 LFDA [25] 32.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "6 KISSME [16] 36.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "1 LADF [21] 39.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "8 TopRank [19] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "6 CNN+RNN [36] 28 57 69 81", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "For face verification, we train our base model on extended version of VGG Face dataset [24], in which we extend the identity number from 2.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "The model is evaluated on YouTube Face Database [35] and IARPA Janus Benchmark A (IJB-A) dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "All faces in training and testing sets are detected and aligned by a multi-task region proposal network as described in [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 35, "context": "06% NAN [38] 95.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "7% FaceNet [26] 95.", "startOffset": 11, "endOffset": 15}, {"referenceID": 26, "context": "39% DeepID2+ [29] 93.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "2% DeepFace-single [30] 91.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "3% EigenPEP [18] 84.", "startOffset": 12, "endOffset": 16}, {"referenceID": 35, "context": "67% NAN [38] 78.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "5% DCNN+metric [4] 78.", "startOffset": 15, "endOffset": 18}, {"referenceID": 28, "context": "1% LSFS [31] 51.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "3% OpenBR [15] 10.", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at GitHub1", "creator": "LaTeX with hyperref package"}}}