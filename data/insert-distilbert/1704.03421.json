{"id": "1704.03421", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Efficient Large Scale Clustering based on data partitioning", "abstract": "clustering techniques are very weakly attractive for extracting characteristics and identifying patterns in datasets. \" however, their application to very large spatial datasets presents quite numerous spatial challenges such as high - dimensionality data, heterogeneity, and high complexity of some algorithms. for instance, some algorithms may have linear complexity but they require the domain knowledge parameters in order to determine their input parameters. distributed clustering techniques constitute a very good alternative to calculating the big dimensional data challenges ( e. g., volume, tree variety, veracity, and velocity ). usually these techniques consist of two spatial phases. the first phase generates local models or patterns and the second one efficiently tends to aggregate the local results to obtain global models. while the first phase vectors can be executed in parallel on each site and, therefore, uniquely efficient, the hybrid aggregation phase is complex, time consuming and may produce incorrect and ambiguous global clusters and otherwise therefore incorrect scaling models. in this paper we propose a new distributed clustering communication approach to deal efficiently with both phases ; generation of local results and generation distortion of global models by aggregation. for the same first phase, our approach is capable of analysing the datasets located in each site using different relational clustering techniques. the aggregation phase is designed in such a way that the final clusters specifications are compact and accurate looking while the overall process is efficient in time operations and memory allocation. for the evaluation, we use two well - known clustering algorithms ; k - means explorer and pure dbscan. one of the key innovative outputs of this distributed clustering computational technique is that the number of global clusters is dynamic ; no need existed to be fixed manually in advance. experimental setup results show that the approach cost is scalable and produces high quality modeling results.", "histories": [["v1", "Tue, 11 Apr 2017 17:05:01 GMT  (1236kb)", "http://arxiv.org/abs/1704.03421v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.LG", "authors": ["malika bendechache", "nhien-an le-khac", "m-tahar kechadi"], "accepted": false, "id": "1704.03421"}, "pdf": {"name": "1704.03421.pdf", "metadata": {"source": "CRF", "title": "Efficient Large Scale Clustering based on Data Partitioning", "authors": ["Malika Bendechache", "Nhien-An Le-Khac"], "emails": ["malika.bendechache@ucdconnect.ie", "an.lekhac@ucd.ie", "tahar.kechadi@ucd.ie"], "sections": [{"heading": null, "text": "The rest of the paper is organised as follows: In the models. In order to take advantage of the mined knowledge at\n1  \u00a0\n \u00a0\nKeywords\u2014Big Data, spatial data, clustering, distributed mining, data analysis, k-means, DBSCAN.\n \u00a0 I. INTRODUCTION  \u00a0\nCurrently, one of the most critical data challenges which has a massive economic need is how to efficiently mine and manage all the data we have collected. This is even more critical when the collected data is located on different sites, and are owned by different organisations [1]. This led to the development of distributed data mining (DDM) techniques to deal with huge, multi-dimensional and heterogeneous datasets, which are distributed over a large number of nodes. Existing DDM techniques are based on performing partial analysis on local data at individual sites followed by the generation of global models by aggregating these local results. These two steps are not independent since naive approaches to local analysis may produce incorrect and ambiguous global data\ndifferent locations, DDM should have a view of the knowledge that not only facilitates their integration, but also minimises the effect of the local results on the global models. Briefly, an efficient management of distributed knowledge is one of the key factors affecting the outputs of these techniques [2], [3], [4], [5]. Moreover, the data that is collected and stored in different locations using different instruments may have different formats and features. Traditional, centralised data mining techniques have not considered all the issues of datadriven applications, such as scalability in both response time and accuracy of solutions, distribution, and heterogeneity [6]. Some DDM approaches are based on ensemble learning, which uses various techniques to aggregate the results [7], among the most cited in the literature: majority voting, weighted voting, and stacking [8], [9].  \u00a0\nDDM is more appropriate for large scale distributed platforms, where datasets are often geographically distributed and owned by different organisations. Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years. However, only a few researches concern distributed clustering for analysing large, heterogeneous and distributed datasets. Recent researches [15], [16], [17] have proposed distributed clustering approaches based on the same 2-step process: perform partial analysis on local data at individual sites and then aggregate them to obtain global results. In this paper, we propose a distributed clustering approach based on the same 2-step process, however, it reduces significantly the amount of information exchanged during the aggregation phase, and generates automatically the correct number of clusters. A case study of an efficient aggregation phase has been developed on spatial datasets and proven to be very efficient; the data exchanged is reduced by more than 98% of the original datasets [18].  \u00a0\nThe approach can use any clustering algorithm to perform the analysis on local datasets. As can be seen in the following sections, we tested the approach with two wellknown centroid-based and density-based clustering algorithms (K-Means and DBSCAN, respectively), and the results are of very high quality. More importantly, this study shows how importance of the local mining algorithms, as the local clusters accuracy affects heavily the quality of the final models.\n2  \u00a0\nnext section we will give an overview of the state-of-theart for distributed data mining and discuss the limitations of traditional techniques. Then we will present the proposed distributed framework and its concepts in Section III. In Section IV, we evaluated the distributed approach using two wellknown algorithms; K-Means and DBSCAN. We discuss more experimental results showing the quality of our algorithm\u2019s results in Section V. Finally, we conclude in Section VI.\n \u00a0 II. RELATED WORK  \u00a0\nDistributed Data Mining (DDM) is a line of research that has attracted much interest in recent years [19]. DDM was developed because of the need to process data that can be very large or geographically distributed across multiple sites [20]. This has two advantages: first, a distributed system has enough power to analyse the data within a reasonable time frame. Second, it would be very advantageous to process data on their respective sites to avoid the transfer of large volumes of data to a central site to avoid heavy communications, network bottlenecks, etc.\n \u00a0\nDDM techniques can be divided into two categories based on the targeted architectures of computing platforms [21]. The first, based on parallelism, uses traditional dedicated and parallel machines with tools for communications between processors. These machines are generally called super-computers and are very expensive. The second category targets a network of autonomous machines. These are called distributed systems, and are characterised by a distributed communication network connecting low-speed machines that can be of different architectures, but they are very abundant [22]. The main goal of the second category of techniques is to distribute the work among the system nodes and try to minimise the response time of the whole application. Some of these techniques have already been developed and implemented in [23], [24].\n \u00a0\nHowever, the traditional DDM methods are not always effective, as they suffer from the problem of scaling. This has led to the development of techniques that rely on ensemble learning [25], [26]. These new techniques are very promising. Integrating ensemble learning methods in DDM, will allow to deal with the scalability problem. One solution to deal with large scale data is to use parallelism, but this is very expensive in terms of communications and processing power. Another solution is to reduce the size of training sets (sampling). Each system node generates a separate sample. These samples will be analysed using a single global algorithm [27], [28]. However, this technique has a disadvantage that the sampling depends on the transfer time which may impact on the quality of the samples [29].\n \u00a0\nClustering algorithms can be divided into two main categories, namely partitioning and hierarchical. Different elaborated taxonomies of existing clustering algorithms are given in the literature. Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35]. These algorithms are further classified into two sub-categories. The first consists of methods requiring multiple rounds of message passing. They require a significant amount of synchronisations. The second subcategory consists of methods that build local clustering models and send them to a central site to build global models [18]. In\n[30] and [34], message-passing versions of the widely used K-Means algorithm were proposed. In [31] and [35], the authors dealt with the parallelisation of the DBSCAN densitybased clustering algorithm. In [32] a parallel message passing version of the BIRCH algorithm was presented. A parallel version of a hierarchical clustering algorithm, called MPC for Message Passing Clustering, which is especially dedicated to Microarray data was introduced in [33]. Most of the parallel approaches need either multiple synchronisation constraints between processes or a global view of the dataset, or both [17].  \u00a0\nAnother approach presented in [17] also applied a merging of local models to create the global models. Current approaches only focus on either merging local models or mining a set of local models to build global ones. If the local models cannot effectively represent local datasets then global models accuracy will be very poor [18]. Both partitioning and hierarchical categories suffer from some drawbacks. For the partitioning class, K-Means algorithm needs the number of clusters fixed in advance, while in the majority of cases K is not known. Furthermore, hierarchical clustering algorithms have overcome this limit: they do not need to provide the number of clusters as an input parameter, but they must define the stopping conditions for clustering decomposition, which is not an easy task.  \u00a0 III. DYNAM IC DISTRIBUTED CLUSTERING  \u00a0\nWe first describe the proposed Dynamic Distributed Clustering (DDC) model, where the local models are based on the boundaries of clusters. We also present an evaluation of the approach with different local clustering techniques including centroid-based (K-Means) and density-based (DBSCAN) techniques.  \u00a0\nbetter that the ones generated by both BIRCH and CURE. As\n3  \u00a0\nThe DDC approach includes two main steps. In the first step, as usual, we cluster the datasets located on each node of the system and select good local representatives. This phase is executed in parallel without communications between the nodes. In this phase we can reach a super speed-up. The next phase, however, collects the local models from each node and affects them to some special nodes in the system called leaders. The leaders are elected according to some characteristics such as their capacity, processing power, connectivity, etc. The leaders are responsible for merging and regenerating the data objects based on the local cluster representatives. The purpose of this step is to improve the quality of the global clusters, as usually the local clusters do not contain enough important information.\n \u00a0 A. Local Models  \u00a0\nThe local clusters are highly dependent on the clustering techniques used by their corresponding nodes. For instance, for spatial datasets, the shape of a cluster is usually dictated by the technique used to obtain them. Moreover, this is not an issue for the first phase, as the accuracy of a cluster affects only the local results of a given node. However, the second phase requires sending and receiving all local clusters to the leaders. As the whole data is very large, this operation will saturate very quickly the network. So, we must avoid sending all the original data through the network. The key idea behind the DDC approach is to send only the cluster\u2019s representatives, which constitute between 1% and 2% of the total size of the data. The cluster representatives consist of the internal data representatives plus the boundary points of the cluster.\n \u00a0\nThere are many existing data reduction techniques in the literature. Many of them are focusing only on the dataset size i.e., they try to reduce the storage capacity without paying attention to the knowledge contained in the data. In [36], an efficient reduction technique has been proposed; it is based on density-based clustering algorithms. Each cluster is represented by a set of carefully selected data-points, called representatives. However, selecting representatives is still a challenge in terms of quality and size [15], [18].\n \u00a0\nThe best way to represent a spatial cluster is by its shape and density. The shape of a cluster is represented by its boundary points (called contour) (see Figure 1). Many algorithms for extracting the boundaries from a cluster can be found in the literature [37], [38], [39], [40], [41]. We used the algorithm proposed in [42], which is based on triangulation to generate the cluster boundaries. It is an efficient algorithm for constructing non-convex boundaries. The algorithm is able to accurately characterise the shape of a wide range of different point distributions and densities with a reasonable complexity of O(n log n).\n \u00a0 B. Global Models  \u00a0\nThe global models (patterns) are generated during the second phase of the DDC. This phase is also executed in a distributed fashion but, unlike the first phase, it has communications overheads. This phase consists of two main steps, which can be repeated until all the global clusters were generated. First, each leader collects the local clusters of its neighbours. Second, the leaders will merge the local clusters\nusing the overlay technique. The process of merging clusters will continue until we reach the root node. The root node will contain the global clusters (see Figure 1).  \u00a0\nNote that, this phase can be executed by a clustering algorithm, which can be the same as in the first phase or completely different one. This approach belongs to the category of hierarchical clustering.  \u00a0\nAs mentioned above, during the second phase, communicating the local clusters to the leaders may generate a huge overhead. Therefore, the objective is to minimise the data communication and computational time, while getting accurate global results. In DDC we only exchange the boundaries of the clusters, instead of exchanging the whole clusters between the system nodes.  \u00a0\nIn the following, we summarise the steps of the DDC approach for spatial data. The nodes of the distributed computing system are organised following a tree topology.  \u00a0\n1) Each node is allocated a dataset representing a portion of the scene or of the overall dataset. 2) Each leaf node executes a local clustering algorithm with its own input parameters. 3) Each node shares its clusters with its neighbours in order to form larger clusters using the overlay technique. 4) The leader nodes contain the results of their groups. 5) Repeat 3 and 4 until all global clusters were gener-\nated.  \u00a0\nNote that the DDC approach suits very well the MapReduce framework, which is widely used in cloud computing systems.  \u00a0 IV. DDC EVALUATION AND VALIDATION  \u00a0\nIn order to evaluate the performance of the DDC approach, we use different local clustering algorithms. In this paper we use a centroid-based algorithm (K-Means) and a density-based Algorithm (DBSCAN).  \u00a0 A. DDC-K-Means  \u00a0\nFollowing the general structure of the approach described above, the DDC with K-Means (DDC-K-Means) is characterised by the fact that in the first phase, called the parallel phase, each node Ni of the system executes the K-Means algorithm on its local dataset to produce Li local clusters and calculate their contours. The rest of the process is the same as described above. In other words, the second phase consists of exchanging the contours located in each node with its neighbourhood nodes. Each leader attempts to merge overlapping contours of its group. Therefore, each leader generates new contours (new clusters). The merge procedure will continue until there is no overlapping contours.  \u00a0\nIt has been shown in [43] that DDC-K-Means dynamically determines the number of the clusters without a priori knowledge about the data or an estimation process of the number of the clusters. DDC-K-Means was also compared to two wellknown clustering algorithms: BIRCH and CURE. The results showed that the quality of the DDC-K-Means\u2019 clusters is much\n4  \u00a0\nexpected, this approach runs much faster than the two other algorithms; BIRCH and CURE.\n \u00a0\nTo summarise, DDC-K-Means does not need the number of global clusters to be given as an input. It is calculated dynamically. Moreover, each local clustering Li with K-Means needs Ki as a parameter, which is not necessarily the exact K for that local clustering. Let K\u0303i be the exact number of local clusters in the node Ni , all it is required is to set Ki such that Ki > K\u0303i . This is much simpler than giving Ki , especially when we do not have enough knowledge about the local dataset characteristics. Nevertheless, it is indeed better to set Ki as close as possible to K\u0303i in order to reduce the processing time in calculating the contours and also merging procedure.\n \u00a0\nFigure 2 shows a comparative study between DDC-K-\nM inP ts. Third, it is insensitive to the input order of points in the dataset. All these features are very important to any clustering algorithm.  \u00a0\n2) DBSCAN Complexity: DBSCAN visits each point of the dataset, possibly multiple times (e.g., as candidates to different clusters). For practical considerations, however, the time complexity is mostly governed by the number of regionQuery invocations. DBSCAN executes exactly one such query for each point, and if an indexing structure is used that executes a neighbourhood query in O(log n), an overall average complexity of O(n log n) is obtained if the parameter Eps is chosen in a meaningful way, (i.e., such that on average only O(log n) points are returned). Without the use of an accelerating index structure, or on degenerated data (e.g., all points within a distance less than Eps), the worst case run time complexity remains O(n2 ). The distance matrix of size\nMeans and two well-known algorithms; BIRCH and CURE. The experiments use five datasets (T1 , T2 , T3 , T4 , T5 ) described O((n2 \u2212 n/2)) can be materialised to avoid distance re2  \u00a0\nin Table I. Note that T5 is the same dataset as T4 for which we removed the noise. Each color represents a separate cluster.\n \u00a0\nAs we can see, DDC-K-Means successfully generates the final clusters for the first three datasets (T1 , T2 and T3 ), whereas BIRCH and CURE fail to generate the expected clusters for all the datasets (T1 , T2 , T3 , T4 and T5 ). However, DDC-K-Means fails to find good clusters for the two last datasets (T4 and T5 ), this is due to the fact that the K-Means algorithm tends to work with convex shape only, because it is based on the centroid principle to generate clusters. Moreover, we can also notice that the results of DDC-K-Means are even worse with dataset which contains noise (T5 ). In fact it returns the whole dataset with the noise as one final cluster for each dataset (see Figure 2). This is because K-Means does not deal with noise.\n \u00a0 B. DDC with DBSCAN  \u00a0\nWhile, the DDC-K-Means performs much better than some well-known clustering algorithms on spatial datasets, it still can not deal with all kinds of datasets; mainly with non-convex shapes. In addition, DDC-K-Means is very sensitive to noise. Therefore, instead of K-Means, we use another clustering algorithm for spatial datasets, which is DBSCAN. DBSCAN is summarised below.\n \u00a0\n1) DBSCAN: DBSCAN (Density-Based spatial Clustering of Applications with Noise) is a well-known density based clustering algorithm capable of discovering clusters with arbitrary shapes and eliminating noisy data [44]. Briefly, DBSCAN clustering algorithm has two main parameters: the radius Eps and minimum points M inP ts. For a given point p, the Eps neighbourhood of p is the set of all the points around p within distance Eps. If the number of points in the Eps neighbourhood of p is smaller than M inP ts, then all the points in this set, together with p, belong to the same cluster. More details can be found in [44].\n \u00a0\nCompared with other popular clustering methods such as K-Means [45], BIRCH [46], and STING [47], DBSCAN has several key features. First, it groups data into clusters with arbitrary shapes. Second, it does not require the number of the clusters to be given as an input. The number of clusters is determined by the nature of the data and the values of Eps and\ncomputations, but this needs O(n ) of memory, whereas a nonmatrix based implementation of DBSCAN only needs O(n) of memory space.  \u00a0\n3) DDC-DBSCAN Algorithm: The approach remains the same (as explained in Section III), the only difference is at local level (See Figure 1). Where, instead of using KMeans for processing local clusters, we use DBSCAN. Each node (ni ) executes DBSCAN on its local dataset to produce Ki local clusters. Once all the local clusters are determined, we calculate their contours. These contours will be used as representatives of their corresponding clusters.  \u00a0\nThe second phase consists of exchanging the contours located in each node with its neighbourhood nodes. This will allow us to identify overlapping contours (clusters). Each leader attempts to merge overlapping contours of its group. Therefore, each leader generates new contours (new clusters). We repeat the second and third steps till we reach the root node. The sub-clusters aggregation is done following a tree structure and the global results are located in the top level of the tree (root node). The algorithm pseudo code is given in Algorithm 1.  \u00a0\nFigure 3 illustrates an example of DDC-DBSCAN. Assume that the distributed computing platform contains five Nodes (N = 5). Each Node executes DBSCAN algorithm with its local parameters (Epsi , M inP tsi ) on its local dataset. As it can be seen in Figure 3 the new approach returned exactly the right number of clusters and their shapes. The approach is insensitive to the way the original data was distributed among the nodes. It is also insensitive to noise and outliers. As we can see, although each node executed DBSCAN locally with different parameters. The global final clusters were correct even on the noisy dataset (T5 ) (See Figure 3).\n5  \u00a0\n \u00a0  \u00a0\nAlgorithm 1: DDC with DBSCAN. input : Xi : Dataset Fragment, Epsi : Distance Epsi for N odei , M inP tsi : minimum points contain clusters generated by N odei , D: tree degree, Li : Local clusters generated by N odei output: Kg : Global Clusters (global results)  \u00a0 level = treeheight; 1) DBSCAN(Xi . Epsi , M inP tsi ); // N odei executes DBSCAN locally. 2) Contour(Li ); // N odei executes the Contour algorithm to generate the boundary of each local cluster. 3) N odei joins a group G of D elements; // N odei joins its neighbourhood 4) Compare cluster of N odei to other node\u2019s clusters in the same group; // look for overlapping between clusters. 5) j = ElectLeaderNode(); // Elect a node which will merge the overlapping clusters. if i <> j then Send (contour i, j); else if level > 0 then level - - ; Repeat 3, 4, and 5 until level=0; else return (Kg : N odei \u2019 clusters); V. EXPERIMENTAL RESULTS  \u00a0 In this section, we study the performance of the DDCDBSCAN approach and demonstrate its effectiveness compared to BIRCH, CURE and DDC-K-Means. We choose these algorithms because either they are in the same category as the proposed technique, such as BIRCH which belongs to hierarchical clustering category, or have an efficient optimisation approach, such as CURE.  \u00a0 BIRCH: We used the BIRCH implementation provided in [46]. It performs a pre- clustering and then uses a centroidbased hierarchical clustering algorithm. Note that the time and space complexity of this approach is quadratic to the number of points after pre-clustering. We set its parameters to the default values suggested in [46].  \u00a0 CURE: We used the implementation of CURE provided in [48]. The algorithm uses representative points with shrinking towards the mean. As described in [48], when two clusters are merged in each step of the algorithm, representative points for the new merged cluster are selected from the ones of the two original clusters rather than all the points in the merged clusters.  \u00a0 A. Experiments  \u00a0 We run experiments with different datasets. We used six types of datasets with different shapes and sizes. The first three datasets (T1 , T2 , and T3 ) are the same datasets used to evaluate DDC-K-Means. The last three datasets (T4 , T5 , and T6 ) are very well-known benchmarks to evaluate density-based clustering algorithms. All the six datasets are summarised in Table I. The number of points and clusters in each dataset is also given. These six datasets contain a set of shapes or patterns which are not easy to extract with traditional techniques.  \u00a0  \u00a0\nTABLE I: The datasets used to test the algorithms.\n \u00a0 Type Dataset Description #Points #Clusters  \u00a0 Convex T1 Big oval (egg shape) 14,000 5 T2 4 small circles and 2 small circles linked 17,080 5  \u00a0 T3 2 small circles, 1 big circle and 2 linked ovals  \u00a0 30,350  \u00a0 4  \u00a0 Non-Convex with Noise T4 Different shapes including noise 8,000 6  \u00a0 T5 Different shapes, with some clusters surrounded by others  \u00a0 10,000  \u00a0 9 T6 Letters with noise 8,000 6  \u00a0  \u00a0 B. Quality of Clustering  \u00a0 We run the four algorithms on the six datasets in order to evaluate the quality of their final clusters. In the case of the DDC approach we took a system that contains five nodes, therefore, the results shown are the aggregation of the five local clustering or the global clusters. Figure 4 shows the returned clusters by each of the four algorithms for convex shapes of the clusters (T1 , T2 , and T3 ) and Figure 5 shows the clusters returned for non-convex shapes of the clusters with noise (T4 , T5 , and T6 ). We use different colours to show the clusters returned by each algorithm.  \u00a0  \u00a0\nFig. 3: Example of DDC-DBSCAN execution. 6\n7  \u00a0\n \u00a0  \u00a0  \u00a0\nFrom the results shown in Figure 4, as expected, since BIRCH cannot find all the clusters correctly. It splits the larger cluster while merging the others. In contrast, CURE generates correctly the majority of the final clusters but it still fails to discover all the clusters. Whereas both DDC-K-Means and DDC-DBSCAN algorithms successfully generate all the clusters with the default parameter settings.\n \u00a0\nFigure 5 shows the clusters generated for the datasets (T4 , T5 , and T6 ). As expected, again BIRCH could not find correct clusters; it tends to work better with convex shapes of the clusters. In addition, BIRCH does not deal with noise. The results of CURE are worse and it is not able to extract clusters with non-convex shapes. We can also see that CURE does not deal with noise. DDC-K-Means fails to find the correct final results. In fact it returns the whole original dataset as one final cluster for each dataset (T4 , T5 , and T6 ) (including the noise). This confirms that the DDC technique is sensitive to the type\nof the algorithm chosen for the first phase. Because the second phase deals only with the merging of the local clusters whether they are correct or not. This issue is corrected by the DDCDBSCAN, as it is well suited for non-convex shapes of the clusters and also for eliminating the noise and outliers. In fact, it generates good final clusters in datasets that have significant amount of noise.  \u00a0\nAs a final observation, these results prove that the DDC framework is very efficient with regard to the accuracy of its results. The only issue is to choose a good clustering algorithm for the first phase. This can be done by exploring the initial datasets along with the question to be answered and choose a clustering algorithm accordingly.  \u00a0\nMoreover, as for the DDC-K-Means, DDC-DBSCAN is dynamic (the correct number of clusters is returned automatically) and efficient (the approach is distributed and minimises the communications).\n \u00a0  \u00a0  \u00a0\nC. Speed-up  \u00a0\nThe goal here is to study the execution time of the four algorithms and demonstrate the impact of using a parallel and distributed architecture to deal with the limited capacity of a centralised system.\n \u00a0\nAs mentioned in Section IV-B2, the execution time for the DDC-DBSCAN algorithm can be generated in two cases. The first case is to include the time required to generate the distance matrix calculation. The second case is to suppose that the distance matrix has already been generated. The reason for this is that the distance matrix is calculated only once.\n \u00a0  \u00a0\nniques on different datasets. Note that the execution times do not include the time for post-processing since these are the same for the four algorithms.\nAs mentioned in Section IV-B2, Table II confirmed the fact that the distance matrix calculation in DBSCAN is very significant. Moreover, DDC-DBSCAN\u2019s execution time is much lower than CUREs execution times across the six datasets. Table II shows also that the DDC-K-Means is very quick which is in line with its polynomial computational complexity. BIRCH is also very fast, however, the quality of its results are not good, it failed in finding the correct clusters across all the six datasets.\nThe DDC-DBSCAN is a bit slower that DDC-K-Means, but it returns high quality results for all the tested benchmarks, much better that DDC-K-Means, which has reasonably good results for convex cluster shapes and very bad results for non-convex cluster shapes. The overall results confirm that the DDC-DBSCAN clustering techniques compares favourably to all the tested algorithms for the combined performance measures (quality of the results and response time or time complexity).  \u00a0 D. Scalability\nThe goal here is to determine the effects of the number of nodes in the system on the execution times. The dataset\n9  \u00a0\ncontains 50, 000 data points. Figure 6 shows the execution time against the number of nodes (x axis is in log2 ) in the system. As one can see, DDC-DBSCAN took only few seconds (including the matrix computation\u2019s time) to cluster 50, 000 data points in a distributed system that contains up to 100 nodes, the algorithm took even less time when we exclude the matrix computation\u2019s time. Thus, the algorithm can comfortably handle high-dimensional data because of its low complexity.\n \u00a0  \u00a0  \u00a0\nVI. CONCLUSION  \u00a0\nIn this paper, we proposed an efficient and flexible distributed clustering framework that can work with existing data mining algorithms. The framework has been tested on spatial datsests using the K-Means and DBSCAN algorithms. The distributed clustering approach is moreover dynamic, for spatial datasets, as it does not need to give the number of correct clusters in advance (as an input parameter). This basically solves one of major shortcomings of K-Means or DBSCAN. The proposed Distributed Dynamic Clustering (DDC) approach has two main phases: the fully parallel phase where each node of the system calculates its own local clusters based on its portion of the entire dataset. There is no communications during this phase, it takes full advantage of task parallelism paradigm. The second phase is also distributed, but it generates some communications between the nodes. However, the overhead due these communications has minimised by using a new concept of cluster representatives. Each cluster is represented by its contour and its density, which count for about 1% of cluster size in general. This DDC framework can easily be implemented using MapReduce mechanism.\n \u00a0\nNote that, as the first phase is fully parallel, each node can use its own clustering algorithm that suits well its local dataset. However, the quality of the DDC final results depends heavily on the local clustering used during the first phase. Therefore, the issue of exploring the original dataset before choosing local clustering algorithms remains the key hurdle of all the data mining techniques.\nThe DDC approach was tested using various benchmarks. The benchmarks were chosen in such a way to reflect all the difficulties of clusters extraction. These difficulties include the shapes of the clusters (convex and non-convex), the data volume, and the computational complexity. Experimental results showed that the approach is very efficient and can deal with various situations (various shapes, densities, size, etc.).  \u00a0\nAs future work, we will study in more details the approach scalability on very large datasets, and we will explore other models for combining neighbouring clusters. We will extend the framework to non-spatial datasets. We will also look at the problem of the data and communications reduction during phase two.  \u00a0 REFERENCES  \u00a0\n[1] J. Han and M. Kamber, Data Mining: Concepts and Techniques, 2nd ed. Morgan Kaufmann Publisher, 2006.\n[2] L. Aouad, N.-A. Le-Khac, and M.-T. Kechadi, \u201cweight clustering technique for distributed data mining applications,\u201d LNCS on advances in data mining \u2013 theoretical aspects and applications, vol. 4597, pp. 120\u2013134, 2007.\n[3] L.Aouad, N.-A. Le-Khac, and M.-T. Kechadi, \u201cGrid-based approaches for distributed data mining applications,\u201d Journal of Algorithms & Computational Technology, vol. 3, no. 4, pp. 517\u2013534, 2009.\n[4] L. Aouad, N.-A. Le-Khac, and M.-T. Kechadi, \u201cPerformance study of distributed apriori-like frequent itemsets mining,\u201d Knowledge and Information Systems, vol. 23, no. 1, pp. 55\u201372, 2010.\n[5] M. Whelan, N.-A. L. Khac, and M.-T. Kechadi, \u201cPerformance evaluation of a density-based clustering method for reducing very large spatio-temporal dataset,\u201d in in Proc. of International Conference on Information and Knowledge Engineering, July 18-21 2011.\n[6] M. Bertolotto, S. Di Martino, F. Ferrucci, and M.-T. Kechadi, \u201cTowards a framework for mining and analysing spatio-temporal datasets,\u201d International Journal of Geographical Information Science, vol. 21, no. 8, pp. 895\u2013906, 2007.\n[7] N.-A. Le-Khac, L. Aouad, and M.-T. Kechadi, \u201cKnowledge map layer for distributed data mining,\u201d Journal of ISAST Transactions on Intelligent Systems, vol. 1, no. 1, 2008.\n[8] P. Chan and S. Stolfo, \u201cA comparative evaluation of voting and metalearning on partitioned data,\u201d in 12th International Conference on Machine Learning, 1995, pp. 90\u201398.\n[9] C. Reeves, Modern heuristic techniques for combinatorial problems. John Wiley & Sons, Inc. New York, NY, USA, 1993.\n[10] T. G. Dietterich, \u201cAn experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization,\u201d Machine Learning, vol. 40, no. 2, pp. 139\u2013157, 2000.\n[11] H. Kargupta and P. Chan, Advances in distributed and Parallel Knowledge Discovery. MIT Press Cambridge, MA, USA, October 2000, vol. 5.\n[12] J.-M. Adamo, Data mining for association rules and sequential patterns: sequential and parallel algorithms. Springer Science & Business Media, 2012.\n[13] N.-A. Le-Khac, L. Aouad, and M.-T. Kechadi, \u201cPerformance study of distributed apriori-like frequent itemsets mining,\u201d Knowledge and Information Systems, vol. 23, no. 1, pp. 55\u201372, 2010.\n[14] N. A. Le Khac, L. M. Aouad, and M.-T. Kechadi, Emergent Web Intelligence: Advanced Semantic Technologies. London: Springer London, 2010, ch. Toward Distributed Knowledge Discovery onGrid Systems, pp. 213\u2013243.\n[15] E. Januzaj, H.-P. Kriegel, and M. Pfeifle, Advances in Database Technology - EDBT 2004: 9th International Conference on Extending Database Technology, Heraklion, Crete, Greece, March 14-18, 2004. Berlin, Heidelberg: Springer Berlin Heidelberg, 2004, ch. DBDC: Density Based Distributed Clustering, pp. 88\u2013105.\n10  \u00a0\n[16] N. Le-Khac, L.Aouad., and M.-T. Kechadi, Data Management. Data, Data Everywhere: 24th British National Conference on Databases, BNCOD 24, Glasgow, UK, July 3-5, 2007. Proceedings. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, ch. A New Approach for Distributed Density Based Clustering on Grid Platform, pp. 247\u2013 258.\n[17] L. Aouad, N.-A. L. Khac, and M.-T. Kechadi, Advances in Data Mining. Theoretical Aspects and Applications: 7th Industrial Conference (ICDM 2007), Leipzig, Germany, July 14-18, 2007. Proceedings. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, ch. Lightweight Clustering Technique for Distributed Data Mining Applications, pp. 120\u2013134.\n[18] J.-F. Laloux, N.-A. Le-Khac, and M.-T. Kechadi, \u201cEfficient distributed approach for density-based clustering,\u201d Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE), 20th IEEE International Workshops, pp. 145\u2013150, 27-29 June 2011.\n[19] M. K. Jiawei Han, Data Mining: Concepts and Techniques, 2nd ed. Elsevier, Diane Cerra, San Francisco, CA 94111, 2006, ch. Introduction.\n[20] Y. Karine ZeitouniLaurent, \u201cLe data mining spatial et les bases de donne\u0301es spatiales,\u201d Revue internationale de ge\u0301omatique. Volume, vol. 9, no. 4, 1999.\n[21] M. J. Zaki, Large-Scale Parallel Data Mining. Berlin, Heidelberg: Springer Berlin Heidelberg, 2000, ch. Parallel and Distributed Data Mining: An Introduction, pp. 1\u201323.\n[22] S. Ghosh, Distributed systems: an algorithmic approach. CRC press, 2014.\n[23] L. Aouad, N.-A. Le-Khac, and M.-T. Kechadi., \u201cImage analysis platform for data management in the meteorological domain,\u201d in 7th Industrial Conference, ICDM 2007, Leipzig, Germany, July 14-18, 2007. Proceedings, vol. 4597. Springer Berlin Heidelberg, 2007, pp. 120\u2013134.\n[24] X. Wu, X. Zhu, G. Q. Wu, and W. Ding, \u201cData mining with big data,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 1, pp. 97\u2013107, 2014.\n[25] L. Rokach, A. Schclar, and E. Itach, \u201cEnsemble methods for multi-label classification,\u201d Expert Systems with Applications, vol. 41, no. 16, pp. 7507 \u2013 7523, 2014.\n[26] R. K. Eric Bauer, \u201cAn empirical comparison of voting classification algorithms: Bagging, boosting, and variants,\u201d springer Link:Machine Learning, vol. 36, pp. 105\u2013139, 1999.\n[27] M. L. Tian Zhang, Raghu Ramakrishnan, \u201cBirch: An efficient data clustering method for very large databases,\u201d in SIGMOD \u201996 Proceedings of the 1996 ACM SIGMOD international conference on Management of data, vol. 25, 1996, pp. 103\u2013114.\n[28] P. J. F. A. K. Jain, M. N. Murty, \u201cData clustering: a review,\u201d ACM Computing Surveys (CSUR), vol. 31, pp. 264\u2013323, 1999.\n[29] B. H. Mokeddem Djamila, \u201cUtilisation des mthodes d\u2019apprentissage ensembliste dans le datamining distribu,\u201d RIST ISSN 1111-0015, vol. 17, 2007.\n[30] I. Dhillon and D. Modha, \u201cA data-clustering algorithm on distributed memory multiprocessor,\u201d in large-Scale Parallel Data Mining, Workshop on Large-Scale Parallel KDD Systems, SIGKDD. Springer-Verlag London, UK, 1999, pp. 245\u2013260.\n[31] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, \u201cA density-based algorithm for discovering clusters in large spatial databases with noise.\u201d in Kdd, vol. 96, no. 34, 1996, pp. 226\u2013231.\n[32] Garg, Mangla, Bhatnagar, and Gupta, \u201cPbirch: A scalable parallel clustering algorithm for incremental data,\u201d Database Engineering and Applications Symposium. IDEAS \u201906. 10th International, Delhi, pp. 315\u2013316, 2006.\n[33] H.Geng, Omaha, and X. Deng, \u201cA new clustering algorithm using message passing and its applications in analyzing microarray data,\u201d in ICMLA \u201905 Proceedings of the Fourth International Conference on Machine Learning and Applications. IEEE, 15-17 December 2005, p. 145150.\n[34] I. D. Dhillon and D. S. Modha, \u201cA data-clustering algorithm on distributed memory multiprocessors,\u201d in Large-Scale Parallel Data Mining. Springer Berlin Heidelberg, 2000, pp. 245\u2013260.\n[35] X. Xu, J. Jger, and H.-P. Kriegel, \u201cA fast parallel clustering algorithm for large spatial databases,\u201d Data Mining and Knowledge Discovery archive, vol. 3, pp. 263\u2013290, September 1999.\n[36] N.-A. Le-Khac, M. Bue, M. Whelan, and M-T.Kechadi, \u201cA knowledgebased data reduction for very large spatio-temporal datasets,\u201d International Conference on Advanced Data Mining and Applications, (ADMA2010), 19-21 November 2010.\n[37] M. Fadilia, M. Melkemib, and A. ElMoataza, Pattern Recognition Letters:Non-convex onion-peeling using a shape hull algorithm. ELSEVIER, 15 October 2004, vol. 24.\n[38] A. Chaudhuri, B. Chaudhuri, and S. Parui, \u201cA novel approach to computation of the shape of a dot pattern and extraction of its perceptual border,\u201d Computer vision and Image Understranding, vol. 68, pp. 257\u2013 275, 03 December 1997.\n[39] M. Melkemi and M. Djebali, \u201cComputing the shape of a planar points set,\u201d Elsevier Science, vol. 33, p. 14231436, 9 September 2000.\n[40] H. Edelsbrunner, D. G. Kirkpatrick, and R. Seidel, \u201cOn the shape of a set of points in the plane,\u201d Information Theory, IEEE Transactions on, vol. 29, no. 4, pp. 551\u2013559, 1983.\n[41] A. Moreira and M. Y. Santos, \u201cConcave hull: A k-nearest neighbours approach for the computation of the region occupied by a set of points,\u201d in International Conference on Computer Graphics Theory and Applications (GRAPP 2007), Barcelona, Spani, 8-11 March 2007, pp. 61\u201368.\n[42] M. Duckhama, L. Kulikb, M. Worboysc, and A. Galtond, \u201cEfficient generation of simple polygons for characterizing the shape of a set of points in the plane,\u201d Elsevier Science Inc. New York, NY, USA, vol. 41, pp. 3224\u20133236, 15 March 2008.\n[43] M. Bendechache and M.-T. Kechadi, \u201cDistributed clustering algorithm for spatial data mining,\u201d in Spatial Data Mining and Geographical Knowledge Services (ICSDM), 2015 2nd IEEE International Conference on, 2015, pp. 60\u201365.\n[44] M.Ester, H. Kriegel, J. Sander, and X. Xu, \u201cA density-based algorithm for discovering clusters in large spatial databases with noise,\u201d In 2nd Int. Conf., Knowledge Discovery and Data Mining (KDD 96), 1996.\n[45] T. Kanungo, S. Jose, D. M. Mount, N. S. Netanyahu, and C. D. Piatko, \u201cAn efficient k-means clustering algorithm: Analysis and implementation,\u201d IEEE Transactions on pattern analysis and machine intelligence, vol. 24, July 2002.\n[46] T. Zhang, R. Ramakrishnan, and M. Livny, \u201cBirch: An efficient data clustering method for very large databases,\u201d in SIGMOD-96 Proceedings of the 1996 ACM SIGMOD international conference on Management of data, vol. 25. ACM New York, USA, 1996, pp. 103\u2013114.\n[47] W. Wang, J. Yang, and R. R. Muntz, \u201cSting: A statistical information grid approach to spatial data mining,\u201d in Proceedings of the 23rd International Conference on Very Large Data Bases, ser. VLDB \u201997. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1997, pp. 186\u2013195.\n[48] S. Guha, R. Rastogi, and K. Shim, \u201cCure: An efficient clustering algorithm for large databases,\u201d in Information Systems, vol. 26. Elsevier Science Ltd. Oxford, UK, 17 November 2001, pp. 35\u201358."}], "references": [{"title": "Data Mining: Concepts and Techniques, 2nd ed", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "weight clustering technique for distributed data mining applications", "author": ["L. Aouad", "N.-A. Le-Khac", "M.-T. Kechadi"], "venue": "LNCS on advances in data mining \u2013 theoretical aspects and applications, vol. 4597, pp. 120\u2013134, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Grid-based approaches for distributed data mining applications", "author": ["L.Aouad", "N.-A. Le-Khac", "M.-T. Kechadi"], "venue": "Journal of Algorithms & Computational Technology, vol. 3, no. 4, pp. 517\u2013534, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance study of distributed apriori-like frequent itemsets mining", "author": ["L. Aouad", "N.-A. Le-Khac", "M.-T. Kechadi"], "venue": "Knowledge and Information Systems, vol. 23, no. 1, pp. 55\u201372, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Performance evaluation of a density-based clustering method for reducing very large spatio-temporal dataset", "author": ["M. Whelan", "N.-A.L. Khac", "M.-T. Kechadi"], "venue": "in Proc. of International Conference on Information and Knowledge Engineering, July 18-21 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards a framework for mining and analysing spatio-temporal datasets", "author": ["M. Bertolotto", "S. Di Martino", "F. Ferrucci", "M.-T. Kechadi"], "venue": "International Journal of Geographical Information Science, vol. 21, no. 8, pp. 895\u2013906, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Knowledge map layer for distributed data mining", "author": ["N.-A. Le-Khac", "L. Aouad", "M.-T. Kechadi"], "venue": "Journal of ISAST Transactions on Intelligent Systems, vol. 1, no. 1, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparative evaluation of voting and metalearning on partitioned data", "author": ["P. Chan", "S. Stolfo"], "venue": "12th International Conference on Machine Learning, 1995, pp. 90\u201398.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Modern heuristic techniques for combinatorial problems", "author": ["C. Reeves"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization", "author": ["T.G. Dietterich"], "venue": "Machine Learning, vol. 40, no. 2, pp. 139\u2013157, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Advances in distributed and Parallel Knowledge Discovery", "author": ["H. Kargupta", "P. Chan"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Data mining for association rules and sequential patterns: sequential and parallel algorithms", "author": ["J.-M. Adamo"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Performance study of distributed apriori-like frequent itemsets mining", "author": ["N.-A. Le-Khac", "L. Aouad", "M.-T. Kechadi"], "venue": "Knowledge and Information Systems, vol. 23, no. 1, pp. 55\u201372, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Emergent Web Intelligence: Advanced Semantic Technologies", "author": ["N.A. Le Khac", "L.M. Aouad", "M.-T. Kechadi"], "venue": "ch. Toward Distributed Knowledge Discovery onGrid Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Advances in Database Technology - EDBT 2004: 9th International Conference on Extending Database Technology, Heraklion, Crete, Greece", "author": ["E. Januzaj", "H.-P. Kriegel", "M. Pfeifle"], "venue": "March 14-18,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Data Management. Data, Data Everywhere: 24th British National Conference on Databases, BNCOD", "author": ["N. Le-Khac", "L.Aouad", "M.-T. Kechadi"], "venue": "Proceedings. Berlin, Heidelberg: Springer Berlin Heidelberg,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Advances in Data Mining. Theoretical Aspects and Applications: 7th Industrial Conference (ICDM", "author": ["L. Aouad", "N.-A.L. Khac", "M.-T. Kechadi"], "venue": "Proceedings. Berlin, Heidelberg: Springer Berlin Heidelberg,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Efficient distributed approach for density-based clustering", "author": ["J.-F. Laloux", "N.-A. Le-Khac", "M.-T. Kechadi"], "venue": "Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE), 20th IEEE International Workshops, pp. 145\u2013150, 27-29 June 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Data Mining: Concepts and Techniques, 2nd ed", "author": ["M.K. Jiawei Han"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Le data mining spatial et les bases de donn\u00e9es spatiales", "author": ["Y. Karine ZeitouniLaurent"], "venue": "Revue internationale de g\u00e9omatique. Volume, vol. 9, no. 4, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Large-Scale Parallel Data Mining", "author": ["M.J. Zaki"], "venue": "ch. Parallel and Distributed Data Mining: An Introduction,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Distributed systems: an algorithmic approach", "author": ["S. Ghosh"], "venue": "CRC press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Image analysis platform for data management in the meteorological domain", "author": ["L. Aouad", "N.-A. Le-Khac", "M.-T. Kechadi."], "venue": "7th Industrial Conference, ICDM 2007, Leipzig, Germany, July 14-18, 2007. Proceedings, vol. 4597. Springer Berlin Heidelberg, 2007, pp. 120\u2013134.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 1, pp. 97\u2013107, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble methods for multi-label classification", "author": ["L. Rokach", "A. Schclar", "E. Itach"], "venue": "Expert Systems with Applications, vol. 41, no. 16, pp. 7507 \u2013 7523, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants", "author": ["R.K. Eric Bauer"], "venue": "springer Link:Machine Learning, vol. 36, pp. 105\u2013139, 1999.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Birch: An efficient data clustering method for very large databases", "author": ["M.L. Tian Zhang", "Raghu Ramakrishnan"], "venue": "SIGMOD \u201996 Proceedings of the 1996 ACM SIGMOD international conference on Management of data, vol. 25, 1996, pp. 103\u2013114.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Data clustering: a review", "author": ["P.J.F.A.K. Jain", "M.N. Murty"], "venue": "ACM Computing Surveys (CSUR), vol. 31, pp. 264\u2013323, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Utilisation des mthodes d\u2019apprentissage ensembliste dans le datamining distribu", "author": ["B.H. Mokeddem Djamila"], "venue": "RIST ISSN 1111-0015, vol. 17, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "A data-clustering algorithm on distributed memory multiprocessor", "author": ["I. Dhillon", "D. Modha"], "venue": "large-Scale Parallel Data Mining, Workshop on Large-Scale Parallel KDD Systems, SIGKDD. Springer-Verlag London, UK, 1999, pp. 245\u2013260.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise.", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "in Kdd,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1996}, {"title": "Pbirch: A scalable parallel clustering algorithm for incremental data", "author": ["Garg", "Mangla", "Bhatnagar", "Gupta"], "venue": "Database Engineering and Applications Symposium. IDEAS \u201906. 10th International, Delhi, pp. 315\u2013316, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "A new clustering algorithm using message passing and its applications in analyzing microarray data", "author": ["H.Geng", "Omaha", "X. Deng"], "venue": "ICMLA \u201905 Proceedings of the Fourth International Conference on Machine Learning and Applications. IEEE, 15-17 December 2005, p. 145150.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "A data-clustering algorithm on distributed memory multiprocessors", "author": ["I.D. Dhillon", "D.S. Modha"], "venue": "Large-Scale Parallel Data Mining. Springer Berlin Heidelberg, 2000, pp. 245\u2013260.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "A fast parallel clustering algorithm for large spatial databases", "author": ["X. Xu", "J. Jger", "H.-P. Kriegel"], "venue": "Data Mining and Knowledge Discovery archive, vol. 3, pp. 263\u2013290, September 1999.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1999}, {"title": "A knowledgebased data reduction for very large spatio-temporal datasets", "author": ["N.-A. Le-Khac", "M. Bue", "M. Whelan", "M-T.Kechadi"], "venue": "International Conference on Advanced Data Mining and Applications, (ADMA2010), 19-21 November 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "ElMoataza, Pattern Recognition Letters:Non-convex onion-peeling using a shape hull algorithm", "author": ["M. Fadilia", "M. Melkemib"], "venue": "EL- SEVIER,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "A novel approach to computation of the shape of a dot pattern and extraction of its perceptual border", "author": ["A. Chaudhuri", "B. Chaudhuri", "S. Parui"], "venue": "Computer vision and Image Understranding, vol. 68, pp. 257\u2013 275, 03 December 1997.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1997}, {"title": "Computing the shape of a planar points set", "author": ["M. Melkemi", "M. Djebali"], "venue": "Elsevier Science, vol. 33, p. 14231436, 9 September 2000.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "On the shape of a set of points in the plane", "author": ["H. Edelsbrunner", "D.G. Kirkpatrick", "R. Seidel"], "venue": "Information Theory, IEEE Transactions on, vol. 29, no. 4, pp. 551\u2013559, 1983.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1983}, {"title": "Concave hull: A k-nearest neighbours approach for the computation of the region occupied by a set of points", "author": ["A. Moreira", "M.Y. Santos"], "venue": "International Conference on Computer Graphics Theory and Applications (GRAPP 2007), Barcelona, Spani, 8-11 March 2007, pp. 61\u201368.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient generation of simple polygons for characterizing the shape of a set of points in the plane", "author": ["M. Duckhama", "L. Kulikb", "M. Worboysc", "A. Galtond"], "venue": "Elsevier Science Inc. New York, NY, USA, vol. 41, pp. 3224\u20133236, 15 March 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed clustering algorithm for spatial data mining", "author": ["M. Bendechache", "M.-T. Kechadi"], "venue": "Spatial Data Mining and Geographical Knowledge Services (ICSDM), 2015 2nd IEEE International Conference on, 2015, pp. 60\u201365.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M.Ester", "H. Kriegel", "J. Sander", "X. Xu"], "venue": "2nd Int. Conf., Knowledge Discovery and Data Mining (KDD 96), 1996.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "S. Jose", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko"], "venue": "IEEE Transactions on pattern analysis and machine intelligence, vol. 24, July 2002.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2002}, {"title": "Birch: An efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "SIGMOD-96 Proceedings of the 1996 ACM SIGMOD international conference on Management of data, vol. 25. ACM New York, USA, 1996, pp. 103\u2013114.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1996}, {"title": "Sting: A statistical information grid approach to spatial data mining", "author": ["W. Wang", "J. Yang", "R.R. Muntz"], "venue": "Proceedings of the 23rd International Conference on Very Large Data Bases, ser. VLDB \u201997. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1997, pp. 186\u2013195.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "Cure: An efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "Information Systems, vol. 26. Elsevier Science Ltd. Oxford, UK, 17 November 2001, pp. 35\u201358.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "This is even more critical when the collected data is located on different sites, and are owned by different organisations [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "Briefly, an efficient management of distributed knowledge is one of the key factors affecting the outputs of these techniques [2], [3], [4], [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "Briefly, an efficient management of distributed knowledge is one of the key factors affecting the outputs of these techniques [2], [3], [4], [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "Briefly, an efficient management of distributed knowledge is one of the key factors affecting the outputs of these techniques [2], [3], [4], [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "Briefly, an efficient management of distributed knowledge is one of the key factors affecting the outputs of these techniques [2], [3], [4], [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "Traditional, centralised data mining techniques have not considered all the issues of datadriven applications, such as scalability in both response time and accuracy of solutions, distribution, and heterogeneity [6].", "startOffset": 212, "endOffset": 215}, {"referenceID": 6, "context": "Some DDM approaches are based on ensemble learning, which uses various techniques to aggregate the results [7], among the most cited in the literature: majority voting, weighted voting, and stacking [8], [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "Some DDM approaches are based on ensemble learning, which uses various techniques to aggregate the results [7], among the most cited in the literature: majority voting, weighted voting, and stacking [8], [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 8, "context": "Some DDM approaches are based on ensemble learning, which uses various techniques to aggregate the results [7], among the most cited in the literature: majority voting, weighted voting, and stacking [8], [9].", "startOffset": 204, "endOffset": 207}, {"referenceID": 9, "context": "Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years.", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years.", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Many DDM methods such as distributed association rules and distributed classification [10], [11], [12], [3], [13], [14] have been proposed and developed in the last few years.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Recent researches [15], [16], [17] have proposed distributed clustering approaches based on the same 2-step process: perform partial analysis on local data at individual sites and then aggregate them to obtain global results.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Recent researches [15], [16], [17] have proposed distributed clustering approaches based on the same 2-step process: perform partial analysis on local data at individual sites and then aggregate them to obtain global results.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Recent researches [15], [16], [17] have proposed distributed clustering approaches based on the same 2-step process: perform partial analysis on local data at individual sites and then aggregate them to obtain global results.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "A case study of an efficient aggregation phase has been developed on spatial datasets and proven to be very efficient; the data exchanged is reduced by more than 98% of the original datasets [18].", "startOffset": 191, "endOffset": 195}, {"referenceID": 18, "context": "Distributed Data Mining (DDM) is a line of research that has attracted much interest in recent years [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 19, "context": "DDM was developed because of the need to process data that can be very large or geographically distributed across multiple sites [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "DDM techniques can be divided into two categories based on the targeted architectures of computing platforms [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "These are called distributed systems, and are characterised by a distributed communication network connecting low-speed machines that can be of different architectures, but they are very abundant [22].", "startOffset": 196, "endOffset": 200}, {"referenceID": 22, "context": "Some of these techniques have already been developed and implemented in [23], [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "Some of these techniques have already been developed and implemented in [23], [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "This has led to the development of techniques that rely on ensemble learning [25], [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "This has led to the development of techniques that rely on ensemble learning [25], [26].", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "These samples will be analysed using a single global algorithm [27], [28].", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "These samples will be analysed using a single global algorithm [27], [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "However, this technique has a disadvantage that the sampling depends on the transfer time which may impact on the quality of the samples [29].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 30, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 115, "endOffset": 119}, {"referenceID": 32, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 121, "endOffset": 125}, {"referenceID": 33, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 34, "context": "Many parallel clustering versions based on these algorithms have been proposed in the literature [17], [30], [31], [32], [33], [34], [35].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "The second subcategory consists of methods that build local clustering models and send them to a central site to build global models [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "In [30] and [34], message-passing versions of the widely used K-Means algorithm were proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [30] and [34], message-passing versions of the widely used K-Means algorithm were proposed.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "In [31] and [35], the authors dealt with the parallelisation of the DBSCAN densitybased clustering algorithm.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "In [31] and [35], the authors dealt with the parallelisation of the DBSCAN densitybased clustering algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "In [32] a parallel message passing version of the BIRCH algorithm was presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "A parallel version of a hierarchical clustering algorithm, called MPC for Message Passing Clustering, which is especially dedicated to Microarray data was introduced in [33].", "startOffset": 169, "endOffset": 173}, {"referenceID": 16, "context": "Most of the parallel approaches need either multiple synchronisation constraints between processes or a global view of the dataset, or both [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "Another approach presented in [17] also applied a merging of local models to create the global models.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "If the local models cannot effectively represent local datasets then global models accuracy will be very poor [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 35, "context": "In [36], an efficient reduction technique has been proposed; it is based on density-based clustering algorithms.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "However, selecting representatives is still a challenge in terms of quality and size [15], [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "However, selecting representatives is still a challenge in terms of quality and size [15], [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 36, "context": "Many algorithms for extracting the boundaries from a cluster can be found in the literature [37], [38], [39], [40], [41].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "Many algorithms for extracting the boundaries from a cluster can be found in the literature [37], [38], [39], [40], [41].", "startOffset": 98, "endOffset": 102}, {"referenceID": 38, "context": "Many algorithms for extracting the boundaries from a cluster can be found in the literature [37], [38], [39], [40], [41].", "startOffset": 104, "endOffset": 108}, {"referenceID": 39, "context": "Many algorithms for extracting the boundaries from a cluster can be found in the literature [37], [38], [39], [40], [41].", "startOffset": 110, "endOffset": 114}, {"referenceID": 40, "context": "Many algorithms for extracting the boundaries from a cluster can be found in the literature [37], [38], [39], [40], [41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 41, "context": "We used the algorithm proposed in [42], which is based on triangulation to generate the cluster boundaries.", "startOffset": 34, "endOffset": 38}, {"referenceID": 42, "context": "It has been shown in [43] that DDC-K-Means dynamically determines the number of the clusters without a priori knowledge about the data or an estimation process of the number of the clusters.", "startOffset": 21, "endOffset": 25}, {"referenceID": 43, "context": "1) DBSCAN: DBSCAN (Density-Based spatial Clustering of Applications with Noise) is a well-known density based clustering algorithm capable of discovering clusters with arbitrary shapes and eliminating noisy data [44].", "startOffset": 212, "endOffset": 216}, {"referenceID": 43, "context": "More details can be found in [44].", "startOffset": 29, "endOffset": 33}, {"referenceID": 44, "context": "Compared with other popular clustering methods such as K-Means [45], BIRCH [46], and STING [47], DBSCAN has several key features.", "startOffset": 63, "endOffset": 67}, {"referenceID": 45, "context": "Compared with other popular clustering methods such as K-Means [45], BIRCH [46], and STING [47], DBSCAN has several key features.", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "Compared with other popular clustering methods such as K-Means [45], BIRCH [46], and STING [47], DBSCAN has several key features.", "startOffset": 91, "endOffset": 95}, {"referenceID": 45, "context": "BIRCH: We used the BIRCH implementation provided in [46].", "startOffset": 52, "endOffset": 56}, {"referenceID": 45, "context": "We set its parameters to the default values suggested in [46].", "startOffset": 57, "endOffset": 61}, {"referenceID": 47, "context": "CURE: We used the implementation of CURE provided in [48].", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "As described in [48], when two clusters are merged in each step of the algorithm, representative points for the new merged cluster are selected from the ones of the two original clusters rather than all the points in the merged clusters.", "startOffset": 16, "endOffset": 20}], "year": 2017, "abstractText": "Clustering techniques are very attractive for extracting and identifying patterns in datasets. However, their application to very large spatial datasets presents numerous challenges such as high-dimensionality data, heterogeneity, and high complexity of some algorithms. For instance, some algorithms may have linear complexity but they require the domain knowledge in order to determine their input parameters. Distributed clustering techniques constitute a very good alternative to the big data challenges (e.g.,Volume, Variety, Veracity, and Velocity). Usually these techniques consist of two phases. The first phase generates local models or patterns and the second one tends to aggregate the local results to obtain global models. While the first phase can be executed in parallel on each site and, therefore, efficient, the aggregation phase is complex, time consuming and may produce incorrect and ambiguous global clusters and therefore incorrect models. In this paper we propose a new distributed clustering approach to deal efficiently with both phases; generation of local results and generation of global models by aggregation. For the first phase, our approach is capable of analysing the datasets located in each site using different clustering techniques. The aggregation phase is designed in such a way that the final clusters are compact and accurate while the overall process is efficient in time and memory allocation. For the evaluation, we use two well-known clustering algorithms; K-Means and DBSCAN. One of the key outputs of this distributed clustering technique is that the number of global clusters is dynamic; no need to be fixed in advance. Experimental results show that the approach is scalable and produces high quality results.", "creator": "Word"}}}