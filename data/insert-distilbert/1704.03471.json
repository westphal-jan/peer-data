{"id": "1704.03471", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "What do Neural Machine Translation Models Learn about Morphology?", "abstract": "modern neural machine input translation ( mt ) models obtain state - of - the - art performance comparisons while maintaining a simple, end - stage to - end digital architecture. however, little is known about or what these models learn poorly about source and target languages during the training process. embodied in this work, we analyze the representations learned by neural mt models at its various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part - of - mouth speech and morphological tagging tasks. we conduct a first thorough investigation along several parameters : word - based vs. character - based representations, depth of access the encoding layer, the identity of the target language, and encoder vs. decoder representations. our data - line driven, implicit quantitative evaluation sheds light on important aspects in the neural mt system and also its enhanced ability to capture word structure.", "histories": [["v1", "Tue, 11 Apr 2017 18:01:07 GMT  (3628kb,D)", "http://arxiv.org/abs/1704.03471v1", "Accepted to ACL 2017"], ["v2", "Mon, 15 May 2017 13:38:20 GMT  (1418kb,D)", "http://arxiv.org/abs/1704.03471v2", "ACL 2017 camera-ready"]], "COMMENTS": "Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yonatan belinkov", "nadir durrani", "fahim dalvi", "hassan sajjad", "james r glass"], "accepted": true, "id": "1704.03471"}, "pdf": {"name": "1704.03471.pdf", "metadata": {"source": "CRF", "title": "What do Neural Machine Translation Models Learn about Morphology?", "authors": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass"], "emails": ["glass}@mit.edu", "hsajjad}@qf.org.qa"], "sections": [{"heading": null, "text": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure."}, {"heading": "1 Introduction", "text": "Neural network models are quickly becoming the predominant approach to machine translation (MT). Training neural MT (NMT) models can be done in an end-to-end fashion, which is simpler and more elegant than traditional MT systems. Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014). The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation (Luong and Manning, 2015; Bentivogli et al., 2016).\nHowever, little is known about what and how much these models learn about each language and its features. Recent work has started exploring the role of the NMT encoder in learning source syntax (Shi et al., 2016), but research studies are yet to answer important questions such as: (i) what do NMT models learn about word morphology? (ii) what is the effect on learning when translating into/from morphologically-rich languages? (iii) what impact do different representations (character vs. word) have on learning? and (iv) what do different modules learn about the syntactic and semantic structure of a language? Answering such questions is imperative for fully understanding the NMT architecture. In this paper, we strive towards exploring (i), (ii), and (iii) by providing quantitative, data-driven answers to the following specific questions:\n\u2022 Which parts of the NMT architecture capture word structure?\n\u2022 What is the division of labor between different components (e.g. different layers or encoder vs. decoder)?\n\u2022 How do different word representations help learn better morphology and modeling of infrequent words?\n\u2022 How does the target language affect the learning of word structure?\nTo achieve this, we follow a simple but effective procedure with three steps: (i) train a neural MT system on a parallel corpus; (ii) use the trained model to extract feature representations for words in a language of interest; and (iii) train a classifier using extracted features to make predictions for another task. We then evaluate the quality of the trained classifier on the given task as a proxy to the quality of the extracted representations. In\nar X\niv :1\n70 4.\n03 47\n1v 1\n[ cs\n.C L\n] 1\n1 A\npr 2\n01 7\nthis way, we obtain a quantitative measure of how well the original MT system learns features that are relevant to the given task.\nWe focus on the tasks of part-of-speech (POS) and full morphological tagging. We investigate how different neural MT systems capture POS and morphology through a series of experiments along several parameters. For instance, we contrast word-based and character-based representations, use different encoding layers, vary source and target languages, and compare extracting features from the encoder vs. the decoder.\nWe experiment with several languages with varying degrees of morphological richness: French, German, Czech, Arabic, and Hebrew. Our analysis reveals interesting insights such as:\n\u2022 Character-based representations are much better for learning morphology, especially for low-frequency words. This improvement is correlated with better BLEU scores. On the other hand, word-based models are sufficient for learning the structure of common words.\n\u2022 Lower layers of the encoder are better at capturing word structure, while deeper networks improve translation quality, suggesting that higher layers focus more on word meaning.\n\u2022 The target language impacts the kind of information learned by the MT system. Translating into morphologically-poorer languages leads to better source-side word representations. This is partly, but not completely, correlated with BLEU scores.\n\u2022 The neural decoder learns very little about word structure. The attention mechanism removes much of the burden of learning word representations from the decoder."}, {"heading": "2 Methodology", "text": "Given a source sentence s = {w1, w2, ..., wN} and a target sentence t = {u1, u2, ..., uM}, we first generate a vector representation for the source sentence using an encoder (Eqn. 1) and then map this vector to the target sentence using a decoder (Eqn. 2) (Sutskever et al., 2014):\nENC : s = {w1, w2, ..., wN} 7\u2192 s \u2208 Rk (1) DEC : s \u2208 Rk 7\u2192 t = {u1, u2, ..., uM} (2)\nIn this work, we use long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997)\nencoder-decoders with attention (Bahdanau et al., 2014), which we train on parallel data.\nAfter training the NMT system, we freeze the parameters of the encoder and use ENC as a feature extractor to generate vectors representing words in the sentence. Let ENCi(s) denote the encoded representation of word wi. For example, this may be the output of the LSTM after word wi. We feed ENCi(s) to a neural classifier that is trained to predict POS or morphological tags and evaluate the quality of the representation based on our ability to train a good classifier. By comparing the performance of classifiers trained with features from different instantiations of ENC, we can evaluate what MT encoders learn about word structure. Figure 1 illustrates this process. We follow a similar procedure for analyzing representation learning in DEC.\nThe classifier itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to extract useful information from them.1 We emphasize that our goal is not to beat the state-of-the-art on a given task, but rather to analyze what NMT models learn about morphology. The classifier is trained with a cross-entropy loss; more details on its architecture are in the supplementary material.\n1We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings."}, {"heading": "3 Data", "text": "Language pairs We experiment with several language pairs, including morphologically-rich languages, that have received relatively significant attention in the MT community. These include Arabic-, German-, French-, and Czech-English pairs. To broaden our analysis and study the effect of having morphologically-rich languages on both source and target sides, we also include ArabicHebrew, two languages with rich and similar morphological systems, and Arabic-German, two languages with rich but different morphologies.\nMT data Our translation models are trained on the WIT3 corpus of TED talks (Cettolo et al., 2012; Cettolo, 2016) made available for IWSLT 2016. This allows for comparable and crosslinguistic analysis. Statistics about each language pair are given in Table 1 (under Pred). We use official dev and test sets for tuning and testing. Reported figures are the averages over test sets.\nAnnotated data We use two kinds of datasets to train POS and morphological classifiers: goldstandard and predicted tags. For predicted tags, we simply used freely available taggers to annotate the MT data. For gold tags, we use gold-annotated datasets. Table 1 gives statistics for datasets with gold and predicted tags; see supplementary material for details on taggers and gold data. We train and test our classifiers on predicted annotations, and similarly on gold annotations, when we have them. We report both results wherever available."}, {"heading": "4 Encoder Analysis", "text": "Recall that after training the NMT system we freeze its parameters and use it only to generate features for the POS/morphology classifier. Given a trained encoder ENC and a sentence swith POS/morphology annotation, we generate word features ENCi(s) for every word in the sentence.\nWe then train a classifier that uses the features ENCi(s) to predict POS or morphological tags."}, {"heading": "4.1 Effect of word representation", "text": "In this section, we compare different word representations extracted with different encoders. Our word-based model uses a word embedding matrix which is initialized randomly and learned with other NMT parameters. For a character-based model we adopt a convolutional neural network (CNN) over character embeddings that is also learned during training (Kim et al., 2015; Costajussa\u0300 and Fonollosa, 2016); see appendix A.1 for specific settings. In both cases we run the encoder over these representations and use its output ENCi(s) as features for the classifier.\nTable 2 shows POS tagging accuracy using features from different NMT encoders. Charbased models always generate better representations for POS tagging, especially in the case of morphologically-richer languages like Arabic and Czech. We observed a similar pattern in the full morphological tagging task. For example, we obtain morphological tagging accuracy of 65.2/79.66 and 67.66/81.66 using word/charbased representations from the Arabic-Hebrew and Arabic-English encoders, respectively.2 The superior morphological power of the char-based model also manifests in better translation quality (measured by BLEU), as shown in Table 2.\nImpact of word frequency Let us look more closely at an example case: Arabic POS and morphological tagging. Figure 3 shows the effect of using word-based vs. char-based feature representations, obtained from the encoder of the Arabic-\n2The results are not far below dedicated taggers (e.g. 95.1/84.1 on Arabic POS/morphology (Pasha et al., 2014)), indicating that NMT models learn quite good representations.\nHebrew system (other language pairs exhibit similar trends). Clearly, the char-based model is superior to the word-based one. This is true for the overall accuracy (+14.3% in POS, +14.5% in morphology), but more so on OOV words (+37.6% in POS, +32.7% in morphology). Figure 2 shows that the gap between word-based and char-based representations increases as the frequency of the word in the training data decreases. In other words, the more frequent the word, the less need there is for character information. These findings make intuitive sense: the char-based model is able to learn character n-gram patterns that are important for identifying word structure, but as the word becomes more frequent the word-based model has seen enough examples to make a decision.\nAnalyzing specific tags In Figure 5 we plot confusion matrices for POS tagging using wordbased and char-based representations (from Arabic encoders). While the char-based representations are overall better, the two models still\nshare similar misclassified tags. Much of the confusion comes from wrongly predicting nouns (NN, NNP). In the word-based case, relatively many tags with determiner (DT+NNP, DT+NNPS, DT+NNS, DT+VBG) are wrongly predicted as non-determined nouns (NN, NNP). In the charbased case, this hardly happens. This suggests that char-based representations are predictive of the presence of a determiner, which in Arabic is expressed as the prefix \u201cAl-\u201d (the definite article), a pattern easily captured by a char-based model.\nIn Figure 4 we plot the difference in POS accuracy when moving from word-based to char-based representations, per POS tag frequency in the training data. Tags closer to the upper-right corner occur more frequently in the training set and are better predicted by char-based compared to wordbased representations. There are a few fairly frequent tags (in the middle-bottom part of the figure) whose accuracy does not improve much when moving from word- to char-based representations: mostly conjunctions, determiners, and certain par-\nticles (CC, DT, WP). But there are several very frequent tags (NN, DT+NN, DT+JJ, VBP, and even PUNC) whose accuracy improves quite a lot. Then there are plural nouns (NNS, DT+NNS) where the char-based model really shines, which makes sense linguistically as plurality in Arabic is usually expressed by certain suffixes (\u201c-wn/yn\u201d for masc. plural, \u201c-At\u201d for fem. plural). The charbased model is thus especially good with frequent tags and infrequent words, which is understandable given that infrequent words typically belong to frequent open categories like nouns and verbs."}, {"heading": "4.2 Effect of encoder depth", "text": "Modern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016). We would like to understand what kind of information different layers capture. Given\na trained model with multiple layers, we extract representations from the different layers in the encoder. Let ENCli(s) denote the encoded representation of word wi after the l-th layer. We vary l and train different classifiers to predict POS or morphological tags. Here we focus on the case of a 2- layer encoder-decoder for simplicity (l \u2208 {1, 2}).\nFigure 6 shows POS tagging results using representations from different encoding layers across five language pairs. The general trend is that passing word vectors through the encoder improves POS tagging, which can be explained by contextual information contained in the representations after one layer. However, it turns out that representations from the 1st layer are better than those from the 2nd layer, at least for the purpose of capturing word structure. Figure 7 shows that the same\npattern holds for both word-based and char-based representations, on Arabic POS and morphological tagging. In all cases, layer 1 representations are better than layer 2 representations.3 In contrast, BLEU scores actually increase when training 2-layer vs. 1-layer models (+1.11/+0.56 BLEU for Arabic-Hebrew word/char-based models). Thus translation quality improves when adding layers but morphology quality degrades. Intuitively, it seems that lower layers of the network learn to represent word structure while higher layers focus more on word meaning. A similar pattern was recently observed in a joint language-vision deep recurrent net (Gelderloos and Chrupa\u0142a, 2016)."}, {"heading": "4.3 Effect of target language", "text": "While translating from morphologically-rich languages is challenging, translating into such languages is even harder. For instance, our basic system obtains BLEU of 24.69/23.2 on Arabic/Czech to English, but only 13.37/13.9 on English to Arabic/Czech. How does the target language affect the learned source language representations? Does translating into a morphologically-rich language require more knowledge about source language morphology? In order to investigate these questions, we fix the source language and train NMT models on different target languages. For example, given an Arabic source we train Arabic-toEnglish/Hebrew/German systems. These target languages represent a morphologically-poor language (English), a morphologically-rich language with similar morphology to the source language (Hebrew), and a morphologically-rich language with different morphology (German). To make a fair comparison, we train the models on the intersection of the training data based on the source language. In this way the experimental setup is completely identical: the models are trained on the same Arabic sentences with different translations.\nFigure 8 shows POS and morphology accuracy of word-based representations from the NMT encoders, as well as corresponding BLEU scores. As expected, translating to English is easier than translating to the morphologically-richer Hebrew and German, resulting in higher BLEU. Despite their similar morphologies, translating Arabic to Hebrew is worse than Arabic to German, which can be attributed to the richer Hebrew morphology\n3We found this result to be also true in French, German, and Czech experiments (see the supplementary material).\ncompared to German. POS and morphology accuracies share an intriguing pattern: the representations that are learned when translating to English are better for predicting POS or morphology than those learned when translating to German, which are in turn better than those learned when translating to Hebrew. This is remarkable given that English is a morphologically-poor language that does not display many of the morphological properties that are found in the Arabic source. In contrast, German and Hebrew have richer morphologies, so one could expect that translating into them would make the model learn more about morphology.\nA possible explanation for this phenomenon is that the Arabic-English model is simply better than the Arabic-Hebrew and Arabic-German models, as hinted by the BLEU scores in Table 2. The inherent difficulty in translating Arabic to Hebrew/German may affect the ability to learn good representations of word structure. To probe this more, we trained an Arabic-Arabic autoencoder on the same training data. We found that it learns to recreate the test sentences extremely well, with very high BLEU scores (Figure 8). However, its word representations are actually inferior for the purpose of POS/morphological tagging. This implies that higher BLEU does not necessarily entail better morphological representations. In other words, a better translation model learns more informative representations, but only when it is actually learning to translate rather than merely memorizing the data as in the autoencoder case. We found this to be consistently true also for charbased experiments, and in other language pairs."}, {"heading": "5 Decoder Analysis", "text": "So far we only looked at the encoder. However, the decoder DEC is a crucial part in an MT system with access to both source and target sentences. In order to examine what the decoder learns about morphology, we first train an NMT system on the parallel corpus. Then, we use the trained model to encode a source sentence and extract features for words in the target sentence. These features are used to train a classifier on POS or morphological tagging on the target side.4 Note that in this case the decoder is given the correct target words oneby-one, similar to the usual NMT training regime.\nTable 3 (1st row) shows the results of using representations extracted with ENC and DEC from the Arabic-English and English-Arabic models, respectively. There is clearly a huge drop in representation quality with the decoder.5 At first, this drop seems correlated with lower BLEU in English to Arabic vs. Arabic to English. However, we observed similar low POS tagging accuracy using decoder representations from high-quality models. For instance, the French-to-English model obtains 37.8 BLEU, but its decoder representations give a mere 54.26 accuracy on English POS tagging.\nAs an alternative explanation for the poor quality of the decoder representations, consider the fundamental tasks of the two NMT modules: encoder and decoder. The encoder\u2019s task is to create a generic, close to language-independent representation of the source sentence, as shown by recent evidence from multilingual NMT (Johnson et al., 2016). The decoder\u2019s task is to use this representation to generate the target sentence in a specific language. Presumably, it is sufficient for the decoder to learn a strong language model to produce morphologically-correct output, without learning much about morphology, while the encoder needs to learn quite a lot about source language morphol-\n4In this section we only experiment with predicted tags for lack of available parallel data with gold POS/morph. tags.\n5Decoder results are above a majority baseline of 20%, so the decoder still learns something about the target language.\nogy in order to create a good generic representation. In the following section we show that the attention mechanism also plays an important role in the division of labor between encoder and decoder."}, {"heading": "5.1 Effect of attention", "text": "Consider the role of the attention mechanism in learning useful representations: during decoding, the attention weights are combined with the decoder\u2019s hidden states to generate the current translation. These two sources of information need to jointly point to the most relevant source word(s) and predict the next most likely word. Thus, the decoder puts significant emphasis on mapping back to the source sentence, which may come at the expense of obtaining a meaningful representation of the current word. We hypothesize that the attention mechanism hurts the quality of the target word representations learned by the decoder.\nTo test this hypothesis, we train NMT models with and without attention and compare the quality of their learned representations. As Table 3 shows (compare 1st and 2nd rows), removing the attention mechanism decreases the quality of the encoder representations, but improves the quality of the decoder representations. Without attention, the decoder is forced to learn more informative representations of the target language."}, {"heading": "5.2 Effect of word representation", "text": "We also conducted experiments to verify our findings regarding word-based versus character-based representations on the decoder side. By character representation we mean a character CNN on the input words. The decoder predictions are still done at the word-level, which enables us to use its hidden states as word representations.\nTable 4 shows POS accuracy of word- vs. charbased representations in the encoder and decoder. While char-based representations improve the encoder, they do not help the decoder. BLEU scores behave similarly: the char-based model leads to better translations in Arabic-to-English, but not\nin English-to-Arabic. A possible explanation for this is that the decoder\u2019s predictions are still done at word level even with the char-based model (which encodes the target input but not the output). In practice, this can lead to generating unknown words. Indeed, in Arabic-to-English the charbased model reduces the number of generated unknowns in the test set by 25%, while in English-toArabic the number of unknowns remains roughly the same between word- and char-based models."}, {"heading": "6 Related Work", "text": "Analysis of neural models The opacity of neural networks has motivated researchers to analyze such models in different ways. One line of work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; Ka\u0301da\u0301r et al., 2016; Qian et al., 2016a). While such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. A different approach tries to provide quantitative analysis by correlating parts of the neural network with linguistic properties, for example by training a classifier to predict features of interest. Different units have been used, from word embeddings (Ko\u0308hn, 2015; Qian et al., 2016b), through LSTM gates or states (Qian et al., 2016a), to sentence embeddings (Adi et al., 2016). Our work is most similar to Shi et al. (2016), who use hidden vectors from a neural MT encoder to predict syntactic properties on the English source side. In contrast, we focus on representations in morphologically-rich languages and evaluate both source and target sides across several criteria. Vylomova et al. (2016) also analyze different representations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations.\nWord representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT (Luong et al., 2010). Such units can be obtained in a pre-processing step \u2013 e.g. by byte-pair en-\ncoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016) \u2013 or learned during training with a character-based convolutional/recurrent sub-network (Costa-jussa\u0300 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-jussa\u0300 and Fonollosa, 2016; Jozefowicz et al., 2016). We evaluate the quality of different representations learned by an MT system augmented with a character CNN in terms of POS and morphological tagging, and contrast them with a purely wordbased system."}, {"heading": "7 Conclusion", "text": "Neural nets have become ubiquitous in machine translation due to their elegant architecture and good performance. The representations they use for linguistic units are crucial for obtaining highquality translation. In this work, we investigated how neural MT models learn word structure. We evaluated their representation quality on POS and morphological tagging in a number of languages. Our results lead to the following conclusions:\n\u2022 Character-based representations are better than word-based ones for learning morphology, especially in rare and unseen words.\n\u2022 Lower layers of the neural network are better at capturing morphology, while deeper networks improve translation performance. We hypothesize that lower layers are more focused on word structure, while higher ones are focused on word meaning.\n\u2022 Translating into morphologically-poorer languages leads to better source-side representations. This is partly correlated with BLEU.\n\u2022 The attentional decoder learns impoverished representations that do not carry much information about morphology.\nThese insights can guide further development of neural MT systems. For instance, jointly learning translation and morphology can possibly lead to better representations and improved translation. Our analysis indicates that this kind of approach\nshould take into account factors such as the encoding layer and the type of word representation.\nAnother area for future work is to extend the analysis to other representations (e.g. byte-pair encoding), deeper networks, and more semanticallyoriented tasks such as semantic parsing."}, {"heading": "A Supplementary Material", "text": "A.1 Training Details POS/Morphological classifier The classifier used for all prediction tasks is a feed-forward network with one hidden layer, dropout (\u03c1 = 0.5), a ReLU non-linearity, and an output layer mapping to the tag set (followed by a Softmax). The size of the hidden layer is set to be identical to the size of the encoder\u2019s hidden state (typically 500 dimensions). We use Adam (Kingma and Ba, 2014) with default parameters to minimize the cross-entropy objective. Training is run with mini-batches of size 16 and stopped once the loss on the dev set stops improving; we allow a patience of 5 epochs.\nNeural MT system We train a 2-layer LSTM encoder-decoder with attention. We use the seq2seq-attn implementation (Kim, 2016) with the following default settings: word vectors and LSTM states have 500 dimensions, SGD with initial learning rate of 1.0 and rate decay of 0.5, and dropout rate of 0.3. The characterbased model is a CNN with a highway network over characters (Kim et al., 2015) with 1000 feature maps and a kernel width of 6 characters. This model was found to be useful for translating morphologically-rich languages (Costa-jussa\u0300 and Fonollosa, 2016). The MT system is trained for 20 epochs, and the model with the best dev loss is used for extracting features for the classifier.\nA.2 Data and Taggers Datasets All of the translation models are trained on the Ted talks corpus included in WIT3 (Cettolo et al., 2012; Cettolo, 2016). Statistics about each language pair are available on the WIT3 website: https://wit3.fbk.eu. For experiments using gold tags, we used the Arabic Treebank for Arabic (with the versions and splits described in the MADAMIRA manual (Pasha et al., 2014)) and the Tiger corpus for German.6\nPOS and morphological taggers We used the following tools to annotate the MT corpora: MADAMIRA (Pasha et al., 2014) for Arabic POS and morphological tags, Tree-Tagger (Schmid, 1994) for Czech and French POS tags, LoPar (Schmid, 2000) for German POS and morphological tags, and MXPOST (Ratnaparkhi, 1998) for English POS tags. These tools are recommended\n6http://www.ims.uni-stuttgart.de/ forschung/ressourcen/korpora/tiger.html\non the Moses website.7 As mentioned before, our goal is not to achieve state-of-the-art results, but rather to study what different components of the NMT architecture learn about word morphology. Please refer to Mueller et al. (2013) for representative POS and morphological tagging accuracies.\nA.3 Supplementary Results We report here results that were omitted from the paper due to the space limit. Table 5 shows encoder results using different layers, languages, and representations (word/char-based). As noted in the paper, all the results consistently show that i) layer 1 performs better than layers 0 and 2; and ii) charbased representations are better than word-based for learning morphology. Table 6 shows that translating into a morphologically-poor language (English) leads to better source representations, and Table 7 provides additional decoder results.\n7http://www.statmt.org/moses/?n=Moses. ExternalTools"}], "references": [{"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1608.04207 .", "citeRegEx": "Adi et al\\.,? 2016", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Segmentation for English-to-Arabic Statistical Machine Translation", "author": ["Ibrahim Badr", "Rabih Zbib", "James Glass."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Lan-", "citeRegEx": "Badr et al\\.,? 2008", "shortCiteRegEx": "Badr et al\\.", "year": 2008}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results", "author": ["Yonatan Belinkov", "James Glass."], "venue": "Proceedings of the Workshop on Semitic Machine Translation. Association for Computational Linguistics,", "citeRegEx": "Belinkov and Glass.,? 2016", "shortCiteRegEx": "Belinkov and Glass.", "year": 2016}, {"title": "Neural versus PhraseBased Machine Translation Quality: a Case Study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "An Arabic-Hebrew parallel corpus of TED talks", "author": ["Mauro Cettolo."], "venue": "Proceedings of the AMTA Workshop on Semitic Machine Translation (SeMaT). Austin, US-TX.", "citeRegEx": "Cettolo.,? 2016", "shortCiteRegEx": "Cettolo.", "year": 2016}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT). Trento, Italy, pages 261\u2013", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Character-based Neural Machine Translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L Elman."], "venue": "Machine learning 7(2-3):195\u2013225.", "citeRegEx": "Elman.,? 1991", "shortCiteRegEx": "Elman.", "year": 1991}, {"title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "author": ["Lieke Gelderloos", "Grzegorz Chrupa\u0142a."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational", "citeRegEx": "Gelderloos and Chrupa\u0142a.,? 2016", "shortCiteRegEx": "Gelderloos and Chrupa\u0142a.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi."], "venue": "arXiv preprint arXiv:1602.08952 .", "citeRegEx": "K\u00e1d\u00e1r et al\\.,? 2016", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "Visualizing and Understanding Recurrent Networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "arXiv preprint arXiv:1506.02078 .", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Seq2seq-attn", "author": ["Yoon Kim."], "venue": "https:// github.com/harvardnlp/seq2seq-attn.", "citeRegEx": "Kim.,? 2016", "shortCiteRegEx": "Kim.", "year": 2016}, {"title": "Character-aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615 .", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Factored Translation Models", "author": ["Philipp Koehn", "Hieu Hoang."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-", "citeRegEx": "Koehn and Hoang.,? 2007", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Empirical Methods for Compound Splitting", "author": ["Philipp Koehn", "Kevin Knight."], "venue": "10th Conference of the European Chapter of the Association for Computational Linguistics. pages 187\u2013194. http://www.aclweb.org/anthology/E03-1076.", "citeRegEx": "Koehn and Knight.,? 2003", "shortCiteRegEx": "Koehn and Knight.", "year": 2003}, {"title": "What\u2019s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation", "author": ["Arne K\u00f6hn."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "K\u00f6hn.,? 2015", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "Stanford Neural Machine Translation Systems for Spoken Language Domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the International Workshop on Spoken Language Translation. Da Nang, Vietnam.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "author": ["Minh-Thang Luong", "D. Christopher Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages", "author": ["Minh-Thang Luong", "Preslav Nakov", "Min-Yen Kan."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Luong et al\\.,? 2010", "shortCiteRegEx": "Luong et al\\.", "year": 2010}, {"title": "Efficient Higher-Order CRFs for Morphological Tagging", "author": ["Thomas Mueller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Mueller et al\\.,? 2013", "shortCiteRegEx": "Mueller et al\\.", "year": 2013}, {"title": "Improving SMT quality with morpho-syntactic analysis", "author": ["Sonja Nieflen", "Hermann Ney."], "venue": "COLING 2000 Volume 2: The 18th International Conference on Computational Linguistics. http://www.aclweb.org/anthology/C00-2162.", "citeRegEx": "Nieflen and Ney.,? 2000", "shortCiteRegEx": "Nieflen and Ney.", "year": 2000}, {"title": "MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation", "author": ["Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "Analyzing Linguistic Knowledge in Sequential Model of Sentence", "author": ["Peng Qian", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Qian et al\\.,? 2016a", "shortCiteRegEx": "Qian et al\\.", "year": 2016}, {"title": "Investigating Language Universal and Spe", "author": ["Peng Qian", "Xipeng Qiu", "Xuanjing Huang"], "venue": null, "citeRegEx": "Qian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2016}, {"title": "Maximum Entropy Models for Natural Language Ambiguity Resolution", "author": ["Adwait Ratnaparkhi."], "venue": "Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.", "citeRegEx": "Ratnaparkhi.,? 1998", "shortCiteRegEx": "Ratnaparkhi.", "year": 1998}, {"title": "Part-of-Speech Tagging with Neural Networks", "author": ["Helmut Schmid."], "venue": "Proceedings of the 15th International Conference on Computational Linguistics (Coling 1994). Coling 1994 Organizing Committee, Kyoto, Japan, pages 172\u2013176.", "citeRegEx": "Schmid.,? 1994", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "LoPar: Design and Implementation", "author": ["Helmut Schmid."], "venue": "Bericht des Sonderforschungsbereiches \u201cSprachtheoretische Grundlagen fr die Computerlinguistik\u201d 149, Institute for Computational Linguistics, University of Stuttgart.", "citeRegEx": "Schmid.,? 2000", "shortCiteRegEx": "Schmid.", "year": 2000}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does String-Based Neural MT Learn Source Syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "author": ["Ekaterina Vylomova", "Trevor Cohn", "Xuanli He", "Gholamreza Haffari."], "venue": "arXiv preprint arXiv:1606.04217 .", "citeRegEx": "Vylomova et al\\.,? 2016", "shortCiteRegEx": "Vylomova et al\\.", "year": 2016}, {"title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."], "venue": "Transactions of the Association for Computational Linguistics 4:371\u2013383.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 192, "endOffset": 239}, {"referenceID": 33, "context": "Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the attention mechanism (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 192, "endOffset": 239}, {"referenceID": 20, "context": "The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation (Luong and Manning, 2015; Bentivogli et al., 2016).", "startOffset": 124, "endOffset": 174}, {"referenceID": 4, "context": "The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation (Luong and Manning, 2015; Bentivogli et al., 2016).", "startOffset": 124, "endOffset": 174}, {"referenceID": 32, "context": "Recent work has started exploring the role of the NMT encoder in learning source syntax (Shi et al., 2016), but research studies are yet to answer important questions such", "startOffset": 88, "endOffset": 106}, {"referenceID": 33, "context": "2) (Sutskever et al., 2014):", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "In this work, we use long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) Figure 1: Illustration of our approach: (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features.", "startOffset": 51, "endOffset": 85}, {"referenceID": 2, "context": "encoder-decoders with attention (Bahdanau et al., 2014), which we train on parallel data.", "startOffset": 32, "endOffset": 55}, {"referenceID": 26, "context": "We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings.", "startOffset": 125, "endOffset": 145}, {"referenceID": 15, "context": "model we adopt a convolutional neural network (CNN) over character embeddings that is also learned during training (Kim et al., 2015; Costajuss\u00e0 and Fonollosa, 2016); see appendix A.", "startOffset": 115, "endOffset": 165}, {"referenceID": 25, "context": "1 on Arabic POS/morphology (Pasha et al., 2014)), indicating that NMT models learn quite good representations.", "startOffset": 27, "endOffset": 47}, {"referenceID": 35, "context": "Modern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016).", "startOffset": 73, "endOffset": 109}, {"referenceID": 9, "context": "A similar pattern was recently observed in a joint language-vision deep recurrent net (Gelderloos and Chrupa\u0142a, 2016).", "startOffset": 86, "endOffset": 117}, {"referenceID": 8, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 13, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 12, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 26, "context": "work visualizes hidden unit activations in recurrent neural networks that are trained for a given task (Elman, 1991; Karpathy et al., 2015; K\u00e1d\u00e1r et al., 2016; Qian et al., 2016a).", "startOffset": 103, "endOffset": 179}, {"referenceID": 19, "context": "Different units have been used, from word embeddings (K\u00f6hn, 2015; Qian et al., 2016b), through LSTM gates or states (Qian et al.", "startOffset": 53, "endOffset": 85}, {"referenceID": 26, "context": ", 2016b), through LSTM gates or states (Qian et al., 2016a), to sentence embeddings (Adi et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 0, "context": ", 2016a), to sentence embeddings (Adi et al., 2016).", "startOffset": 33, "endOffset": 51}, {"referenceID": 0, "context": ", 2016a), to sentence embeddings (Adi et al., 2016). Our work is most similar to Shi et al. (2016), who use hidden vectors from a neural MT encoder to", "startOffset": 34, "endOffset": 99}, {"referenceID": 34, "context": "Vylomova et al. (2016) also analyze different representations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations.", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 198, "endOffset": 264}, {"referenceID": 18, "context": "Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 198, "endOffset": 264}, {"referenceID": 1, "context": "Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Knight, 2003; Badr et al., 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 198, "endOffset": 264}, {"referenceID": 17, "context": ", 2008) and factored translation models (Koehn and Hoang, 2007).", "startOffset": 40, "endOffset": 63}, {"referenceID": 22, "context": "Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT (Luong et al., 2010).", "startOffset": 135, "endOffset": 155}, {"referenceID": 31, "context": "by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 7, "context": "ing with a character-based convolutional/recurrent sub-network (Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016).", "startOffset": 63, "endOffset": 144}, {"referenceID": 21, "context": "ing with a character-based convolutional/recurrent sub-network (Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016).", "startOffset": 63, "endOffset": 144}, {"referenceID": 34, "context": "ing with a character-based convolutional/recurrent sub-network (Costa-juss\u00e0 and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016).", "startOffset": 63, "endOffset": 144}, {"referenceID": 15, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}, {"referenceID": 3, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}, {"referenceID": 7, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}, {"referenceID": 11, "context": "Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016; Jozefowicz et al., 2016).", "startOffset": 98, "endOffset": 200}], "year": 2017, "abstractText": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.", "creator": "LaTeX with hyperref package"}}}