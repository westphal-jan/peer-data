{"id": "1704.03718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Deep Extreme Multi-label Learning", "abstract": "extreme negative multi - label learning or classification has been a practical and important problem formulation since the negative boom of big data. yet the main challenge lies in the exponential label space which involves 2l possible label sets so when drawing the label dimension l is very large e. g. 0 in millions for future wikipedia labels. alternatively this paper approach is motivated to better explore the label conceptual space then by build - ing and modeling an explicit label graph. in the meanwhile, deep statistical learning has been widely studied and used in various classification problems includ - ing multi - label classification, however it has not been sufficiently loudly studied in this extreme but significant practi - cal case, where commonly the noisy label space can be as large as in millions. in this paper, we propose a practical prototype deep embedding method for extreme multi - label classifi - cation. our method typically harvests the ideas of non - linear embedding and requires modeling label space with graph priors at the same time. extensive experiments on public datasets for xml show that our implementation method per - form competitively against state - of - the - art result.", "histories": [["v1", "Wed, 12 Apr 2017 12:09:40 GMT  (367kb,D)", "http://arxiv.org/abs/1704.03718v1", "7 pages, 7 figures"], ["v2", "Mon, 11 Sep 2017 07:46:31 GMT  (262kb,D)", "http://arxiv.org/abs/1704.03718v2", "9 pages, 7 figures"], ["v3", "Thu, 19 Oct 2017 15:59:32 GMT  (287kb,D)", "http://arxiv.org/abs/1704.03718v3", "9 pages, 8 figures"]], "COMMENTS": "7 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wenjie zhang", "liwei wang", "xiangfeng wang", "junchi yan", "hongyuan zha"], "accepted": false, "id": "1704.03718"}, "pdf": {"name": "1704.03718.pdf", "metadata": {"source": "CRF", "title": "Deep Extreme Multi-label Learning", "authors": ["Wenjie Zhang", "Liwei Wang", "Junchi Yan", "Xiangfeng Wang", "Hongyuan Zha"], "emails": ["izhangwenjie@gmail.com", "lwang97@illinois.edu", "jcyan@sei.ecnu.edu.cn", "xfwang@sei.ecnu.edu.cn", "zha@sei.ecnu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In machine learning field, eXtreme Multi-label Learning (XML) addresses the problem of learning a classifier that can automatically tag a data sample with the most relevant subset of labels from a large label set. For instance, there are more than a million labels (i.e. categories) on Wikipedia and one may wish to build a classifier that can annotate a new article or web page with a subset of most relevant Wikipedia categories. Extreme multi-label learning or specifically classification, is a very challenging research problem for the need to simultaneously deal with massive labels, dimensions and training points. Compared with traditional multi-label classification methods [Tsoumakas and Katakis, 2006], extreme multi-label classification methods focus on tackling the problem of extremely high input dimensions for both input feature dimension and label dimension. It should also be emphasized that multi-label learning is distinct from multi-class classification [Wu et al., 2004] whose aim is to predict a single mutually exclusive label. In contrast, XML allows for the coexistence of more than one labels for a single data sample.\nOne straightforward method for classification is to train an independent one-against-all classifier for each label dimension. Regrettably, in the context of extreme multi-label classification, this is not feasible since it will be almost computationally intractable to train a massive number of e.g. one million classifiers. The issue could be ameliorated if a label hierarchy was provided. However, such a hierarchy is often unavailable in many applications [Bhatia et al., 2015]. The pain also lies in the prediction stage, where all the classifiers need to be evaluated for each testing data sample. To address these challenges, state-of-the-art extreme multi-label classification methods have been proposed recently, which in general can be divided into two categories: tree based methods and embedding based methods.\nTree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods. The idea is to learn a hierarchy from the training data. The root is initialized to contain the whole label set. A node partition formulation is then optimized to determine which labels should be assigned to the left child or to the right. Nodes are recursively partitioned till each leaf contains a small number of labels. In the prediction stage, a testing data sample is passed down the tree until it arrives at the leaf nodes whereby its predicted labels are finally determined.\nAnother important line of research is the embedding based method. These approaches attempt to make training and prediction tractable by assuming that the training label matrix (of which each column/row corresponds a training sample\u2019s label vector) is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimension linear subspace. While for prediction, labels for a novel sample are predicted by post-processing where a decompression matrix lifts the embedded label vectors back to the original extremely high dimensional label space. However, the fundamental problem of the embedding method is the low-rank label space assumption, which is violated in most real world applications. In generally, traditional embedding methods are more efficient for computing than tree based method at the expense of lower accuracy.\nNotably, the state-of-the-art embedding based method SLEEC (Sparse Local Embeddings for Extreme Multi-label Classification) [Bhatia et al., 2015] achieves significant accuracy gain while still being computationally economical,\nar X\niv :1\n70 4.\n03 71\n8v 1\n[ cs\n.L G\n] 1\n2 A\npr 2\n01 7\nwhich is attributed to the non-linearity modeled through neighbourhood preserving constraints. This inspires us to take the step further along this direction. It is intriguing to take a deep neural network approach for non-linearity modeling. However, to the best of our knowledge, there is very few prior art on deep learning for XML.\nPerhaps more importantly, we make an observation that the label graph structure is also very important in those tree based methods where the node is each label dimension. However, they seem to be totally ignored in state-of-the-art embedding methods. For instance, SLEEC focuses on dimension reduction on the raw label matrix (of which each column/row corresponds a training sample\u2019s label vector) rather than modeling of the label graph structure as mentioned above.\nIn this paper, we propose a deep learning based approach for the extreme multi-label classification problem. We extend traditional deep learning framework for multi-label classification with millions of labels via building non-linear embedding for both feature and label space. As far as we know, our method is the first that models the feature space non-linearity and label structure at the same time in solving this problem. In particular, a deep learning model is devised.\nContribution In a nutshell, the contributions are: \u2022 To the best of our knowledge, this is the first work for\nexplicit label graph structure modeling for the extreme multi-label classification problem. Note the label hierarchy explored by the tree based methods is different from label graph. We also present the first deep neural network to solve the label embedding task for the XML problem. The network is devised based on a factorization perspective to the label graph \u2013 see Fig.1. \u2022 This paper is also the first XML model with a deep neu-\nral network for joint embedding of feature space and label space \u2013 see Fig.2. In fact, no prior art is identified for exploring deep learning neither for feature space reduction nor for label space reduction in the XML setting. \u2022 Our model achieves state-of-the-art accuracy on vari-\nous public benchmark and the source code will be made available with release of the paper for reproducibility."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Classification methods for XML", "text": "As the XML problem is in general algorithmically intractable for one-against-all classifier 1, as mentioned in Section 1, various tree based and embedding based models are devised.\nTree based methods for XML The label partitioning by sub-linear ranking (LPSR) method [Weston et al., 2013] is focused on reducing the prediction time by learning a hierarchy over a base classifier. While it can also incur quite expensive costs since it needs to learn the hierarchy in addition to the base classifier. The multi-label random forest method (MLRF) [Agrawal et al., 2013] seeks to learn an ensemble\n1It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al., 2015].\nof randomized trees instead of relying on the learning of a base classifier. Both MLRF and LPSR suffer the high training costs. FastXML [Prabhu and Varma, 2014] is proposed to learn a hierarchy not over the label space but over the feature space. FastXML defines the set of labels active in a region to be the union of the labels of all training points present in that region. Predictions are made by returning the ranked list of the most frequently occurring labels in all the leaf nodes in the ensemble containing the novel point.\nEmbedding methods for XML To make training and prediction tractable, embedding based approaches aim to reduce the effective number of labels, which is often based on the low-rank assumption. Specifically, given I training samples (Xi,Yi) for the feature vector Xi \u2208 Rm and the label vector Yi \u2208 {0, 1}n whereby m and n are supposed both to be very high, embedding methods linearly project the label vectors onto a lower k-dimension space: yi = UYi for yi \u2208 Rk and k m,n. Then a regressor is learned for the mapping between yi and Xi. Labels for a testing sample X\u0304 is predicted by lifting the predicted low-dimension y\u0304 label to the original label space Y\u0304 via linear multiplication.\nIn fact, the main difference of existing embedding models often lies in the choice of compression and decompression techniques for embedding and lifting, including compressed sensing [Hsu et al., 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al., 2013]. The accuracy for embedding methods achieve significant gain by the recently proposed embedding method SLEEC [Bhatia et al., 2015]. It differs from the previous embedding methods in two facts: i) for training, instead of training a global projection matrix onto a linear low-rank subspace, it incorporates the non-linear neighborhood constraints in the low-dimension embedding space; ii) for prediction, rather than using a decompression matrix for dimension lifting, it uses a simple k-nearest neighbor (k-NN) classifier in the embedding space. Our method falls into the embedding approaches, while we take one step further by exploring neural networks to model the label space structure as well as for feature space embedding."}, {"heading": "2.2 Traditional multi-label classification", "text": "Multi-label classification (MLC) is a classic problem in machine learning. Traditionally this problem is tackled with a moderate number of labels [Tsoumakas and Katakis, 2006]. This makes it different from the XML problem where it involves millions of or more labels for each sample. Early MLC methods [Boutell et al., 2004] transform the MLC problem either into one or more single-label classification or regression problems. Recent approaches [Cheng et al., 2010; Bi and Kwok, 2011] try to solve the multi-label learning directly. However, when the number of labels grows rapidly, these methods can easily become computationally unaffordable. For instance, for tree based models for traditional MLC [Zhang and Zhang, 2010; Bi and Kwok, 2011], with the large feature dimension and the huge samples number, the trees will be giant which leads to the intractability for training. There is also a principled generalization for the naive one-against-all method [Hariharan et al., 2010] which cost-\neffectively explores the label priors. Same as 1-vs-all, the method is algorithmically not scalable to XML.\nMeanwhile, in cross modal retrieval field, deep neural network [Wang et al., 2016] has been designed for learning the shared embedding space between images and texts. However, the input features of text are usually with small dimensions and therefore, the existing deep embedding framework can not be directly applied in the extreme multi-label settings."}, {"heading": "3 Deep Learning for XML", "text": "Our approach involves the training and prediction stage. In the training stage, via two neural networks, we map the highdimensional feature and high-dimensional label vectors into a common embedding space. In the prediction stage, a standard k-NN based classifier is used in the embedded feature space to determine the final label predictions."}, {"heading": "3.1 Training stage", "text": "Label space embedding We train a neural network for matrix factorization as shown in Fig.1 to model the label graph structure of the training data. Specifically, the input of p and q can be simply the row and column index of the nonzero entries in the label graph matrix M \u2208 Rn\u00d7n (we will show its definition later in this section) where n is the number of dimensions for the label space. While the other inputM(p, q) is the corresponding entry value in the corresponding position.\nFeature embedding Given the low-dimension representation of Y \u2208 {0, 1}1\u00d7n by y\u2032=YP where P \u2208 Rn\u00d7k can be computed by the learned weights in the embedding layer in\nFig.1, another network is trained to find a mapping that the sample\u2019s feature vector is embedded into the same space with the low-dimension representation of Y. Moreover x and y are close to each other as measured by the `2 norm: \u2016x\u2212y\u20162. The hope is that such an embedding will be effective as it involves the information from the labels. The feature embedding network is shown in Fig.2.\nThe working flow of our method is shown in Algorithm 1, termed by DXML (Deep embedding for XML classification).\nComments Here we would like to clarify some technical points for readers to better understand our approach.\ni) Definition of the label graph matrix M: one important motivation of this paper is to explore the label information of the training data. Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al., 2015; Tu et al., 2016] to define our label graph matrix by:\nM = (A + A2)/2 (1)\nwhere A is the adjacency matrix derived from the label graph structure as exemplified in Fig.3. Each node in the label graph\nAlgorithm 1 Deep embedding for XML classification. 1: Training stage 2: Build M by label graph; 3: Train the matrix factorization deep neural network as\nshown in Fig.1 to obtain P and Q. The loss is the `2 norm linear regression loss for \u2016Mij \u2212 Pi \u2217Qj\u20162; 4: Project the sample label vector Y by y\u2032 = YP; 5: Train the joint embedding deep neural network shown in\nFig.2 to obtain the mapping from raw feature X to embedded feature x. The loss is also the `2 norm linear regression loss in the embedding space \u2016x\u2212 y\u20162;\n6: Prediction stage 7: Y = \u2211 i:xi\u2208kNN(x) Yi\ndenotes a label dimension and there is an edge if the two connecting label dimensions co-appear on at least one sample.\nWe briefly explain the background for defining M via Eq.1 and the readers are referred to [Yang et al., 2015] for details. In [Yang et al., 2015], the authors first prove the state-of-theart network representation method DeepWalk [Perozzi et al., 2014] is equivalent to perform matrix factorization on a matrix M\u0304 \u2208 R|V |\u00d7|V | where each entry M\u0304ij is logarithm of the average probability that vertex vi randomly walks to vertex vj in t steps. The rigorous definition is [Yang et al., 2015]:\nM\u0304ij = [ei(A + A2 + \u00b7+ At)]j/t (2)\nwhere At is the multiplication of matrix A by t times, and ei denotes an indicator vector, in which the i-th entry is 1 and the others are all zero. We simplify Eq.2 in line with [Yang et al., 2015] since log(M\u0304) has much more non-zero entries than M\u0304 and the complexity of matrix factorization is proportional to the number of non-zero entries. This tradeoff is further verified in recent work [Tu et al., 2016] for text representation.\nii) Motivation for network in Fig.1 Roughly speaking, the network in Fig.1 can be regarded as an improvement over the traditional matrix factorization task \u2013 one classic formulation\nis as follows, which can be readily solved via off-the-shelf solvers e.g. the Alternating Minimization Algorithm (AMA) [Sullivan, 1998] given the real-valued matrix M:\nmin P,Q \u2016M\u2212 PQ\u20162F +\n\u03b2 2 (\u2016P\u20162F + \u2016Q\u20162F ) (3)\nwhere the norm is the Frobenius norm. We set the regularization weight \u03b2 = 0.001 in this paper.\nHowever, the neural network embodiment is more flexible as it can involve nonlinearity e.g. the ReLU [Glorot et al., 2015] (speedup convergence) modules in in Fig.1, with the potential to improve the representation power. This improvement is also verified in our test as will be shown later in the paper.\niii) Why not a siamese network is used in Fig.2 There are various siamese (twin) network architectures used in many recent work on deep learning [Wang et al., 2016; Bertinetto et al., 2016]. However, here our loss is the `2 regression loss rather than the popular ranking loss used in [Wang et al., 2016] as we cannot efficiently derive useful ranking information from the training data2. There can incur degenerating cases as the two branches both tend to map the input vector into vectors with zero-valued entries which incurs training difficulty. Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.1 to find the embedding vector for the label space. This also makes the training more efficient in Fig.2. We leave the siamese network for XML in future work."}, {"heading": "3.2 Prediction stage", "text": "The prediction is relatively simple and standard, which is in line with SLEEC [Bhatia et al., 2015]: given a test sample,\n2In the XML setting, similar to [Wang et al., 2016], the positive/negataive sample can be defined with a certain label dimension. However, as there are millions of label dimensions, the number of derived postive/negative sample pair can be intimidating.\nwe perform k-NN search in its low-dimensional feature representation to find its similar samples from the training dataset. Then the average (or simply sum) of the k-nearest neighbors\u2019 labels is set as final label prediction."}, {"heading": "4 Experiments", "text": "Experiments are carried out on publicly available XML benchmark data from the Extreme Classification repository3. It includes both small-scale dataset [Prabhu and Varma, 2014] and large-scale dataset [Bhatia et al., 2015], in comparison with state-of-the-art peer methods for both embedding based and tree based models. The source code of our method will be released with the publication of this paper."}, {"heading": "4.1 Protocol", "text": "Platform The experiments are implemented under CentOS6.5 64-bit system, with Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz \u00d76 CPU, GeForce GTX TITAN-X and 32G RAM. The code is written in Python 2.7 and the network is built by MXNet4 [Chen et al., 2015], which is a flexible and efficient library for deep learning and has been chosen by Amazon as the official deep learning framework for its web service.\nDatasets The tested multi-label datasets include WikiLSHTC (320K labels), DeliciousLarge [Wetzker et al., 2008] (200K labels) and Wiki10 [Zubiaga, 2012] (30K labels). All the datasets are publicly available. It should be noted that, some other methods do not scale well on such large datasets. Therefore, we also present comparisons on public relatively small datasets such as BibTex [Katakis et al., 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al., 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al., 2008] and EURLex [Menca and Furnkranz, 2008]. The statistics of these benchmarks are listed in Table 1.\nBaseline algorithms for comparison Our primary focus is to compare with those state-of-the-art extreme multi-label classification methods, such as embedding based methods SLEEC [Bhatia et al., 2015], LEML [Yu et al., 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al., 2013]. Our method can be considered as natural combination of label graph and deep embedding. Tech-\n3http://research.microsoft.com/enus/um/people/manik/downloads/XC/XMLRepository.html\n4https://github.com/dmlc/mxnet\nniques such as compressed sensing (CS) [Hsu et al., 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al., 2012] can only be trained on small datasets using commodity computational hardware.\nHyper-parameter setting For our method, we set the embedding dimension to be 100 for the small data sets and 150 for the large dataset. The structure of the network for all tests are kept the same as shown in Fig.1 and Fig.2. we train our networks using SGD (stochastic gradient descent) with momentum 0.9, weight decay 0.0005 and the fixed learning rate 0.002. In order to reduce the risk of over-fitting, we also use a Dropout layer after ReLU with probability = 0.2. The remaining two significant hyper-parameters, the number of hidden neurons and batch size in the network and the k in k-NN are all set by cross-validation on a validation set.\nEvaluation metrics The evaluation metric in [Bhatia et al., 2015] is precison@k, Precision at k (P@k)has been widely adopted as the choice of metric for evaluating extreme multilabel algorithms. The precision @k is the fraction of correct positive labels, which is the number of correct predictions over k. It decreases as k increases. Such a metric encourages the correct label to be ranked higher: p@k = #correctk ."}, {"heading": "4.2 Results and discussion", "text": "We report the experimental results on various datasets. In general, our method DXML outperform almost all other methods except for SLEEC.\nResults on small datasets From Table 2, one can see that our method DXML can mostly be ranked as top 2 on all the four datasets. On MediaMill, DXML outperforms LEML and FastXML by nearly 4% for P@3 and P@5. On EurLEX, DXML outperforms LEML and FastXML by around 10% and 5% respectively. While on Bibtex, it slightly underperforms other methods. The main reason we conjecture is that the Bibtex dataset has only 4,880 samples which can be an issue for training our deep neural network based approach.\nResults on large datasets As observed from Table 3, on the dataset Wiki10, DXML outperforms LEML and LPSRNB by about 10% regarding with P@1. While our method slightly outperforms LFML and FastXML on the DeliciousLarge dataset, so for the results on the WikiLSHTC dataset.\nResults for ablation test We also perform ablation test to study the efficacy of computing the projection matrix P via our neural network in Fig.1 compared with the matrix fac-\ntorization baseline (solving Eq.3 by the AMA solver [Sullivan, 1998]). As shown in Fig.4 our neural network based model (abbreviated by nn) outperforms the matrix factorization based one (abbreviated by mf).\nOur discussion based on the results is as follow: Our deep learning based model DXML shows competitiveness not only on large datasets but also smaller ones. In particular, we achieve comparable results with the state-of-theart method SLEEC. While our method exhibits certain nice properties over SLEEC: i) Compared with SLEEC, DXML has better flexibility and scalability. More specically, we think our network in Fig.2 can adapt to different domains by using RNN for time series, CNN for image. Also we can improve the classication accuracy by increasing the network capacity; ii) For new arrival data, SLEEC has to be trained from scratch. In contrast, DXML allows for incremental learning. iii) DXML can be trained with off-the-shelf stochastic optimization solvers and its memory cost can be flexibly adjusted\nby the mini-batch size. Especially, with the advent of GPU era, our methods is more applicable in real industrial usage."}, {"heading": "5 Conclusion", "text": "For the extreme multi-label learning (XML) problem, this paper starts with modeling the large-scale label space via the label graph. In contrast, existing XML methods either explore the label hierarchy as done by many tree based method or perform dimension reduction on the raw label/sample matrix. Moreover, a deep neural network is devised to explore the label space effectively. We also explore deep neural network for learning the embedding function for the feature space as induced by the embedding label space. Extensive experimental results corroborate the efficacy of our method. We leave for future work for more advanced training mechanism for end-to-end deep learning paradigm for XML classification."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Rahul Agrawal", "Archit Gupta", "Yashoteja Prabhu", "Manik Varma"], "venue": "WWW,", "citeRegEx": "Agrawal et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Dismec-distributed sparse machines for extreme multi-label classification", "author": ["Rohit Babbar", "Bernhard Shoelkopf"], "venue": "WSDM,", "citeRegEx": "Babbar and Shoelkopf. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "The landmark selection method for multiple output prediction", "author": ["Krishnakumar Balasubramanian", "Guy Lebanon"], "venue": "ICML,", "citeRegEx": "Balasubramanian and Lebanon. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Fullyconvolutional siamese network for object tracking", "author": ["Luca Bertinetto", "Jack Valmadre", "Joao F. HenriquesAndrea", "Vedaldi", "Philip H.S. Torr"], "venue": "ECCV,", "citeRegEx": "Bertinetto et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Prateek Jain", "Manik Varma"], "venue": "NIPS,", "citeRegEx": "Bhatia et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-label classification on tree-and dag-structured hierarchies", "author": ["Wei Bi", "James T Kwok"], "venue": "ICML,", "citeRegEx": "Bi and Kwok. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient multi-label classification with many labels", "author": ["Wei Bi", "James Tin-Yau Kwok"], "venue": "ICML,", "citeRegEx": "Bi and Kwok. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Pattern recognition", "author": ["Matthew R Boutell", "Jiebo Luo", "Xipeng Shen", "Christopher M Brown. Learning multi-label scene classification"], "venue": "37(9):1757\u20131771,", "citeRegEx": "Boutell et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Featureaware label space dimension reduction for multi-label classification", "author": ["Yao-Nan Chen", "Hsuan-Tien Lin"], "venue": "NIPS,", "citeRegEx": "Chen and Lin. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "arXiv preprint arXiv:1512.01274,", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["Weiwei Cheng", "Eyke H\u00fcllermeier", "Krzysztof J Dembczynski"], "venue": "ICML,", "citeRegEx": "Cheng et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust bloom filters for large multilabel classification tasks", "author": ["Moustapha M Cisse", "Nicolas Usunier", "Thierry Artieres", "Patrick Gallinari"], "venue": "NIPS,", "citeRegEx": "Cisse et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "AISTATS,", "citeRegEx": "Glorot et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale max-margin multi-label classification with priors", "author": ["Bharath Hariharan", "Lihi Zelnik-Manor", "Manik Varma", "Svn Vishwanathan"], "venue": "ICML,", "citeRegEx": "Hariharan et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine learning", "author": ["Bharath Hariharan", "SVN Vishwanathan", "Manik Varma. Efficient max-margin multi-label classification with applications to zero-shot learning"], "venue": "88(1-2):127\u2013155,", "citeRegEx": "Hariharan et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S. Kakade", "J. Langford", "T. Zhang"], "venue": "NIPS", "citeRegEx": "Hsu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Ioannis Katakis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "ECML PKDD discovery challenge,", "citeRegEx": "Katakis et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain", "author": ["E. Menca", "J. Furnkranz"], "venue": "ECML/PKDD", "citeRegEx": "Menca and Furnkranz. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Deepwalk: Online learning of social representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "KDD,", "citeRegEx": "Perozzi et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Fastxml: a fast", "author": ["Yashoteja Prabhu", "Manik Varma"], "venue": "accurate and stable tree-classifier for extreme multi-label learning. In KDD,", "citeRegEx": "Prabhu and Varma. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The challenge problem for automated detection of 101 semantic concepts in multimedia", "author": ["Cees GM Snoek", "Marcel Worring", "Jan C Van Gemert", "Jan-Mark Geusebroek", "Arnold WM Smeulders"], "venue": "ACM-MM,", "citeRegEx": "Snoek et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Alternating minimization algorithms: from blahut-arimoto to expectationmaximization", "author": ["J. Sullivan"], "venue": "A. Vardy, Ed., Codes, Curves, and Signals: Common Threads in Communications", "citeRegEx": "Sullivan. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Neural Computation", "author": ["Farbound Tai", "Hsuan-Tien Lin. Multilabel classification with principal label space transformation"], "venue": "24(9):2508\u20132542,", "citeRegEx": "Tai and Lin. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining, 3(3),", "citeRegEx": "Tsoumakas and Katakis. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Effective and effcient multilabel classification in domains with large number of labels", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "ECML/PKDD", "citeRegEx": "Tsoumakas et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Max-margin deepwalk: Discriminative learning of network representation", "author": ["Cunchao Tu", "Weicheng Zhang", "Zhiyuan Liu", "Maosong Sun"], "venue": "IJCAI,", "citeRegEx": "Tu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["Liwei Wang", "Yin Li", "Svetlana Lazebnik"], "venue": "CVPR,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Label partitioning for sublinear ranking", "author": ["Jason Weston", "Ameesh Makadia", "Hector Yee"], "venue": "ICML,", "citeRegEx": "Weston et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Analyzing social bookmarking systems: A del", "author": ["Robert Wetzker", "Carsten Zimmermann", "Christian Bauckhage"], "venue": "icio. us cookbook. In ECAI Mining Social Data Workshop,", "citeRegEx": "Wetzker et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Journal of Machine Learning Research", "author": ["Ting-Fan Wu", "Chih-Jen Lin", "Ruby C Weng. Probability estimates for multi-class classification by pairwise coupling"], "venue": "5(Aug):975\u2013 1005,", "citeRegEx": "Wu et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Network representation learning with rich text information", "author": ["Cheng Yang", "Zhiyuan Liu", "Deli Zhao", "Maosong Sun", "Edward Y. Chang"], "venue": "IJCAI,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ICML", "author": ["Hsiang-Fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit S Dhillon. Large-scale multi-label learning with missing labels"], "venue": "pages 593\u2013601,", "citeRegEx": "Yu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In AISTATS", "author": ["Yi Zhang", "Jeff G Schneider. Multi-label output codes using canonical correlation analysis"], "venue": "pages 873\u2013882,", "citeRegEx": "Zhang and Schneider. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Multilabel learning by exploiting label dependency", "author": ["Min-Ling Zhang", "Kun Zhang"], "venue": "KDD,", "citeRegEx": "Zhang and Zhang. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhancing navigation on wikipedia with social tags", "author": ["Arkaitz Zubiaga"], "venue": "arXiv preprint arXiv:1202.5469,", "citeRegEx": "Zubiaga. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Compared with traditional multi-label classification methods [Tsoumakas and Katakis, 2006], extreme multi-label classification methods focus on tackling the problem of extremely high input dimensions for both input feature dimension and label dimension.", "startOffset": 61, "endOffset": 90}, {"referenceID": 29, "context": "It should also be emphasized that multi-label learning is distinct from multi-class classification [Wu et al., 2004] whose aim is to predict a single mutually exclusive label.", "startOffset": 99, "endOffset": 116}, {"referenceID": 4, "context": "However, such a hierarchy is often unavailable in many applications [Bhatia et al., 2015].", "startOffset": 68, "endOffset": 89}, {"referenceID": 27, "context": "Tree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods.", "startOffset": 19, "endOffset": 86}, {"referenceID": 0, "context": "Tree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods.", "startOffset": 19, "endOffset": 86}, {"referenceID": 19, "context": "Tree based methods [Weston et al., 2013; Agrawal et al., 2013; Prabhu and Varma, 2014] have become popular as they enjoy notable accuracy improvement over traditional embedding methods.", "startOffset": 19, "endOffset": 86}, {"referenceID": 4, "context": "Notably, the state-of-the-art embedding based method SLEEC (Sparse Local Embeddings for Extreme Multi-label Classification) [Bhatia et al., 2015] achieves significant accuracy gain while still being computationally economical, ar X iv :1 70 4.", "startOffset": 124, "endOffset": 145}, {"referenceID": 27, "context": "Tree based methods for XML The label partitioning by sub-linear ranking (LPSR) method [Weston et al., 2013] is focused on reducing the prediction time by learning a hierarchy over a base classifier.", "startOffset": 86, "endOffset": 107}, {"referenceID": 0, "context": "The multi-label random forest method (MLRF) [Agrawal et al., 2013] seeks to learn an ensemble", "startOffset": 44, "endOffset": 66}, {"referenceID": 1, "context": "It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al.", "startOffset": 47, "endOffset": 75}, {"referenceID": 19, "context": "It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al.", "startOffset": 255, "endOffset": 279}, {"referenceID": 4, "context": "It is also worth noting that the recent effort [Babbar and Shoelkopf, 2017] shows that, via intensive system level parallelization, a careful implementation of one-vs-rest mechanism is attainable with competitive accuracy against state-of-the-art FastXML [Prabhu and Varma, 2014] and SLEEC [Bhatia et al., 2015].", "startOffset": 290, "endOffset": 311}, {"referenceID": 19, "context": "FastXML [Prabhu and Varma, 2014] is proposed to learn a hierarchy not over the label space but over the feature space.", "startOffset": 8, "endOffset": 32}, {"referenceID": 15, "context": "In fact, the main difference of existing embedding models often lies in the choice of compression and decompression techniques for embedding and lifting, including compressed sensing [Hsu et al., 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 183, "endOffset": 201}, {"referenceID": 32, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 22, "endOffset": 49}, {"referenceID": 22, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 2, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 92, "endOffset": 146}, {"referenceID": 6, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al.", "startOffset": 92, "endOffset": 146}, {"referenceID": 11, "context": ", 2009], output codes [Zhang and Schneider, 2011], SVD [Tai and Lin, 2012], landmark labels [Balasubramanian and Lebanon, 2012; Bi and Kwok, 2013], Bloom filters [Cisse et al., 2013].", "startOffset": 162, "endOffset": 182}, {"referenceID": 4, "context": "The accuracy for embedding methods achieve significant gain by the recently proposed embedding method SLEEC [Bhatia et al., 2015].", "startOffset": 108, "endOffset": 129}, {"referenceID": 23, "context": "Traditionally this problem is tackled with a moderate number of labels [Tsoumakas and Katakis, 2006].", "startOffset": 71, "endOffset": 100}, {"referenceID": 7, "context": "Early MLC methods [Boutell et al., 2004] transform the MLC problem either into one or more single-label classification or regression problems.", "startOffset": 18, "endOffset": 40}, {"referenceID": 10, "context": "Recent approaches [Cheng et al., 2010; Bi and Kwok, 2011] try to solve the multi-label learning directly.", "startOffset": 18, "endOffset": 57}, {"referenceID": 5, "context": "Recent approaches [Cheng et al., 2010; Bi and Kwok, 2011] try to solve the multi-label learning directly.", "startOffset": 18, "endOffset": 57}, {"referenceID": 33, "context": "For instance, for tree based models for traditional MLC [Zhang and Zhang, 2010; Bi and Kwok, 2011], with the large feature dimension and the huge samples number, the trees will be giant which leads to the intractability for training.", "startOffset": 56, "endOffset": 98}, {"referenceID": 5, "context": "For instance, for tree based models for traditional MLC [Zhang and Zhang, 2010; Bi and Kwok, 2011], with the large feature dimension and the huge samples number, the trees will be giant which leads to the intractability for training.", "startOffset": 56, "endOffset": 98}, {"referenceID": 13, "context": "There is also a principled generalization for the naive one-against-all method [Hariharan et al., 2010] which cost-", "startOffset": 79, "endOffset": 103}, {"referenceID": 26, "context": "Meanwhile, in cross modal retrieval field, deep neural network [Wang et al., 2016] has been designed for learning the shared embedding space between images and texts.", "startOffset": 63, "endOffset": 82}, {"referenceID": 18, "context": "Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al.", "startOffset": 102, "endOffset": 160}, {"referenceID": 30, "context": "Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al.", "startOffset": 102, "endOffset": 160}, {"referenceID": 25, "context": "Motivated by the recent work in natural language processing for Network Representation Learning (NRL) [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al.", "startOffset": 102, "endOffset": 160}, {"referenceID": 30, "context": ", 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al., 2015; Tu et al., 2016] to define our label graph matrix by:", "startOffset": 87, "endOffset": 123}, {"referenceID": 25, "context": ", 2016] which aims to encode network structure into a low-dimensional space, we follow [Yang et al., 2015; Tu et al., 2016] to define our label graph matrix by:", "startOffset": 87, "endOffset": 123}, {"referenceID": 30, "context": "1 and the readers are referred to [Yang et al., 2015] for details.", "startOffset": 34, "endOffset": 53}, {"referenceID": 30, "context": "In [Yang et al., 2015], the authors first prove the state-of-theart network representation method DeepWalk [Perozzi et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 18, "context": ", 2015], the authors first prove the state-of-theart network representation method DeepWalk [Perozzi et al., 2014] is equivalent to perform matrix factorization on a matrix M\u0304 \u2208 R|V |\u00d7|V | where each entry M\u0304ij is logarithm of the average probability that vertex vi randomly walks to vertex vj in t steps.", "startOffset": 92, "endOffset": 114}, {"referenceID": 30, "context": "The rigorous definition is [Yang et al., 2015]:", "startOffset": 27, "endOffset": 46}, {"referenceID": 30, "context": "2 in line with [Yang et al., 2015] since log(M\u0304) has much more non-zero entries than M\u0304 and the complexity of matrix factorization is proportional to the number of non-zero entries.", "startOffset": 15, "endOffset": 34}, {"referenceID": 25, "context": "This tradeoff is further verified in recent work [Tu et al., 2016] for text representation.", "startOffset": 49, "endOffset": 66}, {"referenceID": 21, "context": "the Alternating Minimization Algorithm (AMA) [Sullivan, 1998] given the real-valued matrix M:", "startOffset": 45, "endOffset": 61}, {"referenceID": 12, "context": "the ReLU [Glorot et al., 2015] (speedup convergence) modules in in Fig.", "startOffset": 9, "endOffset": 30}, {"referenceID": 26, "context": "2 There are various siamese (twin) network architectures used in many recent work on deep learning [Wang et al., 2016; Bertinetto et al., 2016].", "startOffset": 99, "endOffset": 143}, {"referenceID": 3, "context": "2 There are various siamese (twin) network architectures used in many recent work on deep learning [Wang et al., 2016; Bertinetto et al., 2016].", "startOffset": 99, "endOffset": 143}, {"referenceID": 26, "context": "However, here our loss is the `2 regression loss rather than the popular ranking loss used in [Wang et al., 2016] as we cannot efficiently derive useful ranking information from the training data2.", "startOffset": 94, "endOffset": 113}, {"referenceID": 18, "context": "Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.", "startOffset": 125, "endOffset": 183}, {"referenceID": 30, "context": "Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.", "startOffset": 125, "endOffset": 183}, {"referenceID": 25, "context": "Perhaps more importantly, we are inspired by the recent success for unsupervised representation learning for the label space [Perozzi et al., 2014; Yang et al., 2015; Tu et al., 2016] thus we use the network in Fig.", "startOffset": 125, "endOffset": 183}, {"referenceID": 4, "context": "The prediction is relatively simple and standard, which is in line with SLEEC [Bhatia et al., 2015]: given a test sample,", "startOffset": 78, "endOffset": 99}, {"referenceID": 26, "context": "In the XML setting, similar to [Wang et al., 2016], the positive/negataive sample can be defined with a certain label dimension.", "startOffset": 31, "endOffset": 50}, {"referenceID": 19, "context": "It includes both small-scale dataset [Prabhu and Varma, 2014] and large-scale dataset [Bhatia et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": "It includes both small-scale dataset [Prabhu and Varma, 2014] and large-scale dataset [Bhatia et al., 2015], in comparison with state-of-the-art peer methods for both embedding based and tree based models.", "startOffset": 86, "endOffset": 107}, {"referenceID": 9, "context": "7 and the network is built by MXNet4 [Chen et al., 2015], which is a flexible and efficient library for deep learning and has been chosen by Amazon as the official deep learning framework for its web service.", "startOffset": 37, "endOffset": 56}, {"referenceID": 28, "context": "Datasets The tested multi-label datasets include WikiLSHTC (320K labels), DeliciousLarge [Wetzker et al., 2008] (200K labels) and Wiki10 [Zubiaga, 2012] (30K labels).", "startOffset": 89, "endOffset": 111}, {"referenceID": 34, "context": ", 2008] (200K labels) and Wiki10 [Zubiaga, 2012] (30K labels).", "startOffset": 33, "endOffset": 48}, {"referenceID": 16, "context": "Therefore, we also present comparisons on public relatively small datasets such as BibTex [Katakis et al., 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al.", "startOffset": 90, "endOffset": 136}, {"referenceID": 19, "context": "Therefore, we also present comparisons on public relatively small datasets such as BibTex [Katakis et al., 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al.", "startOffset": 90, "endOffset": 136}, {"referenceID": 20, "context": ", 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al., 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 19, "context": ", 2008; Prabhu and Varma, 2014], MediaMill [Snoek et al., 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 24, "context": ", 2006; Prabhu and Varma, 2014], Delicious [Tsoumakas et al., 2008] and EURLex [Menca and Furnkranz, 2008].", "startOffset": 43, "endOffset": 67}, {"referenceID": 17, "context": ", 2008] and EURLex [Menca and Furnkranz, 2008].", "startOffset": 19, "endOffset": 46}, {"referenceID": 4, "context": "Baseline algorithms for comparison Our primary focus is to compare with those state-of-the-art extreme multi-label classification methods, such as embedding based methods SLEEC [Bhatia et al., 2015], LEML [Yu et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 31, "context": ", 2015], LEML [Yu et al., 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 19, "context": ", 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 27, "context": ", 2014] and tree based like FastXML [Prabhu and Varma, 2014] and LPSR [Weston et al., 2013].", "startOffset": 70, "endOffset": 91}, {"referenceID": 15, "context": "com/dmlc/mxnet niques such as compressed sensing (CS) [Hsu et al., 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 8, "context": ", 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 6, "context": ", 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 14, "context": ", 2009], CPLST [Chen and Lin, 2012], ML-CSSP [Bi and Kwok, 2013], one-vs-all [Hariharan et al., 2012] can only be trained on small datasets using commodity computational hardware.", "startOffset": 77, "endOffset": 101}, {"referenceID": 4, "context": "Evaluation metrics The evaluation metric in [Bhatia et al., 2015] is precison@k, Precision at k (P@k)has been widely adopted as the choice of metric for evaluating extreme multilabel algorithms.", "startOffset": 44, "endOffset": 65}, {"referenceID": 21, "context": "3 by the AMA solver [Sullivan, 1998]).", "startOffset": 20, "endOffset": 36}], "year": 2017, "abstractText": "Extreme multi-label learning or classification has been a practical and important problem since the boom of big data. The main challenge lies in the exponential label space which involves 2 possible label sets when the label dimension L is very large e.g. in millions for Wikipedia labels. This paper is motivated to better explore the label space by building and modeling an explicit label graph. In the meanwhile, deep learning has been widely studied and used in various classification problems including multi-label classification, however it has not been sufficiently studied in this extreme but practical case, where the label space can be as large as in millions. In this paper, we propose a practical deep embedding method for extreme multi-label classification. Our method harvests the ideas of non-linear embedding and modeling label space with graph priors at the same time. Extensive experiments on public datasets for XML show that our method perform competitively against state-of-the-art result.", "creator": "LaTeX with hyperref package"}}}