{"id": "1704.03940", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "PACRR: A Position-Aware Neural IR Model for Relevance Matching", "abstract": "in order to adopt deep learning for ad - hoc information retrieval, it has is essential to establish suitable representations either of query - specification document complement pairs and to design the neural architectures... that are able simultaneously to digest such representations. moreover in particular, they ought possible to capture all relevant information required to assess the relevance of a document for a given user query, including term validity overlap as well as positional information such what as absolute proximity peaks and term dependencies. while previous crossover work has successfully captured unigram term matches, none has successfully used position - dependent information on a standard benchmark test collection. in this parallel work, we address this gap by encoding the relevance matching in terms of similarity matrices and using a deep model to digest such matrices. we present a novel model architecture consisting simply of special convolutional layers to capture term dependencies and proximity among query term occurrences, followed by a fourth recurrent layer to capture relevance over different query terms. extensive experiments on trec web track data searches confirm that the proposed model with similarity matrix representations yields improved search results.", "histories": [["v1", "Wed, 12 Apr 2017 21:56:59 GMT  (238kb,D)", "https://arxiv.org/abs/1704.03940v1", null], ["v2", "Sun, 7 May 2017 16:12:36 GMT  (260kb,D)", "http://arxiv.org/abs/1704.03940v2", "Experimental comparisons with baselines including Duet-local, DRMM and MatchPyramid"], ["v3", "Fri, 21 Jul 2017 23:02:06 GMT  (152kb,D)", "http://arxiv.org/abs/1704.03940v3", "To appear in EMNLP2017"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["kai hui", "andrew yates", "klaus berberich", "gerard de melo"], "accepted": true, "id": "1704.03940"}, "pdf": {"name": "1704.03940.pdf", "metadata": {"source": "CRF", "title": "PACRR: A Position-Aware Neural IR Model for Relevance Matching", "authors": ["Kai Hui", "Andrew Yates", "Klaus Berberich"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Despite the widespread use of deep neural models across a range of linguistic tasks, to what extent such models can improve information retrieval (IR) and which components a deep neural model for IR should include remain open questions. In ad-hoc IR, the goal is to produce a ranking of relevant documents given an open-domain (\u201cad hoc\u201d) query and a document collection. A ranking model thus aims at evaluating the interactions between different documents and a query, assigning higher scores to documents that better match the query. Learning to rank models, like the recent IRGAN model (Wang et al., 2017), rely on handcrafted features to encode query document interactions, e.g., the relevance scores from unsupervised ranking models. Neural IR models differ in that they extract interactions directly based on the queries and documents. Many early neural IR models can be categorized as seman-\ntic matching models, as they embed both queries and documents into a low-dimensional space, and then assess their similarity based on such dense representations. Examples in this regard include DSSM (Huang et al., 2013) and DESM (Mitra et al., 2016). The notion of relevance is inherently asymmetric, however, making it different from well-studied semantic matching tasks such as semantic relatedness and paraphrase detection. Instead, relevance matching models such as MatchPyramid (Pang et al., 2016), DRMM (Guo et al., 2016) and the recent K-NRM (Xiong et al., 2017) resemble traditional IR retrieval measures in that they directly consider the relevance of documents\u2019 contents with respect to the query. The DUET model (Mitra et al., 2017) is a hybrid approach that combines signals from a local model for relevance matching and a distributed model for semantic matching. The two classes of models are fairly distinct. In this work, we focus on relevance matching models.\nGiven that relevance matching approaches mirror ideas from traditional retrieval models, the decades of research on ad-hoc IR can guide us with regard to the specific kinds of relevance signals a model ought to capture. Unigram matches are the most obvious signals to be modeled, as a counterpart to the term frequencies that appear in almost all traditional retrieval models. Beyond this, positional information, including where query terms occur and how they depend on each other, can also be exploited, as demonstrated in retrieval models that are aware of term proximity (Tao and Zhai, 2007) and term dependencies (Huston and Croft, 2014; Metzler and Croft, 2005). Query coverage is another factor that can be used to ensure that, for queries with multiple terms, top-ranked documents contain multiple query terms rather than emphasizing only one query term. For example, given the query ar X iv :1 70 4.\n03 94\n0v 3\n[ cs\n.I R\n] 2\n1 Ju\nl 2 01\n7\n\u201cdog adoption requirements\u201d, unigram matching signals correspond to the occurrences of the individual terms \u201cdog\u201d, \u201cadoption\u201d, or \u201crequirements\u201d. When considering positional information, text passages with \u201cdog adoption\u201d or \u201crequirements for dog adoption\u201d are highlighted, distinguishing them from text that only includes individual terms. Query coverage, meanwhile, further emphasizes that matching signals for \u201cdog\u201d, \u201cadoption\u201d, and \u201crequirements\u201d should all be included in a document.\nSimilarity signals from unigram matches are taken as input by DRMM (Guo et al., 2016) after being summarized as histograms, whereas K-NRM (Xiong et al., 2017) directly digests a query-document similarity matrix and summarizes it with multiple kernel functions. As for positional information, both the MatchPyramid (Pang et al., 2016) and local DUET (Mitra et al., 2017) models account for it by incorporating convolutional layers based on similarity matrices between queries and documents. Although this leads to more complex models, both have difficulty in significantly outperforming the DRMM model (Guo et al., 2016; Mitra et al., 2017). This indicates that it is non-trivial to go beyond unigrams by utilizing positional information in deep neural IR models. Intuitively, unlike in standard sequencebased models, the interactions between a query and a document are sequential along the query axis as well as along the document axis, making the problem multi-dimensional in nature. In addition, this makes it non-trivial to combine matching signals from different parts of the documents and over different query terms. In fact, we argue that both MatchPyramid and local DUET models fail to fully account for one or more of the aforementioned factors. For example, as a pioneering work, MatchPyramid is mainly motivated by models developed in computer vision, resulting in its disregard of certain IR-specific considerations in the design of components, such as pooling sizes that ignore the query and document dimensions. Meanwhile, local DUET\u2019s CNN filters match entire documents against individual query terms, neglecting proximity and possible dependencies among different query terms.\nWe conjecture that a suitable combination of convolutional kernels and recurrent layers can lead to a model that better accounts for these factors. In particular, we present a novel re-ranking model\ncalled PACRR (Position-Aware ConvolutionalRecurrent Relevance Matching). Our approach first produces similarity matrices that record the semantic similarity between each query term and each individual term occurring in a document. These matrices are then fed through a series of convolutional, max-k-pooling, and recurrent layers so as to capture interactions corresponding to, for instance, bigram and trigram matches, and finally to aggregate the signals in order to produce global relevance assessments. In our model, the convolutional layers are designed to capture both unigram matching and positional information over text windows with different lengths; k-max pooling layers are along the query dimension, preserving matching signals over different query terms; the recurrent layer combines signals from different query terms to produce a query-document relevance score. Organization. The rest of this paper unfolds as follows. Section 2 describes our approach for computing similarity matrices and the architecture of our deep learning model. The setup and results of our extensive experimental evaluation can be found in Section 3, before concluding in Section 4."}, {"heading": "2 The PACRR Model", "text": "We now describe our proposed PACRR approach, which consists of two main parts: a relevance matching component that converts each querydocument pair into a similarity matrix sim |q|\u00d7|d| and a deep architecture that takes a given querydocument similarity matrix as input and produces a query-document relevance score rel(q, d). Note that in principle the proposed model can be trained end-to-end by backpropagating through the word embeddings, as in (Xiong et al., 2017). In this work, however, we focus on highlighting the building blocks aiming at capturing positional information, and freeze the word embedding layer to achieve better efficiency. The pipeline is summarized in Figure 1."}, {"heading": "2.1 Relevance Matching", "text": "We first encode the query-document relevance matching via query-document similarity matrices sim |q|\u00d7|d| that encodes the similarity between terms from a query q and a document d, where simij corresponds to the similarity between the i-th term from q and the j-th term from d. When using cosine similarity, we have\nsim \u2208 [\u22121, 1]|q|\u00d7|d|. As suggested in (Hui et al., 2017), query-document similarity matrices preserve a rich signal that can be used to perform relevance matching beyond unigram matches. In particular, n-gram matching corresponds to consecutive document terms that are highly similar to at least one of the query terms. Query coverage is reflected in the number of rows in sim that include at least one cell with high similarity. The similarity between a query term q and document term d is calculated by taking the cosine similarity using the pre-trained1 word2vec (Mikolov et al., 2013).\nThe subsequent processing in PACRR\u2019s convolutional layers requires that each query-document similarity matrix have the same dimensionality. Given that the lengths of queries and documents vary, we first transform the raw similarity matrices sim |q|\u00d7|d| into sim lq\u00d7ld matrices with uniform lq and ld as the number of rows and columns. We unify the query dimension lq by zero padding it to the maximum query length. With regard to the document dimension ld, we describe two strategies: firstk and kwindow.\nPACRR-firstk. Akin to (Mitra et al., 2017), the firstk distillation method simply keeps the first k columns in the matrix, which correspond to the first k terms in the document. If k > |d|, the remaining columns are zero padded.\n1https://code.google.com/archive/p/ word2vec/\nPACRR-kwindow. As suggested in (Guo et al., 2016), relevance matching is local. Document terms that have a low query similarity relative to a document\u2019s other terms cannot contribute substantially to the document\u2019s relevance score. Thus relevance matching can be extracted in terms of pieces of text that include relevant information. That is, one can segment documents according to relevance relative to the given query and retain only the text that is highly relevant to the given query. Given this observation, we prune query-document similarity cells with a low similarity score. In the case of unigrams, we simply choose the top ld terms with the highest similarity to query terms. In the case for text snippets beyond length n, we produce a similarity matrix simnlq\u00d7ld for each query-document pair and each n, because n consecutive terms must be co-considered later on. For each text snippet with length n in the document, kwindow calculates the maximum similarity between each term and the query terms, and then calculates the average similarity over each nterm window. It then selects the top k = bld/nc windows by averaging similarity and discards all other terms in the document. The document dimension is zero padded if bld/nc is not a multiple of k. When the convolutional layer later operates on a similarity matrix produced by kwindow, the model\u2019s stride is set to n since it can consider at most n consecutive terms that are present in the original document. This variant\u2019s output is a simi-\nlarity matrix simnlq\u00d7ld for each size n."}, {"heading": "2.2 Deep Retrieval Model", "text": "Given a query-document similarity matrix sim lq\u00d7ld as input, our deep architecture relies on convolutional layers to match every text snippet with length n in a query and in a document to produce similarity signals for different n. Subsequently, two consecutive max pooling layers extract the document\u2019s strongest similarity cues for each n. Finally, a recurrent layer aggregates these salient signals to predict a global query-document relevance score rel(q, d).\nConvolutional relevance matching over local text snippets. The purpose of this step is to match text snippets with different length from a query and a document given their query-document similarity matrix as input. This is accomplished by applying multiple two-dimensional convolutional layers with different kernel sizes to the input similarity matrix. Each convolutional layer is responsible for a specific n; by applying its kernel on n\u00d7n windows, it produces a similarity signal for each window. When the firstk method is used, each convolutional layer receives the same similarity matrix sim lq\u00d7ld as input because firstk produces the same similarity matrix regardless of the n. When the kwindow method is used, each convolutional layer receives a similarity matrix simnlq\u00d7ld corresponding to the convolutional layer with a n \u00d7 n kernel. We use lg\u22121 different convolutional layers with kernel sizes 2 \u00d7 2, 3 \u00d7 3, . . . , lg \u00d7 lg, corresponding to bi-gram, tri-gram, . . . , lg-gram matching, respectively, where the length of the longest text snippet to consider is governed by a hyperparameter lg. The original similarity matrix corresponds to unigram matching, while a convolutional layer with kernel size n\u00d7n is responsible for capturing matching signals on n-term text snippets. Each convolutional layer applies nf different filters to its input, where nf is another hyperparameter. We use a stride of size (1, 1) for the firstk distillation method, meaning that the convolutional kernel advances one step at a time in both the query and document dimensions. For the kwindow distillation method, we use a stride of (1, n) to move the convolutional kernel one step at a time in the query dimension, but n steps at a time in the document dimension. This ensures that the convolutional kernel only operates over consecutive terms that existed in the original document. Thus,\nwe end up with lg \u2212 1 matrices Cnlq\u00d7ld\u00d7nf , and the original similarity matrix is directly employed to handle the signals over unigrams.\nTwo max pooling layers. The purpose of this step is to capture the ns strongest similarity signals for each query term. Measuring the similarity signals separately for each query term allows the model to consider query term coverage, while capturing the ns strongest similarity signals for each query term allows the model to consider signals from different kinds of relevance matching patterns, e.g., n-gram matching and non-contiguous matching. In practice, we use a small ns to prevent the model from being biased by document length; while each similarity matrix contains the same number of document term scores, longer documents have more opportunity to contain terms that are similar to query terms. To capture the strongest ns similarity signals for each query term, we first perform max pooling over the filter dimension nf to keep only the strongest signal from the nf different filters, assuming that there only exists one particular true matching pattern in a given n \u00d7 n window, which serves different purposes compared with other tasks, such as the sub-sampling in computer vision. We then perform k-max pooling (Kalchbrenner et al., 2014) over the query dimension lq to keep the strongest ns similarity signals for each query term. Both pooling steps are performed on each of the lg \u2212 1 matrices Ci from the convolutional layer and on the original similarity matrix, which captures unigram matching, to produce the 3-dimensional tensor Plq\u00d7lg\u00d7ns . This tensor contains the ns strongest signals for each query term and for each n-gram size across all nf filters.\nRecurrent layer for global relevance. Finally, our model transforms the query term similarity signals in Plq\u00d7lg\u00d7ns into a single document relevance score rel(q, d). It achieves this by applying a recurrent layer toP , taking a sequence of vectors as input and learning weights to transform them into the final relevance score. More precisely, akin to (Guo et al., 2016), the IDF of each query term qi is passed through a softmax layer for normalization. Thereafter, we split up the query term dimension to produce a matrix Plg\u00d7ns for each query term qi, subsequently forming the recurrent layer\u2019s input by flattening each matrix Plg\u00d7ns into a vector by concatenating the matrix\u2019s rows together and appending query term qi\u2019s normalized IDF onto the end of the vector. This sequence\nof vectors for each query term qi is passed into a Long Short-Term Memory (LSTM) recurrent layer (Hochreiter and Schmidhuber, 1997) with an output dimensionality of one. That is, the LSTM\u2019s input is a sequence of query term vectors where each vector is composed of the query term\u2019s normalized IDF and the aforementioned salient signals for the query term along different kernel sizes. The LSTM\u2019s output is then used as our document relevance score rel(q, d).\nTraining objective. Our model is trained on triples consisting of a query q, relevant document d+, and non-relevant document d\u2212, minimizing a standard pairwise max margin loss as in Eq. 1.\nL(q,d+,d\u2212;\u0398)=max(0,1\u2212rel(q,d+)+rel(q,d\u2212)) (1)"}, {"heading": "3 Evaluation", "text": "In this section, we empirically evaluate PACRR models using manual relevance judgments from the standard TREC Web Track. We compare them against several state-of-the-art neural IR models2, including DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), MatchPyramid (Pang et al., 2016), and K-NRM (Xiong et al., 2017). The comparisons are over three task settings: reranking search results from a simple initial ranker (RERANKSIMPLE); re-ranking all runs from the TREC Web Track (RERANKALL); and examining neural IR models\u2019 classification accuracy between document pairs (PAIRACCURACY)."}, {"heading": "3.1 Experimental Setup", "text": "We rely on the widely-used 2009\u20132014 TREC Web Track ad-hoc task benchmarks3. The benchmarks are based on the CLUEWEB09 and CLUEWEB12 datasets as document collections. In total, there are 300 queries and more than 100k judgments (qrels). Three years (2012\u201314) of query-likelihood baselines4 provided by TREC5 serve as baseline runs in the RERANKSIMPLE benchmark. In the RERANKALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 (2010), 62 (2011), 48 (2012), 50 (2013), and 27\n2We also attempted to include IRGAN (Wang et al., 2017) model as a baseline, but failed to obtain reasonable results when training on TREC data.\n3http://trec.nist.gov/tracks.html 4Terrier (Ounis et al., 2006) version without filtering spam\ndocuments 5https://github.com/trec-web/ trec-web-2014\n(2014) runs. ERR@20 (Chapelle et al., 2009) and nDCG@20 (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002) are employed as evaluation measures, and both are computed with the script from TREC6.\nTraining. At each step, we perform Stochastic Gradient Descent (SGD) with a mini-batch of 32 triples. For the purpose of choosing the triples, we consider all documents that are judged with a label more relevant than Rel7 as highly relevant, and put the remaining relevant documents into a relevant group. To pick each triple, we sample a relevance group with probability proportional to the number of documents in the group within the training set, and then we randomly sample a document with the chosen label to serve as the positive document d+. If the chosen group is the highly relevant group, we randomly sample a document from the relevant group to serve as the negative document d\u2212. If the chosen group is the relevant group, we randomly sample a non-relevant document as d\u2212. This sampling procedure ensures that we differentiate between highly relevant documents (i.e., those with a relevance label of HRel, Key or Nav) and relevant documents (i.e., those are labeled as Rel). The training continues until a given number of iterations is reached. The model\n6http://trec.nist.gov/data/web/12/gdeval.pl 7Judgments from TREC include junk pages (Junk), nonrelevance (NRel), relevance (Rel), high relevance (HRel), key pages (Key) and navigational pages (Nav).\nis saved at every iteration. We use the model with the best ERR@20 on the validation set to make predictions. Proceeding in a round-robin manner, we report test results on one year by exploiting the respective remaining five years (250 queries) for training. From these 250 queries, we reserve 50 random queries as a held-out set for validation and hyper-parameter tuning, while the remaining 200 queries serve as the actual training set.\nAs mentioned, model parameters and training iterations are chosen by maximizing the ERR@20 on the validation set. The selected model is then used to make predictions on the test data. An example of this training procedure is shown in Figure 2. There are four hyper-parameters that govern the behavior of the proposed PACRR-kwindow and PACRR-firstk: the unified length of the document dimension ld, the k-max pooling size ns, the maximum n-gram size lg, and the number of filters used in convolutional layers nf . Due to limited computational resources, we determine the range of hyper-parameters to consider based on pilot experiments and domain insights. In particular, we evaluate ld \u2208 [256, 384, 512, 640, 768], ns \u2208 [1, 2, 3, 4], and lg \u2208 [2, 3, 4]. Due to the limited possible matching patterns given a small kernel size (e.g., lg = 3), nf is fixed to 32. For PACRR-firstk, we intuitively desire to retain as much information as possible from the input, and thus ld is always set to 768.\nDRMM (DRMMLCH\u00d7IDF ), DUET, MatchPyramid and K-NRM are trained under the same settings using the hyperparameters described in their respective papers. In particular, as our focus is on the deep relevance matching model as mentioned in Section 1, we only compare against DUET\u2019s local model, denoted as DUETL. In addition, K-NRM is trained slightly different from the one described in (Xiong et al., 2017), namely, with a frozen word embedding layer. This is to guarantee its fair comparison with other models, given that most of the compared models can be enhanced by co-training the embedding layers, whereas the focus here is the strength coming from the model architecture. A fully connected middle layer with 30 neurons is added to compensate for the reduction of trainable parameters in K-NRM, mirroring the size of DRMM\u2019s first fully connected layer.\nAll models are implemented with Keras (Chollet et al., 2015) using Tensorflow as backend, and are trained on servers with multiple CPU cores. In\nparticular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant."}, {"heading": "3.2 Results", "text": "RERANKSIMPLE. We first examine the proposed model by re-ranking the search results from the QL baseline on Web Track 2012\u201314. The results are summarized in Table 1. It can be seen that DRMM can significantly improve QL on WT12 and WT14, whereas MatchPyramid fails on WT12 under ERR@20. While DUETL and K-NRM can consistently outperform QL, the two variants of PACRR are the only models that can achieve significant improvements at a 95% significance level on all years under both ERR@20 and nDCG@20. More remarkably, by solely re-ranking the search results from QL, PACRR-firstk can already rank within the top-3 participating systems on all three years as measured by both ERR and nDCG. The re-ranked search results from PACRR-kwindow also ranks within the top-5 based on nDCG@20. On average, both PACRR-kwindow and PACRRfirstk achieve 60% improvements over QL.\nRERANKALL. In this part, we would like to further examine the performance of the proposed models in re-ranking different sets of search results. Thus, we extend our analysis to re-rank search results from all submitted runs from six years of the TREC Web Track ad-hoc task. In particular, we only consider the judged documents from TREC, which loosely correspond to top-20 documents in each run. The tested models make predictions for individual documents, which are used to re-rank the documents within each submitted run. Given that there are about 50 runs for each year, it is no longer feasible to list the scores for each re-ranked run. Instead, we summarize the results by comparing the performance of each run before and after re-ranking, and provide statistics over each year to compare the methods under consideration in Table 2. In the top portion of Table 2, we report the relative changes in metrics before and after re-ranking in terms of percentages (\u201caverage \u2206 measure score\u201d). In the bottom portion, we report the percentage of systems whose results have increased after re-ranking. Note that these results assess two different aspects: the average \u2206 measure score in Table 2 captures the degree to which a model can improve an initial run, while the percentages of runs indicate to what extent an\nimprovement can be achieved over runs from different systems. In other words, the former measures the strength of the models, while the latter measures the adaptability of the models. Both PACRR variants improve upon existing rankings by at least 10% across different years. Remarkably, in terms of nDCG@20, at least 80% of the submitted runs are improved after re-ranking by the proposed models on individual years, and on 2010\u201312, all submitted runs are consistently improved by PACRR-firstk. Moreover, both variants\nof PACRR can significantly outperform all baseline models on at least three years out of the six years in terms of average improvement. However, it is clear that none of the tested models can make consistent improvements over all submitted runs across all six years. In other words, there still exist document pairs that are predicted contradicting to the judgments from TREC. Thus, in the next part, we further investigate the performance in terms of prediction over document pairs.\nPAIRACCURACY. The ranking of documents can\nbe decomposed into rankings of document pairs as suggested in (Radinsky and Ailon, 2011). Specifically, a model\u2019s retrieval quality can be examined by checking across a range of individual document pairs, namely, how likely a model can assign a higher score for a more relevant document. Thus, it is possible for us to compare different models over the same set of complete judgments, removing the issue of different initial runs. Moreover, although ranking is our ultimate target, a direct inspection of pairwise prediction results can indicate which kinds of document pairs a model succeeds at or fails on. We first convert the graded judgments from TREC into ranked document pairs by comparing their labels. Document pairs are created among documents that have different labels. A prediction is counted as correct if it assigns a higher score to the document from the pair that is labeled with a higher degree of relevance. The judgments from TREC contain at most six relevance levels, and we merge and unify the original levels from the six years into four grades, namely, Nav, HRel, Rel and NRel. We compute the accuracy for each pair of labels. The statistics are summarized in Table 3. The volume column lists the percentage of a given label combination out of all document pairs, and the # query column provides the number of queries for which the label combination exists. In Table 3, we observe that both PACRR models always perform better than all baselines on label combinations HRel vs. NRel, Rel vs. NRel and Nav vs. NRel, which in total cover 90% of all document pairs. Meanwhile,\napart from Nav-Rel, there is no significant difference when distinguishing Nav from other types. K-NRM and DRMM perform better than the other two baseline models."}, {"heading": "3.3 Discussion", "text": "Hyper-parameters. As mentioned, models are selected based on the ERR@20 over validation data. Hence, it is sufficient to use a reasonable and representative validation dataset, rather than handpicking a specific set of parameter settings. However, to gain a better understanding of the influence of different hyper-parameters, we explore PACRR-kwindow\u2019s effectiveness when several hyper-parameters are varied. The results when re-ranking QL search results are given in Figure 3. The results are reported based on the models with the highest validation scores after fixing certain hyper-parameters. For example, the ERR@20 in the leftmost figure is obtained when fixing ld to the values shown. The crosses in Figure 3 correspond to the models that were selected for use on the test data, based on their validation set scores. It can be seen that the selected models are not necessarily the best model on the test data, as evidenced by the differences between validation and test data results, but we consistently obtain scores within a reasonable margin. Owing to space constraints, we omit the plots for PACRR-firstk.\nChoice between kwindow and firstk approaches. As mentioned, both PACRR-kwindow and PACRRfirstk serve to address the variable-length challenge for documents and queries, and to make the"}, {"heading": "256 384 512 640 768", "text": "training feasible and more efficient. In general, if both training and test documents are known to be short enough to fit in memory, then PACRR-firstk can be used directly. Otherwise, PACRR-kwindow is a reasonable choice to provide comparable results. Alternatively, one can regard this choice as another hyper-parameter, and make a selection based on held-out validation data.\nAccuracy in PAIRACCURACY. Beyond the observations in Section 3.2, we further examine the methods\u2019 accuracy over binary judgments by merging the Nav, HRel and Rel labels. The accuracies become 73.5%, 74.1% and 67.4% for PACRRkwindow, PACRR-firstk, and DRMM, respectively. Note that the manual judgments that indicate a document as relevant or non-relevant relative to a given query contain disagreements (Carterette et al., 2008; Voorhees, 2000) and errors (Alonso and Mizzaro, 2012). In particular, a 64% agreement (cf. Table 2 (b) therein) is observed over the inferred relative order among document pairs based on graded judgments from six trained judges (Carterette et al., 2008). When reproducing TREC judgments, Al-Maskari et al. (Al-Maskari et al., 2008) reported a 74% agreement (cf. Table 1 therein) with the original judgments from TREC when a group of users re-judged 56 queries on the TREC-8 document collections. Meanwhile, Alonso and Mizzaro (Alonso and Mizzaro, 2012) observed a 77% agreement relative to judgments from TREC when collecting judgments via crowdsourcing. Therefore, the more than 73% agreement achieved by both PACRR methods is close to the aforementioned agreement levels among different human assessors. However, when distinguishing Nav, HRel, and Rel, the tested models still fall significantly short of the human judges\u2019\nagreement levels. These distinctions are important for a successful ranker, especially when measuring with graded metrics such as ERR@20 and nDCG@20. Hence, further research is needed for better discrimination among relevant documents with different degrees of relevance. In addition, as for the distinction between Nav documents and Rel or HRel documents, we argue that since Nav actually indicates that a document mainly satisfies a navigational intent, this makes such documents qualitatively different from Rel and HRel documents. Specifically, a Nav is more relevant for a user with navigational intent, whereas for other users it may in some cases be less useful than a document that directly includes highly pertinent information content. Therefore, we hypothesize that further improvements can be obtained by introducing a classifier for user intents, e.g., navigational pages, before employing neural IR models."}, {"heading": "4 Conclusion", "text": "In this work, we have demonstrated the importance of preserving positional information for neural IR models by incorporating domain insights into the proposed PACRR model. In particular, PACRR captures term dependencies and proximity through multiple convolutional layers with different sizes. Thereafter, following two max-pooling layers, it combines salient signals over different query terms with a recurrent layer. Extensive experiments show that PACRR substantially outperforms four state-of-the-art neural IR models on TREC Web Track ad-hoc datasets and can dramatically improve search results when used as a reranking model."}], "references": [{"title": "Relevance judgments between trec and nontrec assessors", "author": ["Azzah Al-Maskari", "Mark Sanderson", "Paul Clough."], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 683\u2013", "citeRegEx": "Al.Maskari et al\\.,? 2008", "shortCiteRegEx": "Al.Maskari et al\\.", "year": 2008}, {"title": "Using crowdsourcing for trec relevance assessment", "author": ["Omar Alonso", "Stefano Mizzaro."], "venue": "Information Processing & Management, 48(6):1053\u2013 1066.", "citeRegEx": "Alonso and Mizzaro.,? 2012", "shortCiteRegEx": "Alonso and Mizzaro.", "year": 2012}, {"title": "Here or there: Preference Judgments for Relevance", "author": ["Ben Carterette", "Paul N Bennett", "David Maxwell Chickering", "Susan T Dumais."], "venue": "Advances in Information Retrieval, pages 16\u201327. Springer.", "citeRegEx": "Carterette et al\\.,? 2008", "shortCiteRegEx": "Carterette et al\\.", "year": 2008}, {"title": "Expected reciprocal rank for graded relevance", "author": ["Olivier Chapelle", "Donald Metlzer", "Ya Zhang", "Pierre Grinspan."], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, CIKM \u201909, pages 621\u2013630, New York, NY,", "citeRegEx": "Chapelle et al\\.,? 2009", "shortCiteRegEx": "Chapelle et al\\.", "year": 2009}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet"], "venue": "https:// github.com/fchollet/keras.", "citeRegEx": "Chollet,? 2015", "shortCiteRegEx": "Chollet", "year": 2015}, {"title": "A deep relevance matching model for ad-hoc retrieval", "author": ["Jiafeng Guo", "Yixing Fan", "Qingyao Ai", "W Bruce Croft."], "venue": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 55\u201364. ACM.", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of the 22Nd ACM International Conference on Information & Knowl-", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Position-aware representations for relevance matching in neural information retrieval", "author": ["Kai Hui", "Andrew Yates", "Klaus Berberich", "Gerard de Melo."], "venue": "Proceedings of the 26th International Conference on World Wide Web Companion, pages 799\u2013800. In-", "citeRegEx": "Hui et al\\.,? 2017", "shortCiteRegEx": "Hui et al\\.", "year": 2017}, {"title": "A comparison of retrieval models using term dependencies", "author": ["Samuel Huston", "W. Bruce Croft."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China,", "citeRegEx": "Huston and Croft.,? 2014", "shortCiteRegEx": "Huston and Croft.", "year": 2014}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen."], "venue": "ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446.", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.,? 2002", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.", "year": 2002}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1404.2188.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A markov random field model for term dependencies", "author": ["Donald Metzler", "W Bruce Croft."], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 472\u2013479. ACM.", "citeRegEx": "Metzler and Croft.,? 2005", "shortCiteRegEx": "Metzler and Croft.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to match using local and distributed representations of text for web search", "author": ["Bhaskar Mitra", "Fernando Diaz", "Nick Craswell."], "venue": "Proceedings of WWW 2017. ACM.", "citeRegEx": "Mitra et al\\.,? 2017", "shortCiteRegEx": "Mitra et al\\.", "year": 2017}, {"title": "A dual embedding space model for document ranking", "author": ["Bhaskar Mitra", "Eric Nalisnick", "Nick Craswell", "Rich Caruana."], "venue": "arXiv preprint arXiv:1602.01137.", "citeRegEx": "Mitra et al\\.,? 2016", "shortCiteRegEx": "Mitra et al\\.", "year": 2016}, {"title": "Terrier: A high performance and scalable information retrieval platform", "author": ["Iadh Ounis", "Gianni Amati", "Vassilis Plachouras", "Ben He", "Craig Macdonald", "Christina Lioma."], "venue": "Proceedings of the OSIR Workshop, pages 18\u201325.", "citeRegEx": "Ounis et al\\.,? 2006", "shortCiteRegEx": "Ounis et al\\.", "year": 2006}, {"title": "A study of matchpyramid models on ad-hoc retrieval", "author": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Xueqi Cheng."], "venue": "CoRR, abs/1606.04648.", "citeRegEx": "Pang et al\\.,? 2016", "shortCiteRegEx": "Pang et al\\.", "year": 2016}, {"title": "Ranking from pairs and triplets: Information quality, evaluation methods and query complexity", "author": ["Kira Radinsky", "Nir Ailon."], "venue": "Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM \u201911, pages 105\u2013", "citeRegEx": "Radinsky and Ailon.,? 2011", "shortCiteRegEx": "Radinsky and Ailon.", "year": 2011}, {"title": "An exploration of proximity measures in information retrieval", "author": ["Tao Tao", "ChengXiang Zhai."], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 295\u2013302. ACM.", "citeRegEx": "Tao and Zhai.,? 2007", "shortCiteRegEx": "Tao and Zhai.", "year": 2007}, {"title": "Variations in relevance judgments and the measurement of retrieval effectiveness", "author": ["Ellen M Voorhees."], "venue": "Information processing & management, 36(5):697\u2013716.", "citeRegEx": "Voorhees.,? 2000", "shortCiteRegEx": "Voorhees.", "year": 2000}, {"title": "Irgan: A minimax game for unifying generative and discriminative information retrieval models", "author": ["Jun Wang", "Lantao Yu", "Weinan Zhang", "Yu Gong", "Yinghui Xu", "Benyou Wang", "Peng Zhang", "Dell Zhang."], "venue": "arXiv preprint arXiv:1705.10513.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "End-to-end neural ad-hoc ranking with kernel pooling", "author": ["Chenyan Xiong", "Zhuyun Dai", "Jamie Callan", "Zhiyuan Liu", "Russell Power."], "venue": "arXiv preprint arXiv:1706.06613.", "citeRegEx": "Xiong et al\\.,? 2017", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 21, "context": "Learning to rank models, like the recent IRGAN model (Wang et al., 2017), rely on handcrafted features to encode query document interactions, e.", "startOffset": 53, "endOffset": 72}, {"referenceID": 7, "context": "Examples in this regard include DSSM (Huang et al., 2013) and DESM (Mitra et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 15, "context": ", 2013) and DESM (Mitra et al., 2016).", "startOffset": 17, "endOffset": 37}, {"referenceID": 17, "context": "Instead, relevance matching models such as MatchPyramid (Pang et al., 2016), DRMM (Guo et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 22, "context": "2016) and the recent K-NRM (Xiong et al., 2017) resemble traditional IR retrieval measures in that they directly consider the relevance of documents\u2019 contents with respect to the query.", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "The DUET model (Mitra et al., 2017) is a hybrid approach", "startOffset": 15, "endOffset": 35}, {"referenceID": 19, "context": "yond this, positional information, including where query terms occur and how they depend on each other, can also be exploited, as demonstrated in retrieval models that are aware of term proximity (Tao and Zhai, 2007) and term dependen-", "startOffset": 196, "endOffset": 216}, {"referenceID": 9, "context": "cies (Huston and Croft, 2014; Metzler and Croft, 2005).", "startOffset": 5, "endOffset": 54}, {"referenceID": 12, "context": "cies (Huston and Croft, 2014; Metzler and Croft, 2005).", "startOffset": 5, "endOffset": 54}, {"referenceID": 5, "context": "Similarity signals from unigram matches are taken as input by DRMM (Guo et al., 2016) after being summarized as histograms, whereas K-NRM (Xiong et al.", "startOffset": 67, "endOffset": 85}, {"referenceID": 22, "context": ", 2016) after being summarized as histograms, whereas K-NRM (Xiong et al., 2017) directly digests a", "startOffset": 60, "endOffset": 80}, {"referenceID": 17, "context": "As for positional information, both the MatchPyramid (Pang et al., 2016) and local DUET (Mitra et al.", "startOffset": 53, "endOffset": 72}, {"referenceID": 14, "context": ", 2016) and local DUET (Mitra et al., 2017) models account for it by incorporating convolu-", "startOffset": 23, "endOffset": 43}, {"referenceID": 5, "context": "Although this leads to more complex models, both have difficulty in significantly outperforming the DRMM model (Guo et al., 2016; Mitra et al., 2017).", "startOffset": 111, "endOffset": 149}, {"referenceID": 14, "context": "Although this leads to more complex models, both have difficulty in significantly outperforming the DRMM model (Guo et al., 2016; Mitra et al., 2017).", "startOffset": 111, "endOffset": 149}, {"referenceID": 22, "context": "Note that in principle the proposed model can be trained end-to-end by backpropagating through the word embeddings, as in (Xiong et al., 2017).", "startOffset": 122, "endOffset": 142}, {"referenceID": 8, "context": "As suggested in (Hui et al., 2017), query-document similarity matrices preserve a rich signal that can be used to perform", "startOffset": 16, "endOffset": 34}, {"referenceID": 13, "context": "The similarity between a query term q and document term d is calculated by taking the cosine similarity using the pre-trained1 word2vec (Mikolov et al., 2013).", "startOffset": 136, "endOffset": 158}, {"referenceID": 14, "context": "Akin to (Mitra et al., 2017), the firstk distillation method simply keeps the first k columns in the matrix, which correspond to the first k terms in the document.", "startOffset": 8, "endOffset": 28}, {"referenceID": 5, "context": "As suggested in (Guo et al., 2016), relevance matching is local.", "startOffset": 16, "endOffset": 34}, {"referenceID": 11, "context": "We then perform k-max pooling (Kalchbrenner et al., 2014) over the query dimension lq to keep the strongest ns similarity signals for each query term.", "startOffset": 30, "endOffset": 57}, {"referenceID": 5, "context": "More precisely, akin to (Guo et al., 2016), the IDF of each query term qi is passed through a softmax layer for normalization.", "startOffset": 24, "endOffset": 42}, {"referenceID": 6, "context": "of vectors for each query term qi is passed into a Long Short-Term Memory (LSTM) recurrent layer (Hochreiter and Schmidhuber, 1997) with an output dimensionality of one.", "startOffset": 97, "endOffset": 131}, {"referenceID": 5, "context": "We compare them against several state-of-the-art neural IR models2, including DRMM (Guo et al., 2016), DUET (Mitra et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 14, "context": ", 2016), DUET (Mitra et al., 2017), MatchPyramid (Pang", "startOffset": 14, "endOffset": 34}, {"referenceID": 22, "context": ", 2016), and K-NRM (Xiong et al., 2017).", "startOffset": 19, "endOffset": 39}, {"referenceID": 21, "context": "We also attempted to include IRGAN (Wang et al., 2017) model as a baseline, but failed to obtain reasonable results when training on TREC data.", "startOffset": 35, "endOffset": 54}, {"referenceID": 16, "context": "html Terrier (Ounis et al., 2006) version without filtering spam documents https://github.", "startOffset": 13, "endOffset": 33}, {"referenceID": 3, "context": "ERR@20 (Chapelle et al., 2009) and nDCG@20 (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) are", "startOffset": 7, "endOffset": 30}, {"referenceID": 10, "context": ", 2009) and nDCG@20 (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) are", "startOffset": 20, "endOffset": 51}, {"referenceID": 22, "context": "In addition, K-NRM is trained slightly different from the one described in (Xiong et al., 2017), namely, with a frozen word embedding layer.", "startOffset": 75, "endOffset": 95}, {"referenceID": 18, "context": "be decomposed into rankings of document pairs as suggested in (Radinsky and Ailon, 2011).", "startOffset": 62, "endOffset": 88}, {"referenceID": 2, "context": "document as relevant or non-relevant relative to a given query contain disagreements (Carterette et al., 2008; Voorhees, 2000) and errors (Alonso and Mizzaro, 2012).", "startOffset": 85, "endOffset": 126}, {"referenceID": 20, "context": "document as relevant or non-relevant relative to a given query contain disagreements (Carterette et al., 2008; Voorhees, 2000) and errors (Alonso and Mizzaro, 2012).", "startOffset": 85, "endOffset": 126}, {"referenceID": 1, "context": ", 2008; Voorhees, 2000) and errors (Alonso and Mizzaro, 2012).", "startOffset": 35, "endOffset": 61}, {"referenceID": 2, "context": "Table 2 (b) therein) is observed over the inferred relative order among document pairs based on graded judgments from six trained judges (Carterette et al., 2008).", "startOffset": 137, "endOffset": 162}, {"referenceID": 0, "context": "(Al-Maskari et al., 2008) reported a 74% agreement (cf.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Meanwhile, Alonso and Mizzaro (Alonso and Mizzaro, 2012) observed a 77% agreement relative to judgments from TREC when collecting judgments via crowdsourcing.", "startOffset": 30, "endOffset": 56}], "year": 2017, "abstractText": "In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years\u2019 TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks.", "creator": "LaTeX with hyperref package"}}}